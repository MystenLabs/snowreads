{"id":"2408.06827","title":"PRESENT: Zero-Shot Text-to-Prosody Control","authors":"Perry Lam, Huayun Zhang, Nancy F. Chen, Berrak Sisman, Dorien\n  Herremans","authorsParsed":[["Lam","Perry",""],["Zhang","Huayun",""],["Chen","Nancy F.",""],["Sisman","Berrak",""],["Herremans","Dorien",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 11:39:07 GMT"}],"updateDate":"2024-08-14","timestamp":1723549147000,"abstract":"  Current strategies for achieving fine-grained prosody control in speech\nsynthesis entail extracting additional style embeddings or adopting more\ncomplex architectures. To enable zero-shot application of pretrained\ntext-to-speech (TTS) models, we present PRESENT (PRosody Editing without Style\nEmbeddings or New Training), which exploits explicit prosody prediction in\nFastSpeech2-based models by modifying the inference process directly. We apply\nour text-to-prosody framework to zero-shot language transfer using a JETS model\nexclusively trained on English LJSpeech data. We obtain character error rates\n(CER) of 12.8%, 18.7% and 5.9% for German, Hungarian and Spanish respectively,\nbeating the previous state-of-the-art CER by over 2x for all three languages.\nFurthermore, we allow subphoneme-level control, a first in this field. To\nevaluate its effectiveness, we show that PRESENT can improve the prosody of\nquestions, and use it to generate Mandarin, a tonal language where vowel pitch\nvaries at subphoneme level. We attain 25.3% hanzi CER and 13.0% pinyin CER with\nthe JETS model. All our code and audio samples are available online.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}