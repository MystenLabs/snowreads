{"id":"2407.14103","title":"Zero-Shot Underwater Gesture Recognition","authors":"Sandipan Sarma, Gundameedi Sai Ram Mohan, Hariansh Sehgal, Arijit Sur","authorsParsed":[["Sarma","Sandipan",""],["Mohan","Gundameedi Sai Ram",""],["Sehgal","Hariansh",""],["Sur","Arijit",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 08:16:46 GMT"}],"updateDate":"2024-07-22","timestamp":1721377006000,"abstract":"  Hand gesture recognition allows humans to interact with machines\nnon-verbally, which has a huge application in underwater exploration using\nautonomous underwater vehicles. Recently, a new gesture-based language called\nCADDIAN has been devised for divers, and supervised learning methods have been\napplied to recognize the gestures with high accuracy. However, such methods\nfail when they encounter unseen gestures in real time. In this work, we\nadvocate the need for zero-shot underwater gesture recognition (ZSUGR), where\nthe objective is to train a model with visual samples of gestures from a few\n``seen'' classes only and transfer the gained knowledge at test time to\nrecognize semantically-similar unseen gesture classes as well. After discussing\nthe problem and dataset-specific challenges, we propose new seen-unseen splits\nfor gesture classes in CADDY dataset. Then, we present a two-stage framework,\nwhere a novel transformer learns strong visual gesture cues and feeds them to a\nconditional generative adversarial network that learns to mimic feature\ndistribution. We use the trained generator as a feature synthesizer for unseen\nclasses, enabling zero-shot learning. Extensive experiments demonstrate that\nour method outperforms the existing zero-shot techniques. We conclude by\nproviding useful insights into our framework and suggesting directions for\nfuture research.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4AlKbF8fwJDKxAF3H9e9QmgHIqShfN1mbBt9_mDCOPA","pdfSize":"10000416"}
