{"id":"2407.08723","title":"Topological Generalization Bounds for Discrete-Time Stochastic\n  Optimization Algorithms","authors":"Rayna Andreeva, Benjamin Dupuis, Rik Sarkar, Tolga Birdal, Umut\n  \\c{S}im\\c{s}ekli","authorsParsed":[["Andreeva","Rayna",""],["Dupuis","Benjamin",""],["Sarkar","Rik",""],["Birdal","Tolga",""],["Şimşekli","Umut",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:56:03 GMT"}],"updateDate":"2024-07-12","timestamp":1720720563000,"abstract":"  We present a novel set of rigorous and computationally efficient\ntopology-based complexity notions that exhibit a strong correlation with the\ngeneralization gap in modern deep neural networks (DNNs). DNNs show remarkable\ngeneralization properties, yet the source of these capabilities remains\nelusive, defying the established statistical learning theory. Recent studies\nhave revealed that properties of training trajectories can be indicative of\ngeneralization. Building on this insight, state-of-the-art methods have\nleveraged the topology of these trajectories, particularly their fractal\ndimension, to quantify generalization. Most existing works compute this\nquantity by assuming continuous- or infinite-time training dynamics,\ncomplicating the development of practical estimators capable of accurately\npredicting generalization without access to test data. In this paper, we\nrespect the discrete-time nature of training trajectories and investigate the\nunderlying topological quantities that can be amenable to topological data\nanalysis tools. This leads to a new family of reliable topological complexity\nmeasures that provably bound the generalization error, eliminating the need for\nrestrictive geometric assumptions. These measures are computationally friendly,\nenabling us to propose simple yet effective algorithms for computing\ngeneralization indices. Moreover, our flexible framework can be extended to\ndifferent domains, tasks, and architectures. Our experimental results\ndemonstrate that our new complexity measures correlate highly with\ngeneralization error in industry-standards architectures such as transformers\nand deep graph networks. Our approach consistently outperforms existing\ntopological bounds across a wide range of datasets, models, and optimizers,\nhighlighting the practical relevance and effectiveness of our complexity\nmeasures.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Algebraic Topology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}