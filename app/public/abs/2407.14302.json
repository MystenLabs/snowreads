{"id":"2407.14302","title":"Dyn-Adapter: Towards Disentangled Representation for Efficient Visual\n  Recognition","authors":"Yurong Zhang, Honghao Chen, Xinyu Zhang, Xiangxiang Chu, Li Song","authorsParsed":[["Zhang","Yurong",""],["Chen","Honghao",""],["Zhang","Xinyu",""],["Chu","Xiangxiang",""],["Song","Li",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 13:33:38 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 07:57:17 GMT"}],"updateDate":"2024-07-24","timestamp":1721396018000,"abstract":"  Parameter-efficient transfer learning (PETL) is a promising task, aiming to\nadapt the large-scale pre-trained model to downstream tasks with a relatively\nmodest cost. However, current PETL methods struggle in compressing\ncomputational complexity and bear a heavy inference burden due to the complete\nforward process. This paper presents an efficient visual recognition paradigm,\ncalled Dynamic Adapter (Dyn-Adapter), that boosts PETL efficiency by subtly\ndisentangling features in multiple levels. Our approach is simple: first, we\ndevise a dynamic architecture with balanced early heads for multi-level feature\nextraction, along with adaptive training strategy. Second, we introduce a\nbidirectional sparsity strategy driven by the pursuit of powerful\ngeneralization ability. These qualities enable us to fine-tune efficiently and\neffectively: we reduce FLOPs during inference by 50%, while maintaining or even\nyielding higher recognition accuracy. Extensive experiments on diverse datasets\nand pretrained backbones demonstrate the potential of Dyn-Adapter serving as a\ngeneral efficiency booster for PETL in vision recognition tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}