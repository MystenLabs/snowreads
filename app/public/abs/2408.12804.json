{"id":"2408.12804","title":"Universal dimensions of visual representation","authors":"Zirui Chen and Michael F. Bonner","authorsParsed":[["Chen","Zirui",""],["Bonner","Michael F.",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 02:48:44 GMT"}],"updateDate":"2024-08-26","timestamp":1724381324000,"abstract":"  Do neural network models of vision learn brain-aligned representations\nbecause they share architectural constraints and task objectives with\nbiological vision or because they learn universal features of natural image\nprocessing? We characterized the universality of hundreds of thousands of\nrepresentational dimensions from visual neural networks with varied\nconstruction. We found that networks with varied architectures and task\nobjectives learn to represent natural images using a shared set of latent\ndimensions, despite appearing highly distinct at a surface level. Next, by\ncomparing these networks with human brain representations measured with fMRI,\nwe found that the most brain-aligned representations in neural networks are\nthose that are universal and independent of a network's specific\ncharacteristics. Remarkably, each network can be reduced to fewer than ten of\nits most universal dimensions with little impact on its representational\nsimilarity to the human brain. These results suggest that the underlying\nsimilarities between artificial and biological vision are primarily governed by\na core set of universal image representations that are convergently learned by\ndiverse systems.\n","subjects":["Quantitative Biology/Neurons and Cognition","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"ERaiVPsVZbCty-YMU0Q8cH7FfLVpZD-98Fm59Y7yf5s","pdfSize":"21290825"}
