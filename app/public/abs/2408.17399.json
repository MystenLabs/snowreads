{"id":"2408.17399","title":"How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face\n  Recognition","authors":"Pedro C. Neto, Ivona Colakovic, Sa\\v{s}o Karakati\\v{c}, Ana F.\n  Sequeira","authorsParsed":[["Neto","Pedro C.",""],["Colakovic","Ivona",""],["Karakatič","Sašo",""],["Sequeira","Ana F.",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 16:35:28 GMT"}],"updateDate":"2024-09-02","timestamp":1725035728000,"abstract":"  Leveraging the capabilities of Knowledge Distillation (KD) strategies, we\ndevise a strategy to fight the recent retraction of face recognition datasets.\nGiven a pretrained Teacher model trained on a real dataset, we show that\ncarefully utilising synthetic datasets, or a mix between real and synthetic\ndatasets to distil knowledge from this teacher to smaller students can yield\nsurprising results. In this sense, we trained 33 different models with and\nwithout KD, on different datasets, with different architectures and losses. And\nour findings are consistent, using KD leads to performance gains across all\nethnicities and decreased bias. In addition, it helps to mitigate the\nperformance gap between real and synthetic datasets. This approach addresses\nthe limitations of synthetic data training, improving both the accuracy and\nfairness of face recognition models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}