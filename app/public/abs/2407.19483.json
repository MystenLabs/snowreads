{"id":"2407.19483","title":"Nearest-Neighbours Neural Network architecture for efficient sampling of\n  statistical physics models","authors":"Luca Maria Del Bono, Federico Ricci-Tersenghi, Francesco Zamponi","authorsParsed":[["Del Bono","Luca Maria",""],["Ricci-Tersenghi","Federico",""],["Zamponi","Francesco",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 12:26:44 GMT"}],"updateDate":"2024-07-30","timestamp":1722169604000,"abstract":"  The task of sampling efficiently the Gibbs-Boltzmann distribution of\ndisordered systems is important both for the theoretical understanding of these\nmodels and for the solution of practical optimization problems. Unfortunately,\nthis task is known to be hard, especially for spin glasses at low temperatures.\nRecently, many attempts have been made to tackle the problem by mixing\nclassical Monte Carlo schemes with newly devised Neural Networks that learn to\npropose smart moves. In this article we introduce the Nearest-Neighbours Neural\nNetwork (4N) architecture, a physically-interpretable deep architecture whose\nnumber of parameters scales linearly with the size of the system and that can\nbe applied to a large variety of topologies. We show that the 4N architecture\ncan accurately learn the Gibbs-Boltzmann distribution for the two-dimensional\nEdwards-Anderson model, and specifically for some of its most difficult\ninstances. In particular, it captures properties such as the energy, the\ncorrelation function and the overlap probability distribution. Finally, we show\nthat the 4N performance increases with the number of layers, in a way that\nclearly connects to the correlation length of the system, thus providing a\nsimple and interpretable criterion to choose the optimal depth.\n","subjects":["Condensed Matter/Disordered Systems and Neural Networks","Condensed Matter/Statistical Mechanics","Physics/Computational Physics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}