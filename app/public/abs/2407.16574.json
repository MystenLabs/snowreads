{"id":"2407.16574","title":"TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement\n  Learning from Human Feedback","authors":"Eunseop Yoon, Hee Suk Yoon, SooHwan Eom, Gunsoo Han, Daniel Wontae\n  Nam, Daejin Jo, Kyoung-Woon On, Mark A. Hasegawa-Johnson, Sungwoong Kim,\n  Chang D. Yoo","authorsParsed":[["Yoon","Eunseop",""],["Yoon","Hee Suk",""],["Eom","SooHwan",""],["Han","Gunsoo",""],["Nam","Daniel Wontae",""],["Jo","Daejin",""],["On","Kyoung-Woon",""],["Hasegawa-Johnson","Mark A.",""],["Kim","Sungwoong",""],["Yoo","Chang D.",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 15:27:37 GMT"}],"updateDate":"2024-07-24","timestamp":1721748457000,"abstract":"  Reinforcement Learning from Human Feedback (RLHF) leverages human preference\ndata to train language models to align more closely with human essence. These\nhuman preference data, however, are labeled at the sequence level, creating a\nmismatch between sequence-level preference labels and tokens, which are\nautoregressively generated from the language model. Although several recent\napproaches have tried to provide token-level (i.e., dense) rewards for each\nindividual token, these typically rely on predefined discrete reward values\n(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying\ndegrees of preference inherent to each token. To address this limitation, we\nintroduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a\ndiscriminator trained to distinguish positive and negative tokens, and the\nconfidence of the discriminator is used to assign continuous rewards to each\ntoken considering the context. Extensive experiments show that our proposed\nTLCR leads to consistent performance improvements over previous sequence-level\nor token-level discrete rewards on open-ended generation benchmarks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}