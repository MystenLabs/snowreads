{"id":"2408.15239","title":"Generative Inbetweening: Adapting Image-to-Video Models for Keyframe\n  Interpolation","authors":"Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman,\n  Aleksander Holynski, Steven M. Seitz","authorsParsed":[["Wang","Xiaojuan",""],["Zhou","Boyang",""],["Curless","Brian",""],["Kemelmacher-Shlizerman","Ira",""],["Holynski","Aleksander",""],["Seitz","Steven M.",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 17:57:14 GMT"}],"updateDate":"2024-08-28","timestamp":1724781434000,"abstract":"  We present a method for generating video sequences with coherent motion\nbetween a pair of input key frames. We adapt a pretrained large-scale\nimage-to-video diffusion model (originally trained to generate videos moving\nforward in time from a single input image) for key frame interpolation, i.e.,\nto produce a video in between two input frames. We accomplish this adaptation\nthrough a lightweight fine-tuning technique that produces a version of the\nmodel that instead predicts videos moving backwards in time from a single input\nimage. This model (along with the original forward-moving model) is\nsubsequently used in a dual-directional diffusion sampling process that\ncombines the overlapping model estimates starting from each of the two\nkeyframes. Our experiments show that our method outperforms both existing\ndiffusion-based methods and traditional frame interpolation techniques.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}