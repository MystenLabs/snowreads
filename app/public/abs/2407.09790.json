{"id":"2407.09790","title":"Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular\n  Prediction with Tree-hybrid MLPs","authors":"Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, Jian Wu","authorsParsed":[["Yan","Jiahuan",""],["Chen","Jintai",""],["Wang","Qianxing",""],["Chen","Danny Z.",""],["Wu","Jian",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 07:13:32 GMT"}],"updateDate":"2024-07-16","timestamp":1720854812000,"abstract":"  Tabular datasets play a crucial role in various applications. Thus,\ndeveloping efficient, effective, and widely compatible prediction algorithms\nfor tabular data is important. Currently, two prominent model types, Gradient\nBoosted Decision Trees (GBDTs) and Deep Neural Networks (DNNs), have\ndemonstrated performance advantages on distinct tabular prediction tasks.\nHowever, selecting an effective model for a specific tabular dataset is\nchallenging, often demanding time-consuming hyperparameter tuning. To address\nthis model selection dilemma, this paper proposes a new framework that\namalgamates the advantages of both GBDTs and DNNs, resulting in a DNN algorithm\nthat is as efficient as GBDTs and is competitively effective regardless of\ndataset preferences for GBDTs or DNNs. Our idea is rooted in an observation\nthat deep learning (DL) offers a larger parameter space that can represent a\nwell-performing GBDT model, yet the current back-propagation optimizer\nstruggles to efficiently discover such optimal functionality. On the other\nhand, during GBDT development, hard tree pruning, entropy-driven feature gate,\nand model ensemble have proved to be more adaptable to tabular data. By\ncombining these key components, we present a Tree-hybrid simple MLP (T-MLP). In\nour framework, a tensorized, rapidly trained GBDT feature gate, a DNN\narchitecture pruning approach, as well as a vanilla back-propagation optimizer\ncollaboratively train a randomly initialized MLP model. Comprehensive\nexperiments show that T-MLP is competitive with extensively tuned DNNs and\nGBDTs in their dominating tabular benchmarks (88 datasets) respectively, all\nachieved with compact model storage and significantly reduced training\nduration.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}