{"id":"2407.17011","title":"Unveiling In-Context Learning: A Coordinate System to Understand Its\n  Working Mechanism","authors":"Anhao Zhao, Fanghua Ye, Jinlan Fu, and Xiaoyu Shen","authorsParsed":[["Zhao","Anhao",""],["Ye","Fanghua",""],["Fu","Jinlan",""],["Shen","Xiaoyu",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 05:26:52 GMT"}],"updateDate":"2024-07-25","timestamp":1721798812000,"abstract":"  Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nattributes it to LLMs' inherent ability of task recognition, deeming label\ncorrectness and shot numbers of demonstrations as not crucial; the other\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether LLMs can recognize the task and whether similar examples are\npresented in the demonstrations. We propose the peak inverse rank metric to\ndetect the task recognition ability of LLMs and study LLMs' reactions to\ndifferent definitions of similarity. Based on these, we conduct extensive\nexperiments to elucidate how ICL functions across each quadrant on multiple\nrepresentative classification tasks. Finally, we extend our analyses to\ngeneration tasks, showing that our coordinate system can also be used to\ninterpret ICL for generation tasks effectively.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}