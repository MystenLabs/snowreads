{"id":"2407.05664","title":"How DNNs break the Curse of Dimensionality: Compositionality and\n  Symmetry Learning","authors":"Arthur Jacot, Seok Hoan Choi, Yuxiao Wen","authorsParsed":[["Jacot","Arthur",""],["Choi","Seok Hoan",""],["Wen","Yuxiao",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 06:59:29 GMT"}],"updateDate":"2024-07-09","timestamp":1720421969000,"abstract":"  We show that deep neural networks (DNNs) can efficiently learn any\ncomposition of functions with bounded $F_{1}$-norm, which allows DNNs to break\nthe curse of dimensionality in ways that shallow networks cannot. More\nspecifically, we derive a generalization bound that combines a covering number\nargument for compositionality, and the $F_{1}$-norm (or the related Barron\nnorm) for large width adaptivity. We show that the global minimizer of the\nregularized loss of DNNs can fit for example the composition of two functions\n$f^{*}=h\\circ g$ from a small number of observations, assuming $g$ is\nsmooth/regular and reduces the dimensionality (e.g. $g$ could be the modulo map\nof the symmetries of $f^{*}$), so that $h$ can be learned in spite of its low\nregularity. The measures of regularity we consider is the Sobolev norm with\ndifferent levels of differentiability, which is well adapted to the $F_{1}$\nnorm. We compute scaling laws empirically and observe phase transitions\ndepending on whether $g$ or $h$ is harder to learn, as predicted by our theory.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}