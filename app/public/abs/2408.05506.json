{"id":"2408.05506","title":"Your Context Is Not an Array: Unveiling Random Access Limitations in\n  Transformers","authors":"MohammadReza Ebrahimi, Sunny Panchal, Roland Memisevic","authorsParsed":[["Ebrahimi","MohammadReza",""],["Panchal","Sunny",""],["Memisevic","Roland",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 10:12:09 GMT"}],"updateDate":"2024-08-13","timestamp":1723284729000,"abstract":"  Despite their recent successes, Transformer-based large language models show\nsurprising failure modes. A well-known example of such failure modes is their\ninability to length-generalize: solving problem instances at inference time\nthat are longer than those seen during training. In this work, we further\nexplore the root cause of this failure by performing a detailed analysis of\nmodel behaviors on the simple parity task. Our analysis suggests that length\ngeneralization failures are intricately related to a model's inability to\nperform random memory accesses within its context window. We present supporting\nevidence for this hypothesis by demonstrating the effectiveness of\nmethodologies that circumvent the need for indexing or that enable random token\naccess indirectly, through content-based addressing. We further show where and\nhow the failure to perform random memory access manifests through attention map\nvisualizations.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}