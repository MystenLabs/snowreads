{"id":"2407.12684","title":"4Dynamic: Text-to-4D Generation with Hybrid Priors","authors":"Yu-Jie Yuan, Leif Kobbelt, Jiwen Liu, Yuan Zhang, Pengfei Wan, Yu-Kun\n  Lai, Lin Gao","authorsParsed":[["Yuan","Yu-Jie",""],["Kobbelt","Leif",""],["Liu","Jiwen",""],["Zhang","Yuan",""],["Wan","Pengfei",""],["Lai","Yu-Kun",""],["Gao","Lin",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 16:02:55 GMT"}],"updateDate":"2024-07-18","timestamp":1721232175000,"abstract":"  Due to the fascinating generative performance of text-to-image diffusion\nmodels, growing text-to-3D generation works explore distilling the 2D\ngenerative priors into 3D, using the score distillation sampling (SDS) loss, to\nbypass the data scarcity problem. The existing text-to-3D methods have achieved\npromising results in realism and 3D consistency, but text-to-4D generation\nstill faces challenges, including lack of realism and insufficient dynamic\nmotions. In this paper, we propose a novel method for text-to-4D generation,\nwhich ensures the dynamic amplitude and authenticity through direct supervision\nprovided by a video prior. Specifically, we adopt a text-to-video diffusion\nmodel to generate a reference video and divide 4D generation into two stages:\nstatic generation and dynamic generation. The static 3D generation is achieved\nunder the guidance of the input text and the first frame of the reference\nvideo, while in the dynamic generation stage, we introduce a customized SDS\nloss to ensure multi-view consistency, a video-based SDS loss to improve\ntemporal consistency, and most importantly, direct priors from the reference\nvideo to ensure the quality of geometry and texture. Moreover, we design a\nprior-switching training strategy to avoid conflicts between different priors\nand fully leverage the benefits of each prior. In addition, to enrich the\ngenerated motion, we further introduce a dynamic modeling representation\ncomposed of a deformation network and a topology network, which ensures dynamic\ncontinuity while modeling topological changes. Our method not only supports\ntext-to-4D generation but also enables 4D generation from monocular videos. The\ncomparison experiments demonstrate the superiority of our method compared to\nexisting methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}