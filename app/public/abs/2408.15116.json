{"id":"2408.15116","title":"Evaluating Stability of Unreflective Alignment","authors":"James Lucassen, Mark Henry, Philippa Wright, Owen Yeung","authorsParsed":[["Lucassen","James",""],["Henry","Mark",""],["Wright","Philippa",""],["Yeung","Owen",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 14:55:15 GMT"}],"updateDate":"2024-08-28","timestamp":1724770515000,"abstract":"  Many theoretical obstacles to AI alignment are consequences of reflective\nstability - the problem of designing alignment mechanisms that the AI would not\ndisable if given the option. However, problems stemming from reflective\nstability are not obviously present in current LLMs, leading to disagreement\nover whether they will need to be solved to enable safe delegation of cognitive\nlabor. In this paper, we propose Counterfactual Priority Change (CPC)\ndestabilization as a mechanism by which reflective stability problems may arise\nin future LLMs. We describe two risk factors for CPC-destabilization: 1)\nCPC-based stepping back and 2) preference instability. We develop preliminary\nevaluations for each of these risk factors, and apply them to frontier LLMs.\nOur findings indicate that in current LLMs, increased scale and capability are\nassociated with increases in both CPC-based stepping back and preference\ninstability, suggesting that CPC-destabilization may cause reflective stability\nproblems in future LLMs.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}