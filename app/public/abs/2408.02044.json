{"id":"2408.02044","title":"Fine-tuning multilingual language models in Twitter/X sentiment\n  analysis: a study on Eastern-European V4 languages","authors":"Tom\\'a\\v{s} Filip, Martin Pavl\\'i\\v{c}ek, Petr Sos\\'ik","authorsParsed":[["Filip","Tomáš",""],["Pavlíček","Martin",""],["Sosík","Petr",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 14:35:30 GMT"}],"updateDate":"2024-08-06","timestamp":1722782130000,"abstract":"  The aspect-based sentiment analysis (ABSA) is a standard NLP task with\nnumerous approaches and benchmarks, where large language models (LLM) represent\nthe current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data\nin underrepresented languages. On such narrow tasks, small tuned language\nmodels can often outperform universal large ones, providing available and cheap\nsolutions.\n  We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for\nclassification of sentiment towards Russia and Ukraine in the context of the\nongoing military conflict. The training/testing dataset was obtained from the\nacademic API from Twitter/X during 2023, narrowed to the languages of the V4\ncountries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their\nperformance under a variety of settings including translations, sentiment\ntargets, in-context learning and more, using GPT4 as a reference model. We\ndocument several interesting phenomena demonstrating, among others, that some\nmodels are much better fine-tunable on multilingual Twitter tasks than others,\nand that they can reach the SOTA level with a very small training set. Finally\nwe identify combinations of settings providing the best results.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}