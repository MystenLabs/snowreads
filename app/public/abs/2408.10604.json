{"id":"2408.10604","title":"Multilingual Non-Factoid Question Answering with Silver Answers","authors":"Ritwik Mishra, Sreeram Vennam, Rajiv Ratn Shah, Ponnurangam Kumaraguru","authorsParsed":[["Mishra","Ritwik",""],["Vennam","Sreeram",""],["Shah","Rajiv Ratn",""],["Kumaraguru","Ponnurangam",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 07:37:06 GMT"}],"updateDate":"2024-08-21","timestamp":1724139426000,"abstract":"  Most existing Question Answering Datasets (QuADs) primarily focus on\nfactoid-based short-context Question Answering (QA) in high-resource languages.\nHowever, the scope of such datasets for low-resource languages remains limited,\nwith only a few works centered on factoid-based QuADs and none on non-factoid\nQuADs. Therefore, this work presents MuNfQuAD, a multilingual QuAD with\nnon-factoid questions. It utilizes interrogative sub-headings from BBC news\narticles as questions and the corresponding paragraphs as silver answers. The\ndataset comprises over 370K QA pairs across 38 languages, encompassing several\nlow-resource languages, and stands as the largest multilingual QA dataset to\ndate. Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden\nset), we observe that 98\\% of questions can be answered using their\ncorresponding silver answer. Our fine-tuned Answer Paragraph Selection (APS)\nmodel outperforms the baselines. The APS model attained an accuracy of 80\\% and\n72\\%, as well as a macro F1 of 72\\% and 66\\%, on the MuNfQuAD testset and the\ngolden set, respectively. Furthermore, the APS model effectively generalizes\ncertain a language within the golden set, even after being fine-tuned on silver\nlabels.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}