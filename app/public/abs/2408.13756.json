{"id":"2408.13756","title":"Revisit the Partial Coloring Method: Prefix Spencer and Sampling","authors":"Dongrun Cai, Xue Chen, Wenxuan Shu, Haoyu Wang, Guangyi Zou","authorsParsed":[["Cai","Dongrun",""],["Chen","Xue",""],["Shu","Wenxuan",""],["Wang","Haoyu",""],["Zou","Guangyi",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 07:55:42 GMT"}],"updateDate":"2024-08-27","timestamp":1724572542000,"abstract":"  As the most powerful tool in discrepancy theory, the partial coloring method\nhas wide applications in many problems including the Beck-Fiala problem and\nSpencer's celebrated result. Currently, there are two major algorithmic methods\nfor the partial coloring method: the first approach uses linear algebraic\ntools; and the second is called Gaussian measure algorithm. We explore the\nadvantages of these two methods and show the following results for them\nseparately.\n  1. Spencer conjectured that the prefix discrepancy of any $\\mathbf{A} \\in\n\\{0,1\\}^{m \\times n}$ is $O(\\sqrt{m})$. We show how to find a partial coloring\nwith prefix discrepancy $O(\\sqrt{m})$ and $\\Omega(n)$ entries in $\\{ \\pm 1\\}$\nefficiently. To the best of our knowledge, this provides the first partial\ncoloring whose prefix discrepancy is almost optimal. However, unlike the\nclassical discrepancy problem, there is no reduction on the number of variables\n$n$ for the prefix problem. By recursively applying partial coloring, we obtain\na full coloring with prefix discrepancy $O(\\sqrt{m} \\cdot \\log\n\\frac{O(n)}{m})$. Prior to this work, the best bounds of the prefix Spencer\nconjecture for arbitrarily large $n$ were $2m$ and $O(\\sqrt{m \\log n})$.\n  2. Our second result extends the first linear algebraic approach to a\nsampling algorithm in Spencer's classical setting. On the first hand, Spencer\nproved that there are $1.99^m$ good colorings with discrepancy $O(\\sqrt{m})$.\nHence a natural question is to design efficient random sampling algorithms in\nSpencer's setting. On the other hand, some applications of discrepancy theory,\nprefer a random solution instead of a fixed one. Our second result is an\nefficient sampling algorithm whose random output has min-entropy $\\Omega(n)$\nand discrepancy $O(\\sqrt{m})$. Moreover, our technique extends the linear\nalgebraic framework by incorporating leverage scores of randomized matrix\nalgorithms.\n","subjects":["Computing Research Repository/Data Structures and Algorithms"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}