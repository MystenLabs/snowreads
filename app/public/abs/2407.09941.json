{"id":"2407.09941","title":"Hydra: Bidirectional State Space Models Through Generalized Matrix\n  Mixers","authors":"Sukjun Hwang, Aakash Lahoti, Tri Dao, Albert Gu","authorsParsed":[["Hwang","Sukjun",""],["Lahoti","Aakash",""],["Dao","Tri",""],["Gu","Albert",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 16:34:18 GMT"}],"updateDate":"2024-07-16","timestamp":1720888458000,"abstract":"  A wide array of sequence models are built on a framework modeled after\nTransformers, comprising alternating sequence mixer and channel mixer layers.\nThis paper studies a unifying matrix mixer view of sequence mixers that can be\nconceptualized as a linear map on the input sequence. This framework\nencompasses a broad range of well-known sequence models, including the\nself-attention of Transformers as well as recent strong alternatives such as\nstructured state space models (SSMs), and allows understanding downstream\ncharacteristics such as efficiency and expressivity through properties of their\nstructured matrix class. We identify a key axis of matrix parameterizations\ntermed sequence alignment, which increases the flexibility and performance of\nmatrix mixers, providing insights into the strong performance of Transformers\nand recent SSMs such as Mamba. Furthermore, the matrix mixer framework offers a\nsystematic approach to developing sequence mixers with desired properties,\nallowing us to develop several new sub-quadratic sequence models. In\nparticular, we propose a natural bidirectional extension of the Mamba model\n(Hydra), parameterized as a quasiseparable matrix mixer, which demonstrates\nsuperior performance over other sequence models including Transformers on\nnon-causal tasks. As a drop-in replacement for attention layers, Hydra\noutperforms BERT by 0.8 points on the GLUE benchmark and ViT by 2% Top-1\naccuracy on ImageNet.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}