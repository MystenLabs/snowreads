{"id":"2407.21631","title":"RoadFormer+: Delivering RGB-X Scene Parsing through Scale-Aware\n  Information Decoupling and Advanced Heterogeneous Feature Fusion","authors":"Jianxin Huang, Jiahang Li, Ning Jia, Yuxiang Sun, Chengju Liu, Qijun\n  Chen, and Rui Fan","authorsParsed":[["Huang","Jianxin",""],["Li","Jiahang",""],["Jia","Ning",""],["Sun","Yuxiang",""],["Liu","Chengju",""],["Chen","Qijun",""],["Fan","Rui",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 14:25:16 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 07:42:14 GMT"}],"updateDate":"2024-08-23","timestamp":1722435916000,"abstract":"  Task-specific data-fusion networks have marked considerable achievements in\nurban scene parsing. Among these networks, our recently proposed RoadFormer\nsuccessfully extracts heterogeneous features from RGB images and surface normal\nmaps and fuses these features through attention mechanisms, demonstrating\ncompelling efficacy in RGB-Normal road scene parsing. However, its performance\nsignificantly deteriorates when handling other types/sources of data or\nperforming more universal, all-category scene parsing tasks. To overcome these\nlimitations, this study introduces RoadFormer+, an efficient, robust, and\nadaptable model capable of effectively fusing RGB-X data, where ``X'',\nrepresents additional types/modalities of data such as depth, thermal, surface\nnormal, and polarization. Specifically, we propose a novel hybrid feature\ndecoupling encoder to extract heterogeneous features and decouple them into\nglobal and local components. These decoupled features are then fused through a\ndual-branch multi-scale heterogeneous feature fusion block, which employs\nparallel Transformer attentions and convolutional neural network modules to\nmerge multi-scale features across different scales and receptive fields. The\nfused features are subsequently fed into a decoder to generate the final\nsemantic predictions. Notably, our proposed RoadFormer+ ranks first on the\nKITTI Road benchmark and achieves state-of-the-art performance in mean\nintersection over union on the Cityscapes, MFNet, FMB, and ZJU datasets.\nMoreover, it reduces the number of learnable parameters by 65\\% compared to\nRoadFormer. Our source code will be publicly available at\nmias.group/RoadFormerPlus.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}