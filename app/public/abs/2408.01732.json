{"id":"2408.01732","title":"Landmark-guided Diffusion Model for High-fidelity and Temporally\n  Coherent Talking Head Generation","authors":"Jintao Tan, Xize Cheng, Lingyu Xiong, Lei Zhu, Xiandong Li, Xianjia\n  Wu, Kai Gong, Minglei Li, Yi Cai","authorsParsed":[["Tan","Jintao",""],["Cheng","Xize",""],["Xiong","Lingyu",""],["Zhu","Lei",""],["Li","Xiandong",""],["Wu","Xianjia",""],["Gong","Kai",""],["Li","Minglei",""],["Cai","Yi",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 10:19:38 GMT"}],"updateDate":"2024-08-06","timestamp":1722680378000,"abstract":"  Audio-driven talking head generation is a significant and challenging task\napplicable to various fields such as virtual avatars, film production, and\nonline conferences. However, the existing GAN-based models emphasize generating\nwell-synchronized lip shapes but overlook the visual quality of generated\nframes, while diffusion-based models prioritize generating high-quality frames\nbut neglect lip shape matching, resulting in jittery mouth movements. To\naddress the aforementioned problems, we introduce a two-stage diffusion-based\nmodel. The first stage involves generating synchronized facial landmarks based\non the given speech. In the second stage, these generated landmarks serve as a\ncondition in the denoising process, aiming to optimize mouth jitter issues and\ngenerate high-fidelity, well-synchronized, and temporally coherent talking head\nvideos. Extensive experiments demonstrate that our model yields the best\nperformance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"w-iH2I_U5iiRvCYNcWerRIR1mVbSy1EtCIJ-MkMikjA","pdfSize":"6878592"}
