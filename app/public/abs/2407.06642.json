{"id":"2407.06642","title":"Powerful and Flexible: Personalized Text-to-Image Generation via\n  Reinforcement Learning","authors":"Fanyue Wei, Wei Zeng, Zhenyang Li, Dawei Yin, Lixin Duan, Wen Li","authorsParsed":[["Wei","Fanyue",""],["Zeng","Wei",""],["Li","Zhenyang",""],["Yin","Dawei",""],["Duan","Lixin",""],["Li","Wen",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 08:11:53 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 15:34:04 GMT"}],"updateDate":"2024-07-19","timestamp":1720512713000,"abstract":"  Personalized text-to-image models allow users to generate varied styles of\nimages (specified with a sentence) for an object (specified with a set of\nreference images). While remarkable results have been achieved using\ndiffusion-based generation models, the visual structure and details of the\nobject are often unexpectedly changed during the diffusion process. One major\nreason is that these diffusion-based approaches typically adopt a simple\nreconstruction objective during training, which can hardly enforce appropriate\nstructural consistency between the generated and the reference images. To this\nend, in this paper, we design a novel reinforcement learning framework by\nutilizing the deterministic policy gradient method for personalized\ntext-to-image generation, with which various objectives, differential or even\nnon-differential, can be easily incorporated to supervise the diffusion models\nto improve the quality of the generated images. Experimental results on\npersonalized text-to-image generation benchmark datasets demonstrate that our\nproposed approach outperforms existing state-of-the-art methods by a large\nmargin on visual fidelity while maintaining text-alignment. Our code is\navailable at: \\url{https://github.com/wfanyue/DPG-T2I-Personalization}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}