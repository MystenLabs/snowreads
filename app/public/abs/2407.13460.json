{"id":"2407.13460","title":"SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by\n  Disentangled Variational Autoencoders","authors":"Sheng-Wei Li and Zi-Xiang Wei and Wei-Jie Chen and Yi-Hsin Yu and\n  Chih-Yuan Yang and Jane Yung-jen Hsu","authorsParsed":[["Li","Sheng-Wei",""],["Wei","Zi-Xiang",""],["Chen","Wei-Jie",""],["Yu","Yi-Hsin",""],["Yang","Chih-Yuan",""],["Hsu","Jane Yung-jen",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 12:35:46 GMT"}],"updateDate":"2024-07-19","timestamp":1721306146000,"abstract":"  Existing zero-shot skeleton-based action recognition methods utilize\nprojection networks to learn a shared latent space of skeleton features and\nsemantic embeddings. The inherent imbalance in action recognition datasets,\ncharacterized by variable skeleton sequences yet constant class labels,\npresents significant challenges for alignment. To address the imbalance, we\npropose SA-DVAE -- Semantic Alignment via Disentangled Variational\nAutoencoders, a method that first adopts feature disentanglement to separate\nskeleton features into two independent parts -- one is semantic-related and\nanother is irrelevant -- to better align skeleton and semantic features. We\nimplement this idea via a pair of modality-specific variational autoencoders\ncoupled with a total correction penalty. We conduct experiments on three\nbenchmark datasets: NTU RGB+D, NTU RGB+D 120 and PKU-MMD, and our experimental\nresults show that SA-DAVE produces improved performance over existing methods.\nThe code is available at https://github.com/pha123661/SA-DVAE.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}