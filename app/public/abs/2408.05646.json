{"id":"2408.05646","title":"Eigen Attention: Attention in Low-Rank Space for KV Cache Compression","authors":"Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy","authorsParsed":[["Saxena","Utkarsh",""],["Saha","Gobinda",""],["Choudhary","Sakshi",""],["Roy","Kaushik",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 22:47:12 GMT"}],"updateDate":"2024-08-13","timestamp":1723330032000,"abstract":"  Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gUOH5qX7cO3hMSoJkw2Ny64A7Ko7ra_BIQBv9AAf0c8","pdfSize":"931358"}
