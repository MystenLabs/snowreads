{"id":"2408.12590","title":"xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations","authors":"Can Qin, Congying Xia, Krithika Ramakrishnan, Michael Ryoo, Lifu Tu,\n  Yihao Feng, Manli Shu, Honglu Zhou, Anas Awadalla, Jun Wang, Senthil\n  Purushwalkam, Le Xue, Yingbo Zhou, Huan Wang, Silvio Savarese, Juan Carlos\n  Niebles, Zeyuan Chen, Ran Xu, Caiming Xiong","authorsParsed":[["Qin","Can",""],["Xia","Congying",""],["Ramakrishnan","Krithika",""],["Ryoo","Michael",""],["Tu","Lifu",""],["Feng","Yihao",""],["Shu","Manli",""],["Zhou","Honglu",""],["Awadalla","Anas",""],["Wang","Jun",""],["Purushwalkam","Senthil",""],["Xue","Le",""],["Zhou","Yingbo",""],["Wang","Huan",""],["Savarese","Silvio",""],["Niebles","Juan Carlos",""],["Chen","Zeyuan",""],["Xu","Ran",""],["Xiong","Caiming",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 17:55:22 GMT"},{"version":"v2","created":"Sat, 31 Aug 2024 05:12:09 GMT"}],"updateDate":"2024-09-04","timestamp":1724349322000,"abstract":"  We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}