{"id":"2408.14442","title":"Model Parallel Training and Transfer Learning for Convolutional Neural\n  Networks by Domain Decomposition","authors":"Axel Klawonn and Martin Lanser and Janine Weber","authorsParsed":[["Klawonn","Axel",""],["Lanser","Martin",""],["Weber","Janine",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 17:35:01 GMT"}],"updateDate":"2024-08-27","timestamp":1724693701000,"abstract":"  Deep convolutional neural networks (CNNs) have been shown to be very\nsuccessful in a wide range of image processing applications. However, due to\ntheir increasing number of model parameters and an increasing availability of\nlarge amounts of training data, parallelization strategies to efficiently train\ncomplex CNNs are necessary. In previous work by the authors, a novel model\nparallel CNN architecture was proposed which is loosely inspired by domain\ndecomposition. In particular, the novel network architecture is based on a\ndecomposition of the input data into smaller subimages. For each of these\nsubimages, local CNNs with a proportionally smaller number of parameters are\ntrained in parallel and the resulting local classifications are then aggregated\nin a second step by a dense feedforward neural network (DNN). In the present\nwork, we compare the resulting CNN-DNN architecture to less costly alternatives\nto combine the local classifications into a final, global decision.\nAdditionally, we investigate the performance of the CNN-DNN trained as one\ncoherent model as well as using a transfer learning strategy, where the\nparameters of the pre-trained local CNNs are used as initial values for a\nsubsequently trained global coherent CNN-DNN model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}