{"id":"2408.17129","title":"Controllable Edge-Type-Specific Interpretation in Multi-Relational Graph\n  Neural Networks for Drug Response Prediction","authors":"Xiaodi Li, Jianfeng Gui, Qian Gao, Haoyuan Shi, Zhenyu Yue","authorsParsed":[["Li","Xiaodi",""],["Gui","Jianfeng",""],["Gao","Qian",""],["Shi","Haoyuan",""],["Yue","Zhenyu",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 09:14:38 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 08:45:37 GMT"}],"updateDate":"2024-09-04","timestamp":1725009278000,"abstract":"  Graph Neural Networks have been widely applied in critical decision-making\nareas that demand interpretable predictions, leading to the flourishing\ndevelopment of interpretability algorithms. However, current graph\ninterpretability algorithms tend to emphasize generality and often overlook\nbiological significance, thereby limiting their applicability in predicting\ncancer drug responses. In this paper, we propose a novel post-hoc\ninterpretability algorithm for cancer drug response prediction, CETExplainer,\nwhich incorporates a controllable edge-type-specific weighting mechanism. It\nconsiders the mutual information between subgraphs and predictions, proposing a\nstructural scoring approach to provide fine-grained, biologically meaningful\nexplanations for predictive models. We also introduce a method for constructing\nground truth based on real-world datasets to quantitatively evaluate the\nproposed interpretability algorithm. Empirical analysis on the real-world\ndataset demonstrates that CETExplainer achieves superior stability and improves\nexplanation quality compared to leading algorithms, thereby offering a robust\nand insightful tool for cancer drug prediction.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"t3rE5IOC1pnlOUKSxB1dXTTm2RHjK0vyusKwSYT3EvA","pdfSize":"413765"}
