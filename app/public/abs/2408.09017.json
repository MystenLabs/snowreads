{"id":"2408.09017","title":"Meta Knowledge for Retrieval Augmented Large Language Models","authors":"Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan\n  Taws and Tarik Borogovac","authorsParsed":[["Mombaerts","Laurent",""],["Ding","Terry",""],["Banerjee","Adi",""],["Felice","Florian",""],["Taws","Jonathan",""],["Borogovac","Tarik",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 20:55:21 GMT"}],"updateDate":"2024-08-20","timestamp":1723841721000,"abstract":"  Retrieval Augmented Generation (RAG) is a technique used to augment Large\nLanguage Models (LLMs) with contextually relevant, time-critical, or\ndomain-specific information without altering the underlying model parameters.\nHowever, constructing RAG systems that can effectively synthesize information\nfrom large and diverse set of documents remains a significant challenge. We\nintroduce a novel data-centric RAG workflow for LLMs, transforming the\ntraditional retrieve-then-read system into a more advanced\nprepare-then-rewrite-then-retrieve-then-read framework, to achieve higher\ndomain expert-level understanding of the knowledge base. Our methodology relies\non generating metadata and synthetic Questions and Answers (QA) for each\ndocument, as well as introducing the new concept of Meta Knowledge Summary (MK\nSummary) for metadata-based clusters of documents. The proposed innovations\nenable personalized user-query augmentation and in-depth information retrieval\nacross the knowledge base. Our research makes two significant contributions:\nusing LLMs as evaluators and employing new comparative performance metrics, we\ndemonstrate that (1) using augmented queries with synthetic question matching\nsignificantly outperforms traditional RAG pipelines that rely on document\nchunking (p < 0.01), and (2) meta knowledge-augmented queries additionally\nsignificantly improve retrieval precision and recall, as well as the final\nanswers breadth, depth, relevancy, and specificity. Our methodology is\ncost-effective, costing less than $20 per 2000 research papers using Claude 3\nHaiku, and can be adapted with any fine-tuning of either the language or\nembedding models to further enhance the performance of end-to-end RAG\npipelines.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}