{"id":"2407.12212","title":"Generalized Coverage for More Robust Low-Budget Active Learning","authors":"Wonho Bae, Junhyug Noh, Danica J. Sutherland","authorsParsed":[["Bae","Wonho",""],["Noh","Junhyug",""],["Sutherland","Danica J.",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 23:21:51 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 18:43:22 GMT"}],"updateDate":"2024-07-26","timestamp":1721172111000,"abstract":"  The ProbCover method of Yehuda et al. is a well-motivated algorithm for\nactive learning in low-budget regimes, which attempts to \"cover\" the data\ndistribution with balls of a given radius at selected data points. We\ndemonstrate, however, that the performance of this algorithm is extremely\nsensitive to the choice of this radius hyper-parameter, and that tuning it is\nquite difficult, with the original heuristic frequently failing. We thus\nintroduce (and theoretically motivate) a generalized notion of \"coverage,\"\nincluding ProbCover's objective as a special case, but also allowing smoother\nnotions that are far more robust to hyper-parameter choice. We propose an\nefficient greedy method to optimize this coverage, generalizing ProbCover's\nalgorithm; due to its close connection to kernel herding, we call it\n\"MaxHerding.\" The objective can also be optimized non-greedily through a\nvariant of $k$-medoids, clarifying the relationship to other low-budget active\nlearning methods. In comprehensive experiments, MaxHerding surpasses existing\nactive learning methods across multiple low-budget image classification\nbenchmarks, and does so with less computational cost than most competitive\nmethods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}