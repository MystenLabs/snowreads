{"id":"2408.03247","title":"Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons","authors":"Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel\n  Dajun Zeng","authorsParsed":[["Wang","Yifei",""],["Chen","Yuheng",""],["Wen","Wanting",""],["Sheng","Yu",""],["Li","Linjing",""],["Zeng","Daniel Dajun",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 15:07:08 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 02:16:23 GMT"}],"updateDate":"2024-08-14","timestamp":1722956828000,"abstract":"  In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bMwItarfFYK44kpTSmPm0zMfN7TqIRrdq2sVa9BFIgk","pdfSize":"2169686"}
