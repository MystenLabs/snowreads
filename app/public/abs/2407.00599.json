{"id":"2407.00599","title":"Parm: Efficient Training of Large Sparsely-Activated Models with\n  Dedicated Schedules","authors":"Xinglin Pan, Wenxiang Lin, Shaohuai Shi, Xiaowen Chu, Weinong Sun, Bo\n  Li","authorsParsed":[["Pan","Xinglin",""],["Lin","Wenxiang",""],["Shi","Shaohuai",""],["Chu","Xiaowen",""],["Sun","Weinong",""],["Li","Bo",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 05:55:11 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 01:51:11 GMT"}],"updateDate":"2024-07-04","timestamp":1719726911000,"abstract":"  Sparsely-activated Mixture-of-Expert (MoE) layers have found practical\napplications in enlarging the model size of large-scale foundation models, with\nonly a sub-linear increase in computation demands. Despite the wide adoption of\nhybrid parallel paradigms like model parallelism, expert parallelism, and\nexpert-sharding parallelism (i.e., MP+EP+ESP) to support MoE model training on\nGPU clusters, the training efficiency is hindered by communication costs\nintroduced by these parallel paradigms. To address this limitation, we propose\nParm, a system that accelerates MP+EP+ESP training by designing two dedicated\nschedules for placing communication tasks. The proposed schedules eliminate\nredundant computations and communications and enable overlaps between\nintra-node and inter-node communications, ultimately reducing the overall\ntraining time. As the two schedules are not mutually exclusive, we provide\ncomprehensive theoretical analyses and derive an automatic and accurate\nsolution to determine which schedule should be applied in different scenarios.\nExperimental results on an 8-GPU server and a 32-GPU cluster demonstrate that\nParm outperforms the state-of-the-art MoE training system, DeepSpeed-MoE,\nachieving 1.13$\\times$ to 5.77$\\times$ speedup on 1296 manually configured MoE\nlayers and approximately 3$\\times$ improvement on two real-world MoE models\nbased on BERT and GPT-2.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}