{"id":"2408.06741","title":"Improving Synthetic Image Detection Towards Generalization: An Image\n  Transformation Perspective","authors":"Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, Fuli Feng","authorsParsed":[["Li","Ouxiang",""],["Cai","Jiayin",""],["Hao","Yanbin",""],["Jiang","Xiaolong",""],["Hu","Yao",""],["Feng","Fuli",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 09:01:12 GMT"}],"updateDate":"2024-08-14","timestamp":1723539672000,"abstract":"  With recent generative models facilitating photo-realistic image synthesis,\nthe proliferation of synthetic images has also engendered certain negative\nimpacts on social platforms, thereby raising an urgent imperative to develop\neffective detectors. Current synthetic image detection (SID) pipelines are\nprimarily dedicated to crafting universal artifact features, accompanied by an\noversight about SID training paradigm. In this paper, we re-examine the SID\nproblem and identify two prevalent biases in current training paradigms, i.e.,\nweakened artifact features and overfitted artifact features. Meanwhile, we\ndiscover that the imaging mechanism of synthetic images contributes to\nheightened local correlations among pixels, suggesting that detectors should be\nequipped with local awareness. In this light, we propose SAFE, a lightweight\nand effective detector with three simple image transformations. Firstly, for\nweakened artifact features, we substitute the down-sampling operator with the\ncrop operator in image pre-processing to help circumvent artifact distortion.\nSecondly, for overfitted artifact features, we include ColorJitter and\nRandomRotation as additional data augmentations, to help alleviate irrelevant\nbiases from color discrepancies and semantic differences in limited training\nsamples. Thirdly, for local awareness, we propose a patch-based random masking\nstrategy tailored for SID, forcing the detector to focus on local regions at\ntraining. Comparative experiments are conducted on an open-world dataset,\ncomprising synthetic images generated by 26 distinct generative models. Our\npipeline achieves a new state-of-the-art performance, with remarkable\nimprovements of 4.5% in accuracy and 2.9% in average precision against existing\nmethods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}