{"id":"2407.12514","title":"On Initializing Transformers with Pre-trained Embeddings","authors":"Ha Young Kim, Niranjan Balasubramanian, Byungkon Kang","authorsParsed":[["Kim","Ha Young",""],["Balasubramanian","Niranjan",""],["Kang","Byungkon",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 11:57:10 GMT"}],"updateDate":"2024-07-18","timestamp":1721217430000,"abstract":"  It has become common practice now to use random initialization schemes,\nrather than the pre-trained embeddings, when training transformer based models\nfrom scratch. Indeed, we find that pre-trained word embeddings from GloVe, and\nsome sub-word embeddings extracted from language models such as T5 and mT5 fare\nmuch worse compared to random initialization. This is counter-intuitive given\nthe well-known representational and transfer-learning advantages of\npre-training. Interestingly, we also find that BERT and mBERT embeddings fare\nbetter than random initialization, showing the advantages of pre-trained\nrepresentations. In this work, we posit two potential factors that contribute\nto these mixed results: the model sensitivity to parameter distribution and the\nembedding interactions with position encodings. We observe that pre-trained\nGloVe, T5, and mT5 embeddings have a wider distribution of values. As argued in\nthe initialization studies, such large value initializations can lead to poor\ntraining because of saturated outputs. Further, the larger embedding values\ncan, in effect, absorb the smaller position encoding values when added\ntogether, thus losing position information. Standardizing the pre-trained\nembeddings to a narrow range (e.g. as prescribed by Xavier) leads to\nsubstantial gains for Glove, T5, and mT5 embeddings. On the other hand, BERT\npre-trained embeddings, while larger, are still relatively closer to Xavier\ninitialization range which may allow it to effectively transfer the pre-trained\nknowledge.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mGzDLhvNDdhp4Iq0smzVE8Tw2RC5iSTLpemP0iw8PHo","pdfSize":"498127","objectId":"0x6d99a5850bdff70a1a1466422492d424151d7e767f19b40f7990bfd3623580af","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
