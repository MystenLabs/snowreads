{"id":"2407.13368","title":"Affordance Perception by a Knowledge-Guided Vision-Language Model with\n  Efficient Error Correction","authors":"Gertjan Burghouts, Marianne Schaaphok, Michael van Bekkum, Wouter\n  Meijer, Fieke Hillerstr\\\"om, Jelle van Mil","authorsParsed":[["Burghouts","Gertjan",""],["Schaaphok","Marianne",""],["van Bekkum","Michael",""],["Meijer","Wouter",""],["Hillerstr√∂m","Fieke",""],["van Mil","Jelle",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 10:24:22 GMT"}],"updateDate":"2024-07-19","timestamp":1721298262000,"abstract":"  Mobile robot platforms will increasingly be tasked with activities that\ninvolve grasping and manipulating objects in open world environments.\nAffordance understanding provides a robot with means to realise its goals and\nexecute its tasks, e.g. to achieve autonomous navigation in unknown buildings\nwhere it has to find doors and ways to open these. In order to get actionable\nsuggestions, robots need to be able to distinguish subtle differences between\nobjects, as they may result in different action sequences: doorknobs require\ngrasp and twist, while handlebars require grasp and push. In this paper, we\nimprove affordance perception for a robot in an open-world setting. Our\ncontribution is threefold: (1) We provide an affordance representation with\nprecise, actionable affordances; (2) We connect this knowledge base to a\nfoundational vision-language models (VLM) and prompt the VLM for a wider\nvariety of new and unseen objects; (3) We apply a human-in-the-loop for\ncorrections on the output of the VLM. The mix of affordance representation,\nimage detection and a human-in-the-loop is effective for a robot to search for\nobjects to achieve its goals. We have demonstrated this in a scenario of\nfinding various doors and the many different ways to open them.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"x4GnloQ40uIzslnCoBiyzSu8g3ZGVBGIA9qdZKl5Aos","pdfSize":"2547995"}
