{"id":"2407.13237","title":"LLM-Empowered State Representation for Reinforcement Learning","authors":"Boyuan Wang, Yun Qu, Yuhang Jiang, Jianzhun Shao, Chang Liu, Wenming\n  Yang, Xiangyang Ji","authorsParsed":[["Wang","Boyuan",""],["Qu","Yun",""],["Jiang","Yuhang",""],["Shao","Jianzhun",""],["Liu","Chang",""],["Yang","Wenming",""],["Ji","Xiangyang",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 07:47:51 GMT"}],"updateDate":"2024-07-19","timestamp":1721288871000,"abstract":"  Conventional state representations in reinforcement learning often omit\ncritical task-related details, presenting a significant challenge for value\nnetworks in establishing accurate mappings from states to task rewards.\nTraditional methods typically depend on extensive sample learning to enrich\nstate representations with task-specific information, which leads to low sample\nefficiency and high time costs. Recently, surging knowledgeable large language\nmodels (LLM) have provided promising substitutes for prior injection with\nminimal human intervention. Motivated by this, we propose LLM-Empowered State\nRepresentation (LESR), a novel approach that utilizes LLM to autonomously\ngenerate task-related state representation codes which help to enhance the\ncontinuity of network mappings and facilitate efficient training. Experimental\nresults demonstrate LESR exhibits high sample efficiency and outperforms\nstate-of-the-art baselines by an average of 29% in accumulated reward in Mujoco\ntasks and 30% in success rates in Gym-Robotics tasks.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}