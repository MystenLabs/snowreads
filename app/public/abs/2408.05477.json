{"id":"2408.05477","title":"Scene123: One Prompt to 3D Scene Generation via Video-Assisted and\n  Consistency-Enhanced MAE","authors":"Yiying Yang, Fukun Yin, Jiayuan Fan, Xin Chen, Wanzhang Li, Gang Yu","authorsParsed":[["Yang","Yiying",""],["Yin","Fukun",""],["Fan","Jiayuan",""],["Chen","Xin",""],["Li","Wanzhang",""],["Yu","Gang",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 08:09:57 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 10:16:00 GMT"}],"updateDate":"2024-08-21","timestamp":1723277397000,"abstract":"  As Artificial Intelligence Generated Content (AIGC) advances, a variety of\nmethods have been developed to generate text, images, videos, and 3D objects\nfrom single or multimodal inputs, contributing efforts to emulate human-like\ncognitive content creation. However, generating realistic large-scale scenes\nfrom a single input presents a challenge due to the complexities involved in\nensuring consistency across extrapolated views generated by models. Benefiting\nfrom recent video generation models and implicit neural representations, we\npropose Scene123, a 3D scene generation model, that not only ensures realism\nand diversity through the video generation framework but also uses implicit\nneural fields combined with Masked Autoencoders (MAE) to effectively ensures\nthe consistency of unseen areas across views. Specifically, we initially warp\nthe input image (or an image generated from text) to simulate adjacent views,\nfilling the invisible areas with the MAE model. However, these filled images\nusually fail to maintain view consistency, thus we utilize the produced views\nto optimize a neural radiance field, enhancing geometric consistency.\n  Moreover, to further enhance the details and texture fidelity of generated\nviews, we employ a GAN-based Loss against images derived from the input image\nthrough the video generation model. Extensive experiments demonstrate that our\nmethod can generate realistic and consistent scenes from a single prompt. Both\nqualitative and quantitative results indicate that our approach surpasses\nexisting state-of-the-art methods. We show encourage video examples at\nhttps://yiyingyang12.github.io/Scene123.github.io/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}