{"id":"2407.07577","title":"IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language\n  Model","authors":"Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng\n  Xiao, Sidi Yang, Yujiu Yang, Ping Luo","authorsParsed":[["Ji","Yatai",""],["Zhang","Shilong",""],["Wu","Jie",""],["Sun","Peize",""],["Chen","Weifeng",""],["Xiao","Xuefeng",""],["Yang","Sidi",""],["Yang","Yujiu",""],["Luo","Ping",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 12:11:59 GMT"}],"updateDate":"2024-07-11","timestamp":1720613519000,"abstract":"  The rapid advancement of Large Vision-Language models (LVLMs) has\ndemonstrated a spectrum of emergent capabilities. Nevertheless, current models\nonly focus on the visual content of a single scenario, while their ability to\nassociate instances across different scenes has not yet been explored, which is\nessential for understanding complex visual content, such as movies with\nmultiple characters and intricate plots. Towards movie understanding, a\ncritical initial step for LVLMs is to unleash the potential of character\nidentities memory and recognition across multiple visual scenarios. To achieve\nthe goal, we propose visual instruction tuning with ID reference and develop an\nID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research\nintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and\nrecognition across four dimensions: matching, location, question-answering, and\ncaptioning. Our findings highlight the limitations of existing LVLMs in\nrecognizing and associating instance identities with ID reference. This paper\npaves the way for future artificial intelligence systems to possess\nmulti-identity visual inputs, thereby facilitating the comprehension of complex\nvisual narratives like movies.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}