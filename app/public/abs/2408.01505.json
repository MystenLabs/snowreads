{"id":"2408.01505","title":"MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a\n  Mixture of Dyadic Experts","authors":"Lin Ning, Harsh Lara, Meiqi Guo, Abhinav Rastogi","authorsParsed":[["Ning","Lin",""],["Lara","Harsh",""],["Guo","Meiqi",""],["Rastogi","Abhinav",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 18:05:10 GMT"}],"updateDate":"2024-08-06","timestamp":1722621910000,"abstract":"  Parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA)\nhave revolutionized the adaptation of large language models (LLMs) to diverse\ntasks. Recent efforts have explored mixtures of LoRA modules for multi-task\nsettings. However, our analysis reveals redundancy in the down-projection\nmatrices of these architectures. This observation motivates our proposed\nmethod, Mixture of Dyadic Experts (MoDE), which introduces a novel design for\nefficient multi-task adaptation. This is done by sharing the down-projection\nmatrix across tasks and employing atomic rank-one adapters, coupled with\nrouters that allow more sophisticated task-level specialization. Our design\nallows for more fine-grained mixing, thereby increasing the model's ability to\njointly handle multiple tasks. We evaluate MoDE on the Supernatural\nInstructions (SNI) benchmark consisting of a diverse set of 700+ tasks and\ndemonstrate that it outperforms state-of-the-art multi-task parameter-efficient\nfine-tuning (PEFT) methods, without introducing additional parameters. Our\nfindings contribute to a deeper understanding of parameter efficiency in\nmulti-task LLM adaptation and provide a practical solution for deploying\nhigh-performing, lightweight models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"juMm1FA02dnP3T8RJ1TjBQeQwVazS_lHPUpaRe9b1AU","pdfSize":"1440742"}
