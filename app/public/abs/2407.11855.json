{"id":"2407.11855","title":"Scaling Sign Language Translation","authors":"Biao Zhang and Garrett Tanzer and Orhan Firat","authorsParsed":[["Zhang","Biao",""],["Tanzer","Garrett",""],["Firat","Orhan",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:36:58 GMT"}],"updateDate":"2024-07-17","timestamp":1721144218000,"abstract":"  Sign language translation (SLT) addresses the problem of translating\ninformation from a sign language in video to a spoken language in text.\nExisting studies, while showing progress, are often limited to narrow domains\nand/or few sign languages and struggle with open-domain tasks. In this paper,\nwe push forward the frontier of SLT by scaling pretraining data, model size,\nand number of translation directions. We perform large-scale SLT pretraining on\ndifferent data including 1) noisy multilingual YouTube SLT data, 2) parallel\ntext corpora, and 3) SLT data augmented by translating video captions to other\nlanguages with off-the-shelf machine translation models. We unify different\npretraining tasks with task-specific prompts under the encoder-decoder\narchitecture, and initialize the SLT model with pretrained (m/By)T5 models\nacross model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL\nto 42 spoken languages) demonstrate the significance of data/model scaling and\ncross-lingual cross-modal transfer, as well as the feasibility of zero-shot\nSLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT\nbenchmarks covering 5 sign languages. Experiments show substantial quality\nimprovements over the vanilla baselines, surpassing the previous\nstate-of-the-art (SOTA) by wide margins.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}