{"id":"2407.08018","title":"A Stochastic Objective-Function-Free Adaptive Regularization Method with\n  Optimal Complexity","authors":"Serge Gratton and Sadok Jerad and Philippe L. Toint","authorsParsed":[["Gratton","Serge",""],["Jerad","Sadok",""],["Toint","Philippe L.",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 19:52:37 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 07:59:25 GMT"}],"updateDate":"2024-07-22","timestamp":1720641157000,"abstract":"  A fully stochastic second-order adaptive-regularization method for\nunconstrained nonconvex optimization is presented which never computes the\nobjective-function value, but yet achieves the optimal\n$\\mathcal{O}(\\epsilon^{-3/2})$ complexity bound for finding first-order\ncritical points. The method is noise-tolerant and the inexactness conditions\nrequired for convergence depend on the history of past steps. Applications to\ncases where derivative evaluation is inexact and to minimization of finite sums\nby sampling are discussed. Numerical experiments on large binary classification\nproblems illustrate the potential of the new method.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}