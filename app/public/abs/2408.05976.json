{"id":"2408.05976","title":"Global-to-Local Support Spectrums for Language Model Explainability","authors":"Lucas Agussurja, Xinyang Lu, Bryan Kian Hsiang Low","authorsParsed":[["Agussurja","Lucas",""],["Lu","Xinyang",""],["Low","Bryan Kian Hsiang",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 08:05:30 GMT"}],"updateDate":"2024-08-13","timestamp":1723449930000,"abstract":"  Existing sample-based methods, like influence functions and representer\npoints, measure the importance of a training point by approximating the effect\nof its removal from training. As such, they are skewed towards outliers and\npoints that are very close to the decision boundaries. The explanations\nprovided by these methods are often static and not specific enough for\ndifferent test points. In this paper, we propose a method to generate an\nexplanation in the form of support spectrums which are based on two main ideas:\nthe support sets and a global-to-local importance measure. The support set is\nthe set of training points, in the predicted class, that ``lie in between'' the\ntest point and training points in the other classes. They indicate how well the\ntest point can be distinguished from the points not in the predicted class. The\nglobal-to-local importance measure is obtained by decoupling existing methods\ninto the global and local components which are then used to select the points\nin the support set. Using this method, we are able to generate explanations\nthat are tailored to specific test points. In the experiments, we show the\neffectiveness of the method in image classification and text generation tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}