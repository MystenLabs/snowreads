{"id":"2408.13442","title":"A Law of Next-Token Prediction in Large Language Models","authors":"Hangfeng He, Weijie J. Su","authorsParsed":[["He","Hangfeng",""],["Su","Weijie J.",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 02:48:40 GMT"}],"updateDate":"2024-08-27","timestamp":1724467720000,"abstract":"  Large language models (LLMs) have been widely employed across various\napplication domains, yet their black-box nature poses significant challenges to\nunderstanding how these models process input data internally to make\npredictions. In this paper, we introduce a precise and quantitative law that\ngoverns the learning of contextualized token embeddings through intermediate\nlayers in pre-trained LLMs for next-token prediction. Our findings reveal that\neach layer contributes equally to enhancing prediction accuracy, from the\nlowest to the highest layer -- a universal phenomenon observed across a diverse\narray of open-source LLMs, built on architectures such as Transformer, RWKV,\nand Mamba. We demonstrate that this law offers new perspectives and insights to\ninform and guide practices in LLM development and applications, including model\nscaling, pre-training tasks, and information flow. Overall, our law enables\nmore fine-grained approaches to the design, training, and interpretation of\nLLMs through scrutinizing their internal data processing mechanisms.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}