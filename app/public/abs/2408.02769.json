{"id":"2408.02769","title":"From Recognition to Prediction: Leveraging Sequence Reasoning for Action\n  Anticipation","authors":"Xin Liu, Chao Hao, Zitong Yu, Huanjing Yue, Jingyu Yang","authorsParsed":[["Liu","Xin",""],["Hao","Chao",""],["Yu","Zitong",""],["Yue","Huanjing",""],["Yang","Jingyu",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 18:38:29 GMT"}],"updateDate":"2024-08-07","timestamp":1722883109000,"abstract":"  The action anticipation task refers to predicting what action will happen\nbased on observed videos, which requires the model to have a strong ability to\nsummarize the present and then reason about the future. Experience and common\nsense suggest that there is a significant correlation between different\nactions, which provides valuable prior knowledge for the action anticipation\ntask. However, previous methods have not effectively modeled this underlying\nstatistical relationship. To address this issue, we propose a novel end-to-end\nvideo modeling architecture that utilizes attention mechanisms, named\nAnticipation via Recognition and Reasoning (ARR). ARR decomposes the action\nanticipation task into action recognition and sequence reasoning tasks, and\neffectively learns the statistical relationship between actions by next action\nprediction (NAP). In comparison to existing temporal aggregation strategies,\nARR is able to extract more effective features from observable videos to make\nmore reasonable predictions. In addition, to address the challenge of\nrelationship modeling that requires extensive training data, we propose an\ninnovative approach for the unsupervised pre-training of the decoder, which\nleverages the inherent temporal dynamics of video to enhance the reasoning\ncapabilities of the network. Extensive experiments on the Epic-kitchen-100,\nEGTEA Gaze+, and 50salads datasets demonstrate the efficacy of the proposed\nmethods. The code is available at https://github.com/linuxsino/ARR.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}