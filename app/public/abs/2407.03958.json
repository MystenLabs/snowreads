{"id":"2407.03958","title":"Stark: Social Long-Term Multi-Modal Conversation with Persona\n  Commonsense Knowledge","authors":"Young-Jun Lee, Dokyong Lee, Junyoung Youn, Kyeongjin Oh, Byungsoo Ko,\n  Jonghwan Hyeon, Ho-Jin Choi","authorsParsed":[["Lee","Young-Jun",""],["Lee","Dokyong",""],["Youn","Junyoung",""],["Oh","Kyeongjin",""],["Ko","Byungsoo",""],["Hyeon","Jonghwan",""],["Choi","Ho-Jin",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 14:26:49 GMT"}],"updateDate":"2024-07-08","timestamp":1720103209000,"abstract":"  Humans share a wide variety of images related to their personal experiences\nwithin conversations via instant messaging tools. However, existing works focus\non (1) image-sharing behavior in singular sessions, leading to limited\nlong-term social interaction, and (2) a lack of personalized image-sharing\nbehavior. In this work, we introduce Stark, a large-scale long-term multi-modal\nconversation dataset that covers a wide range of social personas in a\nmulti-modality format, time intervals, and images. To construct Stark\nautomatically, we propose a novel multi-modal contextualization framework, Mcu,\nthat generates long-term multi-modal dialogue distilled from ChatGPT and our\nproposed Plan-and-Execute image aligner. Using our Stark, we train a\nmulti-modal conversation model, Ultron 7B, which demonstrates impressive visual\nimagination ability. Furthermore, we demonstrate the effectiveness of our\ndataset in human evaluation. We make our source code and dataset publicly\navailable.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}