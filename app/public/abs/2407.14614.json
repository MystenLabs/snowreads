{"id":"2407.14614","title":"Evaluating language models as risk scores","authors":"Andr\\'e F. Cruz, Moritz Hardt, Celestine Mendler-D\\\"unner","authorsParsed":[["Cruz","André F.",""],["Hardt","Moritz",""],["Mendler-Dünner","Celestine",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 18:13:37 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 17:03:18 GMT"}],"updateDate":"2024-09-18","timestamp":1721412817000,"abstract":"  Current question-answering benchmarks predominantly focus on accuracy in\nrealizable prediction tasks. Conditioned on a question and answer-key, does the\nmost likely token match the ground truth? Such benchmarks necessarily fail to\nevaluate language models' ability to quantify outcome uncertainty. In this\nwork, we focus on the use of language models as risk scores for unrealizable\nprediction tasks. We introduce folktexts, a software package to systematically\ngenerate risk scores using language models, and evaluate them against US Census\ndata products. A flexible API enables the use of different prompting schemes,\nlocal or web-hosted models, and diverse census columns that can be used to\ncompose custom prediction tasks. We demonstrate the utility of folktexts\nthrough a sweep of empirical insights into the statistical properties of 17\nrecent large language models across five natural text benchmark tasks. We find\nthat zero-shot risk scores produced by multiple-choice question-answering have\nhigh predictive signal but are widely miscalibrated. Base models consistently\noverestimate outcome uncertainty, while instruction-tuned models underestimate\nuncertainty and produce over-confident risk scores. In fact, instruction-tuning\npolarizes answer distribution regardless of true underlying data uncertainty.\nConversely, verbally querying models for probability estimates results in\nsubstantially improved calibration across all instruction-tuned models. These\ndifferences in ability to quantify data uncertainty cannot be revealed in\nrealizable settings, and highlight a blind-spot in the current evaluation\necosystem that \\folktexts covers.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}