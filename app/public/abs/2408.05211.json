{"id":"2408.05211","title":"VITA: Towards Open-Source Interactive Omni Multimodal LLM","authors":"Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan\n  Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He,\n  Rongrong Ji, Yunsheng Wu, Caifeng Shan, Xing Sun","authorsParsed":[["Fu","Chaoyou",""],["Lin","Haojia",""],["Long","Zuwei",""],["Shen","Yunhang",""],["Zhao","Meng",""],["Zhang","Yifan",""],["Dong","Shaoqi",""],["Wang","Xiong",""],["Yin","Di",""],["Ma","Long",""],["Zheng","Xiawu",""],["He","Ran",""],["Ji","Rongrong",""],["Wu","Yunsheng",""],["Shan","Caifeng",""],["Sun","Xing",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 17:59:49 GMT"},{"version":"v2","created":"Tue, 10 Sep 2024 13:21:08 GMT"}],"updateDate":"2024-09-11","timestamp":1723226389000,"abstract":"  The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}