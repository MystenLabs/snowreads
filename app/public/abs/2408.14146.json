{"id":"2408.14146","title":"TSAK: Two-Stage Semantic-Aware Knowledge Distillation for Efficient\n  Wearable Modality and Model Optimization in Manufacturing Lines","authors":"Hymalai Bello, Daniel Gei{\\ss}ler, Sungho Suh, Bo Zhou and Paul\n  Lukowicz","authorsParsed":[["Bello","Hymalai",""],["Gei√üler","Daniel",""],["Suh","Sungho",""],["Zhou","Bo",""],["Lukowicz","Paul",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 09:44:21 GMT"}],"updateDate":"2024-08-28","timestamp":1724665461000,"abstract":"  Smaller machine learning models, with less complex architectures and sensor\ninputs, can benefit wearable sensor-based human activity recognition (HAR)\nsystems in many ways, from complexity and cost to battery life. In the specific\ncase of smart factories, optimizing human-robot collaboration hinges on the\nimplementation of cutting-edge, human-centric AI systems. To this end, workers'\nactivity recognition enables accurate quantification of performance metrics,\nimproving efficiency holistically. We present a two-stage semantic-aware\nknowledge distillation (KD) approach, TSAK, for efficient, privacy-aware, and\nwearable HAR in manufacturing lines, which reduces the input sensor modalities\nas well as the machine learning model size, while reaching similar recognition\nperformance as a larger multi-modal and multi-positional teacher model. The\nfirst stage incorporates a teacher classifier model encoding attention, causal,\nand combined representations. The second stage encompasses a semantic\nclassifier merging the three representations from the first stage. To evaluate\nTSAK, we recorded a multi-modal dataset at a smart factory testbed with\nwearable and privacy-aware sensors (IMU and capacitive) located on both\nworkers' hands. In addition, we evaluated our approach on OpenPack, the only\navailable open dataset mimicking the wearable sensor placements on both hands\nin the manufacturing HAR scenario. We compared several KD strategies with\ndifferent representations to regulate the training process of a smaller student\nmodel. Compared to the larger teacher model, the student model takes fewer\nsensor channels from a single hand, has 79% fewer parameters, runs 8.88 times\nfaster, and requires 96.6% less computing power (FLOPS).\n","subjects":["Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"orTNUAzwYBCcmxTrtDfXJqcbp2L-n2Ob2DMFwkmIS70","pdfSize":"1730198"}
