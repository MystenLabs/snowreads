{"id":"2408.15417","title":"Implicit Geometry of Next-token Prediction: From Language Sparsity\n  Patterns to Model Representations","authors":"Yize Zhao, Tina Behnia, Vala Vakilian, Christos Thrampoulidis","authorsParsed":[["Zhao","Yize",""],["Behnia","Tina",""],["Vakilian","Vala",""],["Thrampoulidis","Christos",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 21:46:47 GMT"}],"updateDate":"2024-08-29","timestamp":1724795207000,"abstract":"  Next-token prediction (NTP) over large text corpora has become the go-to\nparadigm to train large language models. Yet, it remains unclear how NTP\ninfluences the mapping of linguistic patterns to geometric properties of the\nresulting model representations. We frame training of large language models as\nsoft-label classification over sparse probabilistic label vectors, coupled with\nan analytical approximation that allows unrestricted generation of context\nembeddings. This approach links NTP training to rank-constrained, nuclear-norm\nregularized optimization in the logit domain, offering a framework for\nanalyzing the geometry of word and context embeddings. In large embedding\nspaces, we find that NTP implicitly favors learning logits with a sparse plus\nlow-rank structure. While the sparse component captures the co-occurrence\nfrequency of context-word pairs, the orthogonal low-rank component, which\nbecomes dominant as training progresses, depends solely on the sparsity pattern\nof the co-occurrence matrix. Consequently, when projected onto an appropriate\nsubspace, representations of contexts that are followed by the same set of\nnext-tokens collapse, a phenomenon we term subspace-collapse. We validate our\nfindings on synthetic and small-scale real language datasets. Finally, we\noutline potential research directions aimed at deepening the understanding of\nNTP's influence on the learning of linguistic patterns and regularities.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}