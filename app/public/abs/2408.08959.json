{"id":"2408.08959","title":"Adaptive Guardrails For Large Language Models via Trust Modeling and\n  In-Context Learning","authors":"Jinwei Hu, Yi Dong, Xiaowei Huang","authorsParsed":[["Hu","Jinwei",""],["Dong","Yi",""],["Huang","Xiaowei",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 18:07:48 GMT"}],"updateDate":"2024-08-20","timestamp":1723831668000,"abstract":"  Guardrails have become an integral part of Large language models (LLMs), by\nmoderating harmful or toxic response in order to maintain LLMs' alignment to\nhuman expectations. However, the existing guardrail methods do not consider\ndifferent needs and access rights of individual users, and treat all the users\nwith the same rule. This study introduces an adaptive guardrail mechanism,\nsupported by trust modeling and enhanced with in-context learning, to\ndynamically modulate access to sensitive content based on user trust metrics.\nBy leveraging a combination of direct interaction trust and authority-verified\ntrust, the system precisely tailors the strictness of content moderation to\nalign with the user's credibility and the specific context of their inquiries.\nOur empirical evaluations demonstrate that the adaptive guardrail effectively\nmeets diverse user needs, outperforming existing guardrails in practicality\nwhile securing sensitive information and precisely managing potentially\nhazardous content through a context-aware knowledge base. This work is the\nfirst to introduce trust-oriented concept within a guardrail system, offering a\nscalable solution that enriches the discourse on ethical deployment for\nnext-generation LLMs.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}