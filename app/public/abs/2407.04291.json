{"id":"2407.04291","title":"We Need Variations in Speech Synthesis: Sub-center Modelling for Speaker\n  Embeddings","authors":"Ismail Rasim Ulgen, Carlos Busso, John H. L. Hansen, and Berrak Sisman","authorsParsed":[["Ulgen","Ismail Rasim",""],["Busso","Carlos",""],["Hansen","John H. L.",""],["Sisman","Berrak",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 06:54:24 GMT"}],"updateDate":"2024-07-08","timestamp":1720162464000,"abstract":"  In speech synthesis, modeling of rich emotions and prosodic variations\npresent in human voice are crucial to synthesize natural speech. Although\nspeaker embeddings have been widely used in personalized speech synthesis as\nconditioning inputs, they are designed to lose variation to optimize speaker\nrecognition accuracy. Thus, they are suboptimal for speech synthesis in terms\nof modeling the rich variations at the output speech distribution. In this\nwork, we propose a novel speaker embedding network which utilizes multiple\nclass centers in the speaker classification training rather than a single class\ncenter as traditional embeddings. The proposed approach introduces variations\nin the speaker embedding while retaining the speaker recognition performance\nsince model does not have to map all of the utterances of a speaker into a\nsingle class center. We apply our proposed embedding in voice conversion task\nand show that our method provides better naturalness and prosody in synthesized\nspeech.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}