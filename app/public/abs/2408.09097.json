{"id":"2408.09097","title":"Depth-guided Texture Diffusion for Image Semantic Segmentation","authors":"Wei Sun, Yuan Li, Qixiang Ye, Jianbin Jiao, Yanzhao Zhou","authorsParsed":[["Sun","Wei",""],["Li","Yuan",""],["Ye","Qixiang",""],["Jiao","Jianbin",""],["Zhou","Yanzhao",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 04:55:03 GMT"}],"updateDate":"2024-08-20","timestamp":1723870503000,"abstract":"  Depth information provides valuable insights into the 3D structure especially\nthe outline of objects, which can be utilized to improve the semantic\nsegmentation tasks. However, a naive fusion of depth information can disrupt\nfeature and compromise accuracy due to the modality gap between the depth and\nthe vision. In this work, we introduce a Depth-guided Texture Diffusion\napproach that effectively tackles the outlined challenge. Our method extracts\nlow-level features from edges and textures to create a texture image. This\nimage is then selectively diffused across the depth map, enhancing structural\ninformation vital for precisely extracting object outlines. By integrating this\nenriched depth map with the original RGB image into a joint feature embedding,\nour method effectively bridges the disparity between the depth map and the\nimage, enabling more accurate semantic segmentation. We conduct comprehensive\nexperiments across diverse, commonly-used datasets spanning a wide range of\nsemantic segmentation tasks, including Camouflaged Object Detection (COD),\nSalient Object Detection (SOD), and indoor semantic segmentation. With\nsource-free estimated depth or depth captured by depth cameras, our method\nconsistently outperforms existing baselines and achieves new state-of-theart\nresults, demonstrating the effectiveness of our Depth-guided Texture Diffusion\nfor image semantic segmentation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}