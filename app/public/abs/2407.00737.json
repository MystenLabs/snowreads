{"id":"2407.00737","title":"LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image\n  Generation","authors":"Mushui Liu, Yuhang Ma, Yang Zhen, Jun Dan, Yunlong Yu, Zeng Zhao,\n  Zhipeng Hu, Bai Liu, Changjie Fan","authorsParsed":[["Liu","Mushui",""],["Ma","Yuhang",""],["Zhen","Yang",""],["Dan","Jun",""],["Yu","Yunlong",""],["Zhao","Zeng",""],["Hu","Zhipeng",""],["Liu","Bai",""],["Fan","Changjie",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 15:50:32 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 08:33:31 GMT"}],"updateDate":"2024-08-28","timestamp":1719762632000,"abstract":"  Diffusion models have exhibited substantial success in text-to-image\ngeneration. However, they often encounter challenges when dealing with complex\nand dense prompts involving multiple objects, attribute binding, and long\ndescriptions. In this paper, we propose a novel framework called\n\\textbf{LLM4GEN}, which enhances the semantic understanding of text-to-image\ndiffusion models by leveraging the representation of Large Language Models\n(LLMs). It can be seamlessly incorporated into various diffusion models as a\nplug-and-play component. A specially designed Cross-Adapter Module (CAM)\nintegrates the original text features of text-to-image models with LLM\nfeatures, thereby enhancing text-to-image generation. Additionally, to\nfacilitate and correct entity-attribute relationships in text prompts, we\ndevelop an entity-guided regularization loss to further improve generation\nperformance. We also introduce DensePrompts, which contains $7,000$ dense\nprompts to provide a comprehensive evaluation for the text-to-image generation\ntask. Experiments indicate that LLM4GEN significantly improves the semantic\nalignment of SD1.5 and SDXL, demonstrating increases of 9.69\\% and 12.90\\% in\ncolor on T2I-CompBench, respectively. Moreover, it surpasses existing models in\nterms of sample quality, image-text alignment, and human evaluation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}