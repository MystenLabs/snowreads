{"id":"2408.09125","title":"Markov Balance Satisfaction Improves Performance in Strictly Batch\n  Offline Imitation Learning","authors":"Rishabh Agrawal, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar","authorsParsed":[["Agrawal","Rishabh",""],["Dahlin","Nathan",""],["Jain","Rahul",""],["Nayyar","Ashutosh",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 07:17:19 GMT"}],"updateDate":"2024-08-20","timestamp":1723879039000,"abstract":"  Imitation learning (IL) is notably effective for robotic tasks where directly\nprogramming behaviors or defining optimal control costs is challenging. In this\nwork, we address a scenario where the imitator relies solely on observed\nbehavior and cannot make environmental interactions during learning. It does\nnot have additional supplementary datasets beyond the expert's dataset nor any\ninformation about the transition dynamics. Unlike state-of-the-art (SOTA) IL\nmethods, this approach tackles the limitations of conventional IL by operating\nin a more constrained and realistic setting. Our method uses the Markov balance\nequation and introduces a novel conditional density estimation-based imitation\nlearning framework. It employs conditional normalizing flows for transition\ndynamics estimation and aims at satisfying a balance equation for the\nenvironment. Through a series of numerical experiments on Classic Control and\nMuJoCo environments, we demonstrate consistently superior empirical performance\ncompared to many SOTA IL algorithms.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}