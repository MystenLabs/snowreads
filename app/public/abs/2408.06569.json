{"id":"2408.06569","title":"Social Debiasing for Fair Multi-modal LLMs","authors":"Harry Cheng, Yangyang Guo, Qingpei Guo, Ming Yang, Tian Gan, Liqiang\n  Nie","authorsParsed":[["Cheng","Harry",""],["Guo","Yangyang",""],["Guo","Qingpei",""],["Yang","Ming",""],["Gan","Tian",""],["Nie","Liqiang",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 02:08:32 GMT"}],"updateDate":"2024-08-14","timestamp":1723514912000,"abstract":"  Multi-modal Large Language Models (MLLMs) have advanced significantly,\noffering powerful vision-language understanding capabilities. However, these\nmodels often inherit severe social biases from their training datasets, leading\nto unfair predictions based on attributes like race and gender. This paper\naddresses the issue of social biases in MLLMs by i) Introducing a comprehensive\nCounterfactual dataset with Multiple Social Concepts (CMSC), which provides a\nmore diverse and extensive training set compared to existing datasets. ii)\nProposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by\nrevisiting the MLLM training process, rescaling the autoregressive loss\nfunction, and improving data sampling methods to counteract biases. Through\nextensive experiments on various MLLMs, our CMSC dataset and ASD method\ndemonstrate a significant reduction in social biases while maintaining the\nmodels' original performance.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}