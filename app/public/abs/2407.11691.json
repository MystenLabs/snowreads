{"id":"2407.11691","title":"VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality\n  Models","authors":"Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan\n  Liu, Amit Agarwal, Zhe Chen, Mo Li, Yubo Ma, Hailong Sun, Xiangyu Zhao, Junbo\n  Cui, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, Kai Chen","authorsParsed":[["Duan","Haodong",""],["Yang","Junming",""],["Qiao","Yuxuan",""],["Fang","Xinyu",""],["Chen","Lin",""],["Liu","Yuan",""],["Agarwal","Amit",""],["Chen","Zhe",""],["Li","Mo",""],["Ma","Yubo",""],["Sun","Hailong",""],["Zhao","Xiangyu",""],["Cui","Junbo",""],["Dong","Xiaoyi",""],["Zang","Yuhang",""],["Zhang","Pan",""],["Wang","Jiaqi",""],["Lin","Dahua",""],["Chen","Kai",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 13:06:15 GMT"},{"version":"v2","created":"Wed, 11 Sep 2024 17:10:36 GMT"}],"updateDate":"2024-09-12","timestamp":1721135175000,"abstract":"  We present VLMEvalKit: an open-source toolkit for evaluating large\nmulti-modality models based on PyTorch. The toolkit aims to provide a\nuser-friendly and comprehensive framework for researchers and developers to\nevaluate existing multi-modality models and publish reproducible evaluation\nresults. In VLMEvalKit, we implement over 70 different large multi-modality\nmodels, including both proprietary APIs and open-source models, as well as more\nthan 20 different multi-modal benchmarks. By implementing a single interface,\nnew models can be easily added to the toolkit, while the toolkit automatically\nhandles the remaining workloads, including data preparation, distributed\ninference, prediction post-processing, and metric calculation. Although the\ntoolkit is currently mainly used for evaluating large vision-language models,\nits design is compatible with future updates that incorporate additional\nmodalities, such as audio and video. Based on the evaluation results obtained\nwith the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to\ntrack the progress of multi-modality learning research. The toolkit is released\nat https://github.com/open-compass/VLMEvalKit and is actively maintained.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}