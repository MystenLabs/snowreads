{"id":"2408.10578","title":"Where to Fetch: Extracting Visual Scene Representation from Large\n  Pre-Trained Models for Robotic Goal Navigation","authors":"Yu Li, Dayou Li, Chenkun Zhao, Ruifeng Wang, Ran Song and Wei Zhang","authorsParsed":[["Li","Yu",""],["Li","Dayou",""],["Zhao","Chenkun",""],["Wang","Ruifeng",""],["Song","Ran",""],["Zhang","Wei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 06:36:40 GMT"}],"updateDate":"2024-08-21","timestamp":1724135800000,"abstract":"  To complete a complex task where a robot navigates to a goal object and\nfetches it, the robot needs to have a good understanding of the instructions\nand the surrounding environment. Large pre-trained models have shown\ncapabilities to interpret tasks defined via language descriptions. However,\nprevious methods attempting to integrate large pre-trained models with daily\ntasks are not competent in many robotic goal navigation tasks due to poor\nunderstanding of the environment. In this work, we present a visual scene\nrepresentation built with large-scale visual language models to form a feature\nrepresentation of the environment capable of handling natural language queries.\nCombined with large language models, this method can parse language\ninstructions into action sequences for a robot to follow, and accomplish goal\nnavigation with querying the scene representation. Experiments demonstrate that\nour method enables the robot to follow a wide range of instructions and\ncomplete complex goal navigation tasks.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}