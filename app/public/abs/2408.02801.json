{"id":"2408.02801","title":"Sparse Deep Learning Models with the $\\ell_1$ Regularization","authors":"Lixin Shen, Rui Wang, Yuesheng Xu, Mingsong Yan","authorsParsed":[["Shen","Lixin",""],["Wang","Rui",""],["Xu","Yuesheng",""],["Yan","Mingsong",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 19:38:45 GMT"}],"updateDate":"2024-08-07","timestamp":1722886725000,"abstract":"  Sparse neural networks are highly desirable in deep learning in reducing its\ncomplexity. The goal of this paper is to study how choices of regularization\nparameters influence the sparsity level of learned neural networks. We first\nderive the $\\ell_1$-norm sparsity-promoting deep learning models including\nsingle and multiple regularization parameters models, from a statistical\nviewpoint. We then characterize the sparsity level of a regularized neural\nnetwork in terms of the choice of the regularization parameters. Based on the\ncharacterizations, we develop iterative algorithms for selecting regularization\nparameters so that the weight parameters of the resulting deep neural network\nenjoy prescribed sparsity levels. Numerical experiments are presented to\ndemonstrate the effectiveness of the proposed algorithms in choosing desirable\nregularization parameters and obtaining corresponding neural networks having\nboth of predetermined sparsity levels and satisfactory approximation accuracy.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}