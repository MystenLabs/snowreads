{"id":"2408.13290","title":"Multi-modal Intermediate Feature Interaction AutoEncoder for Overall\n  Survival Prediction of Esophageal Squamous Cell Cancer","authors":"Chengyu Wu, Yatao Zhang, Yaqi Wang, Qifeng Wang, Shuai Wang","authorsParsed":[["Wu","Chengyu",""],["Zhang","Yatao",""],["Wang","Yaqi",""],["Wang","Qifeng",""],["Wang","Shuai",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 09:11:05 GMT"}],"updateDate":"2024-08-27","timestamp":1724404265000,"abstract":"  Survival prediction for esophageal squamous cell cancer (ESCC) is crucial for\ndoctors to assess a patient's condition and tailor treatment plans. The\napplication and development of multi-modal deep learning in this field have\nattracted attention in recent years. However, the prognostically relevant\nfeatures between cross-modalities have not been further explored in previous\nstudies, which could hinder the performance of the model. Furthermore, the\ninherent semantic gap between different modal feature representations is also\nignored. In this work, we propose a novel autoencoder-based deep learning model\nto predict the overall survival of the ESCC. Two novel modules were designed\nfor multi-modal prognosis-related feature reinforcement and modeling ability\nenhancement. In addition, a novel joint loss was proposed to make the\nmulti-modal feature representations more aligned. Comparison and ablation\nexperiments demonstrated that our model can achieve satisfactory results in\nterms of discriminative ability, risk stratification, and the effectiveness of\nthe proposed modules.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"KB6f_uZB6WgRFWouNNi0srGNQMvxpETEUHShl40cst4","pdfSize":"1990362"}
