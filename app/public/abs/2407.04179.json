{"id":"2407.04179","title":"Defense Against Syntactic Textual Backdoor Attacks with Token\n  Substitution","authors":"Xinglin Li, Xianwen He, Yao Li, Minhao Cheng","authorsParsed":[["Li","Xinglin",""],["He","Xianwen",""],["Li","Yao",""],["Cheng","Minhao",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 22:48:57 GMT"}],"updateDate":"2024-07-08","timestamp":1720133337000,"abstract":"  Textual backdoor attacks present a substantial security risk to Large\nLanguage Models (LLM). It embeds carefully chosen triggers into a victim model\nat the training stage, and makes the model erroneously predict inputs\ncontaining the same triggers as a certain class. Prior backdoor defense methods\nprimarily target special token-based triggers, leaving syntax-based triggers\ninsufficiently addressed. To fill this gap, this paper proposes a novel online\ndefense algorithm that effectively counters syntax-based as well as special\ntoken-based backdoor attacks. The algorithm replaces semantically meaningful\nwords in sentences with entirely different ones but preserves the syntactic\ntemplates or special tokens, and then compares the predicted labels before and\nafter the substitution to determine whether a sentence contains triggers.\nExperimental results confirm the algorithm's performance against these two\ntypes of triggers, offering a comprehensive defense strategy for model\nintegrity.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}