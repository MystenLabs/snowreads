{"id":"2408.15045","title":"DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding","authors":"Wenhui Liao, Jiapeng Wang, Hongliang Li, Chengyu Wang, Jun Huang,\n  Lianwen Jin","authorsParsed":[["Liao","Wenhui",""],["Wang","Jiapeng",""],["Li","Hongliang",""],["Wang","Chengyu",""],["Huang","Jun",""],["Jin","Lianwen",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 13:13:38 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 08:32:44 GMT"}],"updateDate":"2024-08-29","timestamp":1724764418000,"abstract":"  Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}