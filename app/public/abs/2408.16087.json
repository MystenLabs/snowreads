{"id":"2408.16087","title":"Unlocking Global Optimality in Bilevel Optimization: A Pilot Study","authors":"Quan Xiao, Tianyi Chen","authorsParsed":[["Xiao","Quan",""],["Chen","Tianyi",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 18:34:54 GMT"}],"updateDate":"2024-08-30","timestamp":1724870094000,"abstract":"  Bilevel optimization has witnessed a resurgence of interest, driven by its\ncritical role in trustworthy and efficient machine learning applications.\nRecent research has focused on proposing efficient methods with provable\nconvergence guarantees. However, while many prior works have established\nconvergence to stationary points or local minima, obtaining the global optimum\nof bilevel optimization remains an important yet open problem. The difficulty\nlies in the fact that unlike many prior non-convex single-level problems, this\nbilevel problem does not admit a ``benign\" landscape, and may indeed have\nmultiple spurious local solutions. Nevertheless, attaining the global\noptimality is indispensable for ensuring reliability, safety, and\ncost-effectiveness, particularly in high-stakes engineering applications that\nrely on bilevel optimization. In this paper, we first explore the challenges of\nestablishing a global convergence theory for bilevel optimization, and present\ntwo sufficient conditions for global convergence. We provide algorithm-specific\nproofs to rigorously substantiate these sufficient conditions along the\noptimization trajectory, focusing on two specific bilevel learning scenarios:\nrepresentation learning and data hypercleaning (a.k.a. reweighting).\nExperiments corroborate the theoretical findings, demonstrating convergence to\nglobal minimum in both cases.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}