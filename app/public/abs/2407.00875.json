{"id":"2407.00875","title":"MoE-CT: A Novel Approach For Large Language Models Training With\n  Resistance To Catastrophic Forgetting","authors":"Tianhao Li, Shangjie Li, Binbin Xie, Deyi Xiong, Baosong Yang","authorsParsed":[["Li","Tianhao",""],["Li","Shangjie",""],["Xie","Binbin",""],["Xiong","Deyi",""],["Yang","Baosong",""]],"versions":[{"version":"v1","created":"Tue, 25 Jun 2024 11:03:45 GMT"}],"updateDate":"2024-07-02","timestamp":1719313425000,"abstract":"  The advent of large language models (LLMs) has predominantly catered to\nhigh-resource languages, leaving a disparity in performance for low-resource\nlanguages. Conventional Continual Training (CT) approaches to bridge this gap\noften undermine a model's original linguistic proficiency when expanding to\nmultilingual contexts. Addressing this issue, we introduce a novel MoE-CT\narchitecture, a paradigm that innovatively separates the base model's learning\nfrom the multilingual expansion process. Our design freezes the original LLM\nparameters, thus safeguarding its performance in high-resource languages, while\nan appended MoE module, trained on diverse language datasets, augments\nlow-resource language proficiency. Our approach significantly outperforms\nconventional CT methods, as evidenced by our experiments, which show marked\nimprovements in multilingual benchmarks without sacrificing the model's\noriginal language performance. Moreover, our MoE-CT framework demonstrates\nenhanced resistance to forgetting and superior transfer learning capabilities.\nBy preserving the base model's integrity and focusing on strategic parameter\nexpansion, our methodology advances multilingual language modeling and\nrepresents a significant step forward for low-resource language inclusion in\nLLMs, indicating a fruitful direction for future research in language\ntechnologies.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}