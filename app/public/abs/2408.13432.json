{"id":"2408.13432","title":"Integrating Multi-Head Convolutional Encoders with Cross-Attention for\n  Improved SPARQL Query Translation","authors":"Yi-Hui Chen and Eric Jui-Lin Lu and Kwan-Ho Cheng","authorsParsed":[["Chen","Yi-Hui",""],["Lu","Eric Jui-Lin",""],["Cheng","Kwan-Ho",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 01:58:28 GMT"}],"updateDate":"2024-08-27","timestamp":1724464708000,"abstract":"  The main task of the KGQA system (Knowledge Graph Question Answering) is to\nconvert user input questions into query syntax (such as SPARQL). With the rise\nof modern popular encoders and decoders like Transformer and ConvS2S, many\nscholars have shifted the research direction of SPARQL generation to the Neural\nMachine Translation (NMT) architecture or the generative AI field of\nText-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query\nsyntax as a language. It uses NMT-based translation models to translate natural\nlanguage questions into query syntax. Scholars use popular architectures\nequipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to\ntrain translation models for query syntax. To achieve better query results,\nthis paper improved the ConvS2S encoder and added multi-head attention from the\nTransformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the\nn-gram language model. The principle is to use convolutional layers to capture\nlocal hidden features in the input sequence with different receptive fields,\nusing multi-head attention to calculate dependencies between them. Ultimately,\nwe found that the translation model based on the Multi-Head Conv encoder\nachieved better performance than other encoders, obtaining 76.52\\% and 83.37\\%\nBLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0\ndatasets, respectively. Additionally, in the end-to-end system experiments on\nthe QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other\nKGQA systems, with Macro F1-measures reaching 52\\% and 66\\%, respectively.\nMoreover, the experimental results show that with limited computational\nresources, if one possesses an excellent encoder-decoder architecture and\ncross-attention, experts and scholars can achieve outstanding performance\nequivalent to large pre-trained models using only general embeddings.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}