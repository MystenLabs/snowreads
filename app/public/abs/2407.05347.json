{"id":"2407.05347","title":"A Queueing Theoretic Perspective on Low-Latency LLM Inference with\n  Variable Token Length","authors":"Yuqing Yang and Yuedong Xu and Lei Jiao","authorsParsed":[["Yang","Yuqing",""],["Xu","Yuedong",""],["Jiao","Lei",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 12:49:56 GMT"}],"updateDate":"2024-07-09","timestamp":1720356596000,"abstract":"  Large language models (LLMs) propel the prosperity of interactive AI\napplications showcased by ChatGPT that demand timely response of inference\nservices. However, LLM inference is computation intensive and memory intensive,\nand improper parameter configuration at LLM platforms may exacerbate the\ninference time. In this paper, we analyze the impact of LLM output token\ndistribution on the inference queueing delay, where the max-token clipping and\nthe batched inference are considered. By formulating an M/G/1 model, we observe\nthat enforcing a maximum output token limit on a very small fraction of\ninference requests can significantly reduce the queueing delay, and our model\nfacilitates the selection of the optimal limit. For the batch inference, we\nmodel the service process as a bulk queue in which the batch processing time is\naffected by the batch size and the maximum token size inside this batch\njointly. The queueing delays of the batching of all buffered requests (dynamic\nbatching), the batching of constant number of requests (fixed batching), and\nthe batching without intra-batch waiting (elastic batching) are derived.\nExperimental results show that our mathematical models coincide with the\nevent-driven simulations well.\n","subjects":["Computing Research Repository/Networking and Internet Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}