{"id":"2407.15771","title":"Local Occupancy-Enhanced Object Grasping with Multiple Triplanar\n  Projection","authors":"Kangqi Ma, Hao Dong, Yadong Mu","authorsParsed":[["Ma","Kangqi",""],["Dong","Hao",""],["Mu","Yadong",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 16:22:28 GMT"}],"updateDate":"2024-07-23","timestamp":1721665348000,"abstract":"  This paper addresses the challenge of robotic grasping of general objects.\nSimilar to prior research, the task reads a single-view 3D observation (i.e.,\npoint clouds) captured by a depth camera as input. Crucially, the success of\nobject grasping highly demands a comprehensive understanding of the shape of\nobjects within the scene. However, single-view observations often suffer from\nocclusions (including both self and inter-object occlusions), which lead to\ngaps in the point clouds, especially in complex cluttered scenes. This renders\nincomplete perception of the object shape and frequently causes failures or\ninaccurate pose estimation during object grasping. In this paper, we tackle\nthis issue with an effective albeit simple solution, namely completing\ngrasping-related scene regions through local occupancy prediction. Following\nprior practice, the proposed model first runs by proposing a number of most\nlikely grasp points in the scene. Around each grasp point, a module is designed\nto infer any voxel in its neighborhood to be either void or occupied by some\nobject. Importantly, the occupancy map is inferred by fusing both local and\nglobal cues. We implement a multi-group tri-plane scheme for efficiently\naggregating long-distance contextual information. The model further estimates\n6-DoF grasp poses utilizing the local occupancy-enhanced object shape\ninformation and returns the top-ranked grasp proposal. Comprehensive\nexperiments on both the large-scale GraspNet-1Billion benchmark and real\nrobotic arm demonstrate that the proposed method can effectively complete the\nunobserved parts in cluttered and occluded scenes. Benefiting from the\noccupancy-enhanced feature, our model clearly outstrips other competing methods\nunder various performance metrics such as grasping average precision.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}