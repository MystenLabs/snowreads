{"id":"2408.05887","title":"Statistically Optimal Uncertainty Quantification for Expensive Black-Box\n  Models","authors":"Shengyi He, Henry Lam","authorsParsed":[["He","Shengyi",""],["Lam","Henry",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 01:36:39 GMT"}],"updateDate":"2024-08-13","timestamp":1723426599000,"abstract":"  Uncertainty quantification, by means of confidence interval (CI)\nconstruction, has been a fundamental problem in statistics and also important\nin risk-aware decision-making. In this paper, we revisit the basic problem of\nCI construction, but in the setting of expensive black-box models. This means\nwe are confined to using a low number of model runs, and without the ability to\nobtain auxiliary model information such as gradients. In this case, there exist\nclassical methods based on data splitting, and newer methods based on suitable\nresampling. However, while all these resulting CIs have similarly accurate\ncoverage in large sample, their efficiencies in terms of interval length\ndiffer, and a systematic understanding of which method and configuration\nattains the shortest interval appears open. Motivated by this, we create a\ntheoretical framework to study the statistical optimality on CI tightness under\ncomputation constraint. Our theory shows that standard batching, but also\ncarefully constructed new formulas using uneven-size or overlapping batches,\nbatched jackknife, and the so-called cheap bootstrap and its weighted\ngeneralizations, are statistically optimal. Our developments build on a new\nbridge of the classical notion of uniformly most accurate unbiasedness with\nbatching and resampling, by viewing model runs as asymptotically Gaussian\n\"data\", as well as a suitable notion of homogeneity for CIs.\n","subjects":["Statistics/Methodology","Statistics/Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}