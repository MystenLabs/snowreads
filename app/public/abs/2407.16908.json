{"id":"2407.16908","title":"Generation Constraint Scaling Can Mitigate Hallucination","authors":"Georgios Kollias, Payel Das, Subhajit Chaudhury","authorsParsed":[["Kollias","Georgios",""],["Das","Payel",""],["Chaudhury","Subhajit",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 23:58:19 GMT"}],"updateDate":"2024-07-25","timestamp":1721779099000,"abstract":"  Addressing the issue of hallucinations in large language models (LLMs) is a\ncritical challenge. As the cognitive mechanisms of hallucination have been\nrelated to memory, here we explore hallucination for LLM that is enabled with\nexplicit memory mechanisms. We empirically demonstrate that by simply scaling\nthe readout vector that constrains generation in a memory-augmented LLM\ndecoder, hallucination mitigation can be achieved in a training-free manner.\nOur method is geometry-inspired and outperforms a state-of-the-art LLM editing\nmethod on the task of generation of Wikipedia-like biography entries both in\nterms of generation quality and runtime complexity.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}