{"id":"2408.02657","title":"Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation\n  with Multimodal Generative Pretraining","authors":"Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng\n  Li, Peng Gao","authorsParsed":[["Liu","Dongyang",""],["Zhao","Shitian",""],["Zhuo","Le",""],["Lin","Weifeng",""],["Qiao","Yu",""],["Li","Hongsheng",""],["Gao","Peng",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 17:46:53 GMT"}],"updateDate":"2024-08-06","timestamp":1722880013000,"abstract":"  We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. Unlike existing\nautoregressive image generation approaches, Lumina-mGPT employs a pretrained\ndecoder-only transformer as a unified framework for modeling multimodal token\nsequences. Our key insight is that a simple decoder-only transformer with\nmultimodal Generative PreTraining (mGPT), utilizing the next-token prediction\nobjective on massive interleaved text-image sequences, can learn broad and\ngeneral multimodal capabilities, thereby illuminating photorealistic\ntext-to-image generation. Building on these pretrained models, we propose\nFlexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text\npairs to fully unlock their potential for high-aesthetic image synthesis at any\nresolution while maintaining their general multimodal capabilities.\nFurthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),\ntransforming Lumina-mGPT into a foundation model that seamlessly achieves\nomnipotent task unification. The resulting model demonstrates versatile\nmultimodal capabilities, including visual generation tasks like flexible\ntext-to-image generation and controllable generation, visual recognition tasks\nlike segmentation and depth estimation, and vision-language tasks like\nmultiturn visual question answering. Additionally, we analyze the differences\nand similarities between diffusion-based and autoregressive methods in a direct\ncomparison.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9CNAEDK-B8evJMB0rLF_xOKa4GtYCc_71WPnXRFRGa4","pdfSize":"7690057","txDigest":"BrCkQeMvaEU5Ae8JK2JFw8o4r8ZAup3Kb5UfWfL4ccCU","endEpoch":"1","status":"CERTIFIED"}
