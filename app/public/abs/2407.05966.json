{"id":"2407.05966","title":"On Bellman equations for continuous-time policy evaluation I:\n  discretization and approximation","authors":"Wenlong Mou, Yuhua Zhu","authorsParsed":[["Mou","Wenlong",""],["Zhu","Yuhua",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 14:05:03 GMT"}],"updateDate":"2024-07-09","timestamp":1720447503000,"abstract":"  We study the problem of computing the value function from a\ndiscretely-observed trajectory of a continuous-time diffusion process. We\ndevelop a new class of algorithms based on easily implementable numerical\nschemes that are compatible with discrete-time reinforcement learning (RL) with\nfunction approximation. We establish high-order numerical accuracy as well as\nthe approximation error guarantees for the proposed approach. In contrast to\ndiscrete-time RL problems where the approximation factor depends on the\neffective horizon, we obtain a bounded approximation factor using the\nunderlying elliptic structures, even if the effective horizon diverges to\ninfinity.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis","Mathematics/Optimization and Control","Mathematics/Probability"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}