{"id":"2408.12163","title":"Preference-Guided Reflective Sampling for Aligning Language Models","authors":"Hai Ye, Hwee Tou Ng","authorsParsed":[["Ye","Hai",""],["Ng","Hwee Tou",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 07:18:46 GMT"}],"updateDate":"2024-08-23","timestamp":1724311126000,"abstract":"  Large language models (LLMs) are aligned with human preferences by\nreinforcement learning from human feedback (RLHF). Effective data sampling is\ncrucial for RLHF, as it determines the efficiency of model training, ensuring\nthat models learn from the informative samples. To achieve better data\ngeneration, we propose a new sampling method called Preference-Guided\nReflective Sampling (PRS). PRS frames the response generation as an\noptimization process to the explicitly specified user preference described in\nnatural language. It employs a tree-based generation framework to enable an\nefficient sampling process, which guides the direction of generation through\npreference and better explores the sampling space with adaptive\nself-refinement. Notably, PRS can align LLMs to diverse preferences. We study\npreference-controlled text generation for instruction following and\nkeyword-focused document summarization. Our findings indicate that PRS, across\ndifferent LLM policies, generates training data with much higher rewards than\nstrong baselines. PRS also excels in post-RL training.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zV4KP61iVsC_Xv3MHdIUXIzR8g3BjPH0558-gsXX9JA","pdfSize":"4523276"}
