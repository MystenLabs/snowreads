{"id":"2407.18517","title":"SLIM: Style-Linguistics Mismatch Model for Generalized Audio Deepfake\n  Detection","authors":"Yi Zhu, Surya Koppisetti, Trang Tran, Gaurav Bharaj","authorsParsed":[["Zhu","Yi",""],["Koppisetti","Surya",""],["Tran","Trang",""],["Bharaj","Gaurav",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 05:23:41 GMT"}],"updateDate":"2024-07-29","timestamp":1721971421000,"abstract":"  Audio deepfake detection (ADD) is crucial to combat the misuse of speech\nsynthesized from generative AI models. Existing ADD models suffer from\ngeneralization issues, with a large performance discrepancy between in-domain\nand out-of-domain data. Moreover, the black-box nature of existing models\nlimits their use in real-world scenarios, where explanations are required for\nmodel decisions. To alleviate these issues, we introduce a new ADD model that\nexplicitly uses the StyleLInguistics Mismatch (SLIM) in fake speech to separate\nthem from real speech. SLIM first employs self-supervised pretraining on only\nreal samples to learn the style-linguistics dependency in the real class. The\nlearned features are then used in complement with standard pretrained acoustic\nfeatures (e.g., Wav2vec) to learn a classifier on the real and fake classes.\nWhen the feature encoders are frozen, SLIM outperforms benchmark methods on\nout-of-domain datasets while achieving competitive results on in-domain data.\nThe features learned by SLIM allow us to quantify the (mis)match between style\nand linguistic content in a sample, hence facilitating an explanation of the\nmodel decision.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}