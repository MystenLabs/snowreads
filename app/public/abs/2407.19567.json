{"id":"2407.19567","title":"Sharp Bounds for Poly-GNNs and the Effect of Graph Noise","authors":"Luciano Vinas and Arash A. Amini","authorsParsed":[["Vinas","Luciano",""],["Amini","Arash A.",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 19:23:56 GMT"}],"updateDate":"2024-07-30","timestamp":1722194636000,"abstract":"  We investigate the classification performance of graph neural networks with\ngraph-polynomial features, poly-GNNs, on the problem of semi-supervised node\nclassification. We analyze poly-GNNs under a general contextual stochastic\nblock model (CSBM) by providing a sharp characterization of the rate of\nseparation between classes in their output node representations. A question of\ninterest is whether this rate depends on the depth of the network $k$, i.e.,\nwhether deeper networks can achieve a faster separation? We provide a negative\nanswer to this question: for a sufficiently large graph, a depth $k > 1$\npoly-GNN exhibits the same rate of separation as a depth $k=1$ counterpart. Our\nanalysis highlights and quantifies the impact of ``graph noise'' in deep GNNs\nand shows how noise in the graph structure can dominate other sources of signal\nin the graph, negating any benefit further aggregation provides. Our analysis\nalso reveals subtle differences between even and odd-layered GNNs in how the\nfeature noise propagates.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-Khdh4EjaAjN_5CLVv5H74NxbokjQfs-49v6YxX_tPM","pdfSize":"496201"}
