{"id":"2407.12192","title":"Towards Dataset-scale and Feature-oriented Evaluation of Text\n  Summarization in Large Language Model Prompts","authors":"Sam Yu-Te Lee, Aryaman Bahukhandi, Dongyu Liu, and Kwan-Liu Ma","authorsParsed":[["Lee","Sam Yu-Te",""],["Bahukhandi","Aryaman",""],["Liu","Dongyu",""],["Ma","Kwan-Liu",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 21:36:43 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 05:08:03 GMT"},{"version":"v3","created":"Mon, 9 Sep 2024 21:07:47 GMT"}],"updateDate":"2024-09-11","timestamp":1721165803000,"abstract":"  Recent advancements in Large Language Models (LLMs) and Prompt Engineering\nhave made chatbot customization more accessible, significantly reducing\nbarriers to tasks that previously required programming skills. However, prompt\nevaluation, especially at the dataset scale, remains complex due to the need to\nassess prompts across thousands of test instances within a dataset. Our study,\nbased on a comprehensive literature review and pilot study, summarized five\ncritical challenges in prompt evaluation. In response, we introduce a\nfeature-oriented workflow for systematic prompt evaluation. In the context of\ntext summarization, our workflow advocates evaluation with summary\ncharacteristics (feature metrics) such as complexity, formality, or\nnaturalness, instead of using traditional quality metrics like ROUGE. This\ndesign choice enables a more user-friendly evaluation of prompts, as it guides\nusers in sorting through the ambiguity inherent in natural language. To support\nthis workflow, we introduce Awesum, a visual analytics system that facilitates\nidentifying optimal prompt refinements for text summarization through\ninteractive visualizations, featuring a novel Prompt Comparator design that\nemploys a BubbleSet-inspired design enhanced by dimensionality reduction\ntechniques. We evaluate the effectiveness and general applicability of the\nsystem with practitioners from various domains and found that (1) our design\nhelps overcome the learning curve for non-technical people to conduct a\nsystematic evaluation of summarization prompts, and (2) our feature-oriented\nworkflow has the potential to generalize to other NLG and image-generation\ntasks. For future works, we advocate moving towards feature-oriented evaluation\nof LLM prompts and discuss unsolved challenges in terms of human-agent\ninteraction.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"er07vxNW_8gVTSHbOmVOl31x5L5EsuLaB4SzYdORd5Y","pdfSize":"11194361"}
