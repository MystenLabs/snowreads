{"id":"2407.08100","title":"Non-convergence of Adam and other adaptive stochastic gradient descent\n  optimization methods for non-vanishing learning rates","authors":"Steffen Dereich and Robin Graeber and Arnulf Jentzen","authorsParsed":[["Dereich","Steffen",""],["Graeber","Robin",""],["Jentzen","Arnulf",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 00:10:35 GMT"}],"updateDate":"2024-07-12","timestamp":1720656635000,"abstract":"  Deep learning algorithms - typically consisting of a class of deep neural\nnetworks trained by a stochastic gradient descent (SGD) optimization method -\nare nowadays the key ingredients in many artificial intelligence (AI) systems\nand have revolutionized our ways of working and living in modern societies. For\nexample, SGD methods are used to train powerful large language models (LLMs)\nsuch as versions of ChatGPT and Gemini, SGD methods are employed to create\nsuccessful generative AI based text-to-image creation models such as\nMidjourney, DALL-E, and Stable Diffusion, but SGD methods are also used to\ntrain DNNs to approximately solve scientific models such as partial\ndifferential equation (PDE) models from physics and biology and optimal control\nand stopping problems from engineering. It is known that the plain vanilla\nstandard SGD method fails to converge even in the situation of several convex\noptimization problems if the learning rates are bounded away from zero.\nHowever, in many practical relevant training scenarios, often not the plain\nvanilla standard SGD method but instead adaptive SGD methods such as the\nRMSprop and the Adam optimizers, in which the learning rates are modified\nadaptively during the training process, are employed. This naturally rises the\nquestion whether such adaptive optimizers, in which the learning rates are\nmodified adaptively during the training process, do converge in the situation\nof non-vanishing learning rates. In this work we answer this question\nnegatively by proving that adaptive SGD methods such as the popular Adam\noptimizer fail to converge to any possible random limit point if the learning\nrates are asymptotically bounded away from zero. In our proof of this\nnon-convergence result we establish suitable pathwise a priori bounds for a\nclass of accelerated and adaptive SGD methods, which are also of independent\ninterest.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Mathematics/Probability"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}