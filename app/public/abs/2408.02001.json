{"id":"2408.02001","title":"AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and\n  Accurate Diagnosis","authors":"Townim F. Chowdhury, Vu Minh Hieu Phan, Kewen Liao, Minh-Son To,\n  Yutong Xie, Anton van den Hengel, Johan W. Verjans, and Zhibin Liao","authorsParsed":[["Chowdhury","Townim F.",""],["Phan","Vu Minh Hieu",""],["Liao","Kewen",""],["To","Minh-Son",""],["Xie","Yutong",""],["Hengel","Anton van den",""],["Verjans","Johan W.",""],["Liao","Zhibin",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 11:59:09 GMT"}],"updateDate":"2024-08-06","timestamp":1722772749000,"abstract":"  The integration of vision-language models such as CLIP and Concept Bottleneck\nModels (CBMs) offers a promising approach to explaining deep neural network\n(DNN) decisions using concepts understandable by humans, addressing the\nblack-box concern of DNNs. While CLIP provides both explainability and\nzero-shot classification capability, its pre-training on generic image and text\ndata may limit its classification accuracy and applicability to medical image\ndiagnostic tasks, creating a transfer learning problem. To maintain\nexplainability and address transfer learning needs, CBM methods commonly design\npost-processing modules after the bottleneck module. However, this way has been\nineffective. This paper takes an unconventional approach by re-examining the\nCBM framework through the lens of its geometrical representation as a simple\nlinear classification system. The analysis uncovers that post-CBM fine-tuning\nmodules merely rescale and shift the classification outcome of the system,\nfailing to fully leverage the system's learning potential. We introduce an\nadaptive module strategically positioned between CLIP and CBM to bridge the gap\nbetween source and downstream domains. This simple yet effective approach\nenhances classification performance while preserving the explainability\nafforded by the framework. Our work offers a comprehensive solution that\nencompasses the entire process, from concept discovery to model training,\nproviding a holistic recipe for leveraging the strengths of GPT, CLIP, and CBM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}