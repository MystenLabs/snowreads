{"id":"2408.06549","title":"Prioritizing Modalities: Flexible Importance Scheduling in Federated\n  Multimodal Learning","authors":"Jieming Bian, Lei Wang, Jie Xu","authorsParsed":[["Bian","Jieming",""],["Wang","Lei",""],["Xu","Jie",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 01:14:27 GMT"}],"updateDate":"2024-08-14","timestamp":1723511667000,"abstract":"  Federated Learning (FL) is a distributed machine learning approach that\nenables devices to collaboratively train models without sharing their local\ndata, ensuring user privacy and scalability. However, applying FL to real-world\ndata presents challenges, particularly as most existing FL research focuses on\nunimodal data. Multimodal Federated Learning (MFL) has emerged to address these\nchallenges, leveraging modality-specific encoder models to process diverse\ndatasets. Current MFL methods often uniformly allocate computational\nfrequencies across all modalities, which is inefficient for IoT devices with\nlimited resources. In this paper, we propose FlexMod, a novel approach to\nenhance computational efficiency in MFL by adaptively allocating training\nresources for each modality encoder based on their importance and training\nrequirements. We employ prototype learning to assess the quality of modality\nencoders, use Shapley values to quantify the importance of each modality, and\nadopt the Deep Deterministic Policy Gradient (DDPG) method from deep\nreinforcement learning to optimize the allocation of training resources. Our\nmethod prioritizes critical modalities, optimizing model performance and\nresource utilization. Experimental results on three real-world datasets\ndemonstrate that our proposed method significantly improves the performance of\nMFL models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"R6F-SQZphtwO4XgUmj1Up2Xx1ZQWeanlfUvkp5R6l6w","pdfSize":"1137216"}
