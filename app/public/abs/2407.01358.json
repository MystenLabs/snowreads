{"id":"2407.01358","title":"Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language\n  Models","authors":"Xiaolin Xing, Zhiwei He, Haoyu Xu, Xing Wang, Rui Wang, Yu Hong","authorsParsed":[["Xing","Xiaolin",""],["He","Zhiwei",""],["Xu","Haoyu",""],["Wang","Xing",""],["Wang","Rui",""],["Hong","Yu",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 15:11:37 GMT"}],"updateDate":"2024-07-02","timestamp":1719846697000,"abstract":"  This paper investigates the cross-lingual inconsistencies observed in Large\nLanguage Models (LLMs), such as ChatGPT, Llama, and Baichuan, which have shown\nexceptional performance in various Natural Language Processing (NLP) tasks.\nDespite their successes, these models often exhibit significant inconsistencies\nwhen processing the same concepts across different languages. This study\nfocuses on three primary questions: the existence of cross-lingual\ninconsistencies in LLMs, the specific aspects in which these inconsistencies\nmanifest, and the correlation between cross-lingual consistency and\nmultilingual capabilities of LLMs.To address these questions, we propose an\ninnovative evaluation method for Cross-lingual Semantic Consistency (xSC) using\nthe LaBSE model. We further introduce metrics for Cross-lingual Accuracy\nConsistency (xAC) and Cross-lingual Timeliness Consistency (xTC) to\ncomprehensively assess the models' performance regarding semantic, accuracy,\nand timeliness inconsistencies. By harmonizing these metrics, we provide a\nholistic measurement of LLMs' cross-lingual consistency. Our findings aim to\nenhance the understanding and improvement of multilingual capabilities and\ninterpretability in LLMs, contributing to the development of more robust and\nreliable multilingual language models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}