{"id":"2407.12315","title":"ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via\n  Modal Fusion Map","authors":"Yilin Ye, Shishi Xiao, Xingchen Zeng, Wei Zeng","authorsParsed":[["Ye","Yilin",""],["Xiao","Shishi",""],["Zeng","Xingchen",""],["Zeng","Wei",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 04:49:56 GMT"}],"updateDate":"2024-07-18","timestamp":1721191796000,"abstract":"  Multi-modal embeddings form the foundation for vision-language models, such\nas CLIP embeddings, the most widely used text-image embeddings. However, these\nembeddings are vulnerable to subtle misalignment of cross-modal features,\nresulting in decreased model performance and diminished generalization. To\naddress this problem, we design ModalChorus, an interactive system for visual\nprobing and alignment of multi-modal embeddings. ModalChorus primarily offers a\ntwo-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel\nparametric dimensionality reduction method that integrates both metric and\nnonmetric objectives to enhance modality fusion; and 2) embedding alignment\nthat allows users to interactively articulate intentions for both point-set and\nset-set alignments. Quantitative and qualitative comparisons for CLIP\nembeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and\ndata fusion (e.g., data context map) methods demonstrate the advantages of MFM\nin showcasing cross-modal features over common vision-language datasets. Case\nstudies reveal that ModalChorus can facilitate intuitive discovery of\nmisalignment and efficient re-alignment in scenarios ranging from zero-shot\nclassification to cross-modal retrieval and generation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}