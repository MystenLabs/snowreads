{"id":"2408.05026","title":"Retrieval-augmented code completion for local projects using large\n  language models","authors":"Marko Hostnik and Marko Robnik-\\v{S}ikonja","authorsParsed":[["Hostnik","Marko",""],["Robnik-Å ikonja","Marko",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 12:26:57 GMT"}],"updateDate":"2024-08-12","timestamp":1723206417000,"abstract":"  The use of large language models (LLMs) is becoming increasingly widespread\namong software developers. However, privacy and computational requirements are\nproblematic with commercial solutions and the use of LLMs. In this work, we\nfocus on using LLMs with around 160 million parameters that are suitable for\nlocal execution and augmentation with retrieval from local projects. We train\ntwo models based on the transformer architecture, the generative model GPT-2\nand the retrieval-adapted RETRO model, on open-source Python files, and\nempirically evaluate and compare them, confirming the benefits of vector\nembedding based retrieval. Further, we improve our models' performance with\nIn-context retrieval-augmented generation, which retrieves code snippets based\non the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented\ngeneration on larger models and conclude that, despite its simplicity, the\napproach is more suitable than using the RETRO architecture. We highlight the\nkey role of proper tokenization in achieving the full potential of LLMs in code\ncompletion.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}