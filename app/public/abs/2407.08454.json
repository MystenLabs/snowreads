{"id":"2407.08454","title":"Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks","authors":"Zheng Wang and Boxiao Jin and Zhongzhi Yu and Minjia Zhang","authorsParsed":[["Wang","Zheng",""],["Jin","Boxiao",""],["Yu","Zhongzhi",""],["Zhang","Minjia",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 12:50:42 GMT"},{"version":"v2","created":"Sun, 21 Jul 2024 02:37:11 GMT"}],"updateDate":"2024-07-23","timestamp":1720702242000,"abstract":"  How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VuEygdgM6EXrHkEwPKYCY4CVvfqOXhkJRVgdef-ONPE","pdfSize":"7147072"}
