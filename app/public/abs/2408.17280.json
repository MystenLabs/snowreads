{"id":"2408.17280","title":"Flexible and Effective Mixing of Large Language Models into a Mixture of\n  Domain Experts","authors":"Rhui Dih Lee and Laura Wynter and Raghu Kiran Ganti","authorsParsed":[["Lee","Rhui Dih",""],["Wynter","Laura",""],["Ganti","Raghu Kiran",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 13:28:45 GMT"},{"version":"v2","created":"Wed, 11 Sep 2024 02:52:19 GMT"}],"updateDate":"2024-09-12","timestamp":1725024525000,"abstract":"  We present a toolkit for creating low-cost Mixture-of-Domain-Experts (MOE)\nfrom trained models. The toolkit can be used for creating a mixture from models\nor from adapters. We perform extensive tests and offer guidance on defining the\narchitecture of the resulting MOE using the toolkit. A public repository is\navailable.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}