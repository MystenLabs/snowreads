{"id":"2408.01225","title":"Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation\n  with Volumetric Visual Data Fusion","authors":"Ke Li, Reinhard Bacher, Susanne Schmidt, Wim Leemans, Frank Steinicke","authorsParsed":[["Li","Ke",""],["Bacher","Reinhard",""],["Schmidt","Susanne",""],["Leemans","Wim",""],["Steinicke","Frank",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 12:29:02 GMT"}],"updateDate":"2024-08-05","timestamp":1722601742000,"abstract":"  We introduce Reality Fusion, a novel robot teleoperation system that\nlocalizes, streams, projects, and merges a typical onboard depth sensor with a\nphotorealistic, high resolution, high framerate, and wide field of view (FoV)\nrendering of the complex remote environment represented as 3D Gaussian splats\n(3DGS). Our framework enables robust egocentric and exocentric robot\nteleoperation in immersive VR, with the 3DGS effectively extending spatial\ninformation of a depth sensor with limited FoV and balancing the trade-off\nbetween data streaming costs and data visual quality. We evaluated our\nframework through a user study with 24 participants, which revealed that\nReality Fusion leads to significantly better user performance, situation\nawareness, and user preferences. To support further research and development,\nwe provide an open-source implementation with an easy-to-replicate custom-made\ntelepresence robot, a high-performance virtual reality 3DGS renderer, and an\nimmersive robot control package. (Source code:\nhttps://github.com/uhhhci/RealityFusion)\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"jjxzZztGw1FoX_2YSQPN_o_sRbSwETGSwXESxd-56_s","pdfSize":"3860323"}
