{"id":"2408.03307","title":"Pre-training and in-context learning IS Bayesian inference a la De\n  Finetti","authors":"Naimeng Ye, Hanming Yang, Andrew Siah and Hongseok Namkoong","authorsParsed":[["Ye","Naimeng",""],["Yang","Hanming",""],["Siah","Andrew",""],["Namkoong","Hongseok",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 17:16:10 GMT"}],"updateDate":"2024-08-07","timestamp":1722964570000,"abstract":"  Accurately gauging uncertainty on the underlying environment is a\nlongstanding goal of intelligent systems. We characterize which latent concepts\npre-trained sequence models are naturally able to reason with. We go back to De\nFinetti's predictive view of Bayesian reasoning: instead of modeling latent\nparameters through priors and likelihoods like topic models do, De Finetti has\nlong advocated for modeling exchangeable (permutation invariant) sequences of\nobservables. According to this view, pre-training autoregressive models\nformulates informed beliefs based on prior observations (\"empirical Bayes\"),\nand forward generation is a simulated instantiation of an environment\n(\"posterior inference\"). This connection allows extending in-context learning\n(ICL) beyond predictive settings, highlighting sequence models' ability to\nperform explicit statistical inference. In particular, we show the sequence\nprediction loss over exchangeable documents controls performance on downstream\ntasks where uncertainty quantification is key. Empirically, we propose and\ndemonstrate several approaches for encoding exchangeability in sequence model\narchitectures: data augmentation, regularization, and causal masking.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}