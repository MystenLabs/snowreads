{"id":"2407.10106","title":"DistillSeq: A Framework for Safety Alignment Testing in Large Language\n  Models using Knowledge Distillation","authors":"Mingke Yang, Yuqi Chen, Yi Liu and Ling Shi","authorsParsed":[["Yang","Mingke",""],["Chen","Yuqi",""],["Liu","Yi",""],["Shi","Ling",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 07:21:54 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 10:27:17 GMT"},{"version":"v3","created":"Sat, 20 Jul 2024 14:50:34 GMT"},{"version":"v4","created":"Sun, 8 Sep 2024 07:09:33 GMT"}],"updateDate":"2024-09-17","timestamp":1720941714000,"abstract":"  Large Language Models (LLMs) have showcased their remarkable capabilities in\ndiverse domains, encompassing natural language understanding, translation, and\neven code generation. The potential for LLMs to generate harmful content is a\nsignificant concern. This risk necessitates rigorous testing and comprehensive\nevaluation of LLMs to ensure safe and responsible use. However, extensive\ntesting of LLMs requires substantial computational resources, making it an\nexpensive endeavor. Therefore, exploring cost-saving strategies during the\ntesting phase is crucial to balance the need for thorough evaluation with the\nconstraints of resource availability. To address this, our approach begins by\ntransferring the moderation knowledge from an LLM to a small model.\nSubsequently, we deploy two distinct strategies for generating malicious\nqueries: one based on a syntax tree approach, and the other leveraging an\nLLM-based method. Finally, our approach incorporates a sequential filter-test\nprocess designed to identify test cases that are prone to eliciting toxic\nresponses. Our research evaluated the efficacy of DistillSeq across four LLMs:\nGPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the\nobserved attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4%\nfor GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the\napplication of DistillSeq, these success rates notably increased to 58.5%,\n50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation\nin attack success rate by a factor of 93.0% when compared to scenarios without\nthe use of DistillSeq. Such findings highlight the significant enhancement\nDistillSeq offers in terms of reducing the time and resource investment\nrequired for effectively testing LLMs.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"afvy5_3ZerqVsoLvvIK6xf5gDE7xup5sxwpOdZkm1Y8","pdfSize":"1385661"}
