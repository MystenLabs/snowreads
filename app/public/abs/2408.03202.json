{"id":"2408.03202","title":"A Debiased Nearest Neighbors Framework for Multi-Label Text\n  Classification","authors":"Zifeng Cheng, Zhiwei Jiang, Yafeng Yin, Zhaoling Chen, Cong Wang,\n  Shiping Ge, Qiguo Huang, Qing Gu","authorsParsed":[["Cheng","Zifeng",""],["Jiang","Zhiwei",""],["Yin","Yafeng",""],["Chen","Zhaoling",""],["Wang","Cong",""],["Ge","Shiping",""],["Huang","Qiguo",""],["Gu","Qing",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 14:00:23 GMT"}],"updateDate":"2024-08-07","timestamp":1722952823000,"abstract":"  Multi-Label Text Classification (MLTC) is a practical yet challenging task\nthat involves assigning multiple non-exclusive labels to each document.\nPrevious studies primarily focus on capturing label correlations to assist\nlabel prediction by introducing special labeling schemes, designing specific\nmodel structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor\n($k$NN) framework has shown promise by retrieving labeled samples as references\nto mine label co-occurrence information in the embedding space. However, two\ncritical biases, namely embedding alignment bias and confidence estimation\nbias, are often overlooked, adversely affecting prediction performance. In this\npaper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC,\nspecifically designed to mitigate these biases. To address embedding alignment\nbias, we propose a debiased contrastive learning strategy, enhancing neighbor\nconsistency on label co-occurrence. For confidence estimation bias, we present\na debiased confidence estimation strategy, improving the adaptive combination\nof predictions from $k$NN and inductive binary classifications. Extensive\nexperiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2,\nAmazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.\nBesides, our method does not introduce any extra parameters.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}