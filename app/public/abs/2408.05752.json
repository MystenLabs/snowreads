{"id":"2408.05752","title":"RTF-Q: Efficient Unsupervised Domain Adaptation with Retraining-free\n  Quantization","authors":"Nanyang Du, Chen Tang, Yuxiao Jiang, Yuan Meng, Zhi Wang","authorsParsed":[["Du","Nanyang",""],["Tang","Chen",""],["Jiang","Yuxiao",""],["Meng","Yuan",""],["Wang","Zhi",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 11:53:29 GMT"},{"version":"v2","created":"Fri, 13 Sep 2024 12:37:34 GMT"}],"updateDate":"2024-09-16","timestamp":1723377209000,"abstract":"  Performing unsupervised domain adaptation on resource-constrained edge\ndevices is challenging. Existing research typically adopts architecture\noptimization (e.g., designing slimmable networks) but requires expensive\ntraining costs. Moreover, it does not consider the considerable precision\nredundancy of parameters and activations. To address these limitations, we\npropose efficient unsupervised domain adaptation with ReTraining-Free\nQuantization (RTF-Q). Our approach uses low-precision quantization\narchitectures with varying computational costs, adapting to devices with\ndynamic computation budgets. We subtly configure subnet dimensions and leverage\nweight-sharing to optimize multiple architectures within a single set of\nweights, enabling the use of pre-trained models from open-source repositories.\nAdditionally, we introduce multi-bitwidth joint training and the SandwichQ\nrule, both of which are effective in handling multiple quantization bit-widths\nacross subnets. Experimental results demonstrate that our network achieves\ncompetitive accuracy with state-of-the-art methods across three benchmarks\nwhile significantly reducing memory and computational costs.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}