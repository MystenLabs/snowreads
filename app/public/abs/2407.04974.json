{"id":"2407.04974","title":"Multi-agent Off-policy Actor-Critic Reinforcement Learning for Partially\n  Observable Environments","authors":"Ainur Zhaikhan and Ali H. Sayed","authorsParsed":[["Zhaikhan","Ainur",""],["Sayed","Ali H.",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 06:51:14 GMT"}],"updateDate":"2024-07-09","timestamp":1720248674000,"abstract":"  This study proposes the use of a social learning method to estimate a global\nstate within a multi-agent off-policy actor-critic algorithm for reinforcement\nlearning (RL) operating in a partially observable environment. We assume that\nthe network of agents operates in a fully-decentralized manner, possessing the\ncapability to exchange variables with their immediate neighbors. The proposed\ndesign methodology is supported by an analysis demonstrating that the\ndifference between final outcomes, obtained when the global state is fully\nobserved versus estimated through the social learning method, is\n$\\varepsilon$-bounded when an appropriate number of iterations of social\nlearning updates are implemented. Unlike many existing dec-POMDP-based RL\napproaches, the proposed algorithm is suitable for model-free multi-agent\nreinforcement learning as it does not require knowledge of a transition model.\nFurthermore, experimental results illustrate the efficacy of the algorithm and\ndemonstrate its superiority over the current state-of-the-art methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/"}