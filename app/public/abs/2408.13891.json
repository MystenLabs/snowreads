{"id":"2408.13891","title":"SpeechCaps: Advancing Instruction-Based Universal Speech Models with\n  Multi-Talker Speaking Style Captioning","authors":"Chien-yu Huang, Min-Han Shih, Ke-Han Lu, Chi-Yuan Hsiao, Hung-yi Lee","authorsParsed":[["Huang","Chien-yu",""],["Shih","Min-Han",""],["Lu","Ke-Han",""],["Hsiao","Chi-Yuan",""],["Lee","Hung-yi",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 17:05:26 GMT"}],"updateDate":"2024-08-27","timestamp":1724605526000,"abstract":"  Instruction-based speech processing is becoming popular. Studies show that\ntraining with multiple tasks boosts performance, but collecting diverse,\nlarge-scale tasks and datasets is expensive. Thus, it is highly desirable to\ndesign a fundamental task that benefits other downstream tasks. This paper\nintroduces a multi-talker speaking style captioning task to enhance the\nunderstanding of speaker and prosodic information. We used large language\nmodels to generate descriptions for multi-talker speech. Then, we trained our\nmodel with pre-training on this captioning task followed by instruction tuning.\nEvaluation on Dynamic-SUPERB shows our model outperforming the baseline\npre-trained only on single-talker tasks, particularly in speaker and emotion\nrecognition. Additionally, tests on a multi-talker QA task reveal that current\nmodels struggle with attributes such as gender, pitch, and speaking rate. The\ncode and dataset are available at https://github.com/cyhuang-tw/speechcaps.\n","subjects":["Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}