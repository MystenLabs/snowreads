{"id":"2407.18332","title":"Analyzing Speech Unit Selection for Textless Speech-to-Speech\n  Translation","authors":"Jarod Duret (LIA), Yannick Est\\`eve (LIA), Titouan Parcollet (CAM)","authorsParsed":[["Duret","Jarod","","LIA"],["Est√®ve","Yannick","","LIA"],["Parcollet","Titouan","","CAM"]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 08:53:26 GMT"}],"updateDate":"2024-07-29","timestamp":1720428806000,"abstract":"  Recent advancements in textless speech-to-speech translation systems have\nbeen driven by the adoption of self-supervised learning techniques. Although\nmost state-of-the-art systems adopt a similar architecture to transform source\nlanguage speech into sequences of discrete representations in the target\nlanguage, the criteria for selecting these target speech units remains an open\nquestion. This work explores the selection process through a study of\ndownstream tasks such as automatic speech recognition, speech synthesis,\nspeaker recognition, and emotion recognition. Interestingly, our findings\nreveal a discrepancy in the optimization of discrete speech units: units that\nperform well in resynthesis performance do not necessarily correlate with those\nthat enhance translation efficacy. This discrepancy underscores the nuanced\ncomplexity of target feature selection and its impact on the overall\nperformance of speech-to-speech translation systems.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}