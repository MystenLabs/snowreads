{"id":"2407.17546","title":"Exploring Domain Robust Lightweight Reward Models based on Router\n  Mechanism","authors":"Hyuk Namgoong, Jeesu Jung, Sangkeun Jung, Yoonhyung Roh","authorsParsed":[["Namgoong","Hyuk",""],["Jung","Jeesu",""],["Jung","Sangkeun",""],["Roh","Yoonhyung",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:25:12 GMT"}],"updateDate":"2024-07-26","timestamp":1721841912000,"abstract":"  Recent advancements in large language models have heavily relied on the large\nreward model from reinforcement learning from human feedback for fine-tuning.\nHowever, the use of a single reward model across various domains may not always\nbe optimal, often requiring retraining from scratch when new domain data is\nintroduced. To address these challenges, we explore the utilization of small\nlanguage models operating in a domain-specific manner based on router\nmechanisms. Our three approaches are: 1) utilize mixture of experts to form a\nsingle reward model by modularizing an internal router and experts, 2)\nemploying external router to select the appropriate reward model from multiple\ndomain-specific models, and 3) the framework reduces parameter size by loading\nreward models and router adapters onto a single small language model using\nadapters. Experimental validation underscores the effectiveness of our\napproach, demonstrating performance comparable to baseline methods while also\nreducing the total parameter size.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}