{"id":"2408.07107","title":"Maximizing V-information for Pre-training Superior Foundation Models","authors":"Wenxuan Yang, Weimin Tan, Hanyu Zhang, Bo Yan","authorsParsed":[["Yang","Wenxuan",""],["Tan","Weimin",""],["Zhang","Hanyu",""],["Yan","Bo",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 10:28:54 GMT"},{"version":"v2","created":"Fri, 16 Aug 2024 12:19:44 GMT"}],"updateDate":"2024-08-19","timestamp":1723544934000,"abstract":"  Pre-training foundation models on large-scale datasets demonstrates\nexceptional performance. However, recent research questions this traditional\nnotion, exploring whether an increase in pre-training data always leads to\nenhanced model performance. To address this issue, data-effective learning\napproaches have been introduced. However, current methods in this area lack a\nclear standard for sample selection. Our experiments reveal that by maximizing\nV-information, sample selection can be framed as an optimization problem,\nenabling effective improvement in model performance even with fewer samples.\nUnder this guidance, we develop an optimal data-effective learning method\n(OptiDEL) to maximize V-information. The OptiDEL method generates hard samples\nto achieve or even exceed the performance of models trained on the full dataset\nwhile using substantially less data. We compare the OptiDEL method with\nstate-of-the-art approaches finding that OptiDEL consistently outperforms\nexisting approaches across different datasets, with foundation models trained\non only 5% of the pre-training data surpassing the performance of those trained\non the full dataset.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}