{"id":"2408.05889","title":"Enhancing 3D Transformer Segmentation Model for Medical Image with\n  Token-level Representation Learning","authors":"Xinrong Hu, Dewen Zeng, Yawen Wu, Xueyang Li, Yiyu Shi","authorsParsed":[["Hu","Xinrong",""],["Zeng","Dewen",""],["Wu","Yawen",""],["Li","Xueyang",""],["Shi","Yiyu",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 01:49:13 GMT"}],"updateDate":"2024-08-13","timestamp":1723427353000,"abstract":"  In the field of medical images, although various works find Swin Transformer\nhas promising effectiveness on pixelwise dense prediction, whether pre-training\nthese models without using extra dataset can further boost the performance for\nthe downstream semantic segmentation remains unexplored.Applications of\nprevious representation learning methods are hindered by the limited number of\n3D volumes and high computational cost. In addition, most of pretext tasks\ndesigned specifically for Transformer are not applicable to hierarchical\nstructure of Swin Transformer. Thus, this work proposes a token-level\nrepresentation learning loss that maximizes agreement between token embeddings\nfrom different augmented views individually instead of volume-level global\nfeatures. Moreover, we identify a potential representation collapse exclusively\ncaused by this new loss. To prevent collapse, we invent a simple\n\"rotate-and-restore\" mechanism, which rotates and flips one augmented view of\ninput volume, and later restores the order of tokens in the feature maps. We\nalso modify the contrastive loss to address the discrimination between tokens\nat the same position but from different volumes. We test our pre-training\nscheme on two public medical segmentation datasets, and the results on the\ndownstream segmentation task show more improvement of our methods than other\nstate-of-the-art pre-trainig methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}