{"id":"2407.01492","title":"RegMix: Data Mixture as Regression for Language Model Pre-training","authors":"Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu\n  Dou, Tianyu Pang, Jing Jiang, Min Lin","authorsParsed":[["Liu","Qian",""],["Zheng","Xiaosen",""],["Muennighoff","Niklas",""],["Zeng","Guangtao",""],["Dou","Longxu",""],["Pang","Tianyu",""],["Jiang","Jing",""],["Lin","Min",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:31:03 GMT"}],"updateDate":"2024-07-02","timestamp":1719855063000,"abstract":"  The data mixture for large language model pre-training significantly impacts\nperformance, yet how to determine an effective mixture remains unclear. We\npropose RegMix to automatically identify a high-performing data mixture by\nformulating it as a regression task. RegMix involves training a set of small\nmodels with diverse data mixtures and fitting a regression model to predict\ntheir performance given their respective mixtures. With the fitted regression\nmodel, we simulate the top-ranked mixture and use it to train a large-scale\nmodel with orders of magnitude more compute. To empirically validate RegMix, we\ntrain 512 models with 1M parameters for 1B tokens of different mixtures to fit\nthe regression model and find the optimal mixture. Using this mixture we train\na 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we\nfind performs best among 64 candidate 1B parameter models with other mixtures.\nFurther, our method demonstrates superior performance compared to human\nselection and achieves results that match or surpass DoReMi, while utilizing\nonly 10% of the compute budget. Our experiments also show that (1) Data\nmixtures significantly impact performance with single-task performance\nvariations of up to 14.6%; (2) Web corpora rather than data perceived as\nhigh-quality like Wikipedia have the strongest positive correlation with\ndownstream performance; (3) Domains interact in complex ways often\ncontradicting common sense, thus automatic approaches like RegMix are needed;\n(4) Data mixture effects transcend scaling laws, and our approach captures the\ncomplexity by considering all domains together. Our code is available at\nhttps://github.com/sail-sg/regmix.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NzoomUcdpcro_FUFQrcsOchGwsLONlIIZRIMcWK2fak","pdfSize":"1409818"}
