{"id":"2408.06832","title":"FlatFusion: Delving into Details of Sparse Transformer-based\n  Camera-LiDAR Fusion for Autonomous Driving","authors":"Yutao Zhu, Xiaosong Jia, Xinyu Yang, Junchi Yan","authorsParsed":[["Zhu","Yutao",""],["Jia","Xiaosong",""],["Yang","Xinyu",""],["Yan","Junchi",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 11:46:32 GMT"}],"updateDate":"2024-08-14","timestamp":1723549592000,"abstract":"  The integration of data from diverse sensor modalities (e.g., camera and\nLiDAR) constitutes a prevalent methodology within the ambit of autonomous\ndriving scenarios. Recent advancements in efficient point cloud transformers\nhave underscored the efficacy of integrating information in sparse formats.\nWhen it comes to fusion, since image patches are dense in pixel space with\nambiguous depth, it necessitates additional design considerations for effective\nfusion. In this paper, we conduct a comprehensive exploration of design choices\nfor Transformer-based sparse cameraLiDAR fusion. This investigation encompasses\nstrategies for image-to-3D and LiDAR-to-2D mapping, attention neighbor\ngrouping, single modal tokenizer, and micro-structure of Transformer. By\namalgamating the most effective principles uncovered through our investigation,\nwe introduce FlatFusion, a carefully designed framework for sparse camera-LiDAR\nfusion. Notably, FlatFusion significantly outperforms state-of-the-art sparse\nTransformer-based methods, including UniTR, CMT, and SparseFusion, achieving\n73.7 NDS on the nuScenes validation set with 10.1 FPS with PyTorch.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}