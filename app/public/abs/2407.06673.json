{"id":"2407.06673","title":"CTRL-F: Pairing Convolution with Transformer for Image Classification\n  via Multi-Level Feature Cross-Attention and Representation Learning Fusion","authors":"Hosam S. EL-Assiouti, Hadeer El-Saadawy, Maryam N. Al-Berry, Mohamed\n  F. Tolba","authorsParsed":[["EL-Assiouti","Hosam S.",""],["El-Saadawy","Hadeer",""],["Al-Berry","Maryam N.",""],["Tolba","Mohamed F.",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 08:47:13 GMT"}],"updateDate":"2024-07-10","timestamp":1720514833000,"abstract":"  Transformers have captured growing attention in computer vision, thanks to\nits large capacity and global processing capabilities. However, transformers\nare data hungry, and their ability to generalize is constrained compared to\nConvolutional Neural Networks (ConvNets), especially when trained with limited\ndata due to the absence of the built-in spatial inductive biases present in\nConvNets. In this paper, we strive to optimally combine the strengths of both\nconvolution and transformers for image classification tasks. Towards this end,\nwe present a novel lightweight hybrid network that pairs Convolution with\nTransformers via Representation Learning Fusion and Multi-Level Feature\nCross-Attention named CTRL-F. Our network comprises a convolution branch and a\nnovel transformer module named multi-level feature cross-attention (MFCA). The\nMFCA module operates on multi-level feature representations obtained at\ndifferent convolution stages. It processes small patch tokens and large patch\ntokens extracted from these multi-level feature representations via two\nseparate transformer branches, where both branches communicate and exchange\nknowledge through cross-attention mechanism. We fuse the local responses\nacquired from the convolution path with the global responses acquired from the\nMFCA module using novel representation fusion techniques dubbed adaptive\nknowledge fusion (AKF) and collaborative knowledge fusion (CKF). Experiments\ndemonstrate that our CTRL-F variants achieve state-of-the-art performance,\nwhether trained from scratch on large data or even with low-data regime. For\nInstance, CTRL-F achieves top-1 accuracy of 82.24% and 99.91% when trained from\nscratch on Oxford-102 Flowers and PlantVillage datasets respectively,\nsurpassing state-of-the-art models which showcase the robustness of our model\non image classification tasks. Code at: https://github.com/hosamsherif/CTRL-F\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}