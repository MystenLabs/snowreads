{"id":"2407.01130","title":"Cross-Lingual Transfer Learning for Speech Translation","authors":"Rao Ma, Yassir Fathullah, Mengjie Qian, Siyuan Tang, Mark Gales, Kate\n  Knill","authorsParsed":[["Ma","Rao",""],["Fathullah","Yassir",""],["Qian","Mengjie",""],["Tang","Siyuan",""],["Gales","Mark",""],["Knill","Kate",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 09:51:48 GMT"}],"updateDate":"2024-07-02","timestamp":1719827508000,"abstract":"  There has been increasing interest in building multilingual foundation models\nfor NLP and speech research. Zero-shot cross-lingual transfer has been\ndemonstrated on a range of NLP tasks where a model fine-tuned on task-specific\ndata in one language yields performance gains in other languages. Here, we\nexplore whether speech-based models exhibit the same transfer capability. Using\nWhisper as an example of a multilingual speech foundation model, we examine the\nutterance representation generated by the speech encoder. Despite some\nlanguage-sensitive information being preserved in the audio embedding, words\nfrom different languages are mapped to a similar semantic space, as evidenced\nby a high recall rate in a speech-to-speech retrieval task. Leveraging this\nshared embedding space, zero-shot cross-lingual transfer is demonstrated in\nspeech translation. When the Whisper model is fine-tuned solely on\nEnglish-to-Chinese translation data, performance improvements are observed for\ninput utterances in other languages. Additionally, experiments on low-resource\nlanguages show that Whisper can perform speech translation for utterances from\nlanguages unseen during pre-training by utilizing cross-lingual\nrepresentations.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}