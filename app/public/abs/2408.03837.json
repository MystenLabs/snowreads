{"id":"2408.03837","title":"WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language\n  Models","authors":"Prannaya Gupta, Le Qi Yau, Hao Han Low, I-Shiang Lee, Hugo Maximus\n  Lim, Yu Xin Teoh, Jia Hng Koh, Dar Win Liew, Rishabh Bhardwaj, Rajat\n  Bhardwaj, Soujanya Poria","authorsParsed":[["Gupta","Prannaya",""],["Yau","Le Qi",""],["Low","Hao Han",""],["Lee","I-Shiang",""],["Lim","Hugo Maximus",""],["Teoh","Yu Xin",""],["Koh","Jia Hng",""],["Liew","Dar Win",""],["Bhardwaj","Rishabh",""],["Bhardwaj","Rajat",""],["Poria","Soujanya",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 15:22:44 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 18:05:50 GMT"},{"version":"v3","created":"Mon, 19 Aug 2024 21:57:24 GMT"}],"updateDate":"2024-08-21","timestamp":1723044164000,"abstract":"  WalledEval is a comprehensive AI safety testing toolkit designed to evaluate\nlarge language models (LLMs). It accommodates a diverse range of models,\nincluding both open-weight and API-based ones, and features over 35 safety\nbenchmarks covering areas such as multilingual safety, exaggerated safety, and\nprompt injections. The framework supports both LLM and judge benchmarking and\nincorporates custom mutators to test safety against various text-style\nmutations, such as future tense and paraphrasing. Additionally, WalledEval\nintroduces WalledGuard, a new, small, and performant content moderation tool,\nand two datasets: SGXSTest and HIXSTest, which serve as benchmarks for\nassessing the exaggerated safety of LLMs and judges in cultural contexts. We\nmake WalledEval publicly available at https://github.com/walledai/walledeval.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}