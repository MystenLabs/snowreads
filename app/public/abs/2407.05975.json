{"id":"2407.05975","title":"LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation\n  Capabilities Beyond 100 Languages","authors":"Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, Fei Yuan","authorsParsed":[["Lu","Yinquan",""],["Zhu","Wenhao",""],["Li","Lei",""],["Qiao","Yu",""],["Yuan","Fei",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 14:18:28 GMT"}],"updateDate":"2024-07-09","timestamp":1720448308000,"abstract":"  Large Language Models~(LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting\nextensive multilingual continual pre-training on the LLaMA series models,\nenabling translation support across more than 100 languages. Through a\ncomprehensive analysis of training strategies, such as vocabulary expansion and\ndata augmentation, we develop LLaMAX. Remarkably, without sacrificing its\ngeneralization ability, LLaMAX achieves significantly higher translation\nperformance compared to existing open-source LLMs~(by more than 10 spBLEU\npoints) and performs on-par with specialized translation model~(M2M-100-12B) on\nthe Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve\nas a robust multilingual foundation model. The\ncode~\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and\nmodels~\\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}