{"id":"2408.14252","title":"An Evaluation of Explanation Methods for Black-Box Detectors of\n  Machine-Generated Text","authors":"Loris Schoenegger, Yuxi Xia, Benjamin Roth","authorsParsed":[["Schoenegger","Loris",""],["Xia","Yuxi",""],["Roth","Benjamin",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 13:14:26 GMT"}],"updateDate":"2024-08-28","timestamp":1724678066000,"abstract":"  The increasing difficulty to distinguish language-model-generated from\nhuman-written text has led to the development of detectors of machine-generated\ntext (MGT). However, in many contexts, a black-box prediction is not\nsufficient, it is equally important to know on what grounds a detector made\nthat prediction. Explanation methods that estimate feature importance promise\nto provide indications of which parts of an input are used by classifiers for\nprediction. However, the quality of different explanation methods has not\npreviously been assessed for detectors of MGT. This study conducts the first\nsystematic evaluation of explanation quality for this task. The dimensions of\nfaithfulness and stability are assessed with five automated experiments, and\nusefulness is evaluated in a user study. We use a dataset of ChatGPT-generated\nand human-written documents, and pair predictions of three existing\nlanguage-model-based detectors with the corresponding SHAP, LIME, and Anchor\nexplanations. We find that SHAP performs best in terms of faithfulness,\nstability, and in helping users to predict the detector's behavior. In\ncontrast, LIME, perceived as most useful by users, scores the worst in terms of\nuser performance at predicting the detectors' behavior.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"egDKd1MzdS-ptPcCyItLiHdoq0qPJ_1m1ZZiaMaIb1Q","pdfSize":"2357869"}
