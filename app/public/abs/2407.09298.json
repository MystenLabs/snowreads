{"id":"2407.09298","title":"Transformer Layers as Painters","authors":"Qi Sun, Marc Pickett, Aakash Kumar Nain, Llion Jones","authorsParsed":[["Sun","Qi",""],["Pickett","Marc",""],["Nain","Aakash Kumar",""],["Jones","Llion",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 14:31:05 GMT"},{"version":"v2","created":"Mon, 5 Aug 2024 15:10:25 GMT"}],"updateDate":"2024-08-06","timestamp":1720794665000,"abstract":"  Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ys0jELxdQxi5bF3VNIS3fQ7e5XN6r_n27MQMMMEElIA","pdfSize":"2012602"}
