{"id":"2408.04332","title":"Mitigating Exposure Bias in Online Learning to Rank Recommendation: A\n  Novel Reward Model for Cascading Bandits","authors":"Masoud Mansoury, Bamshad Mobasher, Herke van Hoof","authorsParsed":[["Mansoury","Masoud",""],["Mobasher","Bamshad",""],["van Hoof","Herke",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 09:35:01 GMT"}],"updateDate":"2024-08-09","timestamp":1723109701000,"abstract":"  Exposure bias is a well-known issue in recommender systems where items and\nsuppliers are not equally represented in the recommendation results. This bias\nbecomes particularly problematic over time as a few items are repeatedly\nover-represented in recommendation lists, leading to a feedback loop that\nfurther amplifies this bias. Although extensive research has addressed this\nissue in model-based or neighborhood-based recommendation algorithms, less\nattention has been paid to online recommendation models, such as those based on\ntop-K contextual bandits, where recommendation models are dynamically updated\nwith ongoing user feedback. In this paper, we study exposure bias in a class of\nwell-known contextual bandit algorithms known as Linear Cascading Bandits. We\nanalyze these algorithms in their ability to handle exposure bias and provide a\nfair representation of items in the recommendation results. Our analysis\nreveals that these algorithms fail to mitigate exposure bias in the long run\nduring the course of ongoing user interactions. We propose an Exposure-Aware\nreward model that updates the model parameters based on two factors: 1)\nimplicit user feedback and 2) the position of the item in the recommendation\nlist. The proposed model mitigates exposure bias by controlling the utility\nassigned to the items based on their exposure in the recommendation list. Our\nexperiments with two real-world datasets show that our proposed reward model\nimproves the exposure fairness of the linear cascading bandits over time while\nmaintaining the recommendation accuracy. It also outperforms the current\nbaselines. Finally, we prove a high probability upper regret bound for our\nproposed model, providing theoretical guarantees for its performance.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"hYxWh-MojH13oCfhY0EXPoCyiXqotOcXAXoFvfpdAhA","pdfSize":"4534856"}
