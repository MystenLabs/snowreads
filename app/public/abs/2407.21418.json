{"id":"2407.21418","title":"FTuner: A Fast Dynamic Shape Tensors Program Auto-Tuner for Deep\n  Learning Compilers","authors":"Pengyu Mu, Linquan Wei, Yi Liu, Rui Wang","authorsParsed":[["Mu","Pengyu",""],["Wei","Linquan",""],["Liu","Yi",""],["Wang","Rui",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 08:05:33 GMT"}],"updateDate":"2024-08-01","timestamp":1722413133000,"abstract":"  Many artificial intelligence models process input data of different lengths\nand resolutions, making the shape of the tensors dynamic. The performance of\nthese models depends on the shape of the tensors, which makes it difficult to\noptimize the tensors before the model runs. There are two common solutions to\nthis problem. The first is to add useless data to the input to match a\npre-optimized tensor library. The second is to use small basic tensors to\ncreate a tensor that is closest in size to the input data and then tune it to\nminimize padding. However, this second solution can be time-consuming.\n  This paper proposes a new technique for deep learning compilers called\nFTuner. Instead of using a large design space or training a cost model, we use\nan abstract computational unit called the uKernel to patch together small,\nvarious-sized tensors to match the shape of the input tensor. We determine the\nshape of the uKernel using an analytic hardware information model. Experiments\nshow that the FTuner can achieve comparable operators and end-to-end\nperformance to vendor libraries and achieves 3\\% speedup on existing auto-tuner\nwith the model-training compiler while reducing tuning time by two orders of\nmagnitude.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}