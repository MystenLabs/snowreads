{"id":"2407.09429","title":"Open (Clinical) LLMs are Sensitive to Instruction Phrasings","authors":"Alberto Mario Ceballos Arroyo, Monica Munnangi, Jiuding Sun, Karen\n  Y.C. Zhang, Denis Jered McInerney, Byron C. Wallace, Silvio Amir","authorsParsed":[["Arroyo","Alberto Mario Ceballos",""],["Munnangi","Monica",""],["Sun","Jiuding",""],["Zhang","Karen Y. C.",""],["McInerney","Denis Jered",""],["Wallace","Byron C.",""],["Amir","Silvio",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 17:00:44 GMT"}],"updateDate":"2024-07-15","timestamp":1720803644000,"abstract":"  Instruction-tuned Large Language Models (LLMs) can perform a wide range of\ntasks given natural language instructions to do so, but they are sensitive to\nhow such instructions are phrased. This issue is especially concerning in\nhealthcare, as clinicians are unlikely to be experienced prompt engineers and\nthe potential consequences of inaccurate outputs are heightened in this domain.\n  This raises a practical question: How robust are instruction-tuned LLMs to\nnatural variations in the instructions provided for clinical NLP tasks? We\ncollect prompts from medical doctors across a range of tasks and quantify the\nsensitivity of seven LLMs -- some general, others specialized -- to natural\n(i.e., non-adversarial) instruction phrasings. We find that performance varies\nsubstantially across all models, and that -- perhaps surprisingly --\ndomain-specific models explicitly trained on clinical data are especially\nbrittle, compared to their general domain counterparts. Further, arbitrary\nphrasing differences can affect fairness, e.g., valid but distinct instructions\nfor mortality prediction yield a range both in overall performance, and in\nterms of differences between demographic groups.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"xoulrbJVJ5KICXHQYYJ7-hgLfZVGPBJD_5jyA5jIeYo","pdfSize":"1526465"}
