{"id":"2408.13591","title":"Optimal Kernel Quantile Learning with Random Features","authors":"Caixing Wang and Xingdong Feng","authorsParsed":[["Wang","Caixing",""],["Feng","Xingdong",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 14:26:09 GMT"}],"updateDate":"2024-08-27","timestamp":1724509569000,"abstract":"  The random feature (RF) approach is a well-established and efficient tool for\nscalable kernel methods, but existing literature has primarily focused on\nkernel ridge regression with random features (KRR-RF), which has limitations in\nhandling heterogeneous data with heavy-tailed noises. This paper presents a\ngeneralization study of kernel quantile regression with random features\n(KQR-RF), which accounts for the non-smoothness of the check loss in KQR-RF by\nintroducing a refined error decomposition and establishing a novel connection\nbetween KQR-RF and KRR-RF. Our study establishes the capacity-dependent\nlearning rates for KQR-RF under mild conditions on the number of RFs, which are\nminimax optimal up to some logarithmic factors. Importantly, our theoretical\nresults, utilizing a data-dependent sampling strategy, can be extended to cover\nthe agnostic setting where the target quantile function may not precisely align\nwith the assumed kernel space. By slightly modifying our assumptions, the\ncapacity-dependent error analysis can also be applied to cases with Lipschitz\ncontinuous losses, enabling broader applications in the machine learning\ncommunity. To validate our theoretical findings, simulated experiments and a\nreal data application are conducted.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}