{"id":"2408.05554","title":"Improving Whisper's Recognition Performance for Under-Represented\n  Language Kazakh Leveraging Unpaired Speech and Text","authors":"Jinpeng Li, Yu Pu, Qi Sun, Wei-Qiang Zhang","authorsParsed":[["Li","Jinpeng",""],["Pu","Yu",""],["Sun","Qi",""],["Zhang","Wei-Qiang",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 13:39:13 GMT"}],"updateDate":"2024-08-13","timestamp":1723297153000,"abstract":"  Whisper and other large-scale automatic speech recognition models have made\nsignificant progress in performance. However, their performance on many\nlow-resource languages, such as Kazakh, is not satisfactory. It is worth\nresearching how to utilize low-cost data to improve the performance of Whisper\non under-represented languages. In this study, we utilized easily accessible\nunpaired speech and text data and combined the language model GPT with Whisper\non Kazakh. We implemented end of transcript (EOT) judgment modification and\nhallucination penalty to improve the performance of speech recognition.\nFurther, we employed the decoding average token log probability as a criterion\nto select samples from unlabeled speech data and used pseudo-labeled data to\nfine-tune the model to further improve its performance. Ultimately, we achieved\nmore than 10\\% absolute WER reduction in multiple experiments, and the whole\nprocess has the potential to be generalized to other under-represented\nlanguages.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}