{"id":"2408.09958","title":"AdaResNet: Enhancing Residual Networks with Dynamic Weight Adjustment\n  for Improved Feature Integration","authors":"Hong Su","authorsParsed":[["Su","Hong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 12:58:51 GMT"}],"updateDate":"2024-08-20","timestamp":1724072331000,"abstract":"  In very deep neural networks, gradients can become extremely small during\nbackpropagation, making it challenging to train the early layers. ResNet\n(Residual Network) addresses this issue by enabling gradients to flow directly\nthrough the network via skip connections, facilitating the training of much\ndeeper networks. However, in these skip connections, the input ipd is directly\nadded to the transformed data tfd, treating ipd and tfd equally, without\nadapting to different scenarios. In this paper, we propose AdaResNet\n(Auto-Adapting Residual Network), which automatically adjusts the ratio between\nipd and tfd based on the training data. We introduce a variable,\nweight}_{tfd}^{ipd, to represent this ratio. This variable is dynamically\nadjusted during backpropagation, allowing it to adapt to the training data\nrather than remaining fixed. Experimental results demonstrate that AdaResNet\nachieves a maximum accuracy improvement of over 50\\% compared to traditional\nResNet.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9J4DB-9PT4Kz8tOTiliux5Yti-X6mPZCua5vzUnqCxw","pdfSize":"586336"}
