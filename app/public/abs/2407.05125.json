{"id":"2407.05125","title":"A Joint Approach to Local Updating and Gradient Compression for\n  Efficient Asynchronous Federated Learning","authors":"Jiajun Song, Jiajun Luo, Rongwei Lu, Shuzhao Xie, Bin Chen, Zhi Wang","authorsParsed":[["Song","Jiajun",""],["Luo","Jiajun",""],["Lu","Rongwei",""],["Xie","Shuzhao",""],["Chen","Bin",""],["Wang","Zhi",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 16:19:06 GMT"}],"updateDate":"2024-07-09","timestamp":1720282746000,"abstract":"  Asynchronous Federated Learning (AFL) confronts inherent challenges arising\nfrom the heterogeneity of devices (e.g., their computation capacities) and\nlow-bandwidth environments, both potentially causing stale model updates (e.g.,\nlocal gradients) for global aggregation. Traditional approaches mitigating the\nstaleness of updates typically focus on either adjusting the local updating or\ngradient compression, but not both. Recognizing this gap, we introduce a novel\napproach that synergizes local updating with gradient compression. Our research\nbegins by examining the interplay between local updating frequency and gradient\ncompression rate, and their collective impact on convergence speed. The\ntheoretical upper bound shows that the local updating frequency and gradient\ncompression rate of each device are jointly determined by its computing power,\ncommunication capabilities and other factors. Building on this foundation, we\npropose an AFL framework called FedLuck that adaptively optimizes both local\nupdate frequency and gradient compression rates. Experiments on image\nclassification and speech recognization show that FedLuck reduces communication\nconsumption by 56% and training time by 55% on average, achieving competitive\nperformance in heterogeneous and low-bandwidth scenarios compared to the\nbaselines.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}