{"id":"2408.01691","title":"TreeCSS: An Efficient Framework for Vertical Federated Learning","authors":"Qinbo Zhang, Xiao Yan, Yukai Ding, Quanqing Xu, Chuang Hu, Xiaokai\n  Zhou, Jiawei Jiang","authorsParsed":[["Zhang","Qinbo",""],["Yan","Xiao",""],["Ding","Yukai",""],["Xu","Quanqing",""],["Hu","Chuang",""],["Zhou","Xiaokai",""],["Jiang","Jiawei",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 07:11:57 GMT"}],"updateDate":"2024-08-06","timestamp":1722669117000,"abstract":"  Vertical federated learning (VFL) considers the case that the features of\ndata samples are partitioned over different participants. VFL consists of two\nmain steps, i.e., identify the common data samples for all participants\n(alignment) and train model using the aligned data samples (training). However,\nwhen there are many participants and data samples, both alignment and training\nbecome slow. As such, we propose TreeCSS as an efficient VFL framework that\naccelerates the two main steps. In particular, for sample alignment, we design\nan efficient multi-party private set intersection (MPSI) protocol called\nTree-MPSI, which adopts a tree-based structure and a data-volume-aware\nscheduling strategy to parallelize alignment among the participants. As model\ntraining time scales with the number of data samples, we conduct coreset\nselection (CSS) to choose some representative data samples for training. Our\nCCS method adopts a clustering-based scheme for security and generality, which\nfirst clusters the features locally on each participant and then merges the\nlocal clustering results to select representative samples. In addition, we\nweight the samples according to their distances to the centroids to reflect\ntheir importance to model training. We evaluate the effectiveness and\nefficiency of our TreeCSS framework on various datasets and models. The results\nshow that compared with vanilla VFL, TreeCSS accelerates training by up to\n2.93x and achieves comparable model accuracy.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}