{"id":"2407.16127","title":"Finetuning Generative Large Language Models with Discrimination\n  Instructions for Knowledge Graph Completion","authors":"Yang Liu and Xiaobin Tian and Zequn Sun and Wei Hu","authorsParsed":[["Liu","Yang",""],["Tian","Xiaobin",""],["Sun","Zequn",""],["Hu","Wei",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 02:25:01 GMT"}],"updateDate":"2024-07-24","timestamp":1721701501000,"abstract":"  Traditional knowledge graph (KG) completion models learn embeddings to\npredict missing facts. Recent works attempt to complete KGs in a\ntext-generation manner with large language models (LLMs). However, they need to\nground the output of LLMs to KG entities, which inevitably brings errors. In\nthis paper, we present a finetuning framework, DIFT, aiming to unleash the KG\ncompletion ability of LLMs and avoid grounding errors. Given an incomplete\nfact, DIFT employs a lightweight model to obtain candidate entities and\nfinetunes an LLM with discrimination instructions to select the correct one\nfrom the given candidates. To improve performance while reducing instruction\ndata, DIFT uses a truncated sampling method to select useful facts for\nfinetuning and injects KG embeddings into the LLM. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of our proposed framework.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"vP8QcyETmAR7k6YcmmsvLz7l0tyo11O478aGXAXIM_k","pdfSize":"841443"}
