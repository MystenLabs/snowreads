{"id":"2408.08374","title":"Evaluating Text Classification Robustness to Part-of-Speech Adversarial\n  Examples","authors":"Anahita Samadi and Allison Sullivan","authorsParsed":[["Samadi","Anahita",""],["Sullivan","Allison",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 18:33:54 GMT"}],"updateDate":"2024-08-19","timestamp":1723746834000,"abstract":"  As machine learning systems become more widely used, especially for safety\ncritical applications, there is a growing need to ensure that these systems\nbehave as intended, even in the face of adversarial examples. Adversarial\nexamples are inputs that are designed to trick the decision making process, and\nare intended to be imperceptible to humans. However, for text-based\nclassification systems, changes to the input, a string of text, are always\nperceptible. Therefore, text-based adversarial examples instead focus on trying\nto preserve semantics. Unfortunately, recent work has shown this goal is often\nnot met. To improve the quality of text-based adversarial examples, we need to\nknow what elements of the input text are worth focusing on. To address this, in\nthis paper, we explore what parts of speech have the highest impact of\ntext-based classifiers. Our experiments highlight a distinct bias in CNN\nalgorithms against certain parts of speech tokens within review datasets. This\nfinding underscores a critical vulnerability in the linguistic processing\ncapabilities of CNNs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WTbDDxYW5Ymdan4-wlepO8H5swgtC5-M3VT6TbcVt3s","pdfSize":"552966"}
