{"id":"2408.10703","title":"Large Language Models for Multimodal Deformable Image Registration","authors":"Mingrui Ma, Weijie Wang, Jie Ning, Jianfeng He, Nicu Sebe, Bruno Lepri","authorsParsed":[["Ma","Mingrui",""],["Wang","Weijie",""],["Ning","Jie",""],["He","Jianfeng",""],["Sebe","Nicu",""],["Lepri","Bruno",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 09:58:30 GMT"}],"updateDate":"2024-08-21","timestamp":1724147910000,"abstract":"  The challenge of Multimodal Deformable Image Registration (MDIR) lies in the\nconversion and alignment of features between images of different modalities.\nGenerative models (GMs) cannot retain the necessary information enough from the\nsource modality to the target one, while non-GMs struggle to align features\nacross these two modalities. In this paper, we propose a novel coarse-to-fine\nMDIR framework,LLM-Morph, which is applicable to various pre-trained Large\nLanguage Models (LLMs) to solve these concerns by aligning the deep features\nfrom different modal medical images. Specifically, we first utilize a CNN\nencoder to extract deep visual features from cross-modal image pairs, then we\nuse the first adapter to adjust these tokens, and use LoRA in pre-trained LLMs\nto fine-tune their weights, both aimed at eliminating the domain gap between\nthe pre-trained LLMs and the MDIR task. Third, for the alignment of tokens, we\nutilize other four adapters to transform the LLM-encoded tokens into\nmulti-scale visual features, generating multi-scale deformation fields and\nfacilitating the coarse-to-fine MDIR task. Extensive experiments in MR-CT\nAbdomen and SR-Reg Brain datasets demonstrate the effectiveness of our\nframework and the potential of pre-trained LLMs for MDIR task. Our code is\navailabel at: https://github.com/ninjannn/LLM-Morph.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}