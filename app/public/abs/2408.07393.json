{"id":"2408.07393","title":"Segment Using Just One Example","authors":"Pratik Vora and Sudipan Saha","authorsParsed":[["Vora","Pratik",""],["Saha","Sudipan",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 09:13:06 GMT"}],"updateDate":"2024-08-15","timestamp":1723626786000,"abstract":"  Semantic segmentation is an important topic in computer vision with many\nrelevant application in Earth observation. While supervised methods exist, the\nconstraints of limited annotated data has encouraged development of\nunsupervised approaches. However, existing unsupervised methods resemble\nclustering and cannot be directly mapped to explicit target classes. In this\npaper, we deal with single shot semantic segmentation, where one example for\nthe target class is provided, which is used to segment the target class from\nquery/test images. Our approach exploits recently popular Segment Anything\n(SAM), a promptable foundation model. We specifically design several techniques\nto automatically generate prompts from the only example/key image in such a way\nthat the segmentation is successfully achieved on a stitch or concatenation of\nthe example/key and query/test images. Proposed technique does not involve any\ntraining phase and just requires one example image to grasp the concept.\nFurthermore, no text-based prompt is required for the proposed method. We\nevaluated the proposed techniques on building and car classes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}