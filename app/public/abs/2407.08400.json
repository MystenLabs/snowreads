{"id":"2407.08400","title":"Self-training Language Models for Arithmetic Reasoning","authors":"Marek Kadl\\v{c}\\'ik, Michal \\v{S}tef\\'anik","authorsParsed":[["Kadlčík","Marek",""],["Štefánik","Michal",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 11:06:05 GMT"}],"updateDate":"2024-07-12","timestamp":1720695965000,"abstract":"  Language models achieve impressive results in tasks involving complex\nmultistep reasoning, but scaling these capabilities further traditionally\nrequires expensive collection of more annotated data. In this work, we explore\nthe potential of improving the capabilities of language models without new\ndata, merely using automated feedback to the validity of their predictions in\narithmetic reasoning (self-training). We find that models can substantially\nimprove in both single-round (offline) and online self-training. In the offline\nsetting, supervised methods are able to deliver gains comparable to preference\noptimization, but in online self-training, preference optimization shows to\nlargely outperform supervised training thanks to superior stability and\nrobustness on unseen types of problems.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}