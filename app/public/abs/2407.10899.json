{"id":"2407.10899","title":"Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis","authors":"Yunting Liu, Shreya Bhandari, Zachary A. Pardos","authorsParsed":[["Liu","Yunting",""],["Bhandari","Shreya",""],["Pardos","Zachary A.",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 16:49:26 GMT"}],"updateDate":"2024-07-16","timestamp":1721062166000,"abstract":"  Effective educational measurement relies heavily on the curation of\nwell-designed item pools (i.e., possessing the right psychometric properties).\nHowever, item calibration is time-consuming and costly, requiring a sufficient\nnumber of respondents for the response process. We explore using six different\nLLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)\nand various combinations of them using sampling methods to produce responses\nwith psychometric properties similar to human answers. Results show that some\nLLMs have comparable or higher proficiency in College Algebra than college\nstudents. No single LLM mimics human respondents due to narrow proficiency\ndistributions, but an ensemble of LLMs can better resemble college students'\nability distribution. The item parameters calibrated by LLM-Respondents have\nhigh correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated\ncounterparts, and closely resemble the parameters of the human subset (e.g.\n0.02 Spearman correlation difference). Several augmentation strategies are\nevaluated for their relative performance, with resampling methods proving most\neffective, enhancing the Spearman correlation from 0.89 (human only) to 0.93\n(augmented human).\n","subjects":["Computing Research Repository/Computers and Society","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZiBFlOAK2_tuAy_C64biBjopl-H55zg3Fwl8IbX6cMw","pdfSize":"1460287"}
