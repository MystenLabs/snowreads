{"id":"2408.07388","title":"DPSNN: Spiking Neural Network for Low-Latency Streaming Speech\n  Enhancement","authors":"Tao Sun and Sander Boht\\'e","authorsParsed":[["Sun","Tao",""],["Boht√©","Sander",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 09:08:43 GMT"}],"updateDate":"2024-08-15","timestamp":1723626523000,"abstract":"  Speech enhancement (SE) improves communication in noisy environments,\naffecting areas such as automatic speech recognition, hearing aids, and\ntelecommunications. With these domains typically being power-constrained and\nevent-based while requiring low latency, neuromorphic algorithms in the form of\nspiking neural networks (SNNs) have great potential. Yet, current effective SNN\nsolutions require a contextual sampling window imposing substantial latency,\ntypically around 32ms, too long for many applications. Inspired by Dual-Path\nSpiking Neural Networks (DPSNNs) in classical neural networks, we develop a\ntwo-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural\nNetwork (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional\nNeural Networks (SCNNs) to capture global contextual information, while the\nsecond phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on\nfrequency-related features. In addition, the regularizer suppresses activation\nto further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and\nIntel DNS Datasets, we demonstrate that our approach achieves the very low\nlatency (approximately 5ms) required for applications like hearing aids, while\ndemonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and\nenergy efficiency.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}