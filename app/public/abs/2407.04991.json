{"id":"2407.04991","title":"The Solution for the AIGC Inference Performance Optimization Competition","authors":"Sishun Pan, Haonan Xu, Zhonghua Wan, Yang Yang","authorsParsed":[["Pan","Sishun",""],["Xu","Haonan",""],["Wan","Zhonghua",""],["Yang","Yang",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 07:54:45 GMT"}],"updateDate":"2024-07-09","timestamp":1720252485000,"abstract":"  In recent years, the rapid advancement of large-scale pre-trained language\nmodels based on transformer architectures has revolutionized natural language\nprocessing tasks. Among these, ChatGPT has gained widespread popularity,\ndemonstrating human-level conversational abilities and attracting over 100\nmillion monthly users by late 2022. Concurrently, Baidu's commercial deployment\nof the Ernie Wenxin model has significantly enhanced marketing effectiveness\nthrough AI-driven technologies. This paper focuses on optimizing\nhigh-performance inference for Ernie models, emphasizing GPU acceleration and\nleveraging the Paddle inference framework. We employ techniques such as Faster\nTransformer for efficient model processing, embedding layer pruning to reduce\ncomputational overhead, and FP16 half-precision inference for enhanced\ncomputational efficiency. Additionally, our approach integrates efficient data\nhandling strategies using multi-process parallel processing to minimize\nlatency. Experimental results demonstrate that our optimized solution achieves\nup to an 8.96x improvement in inference speed compared to standard methods,\nwhile maintaining competitive performance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}