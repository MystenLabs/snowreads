{"id":"2408.13019","title":"VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints","authors":"Jinghua Tang, Liyun Zhang, Yu Lu, Dian Ding, Lanqing Yang, YiChao\n  Chen, Minjie Bian, Xiaoshan Li, Guangtao Xue","authorsParsed":[["Tang","Jinghua",""],["Zhang","Liyun",""],["Lu","Yu",""],["Ding","Dian",""],["Yang","Lanqing",""],["Chen","YiChao",""],["Bian","Minjie",""],["Li","Xiaoshan",""],["Xue","Guangtao",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 12:14:18 GMT"}],"updateDate":"2024-08-26","timestamp":1724415258000,"abstract":"  Emotion recognition can enhance humanized machine responses to user commands,\nwhile voiceprint-based perception systems can be easily integrated into\ncommonly used devices like smartphones and stereos. Despite having the largest\nnumber of speakers, there is a noticeable absence of high-quality corpus\ndatasets for emotion recognition using Chinese voiceprints. Hence, this paper\nintroduces the VCEMO dataset to address this deficiency. The proposed dataset\nis constructed from everyday conversations and comprises over 100 users and\n7,747 textual samples. Furthermore, this paper proposes a multimodal-based\nmodel as a benchmark, which effectively fuses speech, text, and external\nknowledge using a co-attention structure. The system employs contrastive\nlearning-based regulation for the uneven distribution of the dataset and the\ndiversity of emotional expressions. The experiments demonstrate the significant\nimprovement of the proposed model over SOTA on the VCEMO and IEMOCAP datasets.\nCode and dataset will be released for research.\n","subjects":["Computing Research Repository/Multimedia","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}