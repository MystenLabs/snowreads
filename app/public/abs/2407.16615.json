{"id":"2407.16615","title":"Lawma: The Power of Specialization for Legal Tasks","authors":"Ricardo Dominguez-Olmedo, Vedant Nanda, Rediet Abebe, Stefan Bechtold,\n  Christoph Engel, Jens Frankenreiter, Krishna Gummadi, Moritz Hardt, Michael\n  Livermore","authorsParsed":[["Dominguez-Olmedo","Ricardo",""],["Nanda","Vedant",""],["Abebe","Rediet",""],["Bechtold","Stefan",""],["Engel","Christoph",""],["Frankenreiter","Jens",""],["Gummadi","Krishna",""],["Hardt","Moritz",""],["Livermore","Michael",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 16:23:04 GMT"}],"updateDate":"2024-07-24","timestamp":1721751784000,"abstract":"  Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal tasks remains limited. We conduct a comprehensive study of 260\nlegal text classification tasks, nearly all new to the machine learning\ncommunity. Starting from GPT-4 as a baseline, we show that it has non-trivial\nbut highly varied zero-shot accuracy, often exhibiting performance that may be\ninsufficient for legal work. We then demonstrate that a lightly fine-tuned\nLlama 3 model vastly outperforms GPT-4 on almost all tasks, typically by\ndouble-digit percentage points. We find that larger models respond better to\nfine-tuning than smaller models. A few tens to hundreds of examples suffice to\nachieve high classification accuracy. Notably, we can fine-tune a single model\non all 260 tasks simultaneously at a small loss in accuracy relative to having\na separate model for each task. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal tasks\nwith some available labeled data, researchers are better off using a fine-tuned\nopen-source model.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"xRCtaoL6r_L78PrzHewNwnv7Jl2Vo7pMVKPFa5Pp-Rc","pdfSize":"599024"}
