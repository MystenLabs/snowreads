{"id":"2407.00088","title":"T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on\n  Edge","authors":"Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong Zhang\n  and Mao Yang","authorsParsed":[["Wei","Jianyu",""],["Cao","Shijie",""],["Cao","Ting",""],["Ma","Lingxiao",""],["Wang","Lei",""],["Zhang","Yanyong",""],["Yang","Mao",""]],"versions":[{"version":"v1","created":"Tue, 25 Jun 2024 08:38:38 GMT"}],"updateDate":"2024-07-02","timestamp":1719304718000,"abstract":"  The deployment of Large Language Models (LLMs) on edge devices is\nincreasingly important to enhance on-device intelligence. Weight quantization\nis crucial for reducing the memory footprint of LLMs on devices. However,\nlow-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low\nprecision weights and high precision activations during inference. Existing\nsystems, lacking native support for mpGEMM, resort to dequantize weights for\nhigh precision computation. Such an indirect way can lead to a significant\ninference overhead.\n  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based\nmethod designed for efficient low-bit LLM (i.e., weight-quantized LLM)\ninference on CPUs. T-MAC directly supports mpGEMM without dequantization, while\nsimultaneously eliminating multiplications and reducing additions required.\nSpecifically, T-MAC transforms the traditional data-type-centric multiplication\nto bit-wise table lookup, and enables a unified and scalable mpGEMM solution.\n  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on\nlow-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in\nthroughput and 70% reduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s\nwith a single core and 71 tokens/s with eight cores on M2-Ultra, and 11\ntokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds\nthe adult average reading speed. T-MAC with LUT-based computing paradigm, paves\nthe way for the practical deployment of low-bit LLMs on resource-constrained\nedge devices without compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}