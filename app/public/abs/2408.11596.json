{"id":"2408.11596","title":"Calibrating the Predictions for Top-N Recommendations","authors":"Masahiro Sato","authorsParsed":[["Sato","Masahiro",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 13:06:28 GMT"}],"updateDate":"2024-08-22","timestamp":1724245588000,"abstract":"  Well-calibrated predictions of user preferences are essential for many\napplications. Since recommender systems typically select the top-N items for\nusers, calibration for those top-N items, rather than for all items, is\nimportant. We show that previous calibration methods result in miscalibrated\npredictions for the top-N items, despite their excellent calibration\nperformance when evaluated on all items. In this work, we address the\nmiscalibration in the top-N recommended items. We first define evaluation\nmetrics for this objective and then propose a generic method to optimize\ncalibration models focusing on the top-N items. It groups the top-N items by\ntheir ranks and optimizes distinct calibration models for each group with\nrank-dependent training weights. We verify the effectiveness of the proposed\nmethod for both explicit and implicit feedback datasets, using diverse classes\nof recommender models.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}