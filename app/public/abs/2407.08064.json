{"id":"2407.08064","title":"TinyGraph: Joint Feature and Node Condensation for Graph Neural Networks","authors":"Yezi Liu, Yanning Shen","authorsParsed":[["Liu","Yezi",""],["Shen","Yanning",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 21:54:12 GMT"}],"updateDate":"2024-07-12","timestamp":1720648452000,"abstract":"  Training graph neural networks (GNNs) on large-scale graphs can be\nchallenging due to the high computational expense caused by the massive number\nof nodes and high-dimensional nodal features. Existing graph condensation\nstudies tackle this problem only by reducing the number of nodes in the graph.\nHowever, the resulting condensed graph data can still be cumbersome.\nSpecifically, although the nodes of the Citeseer dataset are reduced to 0.9%\n(30 nodes) in training, the number of features is 3,703, severely exceeding the\ntraining sample magnitude. Faced with this challenge, we study the problem of\njoint condensation for both features and nodes in large-scale graphs. This task\nis challenging mainly due to 1) the intertwined nature of the node features and\nthe graph structure calls for the feature condensation solver to be\nstructure-aware; and 2) the difficulty of keeping useful information in the\ncondensed graph. To address these challenges, we propose a novel framework\nTinyGraph, to condense features and nodes simultaneously in graphs.\nSpecifically, we cast the problem as matching the gradients of GNN weights\ntrained on the condensed graph and the gradients obtained from training over\nthe original graph, where the feature condensation is achieved by a trainable\nfunction. The condensed graph obtained by minimizing the matching loss along\nthe training trajectory can henceforth retain critical information in the\noriginal graph. Extensive experiments were carried out to demonstrate the\neffectiveness of the proposed TinyGraph. For example, a GNN trained with\nTinyGraph retains 98.5% and 97.5% of the original test accuracy on the Cora and\nCiteseer datasets, respectively, while significantly reducing the number of\nnodes by 97.4% and 98.2%, and the number of features by 90.0% on both datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}