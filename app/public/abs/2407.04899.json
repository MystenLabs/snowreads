{"id":"2407.04899","title":"Algorithmic Language Models with Neurally Compiled Libraries","authors":"Lucas Saldyt, Subbarao Kambhampati","authorsParsed":[["Saldyt","Lucas",""],["Kambhampati","Subbarao",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 00:27:05 GMT"}],"updateDate":"2024-07-09","timestamp":1720225625000,"abstract":"  Important tasks such as reasoning and planning are fundamentally algorithmic,\nmeaning that solving them robustly requires acquiring true reasoning or\nplanning algorithms, rather than shortcuts. Large Language Models lack true\nalgorithmic ability primarily because of the limitations of neural network\noptimization algorithms, their optimization data and optimization objective,\nbut also due to architectural inexpressivity. To solve this, our paper proposes\naugmenting LLMs with a library of fundamental operations and sophisticated\ndifferentiable programs, so that common algorithms do not need to be learned\nfrom scratch. We add memory, registers, basic operations, and adaptive\nrecurrence to a transformer architecture built on LLaMA3. Then, we define a\nmethod for directly compiling algorithms into a differentiable starting\nlibrary, which is used natively and propagates gradients for optimization. In\nthis preliminary study, we explore the feasability of augmenting LLaMA3 with a\ndifferentiable computer, for instance by fine-tuning small transformers on\nsimple algorithmic tasks with variable computational depth.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Programming Languages"],"license":"http://creativecommons.org/licenses/by/4.0/"}