{"id":"2407.17915","title":"The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models","authors":"Zihui Wu, Haichang Gao, Jianping He, Ping Wang","authorsParsed":[["Wu","Zihui",""],["Gao","Haichang",""],["He","Jianping",""],["Wang","Ping",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 10:09:21 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 09:45:34 GMT"},{"version":"v3","created":"Thu, 29 Aug 2024 11:58:46 GMT"}],"updateDate":"2024-08-30","timestamp":1721902161000,"abstract":"  Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}