{"id":"2408.15221","title":"LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet","authors":"Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside,\n  Hugh Zhang, Zifan Wang, Cristina Menghini, Summer Yue","authorsParsed":[["Li","Nathaniel",""],["Han","Ziwen",""],["Steneker","Ian",""],["Primack","Willow",""],["Goodside","Riley",""],["Zhang","Hugh",""],["Wang","Zifan",""],["Menghini","Cristina",""],["Yue","Summer",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 17:33:30 GMT"},{"version":"v2","created":"Wed, 4 Sep 2024 00:58:59 GMT"}],"updateDate":"2024-09-05","timestamp":1724780010000,"abstract":"  Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/"}