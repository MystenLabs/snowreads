{"id":"2407.00531","title":"Interpreting Pretrained Speech Models for Automatic Speech Assessment of\n  Voice Disorders","authors":"Hok-Shing Lau, Mark Huntly, Nathon Morgan, Adesua Iyenoma, Biao Zeng,\n  Tim Bashford","authorsParsed":[["Lau","Hok-Shing",""],["Huntly","Mark",""],["Morgan","Nathon",""],["Iyenoma","Adesua",""],["Zeng","Biao",""],["Bashford","Tim",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 21:14:48 GMT"}],"updateDate":"2024-07-02","timestamp":1719695688000,"abstract":"  Speech contains information that is clinically relevant to some diseases,\nwhich has the potential to be used for health assessment. Recent work shows an\ninterest in applying deep learning algorithms, especially pretrained large\nspeech models to the applications of Automatic Speech Assessment. One question\nthat has not been explored is how these models output the results based on\ntheir inputs. In this work, we train and compare two configurations of Audio\nSpectrogram Transformer in the context of Voice Disorder Detection and apply\nthe attention rollout method to produce model relevance maps, the computed\nrelevance of the spectrogram regions when the model makes predictions. We use\nthese maps to analyse how models make predictions in different conditions and\nto show that the spread of attention is reduced as a model is finetuned, and\nthe model attention is concentrated on specific phoneme regions.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gRJQKsx3oHuAts7zw_KeNDosWfK0LtURmqNqlu1Ih5U","pdfSize":"1066602"}
