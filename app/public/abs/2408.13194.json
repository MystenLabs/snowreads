{"id":"2408.13194","title":"IFH: a Diffusion Framework for Flexible Design of Graph Generative\n  Models","authors":"Samuel Cognolato, Alessandro Sperduti, Luciano Serafini","authorsParsed":[["Cognolato","Samuel",""],["Sperduti","Alessandro",""],["Serafini","Luciano",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 16:24:40 GMT"}],"updateDate":"2024-08-26","timestamp":1724430280000,"abstract":"  Graph generative models can be classified into two prominent families:\none-shot models, which generate a graph in one go, and sequential models, which\ngenerate a graph by successive additions of nodes and edges. Ideally, between\nthese two extreme models lies a continuous range of models that adopt different\nlevels of sequentiality. This paper proposes a graph generative model, called\nInsert-Fill-Halt (IFH), that supports the specification of a sequentiality\ndegree. IFH is based upon the theory of Denoising Diffusion Probabilistic\nModels (DDPM), designing a node removal process that gradually destroys a\ngraph. An insertion process learns to reverse this removal process by inserting\narcs and nodes according to the specified sequentiality degree. We evaluate the\nperformance of IFH in terms of quality, run time, and memory, depending on\ndifferent sequentiality degrees. We also show that using DiGress, a\ndiffusion-based one-shot model, as a generative step in IFH leads to\nimprovement to the model itself, and is competitive with the current\nstate-of-the-art.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"ghWAhchVowN0-rKxJdRMArKJSnG2Zhzi6vLEsucRwNU","pdfSize":"609841"}
