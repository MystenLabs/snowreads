{"id":"2408.06858","title":"SaSLaW: Dialogue Speech Corpus with Audio-visual Egocentric Information\n  Toward Environment-adaptive Dialogue Speech Synthesis","authors":"Osamu Take, Shinnosuke Takamichi, Kentaro Seki, Yoshiaki Bando,\n  Hiroshi Saruwatari","authorsParsed":[["Take","Osamu",""],["Takamichi","Shinnosuke",""],["Seki","Kentaro",""],["Bando","Yoshiaki",""],["Saruwatari","Hiroshi",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 12:38:56 GMT"}],"updateDate":"2024-08-14","timestamp":1723552736000,"abstract":"  This paper presents SaSLaW, a spontaneous dialogue speech corpus containing\nsynchronous recordings of what speakers speak, listen to, and watch. Humans\nconsider the diverse environmental factors and then control the features of\ntheir utterances in face-to-face voice communications. Spoken dialogue systems\ncapable of this adaptation to these audio environments enable natural and\nseamless communications. SaSLaW was developed to model human-speech adjustment\nfor audio environments via first-person audio-visual perceptions in spontaneous\ndialogues. We propose the construction methodology of SaSLaW and display the\nanalysis result of the corpus. We additionally conducted an experiment to\ndevelop text-to-speech models using SaSLaW and evaluate their performance of\nadaptations to audio environments. The results indicate that models\nincorporating hearing-audio data output more plausible speech tailored to\ndiverse audio environments than the vanilla text-to-speech model.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"2tq_JZGAiR-uU4I23Ry2KBssui037X9xqpypXKUbg34","pdfSize":"4170689"}
