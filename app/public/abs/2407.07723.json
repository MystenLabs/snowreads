{"id":"2407.07723","title":"Understanding is Compression","authors":"Ziguang Li, Chao Huang, Xuliang Wang, Haibo Hu, Cole Wyeth, Dongbo Bu,\n  Quan Yu, Wen Gao, Xingwu Liu, Ming Li","authorsParsed":[["Li","Ziguang",""],["Huang","Chao",""],["Wang","Xuliang",""],["Hu","Haibo",""],["Wyeth","Cole",""],["Bu","Dongbo",""],["Yu","Quan",""],["Gao","Wen",""],["Liu","Xingwu",""],["Li","Ming",""]],"versions":[{"version":"v1","created":"Mon, 24 Jun 2024 03:58:11 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 02:45:36 GMT"}],"updateDate":"2024-08-22","timestamp":1719201491000,"abstract":"  Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression.\n  We have previously shown all understanding or learning are compression, under\nreasonable assumptions. Large language models (LLMs) understand data better\nthan ever before. Can they help us to compress data?\n  The LLMs may be seen to approximate the uncomputable Solomonoff induction.\nTherefore, under this new uncomputable paradigm, we present LMCompress.\nLMCompress shatters all previous lossless compression algorithms, doubling the\nlossless compression ratios of JPEG-XL for images, FLAC for audios, and H.264\nfor videos, and quadrupling the compression ratio of bz2 for texts. The better\na large model understands the data, the better LMCompress compresses.\n","subjects":["Computing Research Repository/Information Theory","Computing Research Repository/Artificial Intelligence","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"GPjiG3tsSAsyobK0cNDTC07fAv6rtad68BYVEeg52OY","pdfSize":"519224","objectId":"0x04cfdf747884e75af37ec456d59ff05df93862c829dfbcfce02a793e5252af47","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
