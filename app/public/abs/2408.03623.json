{"id":"2408.03623","title":"Improving Retrieval-Augmented Code Comment Generation by Retrieving for\n  Generation","authors":"Hanzhen Lu, Zhongxin Liu","authorsParsed":[["Lu","Hanzhen",""],["Liu","Zhongxin",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 08:32:55 GMT"}],"updateDate":"2024-08-08","timestamp":1723019575000,"abstract":"  Code comment generation aims to generate high-quality comments from source\ncode automatically and has been studied for years. Recent studies proposed to\nintegrate information retrieval techniques with neural generation models to\ntackle this problem, i.e., Retrieval-Augmented Comment Generation (RACG)\napproaches, and achieved state-of-the-art results. However, the retrievers in\nprevious work are built independently of their generators. This results in that\nthe retrieved exemplars are not necessarily the most useful ones for generating\ncomments, limiting the performance of existing approaches. To address this\nlimitation, we propose a novel training strategy to enable the retriever to\nlearn from the feedback of the generator and retrieve exemplars for generation.\nSpecifically, during training, we use the retriever to retrieve the top-k\nexemplars and calculate their retrieval scores, and use the generator to\ncalculate a generation loss for the sample based on each exemplar. By aligning\nhigh-score exemplars retrieved by the retriever with low-loss exemplars\nobserved by the generator, the retriever can learn to retrieve exemplars that\ncan best improve the quality of the generated comments. Based on this strategy,\nwe propose a novel RACG approach named JOINTCOM and evaluate it on two\nreal-world datasets, JCSD and PCSD. The experimental results demonstrate that\nour approach surpasses the state-of-the-art baselines by 7.3% to 30.0% in terms\nof five metrics on the two datasets. We also conduct a human evaluation to\ncompare JOINTCOM with the best-performing baselines. The results indicate that\nJOINTCOM outperforms the baselines, producing comments that are more natural,\ninformative, and useful.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}