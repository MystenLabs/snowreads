{"id":"2408.16373","title":"Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis","authors":"Zehai Tu, Guangyan Zhang, Yiting Lu, Adaeze Adigwe, Simon King, Yiwen\n  Guo","authorsParsed":[["Tu","Zehai",""],["Zhang","Guangyan",""],["Lu","Yiting",""],["Adigwe","Adaeze",""],["King","Simon",""],["Guo","Yiwen",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 09:31:06 GMT"}],"updateDate":"2024-08-30","timestamp":1724923866000,"abstract":"  Tokenising continuous speech into sequences of discrete tokens and modelling\nthem with language models (LMs) has led to significant success in\ntext-to-speech (TTS) synthesis. Although these models can generate speech with\nhigh quality and naturalness, their synthesised samples can still suffer from\nartefacts, mispronunciation, word repeating, etc. In this paper, we argue these\nundesirable properties could partly be caused by the randomness of\nsampling-based strategies during the autoregressive decoding of LMs. Therefore,\nwe look at maximisation-based decoding approaches and propose Temporal\nRepetition Aware Diverse Beam Search (TRAD-BS) to find the most probable\nsequences of the generated speech tokens. Experiments with two state-of-the-art\nLM-based TTS models demonstrate that our proposed maximisation-based decoding\nstrategy generates speech with fewer mispronunciations and improved speaker\nconsistency.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GiOZqOJ0jrzKkGADd2ulsiNgzh6Ml6W4GyvR2Ya8fnY","pdfSize":"687372"}
