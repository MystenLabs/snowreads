{"id":"2407.03651","title":"Evaluating Language Model Context Windows: A \"Working Memory\" Test and\n  Inference-time Correction","authors":"Amanda Dsouza, Christopher Glaze, Changho Shin, Frederic Sala","authorsParsed":[["Dsouza","Amanda",""],["Glaze","Christopher",""],["Shin","Changho",""],["Sala","Frederic",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 05:46:20 GMT"},{"version":"v2","created":"Sun, 14 Jul 2024 22:47:13 GMT"}],"updateDate":"2024-07-16","timestamp":1720071980000,"abstract":"  Large language models are prominently used in real-world applications, often\ntasked with reasoning over large volumes of documents. An exciting development\nin this space is models boasting extended context capabilities, with some\naccommodating over 2 million tokens. Such long context model capabilities\nremain uncertain in production systems, motivating the need to benchmark their\nperformance on real world use cases. We address this challenge by proposing\nSWiM, an evaluation framework that addresses the limitations of standard tests.\nTesting the framework on eight long context models, we find that even strong\nmodels such as GPT-4 and Claude 3 Opus degrade in performance when information\nis present in the middle of the context window (lost-in-the-middle effect).\nNext, in addition to our benchmark, we propose medoid voting, a simple, but\neffective training-free approach that helps alleviate this effect, by\ngenerating responses a few times, each time randomly permuting documents in the\ncontext, and selecting the medoid answer. We evaluate medoid voting on single\ndocument QA tasks, achieving up to a 24% lift in accuracy. Our code is\navailable at https://github.com/snorkel-ai/long-context-eval.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}