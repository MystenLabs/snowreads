{"id":"2407.20535","title":"DeepSpeech models show Human-like Performance and Processing of Cochlear\n  Implant Inputs","authors":"Cynthia R. Steinhardt, Menoua Keshishian, Nima Mesgarani, Kim\n  Stachenfeld","authorsParsed":[["Steinhardt","Cynthia R.",""],["Keshishian","Menoua",""],["Mesgarani","Nima",""],["Stachenfeld","Kim",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 04:32:27 GMT"}],"updateDate":"2024-07-31","timestamp":1722313947000,"abstract":"  Cochlear implants(CIs) are arguably the most successful neural implant,\nhaving restored hearing to over one million people worldwide. While CI research\nhas focused on modeling the cochlear activations in response to low-level\nacoustic features, we hypothesize that the success of these implants is due in\nlarge part to the role of the upstream network in extracting useful features\nfrom a degraded signal and learned statistics of language to resolve the\nsignal. In this work, we use the deep neural network (DNN) DeepSpeech2, as a\nparadigm to investigate how natural input and cochlear implant-based inputs are\nprocessed over time. We generate naturalistic and cochlear implant-like inputs\nfrom spoken sentences and test the similarity of model performance to human\nperformance on analogous phoneme recognition tests. Our model reproduces error\npatterns in reaction time and phoneme confusion patterns under noise conditions\nin normal hearing and CI participant studies. We then use interpretability\ntechniques to determine where and when confusions arise when processing\nnaturalistic and CI-like inputs. We find that dynamics over time in each layer\nare affected by context as well as input type. Dynamics of all phonemes diverge\nduring confusion and comprehension within the same time window, which is\ntemporally shifted backward in each layer of the network. There is a modulation\nof this signal during processing of CI which resembles changes in human EEG\nsignals in the auditory stream. This reduction likely relates to the reduction\nof encoded phoneme identity. These findings suggest that we have a viable model\nin which to explore the loss of speech-related information in time and that we\ncan use it to find population-level encoding signals to target when optimizing\ncochlear implant inputs to improve encoding of essential speech-related\ninformation and improve perception.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}