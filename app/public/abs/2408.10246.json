{"id":"2408.10246","title":"VyAnG-Net: A Novel Multi-Modal Sarcasm Recognition Model by Uncovering\n  Visual, Acoustic and Glossary Features","authors":"Ananya Pandey, Dinesh Kumar Vishwakarma","authorsParsed":[["Pandey","Ananya",""],["Vishwakarma","Dinesh Kumar",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 15:36:52 GMT"}],"updateDate":"2024-08-21","timestamp":1722872212000,"abstract":"  Various linguistic and non-linguistic clues, such as excessive emphasis on a\nword, a shift in the tone of voice, or an awkward expression, frequently convey\nsarcasm. The computer vision problem of sarcasm recognition in conversation\naims to identify hidden sarcastic, criticizing, and metaphorical information\nembedded in everyday dialogue. Prior, sarcasm recognition has focused mainly on\ntext. Still, it is critical to consider all textual information, audio stream,\nfacial expression, and body position for reliable sarcasm identification.\nHence, we propose a novel approach that combines a lightweight depth attention\nmodule with a self-regulated ConvNet to concentrate on the most crucial\nfeatures of visual data and an attentional tokenizer based strategy to extract\nthe most critical context-specific information from the textual data. The\nfollowing is a list of the key contributions that our experimentation has made\nin response to performing the task of Multi-modal Sarcasm Recognition: an\nattentional tokenizer branch to get beneficial features from the glossary\ncontent provided by the subtitles; a visual branch for acquiring the most\nprominent features from the video frames; an utterance-level feature extraction\nfrom acoustic content and a multi-headed attention based feature fusion branch\nto blend features obtained from multiple modalities. Extensive testing on one\nof the benchmark video datasets, MUSTaRD, yielded an accuracy of 79.86% for\nspeaker dependent and 76.94% for speaker independent configuration\ndemonstrating that our approach is superior to the existing methods. We have\nalso conducted a cross-dataset analysis to test the adaptability of VyAnG-Net\nwith unseen samples of another dataset MUStARD++.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"a1g40J3KpjZt56WWrIF1oo52r_pfHnSJGrJJR7petRU","pdfSize":"1484611"}
