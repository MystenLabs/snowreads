{"id":"2408.11535","title":"SAM-REF: Rethinking Image-Prompt Synergy for Refinement in Segment\n  Anything","authors":"Chongkai Yu, Anqi Li, Xiaochao Qu, Luoqi Liu, Ting Liu","authorsParsed":[["Yu","Chongkai",""],["Li","Anqi",""],["Qu","Xiaochao",""],["Liu","Luoqi",""],["Liu","Ting",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 11:18:35 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 08:25:39 GMT"}],"updateDate":"2024-08-23","timestamp":1724239115000,"abstract":"  The advent of the Segment Anything Model (SAM) marks a significant milestone\nfor interactive segmentation using generalist models. As a late fusion model,\nSAM extracts image embeddings once and merges them with prompts in later\ninteractions. This strategy limits the models ability to extract detailed\ninformation from the prompted target zone. Current specialist models utilize\nthe early fusion strategy that encodes the combination of images and prompts to\ntarget the prompted objects, yet repetitive complex computations on the images\nresult in high latency. The key to these issues is efficiently synergizing the\nimages and prompts. We propose SAM-REF, a two-stage refinement framework that\nfully integrates images and prompts globally and locally while maintaining the\naccuracy of early fusion and the efficiency of late fusion. The first-stage\nGlobalDiff Refiner is a lightweight early fusion network that combines the\nwhole image and prompts, focusing on capturing detailed information for the\nentire object. The second-stage PatchDiff Refiner locates the object detail\nwindow according to the mask and prompts, then refines the local details of the\nobject. Experimentally, we demonstrated the high effectiveness and efficiency\nof our method in tackling complex cases with multiple interactions. Our SAM-REF\nmodel outperforms the current state-of-the-art method in most metrics on\nsegmentation quality without compromising efficiency.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}