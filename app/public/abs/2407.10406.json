{"id":"2407.10406","title":"Towards Scale-Aware Full Surround Monodepth with Transformers","authors":"Yuchen Yang, Xinyi Wang, Dong Li, Lu Tian, Ashish Sirasao, Xun Yang","authorsParsed":[["Yang","Yuchen",""],["Wang","Xinyi",""],["Li","Dong",""],["Tian","Lu",""],["Sirasao","Ashish",""],["Yang","Xun",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 02:54:46 GMT"}],"updateDate":"2024-07-16","timestamp":1721012086000,"abstract":"  Full surround monodepth (FSM) methods can learn from multiple camera views\nsimultaneously in a self-supervised manner to predict the scale-aware depth,\nwhich is more practical for real-world applications in contrast to\nscale-ambiguous depth from a standalone monocular camera. In this work, we\nfocus on enhancing the scale-awareness of FSM methods for depth estimation. To\nthis end, we propose to improve FSM from two perspectives: depth network\nstructure optimization and training pipeline optimization. First, we construct\na transformer-based depth network with neighbor-enhanced cross-view attention\n(NCA). The cross-attention modules can better aggregate the cross-view context\nin both global and neighboring views. Second, we formulate a transformer-based\nfeature matching scheme with progressive training to improve the\nstructure-from-motion (SfM) pipeline. That allows us to learn scale-awareness\nwith sufficient matches and further facilitate network convergence by removing\nmismatches based on SfM loss. Experiments demonstrate that the resulting\nScale-aware full surround monodepth (SA-FSM) method largely improves the\nscale-aware depth predictions without median-scaling at the test time, and\nperforms favorably against the state-of-the-art FSM methods, e.g., surpassing\nSurroundDepth by 3.8% in terms of accuracy at delta<1.25 on the DDAD benchmark.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}