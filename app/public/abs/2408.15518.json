{"id":"2408.15518","title":"Squid: Long Context as a New Modality for Energy-Efficient On-Device\n  Language Models","authors":"Wei Chen, Zhiyuan Li, Shuo Xin, Yihao Wang","authorsParsed":[["Chen","Wei",""],["Li","Zhiyuan",""],["Xin","Shuo",""],["Wang","Yihao",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 04:06:14 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 04:38:16 GMT"}],"updateDate":"2024-09-04","timestamp":1724817974000,"abstract":"  This paper presents Dolphin, a novel decoder-decoder architecture for\nenergy-efficient processing of long contexts in language models. Our approach\naddresses the significant energy consumption and latency challenges inherent in\non-device models. Dolphin employs a compact 0.5B parameter decoder to distill\nextensive contextual information into a memory embedding, substantially\nreducing the input length for the primary 7B parameter decoder model. Inspired\nby vision-language models, we repurpose the image embedding projector to encode\nlong textual contexts, effectively treating extended context as a distinct\nmodality. This innovative method enables processing of substantially longer\ncontexts without the typical computational overhead associated with extended\ninput sequences. Empirical evaluations demonstrate a 10-fold improvement in\nenergy efficiency and a 5-fold reduction in latency compared to conventional\nfull-length context processing methods without losing quality of the response.\nOur work contributes to the development of more sustainable and scalable\nlanguage models for on-device applications, addressing the critical need for\nenergy-efficient and responsive AI technologies in resource-constrained\nenvironments while maintaining the accuracy to understand long contexts. This\nresearch has implications for the broader field of natural language processing,\nparticularly in the domain of efficient model design for resource-limited\nsettings. By enabling more sophisticated AI capabilities on edge devices,\nDolphin paves the way for advanced language processing in a wide range of\napplications where computational resources are at a premium. The Dolphin model\nis publicly available at https://huggingface.co/NexaAIDev/Dolphin.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"DqHmr5ZDTYKYWRNH5z9UGLrH_RuOyhResTKcAOBplO8","pdfSize":"356301"}
