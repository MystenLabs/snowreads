{"id":"2408.01168","title":"Misinforming LLMs: vulnerabilities, challenges and opportunities","authors":"Bo Zhou, Daniel Gei{\\ss}ler, Paul Lukowicz","authorsParsed":[["Zhou","Bo",""],["Gei√üler","Daniel",""],["Lukowicz","Paul",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 10:35:49 GMT"}],"updateDate":"2024-08-05","timestamp":1722594949000,"abstract":"  Large Language Models (LLMs) have made significant advances in natural\nlanguage processing, but their underlying mechanisms are often misunderstood.\nDespite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely\non statistical patterns in word embeddings rather than true cognitive\nprocesses. This leads to vulnerabilities such as \"hallucination\" and\nmisinformation. The paper argues that current LLM architectures are inherently\nuntrustworthy due to their reliance on correlations of sequential patterns of\nword embedding vectors. However, ongoing research into combining generative\ntransformer-based models with fact bases and logic programming languages may\nlead to the development of trustworthy LLMs capable of generating statements\nbased on given truth and explaining their self-reasoning process.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8fLFibwl8hiO8kN08v_0A9T6OvCTRAuVOojekzqwEDw","pdfSize":"498893","txDigest":"BKrwMpFnVtSevMG5Ke2Mndb7UinNAyRhMVkYiiWsTcwN","endEpoch":"1","status":"CERTIFIED"}
