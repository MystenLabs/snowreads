{"id":"2408.16984","title":"Beyond Preferences in AI Alignment","authors":"Tan Zhi-Xuan, Micah Carroll, Matija Franklin, Hal Ashton","authorsParsed":[["Zhi-Xuan","Tan",""],["Carroll","Micah",""],["Franklin","Matija",""],["Ashton","Hal",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 03:14:20 GMT"}],"updateDate":"2024-09-02","timestamp":1724987660000,"abstract":"  The dominant practice of AI alignment assumes (1) that preferences are an\nadequate representation of human values, (2) that human rationality can be\nunderstood in terms of maximizing the satisfaction of preferences, and (3) that\nAI systems should be aligned with the preferences of one or more humans to\nensure that they behave safely and in accordance with our values. Whether\nimplicitly followed or explicitly endorsed, these commitments constitute what\nwe term a preferentist approach to AI alignment. In this paper, we characterize\nand challenge the preferentist approach, describing conceptual and technical\nalternatives that are ripe for further research. We first survey the limits of\nrational choice theory as a descriptive model, explaining how preferences fail\nto capture the thick semantic content of human values, and how utility\nrepresentations neglect the possible incommensurability of those values. We\nthen critique the normativity of expected utility theory (EUT) for humans and\nAI, drawing upon arguments showing how rational agents need not comply with\nEUT, while highlighting how EUT is silent on which preferences are normatively\nacceptable. Finally, we argue that these limitations motivate a reframing of\nthe targets of AI alignment: Instead of alignment with the preferences of a\nhuman user, developer, or humanity-writ-large, AI systems should be aligned\nwith normative standards appropriate to their social roles, such as the role of\na general-purpose assistant. Furthermore, these standards should be negotiated\nand agreed upon by all relevant stakeholders. On this alternative conception of\nalignment, a multiplicity of AI systems will be able to serve diverse ends,\naligned with normative standards that promote mutual benefit and limit harm\ndespite our plural and divergent values.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}