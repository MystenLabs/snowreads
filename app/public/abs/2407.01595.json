{"id":"2407.01595","title":"Fairpriori: Improving Biased Subgroup Discovery for Deep Neural Network\n  Fairness","authors":"Kacy Zhou, Jiawen Wen, Nan Yang, Dong Yuan, Qinghua Lu, Huaming Chen","authorsParsed":[["Zhou","Kacy",""],["Wen","Jiawen",""],["Yang","Nan",""],["Yuan","Dong",""],["Lu","Qinghua",""],["Chen","Huaming",""]],"versions":[{"version":"v1","created":"Tue, 25 Jun 2024 00:15:13 GMT"}],"updateDate":"2024-07-03","timestamp":1719274513000,"abstract":"  While deep learning has become a core functional module of most software\nsystems, concerns regarding the fairness of ML predictions have emerged as a\nsignificant issue that affects prediction results due to discrimination.\nIntersectional bias, which disproportionately affects members of subgroups, is\na prime example of this. For instance, a machine learning model might exhibit\nbias against darker-skinned women, while not showing bias against individuals\nwith darker skin or women. This problem calls for effective fairness testing\nbefore the deployment of such deep learning models in real-world scenarios.\nHowever, research into detecting such bias is currently limited compared to\nresearch on individual and group fairness. Existing tools to investigate\nintersectional bias lack important features such as support for multiple\nfairness metrics, fast and efficient computation, and user-friendly\ninterpretation. This paper introduces Fairpriori, a novel biased subgroup\ndiscovery method, which aims to address these limitations. Fairpriori\nincorporates the frequent itemset generation algorithm to facilitate effective\nand efficient investigation of intersectional bias by producing fast fairness\nmetric calculations on subgroups of a dataset. Through comparison with the\nstate-of-the-art methods (e.g., Themis, FairFictPlay, and TestSGD) under\nsimilar conditions, Fairpriori demonstrates superior effectiveness and\nefficiency when identifying intersectional bias. Specifically, Fairpriori is\neasier to use and interpret, supports a wider range of use cases by\naccommodating multiple fairness metrics, and exhibits higher efficiency in\ncomputing fairness metrics. These findings showcase Fairpriori's potential for\neffectively uncovering subgroups affected by intersectional bias, supported by\nits open-source tooling at https://anonymous.4open.science/r/Fairpriori-0320.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computers and Society","Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/"}