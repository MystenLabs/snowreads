{"id":"2407.10474","title":"Multi-source Knowledge Enhanced Graph Attention Networks for Multimodal\n  Fact Verification","authors":"Han Cao, Lingwei Wei, Wei Zhou, Songlin Hu","authorsParsed":[["Cao","Han",""],["Wei","Lingwei",""],["Zhou","Wei",""],["Hu","Songlin",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 07:01:05 GMT"}],"updateDate":"2024-07-16","timestamp":1721026865000,"abstract":"  Multimodal fact verification is an under-explored and emerging field that has\ngained increasing attention in recent years. The goal is to assess the veracity\nof claims that involve multiple modalities by analyzing the retrieved evidence.\nThe main challenge in this area is to effectively fuse features from different\nmodalities to learn meaningful multimodal representations. To this end, we\npropose a novel model named Multi-Source Knowledge-enhanced Graph Attention\nNetwork (MultiKE-GAT). MultiKE-GAT introduces external multimodal knowledge\nfrom different sources and constructs a heterogeneous graph to capture complex\ncross-modal and cross-source interactions. We exploit a Knowledge-aware Graph\nFusion (KGF) module to learn knowledge-enhanced representations for each claim\nand evidence and eliminate inconsistencies and noises introduced by redundant\nentities. Experiments on two public benchmark datasets demonstrate that our\nmodel outperforms other comparison methods, showing the effectiveness and\nsuperiority of the proposed model.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}