{"id":"2407.11492","title":"MMSD-Net: Towards Multi-modal Stuttering Detection","authors":"Liangyu Nie, Sudarsana Reddy Kadiri, and Ruchit Agrawal","authorsParsed":[["Nie","Liangyu",""],["Kadiri","Sudarsana Reddy",""],["Agrawal","Ruchit",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 08:26:59 GMT"}],"updateDate":"2024-07-17","timestamp":1721118419000,"abstract":"  Stuttering is a common speech impediment that is caused by irregular\ndisruptions in speech production, affecting over 70 million people across the\nworld. Standard automatic speech processing tools do not take speech ailments\ninto account and are thereby not able to generate meaningful results when\npresented with stuttered speech as input. The automatic detection of stuttering\nis an integral step towards building efficient, context-aware speech processing\nsystems. While previous approaches explore both statistical and neural\napproaches for stuttering detection, all of these methods are uni-modal in\nnature. This paper presents MMSD-Net, the first multi-modal neural framework\nfor stuttering detection. Experiments and results demonstrate that\nincorporating the visual signal significantly aids stuttering detection, and\nour model yields an improvement of 2-17% in the F1-score over existing\nstate-of-the-art uni-modal approaches.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computation and Language","Computing Research Repository/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}