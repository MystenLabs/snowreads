{"id":"2407.08963","title":"Local Optima in Diversity Optimization: Non-trivial Offspring Population\n  is Essential","authors":"Denis Antipov, Aneta Neumann, Frank Neumann","authorsParsed":[["Antipov","Denis",""],["Neumann","Aneta",""],["Neumann","Frank",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 03:27:47 GMT"}],"updateDate":"2024-07-15","timestamp":1720754867000,"abstract":"  The main goal of diversity optimization is to find a diverse set of solutions\nwhich satisfy some lower bound on their fitness. Evolutionary algorithms (EAs)\nare often used for such tasks, since they are naturally designed to optimize\npopulations of solutions. This approach to diversity optimization, called EDO,\nhas been previously studied from theoretical perspective, but most studies\nconsidered only EAs with a trivial offspring population such as the $(\\mu + 1)$\nEA. In this paper we give an example instance of a $k$-vertex cover problem,\nwhich highlights a critical difference of the diversity optimization from the\nregular single-objective optimization, namely that there might be a locally\noptimal population from which we can escape only by replacing at least two\nindividuals at once, which the $(\\mu + 1)$ algorithms cannot do.\n  We also show that the $(\\mu + \\lambda)$ EA with $\\lambda \\ge \\mu$ can\neffectively find a diverse population on $k$-vertex cover, if using a mutation\noperator inspired by Branson and Sutton (TCS 2023). To avoid the problem of\nsubset selection which arises in the $(\\mu + \\lambda)$ EA when it optimizes\ndiversity, we also propose the $(1_\\mu + 1_\\mu)$ EA$_D$, which is an analogue\nof the $(1 + 1)$ EA for populations, and which is also efficient at optimizing\ndiversity on the $k$-vertex cover problem.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}