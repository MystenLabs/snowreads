{"id":"2407.12593","title":"EvSign: Sign Language Recognition and Translation with Streaming Events","authors":"Pengyu Zhang and Hao Yin and Zeren Wang and Wenyue Chen and Shengming\n  Li and Dong Wang and Huchuan Lu and Xu Jia","authorsParsed":[["Zhang","Pengyu",""],["Yin","Hao",""],["Wang","Zeren",""],["Chen","Wenyue",""],["Li","Shengming",""],["Wang","Dong",""],["Lu","Huchuan",""],["Jia","Xu",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 14:16:35 GMT"},{"version":"v2","created":"Sun, 21 Jul 2024 12:20:43 GMT"}],"updateDate":"2024-07-23","timestamp":1721225795000,"abstract":"  Sign language is one of the most effective communication tools for people\nwith hearing difficulties. Most existing works focus on improving the\nperformance of sign language tasks on RGB videos, which may suffer from\ndegraded recording conditions, such as fast movement of hands with motion blur\nand textured signer's appearance. The bio-inspired event camera, which\nasynchronously captures brightness change with high speed, could naturally\nperceive dynamic hand movements, providing rich manual clues for sign language\ntasks. In this work, we aim at exploring the potential of event camera in\ncontinuous sign language recognition (CSLR) and sign language translation\n(SLT). To promote the research, we first collect an event-based benchmark\nEvSign for those tasks with both gloss and spoken language annotations. EvSign\ndataset offers a substantial amount of high-quality event streams and an\nextensive vocabulary of glosses and words, thereby facilitating the development\nof sign language tasks. In addition, we propose an efficient transformer-based\nframework for event-based SLR and SLT tasks, which fully leverages the\nadvantages of streaming events. The sparse backbone is employed to extract\nvisual features from sparse events. Then, the temporal coherence is effectively\nutilized through the proposed local token fusion and gloss-aware temporal\naggregation modules. Extensive experimental results are reported on both\nsimulated (PHOENIX14T) and EvSign datasets. Our method performs favorably\nagainst existing state-of-the-art approaches with only 0.34% computational cost\n(0.84G FLOPS per video) and 44.2% network parameters. The project is available\nat https://zhang-pengyu.github.io/EVSign.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}