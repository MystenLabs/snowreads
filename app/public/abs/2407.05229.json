{"id":"2407.05229","title":"HiDe-PET: Continual Learning via Hierarchical Decomposition of\n  Parameter-Efficient Tuning","authors":"Liyuan Wang, Jingyi Xie, Xingxing Zhang, Hang Su, Jun Zhu","authorsParsed":[["Wang","Liyuan",""],["Xie","Jingyi",""],["Zhang","Xingxing",""],["Su","Hang",""],["Zhu","Jun",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 01:50:25 GMT"}],"updateDate":"2024-07-09","timestamp":1720317025000,"abstract":"  The deployment of pre-trained models (PTMs) has greatly advanced the field of\ncontinual learning (CL), enabling positive knowledge transfer and resilience to\ncatastrophic forgetting. To sustain these advantages for sequentially arriving\ntasks, a promising direction involves keeping the pre-trained backbone frozen\nwhile employing parameter-efficient tuning (PET) techniques to instruct\nrepresentation learning. Despite the popularity of Prompt-based PET for CL, its\nempirical design often leads to sub-optimal performance in our evaluation of\ndifferent PTMs and target tasks. To this end, we propose a unified framework\nfor CL with PTMs and PET that provides both theoretical and empirical\nadvancements. We first perform an in-depth theoretical analysis of the CL\nobjective in a pre-training context, decomposing it into hierarchical\ncomponents namely within-task prediction, task-identity inference and\ntask-adaptive prediction. We then present Hierarchical Decomposition PET\n(HiDe-PET), an innovative approach that explicitly optimizes the decomposed\nobjective through incorporating task-specific and task-shared knowledge via\nmainstream PET techniques along with efficient recovery of pre-trained\nrepresentations. Leveraging this framework, we delve into the distinct impacts\nof implementation strategy, PET technique and PET architecture, as well as\nadaptive knowledge accumulation amidst pronounced distribution changes.\nFinally, across various CL scenarios, our approach demonstrates remarkably\nsuperior performance over a broad spectrum of recent strong baselines.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}