{"id":"2408.07004","title":"Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models","authors":"Chun Jie Chong, Chenxi Hou, Zhihao Yao, Seyed Mohammadjavad Seyed\n  Talebi","authorsParsed":[["Chong","Chun Jie",""],["Hou","Chenxi",""],["Yao","Zhihao",""],["Talebi","Seyed Mohammadjavad Seyed",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 16:08:37 GMT"}],"updateDate":"2024-08-14","timestamp":1723565317000,"abstract":"  Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KzBXumOsoQg205Wa14j3O6Yfx7G-QbNIrshpDsuXjG4","pdfSize":"998357"}
