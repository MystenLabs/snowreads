{"id":"2408.01419","title":"DebateQA: Evaluating Question Answering on Debatable Knowledge","authors":"Rongwu Xu, Xuan Qi, Zehan Qi, Wei Xu, Zhijiang Guo","authorsParsed":[["Xu","Rongwu",""],["Qi","Xuan",""],["Qi","Zehan",""],["Xu","Wei",""],["Guo","Zhijiang",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 17:54:34 GMT"}],"updateDate":"2024-08-05","timestamp":1722621274000,"abstract":"  The rise of large language models (LLMs) has enabled us to seek answers to\ninherently debatable questions on LLM chatbots, necessitating a reliable way to\nevaluate their ability. However, traditional QA benchmarks assume fixed answers\nare inadequate for this purpose. To address this, we introduce DebateQA, a\ndataset of 2,941 debatable questions, each accompanied by multiple\nhuman-annotated partial answers that capture a variety of perspectives. We\ndevelop two metrics: Perspective Diversity, which evaluates the\ncomprehensiveness of perspectives, and Dispute Awareness, which assesses if the\nLLM acknowledges the question's debatable nature. Experiments demonstrate that\nboth metrics align with human preferences and are stable across different\nunderlying models. Using DebateQA with two metrics, we assess 12 popular LLMs\nand retrieval-augmented generation methods. Our findings reveal that while LLMs\ngenerally excel at recognizing debatable issues, their ability to provide\ncomprehensive answers encompassing diverse perspectives varies considerably.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"srOQuKlZ45o0mroZEWwWqZt9bCwpwMXB1MI_kB26VvE","pdfSize":"1626618","txDigest":"C5eG8iNbCgrGxQFkj6cccEKTzbxrduBDJe9JfD3XhKLG","endEpoch":"1","status":"CERTIFIED"}
