{"id":"2407.06871","title":"Rethinking Image-to-Video Adaptation: An Object-centric Perspective","authors":"Rui Qian, Shuangrui Ding and Dahua Lin","authorsParsed":[["Qian","Rui",""],["Ding","Shuangrui",""],["Lin","Dahua",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 13:58:10 GMT"}],"updateDate":"2024-07-10","timestamp":1720533490000,"abstract":"  Image-to-video adaptation seeks to efficiently adapt image models for use in\nthe video domain. Instead of finetuning the entire image backbone, many\nimage-to-video adaptation paradigms use lightweight adapters for temporal\nmodeling on top of the spatial module. However, these attempts are subject to\nlimitations in efficiency and interpretability. In this paper, we propose a\nnovel and efficient image-to-video adaptation strategy from the object-centric\nperspective. Inspired by human perception, which identifies objects as key\ncomponents for video understanding, we integrate a proxy task of object\ndiscovery into image-to-video transfer learning. Specifically, we adopt slot\nattention with learnable queries to distill each frame into a compact set of\nobject tokens. These object-centric tokens are then processed through\nobject-time interaction layers to model object state changes across time.\nIntegrated with two novel object-level losses, we demonstrate the feasibility\nof performing efficient temporal reasoning solely on the compressed\nobject-centric representations for video downstream tasks. Our method achieves\nstate-of-the-art performance with fewer tunable parameters, only 5\\% of fully\nfinetuned models and 50\\% of efficient tuning methods, on action recognition\nbenchmarks. In addition, our model performs favorably in zero-shot video object\nsegmentation without further retraining or object annotations, proving the\neffectiveness of object-centric video understanding.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}