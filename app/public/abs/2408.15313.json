{"id":"2408.15313","title":"Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in\n  Language Models","authors":"Wenxuan Zhang, Philip H.S. Torr, Mohamed Elhoseiny, Adel Bibi","authorsParsed":[["Zhang","Wenxuan",""],["Torr","Philip H. S.",""],["Elhoseiny","Mohamed",""],["Bibi","Adel",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 17:31:21 GMT"}],"updateDate":"2024-08-29","timestamp":1724779881000,"abstract":"  Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during the fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nthe supervised optimization, a labeling function is used to capture global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark including comprehensive discriminative and generative\ntasks for helpfulness and harmlessness. The results indicate that our method\nsignificantly outperforms existing approaches in both safety and helpfulness.\nMoreover, BFPO eliminates the need for human prompting and annotation in LLM\nfine-tuning while achieving the same level of safety as methods that heavily\nrely on human labor, with less than 10% of the computational resources. The\ntraining recipes and models will be released.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8Uwl_Et1w1YKxQwdaAVkDu3HNyg8vo-ykWC7Xy2UqHI","pdfSize":"1367616"}
