{"id":"2408.11551","title":"High Performance Unstructured SpMM Computation Using Tensor Cores","authors":"Patrik Okanovic, Grzegorz Kwasniewski, Paolo Sylos Labini, Maciej\n  Besta, Flavio Vella, Torsten Hoefler","authorsParsed":[["Okanovic","Patrik",""],["Kwasniewski","Grzegorz",""],["Labini","Paolo Sylos",""],["Besta","Maciej",""],["Vella","Flavio",""],["Hoefler","Torsten",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 11:57:42 GMT"}],"updateDate":"2024-08-22","timestamp":1724241462000,"abstract":"  High-performance sparse matrix-matrix (SpMM) multiplication is paramount for\nscience and industry, as the ever-increasing sizes of data prohibit using dense\ndata structures. Yet, existing hardware, such as Tensor Cores (TC), is\nill-suited for SpMM, as it imposes strict constraints on data structures that\ncannot be met by unstructured sparsity found in many applications. To address\nthis, we introduce (S)parse (Ma)trix Matrix (T)ensor Core-accelerated (SMaT): a\nnovel SpMM library that utilizes TCs for unstructured sparse matrices. Our\nblock-sparse library leverages the low-level CUDA MMA\n(matrix-matrix-accumulate) API, maximizing the performance offered by modern\nGPUs. Algorithmic optimizations such as sparse matrix permutation further\nimprove performance by minimizing the number of non-zero blocks. The evaluation\non NVIDIA A100 shows that SMaT outperforms SotA libraries (DASP, cuSPARSE, and\nMagicube) by up to 125x (on average 2.6x). SMaT can be used to accelerate many\nworkloads in scientific computing, large-model training, inference, and others.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}