{"id":"2407.09904","title":"Learning a Mini-batch Graph Transformer via Two-stage Interaction\n  Augmentation","authors":"Wenda Li, Kaixuan Chen, Shunyu Liu, Tongya Zheng, Wenjie Huang and\n  Mingli Song","authorsParsed":[["Li","Wenda",""],["Chen","Kaixuan",""],["Liu","Shunyu",""],["Zheng","Tongya",""],["Huang","Wenjie",""],["Song","Mingli",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 14:42:22 GMT"}],"updateDate":"2024-07-16","timestamp":1720881742000,"abstract":"  Mini-batch Graph Transformer (MGT), as an emerging graph learning model, has\ndemonstrated significant advantages in semi-supervised node prediction tasks\nwith improved computational efficiency and enhanced model robustness. However,\nexisting methods for processing local information either rely on sampling or\nsimple aggregation, which respectively result in the loss and squashing of\ncritical neighbor information.Moreover, the limited number of nodes in each\nmini-batch restricts the model's capacity to capture the global characteristic\nof the graph. In this paper, we propose LGMformer, a novel MGT model that\nemploys a two-stage augmented interaction strategy, transitioning from local to\nglobal perspectives, to address the aforementioned bottlenecks.The local\ninteraction augmentation (LIA) presents a neighbor-target interaction\nTransformer (NTIformer) to acquire an insightful understanding of the\nco-interaction patterns between neighbors and the target node, resulting in a\nlocally effective token list that serves as input for the MGT. In contrast,\nglobal interaction augmentation (GIA) adopts a cross-attention mechanism to\nincorporate entire graph prototypes into the target node epresentation, thereby\ncompensating for the global graph information to ensure a more comprehensive\nperception. To this end, LGMformer achieves the enhancement of node\nrepresentations under the MGT paradigm.Experimental results related to node\nclassification on the ten benchmark datasets demonstrate the effectiveness of\nthe proposed method. Our code is available at\nhttps://github.com/l-wd/LGMformer.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}