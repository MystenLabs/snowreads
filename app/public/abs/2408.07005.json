{"id":"2408.07005","title":"Content and Style Aware Audio-Driven Facial Animation","authors":"Qingju Liu, Hyeongwoo Kim, Gaurav Bharaj","authorsParsed":[["Liu","Qingju",""],["Kim","Hyeongwoo",""],["Bharaj","Gaurav",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 16:12:25 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 17:13:34 GMT"}],"updateDate":"2024-08-15","timestamp":1723565545000,"abstract":"  Audio-driven 3D facial animation has several virtual humans applications for\ncontent creation and editing. While several existing methods provide solutions\nfor speech-driven animation, precise control over content (what) and style\n(how) of the final performance is still challenging. We propose a novel\napproach that takes as input an audio, and the corresponding text to extract\ntemporally-aligned content and disentangled style representations, in order to\nprovide controls over 3D facial animation. Our method is trained in two stages,\nthat evolves from audio prominent styles (how it sounds) to visual prominent\nstyles (how it looks). We leverage a high-resource audio dataset in stage I to\nlearn styles that control speech generation in a self-supervised learning\nframework, and then fine-tune this model with low-resource audio/3D mesh pairs\nin stage II to control 3D vertex generation. We employ a non-autoregressive\nseq2seq formulation to model sentence-level dependencies, and better mouth\narticulations. Our method provides flexibility that the style of a reference\naudio and the content of a source audio can be combined to enable audio style\ntransfer. Similarly, the content can be modified, e.g. muting or swapping\nwords, that enables style-preserving content editing.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Graphics","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}