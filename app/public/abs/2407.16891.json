{"id":"2407.16891","title":"Cultural Value Differences of LLMs: Prompt, Language, and Model Size","authors":"Qishuai Zhong, Yike Yun, Aixin Sun","authorsParsed":[["Zhong","Qishuai",""],["Yun","Yike",""],["Sun","Aixin",""]],"versions":[{"version":"v1","created":"Mon, 17 Jun 2024 12:35:33 GMT"}],"updateDate":"2024-07-25","timestamp":1718627733000,"abstract":"  Our study aims to identify behavior patterns in cultural values exhibited by\nlarge language models (LLMs). The studied variants include question ordering,\nprompting language, and model size. Our experiments reveal that each tested LLM\ncan efficiently behave with different cultural values. More interestingly: (i)\nLLMs exhibit relatively consistent cultural values when presented with prompts\nin a single language. (ii) The prompting language e.g., Chinese or English, can\ninfluence the expression of cultural values. The same question can elicit\ndivergent cultural values when the same LLM is queried in a different language.\n(iii) Differences in sizes of the same model (e.g., Llama2-7B vs 13B vs 70B)\nhave a more significant impact on their demonstrated cultural values than model\ndifferences (e.g., Llama2 vs Mixtral). Our experiments reveal that query\nlanguage and model size of LLM are the main factors resulting in cultural value\ndifferences.\n","subjects":["Computing Research Repository/Computers and Society","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}