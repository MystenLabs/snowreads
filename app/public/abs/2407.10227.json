{"id":"2407.10227","title":"KAT: Dependency-aware Automated API Testing with Large Language Models","authors":"Tri Le, Thien Tran, Duy Cao, Vy Le, Tien Nguyen, Vu Nguyen","authorsParsed":[["Le","Tri",""],["Tran","Thien",""],["Cao","Duy",""],["Le","Vy",""],["Nguyen","Tien",""],["Nguyen","Vu",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 14:48:18 GMT"}],"updateDate":"2024-07-16","timestamp":1720968498000,"abstract":"  API testing has increasing demands for software companies. Prior API testing\ntools were aware of certain types of dependencies that needed to be concise\nbetween operations and parameters. However, their approaches, which are mostly\ndone manually or using heuristic-based algorithms, have limitations due to the\ncomplexity of these dependencies. In this paper, we present KAT (Katalon API\nTesting), a novel AI-driven approach that leverages the large language model\nGPT in conjunction with advanced prompting techniques to autonomously generate\ntest cases to validate RESTful APIs. Our comprehensive strategy encompasses\nvarious processes to construct an operation dependency graph from an OpenAPI\nspecification and to generate test scripts, constraint validation scripts, test\ncases, and test data. Our evaluation of KAT using 12 real-world RESTful\nservices shows that it can improve test coverage, detect more undocumented\nstatus codes, and reduce false positives in these services in comparison with a\nstate-of-the-art automated test generation tool. These results indicate the\neffectiveness of using the large language model for generating test scripts and\ndata for API testing.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"6Z41kjGRdZ0Enho5Fl0WN20Hbgs7rI5GKfKSofw49CQ","pdfSize":"415650"}
