{"id":"2407.17348","title":"DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for\n  Task-Oriented Manipulation","authors":"Qian Feng, David S. Martinez Lema, Mohammadhossein Malmir, Hang Li,\n  Jianxiang Feng, Zhaopeng Chen, Alois Knoll","authorsParsed":[["Feng","Qian",""],["Lema","David S. Martinez",""],["Malmir","Mohammadhossein",""],["Li","Hang",""],["Feng","Jianxiang",""],["Chen","Zhaopeng",""],["Knoll","Alois",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 15:17:55 GMT"}],"updateDate":"2024-07-25","timestamp":1721834275000,"abstract":"  We introduce DexGanGrasp, a dexterous grasping synthesis method that\ngenerates and evaluates grasps with single view in real time. DexGanGrasp\ncomprises a Conditional Generative Adversarial Networks (cGANs)-based\nDexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor\nto assess the stability of these grasps. Extensive simulation and real-world\nexpriments showcases the effectiveness of our proposed method, outperforming\nthe baseline FFHNet with an 18.57% higher success rate in real-world\nevaluation. We further extend DexGanGrasp to DexAfford-Prompt, an\nopen-vocabulary affordance grounding pipeline for dexterous grasping leveraging\nMultimodal Large Language Models (MLLMs) and Vision Language Models (VLMs), to\nachieve task-oriented grasping with successful real-world deployments.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}