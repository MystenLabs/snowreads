{"id":"2408.01835","title":"TS-SAM: Fine-Tuning Segment-Anything Model for Downstream Tasks","authors":"Yang Yu, Chen Xu, Kai Wang","authorsParsed":[["Yu","Yang",""],["Xu","Chen",""],["Wang","Kai",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 18:08:51 GMT"}],"updateDate":"2024-08-06","timestamp":1722708531000,"abstract":"  Adapter based fine-tuning has been studied for improving the performance of\nSAM on downstream tasks. However, there is still a significant performance gap\nbetween fine-tuned SAMs and domain-specific models. To reduce the gap, we\npropose Two-Stream SAM (TS-SAM). On the one hand, inspired by the side network\nin Parameter-Efficient Fine-Tuning (PEFT), we designed a lightweight\nConvolutional Side Adapter (CSA), which integrates the powerful features from\nSAM into side network training for comprehensive feature fusion. On the other\nhand, in line with the characteristics of segmentation tasks, we designed\nMulti-scale Refinement Module (MRM) and Feature Fusion Decoder (FFD) to keep\nboth the detailed and semantic features. Extensive experiments on ten public\ndatasets from three tasks demonstrate that TS-SAM not only significantly\noutperforms the recently proposed SAM-Adapter and SSOM, but achieves\ncompetitive performance with the SOTA domain-specific models. Our code is\navailable at: https://github.com/maoyangou147/TS-SAM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}