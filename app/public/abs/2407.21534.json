{"id":"2407.21534","title":"ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large\n  Language Models","authors":"Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo,\n  Hao Fei, Xiaoshuai Sun, Rongrong Ji","authorsParsed":[["Wu","Mingrui",""],["Cai","Xinyue",""],["Ji","Jiayi",""],["Li","Jiale",""],["Huang","Oucheng",""],["Luo","Gen",""],["Fei","Hao",""],["Sun","Xiaoshuai",""],["Ji","Rongrong",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 11:40:29 GMT"}],"updateDate":"2024-08-01","timestamp":1722426029000,"abstract":"  In this work, we propose a training-free method to inject visual referring\ninto Multimodal Large Language Models (MLLMs) through learnable visual token\noptimization. We observe the relationship between text prompt tokens and visual\ntokens in MLLMs, where attention layers model the connection between them. Our\napproach involves adjusting visual tokens from the MLP output during inference,\ncontrolling which text prompt tokens attend to which visual tokens. We optimize\na learnable visual token based on an energy function, enhancing the strength of\nreferential regions in the attention map. This enables detailed region\ndescription and reasoning without the need for substantial training costs or\nmodel retraining. Our method offers a promising direction for integrating\nreferential abilities into MLLMs. Our method support referring with box, mask,\nscribble and point. The results demonstrate that our method exhibits\ncontrollability and interpretability.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qK-R1nj-yI_QMgrbIhtHMCB0VP7riiaN2LTLnJYF5Rw","pdfSize":"2461993"}
