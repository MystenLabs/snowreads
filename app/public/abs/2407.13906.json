{"id":"2407.13906","title":"Crafting Efficient Fine-Tuning Strategies for Large Language Models","authors":"Michael Oliver and Guan Wang","authorsParsed":[["Oliver","Michael",""],["Wang","Guan",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 21:36:00 GMT"}],"updateDate":"2024-07-22","timestamp":1721338560000,"abstract":"  This paper addresses the challenges of efficiently fine-tuning large language\nmodels (LLMs) by exploring data efficiency and hyperparameter optimization. We\ninvestigate the minimum data required for effective fine-tuning and propose a\nnovel hyperparameter optimization method that leverages early-stage model\nperformance. Our experiments demonstrate that fine-tuning with as few as 200\nsamples can improve model accuracy from 70\\% to 88\\% in a product attribute\nextraction task. We identify a saturation point of approximately 6,500 samples,\nbeyond which additional data yields diminishing returns. Our proposed bayesian\nhyperparameter optimization method, which evaluates models at 20\\% of total\ntraining time, correlates strongly with final model performance, with 4 out of\n5 top early-stage models remaining in the top 5 at completion. This approach\nled to a 2\\% improvement in accuracy over baseline models when evaluated on an\nindependent test set. These findings offer actionable insights for\npractitioners, potentially reducing computational load and dependency on\nextensive datasets while enhancing overall performance of fine-tuned LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2_Dkd-W9jympsVIZjB3lYrMjF4-SFrVunCQY7H42u84","pdfSize":"328887","objectId":"0x8913187906cbb59a9cd97bd7cc0b9009d6da023bd15dd64039cd08d5f29a1e7f","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
