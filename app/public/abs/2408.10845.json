{"id":"2408.10845","title":"CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous\n  Driving","authors":"Hidehisa Arai, Keita Miwa, Kento Sasaki, Yu Yamaguchi, Kohei Watanabe,\n  Shunsuke Aoki, Issei Yamamoto","authorsParsed":[["Arai","Hidehisa",""],["Miwa","Keita",""],["Sasaki","Kento",""],["Yamaguchi","Yu",""],["Watanabe","Kohei",""],["Aoki","Shunsuke",""],["Yamamoto","Issei",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 09:53:49 GMT"}],"updateDate":"2024-08-21","timestamp":1724061229000,"abstract":"  Autonomous driving, particularly navigating complex and unanticipated\nscenarios, demands sophisticated reasoning and planning capabilities. While\nMulti-modal Large Language Models (MLLMs) offer a promising avenue for this,\ntheir use has been largely confined to understanding complex environmental\ncontexts or generating high-level driving commands, with few studies extending\ntheir application to end-to-end path planning. A major research bottleneck is\nthe lack of large-scale annotated datasets encompassing vision, language, and\naction. To address this issue, we propose CoVLA (Comprehensive\nVision-Language-Action) Dataset, an extensive dataset comprising real-world\ndriving videos spanning more than 80 hours. This dataset leverages a novel,\nscalable approach based on automated data processing and a caption generation\npipeline to generate accurate driving trajectories paired with detailed natural\nlanguage descriptions of driving environments and maneuvers. This approach\nutilizes raw in-vehicle sensor data, allowing it to surpass existing datasets\nin scale and annotation richness. Using CoVLA, we investigate the driving\ncapabilities of MLLMs that can handle vision, language, and action in a variety\nof driving scenarios. Our results illustrate the strong proficiency of our\nmodel in generating coherent language and action outputs, emphasizing the\npotential of Vision-Language-Action (VLA) models in the field of autonomous\ndriving. This dataset establishes a framework for robust, interpretable, and\ndata-driven autonomous driving systems by providing a comprehensive platform\nfor training and evaluating VLA models, contributing to safer and more reliable\nself-driving vehicles. The dataset is released for academic purpose.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}