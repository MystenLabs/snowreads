{"id":"2407.20267","title":"A Large Encoder-Decoder Family of Foundation Models For Chemical\n  Language","authors":"Eduardo Soares, Victor Shirasuna, Emilio Vital Brazil, Renato\n  Cerqueira, Dmitry Zubarev, Kristin Schmidt","authorsParsed":[["Soares","Eduardo",""],["Shirasuna","Victor",""],["Brazil","Emilio Vital",""],["Cerqueira","Renato",""],["Zubarev","Dmitry",""],["Schmidt","Kristin",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 20:30:39 GMT"}],"updateDate":"2024-07-31","timestamp":1721853039000,"abstract":"  Large-scale pre-training methodologies for chemical language models represent\na breakthrough in cheminformatics. These methods excel in tasks such as\nproperty prediction and molecule generation by learning contextualized\nrepresentations of input tokens through self-supervised learning on large\nunlabeled corpora. Typically, this involves pre-training on unlabeled data\nfollowed by fine-tuning on specific tasks, reducing dependence on annotated\ndatasets and broadening chemical language representation understanding. This\npaper introduces a large encoder-decoder chemical foundation models pre-trained\non a curated dataset of 91 million SMILES samples sourced from PubChem, which\nis equivalent to 4 billion of molecular tokens. The proposed foundation model\nsupports different complex tasks, including quantum property prediction, and\noffer flexibility with two main variants (289M and $8\\times289M$). Our\nexperiments across multiple benchmark datasets validate the capacity of the\nproposed model in providing state-of-the-art results for different tasks. We\nalso provide a preliminary assessment of the compositionality of the embedding\nspace as a prerequisite for the reasoning tasks. We demonstrate that the\nproduced latent space is separable compared to the state-of-the-art with\nfew-shot learning capabilities.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Physics/Chemical Physics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}