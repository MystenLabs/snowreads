{"id":"2408.13991","title":"Dual-CBA: Improving Online Continual Learning via Dual Continual Bias\n  Adaptors from a Bi-level Optimization Perspective","authors":"Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, Minghao Zhou, Deyu\n  Meng","authorsParsed":[["Wang","Quanziang",""],["Wang","Renzhen",""],["Wu","Yichen",""],["Jia","Xixi",""],["Zhou","Minghao",""],["Meng","Deyu",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 03:19:52 GMT"}],"updateDate":"2024-08-27","timestamp":1724642392000,"abstract":"  In online continual learning (CL), models trained on changing distributions\neasily forget previously learned knowledge and bias toward newly received\ntasks. To address this issue, we present Continual Bias Adaptor (CBA), a\nbi-level framework that augments the classification network to adapt to\ncatastrophic distribution shifts during training, enabling the network to\nachieve a stable consolidation of all seen tasks. However, the CBA module\nadjusts distribution shifts in a class-specific manner, exacerbating the\nstability gap issue and, to some extent, fails to meet the need for continual\ntesting in online CL. To mitigate this challenge, we further propose a novel\nclass-agnostic CBA module that separately aggregates the posterior\nprobabilities of classes from new and old tasks, and applies a stable\nadjustment to the resulting posterior probabilities. We combine the two kinds\nof CBA modules into a unified Dual-CBA module, which thus is capable of\nadapting to catastrophic distribution shifts and simultaneously meets the\nreal-time testing requirements of online CL. Besides, we propose Incremental\nBatch Normalization (IBN), a tailored BN module to re-estimate its population\nstatistics for alleviating the feature bias arising from the inner loop\noptimization problem of our bi-level framework. To validate the effectiveness\nof the proposed method, we theoretically provide some insights into how it\nmitigates catastrophic distribution shifts, and empirically demonstrate its\nsuperiority through extensive experiments based on four rehearsal-based\nbaselines and three public continual learning benchmarks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}