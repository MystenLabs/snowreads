{"id":"2408.15721","title":"Defending Text-to-image Diffusion Models: Surprising Efficacy of Textual\n  Perturbations Against Backdoor Attacks","authors":"Oscar Chew, Po-Yi Lu, Jayden Lin, Hsuan-Tien Lin","authorsParsed":[["Chew","Oscar",""],["Lu","Po-Yi",""],["Lin","Jayden",""],["Lin","Hsuan-Tien",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 11:36:43 GMT"}],"updateDate":"2024-08-29","timestamp":1724845003000,"abstract":"  Text-to-image diffusion models have been widely adopted in real-world\napplications due to their ability to generate realistic images from textual\ndescriptions. However, recent studies have shown that these methods are\nvulnerable to backdoor attacks. Despite the significant threat posed by\nbackdoor attacks on text-to-image diffusion models, countermeasures remain\nunder-explored. In this paper, we address this research gap by demonstrating\nthat state-of-the-art backdoor attacks against text-to-image diffusion models\ncan be effectively mitigated by a surprisingly simple defense strategy -\ntextual perturbation. Experiments show that textual perturbations are effective\nin defending against state-of-the-art backdoor attacks with minimal sacrifice\nto generation quality. We analyze the efficacy of textual perturbation from two\nangles: text embedding space and cross-attention maps. They further explain how\nbackdoor attacks have compromised text-to-image diffusion models, providing\ninsights for studying future attack and defense strategies. Our code is\navailable at https://github.com/oscarchew/t2i-backdoor-defense.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}