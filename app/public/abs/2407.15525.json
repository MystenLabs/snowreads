{"id":"2407.15525","title":"Multiple importance sampling for stochastic gradient estimation","authors":"Corentin Sala\\\"un, Xingchang Huang, Iliyan Georgiev, Niloy J. Mitra,\n  Gurprit Singh","authorsParsed":[["Sala√ºn","Corentin",""],["Huang","Xingchang",""],["Georgiev","Iliyan",""],["Mitra","Niloy J.",""],["Singh","Gurprit",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 10:28:56 GMT"}],"updateDate":"2024-07-23","timestamp":1721644136000,"abstract":"  We introduce a theoretical and practical framework for efficient importance\nsampling of mini-batch samples for gradient estimation from single and multiple\nprobability distributions. To handle noisy gradients, our framework dynamically\nevolves the importance distribution during training by utilizing a\nself-adaptive metric. Our framework combines multiple, diverse sampling\ndistributions, each tailored to specific parameter gradients. This approach\nfacilitates the importance sampling of vector-valued gradient estimation.\nRather than naively combining multiple distributions, our framework involves\noptimally weighting data contribution across multiple distributions. This\nadapted combination of multiple importance yields superior gradient estimates,\nleading to faster training convergence. We demonstrate the effectiveness of our\napproach through empirical evaluations across a range of optimization tasks\nlike classification and regression on both image and point cloud datasets.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}