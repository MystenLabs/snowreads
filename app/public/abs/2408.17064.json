{"id":"2408.17064","title":"Instant Adversarial Purification with Adversarial Consistency\n  Distillation","authors":"Chun Tong Lei, Hon Ming Yam, Zhongliang Guo, Chun Pong Lau","authorsParsed":[["Lei","Chun Tong",""],["Yam","Hon Ming",""],["Guo","Zhongliang",""],["Lau","Chun Pong",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 07:49:35 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 06:25:09 GMT"}],"updateDate":"2024-09-04","timestamp":1725004175000,"abstract":"  Neural networks, despite their remarkable performance in widespread\napplications, including image classification, are also known to be vulnerable\nto subtle adversarial noise. Although some diffusion-based purification methods\nhave been proposed, for example, DiffPure, those methods are time-consuming. In\nthis paper, we propose One Step Control Purification (OSCP), a diffusion-based\npurification model that can purify the adversarial image in one Neural Function\nEvaluation (NFE) in diffusion models. We use Latent Consistency Model (LCM) and\nControlNet for our one-step purification. OSCP is computationally friendly and\ntime efficient compared to other diffusion-based purification methods; we\nachieve defense success rate of 74.19\\% on ImageNet, only requiring 0.1s for\neach purification. Moreover, there is a fundamental incongruence between\nconsistency distillation and adversarial perturbation. To address this\nontological dissonance, we propose Gaussian Adversarial Noise Distillation\n(GAND), a novel consistency distillation framework that facilitates a more\nnuanced reconciliation of the latent space dynamics, effectively bridging the\nnatural and adversarial manifolds. Our experiments show that the GAND does not\nneed a Full Fine Tune (FFT); PEFT, e.g., LoRA is sufficient.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}