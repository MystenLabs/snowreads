{"id":"2407.11188","title":"Efficient In-Context Medical Segmentation with Meta-driven Visual Prompt\n  Selection","authors":"Chenwei Wu, David Restrepo, Zitao Shuai, Zhongming Liu, Liyue Shen","authorsParsed":[["Wu","Chenwei",""],["Restrepo","David",""],["Shuai","Zitao",""],["Liu","Zhongming",""],["Shen","Liyue",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 19:22:32 GMT"}],"updateDate":"2024-07-17","timestamp":1721071352000,"abstract":"  In-context learning (ICL) with Large Vision Models (LVMs) presents a\npromising avenue in medical image segmentation by reducing the reliance on\nextensive labeling. However, the ICL performance of LVMs highly depends on the\nchoices of visual prompts and suffers from domain shifts. While existing works\nleveraging LVMs for medical tasks have focused mainly on model-centric\napproaches like fine-tuning, we study an orthogonal data-centric perspective on\nhow to select good visual prompts to facilitate generalization to medical\ndomain. In this work, we propose a label-efficient in-context medical\nsegmentation method by introducing a novel Meta-driven Visual Prompt Selection\nmechanism (MVPS), where a prompt retriever obtained from a meta-learning\nframework actively selects the optimal images as prompts to promote model\nperformance and generalizability. Evaluated on 8 datasets and 4 tasks across 3\nmedical imaging modalities, our proposed approach demonstrates consistent gains\nover existing methods under different scenarios, improving both computational\nand label efficiency. Finally, we show that MVPS is a flexible, finetuning-free\nmodule that could be easily plugged into different backbones and combined with\nother model-centric approaches.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"NJO-twhn4EO4mEw7-_EVIbID3D1HtyWobMEVUZwmCU8","pdfSize":"6022123"}
