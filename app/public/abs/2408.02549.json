{"id":"2408.02549","title":"Generative AI as a Service in 6G Edge-Cloud: Generation Task Offloading\n  by In-context Learning","authors":"Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu, Xue Liu, Zhu Han,\n  and Charlie Zhang","authorsParsed":[["Zhou","Hao",""],["Hu","Chengming",""],["Yuan","Dun",""],["Yuan","Ye",""],["Wu","Di",""],["Liu","Xue",""],["Han","Zhu",""],["Zhang","Charlie",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 15:20:08 GMT"}],"updateDate":"2024-08-06","timestamp":1722871208000,"abstract":"  Generative artificial intelligence (GAI) is a promising technique towards 6G\nnetworks, and generative foundation models such as large language models (LLMs)\nhave attracted considerable interest from academia and telecom industry. This\nwork considers a novel edge-cloud deployment of foundation models in 6G\nnetworks. Specifically, it aims to minimize the service delay of foundation\nmodels by radio resource allocation and task offloading, i.e., offloading\ndiverse content generation tasks to proper LLMs at the network edge or cloud.\nIn particular, we first introduce the communication system model, i.e.,\nallocating radio resources and calculating link capacity to support generated\ncontent transmission, and then we present the LLM inference model to calculate\nthe delay of content generation. After that, we propose a novel in-context\nlearning method to optimize the task offloading decisions. It utilizes LLM's\ninference capabilities, and avoids the difficulty of dedicated model training\nor fine-tuning as in conventional machine learning algorithms. Finally, the\nsimulations demonstrate that the proposed edge-cloud deployment and in-context\nlearning task offloading method can achieve satisfactory generation service\nquality without dedicated model training or fine-tuning.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}