{"id":"2407.11059","title":"Was it Slander? Towards Exact Inversion of Generative Language Models","authors":"Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro","authorsParsed":[["Skapars","Adrians",""],["Manino","Edoardo",""],["Sun","Youcheng",""],["Cordeiro","Lucas C.",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 11:08:06 GMT"}],"updateDate":"2024-07-17","timestamp":1720609686000,"abstract":"  Training large language models (LLMs) requires a substantial investment of\ntime and money. To get a good return on investment, the developers spend\nconsiderable effort ensuring that the model never produces harmful and\noffensive outputs. However, bad-faith actors may still try to slander the\nreputation of an LLM by publicly reporting a forged output. In this paper, we\nshow that defending against such slander attacks requires reconstructing the\ninput of the forged output or proving that it does not exist. To do so, we\npropose and evaluate a search based approach for targeted adversarial attacks\nfor LLMs. Our experiments show that we are rarely able to reconstruct the exact\ninput of an arbitrary output, thus demonstrating that LLMs are still vulnerable\nto slander attacks.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}