{"id":"2407.02327","title":"QSync: Quantization-Minimized Synchronous Distributed Training Across\n  Hybrid Devices","authors":"Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, Yibo Zhu, Chuan Wu","authorsParsed":[["Zhao","Juntao",""],["Wan","Borui",""],["Peng","Yanghua",""],["Lin","Haibin",""],["Zhu","Yibo",""],["Wu","Chuan",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 14:56:47 GMT"}],"updateDate":"2024-07-03","timestamp":1719932207000,"abstract":"  A number of production deep learning clusters have attempted to explore\ninference hardware for DNN training, at the off-peak serving hours with many\ninference GPUs idling. Conducting DNN training with a combination of\nheterogeneous training and inference GPUs, known as hybrid device training,\npresents considerable challenges due to disparities in compute capability and\nsignificant differences in memory capacity. We propose QSync, a training system\nthat enables efficient synchronous data-parallel DNN training over hybrid\ndevices by strategically exploiting quantized operators. According to each\ndevice's available resource capacity, QSync selects a quantization-minimized\nsetting for operators in the distributed DNN training graph, minimizing model\naccuracy degradation but keeping the training efficiency brought by\nquantization. We carefully design a predictor with a bi-directional\nmixed-precision indicator to reflect the sensitivity of DNN layers on\nfixed-point and floating-point low-precision operators, a replayer with a\nneighborhood-aware cost mapper to accurately estimate the latency of\ndistributed hybrid mixed-precision training, and then an allocator that\nefficiently synchronizes workers with minimized model accuracy degradation.\nQSync bridges the computational graph on PyTorch to an optimized backend for\nquantization kernel performance and flexible support for various GPU\narchitectures. Extensive experiments show that QSync's predictor can accurately\nsimulate distributed mixed-precision training with <5% error, with a consistent\n0.27-1.03% accuracy improvement over the from-scratch training tasks compared\nto uniform precision.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}