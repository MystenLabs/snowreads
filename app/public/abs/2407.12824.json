{"id":"2407.12824","title":"Whispering Experts: Neural Interventions for Toxicity Mitigation in\n  Language Models","authors":"Xavier Suau, Pieter Delobelle, Katherine Metcalf, Armand Joulin,\n  Nicholas Apostoloff, Luca Zappella, Pau Rodr\\'iguez","authorsParsed":[["Suau","Xavier",""],["Delobelle","Pieter",""],["Metcalf","Katherine",""],["Joulin","Armand",""],["Apostoloff","Nicholas",""],["Zappella","Luca",""],["Rodr√≠guez","Pau",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 12:48:29 GMT"}],"updateDate":"2024-07-19","timestamp":1719924509000,"abstract":"  An important issue with Large Language Models (LLMs) is their undesired\nability to generate toxic language. In this work, we show that the neurons\nresponsible for toxicity can be determined by their power to discriminate toxic\nsentences, and that toxic language can be mitigated by reducing their\nactivation levels proportionally to this power. We propose AUROC adaptation\n(AurA), an intervention that can be applied to any pre-trained LLM to mitigate\ntoxicity. As the intervention is proportional to the ability of each neuron to\ndiscriminate toxic content, it is free of any model-dependent hyperparameters.\nWe show that AurA can achieve up to $2.2 \\times$ reduction in toxicity with\nonly a $0.72$ perplexity increase. We also show that AurA is effective with\nmodels of different scale (from 1.5B to 40B parameters), and its effectiveness\nin mitigating toxic language, while preserving common-sense zero-shot\nabilities, holds across all scales. AurA can be combined with pre-prompting\nstrategies, boosting its average mitigation potential from $1.28\\times$ to\n$2.35\\times$. Moreover, AurA can counteract adversarial pre-prompts that\nmaliciously elicit toxic content, making it an effective method for deploying\nsafer and less toxic models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}