{"id":"2407.19446","title":"Leave-One-Out Analysis for Nonconvex Robust Matrix Completion with\n  General Thresholding Functions","authors":"Tianming Wang, Ke Wei","authorsParsed":[["Wang","Tianming",""],["Wei","Ke",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 09:47:36 GMT"}],"updateDate":"2024-07-30","timestamp":1722160056000,"abstract":"  We study the problem of robust matrix completion (RMC), where the partially\nobserved entries of an underlying low-rank matrix is corrupted by sparse noise.\nExisting analysis of the non-convex methods for this problem either requires\nthe explicit but empirically redundant regularization in the algorithm or\nrequires sample splitting in the analysis. In this paper, we consider a simple\nyet efficient nonconvex method which alternates between a projected gradient\nstep for the low-rank part and a thresholding step for the sparse noise part.\nInspired by leave-one out analysis for low rank matrix completion, it is\nestablished that the method can achieve linear convergence for a general class\nof thresholding functions, including for example soft-thresholding and SCAD. To\nthe best of our knowledge, this is the first leave-one-out analysis on a\nnonconvex method for RMC. Additionally, when applying our result to low rank\nmatrix completion, it improves the sampling complexity of existing result for\nthe singular value projection method.\n","subjects":["Computing Research Repository/Information Theory","Mathematics/Information Theory","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}