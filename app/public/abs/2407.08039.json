{"id":"2407.08039","title":"Knowledge Overshadowing Causes Amalgamated Hallucination in Large\n  Language Models","authors":"Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi R. Fung, Jing Li,\n  Manling Li, Heng Ji","authorsParsed":[["Zhang","Yuji",""],["Li","Sha",""],["Liu","Jiateng",""],["Yu","Pengfei",""],["Fung","Yi R.",""],["Li","Jing",""],["Li","Manling",""],["Ji","Heng",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 20:37:42 GMT"}],"updateDate":"2024-07-12","timestamp":1720643862000,"abstract":"  Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}