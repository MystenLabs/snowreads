{"id":"2408.12259","title":"Can You Trust Your Metric? Automatic Concatenation-Based Tests for\n  Metric Validity","authors":"Ora Nova Fandina, Leshem Choshen, Eitan Farchi, George Kour, Yotam\n  Perlitz, Orna Raz","authorsParsed":[["Fandina","Ora Nova",""],["Choshen","Leshem",""],["Farchi","Eitan",""],["Kour","George",""],["Perlitz","Yotam",""],["Raz","Orna",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 09:57:57 GMT"}],"updateDate":"2024-08-23","timestamp":1724320677000,"abstract":"  Consider a scenario where a harmfulness detection metric is employed by a\nsystem to filter unsafe responses generated by a Large Language Model. When\nanalyzing individual harmful and unethical prompt-response pairs, the metric\ncorrectly classifies each pair as highly unsafe, assigning the highest score.\nHowever, when these same prompts and responses are concatenated, the metric's\ndecision flips, assigning the lowest possible score, thereby misclassifying the\ncontent as safe and allowing it to bypass the filter. In this study, we\ndiscovered that several harmfulness LLM-based metrics, including GPT-based,\nexhibit this decision-flipping phenomenon. Additionally, we found that even an\nadvanced metric like GPT-4o is highly sensitive to input order. Specifically,\nit tends to classify responses as safe if the safe content appears first,\nregardless of any harmful content that follows, and vice versa. This work\nintroduces automatic concatenation-based tests to assess the fundamental\nproperties a valid metric should satisfy. We applied these tests in a model\nsafety scenario to assess the reliability of harmfulness detection metrics,\nuncovering a number of inconsistencies.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"k2L7XqEcN9NJXCueQhtnBxMdDzn0U4q1nl9aLTsx1DY","pdfSize":"8993023"}
