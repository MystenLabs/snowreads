{"id":"2407.05848","title":"Wavelet Convolutions for Large Receptive Fields","authors":"Shahaf E. Finder, Roy Amoyal, Eran Treister, Oren Freifeld","authorsParsed":[["Finder","Shahaf E.",""],["Amoyal","Roy",""],["Treister","Eran",""],["Freifeld","Oren",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 11:55:10 GMT"},{"version":"v2","created":"Mon, 15 Jul 2024 08:39:57 GMT"}],"updateDate":"2024-07-16","timestamp":1720439710000,"abstract":"  In recent years, there have been attempts to increase the kernel size of\nConvolutional Neural Nets (CNNs) to mimic the global receptive field of Vision\nTransformers' (ViTs) self-attention blocks. That approach, however, quickly hit\nan upper bound and saturated way before achieving a global receptive field. In\nthis work, we demonstrate that by leveraging the Wavelet Transform (WT), it is,\nin fact, possible to obtain very large receptive fields without suffering from\nover-parameterization, e.g., for a $k \\times k$ receptive field, the number of\ntrainable parameters in the proposed method grows only logarithmically with\n$k$. The proposed layer, named WTConv, can be used as a drop-in replacement in\nexisting architectures, results in an effective multi-frequency response, and\nscales gracefully with the size of the receptive field. We demonstrate the\neffectiveness of the WTConv layer within ConvNeXt and MobileNetV2 architectures\nfor image classification, as well as backbones for downstream tasks, and show\nit yields additional properties such as robustness to image corruption and an\nincreased response to shapes over textures. Our code is available at\nhttps://github.com/BGU-CS-VIL/WTConv.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}