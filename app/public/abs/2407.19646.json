{"id":"2407.19646","title":"Foundations for Unfairness in Anomaly Detection -- Case Studies in\n  Facial Imaging Data","authors":"Michael Livanos and Ian Davidson","authorsParsed":[["Livanos","Michael",""],["Davidson","Ian",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 02:04:29 GMT"}],"updateDate":"2024-07-30","timestamp":1722218669000,"abstract":"  Deep anomaly detection (AD) is perhaps the most controversial of data\nanalytic tasks as it identifies entities that are then specifically targeted\nfor further investigation or exclusion. Also controversial is the application\nof AI to facial imaging data. This work explores the intersection of these two\nareas to understand two core questions: \"Who\" these algorithms are being unfair\nto and equally important \"Why\". Recent work has shown that deep AD can be\nunfair to different groups despite being unsupervised with a recent study\nshowing that for portraits of people: men of color are far more likely to be\nchosen to be outliers. We study the two main categories of AD algorithms:\nautoencoder-based and single-class-based which effectively try to compress all\nthe instances with those that can not be easily compressed being deemed to be\noutliers. We experimentally verify sources of unfairness such as the\nunder-representation of a group (e.g. people of color are relatively rare),\nspurious group features (e.g. men are often photographed with hats), and group\nlabeling noise (e.g. race is subjective). We conjecture that lack of\ncompressibility is the main foundation and the others cause it but experimental\nresults show otherwise and we present a natural hierarchy amongst them.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}