{"id":"2407.13755","title":"Random Latent Exploration for Deep Reinforcement Learning","authors":"Srinath Mahankali, Zhang-Wei Hong, Ayush Sekhari, Alexander Rakhlin,\n  Pulkit Agrawal","authorsParsed":[["Mahankali","Srinath",""],["Hong","Zhang-Wei",""],["Sekhari","Ayush",""],["Rakhlin","Alexander",""],["Agrawal","Pulkit",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:55:22 GMT"}],"updateDate":"2024-07-19","timestamp":1721325322000,"abstract":"  The ability to efficiently explore high-dimensional state spaces is essential\nfor the practical success of deep Reinforcement Learning (RL). This paper\nintroduces a new exploration technique called Random Latent Exploration (RLE),\nthat combines the strengths of bonus-based and noise-based (two popular\napproaches for effective exploration in deep RL) exploration strategies. RLE\nleverages the idea of perturbing rewards by adding structured random rewards to\nthe original task rewards in certain (random) states of the environment, to\nencourage the agent to explore the environment during training. RLE is\nstraightforward to implement and performs well in practice. To demonstrate the\npractical effectiveness of RLE, we evaluate it on the challenging Atari and\nIsaacGym benchmarks and show that RLE exhibits higher overall scores across all\nthe tasks than other approaches.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Z2md8Y1J-kRmY7AkOgvxHq54KXF1H8DExndb7PBrP0U","pdfSize":"5696943"}
