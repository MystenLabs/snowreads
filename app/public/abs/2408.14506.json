{"id":"2408.14506","title":"Distilling Long-tailed Datasets","authors":"Zhenghao Zhao, Haoxuan Wang, Yuzhang Shang, Kai Wang, Yan Yan","authorsParsed":[["Zhao","Zhenghao",""],["Wang","Haoxuan",""],["Shang","Yuzhang",""],["Wang","Kai",""],["Yan","Yan",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 15:36:36 GMT"}],"updateDate":"2024-08-28","timestamp":1724513796000,"abstract":"  Dataset distillation (DD) aims to distill a small, information-rich dataset\nfrom a larger one for efficient neural network training. However, existing DD\nmethods struggle with long-tailed datasets, which are prevalent in real-world\nscenarios. By investigating the reasons behind this unexpected result, we\nidentified two main causes: 1) Expert networks trained on imbalanced data\ndevelop biased gradients, leading to the synthesis of similarly imbalanced\ndistilled datasets. Parameter matching, a common technique in DD, involves\naligning the learning parameters of the distilled dataset with that of the\noriginal dataset. However, in the context of long-tailed datasets, matching\nbiased experts leads to inheriting the imbalance present in the original data,\ncausing the distilled dataset to inadequately represent tail classes. 2) The\nexperts trained on such datasets perform suboptimally on tail classes,\nresulting in misguided distillation supervision and poor-quality soft-label\ninitialization. To address these issues, we propose a novel long-tailed dataset\ndistillation method, Long-tailed Aware Dataset distillation (LAD).\nSpecifically, we propose Weight Mismatch Avoidance to avoid directly matching\nthe biased expert trajectories. It reduces the distance between the student and\nthe biased expert trajectories and prevents the tail class bias from being\ndistilled to the synthetic dataset. Moreover, we propose Adaptive Decoupled\nMatching, which jointly matches the decoupled backbone and classifier to\nimprove the tail class performance and initialize reliable soft labels. This\nwork pioneers the field of long-tailed dataset distillation (LTDD), marking the\nfirst effective effort to distill long-tailed datasets.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}