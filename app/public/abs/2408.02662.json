{"id":"2408.02662","title":"Integrating Model-Based Footstep Planning with Model-Free Reinforcement\n  Learning for Dynamic Legged Locomotion","authors":"Ho Jae Lee, Seungwoo Hong, Sangbae Kim","authorsParsed":[["Lee","Ho Jae",""],["Hong","Seungwoo",""],["Kim","Sangbae",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 17:55:23 GMT"}],"updateDate":"2024-08-06","timestamp":1722880523000,"abstract":"  In this work, we introduce a control framework that combines model-based\nfootstep planning with Reinforcement Learning (RL), leveraging desired footstep\npatterns derived from the Linear Inverted Pendulum (LIP) dynamics. Utilizing\nthe LIP model, our method forward predicts robot states and determines the\ndesired foot placement given the velocity commands. We then train an RL policy\nto track the foot placements without following the full reference motions\nderived from the LIP model. This partial guidance from the physics model allows\nthe RL policy to integrate the predictive capabilities of the physics-informed\ndynamics and the adaptability characteristics of the RL controller without\noverfitting the policy to the template model. Our approach is validated on the\nMIT Humanoid, demonstrating that our policy can achieve stable yet dynamic\nlocomotion for walking and turning. We further validate the adaptability and\ngeneralizability of our policy by extending the locomotion task to unseen,\nuneven terrain. During the hardware deployment, we have achieved forward\nwalking speeds of up to 1.5 m/s on a treadmill and have successfully performed\ndynamic locomotion maneuvers such as 90-degree and 180-degree turns.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/"}