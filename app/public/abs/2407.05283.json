{"id":"2407.05283","title":"SCIPaD: Incorporating Spatial Clues into Unsupervised Pose-Depth Joint\n  Learning","authors":"Yi Feng, Zizhan Guo, Qijun Chen, Rui Fan","authorsParsed":[["Feng","Yi",""],["Guo","Zizhan",""],["Chen","Qijun",""],["Fan","Rui",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 06:52:51 GMT"}],"updateDate":"2024-07-09","timestamp":1720335171000,"abstract":"  Unsupervised monocular depth estimation frameworks have shown promising\nperformance in autonomous driving. However, existing solutions primarily rely\non a simple convolutional neural network for ego-motion recovery, which\nstruggles to estimate precise camera poses in dynamic, complicated real-world\nscenarios. These inaccurately estimated camera poses can inevitably deteriorate\nthe photometric reconstruction and mislead the depth estimation networks with\nwrong supervisory signals. In this article, we introduce SCIPaD, a novel\napproach that incorporates spatial clues for unsupervised depth-pose joint\nlearning. Specifically, a confidence-aware feature flow estimator is proposed\nto acquire 2D feature positional translations and their associated confidence\nlevels. Meanwhile, we introduce a positional clue aggregator, which integrates\npseudo 3D point clouds from DepthNet and 2D feature flows into homogeneous\npositional representations. Finally, a hierarchical positional embedding\ninjector is proposed to selectively inject spatial clues into semantic features\nfor robust camera pose decoding. Extensive experiments and analyses demonstrate\nthe superior performance of our model compared to other state-of-the-art\nmethods. Remarkably, SCIPaD achieves a reduction of 22.2\\% in average\ntranslation error and 34.8\\% in average angular error for camera pose\nestimation task on the KITTI Odometry dataset. Our source code is available at\n\\url{https://mias.group/SCIPaD}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}