{"id":"2408.10826","title":"NeuLite: Memory-Efficient Federated Learning via Elastic Progressive\n  Training","authors":"Yebo Wu, Li Li, Chunlin Tian, Dubing Chen, Chengzhong Xu","authorsParsed":[["Wu","Yebo",""],["Li","Li",""],["Tian","Chunlin",""],["Chen","Dubing",""],["Xu","Chengzhong",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 13:21:52 GMT"}],"updateDate":"2024-08-21","timestamp":1724160112000,"abstract":"  Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, intensive memory footprint during the training process\nseverely bottlenecks the deployment of FL on resource-constrained devices in\nreal-world cases. In this paper, we propose NeuLite, a framework that breaks\nthe memory wall through elastic progressive training. Unlike traditional FL,\nwhich updates the full model during the whole training procedure, NeuLite\ndivides the model into blocks and conducts the training process in a\nprogressive manner. Except for the progressive training paradigm, NeuLite\nfurther features the following two key components to guide the training\nprocess: 1) curriculum mentor and 2) training harmonizer. Specifically, the\nCurriculum Mentor devises curriculum-aware training losses for each block,\nassisting them in learning the expected feature representation and mitigating\nthe loss of valuable information. Additionally, the Training Harmonizer\ndevelops a parameter co-adaptation training paradigm to break the information\nisolation across blocks from both forward and backward propagation.\nFurthermore, it constructs output modules for each block to strengthen model\nparameter co-adaptation. Extensive experiments are conducted to evaluate the\neffectiveness of NeuLite across both simulation and hardware testbeds. The\nresults demonstrate that NeuLite effectively reduces peak memory usage by up to\n50.4%. It also enhances model performance by up to 84.2% and accelerates the\ntraining process by up to 1.9X.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}