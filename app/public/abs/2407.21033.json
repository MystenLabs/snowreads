{"id":"2407.21033","title":"Multi-Grained Query-Guided Set Prediction Network for Grounded\n  Multimodal Named Entity Recognition","authors":"Jielong Tang, Zhenxing Wang, Ziyang Gong, Jianxing Yu, Xiangwei Zhu\n  and Jian Yin","authorsParsed":[["Tang","Jielong",""],["Wang","Zhenxing",""],["Gong","Ziyang",""],["Yu","Jianxing",""],["Zhu","Xiangwei",""],["Yin","Jian",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 05:42:43 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 13:09:02 GMT"}],"updateDate":"2024-08-22","timestamp":1721194963000,"abstract":"  Grounded Multimodal Named Entity Recognition (GMNER) is an emerging\ninformation extraction (IE) task, aiming to simultaneously extract entity\nspans, types, and corresponding visual regions of entities from given\nsentence-image pairs data. Recent unified methods employing machine reading\ncomprehension or sequence generation-based frameworks show limitations in this\ndifficult task. The former, utilizing human-designed queries, struggles to\ndifferentiate ambiguous entities, such as Jordan (Person) and off-White x\nJordan (Shoes). The latter, following the one-by-one decoding order, suffers\nfrom exposure bias issues. We maintain that these works misunderstand the\nrelationships of multimodal entities. To tackle these, we propose a novel\nunified framework named Multi-grained Query-guided Set Prediction Network\n(MQSPN) to learn appropriate relationships at intra-entity and inter-entity\nlevels. Specifically, MQSPN consists of a Multi-grained Query Set (MQS) and a\nMultimodal Set Prediction Network (MSP). MQS explicitly aligns entity regions\nwith entity spans by employing a set of learnable queries to strengthen\nintra-entity connections. Based on distinct intra-entity modeling, MSP\nreformulates GMNER as a set prediction, guiding models to establish appropriate\ninter-entity relationships from a global matching perspective. Additionally, we\nincorporate a query-guided Fusion Net (QFNet) to work as a glue network between\nMQS and MSP. Extensive experiments demonstrate that our approach achieves\nstate-of-the-art performances in widely used benchmarks.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}