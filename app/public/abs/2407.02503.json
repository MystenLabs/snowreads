{"id":"2407.02503","title":"Optimizing Deep Reinforcement Learning for Adaptive Robotic Arm Control","authors":"Jonaid Shianifar, Michael Schukat, Karl Mason","authorsParsed":[["Shianifar","Jonaid",""],["Schukat","Michael",""],["Mason","Karl",""]],"versions":[{"version":"v1","created":"Wed, 12 Jun 2024 15:06:54 GMT"}],"updateDate":"2024-07-04","timestamp":1718204814000,"abstract":"  In this paper, we explore the optimization of hyperparameters for the Soft\nActor-Critic (SAC) and Proximal Policy Optimization (PPO) algorithms using the\nTree-structured Parzen Estimator (TPE) in the context of robotic arm control\nwith seven Degrees of Freedom (DOF). Our results demonstrate a significant\nenhancement in algorithm performance, TPE improves the success rate of SAC by\n10.48 percentage points and PPO by 34.28 percentage points, where models\ntrained for 50K episodes. Furthermore, TPE enables PPO to converge to a reward\nwithin 95% of the maximum reward 76% faster than without TPE, which translates\nto about 40K fewer episodes of training required for optimal performance. Also,\nthis improvement for SAC is 80% faster than without TPE. This study underscores\nthe impact of advanced hyperparameter optimization on the efficiency and\nsuccess of deep reinforcement learning algorithms in complex robotic tasks.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}