{"id":"2408.00083","title":"Localized Gaussian Splatting Editing with Contextual Awareness","authors":"Hanyuan Xiao, Yingshu Chen, Huajian Huang, Haolin Xiong, Jing Yang,\n  Pratusha Prasad, Yajie Zhao","authorsParsed":[["Xiao","Hanyuan",""],["Chen","Yingshu",""],["Huang","Huajian",""],["Xiong","Haolin",""],["Yang","Jing",""],["Prasad","Pratusha",""],["Zhao","Yajie",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 18:00:45 GMT"}],"updateDate":"2024-08-02","timestamp":1722448845000,"abstract":"  Recent text-guided generation of individual 3D object has achieved great\nsuccess using diffusion priors. However, these methods are not suitable for\nobject insertion and replacement tasks as they do not consider the background,\nleading to illumination mismatches within the environment. To bridge the gap,\nwe introduce an illumination-aware 3D scene editing pipeline for 3D Gaussian\nSplatting (3DGS) representation. Our key observation is that inpainting by the\nstate-of-the-art conditional 2D diffusion model is consistent with background\nin lighting. To leverage the prior knowledge from the well-trained diffusion\nmodels for 3D object generation, our approach employs a coarse-to-fine\nobjection optimization pipeline with inpainted views. In the first coarse step,\nwe achieve image-to-3D lifting given an ideal inpainted view. The process\nemploys 3D-aware diffusion prior from a view-conditioned diffusion model, which\npreserves illumination present in the conditioning image. To acquire an ideal\ninpainted image, we introduce an Anchor View Proposal (AVP) algorithm to find a\nsingle view that best represents the scene illumination in target region. In\nthe second Texture Enhancement step, we introduce a novel Depth-guided\nInpainting Score Distillation Sampling (DI-SDS), which enhances geometry and\ntexture details with the inpainting diffusion prior, beyond the scope of the\n3D-aware diffusion prior knowledge in the first coarse step. DI-SDS not only\nprovides fine-grained texture enhancement, but also urges optimization to\nrespect scene lighting. Our approach efficiently achieves local editing with\nglobal illumination consistency without explicitly modeling light transport. We\ndemonstrate robustness of our method by evaluating editing in real scenes\ncontaining explicit highlight and shadows, and compare against the\nstate-of-the-art text-to-3D editing methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}