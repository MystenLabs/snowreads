{"id":"2407.05705","title":"Fast and Continual Knowledge Graph Embedding via Incremental LoRA","authors":"Jiajun Liu, Wenjun Ke, Peng Wang, Jiahao Wang, Jinhua Gao, Ziyu Shang,\n  Guozheng Li, Zijie Xu, Ke Ji and Yining Li","authorsParsed":[["Liu","Jiajun",""],["Ke","Wenjun",""],["Wang","Peng",""],["Wang","Jiahao",""],["Gao","Jinhua",""],["Shang","Ziyu",""],["Li","Guozheng",""],["Xu","Zijie",""],["Ji","Ke",""],["Li","Yining",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 08:07:13 GMT"}],"updateDate":"2024-07-09","timestamp":1720426033000,"abstract":"  Continual Knowledge Graph Embedding (CKGE) aims to efficiently learn new\nknowledge and simultaneously preserve old knowledge. Dominant approaches\nprimarily focus on alleviating catastrophic forgetting of old knowledge but\nneglect efficient learning for the emergence of new knowledge. However, in\nreal-world scenarios, knowledge graphs (KGs) are continuously growing, which\nbrings a significant challenge to fine-tuning KGE models efficiently. To\naddress this issue, we propose a fast CKGE framework (\\model), incorporating an\nincremental low-rank adapter (\\mec) mechanism to efficiently acquire new\nknowledge while preserving old knowledge. Specifically, to mitigate\ncatastrophic forgetting, \\model\\ isolates and allocates new knowledge to\nspecific layers based on the fine-grained influence between old and new KGs.\nSubsequently, to accelerate fine-tuning, \\model\\ devises an efficient \\mec\\\nmechanism, which embeds the specific layers into incremental low-rank adapters\nwith fewer training parameters. Moreover, \\mec\\ introduces adaptive rank\nallocation, which makes the LoRA aware of the importance of entities and\nadjusts its rank scale adaptively. We conduct experiments on four public\ndatasets and two new datasets with a larger initial scale. Experimental results\ndemonstrate that \\model\\ can reduce training time by 34\\%-49\\% while still\nachieving competitive link prediction performance against state-of-the-art\nmodels on four public datasets (average MRR score of 21.0\\% vs.\n21.1\\%).Meanwhile, on two newly constructed datasets, \\model\\ saves 51\\%-68\\%\ntraining time and improves link prediction performance by 1.5\\%.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}