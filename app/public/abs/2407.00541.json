{"id":"2407.00541","title":"Answering real-world clinical questions using large language model based\n  systems","authors":"Yen Sia Low (1), Michael L. Jackson (1), Rebecca J. Hyde (1), Robert\n  E. Brown (1), Neil M. Sanghavi (1), Julian D. Baldwin (1), C. William Pike\n  (1), Jananee Muralidharan (1), Gavin Hui (1 and 2), Natasha Alexander (3),\n  Hadeel Hassan (3), Rahul V. Nene (4), Morgan Pike (5), Courtney J. Pokrzywa\n  (6), Shivam Vedak (7), Adam Paul Yan (3), Dong-han Yao (7), Amy R. Zipursky\n  (3), Christina Dinh (1), Philip Ballentine (1), Dan C. Derieg (1), Vladimir\n  Polony (1), Rehan N. Chawdry (1), Jordan Davies (1), Brigham B. Hyde (1),\n  Nigam H. Shah (1 and 7), Saurabh Gombar (1 and 8) ((1) Atropos Health, New\n  York NY, USA, (2) Department of Medicine, University of California, Los\n  Angeles CA, USA, (3) Department of Pediatrics, The Hospital for Sick\n  Children, Toronto ON, Canada, (4) Department of Emergency Medicine,\n  University of California, San Diego CA, USA, (5) Department of Emergency\n  Medicine, University of Michigan, Ann Arbor MI, USA, (6) Department of\n  Surgery, Columbia University, New York NY, USA, (7) Center for Biomedical\n  Informatics Research, Stanford University, Stanford CA, USA (8) Department of\n  Pathology, Stanford University, Stanford CA, USA)","authorsParsed":[["Low","Yen Sia","","1 and 2"],["Jackson","Michael L.","","1 and 2"],["Hyde","Rebecca J.","","1 and 2"],["Brown","Robert E.","","1 and 2"],["Sanghavi","Neil M.","","1 and 2"],["Baldwin","Julian D.","","1 and 2"],["Pike","C. William","","1 and 2"],["Muralidharan","Jananee","","1 and 2"],["Hui","Gavin","","1 and 2"],["Alexander","Natasha","","1 and 7"],["Hassan","Hadeel","","1 and 7"],["Nene","Rahul V.","","1 and 7"],["Pike","Morgan","","1 and 7"],["Pokrzywa","Courtney J.","","1 and 7"],["Vedak","Shivam","","1 and 7"],["Yan","Adam Paul","","1 and 7"],["Yao","Dong-han","","1 and 7"],["Zipursky","Amy R.","","1 and 7"],["Dinh","Christina","","1 and 7"],["Ballentine","Philip","","1 and 7"],["Derieg","Dan C.","","1 and 7"],["Polony","Vladimir","","1 and 7"],["Chawdry","Rehan N.","","1 and 7"],["Davies","Jordan","","1 and 7"],["Hyde","Brigham B.","","1 and 7"],["Shah","Nigam H.","","1 and 7"],["Gombar","Saurabh","","1 and 8"]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 22:39:20 GMT"}],"updateDate":"2024-07-02","timestamp":1719700760000,"abstract":"  Evidence to guide healthcare decisions is often limited by a lack of relevant\nand trustworthy literature as well as difficulty in contextualizing existing\nresearch for a specific patient. Large language models (LLMs) could potentially\naddress both challenges by either summarizing published literature or\ngenerating new studies based on real-world data (RWD). We evaluated the ability\nof five LLM-based systems in answering 50 clinical questions and had nine\nindependent physicians review the responses for relevance, reliability, and\nactionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus,\nGemini Pro 1.5) rarely produced answers that were deemed relevant and\nevidence-based (2% - 10%). In contrast, retrieval augmented generation\n(RAG)-based and agentic LLM systems produced relevant and evidence-based\nanswers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic\nChatRWD was able to answer novel questions compared to other LLMs (65% vs.\n0-9%). These results suggest that while general-purpose LLMs should not be used\nas-is, a purpose-built system for evidence summarization based on RAG and one\nfor generating novel evidence working synergistically would improve\navailability of pertinent evidence for patient care.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mRxXRnISy34tMxhlL2iY7WL76pHcOdWHZVKGZW5L740","pdfSize":"810766"}
