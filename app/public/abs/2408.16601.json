{"id":"2408.16601","title":"Examination of Code generated by Large Language Models","authors":"Robin Beer, Alexander Feix, Tim Guttzeit, Tamara Muras, Vincent\n  M\\\"uller, Maurice Rauscher, Florian Sch\\\"affler, Welf L\\\"owe","authorsParsed":[["Beer","Robin",""],["Feix","Alexander",""],["Guttzeit","Tim",""],["Muras","Tamara",""],["Müller","Vincent",""],["Rauscher","Maurice",""],["Schäffler","Florian",""],["Löwe","Welf",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 15:12:16 GMT"}],"updateDate":"2024-08-30","timestamp":1724944336000,"abstract":"  Large language models (LLMs), such as ChatGPT and Copilot, are transforming\nsoftware development by automating code generation and, arguably, enable rapid\nprototyping, support education, and boost productivity. Therefore, correctness\nand quality of the generated code should be on par with manually written code.\nTo assess the current state of LLMs in generating correct code of high quality,\nwe conducted controlled experiments with ChatGPT and Copilot: we let the LLMs\ngenerate simple algorithms in Java and Python along with the corresponding unit\ntests and assessed the correctness and the quality (coverage) of the generated\n(test) codes. We observed significant differences between the LLMs, between the\nlanguages, between algorithm and test codes, and over time. The present paper\nreports these results together with the experimental methods allowing repeated\nand comparable assessments for more algorithms, languages, and LLMs over time.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}