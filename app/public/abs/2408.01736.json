{"id":"2408.01736","title":"Can LLMs predict the convergence of Stochastic Gradient Descent?","authors":"Oussama Zekri, Abdelhakim Benechehab, Ievgen Redko","authorsParsed":[["Zekri","Oussama",""],["Benechehab","Abdelhakim",""],["Redko","Ievgen",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 10:35:59 GMT"}],"updateDate":"2024-08-06","timestamp":1722681359000,"abstract":"  Large-language models are notoriously famous for their impressive performance\nacross a wide range of tasks. One surprising example of such impressive\nperformance is a recently identified capacity of LLMs to understand the\ngoverning principles of dynamical systems satisfying the Markovian property. In\nthis paper, we seek to explore this direction further by studying the dynamics\nof stochastic gradient descent in convex and non-convex optimization. By\nleveraging the theoretical link between the SGD and Markov chains, we show a\nremarkable zero-shot performance of LLMs in predicting the local minima to\nwhich SGD converges for previously unseen starting points. On a more general\nlevel, we inquire about the possibility of using LLMs to perform zero-shot\nrandomized trials for larger deep learning models used in practice.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}