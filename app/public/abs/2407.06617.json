{"id":"2407.06617","title":"Mobius: A High Efficient Spatial-Temporal Parallel Training Paradigm for\n  Text-to-Video Generation Task","authors":"Yiran Yang, Jinchao Zhang, Ying Deng, Jie Zhou","authorsParsed":[["Yang","Yiran",""],["Zhang","Jinchao",""],["Deng","Ying",""],["Zhou","Jie",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 07:47:16 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 02:03:17 GMT"},{"version":"v3","created":"Mon, 22 Jul 2024 02:51:07 GMT"},{"version":"v4","created":"Tue, 23 Jul 2024 09:08:47 GMT"}],"updateDate":"2024-07-24","timestamp":1720511236000,"abstract":"  Inspired by the success of the text-to-image (T2I) generation task, many\nresearchers are devoting themselves to the text-to-video (T2V) generation task.\nMost of the T2V frameworks usually inherit from the T2I model and add\nextra-temporal layers of training to generate dynamic videos, which can be\nviewed as a fine-tuning task. However, the traditional 3D-Unet is a serial mode\nand the temporal layers follow the spatial layers, which will result in high\nGPU memory and training time consumption according to its serial feature flow.\nWe believe that this serial mode will bring more training costs with the large\ndiffusion model and massive datasets, which are not environmentally friendly\nand not suitable for the development of the T2V. Therefore, we propose a highly\nefficient spatial-temporal parallel training paradigm for T2V tasks, named\nMobius. In our 3D-Unet, the temporal layers and spatial layers are parallel,\nwhich optimizes the feature flow and backpropagation. The Mobius will save 24%\nGPU memory and 12% training time, which can greatly improve the T2V fine-tuning\ntask and provide a novel insight for the AIGC community. We will release our\ncodes in the future.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}