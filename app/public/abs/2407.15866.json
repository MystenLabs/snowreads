{"id":"2407.15866","title":"SmartQuant: CXL-based AI Model Store in Support of Runtime Configurable\n  Weight Quantization","authors":"Rui Xie, Asad Ul Haq, Linsen Ma, Krystal Sun, Sanchari Sen, Swagath\n  Venkataramani, Liu Liu, Tong Zhang","authorsParsed":[["Xie","Rui",""],["Haq","Asad Ul",""],["Ma","Linsen",""],["Sun","Krystal",""],["Sen","Sanchari",""],["Venkataramani","Swagath",""],["Liu","Liu",""],["Zhang","Tong",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 20:39:49 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 19:44:41 GMT"}],"updateDate":"2024-08-20","timestamp":1721248789000,"abstract":"  Recent studies have revealed that, during the inference on generative AI\nmodels such as transformer, the importance of different weights exhibits\nsubstantial context-dependent variations. This naturally manifests a promising\npotential of adaptively configuring weight quantization to improve the\ngenerative AI inference efficiency. Although configurable weight quantization\ncan readily leverage the hardware support of variable-precision arithmetics in\nmodern GPU and AI accelerators, little prior research has studied how one could\nexploit variable weight quantization to proportionally improve the AI model\nmemory access speed and energy efficiency. Motivated by the rapidly maturing\nCXL ecosystem, this work develops a CXL-based design solution to fill this gap.\nThe key is to allow CXL memory controllers play an active role in supporting\nand exploiting runtime configurable weight quantization. Using transformer as a\nrepresentative generative AI model, we carried out experiments that well\ndemonstrate the effectiveness of the proposed design solution.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Hardware Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}