{"id":"2407.06135","title":"ANOLE: An Open, Autoregressive, Native Large Multimodal Models for\n  Interleaved Image-Text Generation","authors":"Ethan Chern, Jiadi Su, Yan Ma, Pengfei Liu","authorsParsed":[["Chern","Ethan",""],["Su","Jiadi",""],["Ma","Yan",""],["Liu","Pengfei",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:08:02 GMT"}],"updateDate":"2024-07-09","timestamp":1720458482000,"abstract":"  Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}