{"id":"2408.08794","title":"Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking\n  Transformers","authors":"Zihang Song, Prabodh Katti, Osvaldo Simeone, Bipin Rajendran","authorsParsed":[["Song","Zihang",""],["Katti","Prabodh",""],["Simeone","Osvaldo",""],["Rajendran","Bipin",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 15:07:54 GMT"}],"updateDate":"2024-08-19","timestamp":1723820874000,"abstract":"  This paper introduces Xpikeformer, a hybrid analog-digital hardware\narchitecture designed to accelerate spiking neural network (SNN)-based\ntransformer models. By combining the energy efficiency and temporal dynamics of\nSNNs with the powerful sequence modeling capabilities of transformers,\nXpikeformer leverages mixed analog-digital computing techniques to enhance\nperformance and energy efficiency. The architecture integrates analog in-memory\ncomputing (AIMC) for feedforward and fully connected layers, and a stochastic\nspiking attention (SSA) engine for efficient attention mechanisms. We detail\nthe design, implementation, and evaluation of Xpikeformer, demonstrating\nsignificant improvements in energy consumption and computational efficiency.\nThrough an image classification task and a wireless communication symbol\ndetection task, we show that Xpikeformer can achieve software-comparable\ninference accuracy. Energy evaluations reveal that Xpikeformer achieves up to a\n$17.8$--$19.2\\times$ reduction in energy consumption compared to\nstate-of-the-art digital ANN transformers and up to a $5.9$--$6.8\\times$\nreduction compared to fully digital SNN transformers. Xpikeformer also achieves\na $12.0\\times$ speedup compared to the GPU implementation of spiking\ntransformers.\n","subjects":["Computing Research Repository/Hardware Architecture"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}