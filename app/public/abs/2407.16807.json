{"id":"2407.16807","title":"In Search for Architectures and Loss Functions in Multi-Objective\n  Reinforcement Learning","authors":"Mikhail Terekhov, Caglar Gulcehre","authorsParsed":[["Terekhov","Mikhail",""],["Gulcehre","Caglar",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 19:17:47 GMT"}],"updateDate":"2024-07-25","timestamp":1721762267000,"abstract":"  Multi-objective reinforcement learning (MORL) is essential for addressing the\nintricacies of real-world RL problems, which often require trade-offs between\nmultiple utility functions. However, MORL is challenging due to unstable\nlearning dynamics with deep learning-based function approximators. The research\npath most taken has been to explore different value-based loss functions for\nMORL to overcome this issue. Our work empirically explores model-free policy\nlearning loss functions and the impact of different architectural choices. We\nintroduce two different approaches: Multi-objective Proximal Policy\nOptimization (MOPPO), which extends PPO to MORL, and Multi-objective Advantage\nActor Critic (MOA2C), which acts as a simple baseline in our ablations. Our\nproposed approach is straightforward to implement, requiring only small\nmodifications at the level of function approximator. We conduct comprehensive\nevaluations on the MORL Deep Sea Treasure, Minecart, and Reacher environments\nand show that MOPPO effectively captures the Pareto front. Our extensive\nablation studies and empirical analyses reveal the impact of different\narchitectural choices, underscoring the robustness and versatility of MOPPO\ncompared to popular MORL approaches like Pareto Conditioned Networks (PCN) and\nEnvelope Q-learning in terms of MORL metrics, including hypervolume and\nexpected utility.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d0XIkkLkG5Z1eIh4qkc4uNHuwkVOdRaGx4Op2tWoPuE","pdfSize":"1386966","objectId":"0xc9908de64e3eba487dd46bfdf6debab2430529ec2a0130050f162d9e2d331b98","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
