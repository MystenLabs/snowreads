{"id":"2408.07654","title":"Graph Triple Attention Network: A Decoupled Perspective","authors":"Xiaotang Wang, Yun Zhu, Haizhou Shi, Yongchao Liu, Chuntao Hong","authorsParsed":[["Wang","Xiaotang",""],["Zhu","Yun",""],["Shi","Haizhou",""],["Liu","Yongchao",""],["Hong","Chuntao",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 16:29:07 GMT"}],"updateDate":"2024-08-15","timestamp":1723652947000,"abstract":"  Graph Transformers (GTs) have recently achieved significant success in the\ngraph domain by effectively capturing both long-range dependencies and graph\ninductive biases. However, these methods face two primary challenges: (1)\nmulti-view chaos, which results from coupling multi-view information\n(positional, structural, attribute), thereby impeding flexible usage and the\ninterpretability of the propagation process. (2) local-global chaos, which\narises from coupling local message passing with global attention, leading to\nissues of overfitting and over-globalizing. To address these challenges, we\npropose a high-level decoupled perspective of GTs, breaking them down into\nthree components and two interaction levels: positional attention, structural\nattention, and attribute attention, alongside local and global interaction.\nBased on this decoupled perspective, we design a decoupled graph triple\nattention network named DeGTA, which separately computes multi-view attentions\nand adaptively integrates multi-view local and global information. This\napproach offers three key advantages: enhanced interpretability, flexible\ndesign, and adaptive integration of local and global information. Through\nextensive experiments, DeGTA achieves state-of-the-art performance across\nvarious datasets and tasks, including node classification and graph\nclassification. Comprehensive ablation studies demonstrate that decoupling is\nessential for improving performance and enhancing interpretability. Our code is\navailable at: https://github.com/wangxiaotang0906/DeGTA\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}