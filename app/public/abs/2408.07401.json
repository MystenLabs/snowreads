{"id":"2408.07401","title":"DataVisT5: A Pre-trained Language Model for Jointly Understanding Text\n  and Data Visualization","authors":"Zhuoyue Wan, Yuanfeng Song, Shuaimin Li, Chen Jason Zhang, Raymond\n  Chi-Wing Wong","authorsParsed":[["Wan","Zhuoyue",""],["Song","Yuanfeng",""],["Li","Shuaimin",""],["Zhang","Chen Jason",""],["Wong","Raymond Chi-Wing",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 09:20:17 GMT"}],"updateDate":"2024-08-15","timestamp":1723627217000,"abstract":"  Data visualization (DV) is the fundamental and premise tool to improve the\nefficiency in conveying the insights behind the big data, which has been widely\naccepted in existing data-driven world. Task automation in DV, such as\nconverting natural language queries to visualizations (i.e., text-to-vis),\ngenerating explanations from visualizations (i.e., vis-to-text), answering\nDV-related questions in free form (i.e. FeVisQA), and explicating tabular data\n(i.e., table-to-text), is vital for advancing the field. Despite their\npotential, the application of pre-trained language models (PLMs) like T5 and\nBERT in DV has been limited by high costs and challenges in handling\ncross-modal information, leading to few studies on PLMs for DV. We introduce\n\\textbf{DataVisT5}, a novel PLM tailored for DV that enhances the T5\narchitecture through a hybrid objective pre-training and multi-task fine-tuning\nstrategy, integrating text and DV datasets to effectively interpret cross-modal\nsemantics. Extensive evaluations on public datasets show that DataVisT5\nconsistently outperforms current state-of-the-art models on various DV-related\ntasks. We anticipate that DataVisT5 will not only inspire further research on\nvertical PLMs but also expand the range of applications for PLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Databases"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}