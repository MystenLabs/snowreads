{"id":"2408.10948","title":"GAIM: Attacking Graph Neural Networks via Adversarial Influence\n  Maximization","authors":"Xiaodong Yang, Xiaoting Li, Huiyuan Chen, Yiwei Cai","authorsParsed":[["Yang","Xiaodong",""],["Li","Xiaoting",""],["Chen","Huiyuan",""],["Cai","Yiwei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 15:41:20 GMT"}],"updateDate":"2024-08-21","timestamp":1724168480000,"abstract":"  Recent studies show that well-devised perturbations on graph structures or\nnode features can mislead trained Graph Neural Network (GNN) models. However,\nthese methods often overlook practical assumptions, over-rely on heuristics, or\nseparate vital attack components. In response, we present GAIM, an integrated\nadversarial attack method conducted on a node feature basis while considering\nthe strict black-box setting. Specifically, we define an adversarial influence\nfunction to theoretically assess the adversarial impact of node perturbations,\nthereby reframing the GNN attack problem into the adversarial influence\nmaximization problem. In our approach, we unify the selection of the target\nnode and the construction of feature perturbations into a single optimization\nproblem, ensuring a unique and consistent feature perturbation for each target\nnode. We leverage a surrogate model to transform this problem into a solvable\nlinear programming task, streamlining the optimization process. Moreover, we\nextend our method to accommodate label-oriented attacks, broadening its\napplicability. Thorough evaluations on five benchmark datasets across three\npopular models underscore the effectiveness of our method in both untargeted\nand label-oriented targeted attacks. Through comprehensive analysis and\nablation studies, we demonstrate the practical value and efficacy inherent to\nour design choices.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}