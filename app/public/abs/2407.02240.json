{"id":"2407.02240","title":"MALT Powers Up Adversarial Attacks","authors":"Odelia Melamed, Gilad Yehudai, Adi Shamir","authorsParsed":[["Melamed","Odelia",""],["Yehudai","Gilad",""],["Shamir","Adi",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 13:02:12 GMT"}],"updateDate":"2024-07-03","timestamp":1719925332000,"abstract":"  Current adversarial attacks for multi-class classifiers choose the target\nclass for a given input naively, based on the classifier's confidence levels\nfor various target classes. We present a novel adversarial targeting method,\n\\textit{MALT - Mesoscopic Almost Linearity Targeting}, based on medium-scale\nalmost linearity assumptions. Our attack wins over the current state of the art\nAutoAttack on the standard benchmark datasets CIFAR-100 and ImageNet and for a\nvariety of robust models. In particular, our attack is \\emph{five times faster}\nthan AutoAttack, while successfully matching all of AutoAttack's successes and\nattacking additional samples that were previously out of reach. We then prove\nformally and demonstrate empirically that our targeting method, although\ninspired by linear predictors, also applies to standard non-linear models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security","Computing Research Repository/Neural and Evolutionary Computing","Statistics/Machine Learning"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}