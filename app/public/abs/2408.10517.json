{"id":"2408.10517","title":"Integrating Multi-Modal Input Token Mixer Into Mamba-Based Decision\n  Models: Decision MetaMamba","authors":"Wall Kim","authorsParsed":[["Kim","Wall",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 03:35:28 GMT"}],"updateDate":"2024-08-21","timestamp":1724124928000,"abstract":"  Return-Conditioned Transformer Decision Models (RCTDM) have demonstrated the\npotential to enhance transformer performance in offline reinforcement learning\nby replacing rewards in the input sequence with returns-to-go. However, to\nachieve the goal of learning an optimal policy from offline datasets composed\nof limited suboptimal trajectories, RCTDM required alternative methods. One\nprominent approach, trajectory stitching, was designed to enable the network to\ncombine multiple trajectories to find the optimal path. To implement this using\nonly transformers without auxiliary networks, it was necessary to shorten the\ninput sequence length to better capture the Markov property in reinforcement\nlearnings. This, however, introduced a trade-off, as it reduced the accuracy of\naction inference. Our study introduces a model named Decision MetaMamba to\nresolve these challenges. DMM employs an input token mixer to extract patterns\nfrom short sequences and uses a State Space Model (SSM) to selectively combine\ninformation from relatively distant sequences. Inspired by Metaformer, this\nstructure was developed by transforming Mamba's input layer into various\nmulti-modal layers. Fortunately, with the advent of Mamba, implemented using\nparallel selective scanning, we achieved a high-performance sequence model\ncapable of replacing transformers. Based on these innovations, DMM demonstrated\nexcellent performance across various datasets in offline RL, confirming that\nmodels using SSM can improve performance by domain-specific alterations of the\ninput layer. Additionally, it maintained its performance even in lightweight\nmodels with fewer parameters. These results suggest that decision models based\non SSM can pave the way for improved outcomes in future developments.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}