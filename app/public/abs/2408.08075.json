{"id":"2408.08075","title":"Independent Policy Mirror Descent for Markov Potential Games: Scaling to\n  Large Number of Players","authors":"Pragnya Alatur, Anas Barakat, Niao He","authorsParsed":[["Alatur","Pragnya",""],["Barakat","Anas",""],["He","Niao",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 11:02:05 GMT"}],"updateDate":"2024-08-16","timestamp":1723719725000,"abstract":"  Markov Potential Games (MPGs) form an important sub-class of Markov games,\nwhich are a common framework to model multi-agent reinforcement learning\nproblems. In particular, MPGs include as a special case the identical-interest\nsetting where all the agents share the same reward function. Scaling the\nperformance of Nash equilibrium learning algorithms to a large number of agents\nis crucial for multi-agent systems. To address this important challenge, we\nfocus on the independent learning setting where agents can only have access to\ntheir local information to update their own policy. In prior work on MPGs, the\niteration complexity for obtaining $\\epsilon$-Nash regret scales linearly with\nthe number of agents $N$. In this work, we investigate the iteration complexity\nof an independent policy mirror descent (PMD) algorithm for MPGs. We show that\nPMD with KL regularization, also known as natural policy gradient, enjoys a\nbetter $\\sqrt{N}$ dependence on the number of agents, improving over PMD with\nEuclidean regularization and prior work. Furthermore, the iteration complexity\nis also independent of the sizes of the agents' action spaces.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Science and Game Theory","Computing Research Repository/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/"}