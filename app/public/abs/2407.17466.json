{"id":"2407.17466","title":"Traversing Pareto Optimal Policies: Provably Efficient Multi-Objective\n  Reinforcement Learning","authors":"Shuang Qiu, Dake Zhang, Rui Yang, Boxiang Lyu, Tong Zhang","authorsParsed":[["Qiu","Shuang",""],["Zhang","Dake",""],["Yang","Rui",""],["Lyu","Boxiang",""],["Zhang","Tong",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:58:49 GMT"}],"updateDate":"2024-07-25","timestamp":1721843929000,"abstract":"  This paper investigates multi-objective reinforcement learning (MORL), which\nfocuses on learning Pareto optimal policies in the presence of multiple reward\nfunctions. Despite MORL's significant empirical success, there is still a lack\nof satisfactory understanding of various MORL optimization targets and\nefficient learning algorithms. Our work offers a systematic analysis of several\noptimization targets to assess their abilities to find all Pareto optimal\npolicies and controllability over learned policies by the preferences for\ndifferent objectives. We then identify Tchebycheff scalarization as a favorable\nscalarization method for MORL. Considering the non-smoothness of Tchebycheff\nscalarization, we reformulate its minimization problem into a new min-max-max\noptimization problem. Then, for the stochastic policy class, we propose\nefficient algorithms using this reformulation to learn Pareto optimal policies.\nWe first propose an online UCB-based algorithm to achieve an $\\varepsilon$\nlearning error with an $\\tilde{\\mathcal{O}}(\\varepsilon^{-2})$ sample\ncomplexity for a single given preference. To further reduce the cost of\nenvironment exploration under different preferences, we propose a\npreference-free framework that first explores the environment without\npre-defined preferences and then generates solutions for any number of\npreferences. We prove that it only requires an\n$\\tilde{\\mathcal{O}}(\\varepsilon^{-2})$ exploration complexity in the\nexploration phase and demands no additional exploration afterward. Lastly, we\nanalyze the smooth Tchebycheff scalarization, an extension of Tchebycheff\nscalarization, which is proved to be more advantageous in distinguishing the\nPareto optimal policies from other weakly Pareto optimal policies based on\nentry values of preference vectors. Furthermore, we extend our algorithms and\ntheoretical analysis to accommodate this optimization target.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}