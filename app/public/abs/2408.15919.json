{"id":"2408.15919","title":"DeMoBot: Deformable Mobile Manipulation with Vision-based Sub-goal\n  Retrieval","authors":"Yuying Zhang, Wenyan Yang, Joni Pajarinen","authorsParsed":[["Zhang","Yuying",""],["Yang","Wenyan",""],["Pajarinen","Joni",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 16:33:21 GMT"}],"updateDate":"2024-08-29","timestamp":1724862801000,"abstract":"  Imitation learning (IL) algorithms typically distill experience into\nparametric behavior policies to mimic expert demonstrations. Despite their\neffectiveness, previous methods often struggle with data efficiency and\naccurately aligning the current state with expert demonstrations, especially in\ndeformable mobile manipulation tasks characterized by partial observations and\ndynamic object deformations. In this paper, we introduce \\textbf{DeMoBot}, a\nnovel IL approach that directly retrieves observations from demonstrations to\nguide robots in \\textbf{De}formable \\textbf{Mo}bile manipulation tasks. DeMoBot\nutilizes vision foundation models to identify relevant expert data based on\nvisual similarity and matches the current trajectory with demonstrated\ntrajectories using trajectory similarity and forward reachability constraints\nto select suitable sub-goals. Once a goal is determined, a motion generation\npolicy will guide the robot to the next state until the task is completed. We\nevaluated DeMoBot using a Spot robot in several simulated and real-world\nsettings, demonstrating its effectiveness and generalizability. With only 20\ndemonstrations, DeMoBot significantly outperforms the baselines, reaching a\n50\\% success rate in curtain opening and 85\\% in gap covering in simulation.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"85-QQYI7Ot52dbt90GwjPhPNSUFSTQtsCKyiM49-Wac","pdfSize":"9642538"}
