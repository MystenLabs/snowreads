{"id":"2407.09499","title":"Self-Consuming Generative Models with Curated Data Provably Optimize\n  Human Preferences","authors":"Damien Ferbach, Quentin Bertrand, Avishek Joey Bose, Gauthier Gidel","authorsParsed":[["Ferbach","Damien",""],["Bertrand","Quentin",""],["Bose","Avishek Joey",""],["Gidel","Gauthier",""]],"versions":[{"version":"v1","created":"Wed, 12 Jun 2024 21:28:28 GMT"}],"updateDate":"2024-07-16","timestamp":1718227708000,"abstract":"  The rapid progress in generative models has resulted in impressive leaps in\ngeneration quality, blurring the lines between synthetic and real data.\nWeb-scale datasets are now prone to the inevitable contamination by synthetic\ndata, directly impacting the training of future generated models. Already, some\ntheoretical results on self-consuming generative models (a.k.a., iterative\nretraining) have emerged in the literature, showcasing that either model\ncollapse or stability could be possible depending on the fraction of generated\ndata used at each retraining step. However, in practice, synthetic data is\noften subject to human feedback and curated by users before being used and\nuploaded online. For instance, many interfaces of popular text-to-image\ngenerative models, such as Stable Diffusion or Midjourney, produce several\nvariations of an image for a given query which can eventually be curated by the\nusers. In this paper, we theoretically study the impact of data curation on\niterated retraining of generative models and show that it can be seen as an\n\\emph{implicit preference optimization mechanism}. However, unlike standard\npreference optimization, the generative model does not have access to the\nreward function or negative samples needed for pairwise comparisons. Moreover,\nour study doesn't require access to the density function, only to samples. We\nprove that, if the data is curated according to a reward model, then the\nexpected reward of the iterative retraining procedure is maximized. We further\nprovide theoretical results on the stability of the retraining loop when using\na positive fraction of real data at each step. Finally, we conduct illustrative\nexperiments on both synthetic datasets and on CIFAR10 showing that such a\nprocedure amplifies biases of the reward model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}