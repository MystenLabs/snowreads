{"id":"2408.13850","title":"Condensed Sample-Guided Model Inversion for Knowledge Distillation","authors":"Kuluhan Binici, Shivam Aggarwal, Cihan Acar, Nam Trung Pham, Karianto\n  Leman, Gim Hee Lee, Tulika Mitra","authorsParsed":[["Binici","Kuluhan",""],["Aggarwal","Shivam",""],["Acar","Cihan",""],["Pham","Nam Trung",""],["Leman","Karianto",""],["Lee","Gim Hee",""],["Mitra","Tulika",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 14:43:27 GMT"}],"updateDate":"2024-08-27","timestamp":1724597007000,"abstract":"  Knowledge distillation (KD) is a key element in neural network compression\nthat allows knowledge transfer from a pre-trained teacher model to a more\ncompact student model. KD relies on access to the training dataset, which may\nnot always be fully available due to privacy concerns or logistical issues\nrelated to the size of the data. To address this, \"data-free\" KD methods use\nsynthetic data, generated through model inversion, to mimic the target data\ndistribution. However, conventional model inversion methods are not designed to\nutilize supplementary information from the target dataset, and thus, cannot\nleverage it to improve performance, even when it is available. In this paper,\nwe consider condensed samples, as a form of supplementary information, and\nintroduce a method for using them to better approximate the target data\ndistribution, thereby enhancing the KD performance. Our approach is versatile,\nevidenced by improvements of up to 11.4% in KD accuracy across various datasets\nand model inversion-based methods. Importantly, it remains effective even when\nusing as few as one condensed sample per class, and can also enhance\nperformance in few-shot scenarios where only limited real data samples are\navailable.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}