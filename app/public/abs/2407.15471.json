{"id":"2407.15471","title":"Convergence of the Iterates for Momentum and RMSProp for Local Smooth\n  Functions: Adaptation is the Key","authors":"Bilel Bensaid (CEA-CESTA, IMB), Ga\\\"el Po\\\"ette (CEA-CESTA), Rodolphe\n  Turpault (IMB)","authorsParsed":[["Bensaid","Bilel","","CEA-CESTA, IMB"],["Poëtte","Gaël","","CEA-CESTA"],["Turpault","Rodolphe","","IMB"]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 08:32:44 GMT"}],"updateDate":"2024-07-23","timestamp":1721637164000,"abstract":"  Both accelerated and adaptive gradient methods are among state of the art\nalgorithms to train neural networks. The tuning of hyperparameters is needed to\nmake them work efficiently. For classical gradient descent, a general and\nefficient way to adapt hyperparameters is the Armijo backtracking. The goal of\nthis work is to generalize the Armijo linesearch to Momentum and RMSProp, two\npopular optimizers of this family, by means of stability theory of dynamical\nsystems. We establish convergence results, under the Lojasiewicz assumption,\nfor these strategies. As a direct result, we obtain the first guarantee on the\nconvergence of the iterates for RMSProp, in the non-convex setting without the\nclassical bounded assumptions.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}