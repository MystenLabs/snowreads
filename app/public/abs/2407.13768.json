{"id":"2407.13768","title":"Addressing Imbalance for Class Incremental Learning in Medical Image\n  Classification","authors":"Xuze Hao, Wenqian Ni, Xuhao Jiang, Weimin Tan, Bo Yan","authorsParsed":[["Hao","Xuze",""],["Ni","Wenqian",""],["Jiang","Xuhao",""],["Tan","Weimin",""],["Yan","Bo",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:59:44 GMT"}],"updateDate":"2024-07-23","timestamp":1721325584000,"abstract":"  Deep convolutional neural networks have made significant breakthroughs in\nmedical image classification, under the assumption that training samples from\nall classes are simultaneously available. However, in real-world medical\nscenarios, there's a common need to continuously learn about new diseases,\nleading to the emerging field of class incremental learning (CIL) in the\nmedical domain. Typically, CIL suffers from catastrophic forgetting when\ntrained on new classes. This phenomenon is mainly caused by the imbalance\nbetween old and new classes, and it becomes even more challenging with\nimbalanced medical datasets. In this work, we introduce two simple yet\neffective plug-in methods to mitigate the adverse effects of the imbalance.\nFirst, we propose a CIL-balanced classification loss to mitigate the classifier\nbias toward majority classes via logit adjustment. Second, we propose a\ndistribution margin loss that not only alleviates the inter-class overlap in\nembedding space but also enforces the intra-class compactness. We evaluate the\neffectiveness of our method with extensive experiments on three benchmark\ndatasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our\napproach outperforms state-of-the-art methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}