{"id":"2408.00264","title":"Clover-2: Accurate Inference for Regressive Lightweight Speculative\n  Decoding","authors":"Bin Xiao, Lujun Gui, Lei Su, Weipeng Chen","authorsParsed":[["Xiao","Bin",""],["Gui","Lujun",""],["Su","Lei",""],["Chen","Weipeng",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 03:43:32 GMT"}],"updateDate":"2024-08-02","timestamp":1722483812000,"abstract":"  Large Language Models (LLMs) frequently suffer from inefficiencies, largely\nattributable to the discord between the requirements of auto-regressive\ndecoding and the architecture of contemporary GPUs. Recently, regressive\nlightweight speculative decoding has garnered attention for its notable\nefficiency improvements in text generation tasks. This approach utilizes a\nlightweight regressive draft model, like a Recurrent Neural Network (RNN) or a\nsingle transformer decoder layer, leveraging sequential information to\niteratively predict potential tokens. Specifically, RNN draft models are\ncomputationally economical but tend to deliver lower accuracy, while attention\ndecoder layer models exhibit the opposite traits. This paper presents Clover-2,\nan advanced iteration of Clover, an RNN-based draft model designed to achieve\ncomparable accuracy to that of attention decoder layer models while maintaining\nminimal computational overhead. Clover-2 enhances the model architecture and\nincorporates knowledge distillation to increase Clover's accuracy and improve\noverall efficiency. We conducted experiments using the open-source Vicuna 7B\nand LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses\nexisting methods across various model architectures, showcasing its efficacy\nand robustness.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}