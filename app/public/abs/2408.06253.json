{"id":"2408.06253","title":"Learning in Time-Varying Monotone Network Games with Dynamic Populations","authors":"Feras Al Taha, Kiran Rokade, Francesca Parise","authorsParsed":[["Taha","Feras Al",""],["Rokade","Kiran",""],["Parise","Francesca",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 16:03:13 GMT"}],"updateDate":"2024-08-13","timestamp":1723478593000,"abstract":"  In this paper, we present a framework for multi-agent learning in a\nnonstationary dynamic network environment. More specifically, we examine\nprojected gradient play in smooth monotone repeated network games in which the\nagents' participation and connectivity vary over time. We model this changing\nsystem with a stochastic network which takes a new independent realization at\neach repetition. We show that the strategy profile learned by the agents\nthrough projected gradient dynamics over the sequence of network realizations\nconverges to a Nash equilibrium of the game in which players minimize their\nexpected cost, almost surely and in the mean-square sense. We then show that\nthe learned strategy profile is an almost Nash equilibrium of the game played\nby the agents at each stage of the repeated game with high probability. Using\nthese two results, we derive non-asymptotic bounds on the regret incurred by\nthe agents.\n","subjects":["Computing Research Repository/Computer Science and Game Theory","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control","Mathematics/Dynamical Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}