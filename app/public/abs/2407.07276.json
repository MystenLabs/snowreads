{"id":"2407.07276","title":"Exploring Camera Encoder Designs for Autonomous Driving Perception","authors":"Barath Lakshmanan, Joshua Chen, Shiyi Lan, Maying Shen, Zhiding Yu,\n  Jose M. Alvarez","authorsParsed":[["Lakshmanan","Barath",""],["Chen","Joshua",""],["Lan","Shiyi",""],["Shen","Maying",""],["Yu","Zhiding",""],["Alvarez","Jose M.",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 23:44:58 GMT"}],"updateDate":"2024-07-11","timestamp":1720568698000,"abstract":"  The cornerstone of autonomous vehicles (AV) is a solid perception system,\nwhere camera encoders play a crucial role. Existing works usually leverage\npre-trained Convolutional Neural Networks (CNN) or Vision Transformers (ViTs)\ndesigned for general vision tasks, such as image classification, segmentation,\nand 2D detection. Although those well-known architectures have achieved\nstate-of-the-art accuracy in AV-related tasks, e.g., 3D Object Detection, there\nremains significant potential for improvement in network design due to the\nnuanced complexities of industrial-level AV dataset. Moreover, existing public\nAV benchmarks usually contain insufficient data, which might lead to inaccurate\nevaluation of those architectures.To reveal the AV-specific model insights, we\nstart from a standard general-purpose encoder, ConvNeXt and progressively\ntransform the design. We adjust different design parameters including width and\ndepth of the model, stage compute ratio, attention mechanisms, and input\nresolution, supported by systematic analysis to each modifications. This\ncustomization yields an architecture optimized for AV camera encoder achieving\n8.79% mAP improvement over the baseline. We believe our effort could become a\nsweet cookbook of image encoders for AV and pave the way to the next-level\ndrive system.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}