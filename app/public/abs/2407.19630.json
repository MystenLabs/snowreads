{"id":"2407.19630","title":"LLMs' Understanding of Natural Language Revealed","authors":"Walid S. Saba","authorsParsed":[["Saba","Walid S.",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 01:21:11 GMT"},{"version":"v2","created":"Fri, 2 Aug 2024 11:26:12 GMT"}],"updateDate":"2024-08-05","timestamp":1722216071000,"abstract":"  Large language models (LLMs) are the result of a massive experiment in\nbottom-up, data-driven reverse engineering of language at scale. Despite their\nutility in a number of downstream NLP tasks, ample research has shown that LLMs\nare incapable of performing reasoning in tasks that require quantification over\nand the manipulation of symbolic variables (e.g., planning and problem\nsolving); see for example [25][26]. In this document, however, we will focus on\ntesting LLMs for their language understanding capabilities, their supposed\nforte. As we will show here, the language understanding capabilities of LLMs\nhave been widely exaggerated. While LLMs have proven to generate human-like\ncoherent language (since that's how they were designed), their language\nunderstanding capabilities have not been properly tested. In particular, we\nbelieve that the language understanding capabilities of LLMs should be tested\nby performing an operation that is the opposite of 'text generation' and\nspecifically by giving the LLM snippets of text as input and then querying what\nthe LLM \"understood\". As we show here, when doing so it will become apparent\nthat LLMs do not truly understand language, beyond very superficial inferences\nthat are essentially the byproduct of the memorization of massive amounts of\ningested text.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Q_ZTwuEJD0jealIi76k2_f0KIiTF1cbAiyPYZwTSqOg","pdfSize":"637106"}
