{"id":"2408.16221","title":"SSDM: Scalable Speech Dysfluency Modeling","authors":"Jiachen Lian, Xuanru Zhou, Zoe Ezzes, Jet Vonk, Brittany Morin, David\n  Baquirin, Zachary Mille, Maria Luisa Gorno Tempini, Gopala Anumanchipalli","authorsParsed":[["Lian","Jiachen",""],["Zhou","Xuanru",""],["Ezzes","Zoe",""],["Vonk","Jet",""],["Morin","Brittany",""],["Baquirin","David",""],["Mille","Zachary",""],["Tempini","Maria Luisa Gorno",""],["Anumanchipalli","Gopala",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 02:35:53 GMT"},{"version":"v2","created":"Sat, 14 Sep 2024 20:57:08 GMT"}],"updateDate":"2024-09-17","timestamp":1724898953000,"abstract":"  Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions suffer from poor scalability. Second, there is a\nlack of a large-scale dysfluency corpus. Third, there is not an effective\nlearning framework. In this paper, we propose \\textit{SSDM: Scalable Speech\nDysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced\nalignment; (2) introduces connectionist subsequence aligner (CSA) to achieve\ndysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus\ncalled Libri-Dys; and (4) develops an end-to-end system by leveraging the power\nof large language models (LLMs). We expect SSDM to serve as a standard in the\narea of dysfluency modeling. Demo is available at\n\\url{https://eureka235.github.io}.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kuucV4Fxn4Glew9LhwvFouYUzaixa_8UAv7WROB2HhI","pdfSize":"9318456"}
