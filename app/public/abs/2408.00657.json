{"id":"2408.00657","title":"Disentangling Dense Embeddings with Sparse Autoencoders","authors":"Charles O'Neill, Christine Ye, Kartheik Iyer, John F. Wu","authorsParsed":[["O'Neill","Charles",""],["Ye","Christine",""],["Iyer","Kartheik",""],["Wu","John F.",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 15:46:22 GMT"},{"version":"v2","created":"Mon, 5 Aug 2024 03:25:01 GMT"}],"updateDate":"2024-08-06","timestamp":1722527182000,"abstract":"  Sparse autoencoders (SAEs) have shown promise in extracting interpretable\nfeatures from complex neural networks. We present one of the first applications\nof SAEs to dense text embeddings from large language models, demonstrating\ntheir effectiveness in disentangling semantic concepts. By training SAEs on\nembeddings of over 420,000 scientific paper abstracts from computer science and\nastronomy, we show that the resulting sparse representations maintain semantic\nfidelity while offering interpretability. We analyse these learned features,\nexploring their behaviour across different model capacities and introducing a\nnovel method for identifying ``feature families'' that represent related\nconcepts at varying levels of abstraction. To demonstrate the practical utility\nof our approach, we show how these interpretable features can be used to\nprecisely steer semantic search, allowing for fine-grained control over query\nsemantics. This work bridges the gap between the semantic richness of dense\nembeddings and the interpretability of sparse representations. We open source\nour embeddings, trained sparse autoencoders, and interpreted features, as well\nas a web app for exploring them.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}