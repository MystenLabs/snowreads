{"id":"2407.00215","title":"LLM Critics Help Catch LLM Bugs","authors":"Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia\n  Nitishinskaya, Maja Trebacz, Jan Leike","authorsParsed":[["McAleese","Nat",""],["Pokorny","Rai Michael",""],["Uribe","Juan Felipe Ceron",""],["Nitishinskaya","Evgenia",""],["Trebacz","Maja",""],["Leike","Jan",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 19:53:17 GMT"}],"updateDate":"2024-07-02","timestamp":1719604397000,"abstract":"  Reinforcement learning from human feedback (RLHF) is fundamentally limited by\nthe capacity of humans to correctly evaluate model output. To improve human\nevaluation ability and overcome that limitation this work trains \"critic\"\nmodels that help humans to more accurately evaluate model-written code. These\ncritics are themselves LLMs trained with RLHF to write natural language\nfeedback highlighting problems in code from real-world assistant tasks. On code\ncontaining naturally occurring LLM errors model-written critiques are preferred\nover human critiques in 63% of cases, and human evaluation finds that models\ncatch more bugs than human contractors paid for code review. We further confirm\nthat our fine-tuned LLM critics can successfully identify hundreds of errors in\nChatGPT training data rated as \"flawless\", even though the majority of those\ntasks are non-code tasks and thus out-of-distribution for the critic model.\nCritics can have limitations of their own, including hallucinated bugs that\ncould mislead humans into making mistakes they might have otherwise avoided,\nbut human-machine teams of critics and contractors catch similar numbers of\nbugs to LLM critics while hallucinating less than LLMs alone.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}