{"id":"2408.03029","title":"Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning","authors":"Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, Tze-Yun Leong","authorsParsed":[["Ma","Haozhe",""],["Luo","Zhengding",""],["Vo","Thanh Vinh",""],["Sima","Kuankuan",""],["Leong","Tze-Yun",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 08:22:16 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 05:59:46 GMT"}],"updateDate":"2024-08-08","timestamp":1722932536000,"abstract":"  Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to relevant baselines.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}