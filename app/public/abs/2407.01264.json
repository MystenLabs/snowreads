{"id":"2407.01264","title":"SignCLIP: Connecting Text and Sign Language by Contrastive Learning","authors":"Zifan Jiang, Gerard Sant, Amit Moryossef, Mathias M\\\"uller, Rico\n  Sennrich, Sarah Ebling","authorsParsed":[["Jiang","Zifan",""],["Sant","Gerard",""],["Moryossef","Amit",""],["MÃ¼ller","Mathias",""],["Sennrich","Rico",""],["Ebling","Sarah",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 13:17:35 GMT"}],"updateDate":"2024-07-02","timestamp":1719839855000,"abstract":"  We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image\nPretraining) to project spoken language text and sign language videos, two\nclasses of natural languages of distinct modalities, into the same space.\nSignCLIP is an efficient method of learning useful visual representations for\nsign language processing from large-scale, multilingual video-text pairs,\nwithout directly optimizing for a specific task or sign language which is often\nof limited size.\n  We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary\nconsisting of ~500 thousand video clips in up to 44 sign languages, and\nevaluate it with various downstream datasets. SignCLIP discerns in-domain\nsigning with notable text-to-video/video-to-text retrieval accuracy. It also\nperforms competitively for out-of-domain downstream tasks such as isolated sign\nlanguage recognition upon essential few-shot prompting or fine-tuning.\n  We analyze the latent space formed by the spoken language text and sign\nlanguage poses, which provides additional linguistic insights. Our code and\nmodels are openly available.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}