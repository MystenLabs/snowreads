{"id":"2408.07147","title":"Controlling the World by Sleight of Hand","authors":"Sruthi Sudhakar, Ruoshi Liu, Basile Van Hoorick, Carl Vondrick,\n  Richard Zemel","authorsParsed":[["Sudhakar","Sruthi",""],["Liu","Ruoshi",""],["Van Hoorick","Basile",""],["Vondrick","Carl",""],["Zemel","Richard",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 18:33:45 GMT"}],"updateDate":"2024-08-15","timestamp":1723574025000,"abstract":"  Humans naturally build mental models of object interactions and dynamics,\nallowing them to imagine how their surroundings will change if they take a\ncertain action. While generative models today have shown impressive results on\ngenerating/editing images unconditionally or conditioned on text, current\nmethods do not provide the ability to perform object manipulation conditioned\non actions, an important tool for world modeling and action planning.\nTherefore, we propose to learn an action-conditional generative models by\nlearning from unlabeled videos of human hands interacting with objects. The\nvast quantity of such data on the internet allows for efficient scaling which\ncan enable high-performing action-conditional models. Given an image, and the\nshape/location of a desired hand interaction, CosHand, synthesizes an image of\na future after the interaction has occurred. Experiments show that the\nresulting model can predict the effects of hand-object interactions well, with\nstrong generalization particularly to translation, stretching, and squeezing\ninteractions of unseen objects in unseen environments. Further, CosHand can be\nsampled many times to predict multiple possible effects, modeling the\nuncertainty of forces in the interaction/environment. Finally, method\ngeneralizes to different embodiments, including non-human hands, i.e. robot\nhands, suggesting that generative video models can be powerful models for\nrobotics.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}