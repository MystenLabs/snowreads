{"id":"2407.11133","title":"Discrete generative diffusion models without stochastic differential\n  equations: a tensor network approach","authors":"Luke Causer, Grant M. Rotskoff and Juan P. Garrahan","authorsParsed":[["Causer","Luke",""],["Rotskoff","Grant M.",""],["Garrahan","Juan P.",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 18:00:11 GMT"}],"updateDate":"2024-08-15","timestamp":1721066411000,"abstract":"  Diffusion models (DMs) are a class of generative machine learning methods\nthat sample a target distribution by transforming samples of a trivial (often\nGaussian) distribution using a learned stochastic differential equation. In\nstandard DMs, this is done by learning a ``score function'' that reverses the\neffect of adding diffusive noise to the distribution of interest. Here we\nconsider the generalisation of DMs to lattice systems with discrete degrees of\nfreedom, and where noise is added via Markov chain jump dynamics. We show how\nto use tensor networks (TNs) to efficiently define and sample such ``discrete\ndiffusion models'' (DDMs) without explicitly having to solve a stochastic\ndifferential equation. We show the following: (i) by parametrising the data and\nevolution operators as TNs, the denoising dynamics can be represented exactly;\n(ii) the auto-regressive nature of TNs allows to generate samples efficiently\nand without bias; (iii) for sampling Boltzmann-like distributions, TNs allow to\nconstruct an efficient learning scheme that integrates well with Monte Carlo.\nWe illustrate this approach to study the equilibrium of two models with\nnon-trivial thermodynamics, the $d=1$ constrained Fredkin chain and the $d=2$\nIsing model.\n","subjects":["Condensed Matter/Statistical Mechanics","Condensed Matter/Disordered Systems and Neural Networks","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}