{"id":"2407.05650","title":"The Dynamic Net Architecture: Learning Robust and Holistic Visual\n  Representations Through Self-Organizing Networks","authors":"Pascal J. Sager and Jan M. Deriu and Benjamin F. Grewe and Thilo\n  Stadelmann and Christoph von der Malsburg","authorsParsed":[["Sager","Pascal J.",""],["Deriu","Jan M.",""],["Grewe","Benjamin F.",""],["Stadelmann","Thilo",""],["von der Malsburg","Christoph",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 06:22:10 GMT"}],"updateDate":"2024-07-09","timestamp":1720419730000,"abstract":"  We present a novel intelligent-system architecture called \"Dynamic Net\nArchitecture\" (DNA) that relies on recurrence-stabilized networks and discuss\nit in application to vision. Our architecture models a (cerebral cortical) area\nwherein elementary feature neurons encode details of visual structures, and\ncoherent nets of such neurons model holistic object structures. By interpreting\nsmaller or larger coherent pieces of an area network as complex features, our\nmodel encodes hierarchical feature representations essentially different than\nartificial neural networks (ANNs).\n  DNA models operate on a dynamic connectionism principle, wherein neural\nactivations stemming from initial afferent signals undergo stabilization\nthrough a self-organizing mechanism facilitated by Hebbian plasticity alongside\nperiodically tightening inhibition. In contrast to ANNs, which rely on\nfeed-forward connections and backpropagation of error, we posit that this\nprocessing paradigm leads to highly robust representations, as by employing\ndynamic lateral connections, irrelevant details in neural activations are\nfiltered out, freeing further processing steps from distracting noise and\npremature decisions.\n  We empirically demonstrate the viability of the DNA by composing line\nfragments into longer lines and show that the construction of nets representing\nlines remains robust even with the introduction of up to $59\\%$ noise at each\nspatial location. Furthermore, we demonstrate the model's capability to\nreconstruct anticipated features from partially obscured inputs and that it can\ngeneralize to patterns not observed during training. In this work, we limit the\nDNA to one cortical area and focus on its internals while providing insights\ninto a standalone area's strengths and shortcomings. Additionally, we provide\nan outlook on how future work can implement invariant object recognition by\ncombining multiple areas.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}