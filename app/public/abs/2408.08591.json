{"id":"2408.08591","title":"Zero-Shot Dual-Path Integration Framework for Open-Vocabulary 3D\n  Instance Segmentation","authors":"Tri Ton, Ji Woo Hong, SooHwan Eom, Jun Yeop Shim, Junyeong Kim, Chang\n  D. Yoo","authorsParsed":[["Ton","Tri",""],["Hong","Ji Woo",""],["Eom","SooHwan",""],["Shim","Jun Yeop",""],["Kim","Junyeong",""],["Yoo","Chang D.",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 07:52:00 GMT"}],"updateDate":"2024-08-19","timestamp":1723794720000,"abstract":"  Open-vocabulary 3D instance segmentation transcends traditional\nclosed-vocabulary methods by enabling the identification of both previously\nseen and unseen objects in real-world scenarios. It leverages a dual-modality\napproach, utilizing both 3D point clouds and 2D multi-view images to generate\nclass-agnostic object mask proposals. Previous efforts predominantly focused on\nenhancing 3D mask proposal models; consequently, the information that could\ncome from 2D association to 3D was not fully exploited. This bias towards 3D\ndata, while effective for familiar indoor objects, limits the system's\nadaptability to new and varied object types, where 2D models offer greater\nutility. Addressing this gap, we introduce Zero-Shot Dual-Path Integration\nFramework that equally values the contributions of both 3D and 2D modalities.\nOur framework comprises three components: 3D pathway, 2D pathway, and Dual-Path\nIntegration. 3D pathway generates spatially accurate class-agnostic mask\nproposals of common indoor objects from 3D point cloud data using a pre-trained\n3D model, while 2D pathway utilizes pre-trained open-vocabulary instance\nsegmentation model to identify a diverse array of object proposals from\nmulti-view RGB-D images. In Dual-Path Integration, our Conditional Integration\nprocess, which operates in two stages, filters and merges the proposals from\nboth pathways adaptively. This process harmonizes output proposals to enhance\nsegmentation capabilities. Our framework, utilizing pre-trained models in a\nzero-shot manner, is model-agnostic and demonstrates superior performance on\nboth seen and unseen data, as evidenced by comprehensive evaluations on the\nScanNet200 and qualitative results on ARKitScenes datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}