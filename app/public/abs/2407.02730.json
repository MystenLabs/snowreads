{"id":"2407.02730","title":"MedVH: Towards Systematic Evaluation of Hallucination for Large Vision\n  Language Models in the Medical Context","authors":"Zishan Gu, Changchang Yin, Fenglin Liu, Ping Zhang","authorsParsed":[["Gu","Zishan",""],["Yin","Changchang",""],["Liu","Fenglin",""],["Zhang","Ping",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 00:59:03 GMT"}],"updateDate":"2024-07-04","timestamp":1719968343000,"abstract":"  Large Vision Language Models (LVLMs) have recently achieved superior\nperformance in various tasks on natural image and text data, which inspires a\nlarge amount of studies for LVLMs fine-tuning and training. Despite their\nadvancements, there has been scant research on the robustness of these models\nagainst hallucination when fine-tuned on smaller datasets. In this study, we\nintroduce a new benchmark dataset, the Medical Visual Hallucination Test\n(MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH\ncomprises five tasks to evaluate hallucinations in LVLMs within the medical\ncontext, which includes tasks for comprehensive understanding of textual and\nvisual input, as well as long textual response generation. Our extensive\nexperiments with both general and medical LVLMs reveal that, although medical\nLVLMs demonstrate promising performance on standard medical tasks, they are\nparticularly susceptible to hallucinations, often more so than the general\nmodels, raising significant concerns about the reliability of these\ndomain-specific models. For medical LVLMs to be truly valuable in real-world\napplications, they must not only accurately integrate medical knowledge but\nalso maintain robust reasoning abilities to prevent hallucination. Our work\npaves the way for future evaluations of these studies.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}