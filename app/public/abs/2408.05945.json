{"id":"2408.05945","title":"MV2DFusion: Leveraging Modality-Specific Object Semantics for\n  Multi-Modal 3D Detection","authors":"Zitian Wang, Zehao Huang, Yulu Gao, Naiyan Wang, Si Liu","authorsParsed":[["Wang","Zitian",""],["Huang","Zehao",""],["Gao","Yulu",""],["Wang","Naiyan",""],["Liu","Si",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 06:46:05 GMT"}],"updateDate":"2024-08-13","timestamp":1723445165000,"abstract":"  The rise of autonomous vehicles has significantly increased the demand for\nrobust 3D object detection systems. While cameras and LiDAR sensors each offer\nunique advantages--cameras provide rich texture information and LiDAR offers\nprecise 3D spatial data--relying on a single modality often leads to\nperformance limitations. This paper introduces MV2DFusion, a multi-modal\ndetection framework that integrates the strengths of both worlds through an\nadvanced query-based fusion mechanism. By introducing an image query generator\nto align with image-specific attributes and a point cloud query generator,\nMV2DFusion effectively combines modality-specific object semantics without\nbiasing toward one single modality. Then the sparse fusion process can be\naccomplished based on the valuable object semantics, ensuring efficient and\naccurate object detection across various scenarios. Our framework's flexibility\nallows it to integrate with any image and point cloud-based detectors,\nshowcasing its adaptability and potential for future advancements. Extensive\nevaluations on the nuScenes and Argoverse2 datasets demonstrate that MV2DFusion\nachieves state-of-the-art performance, particularly excelling in long-range\ndetection scenarios.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}