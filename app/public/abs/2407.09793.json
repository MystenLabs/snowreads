{"id":"2407.09793","title":"Uncovering Weaknesses in Neural Code Generation","authors":"Xiaoli Lian, Shuaisong Wang, Jieping Ma, Fang Liu, Xin Tan, Li Zhang,\n  Lin Shi, Cuiyun Gao","authorsParsed":[["Lian","Xiaoli",""],["Wang","Shuaisong",""],["Ma","Jieping",""],["Liu","Fang",""],["Tan","Xin",""],["Zhang","Li",""],["Shi","Lin",""],["Gao","Cuiyun",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 07:31:43 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 14:34:14 GMT"}],"updateDate":"2024-07-18","timestamp":1720855903000,"abstract":"  Code generation, the task of producing source code from prompts, has seen\nsignificant advancements with the advent of pre-trained large language models\n(PLMs). Despite these achievements, there lacks a comprehensive taxonomy of\nweaknesses about the benchmark and the generated code, which risks the\ncommunity's focus on known issues at the cost of under-explored areas.\n  Our systematic study aims to fill this gap by evaluating five\nstate-of-the-art PLMs: three larger models, CodeGen2.5 with 7 billion\nparameters, CodeGeeX2 with 6 billion parameters, GPT-4 Turbo, and two smaller\nones, UnixCoder with 110 million parameters and CodeT5 base with 220 million\nparameters, across three popular datasets, CoNaLa, HumanEval Plus, and DS-1000.\nWe assess the quality of generated code using match-based and execution-based\nmetrics, then conduct thematic analysis to develop a taxonomy of nine types of\nweaknesses.\n  We dissected weakness distributions in both larger and smaller models,\napplying an extensive methodology that encompasses model-specific as well as\ncollective analysis (union and intersection) across models. Our research\nuncovers three salient findings: 1. In the CoNaLa dataset, inaccurate prompts\nare a notable problem, causing all large models to fail in 26.84% of cases,\nwith even higher failure rates of 40% for smaller models; 2. Missing pivotal\nsemantics is a pervasive issue across benchmarks, with one or more large models\nomitting key semantics in 65.78% of CoNaLa tasks, and similarly high\noccurrences in HumanEval Plus (66.09%) and DS-1000 (80.51%); 3. All models\nstruggle with proper API usage, a challenge amplified by vague or complex\nprompts.\n  Our findings aim to steer researchers towards addressing specific weaknesses\nand challenges in code generation. Furthermore, our annotations can offer a\ntargeted benchmark subset for detailed analysis.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}