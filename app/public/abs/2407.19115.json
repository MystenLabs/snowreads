{"id":"2407.19115","title":"Towards Scalable and Stable Parallelization of Nonlinear RNNs","authors":"Xavier Gonzalez, Andrew Warrington, Jimmy T.H. Smith, Scott W.\n  Linderman","authorsParsed":[["Gonzalez","Xavier",""],["Warrington","Andrew",""],["Smith","Jimmy T. H.",""],["Linderman","Scott W.",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 22:38:11 GMT"}],"updateDate":"2024-07-30","timestamp":1722033491000,"abstract":"  Conventional nonlinear RNNs are not naturally parallelizable across the\nsequence length, whereas transformers and linear RNNs are. Lim et al. [2024]\ntherefore tackle parallelized evaluation of nonlinear RNNs by posing it as a\nfixed point problem, solved with Newton's method. By deriving and applying a\nparallelized form of Newton's method, they achieve huge speedups over\nsequential evaluation. However, their approach inherits cubic computational\ncomplexity and numerical instability. We tackle these weaknesses. To reduce the\ncomputational complexity, we apply quasi-Newton approximations and show they\nconverge comparably to full-Newton, use less memory, and are faster. To\nstabilize Newton's method, we leverage a connection between Newton's method\ndamped with trust regions and Kalman smoothing. This connection allows us to\nstabilize Newtons method, per the trust region, while using efficient\nparallelized Kalman algorithms to retain performance. We compare these methods\nempirically, and highlight the use cases where each algorithm excels.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}