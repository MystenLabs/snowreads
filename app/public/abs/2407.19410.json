{"id":"2407.19410","title":"AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question\n  Answering","authors":"Mahiro Ukai, Shuhei Kurita, Atsushi Hashimoto, Yoshitaka Ushiku,\n  Nakamasa Inoue","authorsParsed":[["Ukai","Mahiro",""],["Kurita","Shuhei",""],["Hashimoto","Atsushi",""],["Ushiku","Yoshitaka",""],["Inoue","Nakamasa",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 06:23:06 GMT"}],"updateDate":"2024-07-30","timestamp":1722147786000,"abstract":"  Visual question answering aims to provide responses to natural language\nquestions given visual input. Recently, visual programmatic models (VPMs),\nwhich generate executable programs to answer questions through large language\nmodels (LLMs), have attracted research interest. However, they often require\nlong input prompts to provide the LLM with sufficient API usage details to\ngenerate relevant code. To address this limitation, we propose AdaCoder, an\nadaptive prompt compression framework for VPMs. AdaCoder operates in two\nphases: a compression phase and an inference phase. In the compression phase,\ngiven a preprompt that describes all API definitions in the Python language\nwith example snippets of code, a set of compressed preprompts is generated,\neach depending on a specific question type. In the inference phase, given an\ninput question, AdaCoder predicts the question type and chooses the appropriate\ncorresponding compressed preprompt to generate code to answer the question.\nNotably, AdaCoder employs a single frozen LLM and pre-defined prompts, negating\nthe necessity of additional training and maintaining adaptability across\ndifferent powerful black-box LLMs such as GPT and Claude. In experiments, we\napply AdaCoder to ViperGPT and demonstrate that it reduces token length by\n71.1%, while maintaining or even improving the performance of visual question\nanswering.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}