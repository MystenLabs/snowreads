{"id":"2407.00416","title":"Too Late to Train, Too Early To Use? A Study on Necessity and Viability\n  of Low-Resource Bengali LLMs","authors":"Tamzeed Mahfuz, Satak Kumar Dey, Ruwad Naswan, Hasnaen Adil, Khondker\n  Salman Sayeed, Haz Sameen Shahgir","authorsParsed":[["Mahfuz","Tamzeed",""],["Dey","Satak Kumar",""],["Naswan","Ruwad",""],["Adil","Hasnaen",""],["Sayeed","Khondker Salman",""],["Shahgir","Haz Sameen",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 11:50:16 GMT"}],"updateDate":"2024-07-02","timestamp":1719661816000,"abstract":"  Each new generation of English-oriented Large Language Models (LLMs) exhibits\nenhanced cross-lingual transfer capabilities and significantly outperforms\nolder LLMs on low-resource languages. This prompts the question: Is there a\nneed for LLMs dedicated to a particular low-resource language? We aim to\nexplore this question for Bengali, a low-to-moderate resource Indo-Aryan\nlanguage native to the Bengal region of South Asia.\n  We compare the performance of open-weight and closed-source LLMs such as\nLLaMA-3 and GPT-4 against fine-tuned encoder-decoder models across a diverse\nset of Bengali downstream tasks, including translation, summarization,\nparaphrasing, question-answering, and natural language inference. Our findings\nreveal that while LLMs generally excel in reasoning tasks, their performance in\ntasks requiring Bengali script generation is inconsistent. Key challenges\ninclude inefficient tokenization of Bengali script by existing LLMs, leading to\nincreased computational costs and potential performance degradation.\nAdditionally, we highlight biases in machine-translated datasets commonly used\nfor Bengali NLP tasks. We conclude that there is a significant need for a\nBengali-oriented LLM, but the field currently lacks the high-quality\npretraining and instruction-tuning datasets necessary to develop a highly\neffective model.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}