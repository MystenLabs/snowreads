{"id":"2407.05693","title":"Sub-SA: Strengthen In-context Learning via Submodular Selective\n  Annotation","authors":"Jian Qian, Miao Sun, Sifan Zhou, Ziyu Zhao, Ruizhi Hun, Patrick Chiang","authorsParsed":[["Qian","Jian",""],["Sun","Miao",""],["Zhou","Sifan",""],["Zhao","Ziyu",""],["Hun","Ruizhi",""],["Chiang","Patrick",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 07:47:30 GMT"},{"version":"v2","created":"Fri, 13 Sep 2024 06:57:01 GMT"}],"updateDate":"2024-09-16","timestamp":1720424850000,"abstract":"  In-context learning (ICL) leverages in-context examples as prompts for the\npredictions of Large Language Models (LLMs). These prompts play a crucial role\nin achieving strong performance. However, the selection of suitable prompts\nfrom a large pool of labeled examples often entails significant annotation\ncosts. To address this challenge, we propose Sub-SA (Submodular Selective\nAnnotation), a submodule-based selective annotation method. The aim of Sub-SA\nis to reduce annotation costs while improving the quality of in-context\nexamples and minimizing the time consumption of the selection process. In\nSub-SA, we design a submodular function that facilitates effective subset\nselection for annotation and demonstrates the characteristics of monotonically\nand submodularity from the theoretical perspective. Specifically, we propose\nRPR (Reward and Penalty Regularization) to better balance the diversity and\nrepresentativeness of the unlabeled dataset attributed to a reward term and a\npenalty term, respectively. Consequently, the selection for annotations can be\neffectively addressed with a simple yet effective greedy search algorithm based\non the submodular function. Finally, we apply the similarity prompt retrieval\nto get the examples for ICL.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}