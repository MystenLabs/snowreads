{"id":"2408.08899","title":"Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov\n  Decision Processes and Tree Search","authors":"Robert J. Moss","authorsParsed":[["Moss","Robert J.",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 20:31:52 GMT"}],"updateDate":"2024-08-20","timestamp":1723408312000,"abstract":"  Eliciting harmful behavior from large language models (LLMs) is an important\ntask to ensure the proper alignment and safety of the models. Often when\ntraining LLMs, ethical guidelines are followed yet alignment failures may still\nbe uncovered through red teaming adversarial attacks. This work frames the\nred-teaming problem as a Markov decision process (MDP) and uses Monte Carlo\ntree search to find harmful behaviors of black-box, closed-source LLMs. We\noptimize token-level prompt suffixes towards targeted harmful behaviors on\nwhite-box LLMs and include a naturalistic loss term, log-perplexity, to\ngenerate more natural language attacks for better interpretability. The\nproposed algorithm, Kov, trains on white-box LLMs to optimize the adversarial\nattacks and periodically evaluates responses from the black-box LLM to guide\nthe search towards more harmful black-box behaviors. In our preliminary study,\nresults indicate that we can jailbreak black-box models, such as GPT-3.5, in\nonly 10 queries, yet fail on GPT-4$-$which may indicate that newer models are\nmore robust to token-level attacks. All work to reproduce these results is open\nsourced (https://github.com/sisl/Kov.jl).\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZW_-J0DQjwITKz0AcK8mxxyxi9B7V_YakCSHG38MMaI","pdfSize":"941092"}
