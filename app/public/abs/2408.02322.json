{"id":"2408.02322","title":"Data time travel and consistent market making: taming reinforcement\n  learning in multi-agent systems with anonymous data","authors":"Vincent Ragel and Damien Challet","authorsParsed":[["Ragel","Vincent",""],["Challet","Damien",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 09:07:36 GMT"}],"updateDate":"2024-08-06","timestamp":1722848856000,"abstract":"  Reinforcement learning works best when the impact of the agent's actions on\nits environment can be perfectly simulated or fully appraised from available\ndata. Some systems are however both hard to simulate and very sensitive to\nsmall perturbations. An additional difficulty arises when an RL agent must\nlearn to be part of a multi-agent system using only anonymous data, which makes\nit impossible to infer the state of each agent, thus to use data directly.\nTypical examples are competitive systems without agent-resolved data such as\nfinancial markets. We introduce consistent data time travel for offline RL as a\nremedy for these problems: instead of using historical data in a sequential\nway, we argue that one needs to perform time travel in historical data, i.e.,\nto adjust the time index so that both the past state and the influence of the\nRL agent's action on the state coincide with real data. This both alleviates\nthe need to resort to imperfect models and consistently accounts for both the\nimmediate and long-term reactions of the system when using anonymous historical\ndata. We apply this idea to market making in limit order books, a notoriously\ndifficult task for RL; it turns out that the gain of the agent is significantly\nhigher with data time travel than with naive sequential data, which suggests\nthat the difficulty of this task for RL may have been overestimated.\n","subjects":["Quantitative Finance/Trading and Market Microstructure"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}