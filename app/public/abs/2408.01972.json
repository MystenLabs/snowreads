{"id":"2408.01972","title":"RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning","authors":"Yukinari Hisaki, Isao Ono","authorsParsed":[["Hisaki","Yukinari",""],["Ono","Isao",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 09:26:00 GMT"}],"updateDate":"2024-08-06","timestamp":1722763560000,"abstract":"  In this paper, we propose an off-policy deep reinforcement learning (DRL)\nmethod utilizing the average reward criterion. While most existing DRL methods\nemploy the discounted reward criterion, this can potentially lead to a\ndiscrepancy between the training objective and performance metrics in\ncontinuing tasks, making the average reward criterion a recommended\nalternative. We introduce RVI-SAC, an extension of the state-of-the-art\noff-policy DRL method, Soft Actor-Critic (SAC), to the average reward\ncriterion. Our proposal consists of (1) Critic updates based on RVI Q-learning,\n(2) Actor updates introduced by the average reward soft policy improvement\ntheorem, and (3) automatic adjustment of Reset Cost enabling the average reward\nreinforcement learning to be applied to tasks with termination. We apply our\nmethod to the Gymnasium's Mujoco tasks, a subset of locomotion tasks, and\ndemonstrate that RVI-SAC shows competitive performance compared to existing\nmethods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}