{"id":"2407.01920","title":"To Forget or Not? Towards Practical Knowledge Unlearning for Large\n  Language Models","authors":"Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang,\n  Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang","authorsParsed":[["Tian","Bozhong",""],["Liang","Xiaozhuan",""],["Cheng","Siyuan",""],["Liu","Qingbin",""],["Wang","Mengru",""],["Sui","Dianbo",""],["Chen","Xi",""],["Chen","Huajun",""],["Zhang","Ningyu",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 03:34:16 GMT"}],"updateDate":"2024-07-03","timestamp":1719891256000,"abstract":"  Large Language Models (LLMs) trained on extensive corpora inevitably retain\nsensitive data, such as personal privacy information and copyrighted material.\nRecent advancements in knowledge unlearning involve updating LLM parameters to\nerase specific knowledge. However, current unlearning paradigms are mired in\nvague forgetting boundaries, often erasing knowledge indiscriminately. In this\nwork, we introduce KnowUnDo, a benchmark containing copyrighted content and\nuser privacy domains to evaluate if the unlearning process inadvertently erases\nessential knowledge. Our findings indicate that existing unlearning methods\noften suffer from excessive unlearning. To address this, we propose a simple\nyet effective method, MemFlex, which utilizes gradient information to precisely\ntarget and unlearn sensitive parameters. Experimental results show that MemFlex\nis superior to existing methods in both precise knowledge unlearning and\ngeneral knowledge retaining of LLMs. Code and dataset will be released at\nhttps://github.com/zjunlp/KnowUnDo.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}