{"id":"2408.02900","title":"MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular\n  Annotations for Medicine","authors":"Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou,\n  Sheng Liu, Lei Xing, James Zou, Cihang Xie, Yuyin Zhou","authorsParsed":[["Xie","Yunfei",""],["Zhou","Ce",""],["Gao","Lang",""],["Wu","Juncheng",""],["Li","Xianhang",""],["Zhou","Hong-Yu",""],["Liu","Sheng",""],["Xing","Lei",""],["Zou","James",""],["Xie","Cihang",""],["Zhou","Yuyin",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 02:09:35 GMT"}],"updateDate":"2024-08-07","timestamp":1722910175000,"abstract":"  This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal\ndataset for medicine, covering over 25 million images across 10 modalities,\nwith multigranular annotations for more than 65 diseases. These enriched\nannotations encompass both global textual information, such as disease/lesion\ntype, modality, region-specific descriptions, and inter-regional relationships,\nas well as detailed local annotations for regions of interest (ROIs), including\nbounding boxes, segmentation masks. Unlike existing approach which is limited\nby the availability of image-text pairs, we have developed the first automated\npipeline that scales up multimodal data by generating multigranular visual and\ntexual annotations (in the form of image-ROI-description triplets) without the\nneed for any paired text descriptions. Specifically, data from over 90\ndifferent sources have been collected, preprocessed, and grounded using\ndomain-specific expert models to identify ROIs related to abnormal regions. We\nthen build a comprehensive knowledge base and prompt multimodal large language\nmodels to perform retrieval-augmented generation with the identified ROIs as\nguidance, resulting in multigranular texual descriptions. Compared to existing\ndatasets, MedTrinity-25M provides the most enriched annotations, supporting a\ncomprehensive range of multimodal tasks such as captioning and report\ngeneration, as well as vision-centric tasks like classification and\nsegmentation. Pretraining on MedTrinity-25M, our model achieves\nstate-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal\nlarge language models and other representative SoTA approaches. This dataset\ncan also be utilized to support large-scale pre-training of multimodal medical\nAI models, contributing to the development of future foundation models in the\nmedical domain.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}