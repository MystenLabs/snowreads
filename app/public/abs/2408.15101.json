{"id":"2408.15101","title":"MTMamba++: Enhancing Multi-Task Dense Scene Understanding via\n  Mamba-Based Decoders","authors":"Baijiong Lin, Weisen Jiang, Pengguang Chen, Shu Liu, and Ying-Cong\n  Chen","authorsParsed":[["Lin","Baijiong",""],["Jiang","Weisen",""],["Chen","Pengguang",""],["Liu","Shu",""],["Chen","Ying-Cong",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 14:36:46 GMT"}],"updateDate":"2024-08-28","timestamp":1724769406000,"abstract":"  Multi-task dense scene understanding, which trains a model for multiple dense\nprediction tasks, has a wide range of application scenarios. Capturing\nlong-range dependency and enhancing cross-task interactions are crucial to\nmulti-task dense prediction. In this paper, we propose MTMamba++, a novel\narchitecture for multi-task scene understanding featuring with a Mamba-based\ndecoder. It contains two types of core blocks: self-task Mamba (STM) block and\ncross-task Mamba (CTM) block. STM handles long-range dependency by leveraging\nstate-space models, while CTM explicitly models task interactions to facilitate\ninformation exchange across tasks. We design two types of CTM block, namely\nF-CTM and S-CTM, to enhance cross-task interaction from feature and semantic\nperspectives, respectively. Experiments on NYUDv2, PASCAL-Context, and\nCityscapes datasets demonstrate the superior performance of MTMamba++ over\nCNN-based and Transformer-based methods. The code is available at\nhttps://github.com/EnVision-Research/MTMamba.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}