{"id":"2407.04999","title":"Rethinking the Effectiveness of Graph Classification Datasets in\n  Benchmarks for Assessing GNNs","authors":"Zhengdao Li, Yong Cao, Kefan Shuai, Yiming Miao and Kai Hwang","authorsParsed":[["Li","Zhengdao",""],["Cao","Yong",""],["Shuai","Kefan",""],["Miao","Yiming",""],["Hwang","Kai",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 08:33:23 GMT"}],"updateDate":"2024-08-14","timestamp":1720254803000,"abstract":"  Graph classification benchmarks, vital for assessing and developing graph\nneural networks (GNNs), have recently been scrutinized, as simple methods like\nMLPs have demonstrated comparable performance. This leads to an important\nquestion: Do these benchmarks effectively distinguish the advancements of GNNs\nover other methodologies? If so, how do we quantitatively measure this\neffectiveness? In response, we first propose an empirical protocol based on a\nfair benchmarking framework to investigate the performance discrepancy between\nsimple methods and GNNs. We further propose a novel metric to quantify the\ndataset effectiveness by considering both dataset complexity and model\nperformance. To the best of our knowledge, our work is the first to thoroughly\nstudy and provide an explicit definition for dataset effectiveness in the graph\nlearning area. Through testing across 16 real-world datasets, we found our\nmetric to align with existing studies and intuitive assumptions. Finally, we\nexplore the causes behind the low effectiveness of certain datasets by\ninvestigating the correlation between intrinsic graph properties and class\nlabels, and we developed a novel technique supporting the\ncorrelation-controllable synthetic dataset generation. Our findings shed light\non the current understanding of benchmark datasets, and our new platform could\nfuel the future evolution of graph classification benchmarks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}