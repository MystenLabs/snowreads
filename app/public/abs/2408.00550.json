{"id":"2408.00550","title":"Mitigating Multilingual Hallucination in Large Vision-Language Models","authors":"Xiaoye Qu, Mingyang Song, Wei Wei, Jianfeng Dong, Yu Cheng","authorsParsed":[["Qu","Xiaoye",""],["Song","Mingyang",""],["Wei","Wei",""],["Dong","Jianfeng",""],["Cheng","Yu",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 13:34:35 GMT"}],"updateDate":"2024-08-02","timestamp":1722519275000,"abstract":"  While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PCCg_l-ieRtBo70RexNtxgIBEGbMWo6ZVL8bT0BuQz8","pdfSize":"1984212","txDigest":"5P8W9zmooiP6w17iYTqp8eCjBZiTAsVNxYDgLAyxKPTk","endEpoch":"1","status":"CERTIFIED"}
