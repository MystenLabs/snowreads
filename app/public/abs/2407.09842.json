{"id":"2407.09842","title":"Eliminating Feature Ambiguity for Few-Shot Segmentation","authors":"Qianxiong Xu, Guosheng Lin, Chen Change Loy, Cheng Long, Ziyue Li, Rui\n  Zhao","authorsParsed":[["Xu","Qianxiong",""],["Lin","Guosheng",""],["Loy","Chen Change",""],["Long","Cheng",""],["Li","Ziyue",""],["Zhao","Rui",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 10:33:03 GMT"}],"updateDate":"2024-07-16","timestamp":1720866783000,"abstract":"  Recent advancements in few-shot segmentation (FSS) have exploited\npixel-by-pixel matching between query and support features, typically based on\ncross attention, which selectively activate query foreground (FG) features that\ncorrespond to the same-class support FG features. However, due to the large\nreceptive fields in deep layers of the backbone, the extracted query and\nsupport FG features are inevitably mingled with background (BG) features,\nimpeding the FG-FG matching in cross attention. Hence, the query FG features\nare fused with less support FG features, i.e., the support information is not\nwell utilized. This paper presents a novel plug-in termed ambiguity elimination\nnetwork (AENet), which can be plugged into any existing cross attention-based\nFSS methods. The main idea is to mine discriminative query FG regions to\nrectify the ambiguous FG features, increasing the proportion of FG information,\nso as to suppress the negative impacts of the doped BG features. In this way,\nthe FG-FG matching is naturally enhanced. We plug AENet into three baselines\nCyCTR, SCCAN and HDMNet for evaluation, and their scores are improved by large\nmargins, e.g., the 1-shot performance of SCCAN can be improved by 3.0%+ on both\nPASCAL-5$^i$ and COCO-20$^i$. The code is available at\nhttps://github.com/Sam1224/AENet.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}