{"id":"2408.14611","title":"Scalable, reproducible, and cost-effective processing of large-scale\n  medical imaging datasets","authors":"Michael E. Kim, Karthik Ramadass, Chenyu Gao, Praitayini Kanakaraj,\n  Nancy R. Newlin, Gaurav Rudravaram, Kurt G. Schilling, Blake E. Dewey, Derek\n  Archer, Timothy J. Hohman, Zhiyuan Li, Shunxing Bao, Bennett A. Landman,\n  Nazirah Mohd Khairi","authorsParsed":[["Kim","Michael E.",""],["Ramadass","Karthik",""],["Gao","Chenyu",""],["Kanakaraj","Praitayini",""],["Newlin","Nancy R.",""],["Rudravaram","Gaurav",""],["Schilling","Kurt G.",""],["Dewey","Blake E.",""],["Archer","Derek",""],["Hohman","Timothy J.",""],["Li","Zhiyuan",""],["Bao","Shunxing",""],["Landman","Bennett A.",""],["Khairi","Nazirah Mohd",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 20:09:48 GMT"}],"updateDate":"2024-08-28","timestamp":1724702988000,"abstract":"  Curating, processing, and combining large-scale medical imaging datasets from\nnational studies is a non-trivial task due to the intense computation and data\nthroughput required, variability of acquired data, and associated financial\noverhead. Existing platforms or tools for large-scale data curation,\nprocessing, and storage have difficulty achieving a viable cost-to-scale ratio\nof computation speed for research purposes, either being too slow or too\nexpensive. Additionally, management and consistency of processing large data in\na team-driven manner is a non-trivial task. We design a BIDS-compliant method\nfor an efficient and robust data processing pipeline of large-scale\ndiffusion-weighted and T1-weighted MRI data compatible with low-cost,\nhigh-efficiency computing systems. Our method accomplishes automated querying\nof data available for processing and process running in a consistent and\nreproducible manner that has long-term stability, while using heterogenous\nlow-cost computational resources and storage systems for efficient processing\nand data transfer. We demonstrate how our organizational structure permits\nefficiency in a semi-automated data processing pipeline and show how our method\nis comparable in processing time to cloud-based computation while being almost\n20 times more cost-effective. Our design allows for fast data throughput speeds\nand low latency to reduce the time for data transfer between storage servers\nand computation servers, achieving an average of 0.60 Gb/s compared to 0.33\nGb/s for using cloud-based processing methods. The design of our workflow\nengine permits quick process running while maintaining flexibility to adapt to\nnewly acquired data.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Databases"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}