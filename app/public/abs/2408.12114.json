{"id":"2408.12114","title":"SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for\n  Large-scale Vision-Language Models","authors":"Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee, and Yong Man Ro","authorsParsed":[["Yu","Youngjoon",""],["Chung","Sangyun",""],["Lee","Byung-Kwan",""],["Ro","Yong Man",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 03:59:48 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 08:35:30 GMT"}],"updateDate":"2024-08-26","timestamp":1724299188000,"abstract":"  Large-scale Vision-Language Models (LVLMs) have significantly advanced with\ntext-aligned vision inputs. They have made remarkable progress in computer\nvision tasks by aligning text modality with vision inputs. There are also\nendeavors to incorporate multi-vision sensors beyond RGB, including thermal,\ndepth, and medical X-ray images. However, we observe that current LVLMs view\nimages taken from multi-vision sensors as if they were in the same RGB domain\nwithout considering the physical characteristics of multi-vision sensors. They\nfail to convey the fundamental multi-vision sensor information from the dataset\nand the corresponding contextual knowledge properly. Consequently, alignment\nbetween the information from the actual physical environment and the text is\nnot achieved correctly, making it difficult to answer complex sensor-related\nquestions that consider the physical environment. In this paper, we aim to\nestablish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK\nthat can reduce the fundamental multi-vision sensor information gap between\nimages and multi-vision sensors. We generated 6,248 vision-language test\nsamples to investigate multi-vision sensory perception and multi-vision sensory\nreasoning on physical sensor knowledge proficiency across different formats,\ncovering different types of sensor-related questions. We utilized these samples\nto assess ten leading LVLMs. The results showed that most models displayed\ndeficiencies in multi-vision sensory reasoning to varying extents. Codes and\ndata are available at https://github.com/top-yun/SPARK\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}