{"id":"2408.10006","title":"Unlocking the Power of LSTM for Long Term Time Series Forecasting","authors":"Yaxuan Kong, Zepu Wang, Yuqi Nie, Tian Zhou, Stefan Zohren, Yuxuan\n  Liang, Peng Sun, Qingsong Wen","authorsParsed":[["Kong","Yaxuan",""],["Wang","Zepu",""],["Nie","Yuqi",""],["Zhou","Tian",""],["Zohren","Stefan",""],["Liang","Yuxuan",""],["Sun","Peng",""],["Wen","Qingsong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 13:59:26 GMT"}],"updateDate":"2024-08-20","timestamp":1724075966000,"abstract":"  Traditional recurrent neural network architectures, such as long short-term\nmemory neural networks (LSTM), have historically held a prominent role in time\nseries forecasting (TSF) tasks. While the recently introduced sLSTM for Natural\nLanguage Processing (NLP) introduces exponential gating and memory mixing that\nare beneficial for long term sequential learning, its potential short memory\nissue is a barrier to applying sLSTM directly in TSF. To address this, we\npropose a simple yet efficient algorithm named P-sLSTM, which is built upon\nsLSTM by incorporating patching and channel independence. These modifications\nsubstantially enhance sLSTM's performance in TSF, achieving state-of-the-art\nresults. Furthermore, we provide theoretical justifications for our design, and\nconduct extensive comparative and analytical experiments to fully validate the\nefficiency and superior performance of our model.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}