{"id":"2407.11606","title":"The Foundations of Tokenization: Statistical and Computational Concerns","authors":"Juan Luis Gastaldi, John Terilla, Luca Malagutti, Brian DuSell, Tim\n  Vieira and Ryan Cotterell","authorsParsed":[["Gastaldi","Juan Luis",""],["Terilla","John",""],["Malagutti","Luca",""],["DuSell","Brian",""],["Vieira","Tim",""],["Cotterell","Ryan",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 11:12:28 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 20:49:37 GMT"}],"updateDate":"2024-08-12","timestamp":1721128348000,"abstract":"  Tokenization - the practice of converting strings of characters over an\nalphabet into sequences of tokens over a vocabulary - is a critical yet\nunder-theorized step in the NLP pipeline. Notably, it remains the only major\nstep not fully integrated into widely used end-to-end neural models. This paper\naims to address this theoretical gap by laying the foundations of tokenization\nfrom a formal perspective. By articulating and extending basic properties about\nthe category of stochastic maps, we propose a unified framework for\nrepresenting and analyzing tokenizer models. This framework allows us to\nestablish general conditions for the use of tokenizers. In particular, we\nformally establish the necessary and sufficient conditions for a tokenizer\nmodel to preserve the consistency of statistical estimators. Additionally, we\ndiscuss statistical and computational concerns crucial for the design and\nimplementation of tokenizer models. The framework and results advanced in this\npaper represent a step toward a robust theoretical foundation for neural\nlanguage modeling.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}