{"id":"2407.16025","title":"Exploring and Addressing Reward Confusion in Offline Preference Learning","authors":"Xin Chen, Sam Toyer, Florian Shkurti","authorsParsed":[["Chen","Xin",""],["Toyer","Sam",""],["Shkurti","Florian",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 20:03:36 GMT"}],"updateDate":"2024-07-24","timestamp":1721678616000,"abstract":"  Spurious correlations in a reward model's training data can prevent\nReinforcement Learning from Human Feedback (RLHF) from identifying the desired\ngoal and induce unwanted behaviors. This paper shows that offline RLHF is\nsusceptible to reward confusion, especially in the presence of spurious\ncorrelations in offline data. We create a benchmark to study this problem and\npropose a method that can significantly reduce reward confusion by leveraging\ntransitivity of preferences while building a global preference chain with\nactive learning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}