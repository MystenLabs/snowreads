{"id":"2407.17229","title":"LPGen: Enhancing High-Fidelity Landscape Painting Generation through\n  Diffusion Model","authors":"Wanggong Yang, Xiaona Wang, Yingrui Qiu and Yifei Zhao","authorsParsed":[["Yang","Wanggong",""],["Wang","Xiaona",""],["Qiu","Yingrui",""],["Zhao","Yifei",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 12:32:24 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 09:29:21 GMT"},{"version":"v3","created":"Mon, 12 Aug 2024 14:28:42 GMT"}],"updateDate":"2024-08-13","timestamp":1721824344000,"abstract":"  Generating landscape paintings expands the possibilities of artistic\ncreativity and imagination. Traditional landscape painting methods involve\nusing ink or colored ink on rice paper, which requires substantial time and\neffort. These methods are susceptible to errors and inconsistencies and lack\nprecise control over lines and colors. This paper presents LPGen, a\nhigh-fidelity, controllable model for landscape painting generation,\nintroducing a novel multi-modal framework that integrates image prompts into\nthe diffusion model. We extract its edges and contours by computing canny edges\nfrom the target landscape image. These, along with natural language text\nprompts and drawing style references, are fed into the latent diffusion model\nas conditions. We implement a decoupled cross-attention strategy to ensure\ncompatibility between image and text prompts, facilitating multi-modal image\ngeneration. A decoder generates the final image. Quantitative and qualitative\nanalyses demonstrate that our method outperforms existing approaches in\nlandscape painting generation and exceeds the current state-of-the-art. The\nLPGen network effectively controls the composition and color of landscape\npaintings, generates more accurate images, and supports further research in\ndeep learning-based landscape painting generation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}