{"id":"2407.16965","title":"3DAttGAN: A 3D Attention-based Generative Adversarial Network for Joint\n  Space-Time Video Super-Resolution","authors":"Congrui Fu, Hui Yuan, Liquan Shen, Raouf Hamzaoui, and Hao Zhang","authorsParsed":[["Fu","Congrui",""],["Yuan","Hui",""],["Shen","Liquan",""],["Hamzaoui","Raouf",""],["Zhang","Hao",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 03:11:47 GMT"}],"updateDate":"2024-07-25","timestamp":1721790707000,"abstract":"  In many applications, including surveillance, entertainment, and restoration,\nthere is a need to increase both the spatial resolution and the frame rate of a\nvideo sequence. The aim is to improve visual quality, refine details, and\ncreate a more realistic viewing experience. Existing space-time video\nsuper-resolution methods do not effectively use spatio-temporal information. To\naddress this limitation, we propose a generative adversarial network for joint\nspace-time video super-resolution. The generative network consists of three\noperations: shallow feature extraction, deep feature extraction, and\nreconstruction. It uses three-dimensional (3D) convolutions to process temporal\nand spatial information simultaneously and includes a novel 3D attention\nmechanism to extract the most important channel and spatial information. The\ndiscriminative network uses a two-branch structure to handle details and motion\ninformation, making the generated results more accurate. Experimental results\non the Vid4, Vimeo-90K, and REDS datasets demonstrate the effectiveness of the\nproposed method. The source code is publicly available at\nhttps://github.com/FCongRui/3DAttGan.git.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}