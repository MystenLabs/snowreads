{"id":"2407.03051","title":"Improving Conversational Abilities of Quantized Large Language Models\n  via Direct Preference Alignment","authors":"Janghwan Lee, Seongmin Park, Sukjin Hong, Minsoo Kim, Du-Seong Chang,\n  Jungwook Choi","authorsParsed":[["Lee","Janghwan",""],["Park","Seongmin",""],["Hong","Sukjin",""],["Kim","Minsoo",""],["Chang","Du-Seong",""],["Choi","Jungwook",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 12:19:06 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 06:21:23 GMT"}],"updateDate":"2024-07-19","timestamp":1720009146000,"abstract":"  The rapid advancement of large language models (LLMs) has facilitated their\ntransformation into conversational chatbots that can grasp contextual nuances\nand generate pertinent sentences, closely mirroring human values through\nadvanced techniques such as instruction tuning and reinforcement learning from\nhuman feedback (RLHF). However, the computational efficiency required for LLMs,\nachieved through techniques like post-training quantization (PTQ), presents\nchallenges such as token-flipping that can impair chatbot performance. In\nresponse, we propose a novel preference alignment approach, quantization-aware\ndirect preference optimization (QDPO), that aligns quantized LLMs with their\nfull-precision counterparts, improving conversational abilities. Evaluated on\ntwo instruction-tuned LLMs in various languages, QDPO demonstrated superior\nperformance in improving conversational abilities compared to established PTQ\nand knowledge-distillation fine-tuning techniques, marking a significant step\nforward in the development of efficient and effective conversational LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}