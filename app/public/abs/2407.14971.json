{"id":"2407.14971","title":"Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and\n  Semantically-Rich Vision-Language Models","authors":"Md Zarif Hossain, Ahmed Imteaj","authorsParsed":[["Hossain","Md Zarif",""],["Imteaj","Ahmed",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 19:53:52 GMT"}],"updateDate":"2024-07-23","timestamp":1721505232000,"abstract":"  Vision-language models (VLMs) have achieved significant strides in recent\ntimes specially in multimodal tasks, yet they remain susceptible to adversarial\nattacks on their vision components. To address this, we propose Sim-CLIP, an\nunsupervised adversarial fine-tuning method that enhances the robustness of the\nwidely-used CLIP vision encoder against such attacks while maintaining semantic\nrichness and specificity. By employing a Siamese architecture with cosine\nsimilarity loss, Sim-CLIP learns semantically meaningful and attack-resilient\nvisual representations without requiring large batch sizes or momentum\nencoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned\nCLIP encoder exhibit significantly enhanced robustness against adversarial\nattacks, while preserving semantic meaning of the perturbed images. Notably,\nSim-CLIP does not require additional training or fine-tuning of the VLM itself;\nreplacing the original vision encoder with our fine-tuned Sim-CLIP suffices to\nprovide robustness. This work underscores the significance of reinforcing\nfoundational models like CLIP to safeguard the reliability of downstream VLM\napplications, paving the way for more secure and effective multimodal systems.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}