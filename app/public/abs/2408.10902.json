{"id":"2408.10902","title":"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs","authors":"John Mendon\\c{c}a, Isabel Trancoso, Alon Lavie","authorsParsed":[["Mendon√ßa","John",""],["Trancoso","Isabel",""],["Lavie","Alon",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 14:45:23 GMT"}],"updateDate":"2024-08-21","timestamp":1724165123000,"abstract":"  Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}