{"id":"2408.10615","title":"Enhancing Robustness in Large Language Models: Prompting for Mitigating\n  the Impact of Irrelevant Information","authors":"Ming Jiang, Tingting Huang, Biao Guo, Yao Lu, Feng Zhang","authorsParsed":[["Jiang","Ming",""],["Huang","Tingting",""],["Guo","Biao",""],["Lu","Yao",""],["Zhang","Feng",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 07:49:38 GMT"}],"updateDate":"2024-08-21","timestamp":1724140178000,"abstract":"  In recent years, Large language models (LLMs) have garnered significant\nattention due to their superior performance in complex reasoning tasks.\nHowever, recent studies may diminish their reasoning capabilities markedly when\nproblem descriptions contain irrelevant information, even with the use of\nadvanced prompting techniques. To further investigate this issue, a dataset of\nprimary school mathematics problems containing irrelevant information, named\nGSMIR, was constructed. Testing prominent LLMs and prompting techniques on this\ndataset revealed that while LLMs can identify irrelevant information, they do\nnot effectively mitigate the interference it causes once identified. A novel\nautomatic construction method, ATF, which enhances the ability of LLMs to\nidentify and self-mitigate the influence of irrelevant information, is proposed\nto address this shortcoming. This method operates in two steps: first, analysis\nof irrelevant information, followed by its filtering. The ATF method, as\ndemonstrated by experimental results, significantly improves the reasoning\nperformance of LLMs and prompting techniques, even in the presence of\nirrelevant information on the GSMIR dataset.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}