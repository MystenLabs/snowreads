{"id":"2407.13097","title":"AlcLaM: Arabic Dialectal Language Model","authors":"Murtadha Ahmed, Saghir Alfasly, Bo Wen, Jamaal Qasem, Mohammed Ahmed,\n  Yunfeng Liu","authorsParsed":[["Ahmed","Murtadha",""],["Alfasly","Saghir",""],["Wen","Bo",""],["Qasem","Jamaal",""],["Ahmed","Mohammed",""],["Liu","Yunfeng",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 02:13:50 GMT"}],"updateDate":"2024-07-19","timestamp":1721268830000,"abstract":"  Pre-trained Language Models (PLMs) are integral to many modern natural\nlanguage processing (NLP) systems. Although multilingual models cover a wide\nrange of languages, they often grapple with challenges like high inference\ncosts and a lack of diverse non-English training data. Arabic-specific PLMs are\ntrained predominantly on modern standard Arabic, which compromises their\nperformance on regional dialects. To tackle this, we construct an Arabic\ndialectal corpus comprising 3.4M sentences gathered from social media\nplatforms. We utilize this corpus to expand the vocabulary and retrain a\nBERT-based model from scratch. Named AlcLaM, our model was trained using only\n13 GB of text, which represents a fraction of the data used by existing models\nsuch as CAMeL, MARBERT, and ArBERT, compared to 7.8%, 10.2%, and 21.3%,\nrespectively. Remarkably, AlcLaM demonstrates superior performance on a variety\nof Arabic NLP tasks despite the limited training data. AlcLaM is available at\nGitHub https://github.com/amurtadha/Alclam and HuggingFace\nhttps://huggingface.co/rahbi.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}