{"id":"2408.10539","title":"Training Matting Models without Alpha Labels","authors":"Wenze Liu, Zixuan Ye, Hao Lu, Zhiguo Cao, Xiangyu Yue","authorsParsed":[["Liu","Wenze",""],["Ye","Zixuan",""],["Lu","Hao",""],["Cao","Zhiguo",""],["Yue","Xiangyu",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 04:34:06 GMT"}],"updateDate":"2024-08-21","timestamp":1724128446000,"abstract":"  The labelling difficulty has been a longstanding problem in deep image\nmatting. To escape from fine labels, this work explores using rough annotations\nsuch as trimaps coarsely indicating the foreground/background as supervision.\nWe present that the cooperation between learned semantics from indicated known\nregions and proper assumed matting rules can help infer alpha values at\ntransition areas. Inspired by the nonlocal principle in traditional image\nmatting, we build a directional distance consistency loss (DDC loss) at each\npixel neighborhood to constrain the alpha values conditioned on the input\nimage. DDC loss forces the distance of similar pairs on the alpha matte and on\nits corresponding image to be consistent. In this way, the alpha values can be\npropagated from learned known regions to unknown transition areas. With only\nimages and trimaps, a matting model can be trained under the supervision of a\nknown loss and the proposed DDC loss. Experiments on AM-2K and P3M-10K dataset\nshow that our paradigm achieves comparable performance with the\nfine-label-supervised baseline, while sometimes offers even more satisfying\nresults than human-labelled ground truth. Code is available at\n\\url{https://github.com/poppuppy/alpha-free-matting}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}