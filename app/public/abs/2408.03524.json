{"id":"2408.03524","title":"EgyBERT: A Large Language Model Pretrained on Egyptian Dialect Corpora","authors":"Faisal Qarah","authorsParsed":[["Qarah","Faisal",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 03:23:55 GMT"}],"updateDate":"2024-08-08","timestamp":1723001035000,"abstract":"  This study presents EgyBERT, an Arabic language model pretrained on 10.4 GB\nof Egyptian dialectal texts. We evaluated EgyBERT's performance by comparing it\nwith five other multidialect Arabic language models across 10 evaluation\ndatasets. EgyBERT achieved the highest average F1-score of 84.25% and an\naccuracy of 87.33%, significantly outperforming all other comparative models,\nwith MARBERTv2 as the second best model achieving an F1-score 83.68% and an\naccuracy 87.19%. Additionally, we introduce two novel Egyptian dialectal\ncorpora: the Egyptian Tweets Corpus (ETC), containing over 34.33 million tweets\n(24.89 million sentences) amounting to 2.5 GB of text, and the Egyptian Forums\nCorpus (EFC), comprising over 44.42 million sentences (7.9 GB of text)\ncollected from various Egyptian online forums. Both corpora are used in\npretraining the new model, and they are the largest Egyptian dialectal corpora\nto date reported in the literature. Furthermore, this is the first study to\nevaluate the performance of various language models on Egyptian dialect\ndatasets, revealing significant differences in performance that highlight the\nneed for more dialect-specific models. The results confirm the effectiveness of\nEgyBERT model in processing and analyzing Arabic text expressed in Egyptian\ndialect, surpassing other language models included in the study. EgyBERT model\nis publicly available on \\url{https://huggingface.co/faisalq/EgyBERT}.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}