{"id":"2407.16353","title":"Sizey: Memory-Efficient Execution of Scientific Workflow Tasks","authors":"Jonathan Bader, Fabian Skalski, Fabian Lehmann, Dominik Scheinert,\n  Jonathan Will, Lauritz Thamsen, Odej Kao","authorsParsed":[["Bader","Jonathan",""],["Skalski","Fabian",""],["Lehmann","Fabian",""],["Scheinert","Dominik",""],["Will","Jonathan",""],["Thamsen","Lauritz",""],["Kao","Odej",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 09:54:07 GMT"}],"updateDate":"2024-07-24","timestamp":1721728447000,"abstract":"  As the amount of available data continues to grow in fields as diverse as\nbioinformatics, physics, and remote sensing, the importance of scientific\nworkflows in the design and implementation of reproducible data analysis\npipelines increases. When developing workflows, resource requirements must be\ndefined for each type of task in the workflow. Typically, task types vary\nwidely in their computational demands because they are simply wrappers for\narbitrary black-box analysis tools. Furthermore, the resource consumption for\nthe same task type can vary considerably as well due to different inputs. Since\nunderestimating memory resources leads to bottlenecks and task failures,\nworkflow developers tend to overestimate memory resources. However,\noverprovisioning of memory wastes resources and limits cluster throughput.\n  Addressing this problem, we propose Sizey, a novel online memory prediction\nmethod for workflow tasks. During workflow execution, Sizey simultaneously\ntrains multiple machine learning models and then dynamically selects the best\nmodel for each workflow task. To evaluate the quality of the model, we\nintroduce a novel resource allocation quality (RAQ) score based on memory\nprediction accuracy and efficiency. Sizey's prediction models are retrained and\nre-evaluated online during workflow execution, continuously incorporating\nmetrics from completed tasks.\n  Our evaluation with a prototype implementation of Sizey uses metrics from six\nreal-world scientific workflows from the popular nf-core framework and shows a\nmedian reduction in memory waste over time of 24.68% compared to the respective\nbest-performing state-of-the-art baseline.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}