{"id":"2408.03922","title":"FMiFood: Multi-modal Contrastive Learning for Food Image Classification","authors":"Xinyue Pan, Jiangpeng He, Fengqing Zhu","authorsParsed":[["Pan","Xinyue",""],["He","Jiangpeng",""],["Zhu","Fengqing",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 17:29:19 GMT"}],"updateDate":"2024-08-08","timestamp":1723051759000,"abstract":"  Food image classification is the fundamental step in image-based dietary\nassessment, which aims to estimate participants' nutrient intake from eating\noccasion images. A common challenge of food images is the intra-class diversity\nand inter-class similarity, which can significantly hinder classification\nperformance. To address this issue, we introduce a novel multi-modal\ncontrastive learning framework called FMiFood, which learns more discriminative\nfeatures by integrating additional contextual information, such as food\ncategory text descriptions, to enhance classification accuracy. Specifically,\nwe propose a flexible matching technique that improves the similarity matching\nbetween text and image embeddings to focus on multiple key information.\nFurthermore, we incorporate the classification objectives into the framework\nand explore the use of GPT-4 to enrich the text descriptions and provide more\ndetailed context. Our method demonstrates improved performance on both the\nUPMC-101 and VFN datasets compared to existing methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}