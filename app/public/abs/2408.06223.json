{"id":"2408.06223","title":"On Effects of Steering Latent Representation for Large Language Model\n  Unlearning","authors":"Dang Huu-Tien, Trung-Tin Pham, Hoang Thanh-Tung, and Naoya Inoue","authorsParsed":[["Huu-Tien","Dang",""],["Pham","Trung-Tin",""],["Thanh-Tung","Hoang",""],["Inoue","Naoya",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 15:24:50 GMT"}],"updateDate":"2024-08-13","timestamp":1723476290000,"abstract":"  Representation Misdirection for Unlearning (RMU), which steers model\nrepresentation in the intermediate layer to a target random representation, is\nan effective method for large language model (LLM) unlearning. Despite its high\nperformance, the underlying cause and explanation remain underexplored. In this\npaper, we first theoretically demonstrate that steering forget representations\nin the intermediate layer reduces token confidence, causing LLMs to generate\nwrong or nonsense responses. Second, we investigate how the coefficient\ninfluences the alignment of forget-sample representations with the random\ndirection and hint at the optimal coefficient values for effective unlearning\nacross different network layers. Third, we show that RMU unlearned models are\nrobust against adversarial jailbreak attacks. Last, our empirical analysis\nshows that RMU is less effective when applied to the middle and later layers in\nLLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet\neffective alternative method that makes unlearning effective with most layers.\nExtensive experiments demonstrate that Adaptive RMU significantly improves the\nunlearning performance compared to prior art while incurring no additional\ncomputational cost.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"vVDuldQHhznw0X-d5n6ThOIxBtfQwFF002U3wUT2-V4","pdfSize":"3321249"}
