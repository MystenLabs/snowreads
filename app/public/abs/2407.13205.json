{"id":"2407.13205","title":"Transformer-based Single-Cell Language Model: A Survey","authors":"Wei Lan, Guohang He, Mingyang Liu, Qingfeng Chen, Junyue Cao, Wei Peng","authorsParsed":[["Lan","Wei",""],["He","Guohang",""],["Liu","Mingyang",""],["Chen","Qingfeng",""],["Cao","Junyue",""],["Peng","Wei",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 06:43:12 GMT"}],"updateDate":"2024-07-19","timestamp":1721284992000,"abstract":"  The transformers have achieved significant accomplishments in the natural\nlanguage processing as its outstanding parallel processing capabilities and\nhighly flexible attention mechanism. In addition, increasing studies based on\ntransformers have been proposed to model single-cell data. In this review, we\nattempt to systematically summarize the single-cell language models and\napplications based on transformers. First, we provide a detailed introduction\nabout the structure and principles of transformers. Then, we review the\nsingle-cell language models and large language models for single-cell data\nanalysis. Moreover, we explore the datasets and applications of single-cell\nlanguage models in downstream tasks such as batch correction, cell clustering,\ncell type annotation, gene regulatory network inference and perturbation\nresponse. Further, we discuss the challenges of single-cell language models and\nprovide promising research directions. We hope this review will serve as an\nup-to-date reference for researchers interested in the direction of single-cell\nlanguage models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}