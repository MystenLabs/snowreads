{"id":"2408.03599","title":"Activations Through Extensions: A Framework To Boost Performance Of\n  Neural Networks","authors":"Chandramouli Kamanchi, Sumanta Mukherjee, Kameshwaran Sampath, Pankaj\n  Dayama, Arindam Jati, Vijay Ekambaram, Dzung Phan","authorsParsed":[["Kamanchi","Chandramouli",""],["Mukherjee","Sumanta",""],["Sampath","Kameshwaran",""],["Dayama","Pankaj",""],["Jati","Arindam",""],["Ekambaram","Vijay",""],["Phan","Dzung",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 07:36:49 GMT"},{"version":"v2","created":"Fri, 16 Aug 2024 01:19:04 GMT"}],"updateDate":"2024-08-19","timestamp":1723016209000,"abstract":"  Activation functions are non-linearities in neural networks that allow them\nto learn complex mapping between inputs and outputs. Typical choices for\nactivation functions are ReLU, Tanh, Sigmoid etc., where the choice generally\ndepends on the application domain. In this work, we propose a\nframework/strategy that unifies several works on activation functions and\ntheoretically explains the performance benefits of these works. We also propose\nnovel techniques that originate from the framework and allow us to obtain\n``extensions'' (i.e. special generalizations of a given neural network) of\nneural networks through operations on activation functions. We theoretically\nand empirically show that ``extensions'' of neural networks have performance\nbenefits compared to vanilla neural networks with insignificant space and time\ncomplexity costs on standard test functions. We also show the benefits of\nneural network ``extensions'' in the time-series domain on real-world datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Numerical Analysis","Computing Research Repository/Neural and Evolutionary Computing","Mathematics/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}