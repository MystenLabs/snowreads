{"id":"2407.09007","title":"Benchmarking Language Model Creativity: A Case Study on Code Generation","authors":"Yining Lu, Dixuan Wang, Tianjian Li, Dongwei Jiang, Daniel Khashabi","authorsParsed":[["Lu","Yining",""],["Wang","Dixuan",""],["Li","Tianjian",""],["Jiang","Dongwei",""],["Khashabi","Daniel",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 05:55:22 GMT"}],"updateDate":"2024-07-15","timestamp":1720763722000,"abstract":"  As LLMs become increasingly prevalent, it is interesting to consider how\n``creative'' these models can be. From cognitive science, creativity consists\nof at least two key characteristics: \\emph{convergent} thinking (purposefulness\nto achieve a given goal) and \\emph{divergent} thinking (adaptability to new\nenvironments or constraints) \\citep{runco2003critical}. In this work, we\nintroduce a framework for quantifying LLM creativity that incorporates the two\ncharacteristics. This is achieved by (1) Denial Prompting pushes LLMs to come\nup with more creative solutions to a given problem by incrementally imposing\nnew constraints on the previous solution, compelling LLMs to adopt new\nstrategies, and (2) defining and computing the NeoGauge metric which examines\nboth convergent and divergent thinking in the generated creative responses by\nLLMs. We apply the proposed framework on Codeforces problems, a natural data\nsource for collecting human coding solutions. We quantify NeoGauge for various\nproprietary and open-source models and find that even the most creative model,\nGPT-4, still falls short of demonstrating human-like creativity. We also\nexperiment with advanced reasoning strategies (MCTS, self-correction, etc.) and\nobserve no significant improvement in creativity. As a by-product of our\nanalysis, we release NeoCoder dataset for reproducing our results on future\nmodels.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}