{"id":"2407.21025","title":"Reinforcement Learning in High-frequency Market Making","authors":"Yuheng Zheng, Zihan Ding","authorsParsed":[["Zheng","Yuheng",""],["Ding","Zihan",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 22:07:48 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 16:51:02 GMT"}],"updateDate":"2024-08-13","timestamp":1720994868000,"abstract":"  This paper establishes a new and comprehensive theoretical analysis for the\napplication of reinforcement learning (RL) in high-frequency market making. We\nbridge the modern RL theory and the continuous-time statistical models in\nhigh-frequency financial economics. Different with most existing literature on\nmethodological research about developing various RL methods for market making\nproblem, our work is a pilot to provide the theoretical analysis. We target the\neffects of sampling frequency, and find an interesting tradeoff between error\nand complexity of RL algorithm when tweaking the values of the time increment\n$\\Delta$ $-$ as $\\Delta$ becomes smaller, the error will be smaller but the\ncomplexity will be larger. We also study the two-player case under the\ngeneral-sum game framework and establish the convergence of Nash equilibrium to\nthe continuous-time game equilibrium as $\\Delta\\rightarrow0$. The Nash\nQ-learning algorithm, which is an online multi-agent RL method, is applied to\nsolve the equilibrium. Our theories are not only useful for practitioners to\nchoose the sampling frequency, but also very general and applicable to other\nhigh-frequency financial decision making problems, e.g., optimal executions, as\nlong as the time-discretization of a continuous-time markov decision process is\nadopted. Monte Carlo simulation evidence support all of our theories.\n","subjects":["Quantitative Finance/Trading and Market Microstructure","Computing Research Repository/Machine Learning","Economics/Econometrics","Quantitative Finance/Statistical Finance","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}