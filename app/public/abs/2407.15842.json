{"id":"2407.15842","title":"Artist: Aesthetically Controllable Text-Driven Stylization without\n  Training","authors":"Ruixiang Jiang, Changwen Chen","authorsParsed":[["Jiang","Ruixiang",""],["Chen","Changwen",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 17:58:05 GMT"}],"updateDate":"2024-07-23","timestamp":1721671085000,"abstract":"  Diffusion models entangle content and style generation during the denoising\nprocess, leading to undesired content modification when directly applied to\nstylization tasks. Existing methods struggle to effectively control the\ndiffusion model to meet the aesthetic-level requirements for stylization. In\nthis paper, we introduce \\textbf{Artist}, a training-free approach that\naesthetically controls the content and style generation of a pretrained\ndiffusion model for text-driven stylization. Our key insight is to disentangle\nthe denoising of content and style into separate diffusion processes while\nsharing information between them. We propose simple yet effective content and\nstyle control methods that suppress style-irrelevant content generation,\nresulting in harmonious stylization results. Extensive experiments demonstrate\nthat our method excels at achieving aesthetic-level stylization requirements,\npreserving intricate details in the content image and aligning well with the\nstyle prompt. Furthermore, we showcase the highly controllability of the\nstylization strength from various perspectives. Code will be released, project\nhome page: https://DiffusionArtist.github.io\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}