{"id":"2408.13089","title":"On the good reliability of an interval-based metric to validate\n  prediction uncertainty for machine learning regression tasks","authors":"Pascal Pernot","authorsParsed":[["Pernot","Pascal",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 14:16:10 GMT"},{"version":"v2","created":"Mon, 26 Aug 2024 06:40:07 GMT"}],"updateDate":"2024-08-27","timestamp":1724422570000,"abstract":"  This short study presents an opportunistic approach to a (more) reliable\nvalidation method for prediction uncertainty average calibration. Considering\nthat variance-based calibration metrics (ZMS, NLL, RCE...) are quite sensitive\nto the presence of heavy tails in the uncertainty and error distributions, a\nshift is proposed to an interval-based metric, the Prediction Interval Coverage\nProbability (PICP). It is shown on a large ensemble of molecular properties\ndatasets that (1) sets of z-scores are well represented by Student's-$t(\\nu)$\ndistributions, $\\nu$ being the number of degrees of freedom; (2) accurate\nestimation of 95 $\\%$ prediction intervals can be obtained by the simple\n$2\\sigma$ rule for $\\nu>3$; and (3) the resulting PICPs are more quickly and\nreliably tested than variance-based calibration metrics. Overall, this method\nenables to test 20 $\\%$ more datasets than ZMS testing. Conditional calibration\nis also assessed using the PICP approach.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}