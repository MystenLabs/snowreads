{"id":"2407.14491","title":"PD-APE: A Parallel Decoding Framework with Adaptive Position Encoding\n  for 3D Visual Grounding","authors":"Chenshu Hou, Liang Peng, Xiaopei Wu, Xiaofei He, Wenxiao Wang","authorsParsed":[["Hou","Chenshu",""],["Peng","Liang",""],["Wu","Xiaopei",""],["He","Xiaofei",""],["Wang","Wenxiao",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:44:33 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 08:40:11 GMT"}],"updateDate":"2024-09-04","timestamp":1721411073000,"abstract":"  3D visual grounding aims to identify objects in 3D point cloud scenes that\nmatch specific natural language descriptions. This requires the model to not\nonly focus on the target object itself but also to consider the surrounding\nenvironment to determine whether the descriptions are met. Most previous works\nattempt to accomplish both tasks within the same module, which can easily lead\nto a distraction of attention. To this end, we propose PD-APE, a dual-branch\ndecoding framework that separately decodes target object attributes and\nsurrounding layouts. Specifically, in the target object branch, the decoder\nprocesses text tokens that describe features of the target object (e.g.,\ncategory and color), guiding the queries to pay attention to the target object\nitself. In the surrounding branch, the queries align with other text tokens\nthat carry surrounding environment information, making the attention maps\naccurately capture the layout described in the text. Benefiting from the\nproposed dual-branch design, the queries are allowed to focus on points\nrelevant to each branch's specific objective. Moreover, we design an adaptive\nposition encoding method for each branch respectively. In the target object\nbranch, the position encoding relies on the relative positions between seed\npoints and predicted 3D boxes. In the surrounding branch, the attention map is\nadditionally guided by the confidence between visual and text features,\nenabling the queries to focus on points that have valuable layout information.\nExtensive experiments demonstrate that we surpass the state-of-the-art on two\nwidely adopted 3D visual grounding datasets, ScanRefer and Nr3D.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}