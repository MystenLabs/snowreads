{"id":"2408.08295","title":"SLCA++: Unleash the Power of Sequential Fine-tuning for Continual\n  Learning with Pre-training","authors":"Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei","authorsParsed":[["Zhang","Gengwei",""],["Wang","Liyuan",""],["Kang","Guoliang",""],["Chen","Ling",""],["Wei","Yunchao",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:50:07 GMT"}],"updateDate":"2024-08-16","timestamp":1723744207000,"abstract":"  In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}