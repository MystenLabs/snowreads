{"id":"2408.11977","title":"An Asymptotically Optimal Coordinate Descent Algorithm for Learning\n  Bayesian Networks from Gaussian Models","authors":"Tong Xu, Simge K\\\"u\\c{c}\\\"ukyavuz, Ali Shojaie, Armeen Taeb","authorsParsed":[["Xu","Tong",""],["Küçükyavuz","Simge",""],["Shojaie","Ali",""],["Taeb","Armeen",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 20:18:03 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 18:14:39 GMT"}],"updateDate":"2024-09-19","timestamp":1724271483000,"abstract":"  This paper studies the problem of learning Bayesian networks from continuous\nobservational data, generated according to a linear Gaussian structural\nequation model. We consider an $\\ell_0$-penalized maximum likelihood estimator\nfor this problem which is known to have favorable statistical properties but is\ncomputationally challenging to solve, especially for medium-sized Bayesian\nnetworks. We propose a new coordinate descent algorithm to approximate this\nestimator and prove several remarkable properties of our procedure: the\nalgorithm converges to a coordinate-wise minimum, and despite the non-convexity\nof the loss function, as the sample size tends to infinity, the objective value\nof the coordinate descent solution converges to the optimal objective value of\nthe $\\ell_0$-penalized maximum likelihood estimator. Finite-sample statistical\nconsistency guarantees are also established. To the best of our knowledge, our\nproposal is the first coordinate descent procedure endowed with optimality and\nstatistical guarantees in the context of learning Bayesian networks. Numerical\nexperiments on synthetic and real data demonstrate that our coordinate descent\nmethod can obtain near-optimal solutions while being scalable.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YFbQONIkgMnRHO60QRer0dDvykoCzz2CRY-QWQViZpQ","pdfSize":"599326"}
