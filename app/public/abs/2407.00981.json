{"id":"2407.00981","title":"VisEval: A Benchmark for Data Visualization in the Era of Large Language\n  Models","authors":"Nan Chen, Yuge Zhang, Jiahang Xu, Kan Ren, Yuqing Yang","authorsParsed":[["Chen","Nan",""],["Zhang","Yuge",""],["Xu","Jiahang",""],["Ren","Kan",""],["Yang","Yuqing",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 05:35:30 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 09:52:44 GMT"}],"updateDate":"2024-08-08","timestamp":1719812130000,"abstract":"  Translating natural language to visualization (NL2VIS) has shown great\npromise for visual data analysis, but it remains a challenging task that\nrequires multiple low-level implementations, such as natural language\nprocessing and visualization design. Recent advancements in pre-trained large\nlanguage models (LLMs) are opening new avenues for generating visualizations\nfrom natural language. However, the lack of a comprehensive and reliable\nbenchmark hinders our understanding of LLMs' capabilities in visualization\ngeneration. In this paper, we address this gap by proposing a new NL2VIS\nbenchmark called VisEval. Firstly, we introduce a high-quality and large-scale\ndataset. This dataset includes 2,524 representative queries covering 146\ndatabases, paired with accurately labeled ground truths. Secondly, we advocate\nfor a comprehensive automated evaluation methodology covering multiple\ndimensions, including validity, legality, and readability. By systematically\nscanning for potential issues with a number of heterogeneous checkers, VisEval\nprovides reliable and trustworthy evaluation outcomes. We run VisEval on a\nseries of state-of-the-art LLMs. Our evaluation reveals prevalent challenges\nand delivers essential insights for future advancements.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}