{"id":"2408.11850","title":"Parallel Speculative Decoding with Adaptive Draft Length","authors":"Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, Winston Hu","authorsParsed":[["Liu","Tianyu",""],["Li","Yun",""],["Lv","Qitan",""],["Liu","Kai",""],["Zhu","Jianchen",""],["Hu","Winston",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 08:32:06 GMT"},{"version":"v2","created":"Wed, 4 Sep 2024 13:14:57 GMT"}],"updateDate":"2024-09-05","timestamp":1723537926000,"abstract":"  Speculative decoding (SD), where an extra draft model is employed to provide\nmultiple \\textit{draft} tokens first and then the original target model\nverifies these tokens in parallel, has shown great power for LLM inference\nacceleration. However, existing SD methods suffer from the mutual waiting\nproblem, i.e., the target model gets stuck when the draft model is\n\\textit{guessing} tokens, and vice versa. This problem is directly incurred by\nthe asynchronous execution of the draft model and the target model, and is\nexacerbated due to the fixed draft length in speculative decoding. To address\nthese challenges, we propose a conceptually simple, flexible, and general\nframework to boost speculative decoding, namely \\textbf{P}arallel\nsp\\textbf{E}culative decoding with \\textbf{A}daptive d\\textbf{R}aft\n\\textbf{L}ength (PEARL). Specifically, PEARL proposes \\textit{pre-verify} to\nverify the first draft token in advance during the drafting phase, and\n\\textit{post-verify} to generate more draft tokens during the verification\nphase. PEARL parallels the drafting phase and the verification phase via\napplying the two strategies, and achieves adaptive draft length for different\nscenarios, which effectively alleviates the mutual waiting problem. Moreover,\nwe theoretically demonstrate that the mean accepted tokens of PEARL is more\nthan existing \\textit{draft-then-verify} works. Experiments on various text\ngeneration benchmarks demonstrate the effectiveness of our \\name, leading to a\nsuperior speedup performance up to \\textbf{3.79$\\times$} and\n\\textbf{1.52$\\times$}, compared to auto-regressive decoding and vanilla\nspeculative decoding, respectively.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OcYeYFhOJcNEkiHXIlEEV54L5HL5EBJlLCZ2NSKM24k","pdfSize":"449557"}
