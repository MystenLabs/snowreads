{"id":"2408.12558","title":"Exploring the Role of Audio in Multimodal Misinformation Detection","authors":"Moyang Liu, Yukun Liu, Ruibo Fu, Zhengqi Wen, Jianhua Tao, Xuefei Liu,\n  Guanjun Li","authorsParsed":[["Liu","Moyang",""],["Liu","Yukun",""],["Fu","Ruibo",""],["Wen","Zhengqi",""],["Tao","Jianhua",""],["Liu","Xuefei",""],["Li","Guanjun",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 17:17:43 GMT"}],"updateDate":"2024-08-23","timestamp":1724347063000,"abstract":"  With the rapid development of deepfake technology, especially the deep audio\nfake technology, misinformation detection on the social media scene meets a\ngreat challenge. Social media data often contains multimodal information which\nincludes audio, video, text, and images. However, existing multimodal\nmisinformation detection methods tend to focus only on some of these\nmodalities, failing to comprehensively address information from all modalities.\nTo comprehensively address the various modal information that may appear on\nsocial media, this paper constructs a comprehensive multimodal misinformation\ndetection framework. By employing corresponding neural network encoders for\neach modality, the framework can fuse different modality information and\nsupport the multimodal misinformation detection task. Based on the constructed\nframework, this paper explores the importance of the audio modality in\nmultimodal misinformation detection tasks on social media. By adjusting the\narchitecture of the acoustic encoder, the effectiveness of different acoustic\nfeature encoders in the multimodal misinformation detection tasks is\ninvestigated. Furthermore, this paper discovers that audio and video\ninformation must be carefully aligned, otherwise the misalignment across\ndifferent audio and video modalities can severely impair the model performance.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}