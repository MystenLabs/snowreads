{"id":"2407.08931","title":"Global-Local Collaborative Inference with LLM for Lidar-Based\n  Open-Vocabulary Detection","authors":"Xingyu Peng, Yan Bai, Chen Gao, Lirong Yang, Fei Xia, Beipeng Mu,\n  Xiaofei Wang, and Si Liu","authorsParsed":[["Peng","Xingyu",""],["Bai","Yan",""],["Gao","Chen",""],["Yang","Lirong",""],["Xia","Fei",""],["Mu","Beipeng",""],["Wang","Xiaofei",""],["Liu","Si",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 02:34:11 GMT"}],"updateDate":"2024-07-15","timestamp":1720751651000,"abstract":"  Open-Vocabulary Detection (OVD) is the task of detecting all interesting\nobjects in a given scene without predefined object classes. Extensive work has\nbeen done to deal with the OVD for 2D RGB images, but the exploration of 3D OVD\nis still limited. Intuitively, lidar point clouds provide 3D information, both\nobject level and scene level, to generate trustful detection results. However,\nprevious lidar-based OVD methods only focus on the usage of object-level\nfeatures, ignoring the essence of scene-level information. In this paper, we\npropose a Global-Local Collaborative Scheme (GLIS) for the lidar-based OVD\ntask, which contains a local branch to generate object-level detection result\nand a global branch to obtain scene-level global feature. With the global-local\ninformation, a Large Language Model (LLM) is applied for chain-of-thought\ninference, and the detection result can be refined accordingly. We further\npropose Reflected Pseudo Labels Generation (RPLG) to generate high-quality\npseudo labels for supervision and Background-Aware Object Localization (BAOL)\nto select precise object proposals. Extensive experiments on ScanNetV2 and SUN\nRGB-D demonstrate the superiority of our methods. Code is released at\nhttps://github.com/GradiusTwinbee/GLIS.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}