{"id":"2407.07457","title":"GLBench: A Comprehensive Benchmark for Graph with Large Language Models","authors":"Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng\n  Cai, Victor Wai Kin Chan, Jia Li","authorsParsed":[["Li","Yuhan",""],["Wang","Peisong",""],["Zhu","Xiao",""],["Chen","Aochuan",""],["Jiang","Haiyun",""],["Cai","Deng",""],["Chan","Victor Wai Kin",""],["Li","Jia",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 08:20:47 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 06:06:33 GMT"}],"updateDate":"2024-07-12","timestamp":1720599647000,"abstract":"  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}