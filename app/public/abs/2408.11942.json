{"id":"2408.11942","title":"What are the limits of cross-lingual dense passage retrieval for\n  low-resource languages?","authors":"Jie Wu, Zhaochun Ren, Suzan Verberne","authorsParsed":[["Wu","Jie",""],["Ren","Zhaochun",""],["Verberne","Suzan",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 18:51:46 GMT"}],"updateDate":"2024-08-23","timestamp":1724266306000,"abstract":"  In this paper, we analyze the capabilities of the multi-lingual Dense Passage\nRetriever (mDPR) for extremely low-resource languages. In the Cross-lingual\nOpen-Retrieval Answer Generation (CORA) pipeline, mDPR achieves success on\nmultilingual open QA benchmarks across 26 languages, of which 9 were unseen\nduring training. These results are promising for Question Answering (QA) for\nlow-resource languages. We focus on two extremely low-resource languages for\nwhich mDPR performs poorly: Amharic and Khmer. We collect and curate datasets\nto train mDPR models using Translation Language Modeling (TLM) and\nquestion--passage alignment. We also investigate the effect of our extension on\nthe language distribution in the retrieval results. Our results on the MKQA and\nAmQA datasets show that language alignment brings improvements to mDPR for the\nlow-resource languages, but the improvements are modest and the results remain\nlow. We conclude that fulfilling CORA's promise to enable multilingual open QA\nin extremely low-resource settings is challenging because the model, the data,\nand the evaluation approach are intertwined. Hence, all three need attention in\nfollow-up work. We release our code for reproducibility and future work:\nhttps://anonymous.4open.science/r/Question-Answering-for-Low-Resource-Languages-B13C/\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"khtYUmyHlZEZsCgR2M5iUZ6TPCvzj-A6aPmcbJ4jHtw","pdfSize":"392693"}
