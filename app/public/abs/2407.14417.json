{"id":"2407.14417","title":"Mixture of Experts with Mixture of Precisions for Tuning Quality of\n  Service","authors":"HamidReza Imani, Abdolah Amirany, and Tarek El-Ghazawi","authorsParsed":[["Imani","HamidReza",""],["Amirany","Abdolah",""],["El-Ghazawi","Tarek",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 15:42:49 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 16:34:00 GMT"}],"updateDate":"2024-09-10","timestamp":1721403769000,"abstract":"  The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 3.81 to 4.00, 13.59 to 14.17, and 7.24 to 7.40\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Performance"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}