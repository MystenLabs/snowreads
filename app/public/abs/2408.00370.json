{"id":"2408.00370","title":"DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer\n  Normalization Mamba-2 framework","authors":"Fan Zhang, Naye Ji, Fuxing Gao, Bozuo Zhao, Jingmei Wu, Yanbing Jiang,\n  Hui Du, Zhenqing Ye, Jiayang Zhu, WeiFan Zhong, Leyao Yan, Xiaomeng Ma","authorsParsed":[["Zhang","Fan",""],["Ji","Naye",""],["Gao","Fuxing",""],["Zhao","Bozuo",""],["Wu","Jingmei",""],["Jiang","Yanbing",""],["Du","Hui",""],["Ye","Zhenqing",""],["Zhu","Jiayang",""],["Zhong","WeiFan",""],["Yan","Leyao",""],["Ma","Xiaomeng",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 08:22:47 GMT"}],"updateDate":"2024-08-02","timestamp":1722500567000,"abstract":"  Speech-driven gesture generation is an emerging domain within virtual human\ncreation, where current methods predominantly utilize Transformer-based\narchitectures that necessitate extensive memory and are characterized by slow\ninference speeds. In response to these limitations, we propose\n\\textit{DiM-Gestures}, a novel end-to-end generative model crafted to create\nhighly personalized 3D full-body gestures solely from raw speech audio,\nemploying Mamba-based architectures. This model integrates a Mamba-based fuzzy\nfeature extractor with a non-autoregressive Adaptive Layer Normalization\n(AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba\nframework and a WavLM pre-trained model, autonomously derives implicit,\ncontinuous fuzzy features, which are then unified into a singular latent\nfeature. This feature is processed by the AdaLN Mamba-2, which implements a\nuniform conditional mechanism across all tokens to robustly model the interplay\nbetween the fuzzy features and the resultant gesture sequence. This innovative\napproach guarantees high fidelity in gesture-speech synchronization while\nmaintaining the naturalness of the gestures. Employing a diffusion model for\ntraining and inference, our framework has undergone extensive subjective and\nobjective evaluations on the ZEGGS and BEAT datasets. These assessments\nsubstantiate our model's enhanced performance relative to contemporary\nstate-of-the-art methods, demonstrating competitive outcomes with the DiTs\narchitecture (Persona-Gestors) while optimizing memory usage and accelerating\ninference speed.\n","subjects":["Computing Research Repository/Graphics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by/4.0/"}