{"id":"2408.01426","title":"MolTRES: Improving Chemical Language Representation Learning for\n  Molecular Property Prediction","authors":"Jun-Hyung Park, Yeachan Kim, Mingyu Lee, Hyuntae Park, SangKeun Lee\n  (Korea University)","authorsParsed":[["Park","Jun-Hyung","","Korea University"],["Kim","Yeachan","","Korea University"],["Lee","Mingyu","","Korea University"],["Park","Hyuntae","","Korea University"],["Lee","SangKeun","","Korea University"]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 01:14:28 GMT"}],"updateDate":"2024-08-06","timestamp":1720487668000,"abstract":"  Chemical representation learning has gained increasing interest due to the\nlimited availability of supervised data in fields such as drug and materials\ndesign. This interest particularly extends to chemical language representation\nlearning, which involves pre-training Transformers on SMILES sequences --\ntextual descriptors of molecules. Despite its success in molecular property\nprediction, current practices often lead to overfitting and limited scalability\ndue to early convergence. In this paper, we introduce a novel chemical language\nrepresentation learning framework, called MolTRES, to address these issues.\nMolTRES incorporates generator-discriminator training, allowing the model to\nlearn from more challenging examples that require structural understanding. In\naddition, we enrich molecular representations by transferring knowledge from\nscientific literature by integrating external materials embedding. Experimental\nresults show that our model outperforms existing state-of-the-art models on\npopular molecular property prediction tasks.\n","subjects":["Physics/Chemical Physics","Condensed Matter/Materials Science","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}