{"id":"2408.09266","title":"Graph Classification with GNNs: Optimisation, Representation and\n  Inductive Bias","authors":"P. Krishna Kumar a and Harish G. Ramaswamy","authorsParsed":[["a","P. Krishna Kumar",""],["Ramaswamy","Harish G.",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 18:15:44 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 09:55:08 GMT"}],"updateDate":"2024-08-26","timestamp":1723918544000,"abstract":"  Theoretical studies on the representation power of GNNs have been centered\naround understanding the equivalence of GNNs, using WL-Tests for detecting\ngraph isomorphism. In this paper, we argue that such equivalence ignores the\naccompanying optimization issues and does not provide a holistic view of the\nGNN learning process. We illustrate these gaps between representation and\noptimization with examples and experiments. We also explore the existence of an\nimplicit inductive bias (e.g. fully connected networks prefer to learn low\nfrequency functions in their input space) in GNNs, in the context of graph\nclassification tasks. We further prove theoretically that the message-passing\nlayers in the graph, have a tendency to search for either discriminative\nsubgraphs, or a collection of discriminative nodes dispersed across the graph,\ndepending on the different global pooling layers used. We empirically verify\nthis bias through experiments over real-world and synthetic datasets. Finally,\nwe show how our work can help in incorporating domain knowledge via attention\nbased architectures, and can evince their capability to discriminate coherent\nsubgraphs.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}