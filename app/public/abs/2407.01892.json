{"id":"2407.01892","title":"GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial\n  Reasoning","authors":"Zhisheng Tang, Mayank Kejriwal","authorsParsed":[["Tang","Zhisheng",""],["Kejriwal","Mayank",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 02:27:46 GMT"}],"updateDate":"2024-07-03","timestamp":1719887266000,"abstract":"  Spatial reasoning, an important faculty of human cognition with many\npractical applications, is one of the core commonsense skills that is not\npurely language-based and, for satisfying (as opposed to optimal) solutions,\nrequires some minimum degree of planning. Existing benchmarks of Commonsense\nSpatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)\ninterpret text-based spatial descriptions rather than directly evaluate a plan\nproduced by the LLM in response to a spatial reasoning scenario. In this paper,\nwe construct a large-scale benchmark called $\\textbf{GRASP}$, which consists of\n16,000 grid-based environments where the agent is tasked with an energy\ncollection problem. These environments include 100 grid instances instantiated\nusing each of the 160 different grid settings, involving five different energy\ndistributions, two modes of agent starting position, and two distinct obstacle\nconfigurations, as well as three kinds of agent constraints. Using GRASP, we\ncompare classic baseline approaches, such as random walk and greedy search\nmethods, with advanced LLMs like GPT-3.5-Turbo and GPT-4o. The experimental\nresults indicate that even these advanced LLMs struggle to consistently achieve\nsatisfactory solutions.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LAiJGFH-JrDmwAUTIfSlC70fgLqsuzKdbXj7A0ewKjU","pdfSize":"3649730"}
