{"id":"2407.14129","title":"Comparing and Contrasting Deep Learning Weather Prediction Backbones on\n  Navier-Stokes and Atmospheric Dynamics","authors":"Matthias Karlbauer, Danielle C. Maddix, Abdul Fatir Ansari, Boran Han,\n  Gaurav Gupta, Yuyang Wang, Andrew Stuart, Michael W. Mahoney","authorsParsed":[["Karlbauer","Matthias",""],["Maddix","Danielle C.",""],["Ansari","Abdul Fatir",""],["Han","Boran",""],["Gupta","Gaurav",""],["Wang","Yuyang",""],["Stuart","Andrew",""],["Mahoney","Michael W.",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 08:59:00 GMT"}],"updateDate":"2024-07-22","timestamp":1721379540000,"abstract":"  Remarkable progress in the development of Deep Learning Weather Prediction\n(DLWP) models positions them to become competitive with traditional numerical\nweather prediction (NWP) models. Indeed, a wide number of DLWP architectures --\nbased on various backbones, including U-Net, Transformer, Graph Neural Network\n(GNN), and Fourier Neural Operator (FNO) -- have demonstrated their potential\nat forecasting atmospheric states. However, due to differences in training\nprotocols, forecast horizons, and data choices, it remains unclear which (if\nany) of these methods and architectures are most suitable for weather\nforecasting and for future model development. Here, we step back and provide a\ndetailed empirical analysis, under controlled conditions, comparing and\ncontrasting the most prominent DLWP models, along with their backbones. We\naccomplish this by predicting synthetic two-dimensional incompressible\nNavier-Stokes and real-world global weather dynamics. In terms of accuracy,\nmemory consumption, and runtime, our results illustrate various tradeoffs. For\nexample, on synthetic data, we observe favorable performance of FNO; and on the\nreal-world WeatherBench dataset, our results demonstrate the suitability of\nConvLSTM and SwinTransformer for short-to-mid-ranged forecasts. For long-ranged\nweather rollouts of up to 365 days, we observe superior stability and physical\nsoundness in architectures that formulate a spherical data representation,\ni.e., GraphCast and Spherical FNO. In addition, we observe that all of these\nmodel backbones ``saturate,'' i.e., none of them exhibit so-called neural\nscaling, which highlights an important direction for future work on these and\nrelated models.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nUFl9yMKw0Nf_FWf1ALnJmKzQHK2rNCWw3VChu-QXcw","pdfSize":"12543056","objectId":"0x9951843a6bcd2d05a4a9639466ef91750e6889bf90b0a718d91330bfd050ad7b","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
