{"id":"2407.15992","title":"Multimodal Input Aids a Bayesian Model of Phonetic Learning","authors":"Sophia Zhi, Roger P. Levy, Stephan C. Meylan","authorsParsed":[["Zhi","Sophia",""],["Levy","Roger P.",""],["Meylan","Stephan C.",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 19:00:11 GMT"}],"updateDate":"2024-07-24","timestamp":1721674811000,"abstract":"  One of the many tasks facing the typically-developing child language learner\nis learning to discriminate between the distinctive sounds that make up words\nin their native language. Here we investigate whether multimodal\ninformation--specifically adult speech coupled with video frames of speakers'\nfaces--benefits a computational model of phonetic learning. We introduce a\nmethod for creating high-quality synthetic videos of speakers' faces for an\nexisting audio corpus. Our learning model, when both trained and tested on\naudiovisual inputs, achieves up to a 8.1% relative improvement on a phoneme\ndiscrimination battery compared to a model trained and tested on audio-only\ninput. It also outperforms the audio model by up to 3.9% when both are tested\non audio-only data, suggesting that visual information facilitates the\nacquisition of acoustic distinctions. Visual information is especially\nbeneficial in noisy audio environments, where an audiovisual model closes 67%\nof the loss in discrimination performance of the audio model in noise relative\nto a non-noisy environment. These results demonstrate that visual information\nbenefits an ideal learner and illustrate some of the ways that children might\nbe able to leverage visual cues when learning to discriminate speech sounds.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}