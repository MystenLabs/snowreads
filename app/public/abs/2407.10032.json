{"id":"2407.10032","title":"LeanQuant: Accurate Large Language Model Quantization with\n  Loss-Error-Aware Grid","authors":"Tianyi Zhang, Anshumali Shrivastava","authorsParsed":[["Zhang","Tianyi",""],["Shrivastava","Anshumali",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 00:23:51 GMT"}],"updateDate":"2024-07-16","timestamp":1720916631000,"abstract":"  Large language models (LLMs) have numerous applications across various\ndomains, but their high computational and memory demands pose significant\ndeployment challenges. Weight quantization is an effective technique for\nreducing the decoding latency and memory requirements of LLMs. Existing\napproaches primarily aim to maintain the quality of quantized models by\npreserving outliers in input features, but they still suffer significant\nquality loss at lower bit widths. Our approach builds on Optimal Brain\nQuantization (OBQ), an iterative weight-update-based quantization framework. We\nidentify a key limitation of OBQ, specifically that its uniform quantization\ngrid is suboptimal for maintaining model quality, as it introduces large errors\nto the task loss. To address this, we propose LeanQuant, which learns a\nloss-error-aware quantization grid by leveraging the inverse diagonal Hessian.\nExtensive empirical evaluations demonstrate that LeanQuant is both efficient\nand accurate; it can quantize a 70-billion-parameter model in 6 hours using a\nsingle 32GB GPU and performs favorably compared to competitive baselines in the\n4-bit, 3-bit, and 2-bit regions.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}