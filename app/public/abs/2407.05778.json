{"id":"2407.05778","title":"When is the consistent prediction likely to be a correct prediction?","authors":"Alex Nguyen, Dheeraj Mekala, Chengyu Dong, Jingbo Shang","authorsParsed":[["Nguyen","Alex",""],["Mekala","Dheeraj",""],["Dong","Chengyu",""],["Shang","Jingbo",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 09:37:27 GMT"}],"updateDate":"2024-07-09","timestamp":1720431447000,"abstract":"  Self-consistency (Wang et al., 2023) suggests that the most consistent answer\nobtained through large language models (LLMs) is more likely to be correct. In\nthis paper, we challenge this argument and propose a nuanced correction. Our\nobservations indicate that consistent answers derived through more computation\ni.e. longer reasoning texts, rather than simply the most consistent answer\nacross all outputs, are more likely to be correct. This is predominantly\nbecause we demonstrate that LLMs can autonomously produce chain-of-thought\n(CoT) style reasoning with no custom prompts merely while generating longer\nresponses, which lead to consistent predictions that are more accurate. In the\nzero-shot setting, by sampling Mixtral-8x7B model multiple times and\nconsidering longer responses, we achieve 86% of its self-consistency\nperformance obtained through zero-shot CoT prompting on the GSM8K and\nMultiArith datasets. Finally, we demonstrate that the probability of LLMs\ngenerating a longer response is quite low, highlighting the need for decoding\nstrategies conditioned on output length.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}