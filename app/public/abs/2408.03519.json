{"id":"2408.03519","title":"RepoMasterEval: Evaluating Code Completion via Real-World Repositories","authors":"Qinyun Wu, Chao Peng, Pengfei Gao, Ruida Hu, Haoyu Gan, Bo Jiang,\n  Jinhe Tang, Zhiwen Deng, Zhanming Guan, Cuiyun Gao, Xia Liu, Ping Yang","authorsParsed":[["Wu","Qinyun",""],["Peng","Chao",""],["Gao","Pengfei",""],["Hu","Ruida",""],["Gan","Haoyu",""],["Jiang","Bo",""],["Tang","Jinhe",""],["Deng","Zhiwen",""],["Guan","Zhanming",""],["Gao","Cuiyun",""],["Liu","Xia",""],["Yang","Ping",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 03:06:57 GMT"}],"updateDate":"2024-08-08","timestamp":1723000017000,"abstract":"  With the growing reliance on automated code completion tools in software\ndevelopment, the need for robust evaluation benchmarks has become critical.\nHowever, existing benchmarks focus more on code generation tasks in function\nand class level and provide rich text description to prompt the model. By\ncontrast, such descriptive prompt is commonly unavailable in real development\nand code completion can occur in wider range of situations such as in the\nmiddle of a function or a code block. These limitations makes the evaluation\npoorly align with the practical scenarios of code completion tools. In this\npaper, we propose RepoMasterEval, a novel benchmark for evaluating code\ncompletion models constructed from real-world Python and TypeScript\nrepositories. Each benchmark datum is generated by masking a code snippet\n(ground truth) from one source code file with existing test suites. To improve\ntest accuracy of model generated code, we employ mutation testing to measure\nthe effectiveness of the test cases and we manually crafted new test cases for\nthose test suites with low mutation score. Our empirical evaluation on 6\nstate-of-the-art models shows that test argumentation is critical in improving\nthe accuracy of the benchmark and RepoMasterEval is able to report difference\nin model performance in real-world scenarios. The deployment of RepoMasterEval\nin a collaborated company for one month also revealed that the benchmark is\nuseful to give accurate feedback during model training and the score is in high\ncorrelation with the model's performance in practice. Based on our findings, we\ncall for the software engineering community to build more LLM benchmarks\ntailored for code generation tools taking the practical and complex development\nenvironment into consideration.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"tIkFOAGu9BTklOHDwajn4EpGgAC_D4BcifIqPpCjTWI","pdfSize":"807620"}
