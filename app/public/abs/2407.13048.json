{"id":"2407.13048","title":"Establishing Knowledge Preference in Language Models","authors":"Sizhe Zhou, Sha Li, Yu Meng, Yizhu Jiao, Heng Ji, Jiawei Han","authorsParsed":[["Zhou","Sizhe",""],["Li","Sha",""],["Meng","Yu",""],["Jiao","Yizhu",""],["Ji","Heng",""],["Han","Jiawei",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 23:16:11 GMT"}],"updateDate":"2024-07-19","timestamp":1721258171000,"abstract":"  Language models are known to encode a great amount of factual knowledge\nthrough pretraining. However, such knowledge might be insufficient to cater to\nuser requests, requiring the model to integrate external knowledge sources and\nadhere to user-provided specifications. When answering questions about ongoing\nevents, the model should use recent news articles to update its response; when\nasked to provide recommendations, the model should prioritize user\nspecifications over retrieved product reviews; when some facts are edited in\nthe model, the updated facts should override all prior knowledge learned by the\nmodel even if they are conflicting. In all of the cases above, the model faces\na decision between its own parametric knowledge, (retrieved) contextual\nknowledge, and user instruction knowledge. In this paper, we (1) unify such\nsettings into the problem of knowledge preference and define a three-level\npreference hierarchy over these knowledge sources; (2) compile a collection of\nexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings\n(with/without user specifications, with/without context documents) to\nsystematically evaluate how well models obey the intended knowledge preference;\nand (3) propose a dataset synthesis method that composes diverse\nquestion-answer pairs with user assumptions and related context to directly\nfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a\n7B model, fine-tuned on only a few thousand examples automatically generated by\nour proposed method, effectively achieves superior performance (more than 18%\nimprovement across all evaluation benchmarks) in adhering to the desired\nknowledge preference hierarchy.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}