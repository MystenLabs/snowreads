{"id":"2408.08262","title":"Coarsening and parallelism with reduction multigrids for hyperbolic\n  Boltzmann transport","authors":"S. Dargaville, R.P. Smedley-Stevenson, P.N. Smith, C.C. Pain","authorsParsed":[["Dargaville","S.",""],["Smedley-Stevenson","R. P.",""],["Smith","P. N.",""],["Pain","C. C.",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:06:56 GMT"}],"updateDate":"2024-08-16","timestamp":1723741616000,"abstract":"  Reduction multigrids have recently shown good performance in hyperbolic\nproblems without the need for Gauss-Seidel smoothers. When applied to the\nhyperbolic limit of the Boltzmann Transport Equation (BTE), these methods\nresult in very close to $\\mathcal{O}(n)$ growth in work with problem size on\nunstructured grids. This scalability relies on the CF splitting producing an\n$A_\\textrm{ff}$ block that is easy to invert. We introduce a parallel two-pass\nCF splitting designed to give diagonally dominant $A_\\textrm{ff}$. The first\npass computes a maximal independent set in the symmetrized strong connections.\nThe second pass converts F-points to C-points based on the row-wise diagonal\ndominance of $A_\\textrm{ff}$. We find this two-pass CF splitting outperforms\ncommon CF splittings available in hypre.\n  Furthermore, parallelisation of reduction multigrids in hyperbolic problems\nis difficult as we require both long-range grid-transfer operators and slow\ncoarsenings (with rates of $\\sim$1/2 in both 2D and 3D). We find that good\nparallel performance in the setup and solve is dependent on several factors:\nrepartitioning the coarse grids, reducing the number of active MPI ranks as we\ncoarsen, truncating the multigrid hierarchy and applying a GMRES polynomial as\na coarse-grid solver.\n  We compare the performance of two different reduction multigrids, AIRG (that\nwe developed previously) and the hypre implementation of $\\ell$AIR. In the\nstreaming limit with AIRG, we demonstrate 81\\% weak scaling efficiency in the\nsolve from 2 to 64 nodes (256 to 8196 cores) with only 8.8k unknowns per core,\nwith solve times up to 5.9$\\times$ smaller than the $\\ell$AIR implementation in\nhypre.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis","Physics/Computational Physics"],"license":"http://creativecommons.org/licenses/by/4.0/"}