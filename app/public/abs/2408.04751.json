{"id":"2408.04751","title":"Sequential Hamiltonian Assembly: Enhancing the training of combinatorial\n  optimization problems on quantum computers","authors":"Navid Roshani, Jonas Stein, Maximilian Zorn, Michael K\\\"olle, Philipp\n  Altmann, Claudia Linnhoff-Popien","authorsParsed":[["Roshani","Navid",""],["Stein","Jonas",""],["Zorn","Maximilian",""],["KÃ¶lle","Michael",""],["Altmann","Philipp",""],["Linnhoff-Popien","Claudia",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 20:32:18 GMT"}],"updateDate":"2024-08-12","timestamp":1723149138000,"abstract":"  A central challenge in quantum machine learning is the design and training of\nparameterized quantum circuits (PQCs). Much like in deep learning, vanishing\ngradients pose significant obstacles to the trainability of PQCs, arising from\nvarious sources. One such source is the presence of non-local loss functions,\nwhich require the measurement of a large subset of qubits involved. To address\nthis issue and facilitate parameter training for quantum applications using\nglobal loss functions, we propose Sequential Hamiltonian Assembly (SHA). SHA\niteratively approximates the loss by assembling it from local components. To\nfurther demonstrate the feasibility of our approach, we extend our previous\ncase study by introducing a new partitioning strategy, a new merger between\nQAOA and SHA, and an evaluation of SHA onto the Max-Cut optimization problem.\nSimulation results show that SHA outperforms conventional parameter training by\n43.89% and the empirical state-of-the-art, Layer-VQE by 29.08% in the mean\naccuracy for Max-Cut. This paves the way for locality-aware learning\ntechniques, mitigating vanishing gradients for a large class of practically\nrelevant problems.\n","subjects":["Physics/Quantum Physics","Computing Research Repository/Emerging Technologies"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"waGg6XyXz7wI0rdBYrGcHPYgajtl9oY2D0WX6ZWpxsA","pdfSize":"584762"}
