{"id":"2407.10955","title":"Enhancing Stochastic Optimization for Statistical Efficiency Using\n  ROOT-SGD with Diminishing Stepsize","authors":"Chris Junchi Li","authorsParsed":[["Li","Chris Junchi",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 17:54:03 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 09:59:35 GMT"}],"updateDate":"2024-08-26","timestamp":1721066043000,"abstract":"  In this paper, we revisit \\textsf{ROOT-SGD}, an innovative method for\nstochastic optimization to bridge the gap between stochastic optimization and\nstatistical efficiency. The proposed method enhances the performance and\nreliability of \\textsf{ROOT-SGD} by integrating a carefully designed\n\\emph{diminishing stepsize strategy}. This approach addresses key challenges in\noptimization, providing robust theoretical guarantees and practical benefits.\nOur analysis demonstrates that \\textsf{ROOT-SGD} with diminishing achieves\noptimal convergence rates while maintaining computational efficiency. By\ndynamically adjusting the learning rate, \\textsf{ROOT-SGD} ensures improved\nstability and precision throughout the optimization process. The findings of\nthis study offer valuable insights for developing advanced optimization\nalgorithms that are both efficient and statistically robust.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}