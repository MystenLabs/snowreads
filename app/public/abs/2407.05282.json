{"id":"2407.05282","title":"UltraEdit: Instruction-based Fine-Grained Image Editing at Scale","authors":"Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai\n  An, Peiyu Yu, Minjia Zhang, Qing Li, Baobao Chang","authorsParsed":[["Zhao","Haozhe",""],["Ma","Xiaojian",""],["Chen","Liang",""],["Si","Shuzheng",""],["Wu","Rujie",""],["An","Kaikai",""],["Yu","Peiyu",""],["Zhang","Minjia",""],["Li","Qing",""],["Chang","Baobao",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 06:50:22 GMT"}],"updateDate":"2024-07-09","timestamp":1720335022000,"abstract":"  This paper presents UltraEdit, a large-scale (approximately 4 million editing\nsamples), automatically generated dataset for instruction-based image editing.\nOur key idea is to address the drawbacks in existing image editing datasets\nlike InstructPix2Pix and MagicBrush, and provide a systematic approach to\nproducing massive and high-quality image editing samples. UltraEdit offers\nseveral distinct advantages: 1) It features a broader range of editing\ninstructions by leveraging the creativity of large language models (LLMs)\nalongside in-context editing examples from human raters; 2) Its data sources\nare based on real images, including photographs and artworks, which provide\ngreater diversity and reduced bias compared to datasets solely generated by\ntext-to-image models; 3) It also supports region-based editing, enhanced by\nhigh-quality, automatically produced region annotations. Our experiments show\nthat canonical diffusion-based editing baselines trained on UltraEdit set new\nrecords on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms\nthe crucial role of real image anchors and region-based editing data. The\ndataset, code, and models can be found in https://ultra-editing.github.io.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}