{"id":"2407.20157","title":"rLLM: Relational Table Learning with LLMs","authors":"Weichen Li, Xiaotong Huang, Jianwu Zheng, Zheng Wang, Chaokun Wang, Li\n  Pan, Jianhua Li","authorsParsed":[["Li","Weichen",""],["Huang","Xiaotong",""],["Zheng","Jianwu",""],["Wang","Zheng",""],["Wang","Chaokun",""],["Pan","Li",""],["Li","Jianhua",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 16:33:40 GMT"}],"updateDate":"2024-07-30","timestamp":1722270820000,"abstract":"  We introduce rLLM (relationLLM), a PyTorch library designed for Relational\nTable Learning (RTL) with Large Language Models (LLMs). The core idea is to\ndecompose state-of-the-art Graph Neural Networks, LLMs, and Table Neural\nNetworks into standardized modules, to enable the fast construction of novel\nRTL-type models in a simple \"combine, align, and co-train\" manner. To\nillustrate the usage of rLLM, we introduce a simple RTL method named\n\\textbf{BRIDGE}. Additionally, we present three novel relational tabular\ndatasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hope\nrLLM can serve as a useful and easy-to-use development framework for\nRTL-related tasks. Our code is available at:\nhttps://github.com/rllm-project/rllm.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}