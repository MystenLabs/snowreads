{"id":"2407.17686","title":"Transformers on Markov Data: Constant Depth Suffices","authors":"Nived Rajaraman, Marco Bondaschi, Kannan Ramchandran, Michael Gastpar,\n  Ashok Vardhan Makkuva","authorsParsed":[["Rajaraman","Nived",""],["Bondaschi","Marco",""],["Ramchandran","Kannan",""],["Gastpar","Michael",""],["Makkuva","Ashok Vardhan",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 01:07:09 GMT"}],"updateDate":"2024-07-26","timestamp":1721869629000,"abstract":"  Attention-based transformers have been remarkably successful at modeling\ngenerative processes across various domains and modalities. In this paper, we\nstudy the behavior of transformers on data drawn from \\kth Markov processes,\nwhere the conditional distribution of the next symbol in a sequence depends on\nthe previous $k$ symbols observed. We observe a surprising phenomenon\nempirically which contradicts previous findings: when trained for sufficiently\nlong, a transformer with a fixed depth and $1$ head per layer is able to\nachieve low test loss on sequences drawn from \\kth Markov sources, even as $k$\ngrows. Furthermore, this low test loss is achieved by the transformer's ability\nto represent and learn the in-context conditional empirical distribution. On\nthe theoretical side, our main result is that a transformer with a single head\nand three layers can represent the in-context conditional empirical\ndistribution for \\kth Markov sources, concurring with our empirical\nobservations. Along the way, we prove that \\textit{attention-only} transformers\nwith $O(\\log_2(k))$ layers can represent the in-context conditional empirical\ndistribution by composing induction heads to track the previous $k$ symbols in\nthe sequence. These results provide more insight into our current understanding\nof the mechanisms by which transformers learn to capture context, by\nunderstanding their behavior on Markov sources.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Information Theory","Mathematics/Information Theory","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}