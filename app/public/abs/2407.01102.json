{"id":"2407.01102","title":"BERGEN: A Benchmarking Library for Retrieval-Augmented Generation","authors":"David Rau, Herv\\'e D\\'ejean, Nadezhda Chirkova, Thibault Formal, Shuai\n  Wang, Vassilina Nikoulina, St\\'ephane Clinchant","authorsParsed":[["Rau","David",""],["Déjean","Hervé",""],["Chirkova","Nadezhda",""],["Formal","Thibault",""],["Wang","Shuai",""],["Nikoulina","Vassilina",""],["Clinchant","Stéphane",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 09:09:27 GMT"}],"updateDate":"2024-07-02","timestamp":1719824967000,"abstract":"  Retrieval-Augmented Generation allows to enhance Large Language Models with\nexternal knowledge. In response to the recent popularity of generative LLMs,\nmany RAG approaches have been proposed, which involve an intricate number of\ndifferent configurations such as evaluation datasets, collections, metrics,\nretrievers, and LLMs. Inconsistent benchmarking poses a major challenge in\ncomparing approaches and understanding the impact of each component in the\npipeline. In this work, we study best practices that lay the groundwork for a\nsystematic evaluation of RAG and present BERGEN, an end-to-end library for\nreproducible research standardizing RAG experiments. In an extensive study\nfocusing on QA, we benchmark different state-of-the-art retrievers, rerankers,\nand LLMs. Additionally, we analyze existing RAG metrics and datasets. Our\nopen-source library BERGEN is available under\n\\url{https://github.com/naver/bergen}.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"I9kju50bmVoUINcHTzCz9HG_seAimcR-1AYFH9glG4M","pdfSize":"906478"}
