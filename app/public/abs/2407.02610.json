{"id":"2407.02610","title":"Towards Federated Learning with On-device Training and Communication in\n  8-bit Floating Point","authors":"Bokun Wang and Axel Berg and Durmus Alp Emre Acar and Chuteng Zhou","authorsParsed":[["Wang","Bokun",""],["Berg","Axel",""],["Acar","Durmus Alp Emre",""],["Zhou","Chuteng",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 18:55:58 GMT"}],"updateDate":"2024-07-04","timestamp":1719946558000,"abstract":"  Recent work has shown that 8-bit floating point (FP8) can be used for\nefficiently training neural networks with reduced computational overhead\ncompared to training in FP32/FP16. In this work, we investigate the use of FP8\ntraining in a federated learning context. This brings not only the usual\nbenefits of FP8 which are desirable for on-device training at the edge, but\nalso reduces client-server communication costs due to significant weight\ncompression. We present a novel method for combining FP8 client training while\nmaintaining a global FP32 server model and provide convergence analysis.\nExperiments with various machine learning models and datasets show that our\nmethod consistently yields communication reductions of at least 2.9x across a\nvariety of tasks and models compared to an FP32 baseline.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}