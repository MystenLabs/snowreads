{"id":"2407.13198","title":"DiveSound: LLM-Assisted Automatic Taxonomy Construction for Diverse\n  Audio Generation","authors":"Baihan Li, Zeyu Xie, Xuenan Xu, Yiwei Guo, Ming Yan, Ji Zhang, Kai Yu,\n  Mengyue Wu","authorsParsed":[["Li","Baihan",""],["Xie","Zeyu",""],["Xu","Xuenan",""],["Guo","Yiwei",""],["Yan","Ming",""],["Zhang","Ji",""],["Yu","Kai",""],["Wu","Mengyue",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 06:23:34 GMT"}],"updateDate":"2024-07-19","timestamp":1721283814000,"abstract":"  Audio generation has attracted significant attention. Despite remarkable\nenhancement in audio quality, existing models overlook diversity evaluation.\nThis is partially due to the lack of a systematic sound class diversity\nframework and a matching dataset. To address these issues, we propose\nDiveSound, a novel framework for constructing multimodal datasets with in-class\ndiversified taxonomy, assisted by large language models. As both textual and\nvisual information can be utilized to guide diverse generation, DiveSound\nleverages multimodal contrastive representations in data construction. Our\nframework is highly autonomous and can be easily scaled up. We provide a\ntextaudio-image aligned diversity dataset whose sound event class tags have an\naverage of 2.42 subcategories. Text-to-audio experiments on the constructed\ndataset show a substantial increase of diversity with the help of the guidance\nof visual information.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}