{"id":"2408.08624","title":"RealMedQA: A pilot biomedical question answering dataset containing\n  realistic clinical questions","authors":"Gregory Kell, Angus Roberts, Serge Umansky, Yuti Khare, Najma Ahmed,\n  Nikhil Patel, Chloe Simela, Jack Coumbe, Julian Rozario, Ryan-Rhys Griffiths,\n  Iain J. Marshall","authorsParsed":[["Kell","Gregory",""],["Roberts","Angus",""],["Umansky","Serge",""],["Khare","Yuti",""],["Ahmed","Najma",""],["Patel","Nikhil",""],["Simela","Chloe",""],["Coumbe","Jack",""],["Rozario","Julian",""],["Griffiths","Ryan-Rhys",""],["Marshall","Iain J.",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 09:32:43 GMT"}],"updateDate":"2024-08-19","timestamp":1723800763000,"abstract":"  Clinical question answering systems have the potential to provide clinicians\nwith relevant and timely answers to their questions. Nonetheless, despite the\nadvances that have been made, adoption of these systems in clinical settings\nhas been slow. One issue is a lack of question-answering datasets which reflect\nthe real-world needs of health professionals. In this work, we present\nRealMedQA, a dataset of realistic clinical questions generated by humans and an\nLLM. We describe the process for generating and verifying the QA pairs and\nassess several QA models on BioASQ and RealMedQA to assess the relative\ndifficulty of matching answers to questions. We show that the LLM is more\ncost-efficient for generating \"ideal\" QA pairs. Additionally, we achieve a\nlower lexical similarity between questions and answers than BioASQ which\nprovides an additional challenge to the top two QA models, as per the results.\nWe release our code and our dataset publicly to encourage further research.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}