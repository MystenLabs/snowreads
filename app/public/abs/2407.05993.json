{"id":"2407.05993","title":"Self-Prior Guided Mamba-UNet Networks for Medical Image Super-Resolution","authors":"Zexin Ji, Beiji Zou, Xiaoyan Kui, Pierre Vera, Su Ruan","authorsParsed":[["Ji","Zexin",""],["Zou","Beiji",""],["Kui","Xiaoyan",""],["Vera","Pierre",""],["Ruan","Su",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 14:41:53 GMT"}],"updateDate":"2024-07-09","timestamp":1720449713000,"abstract":"  In this paper, we propose a self-prior guided Mamba-UNet network\n(SMamba-UNet) for medical image super-resolution. Existing methods are\nprimarily based on convolutional neural networks (CNNs) or Transformers.\nCNNs-based methods fail to capture long-range dependencies, while\nTransformer-based approaches face heavy calculation challenges due to their\nquadratic computational complexity. Recently, State Space Models (SSMs)\nespecially Mamba have emerged, capable of modeling long-range dependencies with\nlinear computational complexity. Inspired by Mamba, our approach aims to learn\nthe self-prior multi-scale contextual features under Mamba-UNet networks, which\nmay help to super-resolve low-resolution medical images in an efficient way.\nSpecifically, we obtain self-priors by perturbing the brightness inpainting of\nthe input image during network training, which can learn detailed texture and\nbrightness information that is beneficial for super-resolution. Furthermore, we\ncombine Mamba with Unet network to mine global features at different levels. We\nalso design an improved 2D-Selective-Scan (ISS2D) module to divide image\nfeatures into different directional sequences to learn long-range dependencies\nin multiple directions, and adaptively fuse sequence information to enhance\nsuper-resolved feature representation. Both qualitative and quantitative\nexperimental results demonstrate that our approach outperforms current\nstate-of-the-art methods on two public medical datasets: the IXI and fastMRI.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}