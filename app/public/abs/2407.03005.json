{"id":"2407.03005","title":"Human-like Linguistic Biases in Neural Speech Models: Phonetic\n  Categorization and Phonotactic Constraints in Wav2Vec2.0","authors":"Marianne de Heer Kloots, Willem Zuidema","authorsParsed":[["Kloots","Marianne de Heer",""],["Zuidema","Willem",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 11:04:31 GMT"}],"updateDate":"2024-07-04","timestamp":1720004671000,"abstract":"  What do deep neural speech models know about phonology? Existing work has\nexamined the encoding of individual linguistic units such as phonemes in these\nmodels. Here we investigate interactions between units. Inspired by classic\nexperiments on human speech perception, we study how Wav2Vec2 resolves\nphonotactic constraints. We synthesize sounds on an acoustic continuum between\n/l/ and /r/ and embed them in controlled contexts where only /l/, only /r/, or\nneither occur in English. Like humans, Wav2Vec2 models show a bias towards the\nphonotactically admissable category in processing such ambiguous sounds. Using\nsimple measures to analyze model internals on the level of individual stimuli,\nwe find that this bias emerges in early layers of the model's Transformer\nmodule. This effect is amplified by ASR finetuning but also present in fully\nself-supervised models. Our approach demonstrates how controlled stimulus\ndesigns can help localize specific linguistic knowledge in neural speech\nmodels.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bFuhZbHv6dVUerb9V45148UjY5uMRynL4xsDXshF_Zg","pdfSize":"4103034","objectId":"0x7f1caa36421a73672ce820d581e8bddce4649cb9ee9bad74c9314b7e2b1dd339","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
