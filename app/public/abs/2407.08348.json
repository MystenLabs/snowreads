{"id":"2407.08348","title":"Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large\n  Language Models -- The Story Goes On","authors":"Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie\n  He, Cheng Cheng, Rui Hu, Yang Liu, Shuicheng Yan, Han Fang, Yahui Zhou","authorsParsed":[["Zeng","Liang",""],["Zhong","Liangjun",""],["Zhao","Liang",""],["Wei","Tianwen",""],["Yang","Liu",""],["He","Jujie",""],["Cheng","Cheng",""],["Hu","Rui",""],["Liu","Yang",""],["Yan","Shuicheng",""],["Fang","Han",""],["Zhou","Yahui",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 09:56:51 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 16:28:33 GMT"}],"updateDate":"2024-07-18","timestamp":1720691811000,"abstract":"  In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}