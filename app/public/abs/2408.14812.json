{"id":"2408.14812","title":"HPT++: Hierarchically Prompting Vision-Language Models with\n  Multi-Granularity Knowledge Generation and Improved Structure Modeling","authors":"Yubin Wang, Xinyang Jiang, De Cheng, Wenli Sun, Dongsheng Li, Cairong\n  Zhao","authorsParsed":[["Wang","Yubin",""],["Jiang","Xinyang",""],["Cheng","De",""],["Sun","Wenli",""],["Li","Dongsheng",""],["Zhao","Cairong",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 06:50:28 GMT"}],"updateDate":"2024-08-28","timestamp":1724741428000,"abstract":"  Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models (VLMs) such as CLIP to downstream tasks. With the emergence\nof large language models (LLMs), recent studies have explored the potential of\nusing category-related descriptions to enhance prompt effectiveness. However,\nconventional descriptions lack explicit structured information necessary to\nrepresent the interconnections among key elements like entities or attributes\nwith relation to a particular category. Since existing prompt tuning methods\ngive little consideration to managing structured knowledge, this paper\nadvocates leveraging LLMs to construct a graph for each description to\nprioritize such structured knowledge. Consequently, we propose a novel approach\ncalled Hierarchical Prompt Tuning (HPT), enabling simultaneous modeling of both\nstructured and conventional linguistic knowledge. Specifically, we introduce a\nrelationship-guided attention module to capture pair-wise associations among\nentities and attributes for low-level prompt learning. In addition, by\nincorporating high-level and global-level prompts modeling overall semantics,\nthe proposed hierarchical structure forges cross-level interlinks and empowers\nthe model to handle more complex and long-term relationships. Finally, by\nenhancing multi-granularity knowledge generation, redesigning the\nrelationship-driven attention re-weighting module, and incorporating consistent\nconstraints on the hierarchical text encoder, we propose HPT++, which further\nimproves the performance of HPT. Our experiments are conducted across a wide\nrange of evaluation settings, including base-to-new generalization,\ncross-dataset evaluation, and domain generalization. Extensive results and\nablation studies demonstrate the effectiveness of our methods, which\nconsistently outperform existing SOTA methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}