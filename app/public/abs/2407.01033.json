{"id":"2407.01033","title":"Neural Networks Trained by Weight Permutation are Universal\n  Approximators","authors":"Yongqiang Cai, Gaohang Chen, Zhonghua Qiao","authorsParsed":[["Cai","Yongqiang",""],["Chen","Gaohang",""],["Qiao","Zhonghua",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 07:33:00 GMT"}],"updateDate":"2024-07-02","timestamp":1719819180000,"abstract":"  The universal approximation property is fundamental to the success of neural\nnetworks, and has traditionally been achieved by training networks without any\nconstraints on their parameters. However, recent experimental research proposed\na novel permutation-based training method, which exhibited a desired\nclassification performance without modifying the exact weight values. In this\npaper, we provide a theoretical guarantee of this permutation training method\nby proving its ability to guide a ReLU network to approximate one-dimensional\ncontinuous functions. Our numerical results further validate this method's\nefficiency in regression tasks with various initializations. The notable\nobservations during weight permutation suggest that permutation training can\nprovide an innovative tool for describing network learning behavior.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}