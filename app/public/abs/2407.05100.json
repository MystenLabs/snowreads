{"id":"2407.05100","title":"Ask Questions with Double Hints: Visual Question Generation with\n  Answer-awareness and Region-reference","authors":"Kai Shen, Lingfei Wu, Siliang Tang, Fangli Xu, Bo Long, Yueting\n  Zhuang, Jian Pei","authorsParsed":[["Shen","Kai",""],["Wu","Lingfei",""],["Tang","Siliang",""],["Xu","Fangli",""],["Long","Bo",""],["Zhuang","Yueting",""],["Pei","Jian",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 15:07:32 GMT"}],"updateDate":"2024-07-09","timestamp":1720278452000,"abstract":"  The visual question generation (VQG) task aims to generate human-like\nquestions from an image and potentially other side information (e.g. answer\ntype). Previous works on VQG fall in two aspects: i) They suffer from one image\nto many questions mapping problem, which leads to the failure of generating\nreferential and meaningful questions from an image. ii) They fail to model\ncomplex implicit relations among the visual objects in an image and also\noverlook potential interactions between the side information and image. To\naddress these limitations, we first propose a novel learning paradigm to\ngenerate visual questions with answer-awareness and region-reference.\nConcretely, we aim to ask the right visual questions with Double Hints -\ntextual answers and visual regions of interests, which could effectively\nmitigate the existing one-to-many mapping issue. Particularly, we develop a\nsimple methodology to self-learn the visual hints without introducing any\nadditional human annotations. Furthermore, to capture these sophisticated\nrelationships, we propose a new double-hints guided Graph-to-Sequence learning\nframework, which first models them as a dynamic graph and learns the implicit\ntopology end-to-end, and then utilizes a graph-to-sequence model to generate\nthe questions with double hints. Experimental results demonstrate the priority\nof our proposed method.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XXXI1s9YhX1JELvSnMBoRVtNU3U5RXK81Vv8bSfqArw","pdfSize":"2336831"}
