{"id":"2408.14007","title":"Using Large Language Models to Document Code: A First Quantitative and\n  Qualitative Assessment","authors":"Ian Guelman, Arthur Greg\\'orio Leal, Laerte Xavier, Marco Tulio\n  Valente","authorsParsed":[["Guelman","Ian",""],["Leal","Arthur Greg√≥rio",""],["Xavier","Laerte",""],["Valente","Marco Tulio",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 04:27:25 GMT"}],"updateDate":"2024-08-27","timestamp":1724646445000,"abstract":"  Code documentation is vital for software development, improving readability\nand comprehension. However, it's often skipped due to its labor-intensive\nnature. AI Language Models present an opportunity to automate the generation of\ncode documentation, easing the burden on developers. While recent studies have\nexplored the use of such models for code documentation, most rely on\nquantitative metrics like BLEU to assess the quality of the generated comments.\nYet, the applicability and accuracy of these metrics on this scenario remain\nuncertain. In this paper, we leveraged OpenAI GPT-3.5 to regenerate the Javadoc\nof 23,850 code snippets with methods and classes. We conducted both\nquantitative and qualitative assessments, employing BLEU alongside human\nevaluation, to assess the quality of the generated comments. Our key findings\nreveal that: (i) in our qualitative analyses, when the documents generated by\nGPT were compared with the original ones, 69.7% were considered equivalent\n(45.7%) or required minor changes to be equivalent (24.0%); (ii) indeed, 22.4%\nof the comments were rated as having superior quality than the original ones;\n(iii) the use of quantitative metrics is susceptible to inconsistencies, for\nexample, comments perceived as having higher quality were unjustly penalized by\nthe BLEU metric.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}