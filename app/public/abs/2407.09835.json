{"id":"2407.09835","title":"Investigating Low-Rank Training in Transformer Language Models:\n  Efficiency and Scaling Analysis","authors":"Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre","authorsParsed":[["Wei","Xiuying",""],["Moalla","Skander",""],["Pascanu","Razvan",""],["Gulcehre","Caglar",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 10:08:55 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 12:43:33 GMT"}],"updateDate":"2024-07-25","timestamp":1720865335000,"abstract":"  State-of-the-art LLMs often rely on scale with high computational costs,\nwhich has sparked a research agenda to reduce parameter counts and costs\nwithout significantly impacting performance. Our study focuses on\nTransformer-based LLMs, specifically applying low-rank parametrization to the\ncomputationally intensive feedforward networks (FFNs), which are less studied\nthan attention blocks. In contrast to previous works, (i) we explore low-rank\nparametrization at scale, up to 1.3B parameters; (ii) within Transformer\nlanguage models rather than convolutional architectures; and (iii) starting\nfrom training from scratch. Experiments on the large RefinedWeb dataset show\nthat low-rank parametrization is both efficient (e.g., 2.6$\\times$ FFN speed-up\nwith 32\\% parameters) and effective during training. Interestingly, these\nstructured FFNs exhibit steeper scaling curves than the original models.\nMotivated by this finding, we develop the wide and structured networks\nsurpassing the current medium-sized and large-sized Transformer in perplexity\nand throughput performance. Our code is available at\nhttps://github.com/CLAIRE-Labo/StructuredFFN/tree/main.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}