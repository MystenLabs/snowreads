{"id":"2407.13193","title":"Retrieval-Augmented Generation for Natural Language Processing: A Survey","authors":"Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan,\n  Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue","authorsParsed":[["Wu","Shangyu",""],["Xiong","Ying",""],["Cui","Yufei",""],["Wu","Haolun",""],["Chen","Can",""],["Yuan","Ye",""],["Huang","Lianming",""],["Liu","Xue",""],["Kuo","Tei-Wei",""],["Guan","Nan",""],["Xue","Chun Jason",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 06:06:53 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 02:00:56 GMT"}],"updateDate":"2024-07-22","timestamp":1721282813000,"abstract":"  Large language models (LLMs) have demonstrated great success in various\nfields, benefiting from their huge amount of parameters that store knowledge.\nHowever, LLMs still suffer from several key issues, such as hallucination\nproblems, knowledge update issues, and lacking domain-specific expertise. The\nappearance of retrieval-augmented generation (RAG), which leverages an external\nknowledge database to augment LLMs, makes up those drawbacks of LLMs. This\npaper reviews all significant techniques of RAG, especially in the retriever\nand the retrieval fusions. Besides, tutorial codes are provided for\nimplementing the representative techniques in RAG. This paper further discusses\nthe RAG training, including RAG with/without datastore update. Then, we\nintroduce the application of RAG in representative natural language processing\ntasks and industrial scenarios. Finally, this paper discusses the future\ndirections and challenges of RAG for promoting its development.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}