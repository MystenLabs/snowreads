{"id":"2407.03297","title":"Improved Noise Schedule for Diffusion Training","authors":"Tiankai Hang, Shuyang Gu","authorsParsed":[["Hang","Tiankai",""],["Gu","Shuyang",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 17:34:55 GMT"}],"updateDate":"2024-07-04","timestamp":1720028095000,"abstract":"  Diffusion models have emerged as the de facto choice for generating visual\nsignals. However, training a single model to predict noise across various\nlevels poses significant challenges, necessitating numerous iterations and\nincurring significant computational costs. Various approaches, such as loss\nweighting strategy design and architectural refinements, have been introduced\nto expedite convergence. In this study, we propose a novel approach to design\nthe noise schedule for enhancing the training of diffusion models. Our key\ninsight is that the importance sampling of the logarithm of the Signal-to-Noise\nratio (logSNR), theoretically equivalent to a modified noise schedule, is\nparticularly beneficial for training efficiency when increasing the sample\nfrequency around $\\log \\text{SNR}=0$. We empirically demonstrate the\nsuperiority of our noise schedule over the standard cosine schedule.\nFurthermore, we highlight the advantages of our noise schedule design on the\nImageNet benchmark, showing that the designed schedule consistently benefits\ndifferent prediction targets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}