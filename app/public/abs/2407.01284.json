{"id":"2407.01284","title":"We-Math: Does Your Large Multimodal Model Achieve Human-like\n  Mathematical Reasoning?","authors":"Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai\n  Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao,\n  Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, Honggang\n  Zhang","authorsParsed":[["Qiao","Runqi",""],["Tan","Qiuna",""],["Dong","Guanting",""],["Wu","Minhui",""],["Sun","Chong",""],["Song","Xiaoshuai",""],["GongQue","Zhuoma",""],["Lei","Shanglin",""],["Wei","Zhe",""],["Zhang","Miaoxuan",""],["Qiao","Runfeng",""],["Zhang","Yifan",""],["Zong","Xiao",""],["Xu","Yida",""],["Diao","Muxi",""],["Bao","Zhimin",""],["Li","Chen",""],["Zhang","Honggang",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 13:39:08 GMT"}],"updateDate":"2024-07-02","timestamp":1719841148000,"abstract":"  Visual mathematical reasoning, as a fundamental visual reasoning ability, has\nreceived widespread attention from the Large Multimodal Models (LMMs)\ncommunity. Existing benchmarks, such as MathVista and MathVerse, focus more on\nthe result-oriented performance but neglect the underlying principles in\nknowledge acquisition and generalization. Inspired by human-like mathematical\nreasoning, we introduce WE-MATH, the first benchmark specifically designed to\nexplore the problem-solving principles beyond end-to-end performance. We\nmeticulously collect and categorize 6.5K visual math problems, spanning 67\nhierarchical knowledge concepts and five layers of knowledge granularity. We\ndecompose composite problems into sub-problems according to the required\nknowledge concepts and introduce a novel four-dimensional metric, namely\nInsufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery\n(CM), and Rote Memorization (RM), to hierarchically assess inherent issues in\nLMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of\nexisting LMMs in visual mathematical reasoning and reveal a negative\ncorrelation between solving steps and problem-specific performance. We confirm\nthe IK issue of LMMs can be effectively improved via knowledge augmentation\nstrategies. More notably, the primary challenge of GPT-4o has significantly\ntransitioned from IK to IG, establishing it as the first LMM advancing towards\nthe knowledge generalization stage. In contrast, other LMMs exhibit a marked\ninclination towards Rote Memorization - they correctly solve composite problems\ninvolving multiple knowledge concepts yet fail to answer sub-problems. We\nanticipate that WE-MATH will open new pathways for advancements in visual\nmathematical reasoning for LMMs. The WE-MATH data and evaluation code are\navailable at https://github.com/We-Math/We-Math.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Symbolic Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}