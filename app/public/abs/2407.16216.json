{"id":"2407.16216","title":"A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO,\n  DPO and More","authors":"Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata\n  Chaudhuri, Shubham Mehrotra, Zixu (James) Zhu, Xiang-Bo Mao, Sitaram Asur, Na\n  (Claire) Cheng","authorsParsed":[["Wang","Zhichao","","James"],["Bi","Bin","","James"],["Pentyala","Shiva Kumar","","James"],["Ramnath","Kiran","","James"],["Chaudhuri","Sougata","","James"],["Mehrotra","Shubham","","James"],["Zixu","","","James"],["Zhu","","","Claire"],["Mao","Xiang-Bo","","Claire"],["Asur","Sitaram","","Claire"],["Na","","","Claire"],["Cheng","",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 06:45:52 GMT"}],"updateDate":"2024-07-24","timestamp":1721717152000,"abstract":"  With advancements in self-supervised learning, the availability of trillions\ntokens in a pre-training corpus, instruction fine-tuning, and the development\nof large Transformers with billions of parameters, large language models (LLMs)\nare now capable of generating factual and coherent responses to human queries.\nHowever, the mixed quality of training data can lead to the generation of\nundesired responses, presenting a significant challenge. Over the past two\nyears, various methods have been proposed from different perspectives to\nenhance LLMs, particularly in aligning them with human expectation. Despite\nthese efforts, there has not been a comprehensive survey paper that categorizes\nand details these approaches. In this work, we aim to address this gap by\ncategorizing these papers into distinct topics and providing detailed\nexplanations of each alignment method, thereby helping readers gain a thorough\nunderstanding of the current state of the field.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}