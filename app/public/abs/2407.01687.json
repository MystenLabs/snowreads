{"id":"2407.01687","title":"Deciphering the Factors Influencing the Efficacy of Chain-of-Thought:\n  Probability, Memorization, and Noisy Reasoning","authors":"Akshara Prabhakar, Thomas L. Griffiths, R. Thomas McCoy","authorsParsed":[["Prabhakar","Akshara",""],["Griffiths","Thomas L.",""],["McCoy","R. Thomas",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 18:01:07 GMT"}],"updateDate":"2024-07-03","timestamp":1719856867000,"abstract":"  Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step\nreasoning capabilities of Large Language Models (LLMs). However, debates\npersist about whether LLMs exhibit abstract generalization or rely on shallow\nheuristics when given CoT prompts. To understand the factors influencing CoT\nreasoning we provide a detailed case study of the symbolic reasoning task of\ndecoding shift ciphers, where letters are shifted forward some number of steps\nin the alphabet. GPT-4 achieves zero accuracy on most shift ciphers with\nstandard prompting, but with CoT its accuracy improves to an average of 32%. By\nfocusing on a single relatively simple task, we are able to identify three\nfactors that systematically affect CoT performance: the probability of the\ntask's expected output (probability), what the model has implicitly learned\nduring pre-training (memorization), and the number of intermediate operations\ninvolved in reasoning (noisy reasoning). We show that these factors can\ndrastically influence the task accuracy; e.g., varying the output's probability\nof occurrence can shift accuracy from 26% to 70%. We also demonstrate that it\nis essential for the model to explicitly produce intermediate steps as output\nthat can be conditioned on to increase the probability of the correct answer.\nOur experiments indicate that as long as the model does so, the validity of the\ndemonstrations in the prompt does not matter. Overall, we conclude that CoT\nprompting performance reflects both memorization and a probabilistic version of\ngenuine reasoning.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}