{"id":"2408.15971","title":"BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems","authors":"Wei Wang and Dan Zhang and Tao Feng and Boyan Wang and Jie Tang","authorsParsed":[["Wang","Wei",""],["Zhang","Dan",""],["Feng","Tao",""],["Wang","Boyan",""],["Tang","Jie",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 17:43:55 GMT"}],"updateDate":"2024-08-29","timestamp":1724867035000,"abstract":"  Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}