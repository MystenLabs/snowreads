{"id":"2408.05097","title":"Hyperbolic Learning with Multimodal Large Language Models","authors":"Paolo Mandica, Luca Franco, Konstantinos Kallidromitis, Suzanne\n  Petryk, Fabio Galasso","authorsParsed":[["Mandica","Paolo",""],["Franco","Luca",""],["Kallidromitis","Konstantinos",""],["Petryk","Suzanne",""],["Galasso","Fabio",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 14:39:15 GMT"}],"updateDate":"2024-08-12","timestamp":1723214355000,"abstract":"  Hyperbolic embeddings have demonstrated their effectiveness in capturing\nmeasures of uncertainty and hierarchical relationships across various\ndeep-learning tasks, including image segmentation and active learning. However,\ntheir application in modern vision-language models (VLMs) has been limited. A\nnotable exception is MERU, which leverages the hierarchical properties of\nhyperbolic space in the CLIP ViT-large model, consisting of hundreds of\nmillions parameters. In our work, we address the challenges of scaling\nmulti-modal hyperbolic models by orders of magnitude in terms of parameters\n(billions) and training complexity using the BLIP-2 architecture. Although\nhyperbolic embeddings offer potential insights into uncertainty not present in\nEuclidean embeddings, our analysis reveals that scaling these models is\nparticularly difficult. We propose a novel training strategy for a hyperbolic\nversion of BLIP-2, which allows to achieve comparable performance to its\nEuclidean counterpart, while maintaining stability throughout the training\nprocess and showing a meaningful indication of uncertainty with each embedding.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}