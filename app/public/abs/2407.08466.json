{"id":"2407.08466","title":"Global Spatial-Temporal Information-based Residual ConvLSTM for Video\n  Space-Time Super-Resolution","authors":"Congrui Fu, Hui Yuan, Shiqi Jiang, Guanghui Zhang, Liquan Shen, and\n  Raouf Hamzaoui","authorsParsed":[["Fu","Congrui",""],["Yuan","Hui",""],["Jiang","Shiqi",""],["Zhang","Guanghui",""],["Shen","Liquan",""],["Hamzaoui","Raouf",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:01:44 GMT"}],"updateDate":"2024-07-12","timestamp":1720702904000,"abstract":"  By converting low-frame-rate, low-resolution videos into high-frame-rate,\nhigh-resolution ones, space-time video super-resolution techniques can enhance\nvisual experiences and facilitate more efficient information dissemination. We\npropose a convolutional neural network (CNN) for space-time video\nsuper-resolution, namely GIRNet. To generate highly accurate features and thus\nimprove performance, the proposed network integrates a feature-level temporal\ninterpolation module with deformable convolutions and a global spatial-temporal\ninformation-based residual convolutional long short-term memory (convLSTM)\nmodule. In the feature-level temporal interpolation module, we leverage\ndeformable convolution, which adapts to deformations and scale variations of\nobjects across different scene locations. This presents a more efficient\nsolution than conventional convolution for extracting features from moving\nobjects. Our network effectively uses forward and backward feature information\nto determine inter-frame offsets, leading to the direct generation of\ninterpolated frame features. In the global spatial-temporal information-based\nresidual convLSTM module, the first convLSTM is used to derive global\nspatial-temporal information from the input features, and the second convLSTM\nuses the previously computed global spatial-temporal information feature as its\ninitial cell state. This second convLSTM adopts residual connections to\npreserve spatial information, thereby enhancing the output features.\nExperiments on the Vimeo90K dataset show that the proposed method outperforms\nstate-of-the-art techniques in peak signal-to-noise-ratio (by 1.45 dB, 1.14 dB,\nand 0.02 dB over STARnet, TMNet, and 3DAttGAN, respectively), structural\nsimilarity index(by 0.027, 0.023, and 0.006 over STARnet, TMNet, and 3DAttGAN,\nrespectively), and visually.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}