{"id":"2408.12798","title":"BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large\n  Language Models","authors":"Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun","authorsParsed":[["Li","Yige",""],["Huang","Hanxun",""],["Zhao","Yunhan",""],["Ma","Xingjun",""],["Sun","Jun",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 02:21:21 GMT"}],"updateDate":"2024-08-26","timestamp":1724379681000,"abstract":"  Generative Large Language Models (LLMs) have made significant strides across\nvarious tasks, but they remain vulnerable to backdoor attacks, where specific\ntriggers in the prompt cause the LLM to generate adversary-desired responses.\nWhile most backdoor research has focused on vision or text classification\ntasks, backdoor attacks in text generation have been largely overlooked. In\nthis work, we introduce \\textit{BackdoorLLM}, the first comprehensive benchmark\nfor studying backdoor attacks on LLMs. \\textit{BackdoorLLM} features: 1) a\nrepository of backdoor benchmarks with a standardized training pipeline, 2)\ndiverse attack strategies, including data poisoning, weight poisoning, hidden\nstate attacks, and chain-of-thought attacks, 3) extensive evaluations with over\n200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and\n4) key insights into the effectiveness and limitations of backdoors in LLMs. We\nhope \\textit{BackdoorLLM} will raise awareness of backdoor threats and\ncontribute to advancing AI safety. The code is available at\n\\url{https://github.com/bboylyg/BackdoorLLM}.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}