{"id":"2407.14505","title":"T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video\n  Generation","authors":"Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, Xihui\n  Liu","authorsParsed":[["Sun","Kaiyue",""],["Huang","Kaiyi",""],["Liu","Xian",""],["Wu","Yue",""],["Xu","Zihan",""],["Li","Zhenguo",""],["Liu","Xihui",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:58:36 GMT"}],"updateDate":"2024-07-22","timestamp":1721411916000,"abstract":"  Text-to-video (T2V) generation models have advanced significantly, yet their\nability to compose different objects, attributes, actions, and motions into a\nvideo remains unexplored. Previous text-to-video benchmarks also neglect this\nimportant ability for evaluation. In this work, we conduct the first systematic\nstudy on compositional text-to-video generation. We propose T2V-CompBench, the\nfirst benchmark tailored for compositional text-to-video generation.\nT2V-CompBench encompasses diverse aspects of compositionality, including\nconsistent attribute binding, dynamic attribute binding, spatial relationships,\nmotion binding, action binding, object interactions, and generative numeracy.\nWe further carefully design evaluation metrics of MLLM-based metrics,\ndetection-based metrics, and tracking-based metrics, which can better reflect\nthe compositional text-to-video generation quality of seven proposed categories\nwith 700 text prompts. The effectiveness of the proposed metrics is verified by\ncorrelation with human evaluations. We also benchmark various text-to-video\ngenerative models and conduct in-depth analysis across different models and\ndifferent compositional categories. We find that compositional text-to-video\ngeneration is highly challenging for current models, and we hope that our\nattempt will shed light on future research in this direction.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}