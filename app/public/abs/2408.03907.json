{"id":"2408.03907","title":"Decoding Biases: Automated Methods and LLM Judges for Gender Bias\n  Detection in Language Models","authors":"Shachi H Kumar, Saurav Sahay, Sahisnu Mazumder, Eda Okur, Ramesh\n  Manuvinakurike, Nicole Beckage, Hsuan Su, Hung-yi Lee, Lama Nachman","authorsParsed":[["Kumar","Shachi H",""],["Sahay","Saurav",""],["Mazumder","Sahisnu",""],["Okur","Eda",""],["Manuvinakurike","Ramesh",""],["Beckage","Nicole",""],["Su","Hsuan",""],["Lee","Hung-yi",""],["Nachman","Lama",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 17:11:34 GMT"}],"updateDate":"2024-08-08","timestamp":1723050694000,"abstract":"  Large Language Models (LLMs) have excelled at language understanding and\ngenerating human-level text. However, even with supervised training and human\nalignment, these LLMs are susceptible to adversarial attacks where malicious\nusers can prompt the model to generate undesirable text. LLMs also inherently\nencode potential biases that can cause various harmful effects during\ninteractions. Bias evaluation metrics lack standards as well as consensus and\nexisting methods often rely on human-generated templates and annotations which\nare expensive and labor intensive. In this work, we train models to\nautomatically create adversarial prompts to elicit biased responses from target\nLLMs. We present LLM- based bias evaluation metrics and also analyze several\nexisting automatic evaluation methods and metrics. We analyze the various\nnuances of model responses, identify the strengths and weaknesses of model\nfamilies, and assess where evaluation methods fall short. We compare these\nmetrics to human evaluation and validate that the LLM-as-a-Judge metric aligns\nwith human judgement on bias in response generation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}