{"id":"2407.07670","title":"Stochastic Gradient Descent for Two-layer Neural Networks","authors":"Dinghao Cao, Zheng-Chu Guo and Lei Shi","authorsParsed":[["Cao","Dinghao",""],["Guo","Zheng-Chu",""],["Shi","Lei",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:58:57 GMT"}],"updateDate":"2024-07-11","timestamp":1720619937000,"abstract":"  This paper presents a comprehensive study on the convergence rates of the\nstochastic gradient descent (SGD) algorithm when applied to overparameterized\ntwo-layer neural networks. Our approach combines the Neural Tangent Kernel\n(NTK) approximation with convergence analysis in the Reproducing Kernel Hilbert\nSpace (RKHS) generated by NTK, aiming to provide a deep understanding of the\nconvergence behavior of SGD in overparameterized two-layer neural networks. Our\nresearch framework enables us to explore the intricate interplay between kernel\nmethods and optimization processes, shedding light on the optimization dynamics\nand convergence properties of neural networks. In this study, we establish\nsharp convergence rates for the last iterate of the SGD algorithm in\noverparameterized two-layer neural networks. Additionally, we have made\nsignificant advancements in relaxing the constraints on the number of neurons,\nwhich have been reduced from exponential dependence to polynomial dependence on\nthe sample size or number of iterations. This improvement allows for more\nflexibility in the design and scaling of neural networks, and will deepen our\ntheoretical understanding of neural network models trained with SGD.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}