{"id":"2408.16730","title":"VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation","authors":"Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli\n  Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou","authorsParsed":[["Wu","Shiwei",""],["Chen","Joya",""],["Lin","Kevin Qinghong",""],["Wang","Qimeng",""],["Gao","Yan",""],["Xu","Qianli",""],["Xu","Tong",""],["Hu","Yao",""],["Chen","Enhong",""],["Shou","Mike Zheng",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 17:21:58 GMT"}],"updateDate":"2024-08-30","timestamp":1724952118000,"abstract":"  A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KK-6GfDFxNdtlTiPnghJ8eTyIJtfmNYocGFQ2hUG0HA","pdfSize":"1791289"}
