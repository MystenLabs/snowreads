{"id":"2407.03236","title":"CATT: Character-based Arabic Tashkeel Transformer","authors":"Faris Alasmary, Orjuwan Zaafarani, Ahmad Ghannam","authorsParsed":[["Alasmary","Faris",""],["Zaafarani","Orjuwan",""],["Ghannam","Ahmad",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 16:05:20 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 17:06:33 GMT"},{"version":"v3","created":"Sun, 14 Jul 2024 10:01:40 GMT"}],"updateDate":"2024-07-16","timestamp":1720022720000,"abstract":"  Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the\ncomprehension of Arabic text by removing ambiguity and minimizing the risk of\nmisinterpretations caused by its absence. It plays a crucial role in improving\nArabic text processing, particularly in applications such as text-to-speech and\nmachine translation. This paper introduces a new approach to training ATD\nmodels. First, we finetuned two transformers, encoder-only and encoder-decoder,\nthat were initialized from a pretrained character-based BERT. Then, we applied\nthe Noisy-Student approach to boost the performance of the best model. We\nevaluated our models alongside 11 commercial and open-source models using two\nmanually labeled benchmark datasets: WikiNews and our CATT dataset. Our\nfindings show that our top model surpasses all evaluated models by relative\nDiacritic Error Rates (DERs) of 30.83\\% and 35.21\\% on WikiNews and CATT,\nrespectively, achieving state-of-the-art in ATD. In addition, we show that our\nmodel outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36\\%. We\nopen-source our CATT models and benchmark dataset for the research\ncommunity\\footnote{https://github.com/abjadai/catt}.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"gg2UnokaoWPQu6AGRdEwOyD-EuBSaDrgGq_e1Zxc-m4","pdfSize":"400230"}
