{"id":"2408.10645","title":"CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation","authors":"Yuting Liu, Jinghao Zhang, Yizhou Dang, Yuliang Liang, Qiang Liu,\n  Guibing Guo, Jianzhe Zhao, Xingwei Wang","authorsParsed":[["Liu","Yuting",""],["Zhang","Jinghao",""],["Dang","Yizhou",""],["Liang","Yuliang",""],["Liu","Qiang",""],["Guo","Guibing",""],["Zhao","Jianzhe",""],["Wang","Xingwei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 08:36:59 GMT"}],"updateDate":"2024-08-21","timestamp":1724143019000,"abstract":"  Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}