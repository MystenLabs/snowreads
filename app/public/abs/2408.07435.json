{"id":"2408.07435","title":"Real-world validation of safe reinforcement learning, model predictive\n  control and decision tree-based home energy management systems","authors":"Julian Ruddick, Glenn Ceusters, Gilles Van Kriekinge, Evgenii Genov,\n  Thierry Coosemans, Maarten Messagie","authorsParsed":[["Ruddick","Julian",""],["Ceusters","Glenn",""],["Van Kriekinge","Gilles",""],["Genov","Evgenii",""],["Coosemans","Thierry",""],["Messagie","Maarten",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 10:12:15 GMT"}],"updateDate":"2024-08-15","timestamp":1723630335000,"abstract":"  Recent advancements in machine learning based energy management approaches,\nspecifically reinforcement learning with a safety layer (OptLayerPolicy) and a\nmetaheuristic algorithm generating a decision tree control policy (TreeC), have\nshown promise. However, their effectiveness has only been demonstrated in\ncomputer simulations. This paper presents the real-world validation of these\nmethods, comparing against model predictive control and simple rule-based\ncontrol benchmark. The experiments were conducted on the electrical\ninstallation of 4 reproductions of residential houses, which all have their own\nbattery, photovoltaic and dynamic load system emulating a non-controllable\nelectrical load and a controllable electric vehicle charger. The results show\nthat the simple rules, TreeC, and model predictive control-based methods\nachieved similar costs, with a difference of only 0.6%. The reinforcement\nlearning based method, still in its training phase, obtained a cost 25.5\\%\nhigher to the other methods. Additional simulations show that the costs can be\nfurther reduced by using a more representative training dataset for TreeC and\naddressing errors in the model predictive control implementation caused by its\nreliance on accurate data from various sources. The OptLayerPolicy safety layer\nallows safe online training of a reinforcement learning agent in the\nreal-world, given an accurate constraint function formulation. The proposed\nsafety layer method remains error-prone, nonetheless, it is found beneficial\nfor all investigated methods. The TreeC method, which does require building a\nrealistic simulation for training, exhibits the safest operational performance,\nexceeding the grid limit by only 27.1 Wh compared to 593.9 Wh for reinforcement\nlearning.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZTMe5jMFfCKTnWcKHhOA3EgrBaixj_B_CLBmLc6ulps","pdfSize":"5606290"}
