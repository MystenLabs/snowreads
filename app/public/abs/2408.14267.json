{"id":"2408.14267","title":"1-Bit FQT: Pushing the Limit of Fully Quantized Training to 1-bit","authors":"Chang Gao, Jianfei Chen, Kang Zhao, Jiaqi Wang and Liping Jing","authorsParsed":[["Gao","Chang",""],["Chen","Jianfei",""],["Zhao","Kang",""],["Wang","Jiaqi",""],["Jing","Liping",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 13:42:43 GMT"}],"updateDate":"2024-08-27","timestamp":1724679763000,"abstract":"  Fully quantized training (FQT) accelerates the training of deep neural\nnetworks by quantizing the activations, weights, and gradients into lower\nprecision. To explore the ultimate limit of FQT (the lowest achievable\nprecision), we make a first attempt to 1-bit FQT. We provide a theoretical\nanalysis of FQT based on Adam and SGD, revealing that the gradient variance\ninfluences the convergence of FQT. Building on these theoretical results, we\nintroduce an Activation Gradient Pruning (AGP) strategy. The strategy leverages\nthe heterogeneity of gradients by pruning less informative gradients and\nenhancing the numerical precision of remaining gradients to mitigate gradient\nvariance. Additionally, we propose Sample Channel joint Quantization (SCQ),\nwhich utilizes different quantization strategies in the computation of weight\ngradients and activation gradients to ensure that the method is friendly to\nlow-bitwidth hardware. Finally, we present a framework to deploy our algorithm.\nFor fine-tuning VGGNet-16 and ResNet-18 on multiple datasets, our algorithm\nachieves an average accuracy improvement of approximately 6%, compared to\nper-sample quantization. Moreover, our training speedup can reach a maximum of\n5.13x compared to full precision training.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}