{"id":"2407.00075","title":"Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference","authors":"Anton Xue, Avishree Khare, Rajeev Alur, Surbhi Goel, Eric Wong","authorsParsed":[["Xue","Anton",""],["Khare","Avishree",""],["Alur","Rajeev",""],["Goel","Surbhi",""],["Wong","Eric",""]],"versions":[{"version":"v1","created":"Fri, 21 Jun 2024 19:18:16 GMT"}],"updateDate":"2024-07-02","timestamp":1718997496000,"abstract":"  We study how to subvert language models from following the rules. We model\nrule-following as inference in propositional Horn logic, a mathematical system\nin which rules have the form \"if $P$ and $Q$, then $R$\" for some propositions\n$P$, $Q$, and $R$. We prove that although transformers can faithfully abide by\nsuch rules, maliciously crafted prompts can nevertheless mislead even\ntheoretically constructed models. Empirically, we find that attacks on our\ntheoretical models mirror popular attacks on large language models. Our work\nsuggests that studying smaller theoretical models can help understand the\nbehavior of large language models in rule-based settings like logical reasoning\nand jailbreak attacks.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}