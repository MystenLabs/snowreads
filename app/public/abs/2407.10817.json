{"id":"2407.10817","title":"Foundational Autoraters: Taming Large Language Models for Better\n  Automatic Evaluation","authors":"Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui,\n  Yun-Hsuan Sung","authorsParsed":[["Vu","Tu",""],["Krishna","Kalpesh",""],["Alzubi","Salaheddin",""],["Tar","Chris",""],["Faruqui","Manaal",""],["Sung","Yun-Hsuan",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:33:45 GMT"}],"updateDate":"2024-07-16","timestamp":1721057625000,"abstract":"  As large language models (LLMs) advance, it becomes more challenging to\nreliably evaluate their output due to the high costs of human evaluation. To\nmake progress towards better LLM autoraters, we introduce FLAMe, a family of\nFoundational Large Autorater Models. FLAMe is trained on our large and diverse\ncollection of 100+ quality assessment tasks comprising 5M+ human judgments,\ncurated and standardized using publicly released human evaluations from\nprevious research. FLAMe significantly improves generalization to a wide\nvariety of held-out tasks, outperforming LLMs trained on proprietary data like\nGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a\npowerful starting point for further downstream fine-tuning, using reward\nmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our\nFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative\nmodel trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more\ncomputationally efficient approach using a novel tail-patch fine-tuning\nstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation\n(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring\napproximately 25x less training datapoints. Overall, our FLAMe variants\noutperform all popular proprietary LLM-as-a-Judge models we consider across 8\nout of 12 autorater evaluation benchmarks, encompassing 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals\nthat FLAMe is significantly less biased than these LLM-as-a-Judge models on the\nCoBBLEr autorater bias benchmark, while effectively identifying high-quality\nresponses for code generation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}