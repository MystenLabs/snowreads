{"id":"2408.09111","title":"Measuring Visual Sycophancy in Multimodal Models","authors":"Jaehyuk Lim, Bruce W. Lee","authorsParsed":[["Lim","Jaehyuk",""],["Lee","Bruce W.",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 06:25:36 GMT"}],"updateDate":"2024-08-20","timestamp":1723875936000,"abstract":"  This paper introduces and examines the phenomenon of \"visual sycophancy\" in\nmultimodal language models, a term we propose to describe these models'\ntendency to disproportionately favor visually presented information, even when\nit contradicts their prior knowledge or responses. Our study employs a\nsystematic methodology to investigate this phenomenon: we present models with\nimages of multiple-choice questions, which they initially answer correctly,\nthen expose the same model to versions with visually pre-marked options. Our\nfindings reveal a significant shift in the models' responses towards the\npre-marked option despite their previous correct answers. Comprehensive\nevaluations demonstrate that visual sycophancy is a consistent and quantifiable\nbehavior across various model architectures. Our findings highlight potential\nlimitations in the reliability of these models when processing potentially\nmisleading visual information, raising important questions about their\napplication in critical decision-making contexts.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"C92LEadMOn0b0RKDpyBD9zEMGPuqeBs5EWjd2WgdL9I","pdfSize":"399704"}
