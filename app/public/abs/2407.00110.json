{"id":"2407.00110","title":"Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services","authors":"Ali Doosthosseini, Jonathan Decker, Hendrik Nolte, Julian M. Kunkel","authorsParsed":[["Doosthosseini","Ali",""],["Decker","Jonathan",""],["Nolte","Hendrik",""],["Kunkel","Julian M.",""]],"versions":[{"version":"v1","created":"Thu, 27 Jun 2024 12:08:21 GMT"},{"version":"v2","created":"Fri, 2 Aug 2024 15:34:22 GMT"}],"updateDate":"2024-08-05","timestamp":1719490101000,"abstract":"  The widespread adoption of large language models (LLMs) has created a\npressing need for an efficient, secure and private serving infrastructure,\nwhich allows researchers to run open source or custom fine-tuned LLMs and\nensures users that their data remains private and is not stored without their\nconsent. While high-performance computing (HPC) systems equipped with\nstate-of-the-art GPUs are well-suited for training LLMs, their batch scheduling\nparadigm is not designed to support real-time serving of AI applications. Cloud\nsystems, on the other hand, are well suited for web services but commonly lack\naccess to the computational power of HPC clusters, especially expensive and\nscarce high-end GPUs, which are required for optimal inference speed. We\npropose an architecture with an implementation consisting of a web service that\nruns on a cloud VM with secure access to a scalable backend running a multitude\nof LLM models on HPC systems. By offering a web service using our HPC\ninfrastructure to host LLMs, we leverage the trusted environment of local\nuniversities and research centers to offer a private and secure alternative to\ncommercial LLM services. Our solution natively integrates with the HPC batch\nscheduler Slurm, enabling seamless deployment on HPC clusters, and is able to\nrun side by side with regular Slurm workloads, while utilizing gaps in the\nschedule created by Slurm. In order to ensure the security of the HPC system,\nwe use the SSH ForceCommand directive to construct a robust circuit breaker,\nwhich prevents successful attacks on the web-facing server from affecting the\ncluster. We have successfully deployed our system as a production service, and\nmade the source code available at \\url{https://github.com/gwdg/chat-ai}\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KZNUd_MCcsd_EZG3BhQR6q9UWWnMBaFi8oMsXH2Lxqw","pdfSize":"1478417","objectId":"0x7a6330abab8df9f98c629a423b1b661e20328ffc1f4a1c8803d619cc83fbc36a","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
