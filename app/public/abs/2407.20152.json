{"id":"2407.20152","title":"Hierarchically Disentangled Recurrent Network for Factorizing System\n  Dynamics of Multi-scale Systems","authors":"Rahul Ghosh, Zac McEachran, Arvind Renganathan, Kelly Lindsay, Somya\n  Sharma, Michael Steinbach, John Nieber, Christopher Duffy, Vipin Kumar","authorsParsed":[["Ghosh","Rahul",""],["McEachran","Zac",""],["Renganathan","Arvind",""],["Lindsay","Kelly",""],["Sharma","Somya",""],["Steinbach","Michael",""],["Nieber","John",""],["Duffy","Christopher",""],["Kumar","Vipin",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 16:25:43 GMT"}],"updateDate":"2024-07-30","timestamp":1722270343000,"abstract":"  We present a knowledge-guided machine learning (KGML) framework for modeling\nmulti-scale processes, and study its performance in the context of streamflow\nforecasting in hydrology. Specifically, we propose a novel hierarchical\nrecurrent neural architecture that factorizes the system dynamics at multiple\ntemporal scales and captures their interactions. This framework consists of an\ninverse and a forward model. The inverse model is used to empirically resolve\nthe system's temporal modes from data (physical model simulations, observed\ndata, or a combination of them from the past), and these states are then used\nin the forward model to predict streamflow. In a hydrological system, these\nmodes can represent different processes, evolving at different temporal scales\n(e.g., slow: groundwater recharge and baseflow vs. fast: surface runoff due to\nextreme rainfall). A key advantage of our framework is that once trained, it\ncan incorporate new observations into the model's context (internal state)\nwithout expensive optimization approaches (e.g., EnKF) that are traditionally\nused in physical sciences for data assimilation. Experiments with several river\ncatchments from the NWS NCRFC region show the efficacy of this ML-based data\nassimilation framework compared to standard baselines, especially for basins\nthat have a long history of observations. Even for basins that have a shorter\nobservation history, we present two orthogonal strategies of training our FHNN\nframework: (a) using simulation data from imperfect simulations and (b) using\nobservation data from multiple basins to build a global model. We show that\nboth of these strategies (that can be used individually or together) are highly\neffective in mitigating the lack of training data. The improvement in forecast\naccuracy is particularly noteworthy for basins where local models perform\npoorly because of data sparsity.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6VsCt6it4OlBF9zWxz-LRz_umpamLa6tMebmzv4Rsog","pdfSize":"749792"}
