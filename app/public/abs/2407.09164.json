{"id":"2407.09164","title":"TAPI: Towards Target-Specific and Adversarial Prompt Injection against\n  Code LLMs","authors":"Yuchen Yang, Hongwei Yao, Bingrun Yang, Yiling He, Yiming Li, Tianwei\n  Zhang, Zhan Qin, Kui Ren","authorsParsed":[["Yang","Yuchen",""],["Yao","Hongwei",""],["Yang","Bingrun",""],["He","Yiling",""],["Li","Yiming",""],["Zhang","Tianwei",""],["Qin","Zhan",""],["Ren","Kui",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 10:59:32 GMT"},{"version":"v2","created":"Mon, 15 Jul 2024 07:54:50 GMT"},{"version":"v3","created":"Mon, 22 Jul 2024 08:19:23 GMT"}],"updateDate":"2024-07-23","timestamp":1720781972000,"abstract":"  Recently, code-oriented large language models (Code LLMs) have been widely\nand successfully used to simplify and facilitate code programming. With these\ntools, developers can easily generate desired complete functional codes based\non incomplete code and natural language prompts. However, a few pioneering\nworks revealed that these Code LLMs are also vulnerable, e.g., against backdoor\nand adversarial attacks. The former could induce LLMs to respond to triggers to\ninsert malicious code snippets by poisoning the training data or model\nparameters, while the latter can craft malicious adversarial input codes to\nreduce the quality of generated codes. However, both attack methods have\nunderlying limitations: backdoor attacks rely on controlling the model training\nprocess, while adversarial attacks struggle with fulfilling specific malicious\npurposes.\n  To inherit the advantages of both backdoor and adversarial attacks, this\npaper proposes a new attack paradigm, i.e., target-specific and adversarial\nprompt injection (TAPI), against Code LLMs. TAPI generates unreadable comments\ncontaining information about malicious instructions and hides them as triggers\nin the external source code. When users exploit Code LLMs to complete codes\ncontaining the trigger, the models will generate attacker-specified malicious\ncode snippets at specific locations. We evaluate our TAPI attack on four\nrepresentative LLMs under three representative malicious objectives and seven\ncases. The results show that our method is highly threatening (achieving an\nattack success rate of up to 98.3%) and stealthy (saving an average of 53.1% of\ntokens in the trigger design). In particular, we successfully attack some\nfamous deployed code completion integrated applications, including CodeGeex and\nGithub Copilot. This further confirms the realistic threat of our attack.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}