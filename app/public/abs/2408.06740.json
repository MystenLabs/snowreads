{"id":"2408.06740","title":"DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with\n  Diffusion","authors":"Yujia Wu, Yiming Shi, Jiwei Wei, Chengwei Sun, Yuyang Zhou, Yang Yang,\n  Heng Tao Shen","authorsParsed":[["Wu","Yujia",""],["Shi","Yiming",""],["Wei","Jiwei",""],["Sun","Chengwei",""],["Zhou","Yuyang",""],["Yang","Yang",""],["Shen","Heng Tao",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 09:00:35 GMT"},{"version":"v2","created":"Sun, 18 Aug 2024 05:43:05 GMT"}],"updateDate":"2024-08-20","timestamp":1723539635000,"abstract":"  Personalized text-to-image generation has gained significant attention for\nits capability to generate high-fidelity portraits of specific identities\nconditioned on user-defined prompts. Existing methods typically involve\ntest-time fine-tuning or instead incorporating an additional pre-trained\nbranch. However, these approaches struggle to simultaneously address the\ndemands of efficiency, identity fidelity, and preserving the model's original\ngenerative capabilities. In this paper, we propose DiffLoRA, a novel approach\nthat leverages diffusion models as a hypernetwork to predict personalized\nlow-rank adaptation (LoRA) weights based on the reference images. By\nintegrating these LoRA weights into the text-to-image model, DiffLoRA achieves\npersonalization during inference without further training. Additionally, we\npropose an identity-oriented LoRA weight construction pipeline to facilitate\nthe training of DiffLoRA. By utilizing the dataset produced by this pipeline,\nour DiffLoRA consistently generates high-performance and accurate LoRA weights.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth time efficiency and maintaining identity fidelity throughout the\npersonalization process.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}