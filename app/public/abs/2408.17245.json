{"id":"2408.17245","title":"Stepwise Weighted Spike Coding for Deep Spiking Neural Networks","authors":"Yiwen Gu, Junchuan Gu, Haibin Shen, Kejie Huang","authorsParsed":[["Gu","Yiwen",""],["Gu","Junchuan",""],["Shen","Haibin",""],["Huang","Kejie",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 12:39:25 GMT"}],"updateDate":"2024-09-02","timestamp":1725021565000,"abstract":"  Spiking Neural Networks (SNNs) seek to mimic the spiking behavior of\nbiological neurons and are expected to play a key role in the advancement of\nneural computing and artificial intelligence. The efficiency of SNNs is often\ndetermined by the neural coding schemes. Existing coding schemes either cause\nhuge delays and energy consumption or necessitate intricate neuron models and\ntraining techniques. To address these issues, we propose a novel Stepwise\nWeighted Spike (SWS) coding scheme to enhance the encoding of information in\nspikes. This approach compresses the spikes by weighting the significance of\nthe spike in each step of neural computation, achieving high performance and\nlow energy consumption. A Ternary Self-Amplifying (TSA) neuron model with a\nsilent period is proposed for supporting SWS-based computing, aimed at\nminimizing the residual error resulting from stepwise weighting in neural\ncomputation. Our experimental results show that the SWS coding scheme\noutperforms the existing neural coding schemes in very deep SNNs, and\nsignificantly reduces operations and latency.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}