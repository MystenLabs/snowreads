{"id":"2407.02098","title":"DM3D: Distortion-Minimized Weight Pruning for Lossless 3D Object\n  Detection","authors":"Kaixin Xu, Qingtian Feng, Hao Chen, Zhe Wang, Xue Geng, Xulei Yang,\n  Min Wu, Xiaoli Li, Weisi Lin","authorsParsed":[["Xu","Kaixin",""],["Feng","Qingtian",""],["Chen","Hao",""],["Wang","Zhe",""],["Geng","Xue",""],["Yang","Xulei",""],["Wu","Min",""],["Li","Xiaoli",""],["Lin","Weisi",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 09:33:32 GMT"}],"updateDate":"2024-07-03","timestamp":1719912812000,"abstract":"  Applying deep neural networks to 3D point cloud processing has attracted\nincreasing attention due to its advanced performance in many areas, such as\nAR/VR, autonomous driving, and robotics. However, as neural network models and\n3D point clouds expand in size, it becomes a crucial challenge to reduce the\ncomputational and memory overhead to meet latency and energy constraints in\nreal-world applications. Although existing approaches have proposed to reduce\nboth computational cost and memory footprint, most of them only address the\nspatial redundancy in inputs, i.e. removing the redundancy of background points\nin 3D data. In this paper, we propose a novel post-training weight pruning\nscheme for 3D object detection that is (1) orthogonal to all existing point\ncloud sparsifying methods, which determines redundant parameters in the\npretrained model that lead to minimal distortion in both locality and\nconfidence (detection distortion); and (2) a universal plug-and-play pruning\nframework that works with arbitrary 3D detection model. This framework aims to\nminimize detection distortion of network output to maximally maintain detection\nprecision, by identifying layer-wise sparsity based on second-order Taylor\napproximation of the distortion. Albeit utilizing second-order information, we\nintroduced a lightweight scheme to efficiently acquire Hessian information, and\nsubsequently perform dynamic programming to solve the layer-wise sparsity.\nExtensive experiments on KITTI, Nuscenes and ONCE datasets demonstrate that our\napproach is able to maintain and even boost the detection precision on pruned\nmodel under noticeable computation reduction (FLOPs). Noticeably, we achieve\nover 3.89x, 3.72x FLOPs reduction on CenterPoint and PVRCNN model,\nrespectively, without mAP decrease, significantly improving the\nstate-of-the-art.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}