{"id":"2408.04129","title":"Out-of-Core Dimensionality Reduction for Large Data via Out-of-Sample\n  Extensions","authors":"Luca Reichmann, David H\\\"agele, Daniel Weiskopf","authorsParsed":[["Reichmann","Luca",""],["HÃ¤gele","David",""],["Weiskopf","Daniel",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 23:30:53 GMT"}],"updateDate":"2024-08-09","timestamp":1723073453000,"abstract":"  Dimensionality reduction (DR) is a well-established approach for the\nvisualization of high-dimensional data sets. While DR methods are often applied\nto typical DR benchmark data sets in the literature, they might suffer from\nhigh runtime complexity and memory requirements, making them unsuitable for\nlarge data visualization especially in environments outside of high-performance\ncomputing. To perform DR on large data sets, we propose the use of\nout-of-sample extensions. Such extensions allow inserting new data into\nexisting projections, which we leverage to iteratively project data into a\nreference projection that consists only of a small manageable subset. This\nprocess makes it possible to perform DR out-of-core on large data, which would\notherwise not be possible due to memory and runtime limitations. For metric\nmultidimensional scaling (MDS), we contribute an implementation with\nout-of-sample projection capability since typical software libraries do not\nsupport it. We provide an evaluation of the projection quality of five common\nDR algorithms (MDS, PCA, t-SNE, UMAP, and autoencoders) using quality metrics\nfrom the literature and analyze the trade-off between the size of the reference\nset and projection quality. The runtime behavior of the algorithms is also\nquantified with respect to reference set size, out-of-sample batch size, and\ndimensionality of the data sets. Furthermore, we compare the out-of-sample\napproach to other recently introduced DR methods, such as PaCMAP and TriMAP,\nwhich claim to handle larger data sets than traditional approaches. To showcase\nthe usefulness of DR on this large scale, we contribute a use case where we\nanalyze ensembles of streamlines amounting to one billion projected instances.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}