{"id":"2407.13421","title":"CycleMix: Mixing Source Domains for Domain Generalization in\n  Style-Dependent Data","authors":"Aristotelis Ballas and Christos Diou","authorsParsed":[["Ballas","Aristotelis",""],["Diou","Christos",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 11:43:26 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 13:09:22 GMT"}],"updateDate":"2024-07-25","timestamp":1721303006000,"abstract":"  As deep learning-based systems have become an integral part of everyday life,\nlimitations in their generalization ability have begun to emerge. Machine\nlearning algorithms typically rely on the i.i.d. assumption, meaning that their\ntraining and validation data are expected to follow the same distribution,\nwhich does not necessarily hold in practice. In the case of image\nclassification, one frequent reason that algorithms fail to generalize is that\nthey rely on spurious correlations present in training data, such as\nassociating image styles with target classes. These associations may not be\npresent in the unseen test data, leading to significant degradation of their\neffectiveness. In this work, we attempt to mitigate this Domain Generalization\n(DG) problem by training a robust feature extractor which disregards features\nattributed to image-style but infers based on style-invariant image\nrepresentations. To achieve this, we train CycleGAN models to learn the\ndifferent styles present in the training data and randomly mix them together to\ncreate samples with novel style attributes to improve generalization.\nExperimental results on the PACS DG benchmark validate the proposed method.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qADkBowCXVuAdW79PEYB5PGh6p1EEC-NTQM7B80in8Y","pdfSize":"12936812"}
