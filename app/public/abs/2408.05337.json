{"id":"2408.05337","title":"VACoDe: Visual Augmented Contrastive Decoding","authors":"Sihyeon Kim, Boryeong Cho, Sangmin Bae, Sumyeong Ahn, Se-Young Yun","authorsParsed":[["Kim","Sihyeon",""],["Cho","Boryeong",""],["Bae","Sangmin",""],["Ahn","Sumyeong",""],["Yun","Se-Young",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 15:49:31 GMT"}],"updateDate":"2024-08-13","timestamp":1722008971000,"abstract":"  Despite the astonishing performance of recent Large Vision-Language Models\n(LVLMs), these models often generate inaccurate responses. To address this\nissue, previous studies have focused on mitigating hallucinations by employing\ncontrastive decoding (CD) with augmented images, which amplifies the contrast\nwith the original image. However, these methods have limitations, including\nreliance on a single augmentation, which is restrictive for certain tasks, as\nwell as the high cost of using external knowledge. In this study, we address\nthese limitations by exploring how to utilize multiple image augmentations.\nThrough extensive experiments, we observed that different augmentations produce\nvarying levels of contrast depending on the task. Based on this observation, we\nintroduce a novel method called VACoDe, Visual Augmented Contrastive Decoding.\nThis method adaptively selects the augmentation with the highest contrast for\neach task using the proposed softmax distance metric. Our empirical tests show\nthat \\alg outperforms previous methods and improves output quality in various\nvision-language tasks. Additionally, VACoDe can be universally applied across\ndifferent model types and sizes without additional training or the use of\nexternal models and data.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}