{"id":"2407.12951","title":"AdaLog: Post-Training Quantization for Vision Transformers with Adaptive\n  Logarithm Quantizer","authors":"Zhuguanyu Wu, Jiaxin Chen, Hanwen Zhong, Di Huang, Yunhong Wang","authorsParsed":[["Wu","Zhuguanyu",""],["Chen","Jiaxin",""],["Zhong","Hanwen",""],["Huang","Di",""],["Wang","Yunhong",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 18:38:48 GMT"}],"updateDate":"2024-07-19","timestamp":1721241528000,"abstract":"  Vision Transformer (ViT) has become one of the most prevailing fundamental\nbackbone networks in the computer vision community. Despite the high accuracy,\ndeploying it in real applications raises critical challenges including the high\ncomputational cost and inference latency. Recently, the post-training\nquantization (PTQ) technique has emerged as a promising way to enhance ViT's\nefficiency. Nevertheless, existing PTQ approaches for ViT suffer from the\ninflexible quantization on the post-Softmax and post-GELU activations that obey\nthe power-law-like distributions. To address these issues, we propose a novel\nnon-uniform quantizer, dubbed the Adaptive Logarithm AdaLog (AdaLog) quantizer.\nIt optimizes the logarithmic base to accommodate the power-law-like\ndistribution of activations, while simultaneously allowing for\nhardware-friendly quantization and de-quantization. By employing the bias\nreparameterization, the AdaLog quantizer is applicable to both the post-Softmax\nand post-GELU activations. Moreover, we develop an efficient Fast Progressive\nCombining Search (FPCS) strategy to determine the optimal logarithm base for\nAdaLog, as well as the scaling factors and zero points for the uniform\nquantizers. Extensive experimental results on public benchmarks demonstrate the\neffectiveness of our approach for various ViT-based architectures and vision\ntasks including classification, object detection, and instance segmentation.\nCode is available at https://github.com/GoatWu/AdaLog.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}