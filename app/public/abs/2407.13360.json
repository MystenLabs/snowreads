{"id":"2407.13360","title":"Ultra-Low-Latency Edge Inference for Distributed Sensing","authors":"Zhanwei Wang, Anders E. Kal{\\o}r, You Zhou, Petar Popovski, and Kaibin\n  Huang","authorsParsed":[["Wang","Zhanwei",""],["Kal√∏r","Anders E.",""],["Zhou","You",""],["Popovski","Petar",""],["Huang","Kaibin",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 10:01:22 GMT"}],"updateDate":"2024-07-19","timestamp":1721296882000,"abstract":"  There is a broad consensus that artificial intelligence (AI) will be a\ndefining component of the sixth-generation (6G) networks. As a specific\ninstance, AI-empowered sensing will gather and process environmental perception\ndata at the network edge, giving rise to integrated sensing and edge AI (ISEA).\nMany applications, such as autonomous driving and industrial manufacturing, are\nlatency-sensitive and require end-to-end (E2E) performance guarantees under\nstringent deadlines. However, the 5G-style ultra-reliable and low-latency\ncommunication (URLLC) techniques designed with communication reliability and\nagnostic to the data may fall short in achieving the optimal E2E performance of\nperceptive wireless systems. In this work, we introduce an ultra-low-latency\n(ultra-LoLa) inference framework for perceptive networks that facilitates the\nanalysis of the E2E sensing accuracy in distributed sensing by jointly\nconsidering communication reliability and inference accuracy. By characterizing\nthe tradeoff between packet length and the number of sensing observations, we\nderive an efficient optimization procedure that closely approximates the\noptimal tradeoff. We validate the accuracy of the proposed method through\nexperimental results, and show that the proposed ultra-Lola inference framework\noutperforms conventional reliability-oriented protocols with respect to sensing\nperformance under a latency constraint.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}