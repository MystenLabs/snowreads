{"id":"2408.06502","title":"Prompt Recovery for Image Generation Models: A Comparative Study of\n  Discrete Optimizers","authors":"Joshua Nathaniel Williams, Avi Schwarzschild, J. Zico Kolter","authorsParsed":[["Williams","Joshua Nathaniel",""],["Schwarzschild","Avi",""],["Kolter","J. Zico",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 21:35:59 GMT"}],"updateDate":"2024-08-14","timestamp":1723498559000,"abstract":"  Recovering natural language prompts for image generation models, solely based\non the generated images is a difficult discrete optimization problem. In this\nwork, we present the first head-to-head comparison of recent discrete\noptimization techniques for the problem of prompt inversion. We evaluate Greedy\nCoordinate Gradients (GCG), PEZ , Random Search, AutoDAN and BLIP2's image\ncaptioner across various evaluation metrics related to the quality of inverted\nprompts and the quality of the images generated by the inverted prompts. We\nfind that focusing on the CLIP similarity between the inverted prompts and the\nground truth image acts as a poor proxy for the similarity between ground truth\nimage and the image generated by the inverted prompts. While the discrete\noptimizers effectively minimize their objectives, simply using responses from a\nwell-trained captioner often leads to generated images that more closely\nresemble those produced by the original prompts.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fJ1uTPpykeRs3QCTengVWHvPdBPtxPs8OT5g6jGohzU","pdfSize":"2333802"}
