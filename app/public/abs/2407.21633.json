{"id":"2407.21633","title":"Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank\n  Adaptation","authors":"Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang","authorsParsed":[["Luo","Xiang",""],["Tang","Zhiwen",""],["Wang","Jin",""],["Zhang","Xuejie",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 14:26:41 GMT"}],"updateDate":"2024-08-01","timestamp":1722436001000,"abstract":"  Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to\ntransition to unfamiliar domains without manual annotation or extensive\nretraining. Prior research has approached this objective by embedding prompts\ninto language models (LMs). Common methodologies include integrating prompts at\nthe input layer or introducing learnable variables at each transformer layer.\nNonetheless, each strategy exhibits inherent limitations. Prompts integrated at\nthe input layer risk underutilization, with their impact potentially\ndiminishing across successive transformer layers. Conversely, the addition of\nlearnable variables to each layer can complicate the training process and\nincrease inference latency. To tackle the issues mentioned above, this paper\nproposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture\ndesigned for zero-shot DST. DualLoRA incorporates two distinct Low-Rank\nAdaptation (LoRA) components, targeting both dialogue context processing and\nprompt optimization, to ensure the comprehensive influence of prompts\nthroughout the transformer model layers. This is achieved without incurring\nadditional inference latency, showcasing an efficient integration into existing\narchitectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,\nDualLoRA demonstrates notable improvements across multiple domains,\noutperforming traditional baseline methods in zero-shot settings. Our code is\naccessible at: \\url{https://github.com/suntea233/DualLoRA}.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}