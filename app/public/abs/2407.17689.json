{"id":"2407.17689","title":"SAM-MIL: A Spatial Contextual Aware Multiple Instance Learning Approach\n  for Whole Slide Image Classification","authors":"Heng Fang and Sheng Huang and Wenhao Tang and Luwen Huangfu and Bo Liu","authorsParsed":[["Fang","Heng",""],["Huang","Sheng",""],["Tang","Wenhao",""],["Huangfu","Luwen",""],["Liu","Bo",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 01:12:48 GMT"}],"updateDate":"2024-07-26","timestamp":1721869968000,"abstract":"  Multiple Instance Learning (MIL) represents the predominant framework in\nWhole Slide Image (WSI) classification, covering aspects such as sub-typing,\ndiagnosis, and beyond. Current MIL models predominantly rely on instance-level\nfeatures derived from pretrained models such as ResNet. These models segment\neach WSI into independent patches and extract features from these local\npatches, leading to a significant loss of global spatial context and\nrestricting the model's focus to merely local features. To address this issue,\nwe propose a novel MIL framework, named SAM-MIL, that emphasizes spatial\ncontextual awareness and explicitly incorporates spatial context by extracting\ncomprehensive, image-level information. The Segment Anything Model (SAM)\nrepresents a pioneering visual segmentation foundational model that can capture\nsegmentation features without the need for additional fine-tuning, rendering it\nan outstanding tool for extracting spatial context directly from raw WSIs. Our\napproach includes the design of group feature extraction based on spatial\ncontext and a SAM-Guided Group Masking strategy to mitigate class imbalance\nissues. We implement a dynamic mask ratio for different segmentation categories\nand supplement these with representative group features of categories.\nMoreover, SAM-MIL divides instances to generate additional pseudo-bags, thereby\naugmenting the training set, and introduces consistency of spatial context\nacross pseudo-bags to further enhance the model's performance. Experimental\nresults on the CAMELYON-16 and TCGA Lung Cancer datasets demonstrate that our\nproposed SAM-MIL model outperforms existing mainstream methods in WSIs\nclassification. Our open-source implementation code is is available at\nhttps://github.com/FangHeng/SAM-MIL.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"EPFB4NhGqHLyAYBtR6zbKE8Ww1TauVDafXretNDqHAU","pdfSize":"6329288"}
