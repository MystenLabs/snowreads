{"id":"2408.01432","title":"VLG-CBM: Training Concept Bottleneck Models with Vision-Language\n  Guidance","authors":"Divyansh Srivastava, Ge Yan, Tsui-Wei Weng","authorsParsed":[["Srivastava","Divyansh",""],["Yan","Ge",""],["Weng","Tsui-Wei",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 19:44:44 GMT"}],"updateDate":"2024-08-06","timestamp":1721331884000,"abstract":"  Concept Bottleneck Models (CBMs) provide interpretable prediction by\nintroducing an intermediate Concept Bottleneck Layer (CBL), which encodes\nhuman-understandable concepts to explain models' decision. Recent works\nproposed to utilize Large Language Models (LLMs) and pre-trained\nVision-Language Models (VLMs) to automate the training of CBMs, making it more\nscalable and automated. However, existing approaches still fall short in two\naspects: First, the concepts predicted by CBL often mismatch the input image,\nraising doubts about the faithfulness of interpretation. Second, it has been\nshown that concept values encode unintended information: even a set of random\nconcepts could achieve comparable test accuracy to state-of-the-art CBMs. To\naddress these critical limitations, in this work, we propose a novel framework\ncalled Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable\nfaithful interpretability with the benefits of boosted performance. Our method\nleverages off-the-shelf open-domain grounded object detectors to provide\nvisually grounded concept annotation, which largely enhances the faithfulness\nof concept prediction while further improving the model performance. In\naddition, we propose a new metric called Number of Effective Concepts (NEC) to\ncontrol the information leakage and provide better interpretability. Extensive\nevaluations across five standard benchmarks show that our method, VLG-CBM,\noutperforms existing methods by at least 4.27% and up to 51.09% on accuracy at\nNEC=5, and by at least 0.45% and up to 29.78% on average accuracy across\ndifferent NECs, while preserves both faithfulness and interpretability of the\nlearned concepts as demonstrated in extensive experiments.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}