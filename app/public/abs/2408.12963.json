{"id":"2408.12963","title":"Open Llama2 Model for the Lithuanian Language","authors":"Art\\=uras Nakvosas, Povilas Daniu\\v{s}is, Vytas Mulevi\\v{c}ius","authorsParsed":[["Nakvosas","Artūras",""],["Daniušis","Povilas",""],["Mulevičius","Vytas",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 10:18:39 GMT"}],"updateDate":"2024-08-26","timestamp":1724408319000,"abstract":"  In this paper, we propose and describe the first open Llama2 large language\nmodels (LLMs) for the Lithuanian language, including an accompanying\nquestion/answer (Q/A) dataset and translations of popular LLM benchmarks. We\nprovide a brief review of open regional LLMs and detailed information on the\nproposed LLMs and their training process. We also conduct an empirical\nevaluation, comparing the perplexities of the proposed LLMs with those of other\nmodern open LLMs. In addition, benchmarking the proposed LLMs against language\nunderstanding tasks reveals that high-quality pretraining datasets may be\nessential for achieving models that perform efficiently on these benchmarks.\nThe full realisations of the described LLMs are available in the accompanying\nopen repository~\\url{https://huggingface.co/neurotechnology}.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ULJxVXhRh_4oQbUUbT7GpjKTMwH_zP5oP1OH8j5XaPQ","pdfSize":"1387220"}
