{"id":"2408.12253","title":"Epsilon: Exploring Comprehensive Visual-Semantic Projection for\n  Multi-Label Zero-Shot Learning","authors":"Ziming Liu, Jingcai Guo, Song Guo, Xiaocheng Lu","authorsParsed":[["Liu","Ziming",""],["Guo","Jingcai",""],["Guo","Song",""],["Lu","Xiaocheng",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 09:45:24 GMT"},{"version":"v2","created":"Sun, 25 Aug 2024 14:13:40 GMT"}],"updateDate":"2024-08-27","timestamp":1724319924000,"abstract":"  This paper investigates a challenging problem of zero-shot learning in the\nmulti-label scenario (MLZSL), wherein the model is trained to recognize\nmultiple unseen classes within a sample (e.g., an image) based on seen classes\nand auxiliary knowledge, e.g., semantic information. Existing methods usually\nresort to analyzing the relationship of various seen classes residing in a\nsample from the dimension of spatial or semantic characteristics and\ntransferring the learned model to unseen ones. However, they neglect the\nintegrity of local and global features. Although the use of the attention\nstructure will accurately locate local features, especially objects, it will\nsignificantly lose its integrity, and the relationship between classes will\nalso be affected. Rough processing of global features will also directly affect\ncomprehensiveness. This neglect will make the model lose its grasp of the main\ncomponents of the image. Relying only on the local existence of seen classes\nduring the inference stage introduces unavoidable bias. In this paper, we\npropose a novel and comprehensive visual-semantic framework for MLZSL, dubbed\nEpsilon, to fully make use of such properties and enable a more accurate and\nrobust visual-semantic projection. In terms of spatial information, we achieve\neffective refinement by group aggregating image features into several semantic\nprompts. It can aggregate semantic information rather than class information,\npreserving the correlation between semantics. In terms of global semantics, we\nuse global forward propagation to collect as much information as possible to\nensure that semantics are not omitted. Experiments on large-scale MLZSL\nbenchmark datasets NUS-Wide and Open-Images-v4 demonstrate that the proposed\nEpsilon outperforms other state-of-the-art methods with large margins.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}