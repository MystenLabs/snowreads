{"id":"2408.09174","title":"TableBench: A Comprehensive and Complex Benchmark for Table Question\n  Answering","authors":"Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun\n  Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, Guanglin Niu, Tongliang\n  Li, Zhoujun Li","authorsParsed":[["Wu","Xianjie",""],["Yang","Jian",""],["Chai","Linzheng",""],["Zhang","Ge",""],["Liu","Jiaheng",""],["Du","Xinrun",""],["Liang","Di",""],["Shu","Daixin",""],["Cheng","Xianfu",""],["Sun","Tianzhen",""],["Niu","Guanglin",""],["Li","Tongliang",""],["Li","Zhoujun",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 11:40:10 GMT"}],"updateDate":"2024-08-20","timestamp":1723894810000,"abstract":"  Recent advancements in Large Language Models (LLMs) have markedly enhanced\nthe interpretation and processing of tabular data, introducing previously\nunimaginable capabilities. Despite these achievements, LLMs still encounter\nsignificant challenges when applied in industrial scenarios, particularly due\nto the increased complexity of reasoning required with real-world tabular data,\nunderscoring a notable disparity between academic benchmarks and practical\napplications. To address this discrepancy, we conduct a detailed investigation\ninto the application of tabular data in industrial scenarios and propose a\ncomprehensive and complex benchmark TableBench, including 18 fields within four\nmajor categories of table question answering (TableQA) capabilities.\nFurthermore, we introduce TableLLM, trained on our meticulously constructed\ntraining set TableInstruct, achieving comparable performance with GPT-3.5.\nMassive experiments conducted on TableBench indicate that both open-source and\nproprietary LLMs still have significant room for improvement to meet real-world\ndemands, where the most advanced model, GPT-4, achieves only a modest score\ncompared to humans.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}