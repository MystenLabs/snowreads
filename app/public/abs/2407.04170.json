{"id":"2407.04170","title":"Attention Normalization Impacts Cardinality Generalization in Slot\n  Attention","authors":"Markus Krimmel, Jan Achterhold, Joerg Stueckler","authorsParsed":[["Krimmel","Markus",""],["Achterhold","Jan",""],["Stueckler","Joerg",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 22:09:01 GMT"}],"updateDate":"2024-07-08","timestamp":1720130941000,"abstract":"  Object-centric scene decompositions are important representations for\ndownstream tasks in fields such as computer vision and robotics. The recently\nproposed Slot Attention module, already leveraged by several derivative works\nfor image segmentation and object tracking in videos, is a deep learning\ncomponent which performs unsupervised object-centric scene decomposition on\ninput images. It is based on an attention architecture, in which latent slot\nvectors, which hold compressed information on objects, attend to localized\nperceptual features from the input image. In this paper, we show that design\ndecisions on normalizing the aggregated values in the attention architecture\nhave considerable impact on the capabilities of Slot Attention to generalize to\na higher number of slots and objects as seen during training. We argue that the\noriginal Slot Attention normalization scheme discards information on the prior\nassignment probability of pixels to slots, which impairs its generalization\ncapabilities. Based on these findings, we propose and investigate alternative\nnormalization approaches which increase the generalization capabilities of Slot\nAttention to varying slot and object counts, resulting in performance gains on\nthe task of unsupervised image segmentation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}