{"id":"2407.06134","title":"Scaling Analog Photonic Accelerators for Byte-Size, Integer General\n  Matrix Multiply (GEMM) Kernels","authors":"Oluwaseun Adewunmi Alo, Sairam Sri Vatsavai, Ishan Thakkar","authorsParsed":[["Alo","Oluwaseun Adewunmi",""],["Vatsavai","Sairam Sri",""],["Thakkar","Ishan",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:07:04 GMT"}],"updateDate":"2024-07-09","timestamp":1720458424000,"abstract":"  Deep Neural Networks (DNNs) predominantly rely on General Matrix Multiply\n(GEMM) kernels, which are often accelerated using specialized hardware\narchitectures. Recently, analog photonic GEMM accelerators have emerged as a\npromising alternative, offering vastly superior speed and energy efficiency\ncompared to traditional electronic accelerators. However, these photonic cannot\nsupport wider than 4-bit integer operands due to their inherent trade-offs\nbetween analog dynamic range and parallelism. This is often inadequate for DNN\ntraining as at least 8-bit wide operands are deemed necessary to prevent\nsignificant accuracy drops. To address these limitations, we introduce a\nscalable photonic GEMM accelerator named SPOGA. SPOGA utilizes enhanced\nfeatures such as analog summation of homodyne optical signals and\nin-transduction positional weighting of operands. By employing an extended\noptical-analog dataflow that minimizes overheads associated with bit-sliced\ninteger arithmetic, SPOGA supports byte-size integer GEMM kernels, achieving\nsignificant improvements in throughput, latency, and energy efficiency.\nSpecifically, SPOGA demonstrates up to 14.4$\\times$, 2$\\times$, and\n28.5$\\times$ improvements in frames-per-second (FPS), FPS/Watt, and\nFPS/Watt/mm$^2$ respectively, compared to existing state-of-the-art photonic\nsolutions.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Emerging Technologies","Computing Research Repository/Performance"],"license":"http://creativecommons.org/licenses/by/4.0/"}