{"id":"2408.09891","title":"Differential Private Stochastic Optimization with Heavy-tailed Data:\n  Towards Optimal Rates","authors":"Puning Zhao, Jiafei Wu, Zhe Liu, Chong Wang, Rongfei Fan, Qingming Li","authorsParsed":[["Zhao","Puning",""],["Wu","Jiafei",""],["Liu","Zhe",""],["Wang","Chong",""],["Fan","Rongfei",""],["Li","Qingming",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 11:07:05 GMT"}],"updateDate":"2024-08-20","timestamp":1724065625000,"abstract":"  We study convex optimization problems under differential privacy (DP). With\nheavy-tailed gradients, existing works achieve suboptimal rates. The main\nobstacle is that existing gradient estimators have suboptimal tail properties,\nresulting in a superfluous factor of $d$ in the union bound. In this paper, we\nexplore algorithms achieving optimal rates of DP optimization with heavy-tailed\ngradients. Our first method is a simple clipping approach. Under bounded $p$-th\norder moments of gradients, with $n$ samples, it achieves\n$\\tilde{O}(\\sqrt{d/n}+\\sqrt{d}(\\sqrt{d}/n\\epsilon)^{1-1/p})$ population risk\nwith $\\epsilon\\leq 1/\\sqrt{d}$. We then propose an iterative updating method,\nwhich is more complex but achieves this rate for all $\\epsilon\\leq 1$. The\nresults significantly improve over existing methods. Such improvement relies on\na careful treatment of the tail behavior of gradient estimators. Our results\nmatch the minimax lower bound in \\cite{kamath2022improved}, indicating that the\ntheoretical limit of stochastic convex optimization under DP is achievable.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security","Computing Research Repository/Data Structures and Algorithms"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}