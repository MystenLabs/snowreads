{"id":"2407.20455","title":"Learning Feature-Preserving Portrait Editing from Generated Pairs","authors":"Bowei Chen, Tiancheng Zhi, Peihao Zhu, Shen Sang, Jing Liu, Linjie Luo","authorsParsed":[["Chen","Bowei",""],["Zhi","Tiancheng",""],["Zhu","Peihao",""],["Sang","Shen",""],["Liu","Jing",""],["Luo","Linjie",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 23:19:42 GMT"}],"updateDate":"2024-07-31","timestamp":1722295182000,"abstract":"  Portrait editing is challenging for existing techniques due to difficulties\nin preserving subject features like identity. In this paper, we propose a\ntraining-based method leveraging auto-generated paired data to learn desired\nediting while ensuring the preservation of unchanged subject features.\nSpecifically, we design a data generation process to create reasonably good\ntraining pairs for desired editing at low cost. Based on these pairs, we\nintroduce a Multi-Conditioned Diffusion Model to effectively learn the editing\ndirection and preserve subject features. During inference, our model produces\naccurate editing mask that can guide the inference process to further preserve\ndetailed subject features. Experiments on costume editing and cartoon\nexpression editing show that our method achieves state-of-the-art quality,\nquantitatively and qualitatively.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}