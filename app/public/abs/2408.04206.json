{"id":"2408.04206","title":"DC Algorithm for Estimation of Sparse Gaussian Graphical Models","authors":"Tomokaze Shiratori, Yuichi Takano","authorsParsed":[["Shiratori","Tomokaze",""],["Takano","Yuichi",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 04:05:50 GMT"}],"updateDate":"2024-08-09","timestamp":1723089950000,"abstract":"  Sparse estimation for Gaussian graphical models is a crucial technique for\nmaking the relationships among numerous observed variables more interpretable\nand quantifiable. Various methods have been proposed, including graphical\nlasso, which utilizes the $\\ell_1$ norm as a regularization term, as well as\nmethods employing non-convex regularization terms. However, most of these\nmethods approximate the $\\ell_0$ norm with convex functions. To estimate more\naccurate solutions, it is desirable to treat the $\\ell_0$ norm directly as a\nregularization term. In this study, we formulate the sparse estimation problem\nfor Gaussian graphical models using the $\\ell_0$ norm and propose a method to\nsolve this problem using the Difference of Convex functions Algorithm (DCA).\nSpecifically, we convert the $\\ell_0$ norm constraint into an equivalent\nlargest-$K$ norm constraint, reformulate the constrained problem into a\npenalized form, and solve it using the DC algorithm (DCA). Furthermore, we\ndesigned an algorithm that efficiently computes using graphical lasso.\nExperimental results with synthetic data show that our method yields results\nthat are equivalent to or better than existing methods. Comparisons of model\nlearning through cross-validation confirm that our method is particularly\nadvantageous in selecting true edges.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}