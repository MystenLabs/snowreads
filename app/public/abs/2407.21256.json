{"id":"2407.21256","title":"Leveraging Adaptive Implicit Representation Mapping for Ultra\n  High-Resolution Image Segmentation","authors":"Ziyu Zhao, Xiaoguang Li, Pingping Cai, Canyu Zhang, and Song Wang","authorsParsed":[["Zhao","Ziyu",""],["Li","Xiaoguang",""],["Cai","Pingping",""],["Zhang","Canyu",""],["Wang","Song",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 00:34:37 GMT"}],"updateDate":"2024-08-01","timestamp":1722386077000,"abstract":"  Implicit representation mapping (IRM) can translate image features to any\ncontinuous resolution, showcasing its potent capability for\nultra-high-resolution image segmentation refinement. Current IRM-based methods\nfor refining ultra-high-resolution image segmentation often rely on CNN-based\nencoders to extract image features and apply a Shared Implicit Representation\nMapping Function (SIRMF) to convert pixel-wise features into segmented results.\nHence, these methods exhibit two crucial limitations. Firstly, the CNN-based\nencoder may not effectively capture long-distance information, resulting in a\nlack of global semantic information in the pixel-wise features. Secondly, SIRMF\nis shared across all samples, which limits its ability to generalize and handle\ndiverse inputs. To address these limitations, we propose a novel approach that\nleverages the newly proposed Adaptive Implicit Representation Mapping (AIRM)\nfor ultra-high-resolution Image Segmentation. Specifically, the proposed method\ncomprises two components: (1) the Affinity Empowered Encoder (AEE), a robust\nfeature extractor that leverages the benefits of the transformer architecture\nand semantic affinity to model long-distance features effectively, and (2) the\nAdaptive Implicit Representation Mapping Function (AIRMF), which adaptively\ntranslates pixel-wise features without neglecting the global semantic\ninformation, allowing for flexible and precise feature translation. We\nevaluated our method on the commonly used ultra-high-resolution segmentation\nrefinement datasets, i.e., BIG and PASCAL VOC 2012. The extensive experiments\ndemonstrate that our method outperforms competitors by a large margin. The code\nis provided in supplementary material.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}