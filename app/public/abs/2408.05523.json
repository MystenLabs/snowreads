{"id":"2408.05523","title":"DeepFace-Attention: Multimodal Face Biometrics for Attention Estimation\n  with Application to e-Learning","authors":"Roberto Daza, Luis F. Gomez, Julian Fierrez, Aythami Morales, Ruben\n  Tolosana, Javier Ortega-Garcia","authorsParsed":[["Daza","Roberto",""],["Gomez","Luis F.",""],["Fierrez","Julian",""],["Morales","Aythami",""],["Tolosana","Ruben",""],["Ortega-Garcia","Javier",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 11:39:11 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 14:34:34 GMT"}],"updateDate":"2024-08-15","timestamp":1723289951000,"abstract":"  This work introduces an innovative method for estimating attention levels\n(cognitive load) using an ensemble of facial analysis techniques applied to\nwebcam videos. Our method is particularly useful, among others, in e-learning\napplications, so we trained, evaluated, and compared our approach on the mEBAL2\ndatabase, a public multi-modal database acquired in an e-learning environment.\nmEBAL2 comprises data from 60 users who performed 8 different tasks. These\ntasks varied in difficulty, leading to changes in their cognitive loads. Our\napproach adapts state-of-the-art facial analysis technologies to quantify the\nusers' cognitive load in the form of high or low attention. Several behavioral\nsignals and physiological processes related to the cognitive load are used,\nsuch as eyeblink, heart rate, facial action units, and head pose, among others.\nFurthermore, we conduct a study to understand which individual features obtain\nbetter results, the most efficient combinations, explore local and global\nfeatures, and how temporary time intervals affect attention level estimation,\namong other aspects. We find that global facial features are more appropriate\nfor multimodal systems using score-level fusion, particularly as the temporal\nwindow increases. On the other hand, local features are more suitable for\nfusion through neural network training with score-level fusion approaches. Our\nmethod outperforms existing state-of-the-art accuracies using the public mEBAL2\nbenchmark.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}