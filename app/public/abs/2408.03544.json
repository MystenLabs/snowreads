{"id":"2408.03544","title":"Unlocking the Non-Native Language Context Limitation: Native Language\n  Prompting Facilitates Knowledge Elicitation","authors":"Baixuan Li, Yunlong Fan, Zhiqiang Gao","authorsParsed":[["Li","Baixuan",""],["Fan","Yunlong",""],["Gao","Zhiqiang",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 04:49:38 GMT"},{"version":"v2","created":"Fri, 16 Aug 2024 10:42:38 GMT"}],"updateDate":"2024-08-19","timestamp":1723006178000,"abstract":"  Multilingual large language models (MLLMs) struggle to answer questions posed\nin non-dominant languages, even though they have acquired the relevant\nknowledge from their dominant language corpus. In contrast, human multilinguals\ncan overcome such non-native language context limitations through Positive\nNative Language Transfer (PNLT). Inspired by the process of PNLT, we analogize\nthe dominant language of MLLMs to the native language of human multilinguals,\nand propose Native Language Prompting (NatLan) to simulate the PNLT observed in\nhuman multilinguals. It explicitly creates native language contexts for MLLMs\nto facilitate the elicitation of the rich native language knowledge during\nquestion-answering, unlocking the limitations imposed by non-native language\ncontexts. By employing multi-MLLM collaboration, NatLan reduces the workload on\neach MLLM in simulating PNLT and refines semantic transfer. On the C-Eval\nbenchmark, NatLan provides up to a 10.1% average accuracy improvement and up to\na 5.0% increase in the hard-level subset across five MLLMs, surpassing all\ntop-notch related methods. Our code is available at\nhttps://github.com/AnonyNLP/NatLan.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}