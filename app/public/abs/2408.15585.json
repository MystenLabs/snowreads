{"id":"2408.15585","title":"Whisper-PMFA: Partial Multi-Scale Feature Aggregation for Speaker\n  Verification using Whisper Models","authors":"Yiyang Zhao, Shuai Wang, Guangzhi Sun, Zehua Chen, Chao Zhang,\n  Mingxing Xu, Thomas Fang Zheng","authorsParsed":[["Zhao","Yiyang",""],["Wang","Shuai",""],["Sun","Guangzhi",""],["Chen","Zehua",""],["Zhang","Chao",""],["Xu","Mingxing",""],["Zheng","Thomas Fang",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 07:21:36 GMT"}],"updateDate":"2024-08-29","timestamp":1724829696000,"abstract":"  In this paper, Whisper, a large-scale pre-trained model for automatic speech\nrecognition, is proposed to apply to speaker verification. A partial\nmulti-scale feature aggregation (PMFA) approach is proposed based on a subset\nof Whisper encoder blocks to derive highly discriminative speaker\nembeddings.Experimental results demonstrate that using the middle to later\nblocks of the Whisper encoder keeps more speaker information. On the VoxCeleb1\nand CN-Celeb1 datasets, our system achieves 1.42% and 8.23% equal error rates\n(EERs) respectively, receiving 0.58% and 1.81% absolute EER reductions over the\nECAPA-TDNN baseline, and 0.46% and 0.97% over the ResNet34 baseline.\nFurthermore, our results indicate that using Whisper models trained on\nmultilingual data can effectively enhance the model's robustness across\nlanguages. Finally, the low-rank adaptation approach is evaluated, which\nreduces the trainable model parameters by approximately 45 times while only\nslightly increasing EER by 0.2%.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GQKkLPvFMUcf-MiMQ0mz0Q2BEnOwl6tKSd-lk62aSOs","pdfSize":"333430"}
