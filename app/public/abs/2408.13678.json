{"id":"2408.13678","title":"A layer-wise analysis of Mandarin and English suprasegmentals in SSL\n  speech models","authors":"Ant\\'on de la Fuente, Dan Jurafsky","authorsParsed":[["de la Fuente","Ant√≥n",""],["Jurafsky","Dan",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 22:03:40 GMT"}],"updateDate":"2024-08-27","timestamp":1724537020000,"abstract":"  This study asks how self-supervised speech models represent suprasegmental\ncategories like Mandarin lexical tone, English lexical stress, and English\nphrasal accents. Through a series of probing tasks, we make layer-wise\ncomparisons of English and Mandarin 12 layer monolingual models. Our findings\nsuggest that 1) English and Mandarin wav2vec 2.0 models learn contextual\nrepresentations of abstract suprasegmental categories which are strongest in\nthe middle third of the network. 2) Models are better at representing features\nthat exist in the language of their training data, and this difference is\ndriven by enriched context in transformer blocks, not local acoustic\nrepresentation. 3) Fine-tuned wav2vec 2.0 improves performance in later layers\ncompared to pre-trained models mainly for lexically contrastive features like\ntone and stress, 4) HuBERT and WavLM learn similar representations to wav2vec\n2.0, differing mainly in later layer performance. Our results extend previous\nunderstanding of how models represent suprasegmentals and offer new insights\ninto the language-specificity and contextual nature of these representations.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}