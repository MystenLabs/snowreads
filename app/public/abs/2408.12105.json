{"id":"2408.12105","title":"You Only Merge Once: Learning the Pareto Set of Preference-Aware Model\n  Merging","authors":"Weiyu Chen, James Kwok","authorsParsed":[["Chen","Weiyu",""],["Kwok","James",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 03:41:14 GMT"}],"updateDate":"2024-08-23","timestamp":1724298074000,"abstract":"  Model merging, which combines multiple models into a single model, has gained\nincreasing popularity in recent years. By efficiently integrating the\ncapabilities of various models without their original training data, this\nsignificantly reduces the parameter count and memory usage. However, current\nmethods can only produce one single merged model. This necessitates a\nperformance trade-off due to conflicts among the various models, and the\nresultant one-size-fits-all model may not align with the preferences of\ndifferent users who may prioritize certain models over others. To address this\nissue, we propose preference-aware model merging, and formulate this as a\nmulti-objective optimization problem in which the performance of the merged\nmodel on each base model's task is treated as an objective. In only one merging\nprocess, the proposed parameter-efficient structure can generate the whole\nPareto set of merged models, each representing the Pareto-optimal model for a\ngiven user-specified preference. Merged models can also be selected from the\nlearned Pareto set that are tailored to different user preferences.\nExperimental results on a number of benchmark datasets demonstrate that the\nproposed preference-aware Pareto Merging can obtain a diverse set of trade-off\nmodels and outperforms state-of-the-art model merging baselines.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}