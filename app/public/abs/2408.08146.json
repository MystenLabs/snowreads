{"id":"2408.08146","title":"KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft\n  Heads with Adversarial Learning","authors":"Kaiqi Zhang and Jing Zhao and Rui Chen","authorsParsed":[["Zhang","Kaiqi",""],["Zhao","Jing",""],["Chen","Rui",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 13:29:48 GMT"}],"updateDate":"2024-08-16","timestamp":1723728588000,"abstract":"  Large Language Models (LLMs) exhibit high inference latency due to their\nautoregressive decoding nature. While the draft head in speculative decoding\nmitigates this issue, its full potential remains unexplored. In this paper, we\nintroduce KOALA (K-layer Optimized Adversarial Learning Architecture), an\northogonal approach to the draft head. By transforming the conventional\nsingle-layer draft head into a multi-layer architecture and incorporating\nadversarial learning into the traditional supervised training, KOALA\nsignificantly improves the accuracy of the draft head in predicting subsequent\ntokens, thus more closely mirroring the functionality of LLMs. Although this\nimprovement comes at the cost of slightly increased drafting overhead, KOALA\nsubstantially unlocks the draft head's potential, greatly enhancing speculative\ndecoding. We conducted comprehensive evaluations of KOALA, including both\nautoregressive and non-autoregressive draft heads across various tasks,\ndemonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is\n10.57%-14.09% faster than the original draft heads.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}