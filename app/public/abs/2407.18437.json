{"id":"2407.18437","title":"Mixed Non-linear Quantization for Vision Transformers","authors":"Gihwan Kim, Jemin Lee, Sihyeong Park, Yongin Kwon, Hyungshin Kim","authorsParsed":[["Kim","Gihwan",""],["Lee","Jemin",""],["Park","Sihyeong",""],["Kwon","Yongin",""],["Kim","Hyungshin",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 00:19:01 GMT"}],"updateDate":"2024-07-29","timestamp":1721953141000,"abstract":"  The majority of quantization methods have been proposed to reduce the model\nsize of Vision Transformers, yet most of them have overlooked the quantization\nof non-linear operations. Only a few works have addressed quantization for\nnon-linear operations, but they applied a single quantization method across all\nnon-linear operations. We believe that this can be further improved by\nemploying a different quantization method for each non-linear operation.\nTherefore, to assign the most error-minimizing quantization method from the\nknown methods to each non-linear layer, we propose a mixed non-linear\nquantization that considers layer-wise quantization sensitivity measured by\nSQNR difference metric. The results show that our method outperforms I-BERT,\nFQ-ViT, and I-ViT in both 8-bit and 6-bit settings for ViT, DeiT, and Swin\nmodels by an average of 0.6%p and 19.6%p, respectively. Our method outperforms\nI-BERT and I-ViT by 0.6%p and 20.8%p, respectively, when training time is\nlimited. We plan to release our code at\nhttps://gitlab.com/ones-ai/mixed-non-linear-quantization.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}