{"id":"2408.06753","title":"Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies","authors":"Marcella Astrid, Enjie Ghorbel, Djamila Aouada","authorsParsed":[["Astrid","Marcella",""],["Ghorbel","Enjie",""],["Aouada","Djamila",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 09:19:59 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 10:53:34 GMT"}],"updateDate":"2024-08-15","timestamp":1723540799000,"abstract":"  Existing methods on audio-visual deepfake detection mainly focus on\nhigh-level features for modeling inconsistencies between audio and visual data.\nAs a result, these approaches usually overlook finer audio-visual artifacts,\nwhich are inherent to deepfakes. Herein, we propose the introduction of\nfine-grained mechanisms for detecting subtle artifacts in both spatial and\ntemporal domains. First, we introduce a local audio-visual model capable of\ncapturing small spatial regions that are prone to inconsistencies with audio.\nFor that purpose, a fine-grained mechanism based on a spatially-local distance\ncoupled with an attention module is adopted. Second, we introduce a\ntemporally-local pseudo-fake augmentation to include samples incorporating\nsubtle temporal inconsistencies in our training set. Experiments on the DFDC\nand the FakeAVCeleb datasets demonstrate the superiority of the proposed method\nin terms of generalization as compared to the state-of-the-art under both\nin-dataset and cross-dataset settings.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}