{"id":"2407.01526","title":"Scalable Nested Optimization for Deep Learning","authors":"Jonathan Lorraine","authorsParsed":[["Lorraine","Jonathan",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:59:41 GMT"}],"updateDate":"2024-07-02","timestamp":1719856781000,"abstract":"  Gradient-based optimization has been critical to the success of machine\nlearning, updating a single set of parameters to minimize a single loss. A\ngrowing number of applications rely on a generalization of this, where we have\na bilevel or nested optimization of which subsets of parameters update on\ndifferent objectives nested inside each other. We focus on motivating examples\nof hyperparameter optimization and generative adversarial networks. However,\nnaively applying classical methods often fails when we look at solving these\nnested problems on a large scale. In this thesis, we build tools for nested\noptimization that scale to deep learning setups.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nnczm6vnlgPs9cXsVCzvW2l2ctKfLyzz1O9o3Yhpx8I","pdfSize":"17186050"}
