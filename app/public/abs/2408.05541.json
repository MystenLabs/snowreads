{"id":"2408.05541","title":"P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for\n  Optimizing LLM Training","authors":"Yingxuan Yang, Huayi Wang, Muning Wen, Weinan Zhang","authorsParsed":[["Yang","Yingxuan",""],["Wang","Huayi",""],["Wen","Muning",""],["Zhang","Weinan",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 12:44:49 GMT"}],"updateDate":"2024-08-13","timestamp":1723293889000,"abstract":"  In the rapidly evolving field of Large Language Models (LLMs), selecting\nhigh-quality data for fine-tuning is essential. This paper focuses on\ntask-specific data pruning and selection to enhance fine-tuning. We introduce\nan innovative framework, termed P3, which improves LLM performance through a\ndynamic, adaptive training strategy. Specifically, P3 comprises the following\ncomponents: (1) Policy-driven Difficulty Measurement: we begin by measuring the\ndifficulty of data based on the model's real-time performance, transitioning\nfrom static, predefined metrics to more dynamic and adaptable ones. (2)\nPace-adaptive Selection: we employ self-paced learning (SPL) to gradually\nselect increasingly challenging data, thereby progressively enhancing the\nmodel's performance. (3) Diversity Promotion: we integrate Determinantal Point\nProcess (DPP) into the selection process to promote the diversity within and\nbetween samples, enriching the learning process. We have validated our method\non two well-known LLM datasets, APPS and MATH, designed for logical reasoning\nscenarios. The results show that our P3 framework significantly improves\ntraining outcomes compared to traditional methods. By fundamentally refining\ndata selection and utilization strategies, P3 not only advances theoretical\nunderstanding of dynamic training approaches but also provides a versatile\nframework that can revolutionize model training in natural language processing.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}