{"id":"2407.10661","title":"Learning to Estimate the Pose of a Peer Robot in a Camera Image by\n  Predicting the States of its LEDs","authors":"Nicholas Carlotti, Mirko Nava, Alessandro Giusti","authorsParsed":[["Carlotti","Nicholas",""],["Nava","Mirko",""],["Giusti","Alessandro",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 12:23:35 GMT"}],"updateDate":"2024-08-14","timestamp":1721046215000,"abstract":"  We consider the problem of training a fully convolutional network to estimate\nthe relative 6D pose of a robot given a camera image, when the robot is\nequipped with independent controllable LEDs placed in different parts of its\nbody. The training data is composed by few (or zero) images labeled with a\nground truth relative pose and many images labeled only with the true state\n(\\textsc{on} or \\textsc{off}) of each of the peer LEDs. The former data is\nexpensive to acquire, requiring external infrastructure for tracking the two\nrobots; the latter is cheap as it can be acquired by two unsupervised robots\nmoving randomly and toggling their LEDs while sharing the true LED states via\nradio. Training with the latter dataset on estimating the LEDs' state of the\npeer robot (\\emph{pretext task}) promotes learning the relative localization\ntask (\\emph{end task}). Experiments on real-world data acquired by two\nautonomous wheeled robots show that a model trained only on the pretext task\nsuccessfully learns to localize a peer robot on the image plane; fine-tuning\nsuch model on the end task with few labeled images yields statistically\nsignificant improvements in 6D relative pose estimation with respect to\nbaselines that do not use pretext-task pre-training, and alternative\napproaches. Estimating the state of multiple independent LEDs promotes learning\nto estimate relative heading. The approach works even when a large fraction of\ntraining images do not include the peer robot and generalizes well to unseen\nenvironments.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/"}