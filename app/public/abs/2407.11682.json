{"id":"2407.11682","title":"MapDistill: Boosting Efficient Camera-based HD Map Construction via\n  Camera-LiDAR Fusion Model Distillation","authors":"Xiaoshuai Hao, Ruikai Li, Hui Zhang, Dingzhe Li, Rong Yin, Sangil\n  Jung, Seung-In Park, ByungIn Yoo, Haimei Zhao, Jing Zhang","authorsParsed":[["Hao","Xiaoshuai",""],["Li","Ruikai",""],["Zhang","Hui",""],["Li","Dingzhe",""],["Yin","Rong",""],["Jung","Sangil",""],["Park","Seung-In",""],["Yoo","ByungIn",""],["Zhao","Haimei",""],["Zhang","Jing",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 13:00:20 GMT"}],"updateDate":"2024-07-17","timestamp":1721134820000,"abstract":"  Online high-definition (HD) map construction is an important and challenging\ntask in autonomous driving. Recently, there has been a growing interest in\ncost-effective multi-view camera-based methods without relying on other sensors\nlike LiDAR. However, these methods suffer from a lack of explicit depth\ninformation, necessitating the use of large models to achieve satisfactory\nperformance. To address this, we employ the Knowledge Distillation (KD) idea\nfor efficient HD map construction for the first time and introduce a novel\nKD-based approach called MapDistill to transfer knowledge from a\nhigh-performance camera-LiDAR fusion model to a lightweight camera-only model.\nSpecifically, we adopt the teacher-student architecture, i.e., a camera-LiDAR\nfusion model as the teacher and a lightweight camera model as the student, and\ndevise a dual BEV transform module to facilitate cross-modal knowledge\ndistillation while maintaining cost-effective camera-only deployment.\nAdditionally, we present a comprehensive distillation scheme encompassing\ncross-modal relation distillation, dual-level feature distillation, and map\nhead distillation. This approach alleviates knowledge transfer challenges\nbetween modalities, enabling the student model to learn improved feature\nrepresentations for HD map construction. Experimental results on the\nchallenging nuScenes dataset demonstrate the effectiveness of MapDistill,\nsurpassing existing competitors by over 7.7 mAP or 4.5X speedup.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}