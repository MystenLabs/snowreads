{"id":"2408.06707","title":"MAIR++: Improving Multi-view Attention Inverse Rendering with Implicit\n  Lighting Representation","authors":"JunYong Choi, SeokYeong Lee, Haesol Park, Seung-Won Jung, Ig-Jae Kim,\n  Junghyun Cho","authorsParsed":[["Choi","JunYong",""],["Lee","SeokYeong",""],["Park","Haesol",""],["Jung","Seung-Won",""],["Kim","Ig-Jae",""],["Cho","Junghyun",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 08:04:23 GMT"}],"updateDate":"2024-08-14","timestamp":1723536263000,"abstract":"  In this paper, we propose a scene-level inverse rendering framework that uses\nmulti-view images to decompose the scene into geometry, SVBRDF, and 3D\nspatially-varying lighting. While multi-view images have been widely used for\nobject-level inverse rendering, scene-level inverse rendering has primarily\nbeen studied using single-view images due to the lack of a dataset containing\nhigh dynamic range multi-view images with ground-truth geometry, material, and\nspatially-varying lighting. To improve the quality of scene-level inverse\nrendering, a novel framework called Multi-view Attention Inverse Rendering\n(MAIR) was recently introduced. MAIR performs scene-level multi-view inverse\nrendering by expanding the OpenRooms dataset, designing efficient pipelines to\nhandle multi-view images, and splitting spatially-varying lighting. Although\nMAIR showed impressive results, its lighting representation is fixed to\nspherical Gaussians, which limits its ability to render images realistically.\nConsequently, MAIR cannot be directly used in applications such as material\nediting. Moreover, its multi-view aggregation networks have difficulties\nextracting rich features because they only focus on the mean and variance\nbetween multi-view features. In this paper, we propose its extended version,\ncalled MAIR++. MAIR++ addresses the aforementioned limitations by introducing\nan implicit lighting representation that accurately captures the lighting\nconditions of an image while facilitating realistic rendering. Furthermore, we\ndesign a directional attention-based multi-view aggregation network to infer\nmore intricate relationships between views. Experimental results show that\nMAIR++ not only achieves better performance than MAIR and single-view-based\nmethods, but also displays robust performance on unseen real-world scenes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}