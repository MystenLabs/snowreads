{"id":"2407.01085","title":"Rethinking LLM-based Preference Evaluation","authors":"Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Jingang Wang,\n  Zhenyu Chen, Hui Xiong","authorsParsed":[["Hu","Zhengyu",""],["Song","Linxin",""],["Zhang","Jieyu",""],["Xiao","Zheyuan",""],["Wang","Jingang",""],["Chen","Zhenyu",""],["Xiong","Hui",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 08:37:41 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 22:23:08 GMT"}],"updateDate":"2024-08-12","timestamp":1719823061000,"abstract":"  The use of large language model (LLM)-based preference evaluations has become\nwidespread for comparing model responses, but it has revealed a notable bias\ntowards longer responses, questioning the reliability of such evaluations. This\npaper explores the length bias in LLM evaluations from a data-centric\nperspective, analyzing 14 commonly used preference datasets and 10 reward\nmodels. Our findings indicate that human preference labeling favors longer\nresponses and this spurious correlation is learned by the reward model and\nsubsequently propagated to the aligned model during training. We decompose the\npreference evaluation metric, i.e., win rate, from the perspective of human to\nidentify the deeper factors and conclude that the win rate is affected by two\naxes of model response: desirability and information mass, where the former is\nlength-independent and related to trustworthiness, and the latter is\nlength-dependent and can be represented by conditional entropy. Controlled\nexperiments demonstrate that response length impacts evaluations by influencing\ninformation mass. To ensure reliable evaluation metrics that assess content\nquality without being confounded by response length, we propose AdapAlpaca, a\nsimple yet effective adjustment to win rate measurement. Specifically, by\nadjusting the lengths of reference answers to match the test model's answers\nwithin the same interval, we debias information mass relative to length,\nensuring a fair model evaluation. Furthermore, we investigate length bias in\nDPO using AlpacaEval and AdapAlpaca. By testing Tulu2 and Tulu2-dpo at 7B, 13B,\nand 70B scales, we found that DPO leads to higher human preference, but this\ngain is amplified by response length, with AlpacaEval showing higher win rates\ngain than AdapAlpaca.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}