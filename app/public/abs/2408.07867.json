{"id":"2408.07867","title":"Continuous Perception Benchmark","authors":"Zeyu Wang, Zhenzhen Weng, Serena Yeung-Levy","authorsParsed":[["Wang","Zeyu",""],["Weng","Zhenzhen",""],["Yeung-Levy","Serena",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 00:45:21 GMT"}],"updateDate":"2024-08-16","timestamp":1723682721000,"abstract":"  Humans continuously perceive and process visual signals. However, current\nvideo models typically either sample key frames sparsely or divide videos into\nchunks and densely sample within each chunk. This approach stems from the fact\nthat most existing video benchmarks can be addressed by analyzing key frames or\naggregating information from separate chunks. We anticipate that the next\ngeneration of vision models will emulate human perception by processing visual\ninput continuously and holistically. To facilitate the development of such\nmodels, we propose the Continuous Perception Benchmark, a video question\nanswering task that cannot be solved by focusing solely on a few frames or by\ncaptioning small chunks and then summarizing using language models. Extensive\nexperiments demonstrate that existing models, whether commercial or\nopen-source, struggle with these tasks, indicating the need for new technical\nadvancements in this direction.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}