{"id":"2407.19435","title":"ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon\n  Intention Understanding","authors":"Zhen Chen, Zongming Zhang, Wenwu Guo, Xingjian Luo, Long Bai, Jinlin\n  Wu, Hongliang Ren, Hongbin Liu","authorsParsed":[["Chen","Zhen",""],["Zhang","Zongming",""],["Guo","Wenwu",""],["Luo","Xingjian",""],["Bai","Long",""],["Wu","Jinlin",""],["Ren","Hongliang",""],["Liu","Hongbin",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 09:25:59 GMT"}],"updateDate":"2024-07-30","timestamp":1722158759000,"abstract":"  Surgical instrument segmentation is crucial in surgical scene understanding,\nthereby facilitating surgical safety. Existing algorithms directly detected all\ninstruments of pre-defined categories in the input image, lacking the\ncapability to segment specific instruments according to the surgeon's\nintention. During different stages of surgery, surgeons exhibit varying\npreferences and focus toward different surgical instruments. Therefore, an\ninstrument segmentation algorithm that adheres to the surgeon's intention can\nminimize distractions from irrelevant instruments and assist surgeons to a\ngreat extent. The recent Segment Anything Model (SAM) reveals the capability to\nsegment objects following prompts, but the manual annotations for prompts are\nimpractical during the surgery. To address these limitations in operating\nrooms, we propose an audio-driven surgical instrument segmentation framework,\nnamed ASI-Seg, to accurately segment the required surgical instruments by\nparsing the audio commands of surgeons. Specifically, we propose an\nintention-oriented multimodal fusion to interpret the segmentation intention\nfrom audio commands and retrieve relevant instrument details to facilitate\nsegmentation. Moreover, to guide our ASI-Seg segment of the required surgical\ninstruments, we devise a contrastive learning prompt encoder to effectively\ndistinguish the required instruments from the irrelevant ones. Therefore, our\nASI-Seg promotes the workflow in the operating rooms, thereby providing\ntargeted support and reducing the cognitive load on surgeons. Extensive\nexperiments are performed to validate the ASI-Seg framework, which reveals\nremarkable advantages over classical state-of-the-art and medical SAMs in both\nsemantic segmentation and intention-oriented segmentation. The source code is\navailable at https://github.com/Zonmgin-Zhang/ASI-Seg.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}