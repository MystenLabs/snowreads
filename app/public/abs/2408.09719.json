{"id":"2408.09719","title":"Work-Efficient Parallel Counting via Sampling","authors":"Hongyang Liu, Yitong Yin, Yiyao Zhang","authorsParsed":[["Liu","Hongyang",""],["Yin","Yitong",""],["Zhang","Yiyao",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 06:18:59 GMT"}],"updateDate":"2024-08-20","timestamp":1724048339000,"abstract":"  We study the problem of estimating the partition function $Z(\\beta) = \\sum_{x\n\\in \\Omega} \\exp[-\\beta \\cdot H(x)]$ of a Gibbs distribution defined by a\nHamiltonian $H(\\cdot)$. It is well known that the partition function $Z(\\beta)$\ncan be well approximated by the simulated annealing method, assuming a sampling\noracle that can generate samples according to the Gibbs distribution of any\ngiven inverse temperature $\\beta$. This method yields the most efficient\nreductions from counting to sampling, including:\n  $\\bullet$ classic non-adaptive (parallel) algorithms with sub-optimal cost\n[DFK89; Bez+08];\n  $\\bullet$ adaptive (sequential) algorithms with near-optimal cost [SVV09;\nHub15; Kol18; HK23].\n  In this paper, we give an algorithm that achieves efficiency in both\nparallelism and total work. Specifically, it provides a reduction from counting\nto sampling using near-optimal total work and logarithmic depth of computation.\nConsequently, it gives work-efficient parallel counting algorithms for several\nimportant models, including the hardcore and Ising models in the uniqueness\nregime.\n","subjects":["Computing Research Repository/Data Structures and Algorithms","Computing Research Repository/Computational Complexity","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}