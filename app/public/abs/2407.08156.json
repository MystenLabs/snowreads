{"id":"2407.08156","title":"AddressCLIP: Empowering Vision-Language Models for City-wide Image\n  Address Localization","authors":"Shixiong Xu, Chenghao Zhang, Lubin Fan, Gaofeng Meng, Shiming Xiang,\n  Jieping Ye","authorsParsed":[["Xu","Shixiong",""],["Zhang","Chenghao",""],["Fan","Lubin",""],["Meng","Gaofeng",""],["Xiang","Shiming",""],["Ye","Jieping",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 03:18:53 GMT"}],"updateDate":"2024-07-12","timestamp":1720667933000,"abstract":"  In this study, we introduce a new problem raised by social media and\nphotojournalism, named Image Address Localization (IAL), which aims to predict\nthe readable textual address where an image was taken. Existing two-stage\napproaches involve predicting geographical coordinates and converting them into\nhuman-readable addresses, which can lead to ambiguity and be\nresource-intensive. In contrast, we propose an end-to-end framework named\nAddressCLIP to solve the problem with more semantics, consisting of two key\ningredients: i) image-text alignment to align images with addresses and scene\ncaptions by contrastive learning, and ii) image-geography matching to constrain\nimage features with the spatial distance in terms of manifold learning.\nAdditionally, we have built three datasets from Pittsburgh and San Francisco on\ndifferent scales specifically for the IAL problem. Experiments demonstrate that\nour approach achieves compelling performance on the proposed datasets and\noutperforms representative transfer learning methods for vision-language\nmodels. Furthermore, extensive ablations and visualizations exhibit the\neffectiveness of the proposed method. The datasets and source code are\navailable at https://github.com/xsx1001/AddressCLIP.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}