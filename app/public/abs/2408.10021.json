{"id":"2408.10021","title":"Detecting Adversarial Attacks in Semantic Segmentation via Uncertainty\n  Estimation: A Deep Analysis","authors":"Kira Maag, Roman Resner and Asja Fischer","authorsParsed":[["Maag","Kira",""],["Resner","Roman",""],["Fischer","Asja",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 14:13:30 GMT"}],"updateDate":"2024-08-20","timestamp":1724076810000,"abstract":"  Deep neural networks have demonstrated remarkable effectiveness across a wide\nrange of tasks such as semantic segmentation. Nevertheless, these networks are\nvulnerable to adversarial attacks that add imperceptible perturbations to the\ninput image, leading to false predictions. This vulnerability is particularly\ndangerous in safety-critical applications like automated driving. While\nadversarial examples and defense strategies are well-researched in the context\nof image classification, there is comparatively less research focused on\nsemantic segmentation. Recently, we have proposed an uncertainty-based method\nfor detecting adversarial attacks on neural networks for semantic segmentation.\nWe observed that uncertainty, as measured by the entropy of the output\ndistribution, behaves differently on clean versus adversely perturbed images,\nand we utilize this property to differentiate between the two. In this extended\nversion of our work, we conduct a detailed analysis of uncertainty-based\ndetection of adversarial attacks including a diverse set of adversarial attacks\nand various state-of-the-art neural networks. Our numerical experiments show\nthe effectiveness of the proposed uncertainty-based detection method, which is\nlightweight and operates as a post-processing step, i.e., no model\nmodifications or knowledge of the adversarial example generation process are\nrequired.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}