{"id":"2408.07481","title":"DeCo: Decoupled Human-Centered Diffusion Video Editing with Motion\n  Consistency","authors":"Xiaojing Zhong, Xinyi Huang, Xiaofeng Yang, Guosheng Lin, Qingyao Wu","authorsParsed":[["Zhong","Xiaojing",""],["Huang","Xinyi",""],["Yang","Xiaofeng",""],["Lin","Guosheng",""],["Wu","Qingyao",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 11:53:40 GMT"}],"updateDate":"2024-08-15","timestamp":1723636420000,"abstract":"  Diffusion models usher a new era of video editing, flexibly manipulating the\nvideo contents with text prompts. Despite the widespread application demand in\nediting human-centered videos, these models face significant challenges in\nhandling complex objects like humans. In this paper, we introduce DeCo, a novel\nvideo editing framework specifically designed to treat humans and the\nbackground as separate editable targets, ensuring global spatial-temporal\nconsistency by maintaining the coherence of each individual component.\nSpecifically, we propose a decoupled dynamic human representation that utilizes\na parametric human body prior to generate tailored humans while preserving the\nconsistent motions as the original video. In addition, we consider the\nbackground as a layered atlas to apply text-guided image editing approaches on\nit. To further enhance the geometry and texture of humans during the\noptimization, we extend the calculation of score distillation sampling into\nnormal space and image space. Moreover, we tackle inconsistent lighting between\nthe edited targets by leveraging a lighting-aware video harmonizer, a problem\npreviously overlooked in decompose-edit-combine approaches. Extensive\nqualitative and numerical experiments demonstrate that DeCo outperforms prior\nvideo editing methods in human-centered videos, especially in longer videos.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}