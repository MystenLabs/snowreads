{"id":"2408.11294","title":"RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining","authors":"Anh-Dung Vo, Minseong Jung, Wonbeen Lee, Daewoo Choi","authorsParsed":[["Vo","Anh-Dung",""],["Jung","Minseong",""],["Lee","Wonbeen",""],["Choi","Daewoo",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 02:49:41 GMT"}],"updateDate":"2024-08-22","timestamp":1724208581000,"abstract":"  The field of Natural Language Processing (NLP) has seen significant\nadvancements with the development of Large Language Models (LLMs). However,\nmuch of this research remains focused on English, often overlooking\nlow-resource languages like Korean. This oversight presents challenges due to\nthe unique non-alphabetic token structure of Korean and the substantial memory\nand computational demands required for LLM training, which frequently lead to\nmemory constraints and out-of-memory errors. To address these issues, we\npresent RedWhale, a model specifically tailored for Korean language processing.\nRedWhale is developed using an efficient continual pretraining approach that\nincludes a comprehensive Korean corpus preprocessing pipeline, a specialized\ntokenizer, an optimized model initialization technique, and a multistage\npretraining strategy. These innovations collectively reduce training time and\ncomputational costs while maintaining high levels of accuracy and\ncomprehension. By leveraging cross-lingual transfer learning, which exploits\nshared linguistic similarities across languages, RedWhale builds on English\nmodels to enhance Korean language processing. Experimental results demonstrate\nthat RedWhale outperforms other leading models on Korean NLP benchmarks,\nincluding the Korean Balanced Evaluation of Significant Tasks (KoBEST), showing\nsuperior understanding and generation of Korean text. Furthermore, RedWhale\nshowed no signs of convergence even after pretraining on 9.7 billion tokens,\nindicating the potential for further improvements with additional training.\nThis work represents a significant advancement in bridging the linguistic\ndivide, particularly in enhancing NLP capabilities for the Korean language.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2aMSuE_MUTR19Zo3URCpNPefEmH_AvRu1SLqqCrBrss","pdfSize":"8007523"}
