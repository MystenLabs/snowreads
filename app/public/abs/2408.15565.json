{"id":"2408.15565","title":"SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large\n  Language Models","authors":"Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, Dong Yu","authorsParsed":[["Yu","Dian",""],["Peng","Baolin",""],["Tian","Ye",""],["Song","Linfeng",""],["Mi","Haitao",""],["Yu","Dong",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 06:33:03 GMT"}],"updateDate":"2024-08-29","timestamp":1724826783000,"abstract":"  There is a growing trend of teaching large language models (LLMs) to solve\nmathematical problems through coding. Existing studies primarily focus on\nprompting powerful, closed-source models to generate seed training data\nfollowed by in-domain data augmentation, equipping LLMs with considerable\ncapabilities for code-aided mathematical reasoning. However, continually\ntraining these models on augmented data derived from a few datasets such as\nGSM8K may impair their generalization abilities and restrict their\neffectiveness to a narrow range of question types. Conversely, the potential of\nimproving such LLMs by leveraging large-scale, expert-written, diverse math\nquestion-answer pairs remains unexplored. To utilize these resources and tackle\nunique challenges such as code response assessment, we propose a novel paradigm\nthat uses a code-based critic model to guide steps including question-code data\nconstruction, quality control, and complementary evaluation. We also explore\ndifferent alignment algorithms with self-generated instruction/preference data\nto foster continuous improvement. Experiments across both in-domain (up to\n+5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate\nthe effectiveness of the proposed paradigm.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"X2k09iaUDMrbq0ZslsslQEmeZR6dZWxXs6JJcoiA_hE","pdfSize":"1567831"}
