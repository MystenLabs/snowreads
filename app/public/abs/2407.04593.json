{"id":"2407.04593","title":"Testing learning hypotheses using neural networks by manipulating\n  learning data","authors":"Cara Su-Yi Leong and Tal Linzen","authorsParsed":[["Leong","Cara Su-Yi",""],["Linzen","Tal",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 15:41:30 GMT"}],"updateDate":"2024-07-08","timestamp":1720194090000,"abstract":"  Although passivization is productive in English, it is not completely general\n-- some exceptions exist (e.g. *One hour was lasted by the meeting). How do\nEnglish speakers learn these exceptions to an otherwise general pattern? Using\nneural network language models as theories of acquisition, we explore the\nsources of indirect evidence that a learner can leverage to learn whether a\nverb can passivize. We first characterize English speakers' judgments of\nexceptions to the passive, confirming that speakers find some verbs more\npassivizable than others. We then show that a neural network language model can\nlearn restrictions to the passive that are similar to those displayed by\nhumans, suggesting that evidence for these exceptions is available in the\nlinguistic input. We test the causal role of two hypotheses for how the\nlanguage model learns these restrictions by training models on modified\ntraining corpora, which we create by altering the existing training corpora to\nremove features of the input implicated by each hypothesis. We find that while\nthe frequency with which a verb appears in the passive significantly affects\nits passivizability, the semantics of the verb does not. This study highlight\nthe utility of altering a language model's training data for answering\nquestions where complete control over a learner's input is vital.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}