{"id":"2408.07500","title":"Cross-Platform Video Person ReID: A New Benchmark Dataset and Adaptation\n  Approach","authors":"Shizhou Zhang, Wenlong Luo, De Cheng, Qingchun Yang, Lingyan Ran,\n  Yinghui Xing, and Yanning Zhang","authorsParsed":[["Zhang","Shizhou",""],["Luo","Wenlong",""],["Cheng","De",""],["Yang","Qingchun",""],["Ran","Lingyan",""],["Xing","Yinghui",""],["Zhang","Yanning",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 12:29:49 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 02:50:56 GMT"}],"updateDate":"2024-09-04","timestamp":1723638589000,"abstract":"  In this paper, we construct a large-scale benchmark dataset for\nGround-to-Aerial Video-based person Re-Identification, named G2A-VReID, which\ncomprises 185,907 images and 5,576 tracklets, featuring 2,788 distinct\nidentities. To our knowledge, this is the first dataset for video ReID under\nGround-to-Aerial scenarios. G2A-VReID dataset has the following\ncharacteristics: 1) Drastic view changes; 2) Large number of annotated\nidentities; 3) Rich outdoor scenarios; 4) Huge difference in resolution.\nAdditionally, we propose a new benchmark approach for cross-platform ReID by\ntransforming the cross-platform visual alignment problem into visual-semantic\nalignment through vision-language model (i.e., CLIP) and applying a\nparameter-efficient Video Set-Level-Adapter module to adapt image-based\nfoundation model to video ReID tasks, termed VSLA-CLIP. Besides, to further\nreduce the great discrepancy across the platforms, we also devise the\nplatform-bridge prompts for efficient visual feature alignment. Extensive\nexperiments demonstrate the superiority of the proposed method on all existing\nvideo ReID datasets and our proposed G2A-VReID dataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XsxNZtpHvIU5vngJpfhNOnAofvlMYFBBVteDinWv1Ec","pdfSize":"1050382"}
