{"id":"2407.03106","title":"Anti-Collapse Loss for Deep Metric Learning Based on Coding Rate Metric","authors":"Xiruo Jiang, Yazhou Yao, Xili Dai, Fumin Shen, Xian-Sheng Hua,\n  Heng-Tao Shen","authorsParsed":[["Jiang","Xiruo",""],["Yao","Yazhou",""],["Dai","Xili",""],["Shen","Fumin",""],["Hua","Xian-Sheng",""],["Shen","Heng-Tao",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 13:44:20 GMT"}],"updateDate":"2024-07-04","timestamp":1720014260000,"abstract":"  Deep metric learning (DML) aims to learn a discriminative high-dimensional\nembedding space for downstream tasks like classification, clustering, and\nretrieval. Prior literature predominantly focuses on pair-based and proxy-based\nmethods to maximize inter-class discrepancy and minimize intra-class diversity.\nHowever, these methods tend to suffer from the collapse of the embedding space\ndue to their over-reliance on label information. This leads to sub-optimal\nfeature representation and inferior model performance. To maintain the\nstructure of embedding space and avoid feature collapse, we propose a novel\nloss function called Anti-Collapse Loss. Specifically, our proposed loss\nprimarily draws inspiration from the principle of Maximal Coding Rate\nReduction. It promotes the sparseness of feature clusters in the embedding\nspace to prevent collapse by maximizing the average coding rate of sample\nfeatures or class proxies. Moreover, we integrate our proposed loss with\npair-based and proxy-based methods, resulting in notable performance\nimprovement. Comprehensive experiments on benchmark datasets demonstrate that\nour proposed method outperforms existing state-of-the-art methods. Extensive\nablation studies verify the effectiveness of our method in preventing embedding\nspace collapse and promoting generalization performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}