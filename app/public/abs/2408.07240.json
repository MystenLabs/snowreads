{"id":"2408.07240","title":"Sensitivity of MCMC-based analyses to small-data removal","authors":"Tin D. Nguyen, Ryan Giordano, Rachael Meager, Tamara Broderick","authorsParsed":[["Nguyen","Tin D.",""],["Giordano","Ryan",""],["Meager","Rachael",""],["Broderick","Tamara",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 00:20:50 GMT"}],"updateDate":"2024-08-15","timestamp":1723594850000,"abstract":"  If the conclusion of a data analysis is sensitive to dropping very few data\npoints, that conclusion might hinge on the particular data at hand rather than\nrepresenting a more broadly applicable truth. How could we check whether this\nsensitivity holds? One idea is to consider every small subset of data, drop it\nfrom the dataset, and re-run our analysis. But running MCMC to approximate a\nBayesian posterior is already very expensive; running multiple times is\nprohibitive, and the number of re-runs needed here is combinatorially large.\nRecent work proposes a fast and accurate approximation to find the worst-case\ndropped data subset, but that work was developed for problems based on\nestimating equations -- and does not directly handle Bayesian posterior\napproximations using MCMC. We make two principal contributions in the present\nwork. We adapt the existing data-dropping approximation to estimators computed\nvia MCMC. Observing that Monte Carlo errors induce variability in the\napproximation, we use a variant of the bootstrap to quantify this uncertainty.\nWe demonstrate how to use our approximation in practice to determine whether\nthere is non-robustness in a problem. Empirically, our method is accurate in\nsimple models, such as linear regression. In models with complicated structure,\nsuch as hierarchical models, the performance of our method is mixed.\n","subjects":["Statistics/Methodology","Statistics/Computation"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-Swib_A_fObY158KaRXXbg-4yGkiZaJEeL6dReySgIY","pdfSize":"1429319"}
