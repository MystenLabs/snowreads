{"id":"2407.15176","title":"Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope","authors":"Xiaoran Liu, Qipeng Guo, Yuerong Song, Zhigeng Liu, Kai Lv, Hang Yan,\n  Linlin Li, Qun Liu, Xipeng Qiu","authorsParsed":[["Liu","Xiaoran",""],["Guo","Qipeng",""],["Song","Yuerong",""],["Liu","Zhigeng",""],["Lv","Kai",""],["Yan","Hang",""],["Li","Linlin",""],["Liu","Qun",""],["Qiu","Xipeng",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 14:23:37 GMT"}],"updateDate":"2024-07-23","timestamp":1721571817000,"abstract":"  The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}