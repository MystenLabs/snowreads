{"id":"2408.04267","title":"Distil-DCCRN: A Small-footprint DCCRN Leveraging Feature-based Knowledge\n  Distillation in Speech Enhancement","authors":"Runduo Han, Weiming Xu, Zihan Zhang, Mingshuai Liu, Lei Xie","authorsParsed":[["Han","Runduo",""],["Xu","Weiming",""],["Zhang","Zihan",""],["Liu","Mingshuai",""],["Xie","Lei",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 07:10:23 GMT"}],"updateDate":"2024-08-09","timestamp":1723101023000,"abstract":"  The deep complex convolution recurrent network (DCCRN) achieves excellent\nspeech enhancement performance by utilizing the audio spectrum's complex\nfeatures. However, it has a large number of model parameters. We propose a\nsmaller model, Distil-DCCRN, which has only 30% of the parameters compared to\nthe DCCRN. To ensure that the performance of Distil-DCCRN matches that of the\nDCCRN, we employ the knowledge distillation (KD) method to use a larger teacher\nmodel to help train a smaller student model. We design a knowledge distillation\n(KD) method, integrating attention transfer and Kullback-Leibler divergence\n(AT-KL) to train the student model Distil-DCCRN. Additionally, we use a model\nwith better performance and a more complicated structure, Uformer, as the\nteacher model. Unlike previous KD approaches that mainly focus on model\noutputs, our method also leverages the intermediate features from the models'\nmiddle layers, facilitating rich knowledge transfer across different structured\nmodels despite variations in layer configurations and discrepancies in the\nchannel and time dimensions of intermediate features. Employing our AT-KL\napproach, Distil-DCCRN outperforms DCCRN as well as several other competitive\nmodels in both PESQ and SI-SNR metrics on the DNS test set and achieves\ncomparable results to DCCRN in DNSMOS.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}