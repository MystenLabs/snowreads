{"id":"2408.07445","title":"Modality Invariant Multimodal Learning to Handle Missing Modalities: A\n  Single-Branch Approach","authors":"Muhammad Saad Saeed, Shah Nawaz, Muhammad Zaigham Zaheer, Muhammad\n  Haris Khan, Karthik Nandakumar, Muhammad Haroon Yousaf, Hassan Sajjad, Tom De\n  Schepper, Markus Schedl","authorsParsed":[["Saeed","Muhammad Saad",""],["Nawaz","Shah",""],["Zaheer","Muhammad Zaigham",""],["Khan","Muhammad Haris",""],["Nandakumar","Karthik",""],["Yousaf","Muhammad Haroon",""],["Sajjad","Hassan",""],["De Schepper","Tom",""],["Schedl","Markus",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 10:32:16 GMT"}],"updateDate":"2024-08-15","timestamp":1723631536000,"abstract":"  Multimodal networks have demonstrated remarkable performance improvements\nover their unimodal counterparts. Existing multimodal networks are designed in\na multi-branch fashion that, due to the reliance on fusion strategies, exhibit\ndeteriorated performance if one or more modalities are missing. In this work,\nwe propose a modality invariant multimodal learning method, which is less\nsusceptible to the impact of missing modalities. It consists of a single-branch\nnetwork sharing weights across multiple modalities to learn inter-modality\nrepresentations to maximize performance as well as robustness to missing\nmodalities. Extensive experiments are performed on four challenging datasets\nincluding textual-visual (UPMC Food-101, Hateful Memes, Ferramenta) and\naudio-visual modalities (VoxCeleb1). Our proposed method achieves superior\nperformance when all modalities are present as well as in the case of missing\nmodalities during training or testing compared to the existing state-of-the-art\nmethods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}