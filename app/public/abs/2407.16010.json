{"id":"2407.16010","title":"AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations","authors":"Ikhtiyor Nematov, Dimitris Sacharidis, Tomer Sagi, Katja Hose","authorsParsed":[["Nematov","Ikhtiyor",""],["Sacharidis","Dimitris",""],["Sagi","Tomer",""],["Hose","Katja",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 19:33:12 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 09:12:13 GMT"}],"updateDate":"2024-08-09","timestamp":1721676792000,"abstract":"  For many use-cases, it is often important to explain the prediction of a\nblack-box model by identifying the most influential training data samples.\nExisting approaches lack customization for user intent and often provide a\nhomogeneous set of explanation samples, failing to reveal the model's reasoning\nfrom different angles.\n  In this paper, we propose AIDE, an approach for providing antithetical (i.e.,\ncontrastive), intent-based, diverse explanations for opaque and complex models.\nAIDE distinguishes three types of explainability intents: interpreting a\ncorrect, investigating a wrong, and clarifying an ambiguous prediction. For\neach intent, AIDE selects an appropriate set of influential training samples\nthat support or oppose the prediction either directly or by contrast. To\nprovide a succinct summary, AIDE uses diversity-aware sampling to avoid\nredundancy and increase coverage of the training data.\n  We demonstrate the effectiveness of AIDE on image and text classification\ntasks, in three ways: quantitatively, assessing correctness and continuity;\nqualitatively, comparing anecdotal evidence from AIDE and other example-based\napproaches; and via a user study, evaluating multiple aspects of AIDE. The\nresults show that AIDE addresses the limitations of existing methods and\nexhibits desirable traits for an explainability method.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}