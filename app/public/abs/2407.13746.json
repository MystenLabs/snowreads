{"id":"2407.13746","title":"Multi-Label Learning with Stronger Consistency Guarantees","authors":"Anqi Mao, Mehryar Mohri, Yutao Zhong","authorsParsed":[["Mao","Anqi",""],["Mohri","Mehryar",""],["Zhong","Yutao",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:51:02 GMT"}],"updateDate":"2024-07-19","timestamp":1721325062000,"abstract":"  We present a detailed study of surrogate losses and algorithms for\nmulti-label learning, supported by $H$-consistency bounds. We first show that,\nfor the simplest form of multi-label loss (the popular Hamming loss), the\nwell-known consistent binary relevance surrogate suffers from a sub-optimal\ndependency on the number of labels in terms of $H$-consistency bounds, when\nusing smooth losses such as logistic losses. Furthermore, this loss function\nfails to account for label correlations. To address these drawbacks, we\nintroduce a novel surrogate loss, multi-label logistic loss, that accounts for\nlabel correlations and benefits from label-independent $H$-consistency bounds.\nWe then broaden our analysis to cover a more extensive family of multi-label\nlosses, including all common ones and a new extension defined based on\nlinear-fractional functions with respect to the confusion matrix. We also\nextend our multi-label logistic losses to more comprehensive multi-label\ncomp-sum losses, adapting comp-sum losses from standard classification to the\nmulti-label learning. We prove that this family of surrogate losses benefits\nfrom $H$-consistency bounds, and thus Bayes-consistency, across any general\nmulti-label loss. Our work thus proposes a unified surrogate loss framework\nbenefiting from strong consistency guarantees for any multi-label loss,\nsignificantly expanding upon previous work which only established\nBayes-consistency and for specific loss functions. Additionally, we adapt\nconstrained losses from standard classification to multi-label constrained\nlosses in a similar way, which also benefit from $H$-consistency bounds and\nthus Bayes-consistency for any multi-label loss. We further describe efficient\ngradient computation algorithms for minimizing the multi-label logistic loss.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}