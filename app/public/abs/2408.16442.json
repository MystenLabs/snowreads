{"id":"2408.16442","title":"Integrating Features for Recognizing Human Activities through Optimized\n  Parameters in Graph Convolutional Networks and Transformer Architectures","authors":"Mohammad Belal (1), Taimur Hassan (2), Abdelfatah Hassan (1), Nael\n  Alsheikh (1), Noureldin Elhendawi (1), Irfan Hussain (1) ((1) Khalifa\n  University of Science and Technology, Abu Dhabi, United Arab Emirates, (2)\n  Abu Dhabi University, Abu Dhabi, United Arab Emirates)","authorsParsed":[["Belal","Mohammad",""],["Hassan","Taimur",""],["Hassan","Abdelfatah",""],["Alsheikh","Nael",""],["Elhendawi","Noureldin",""],["Hussain","Irfan",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 11:07:48 GMT"}],"updateDate":"2024-08-30","timestamp":1724929668000,"abstract":"  Human activity recognition is a major field of study that employs computer\nvision, machine vision, and deep learning techniques to categorize human\nactions. The field of deep learning has made significant progress, with\narchitectures that are extremely effective at capturing human dynamics. This\nstudy emphasizes the influence of feature fusion on the accuracy of activity\nrecognition. This technique addresses the limitation of conventional models,\nwhich face difficulties in identifying activities because of their limited\ncapacity to understand spatial and temporal features. The technique employs\nsensory data obtained from four publicly available datasets: HuGaDB, PKU-MMD,\nLARa, and TUG. The accuracy and F1-score of two deep learning models,\nspecifically a Transformer model and a Parameter-Optimized Graph Convolutional\nNetwork (PO-GCN), were evaluated using these datasets. The feature fusion\ntechnique integrated the final layer features from both models and inputted\nthem into a classifier. Empirical evidence demonstrates that PO-GCN outperforms\nstandard models in activity recognition. HuGaDB demonstrated a 2.3% improvement\nin accuracy and a 2.2% increase in F1-score. TUG showed a 5% increase in\naccuracy and a 0.5% rise in F1-score. On the other hand, LARa and PKU-MMD\nachieved lower accuracies of 64% and 69% respectively. This indicates that the\nintegration of features enhanced the performance of both the Transformer model\nand PO-GCN.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}