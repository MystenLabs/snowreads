{"id":"2408.07877","title":"IReCa: Intrinsic Reward-enhanced Context-aware Reinforcement Learning\n  for Human-AI Coordination","authors":"Xin Hao, Bahareh Nakisa, Mohmmad Naim Rastgoo, Richard Dazeley","authorsParsed":[["Hao","Xin",""],["Nakisa","Bahareh",""],["Rastgoo","Mohmmad Naim",""],["Dazeley","Richard",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 01:33:06 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 22:55:03 GMT"}],"updateDate":"2024-08-29","timestamp":1723685586000,"abstract":"  In human-AI coordination scenarios, human agents usually exhibit asymmetric\nbehaviors that are significantly sparse and unpredictable compared to those of\nAI agents. These characteristics introduce two primary challenges to human-AI\ncoordination: the effectiveness of obtaining sparse rewards and the efficiency\nof training the AI agents. To tackle these challenges, we propose an Intrinsic\nReward-enhanced Context-aware (IReCa) reinforcement learning (RL) algorithm,\nwhich leverages intrinsic rewards to facilitate the acquisition of sparse\nrewards and utilizes environmental context to enhance training efficiency. Our\nIReCa RL algorithm introduces three unique features: (i) it encourages the\nexploration of sparse rewards by incorporating intrinsic rewards that\nsupplement traditional extrinsic rewards from the environment; (ii) it improves\nthe acquisition of sparse rewards by prioritizing the corresponding sparse\nstate-action pairs; and (iii) it enhances the training efficiency by optimizing\nthe exploration and exploitation through innovative context-aware weights of\nextrinsic and intrinsic rewards. Extensive simulations executed in the\nOvercooked layouts demonstrate that our IReCa RL algorithm can increase the\naccumulated rewards by approximately 20% and reduce the epochs required for\nconvergence by approximately 67% compared to state-of-the-art baselines.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}