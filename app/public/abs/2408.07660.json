{"id":"2408.07660","title":"Off-Policy Reinforcement Learning with High Dimensional Reward","authors":"Dong Neuck Lee and Michael R. Kosorok","authorsParsed":[["Lee","Dong Neuck",""],["Kosorok","Michael R.",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 16:44:56 GMT"}],"updateDate":"2024-08-15","timestamp":1723653896000,"abstract":"  Conventional off-policy reinforcement learning (RL) focuses on maximizing the\nexpected return of scalar rewards. Distributional RL (DRL), in contrast,\nstudies the distribution of returns with the distributional Bellman operator in\na Euclidean space, leading to highly flexible choices for utility. This paper\nestablishes robust theoretical foundations for DRL. We prove the contraction\nproperty of the Bellman operator even when the reward space is an\ninfinite-dimensional separable Banach space. Furthermore, we demonstrate that\nthe behavior of high- or infinite-dimensional returns can be effectively\napproximated using a lower-dimensional Euclidean space. Leveraging these\ntheoretical insights, we propose a novel DRL algorithm that tackles problems\nwhich have been previously intractable using conventional reinforcement\nlearning approaches.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}