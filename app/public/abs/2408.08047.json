{"id":"2408.08047","title":"An Efficient Continuous Control Perspective for\n  Reinforcement-Learning-based Sequential Recommendation","authors":"Jun Wang, Likang Wu, Qi Liu, Yu Yang","authorsParsed":[["Wang","Jun",""],["Wu","Likang",""],["Liu","Qi",""],["Yang","Yu",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 09:26:26 GMT"}],"updateDate":"2024-08-16","timestamp":1723713986000,"abstract":"  Sequential recommendation, where user preference is dynamically inferred from\nsequential historical behaviors, is a critical task in recommender systems\n(RSs). To further optimize long-term user engagement, offline\nreinforcement-learning-based RSs have become a mainstream technique as they\nprovide an additional advantage in avoiding global explorations that may harm\nonline users' experiences. However, previous studies mainly focus on discrete\naction and policy spaces, which might have difficulties in handling\ndramatically growing items efficiently.\n  To mitigate this issue, in this paper, we aim to design an algorithmic\nframework applicable to continuous policies. To facilitate the control in the\nlow-dimensional but dense user preference space, we propose an\n\\underline{\\textbf{E}}fficient \\underline{\\textbf{Co}}ntinuous\n\\underline{\\textbf{C}}ontrol framework (ECoC). Based on a statistically tested\nassumption, we first propose the novel unified action representation abstracted\nfrom normalized user and item spaces. Then, we develop the corresponding policy\nevaluation and policy improvement procedures. During this process, strategic\nexploration and directional control in terms of unified actions are carefully\ndesigned and crucial to final recommendation decisions. Moreover, beneficial\nfrom unified actions, the conservatism regularization for policies and value\nfunctions are combined and perfectly compatible with the continuous framework.\nThe resulting dual regularization ensures the successful offline training of\nRL-based recommendation policies. Finally, we conduct extensive experiments to\nvalidate the effectiveness of our framework. The results show that compared to\nthe discrete baselines, our ECoC is trained far more efficiently. Meanwhile,\nthe final policies outperform baselines in both capturing the offline data and\ngaining long-term rewards.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}