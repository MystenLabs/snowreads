{"id":"2407.10734","title":"On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M\n  Microcontrollers","authors":"Mark Deutel, Frank Hannig, Christopher Mutschler, and J\\\"urgen Teich","authorsParsed":[["Deutel","Mark",""],["Hannig","Frank",""],["Mutschler","Christopher",""],["Teich","JÃ¼rgen",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 14:01:34 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 15:36:08 GMT"}],"updateDate":"2024-08-29","timestamp":1721052094000,"abstract":"  On-device training of DNNs allows models to adapt and fine-tune to newly\ncollected data or changing domains while deployed on microcontroller units\n(MCUs). However, DNN training is a resource-intensive task, making the\nimplementation and execution of DNN training algorithms on MCUs challenging due\nto low processor speeds, constrained throughput, limited floating-point\nsupport, and memory constraints. In this work, we explore on-device training of\nDNNs for Cortex-M MCUs. We present a method that enables efficient training of\nDNNs completely in place on the MCU using fully quantized training (FQT) and\ndynamic partial gradient updates. We demonstrate the feasibility of our\napproach on multiple vision and time-series datasets and provide insights into\nthe tradeoff between training accuracy, memory overhead, energy, and latency on\nreal hardware.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}