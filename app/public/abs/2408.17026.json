{"id":"2408.17026","title":"From Text to Emotion: Unveiling the Emotion Annotation Capabilities of\n  LLMs","authors":"Minxue Niu (1), Mimansa Jaiswal (2), Emily Mower Provost (1) ((1)\n  University of Michigan, (2) Independent Researcher)","authorsParsed":[["Niu","Minxue",""],["Jaiswal","Mimansa",""],["Provost","Emily Mower",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 05:50:15 GMT"}],"updateDate":"2024-09-02","timestamp":1724997015000,"abstract":"  Training emotion recognition models has relied heavily on human annotated\ndata, which present diversity, quality, and cost challenges. In this paper, we\nexplore the potential of Large Language Models (LLMs), specifically GPT4, in\nautomating or assisting emotion annotation. We compare GPT4 with supervised\nmodels and or humans in three aspects: agreement with human annotations,\nalignment with human perception, and impact on model training. We find that\ncommon metrics that use aggregated human annotations as ground truth can\nunderestimate the performance, of GPT-4 and our human evaluation experiment\nreveals a consistent preference for GPT-4 annotations over humans across\nmultiple datasets and evaluators. Further, we investigate the impact of using\nGPT-4 as an annotation filtering process to improve model training. Together,\nour findings highlight the great potential of LLMs in emotion annotation tasks\nand underscore the need for refined evaluation methodologies.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}