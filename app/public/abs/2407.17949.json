{"id":"2407.17949","title":"Fast convergence of the Expectation Maximization algorithm under a\n  logarithmic Sobolev inequality","authors":"Rocco Caprio, Adam M Johansen","authorsParsed":[["Caprio","Rocco",""],["Johansen","Adam M",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 11:08:53 GMT"}],"updateDate":"2024-07-26","timestamp":1721905733000,"abstract":"  By utilizing recently developed tools for constructing gradient flows on\nWasserstein spaces, we extend an analysis technique commonly employed to\nunderstand alternating minimization algorithms on Euclidean space to the\nExpectation Maximization (EM) algorithm via its representation as\ncoordinate-wise minimization on the product of a Euclidean space and a space of\nprobability distributions due to Neal and Hinton (1998). In so doing we obtain\nfinite sample error bounds and exponential convergence of the EM algorithm\nunder a natural generalisation of a log-Sobolev inequality. We further\ndemonstrate that the analysis technique is sufficiently flexible to allow also\nthe analysis of several variants of the EM algorithm.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Mathematics/Statistics Theory","Statistics/Computation","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RhYLckHI-i6EY5J7oykgMADu9pX9mimbW0wWfDT8A10","pdfSize":"361430"}
