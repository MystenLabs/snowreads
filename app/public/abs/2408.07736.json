{"id":"2408.07736","title":"Enhancing Model Interpretability with Local Attribution over Global\n  Exploration","authors":"Zhiyu Zhu, Zhibo Jin, Jiayu Zhang, Huaming Chen","authorsParsed":[["Zhu","Zhiyu",""],["Jin","Zhibo",""],["Zhang","Jiayu",""],["Chen","Huaming",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 17:53:08 GMT"}],"updateDate":"2024-08-16","timestamp":1723657988000,"abstract":"  In the field of artificial intelligence, AI models are frequently described\nas `black boxes' due to the obscurity of their internal mechanisms. It has\nignited research interest on model interpretability, especially in attribution\nmethods that offers precise explanations of model decisions. Current\nattribution algorithms typically evaluate the importance of each parameter by\nexploring the sample space. A large number of intermediate states are\nintroduced during the exploration process, which may reach the model's\nOut-of-Distribution (OOD) space. Such intermediate states will impact the\nattribution results, making it challenging to grasp the relative importance of\nfeatures. In this paper, we firstly define the local space and its relevant\nproperties, and we propose the Local Attribution (LA) algorithm that leverages\nthese properties. The LA algorithm comprises both targeted and untargeted\nexploration phases, which are designed to effectively generate intermediate\nstates for attribution that thoroughly encompass the local space. Compared to\nthe state-of-the-art attribution methods, our approach achieves an average\nimprovement of 38.21\\% in attribution effectiveness. Extensive ablation studies\nin our experiments also validate the significance of each component in our\nalgorithm. Our code is available at: https://github.com/LMBTough/LA/\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}