{"id":"2408.06945","title":"Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation","authors":"Yanjie Dong, Haijun Zhang, Gang Wang, Shisheng Cui, Xiping Hu","authorsParsed":[["Dong","Yanjie",""],["Zhang","Haijun",""],["Wang","Gang",""],["Cui","Shisheng",""],["Hu","Xiping",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 15:03:46 GMT"},{"version":"v2","created":"Fri, 16 Aug 2024 15:09:09 GMT"}],"updateDate":"2024-08-19","timestamp":1723561426000,"abstract":"  By using an parametric value function to replace the Monte-Carlo rollouts for\nvalue estimation, the actor-critic (AC) algorithms can reduce the variance of\nstochastic policy gradient so that to improve the convergence rate. While\nexisting works mainly focus on analyzing convergence rate of AC algorithms\nunder Markovian noise, the impacts of momentum on AC algorithms remain largely\nunexplored. In this work, we first propose a heavy-ball momentum based\nadvantage actor-critic (\\mbox{HB-A2C}) algorithm by integrating the heavy-ball\nmomentum into the critic recursion that is parameterized by a linear function.\nWhen the sample trajectory follows a Markov decision process, we quantitatively\ncertify the acceleration capability of the proposed HB-A2C algorithm. Our\ntheoretical results demonstrate that the proposed HB-A2C finds an\n$\\epsilon$-approximate stationary point with $\\oo{\\epsilon^{-2}}$ iterations\nfor reinforcement learning tasks with Markovian noise. Moreover, we also reveal\nthe dependence of learning rates on the length of the sample trajectory. By\ncarefully selecting the momentum factor of the critic recursion, the proposed\nHB-A2C can balance the errors introduced by the initialization and the\nstoschastic approximation.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}