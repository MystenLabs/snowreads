{"id":"2408.16028","title":"ANVIL: Anomaly-based Vulnerability Identification without Labelled\n  Training Data","authors":"Weizhou Wang, Eric Liu, Xiangyu Guo and David Lie","authorsParsed":[["Wang","Weizhou",""],["Liu","Eric",""],["Guo","Xiangyu",""],["Lie","David",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 03:28:17 GMT"}],"updateDate":"2024-08-30","timestamp":1724815697000,"abstract":"  Supervised learning-based software vulnerability detectors often fall short\ndue to the inadequate availability of labelled training data. In contrast,\nLarge Language Models (LLMs) such as GPT-4, are not trained on labelled data,\nbut when prompted to detect vulnerabilities, LLM prediction accuracy is only\nmarginally better than random guessing. In this paper, we explore a different\napproach by reframing vulnerability detection as one of anomaly detection.\nSince the vast majority of code does not contain vulnerabilities and LLMs are\ntrained on massive amounts of such code, vulnerable code can be viewed as an\nanomaly from the LLM's predicted code distribution, freeing the model from the\nneed for labelled data to provide a learnable representation of vulnerable\ncode. Leveraging this perspective, we demonstrate that LLMs trained for code\ngeneration exhibit a significant gap in prediction accuracy when prompted to\nreconstruct vulnerable versus non-vulnerable code.\n  Using this insight, we implement ANVIL, a detector that identifies software\nvulnerabilities at line-level granularity. Our experiments explore the\ndiscriminating power of different anomaly scoring methods, as well as the\nsensitivity of ANVIL to context size. We also study the effectiveness of ANVIL\non various LLM families, and conduct leakage experiments on vulnerabilities\nthat were discovered after the knowledge cutoff of our evaluated LLMs. On a\ncollection of vulnerabilities from the Magma benchmark, ANVIL outperforms\nstate-of-the-art line-level vulnerability detectors, LineVul and LineVD, which\nhave been trained with labelled data, despite ANVIL having never been trained\nwith labelled vulnerabilities. Specifically, our approach achieves $1.62\\times$\nto $2.18\\times$ better Top-5 accuracies and $1.02\\times$ to $1.29\\times$ times\nbetter ROC scores on line-level vulnerability detection tasks.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning","Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/"}