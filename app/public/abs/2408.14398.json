{"id":"2408.14398","title":"Language-specific Calibration for Pruning Multilingual Language Models","authors":"Simon Kurz, Jian-Jia Chen, Lucie Flek, Zhixue Zhao","authorsParsed":[["Kurz","Simon",""],["Chen","Jian-Jia",""],["Flek","Lucie",""],["Zhao","Zhixue",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 16:29:13 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 12:03:54 GMT"}],"updateDate":"2024-08-29","timestamp":1724689753000,"abstract":"  Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OoK7M0AceV4E40UzjBWXDA6dibHofPkh3bsRemza0xA","pdfSize":"1238404"}
