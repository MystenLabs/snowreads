{"id":"2407.13089","title":"MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for\n  Fact-Checking","authors":"Ting-Chih Chen, Chia-Wei Tang, Chris Thomas","authorsParsed":[["Chen","Ting-Chih",""],["Tang","Chia-Wei",""],["Thomas","Chris",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 01:33:20 GMT"}],"updateDate":"2024-07-19","timestamp":1721266400000,"abstract":"  Fact-checking real-world claims often requires reviewing multiple multimodal\ndocuments to assess a claim's truthfulness, which is a highly laborious and\ntime-consuming task. In this paper, we present a summarization model designed\nto generate claim-specific summaries useful for fact-checking from multimodal,\nmulti-document datasets. The model takes inputs in the form of documents,\nimages, and a claim, with the objective of assisting in fact-checking tasks. We\nintroduce a dynamic perceiver-based model that can handle inputs from multiple\nmodalities of arbitrary lengths. To train our model, we leverage a novel\nreinforcement learning-based entailment objective to generate summaries that\nprovide evidence distinguishing between different truthfulness labels. To\nassess the efficacy of our approach, we conduct experiments on both an existing\nbenchmark and a new dataset of multi-document claims that we contribute. Our\napproach outperforms the SOTA approach by 4.6% in the claim verification task\non the MOCHEG dataset and demonstrates strong performance on our new\nMulti-News-Fact-Checking dataset.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}