{"id":"2407.00176","title":"The impact of model size on catastrophic forgetting in Online Continual\n  Learning","authors":"Eunhae Lee","authorsParsed":[["Lee","Eunhae",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 18:29:51 GMT"}],"updateDate":"2024-07-02","timestamp":1719599391000,"abstract":"  This study investigates the impact of model size on Online Continual Learning\nperformance, with a focus on catastrophic forgetting. Employing ResNet\narchitectures of varying sizes, the research examines how network depth and\nwidth affect model performance in class-incremental learning using the\nSplitCIFAR-10 dataset. Key findings reveal that larger models do not guarantee\nbetter Continual Learning performance; in fact, they often struggle more in\nadapting to new tasks, particularly in online settings. These results challenge\nthe notion that larger models inherently mitigate catastrophic forgetting,\nhighlighting the nuanced relationship between model size and Continual Learning\nefficacy. This study contributes to a deeper understanding of model scalability\nand its practical implications in Continual Learning scenarios.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_63Wv-RZiTd19ZqBX5kLOKNICfOpVmZoldsZ2CXDfFU","pdfSize":"1445432","objectId":"0xfbc8dbdcfaf36381e9986214845de5a4feee44c4a2cbf4d8d31a365ff437fa30","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
