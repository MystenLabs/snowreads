{"id":"2408.06781","title":"Do Vision-Language Foundational models show Robust Visual Perception?","authors":"Shivam Chandhok, Pranav Tandon","authorsParsed":[["Chandhok","Shivam",""],["Tandon","Pranav",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 10:06:53 GMT"}],"updateDate":"2024-08-14","timestamp":1723543613000,"abstract":"  Recent advances in vision-language foundational models have enabled\ndevelopment of systems that can perform visual understanding and reasoning\ntasks. However, it is unclear if these models are robust to distribution\nshifts, and how their performance and generalization capabilities vary under\nchanges in data distribution. In this project we strive to answer the question\n\"Are vision-language foundational models robust to distribution shifts like\nhuman perception?\" Specifically, we consider a diverse range of vision-language\nmodels and compare how the performance of these systems is affected by\ncorruption based distribution shifts (such as \\textit{motion blur, fog, snow,\ngaussian noise}) commonly found in practical real-world scenarios. We analyse\nthe generalization capabilities qualitatively and quantitatively on zero-shot\nimage classification task under aforementioned distribution shifts. Our code\nwill be avaible at \\url{https://github.com/shivam-chandhok/CPSC-540-Project}\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}