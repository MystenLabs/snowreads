{"id":"2408.10467","title":"Learning Multimodal Latent Space with EBM Prior and MCMC Inference","authors":"Shiyu Yuan, Carlo Lipizzi, Tian Han","authorsParsed":[["Yuan","Shiyu",""],["Lipizzi","Carlo",""],["Han","Tian",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 00:33:45 GMT"}],"updateDate":"2024-08-21","timestamp":1724114025000,"abstract":"  Multimodal generative models are crucial for various applications. We propose\nan approach that combines an expressive energy-based model (EBM) prior with\nMarkov Chain Monte Carlo (MCMC) inference in the latent space for multimodal\ngeneration. The EBM prior acts as an informative guide, while MCMC inference,\nspecifically through short-run Langevin dynamics, brings the posterior\ndistribution closer to its true form. This method not only provides an\nexpressive prior to better capture the complexity of multimodality but also\nimproves the learning of shared latent variables for more coherent generation\nacross modalities. Our proposed method is supported by empirical experiments,\nunderscoring the effectiveness of our EBM prior with MCMC inference in\nenhancing cross-modal and joint generative tasks in multimodal contexts.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}