{"id":"2407.09447","title":"ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to\n  Identify Likely Toxic Prompts","authors":"Amelia F. Hardy, Houjun Liu, Bernard Lange, Mykel J. Kochenderfer","authorsParsed":[["Hardy","Amelia F.",""],["Liu","Houjun",""],["Lange","Bernard",""],["Kochenderfer","Mykel J.",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 17:33:34 GMT"}],"updateDate":"2024-07-15","timestamp":1720805614000,"abstract":"  Typical schemes for automated red-teaming large language models (LLMs) focus\non discovering prompts that trigger a frozen language model (the defender) to\ngenerate toxic text. This often results in the prompting model (the adversary)\nproducing text that is unintelligible and unlikely to arise. Here, we propose a\nreinforcement learning formulation of the LLM red-teaming task which allows us\nto discover prompts that both (1) trigger toxic outputs from a frozen defender\nand (2) have low perplexity as scored by the defender. We argue these cases are\nmost pertinent in a red-teaming setting because of their likelihood to arise\nduring normal use of the defender model. We solve this formulation through a\nnovel online and weakly supervised variant of Identity Preference Optimization\n(IPO) on GPT-2 and GPT-2 XL defenders. We demonstrate that our policy is\ncapable of generating likely prompts that also trigger toxicity. Finally, we\nqualitatively analyze learned strategies, trade-offs of likelihood and\ntoxicity, and discuss implications. Source code is available for this project\nat: https://github.com/sisl/ASTPrompter/.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}