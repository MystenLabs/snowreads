{"id":"2408.13005","title":"EasyControl: Transfer ControlNet to Video Diffusion for Controllable\n  Generation and Interpolation","authors":"Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han,\n  Hang Xu, Xiaodan Liang","authorsParsed":[["Wang","Cong",""],["Gu","Jiaxi",""],["Hu","Panwen",""],["Zhao","Haoyu",""],["Guo","Yuanfan",""],["Han","Jianhua",""],["Xu","Hang",""],["Liang","Xiaodan",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 11:48:29 GMT"},{"version":"v2","created":"Mon, 16 Sep 2024 15:56:34 GMT"}],"updateDate":"2024-09-17","timestamp":1724413709000,"abstract":"  Following the advancements in text-guided image generation technology\nexemplified by Stable Diffusion, video generation is gaining increased\nattention in the academic community. However, relying solely on text guidance\nfor video generation has serious limitations, as videos contain much richer\ncontent than images, especially in terms of motion. This information can hardly\nbe adequately described with plain text. Fortunately, in computer vision,\nvarious visual representations can serve as additional control signals to guide\ngeneration. With the help of these signals, video generation can be controlled\nin finer detail, allowing for greater flexibility for different applications.\nIntegrating various controls, however, is nontrivial. In this paper, we propose\na universal framework called EasyControl. By propagating and injecting\ncondition features through condition adapters, our method enables users to\ncontrol video generation with a single condition map. With our framework,\nvarious conditions including raw pixels, depth, HED, etc., can be integrated\ninto different Unet-based pre-trained video diffusion models at a low practical\ncost. We conduct comprehensive experiments on public datasets, and both\nquantitative and qualitative results indicate that our method outperforms\nstate-of-the-art methods. EasyControl significantly improves various evaluation\nmetrics across multiple validation datasets compared to previous works.\nSpecifically, for the sketch-to-video generation task, EasyControl achieves an\nimprovement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 compared\nwith VideoComposer. For fidelity, our model demonstrates powerful image\nretention ability, resulting in high FVD and IS in UCF101 and MSR-VTT compared\nto other image-to-video models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UEaV_p4QmgISAj2T2_Zf1g5_Gdi8vUdha0oeuQcLrKw","pdfSize":"2486822"}
