{"id":"2408.03944","title":"Taxonomy Driven Fast Adversarial Training","authors":"Kun Tong, Chengze Jiang, Jie Gui, Yuan Cao","authorsParsed":[["Tong","Kun",""],["Jiang","Chengze",""],["Gui","Jie",""],["Cao","Yuan",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 03:56:27 GMT"}],"updateDate":"2024-08-09","timestamp":1721620587000,"abstract":"  Adversarial training (AT) is an effective defense method against\ngradient-based attacks to enhance the robustness of neural networks. Among\nthem, single-step AT has emerged as a hotspot topic due to its simplicity and\nefficiency, requiring only one gradient propagation in generating adversarial\nexamples. Nonetheless, the problem of catastrophic overfitting (CO) that causes\ntraining collapse remains poorly understood, and there exists a gap between the\nrobust accuracy achieved through single- and multi-step AT. In this paper, we\npresent a surprising finding that the taxonomy of adversarial examples reveals\nthe truth of CO. Based on this conclusion, we propose taxonomy driven fast\nadversarial training (TDAT) which jointly optimizes learning objective, loss\nfunction, and initialization method, thereby can be regarded as a new paradigm\nof single-step AT. Compared with other fast AT methods, TDAT can boost the\nrobustness of neural networks, alleviate the influence of misclassified\nexamples, and prevent CO during the training process while requiring almost no\nadditional computational and memory resources. Our method achieves robust\naccuracy improvement of $1.59\\%$, $1.62\\%$, $0.71\\%$, and $1.26\\%$ on CIFAR-10,\nCIFAR-100, Tiny ImageNet, and ImageNet-100 datasets, when against projected\ngradient descent PGD10 attack with perturbation budget 8/255. Furthermore, our\nproposed method also achieves state-of-the-art robust accuracy against other\nattacks. Code is available at https://github.com/bookman233/TDAT.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}