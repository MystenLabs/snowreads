{"id":"2408.15980","title":"In-Context Imitation Learning via Next-Token Prediction","authors":"Letian Fu and Huang Huang and Gaurav Datta and Lawrence Yunliang Chen\n  and William Chung-Ho Panitch and Fangchen Liu and Hui Li and Ken Goldberg","authorsParsed":[["Fu","Letian",""],["Huang","Huang",""],["Datta","Gaurav",""],["Chen","Lawrence Yunliang",""],["Panitch","William Chung-Ho",""],["Liu","Fangchen",""],["Li","Hui",""],["Goldberg","Ken",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 17:50:19 GMT"}],"updateDate":"2024-08-29","timestamp":1724867419000,"abstract":"  We explore how to enhance next-token prediction models to perform in-context\nimitation learning on a real robot, where the robot executes new tasks by\ninterpreting contextual information provided during the input phase, without\nupdating its underlying policy parameters. We propose In-Context Robot\nTransformer (ICRT), a causal transformer that performs autoregressive\nprediction on sensorimotor trajectories without relying on any linguistic data\nor reward function. This formulation enables flexible and training-free\nexecution of new tasks at test time, achieved by prompting the model with\nsensorimotor trajectories of the new task composing of image observations,\nactions and states tuples, collected through human teleoperation. Experiments\nwith a Franka Emika robot demonstrate that the ICRT can adapt to new tasks\nspecified by prompts, even in environment configurations that differ from both\nthe prompt and the training data. In a multitask environment setup, ICRT\nsignificantly outperforms current state-of-the-art next-token prediction models\nin robotics on generalizing to unseen tasks. Code, checkpoints and data are\navailable on https://icrt.dev/\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nQqcJgIgYHNJsb23wZgj1k0fy3jHKVOeeLEc0C1Nwag","pdfSize":"11444970"}
