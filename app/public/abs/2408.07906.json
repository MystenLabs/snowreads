{"id":"2408.07906","title":"KAN versus MLP on Irregular or Noisy Functions","authors":"Chen Zeng, Jiahui Wang, Haoran Shen, and Qiao Wang","authorsParsed":[["Zeng","Chen",""],["Wang","Jiahui",""],["Shen","Haoran",""],["Wang","Qiao",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 03:24:07 GMT"}],"updateDate":"2024-08-16","timestamp":1723692247000,"abstract":"  In this paper, we compare the performance of Kolmogorov-Arnold Networks (KAN)\nand Multi-Layer Perceptron (MLP) networks on irregular or noisy functions. We\ncontrol the number of parameters and the size of the training samples to ensure\na fair comparison. For clarity, we categorize the functions into six types:\nregular functions, continuous functions with local non-differentiable points,\nfunctions with jump discontinuities, functions with singularities, functions\nwith coherent oscillations, and noisy functions. Our experimental results\nindicate that KAN does not always perform best. For some types of functions,\nMLP outperforms or performs comparably to KAN. Furthermore, increasing the size\nof training samples can improve performance to some extent. When noise is added\nto functions, the irregular features are often obscured by the noise, making it\nchallenging for both MLP and KAN to extract these features effectively. We hope\nthese experiments provide valuable insights for future neural network research\nand encourage further investigations to overcome these challenges.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Numerical Analysis","Computing Research Repository/Neural and Evolutionary Computing","Mathematics/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"pbLdss985nuchQWSQGOZ6xcSDUluHhOaLLkd8a6oPf4","pdfSize":"3118745"}
