{"id":"2408.08345","title":"5%>100%: Breaking Performance Shackles of Full Fine-Tuning on Visual\n  Recognition Tasks","authors":"Dongshuo Yin, Leiyi Hu, Bin Li, Youqun Zhang, Xue Yang","authorsParsed":[["Yin","Dongshuo",""],["Hu","Leiyi",""],["Li","Bin",""],["Zhang","Youqun",""],["Yang","Xue",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:58:10 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 05:08:00 GMT"}],"updateDate":"2024-08-28","timestamp":1723744690000,"abstract":"  Pre-training & fine-tuning can enhance the transferring efficiency and\nperformance in visual tasks. Recent delta-tuning methods provide more options\nfor visual classification tasks. Despite their success, existing visual\ndelta-tuning art fails to exceed the upper limit of full fine-tuning on\nchallenging tasks like object detection and segmentation. To find a competitive\nalternative to full fine-tuning, we propose the Multi-cognitive Visual Adapter\n(Mona) tuning, a novel adapter-based tuning method. First, we introduce\nmultiple vision-friendly filters into the adapter to enhance its ability to\nprocess visual signals, while previous methods mainly rely on language-friendly\nlinear filters. Second, we add the scaled normalization layer in the adapter to\nregulate the distribution of input features for visual filters. To fully\ndemonstrate the practicality and generality of Mona, we conduct experiments on\nmultiple representative visual tasks, including instance segmentation on COCO,\nsemantic segmentation on ADE20K, object detection on Pascal VOC, oriented\nobject detection on DOTA/STAR, and image classification on three common\ndatasets. Exciting results illustrate that Mona surpasses full fine-tuning on\nall these tasks, and is the only delta-tuning method outperforming full\nfine-tuning on the above various tasks. For example, Mona achieves 1%\nperformance gain on the COCO dataset compared to full fine-tuning.\nComprehensive results suggest that Mona-tuning is more suitable for retaining\nand utilizing the capabilities of pre-trained models than full fine-tuning. The\ncode will be released at https://github.com/Leiyi-Hu/mona.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}