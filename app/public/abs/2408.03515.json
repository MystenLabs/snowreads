{"id":"2408.03515","title":"A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic\n  Systems","authors":"Wenxiao Zhang, Xiangrui Kong, Conan Dewitt, Thomas Braunl, Jin B. Hong","authorsParsed":[["Zhang","Wenxiao",""],["Kong","Xiangrui",""],["Dewitt","Conan",""],["Braunl","Thomas",""],["Hong","Jin B.",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 02:48:22 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 01:55:03 GMT"}],"updateDate":"2024-09-10","timestamp":1722998902000,"abstract":"  The integration of Large Language Models (LLMs) like GPT-4o into robotic\nsystems represents a significant advancement in embodied artificial\nintelligence. These models can process multi-modal prompts, enabling them to\ngenerate more context-aware responses. However, this integration is not without\nchallenges. One of the primary concerns is the potential security risks\nassociated with using LLMs in robotic navigation tasks. These tasks require\nprecise and reliable responses to ensure safe and effective operation.\nMulti-modal prompts, while enhancing the robot's understanding, also introduce\ncomplexities that can be exploited maliciously. For instance, adversarial\ninputs designed to mislead the model can lead to incorrect or dangerous\nnavigational decisions. This study investigates the impact of prompt injections\non mobile robot performance in LLM-integrated systems and explores secure\nprompt strategies to mitigate these risks. Our findings demonstrate a\nsubstantial overall improvement of approximately 30.8% in both attack detection\nand system performance with the implementation of robust defence mechanisms,\nhighlighting their critical role in enhancing security and reliability in\nmission-oriented tasks.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"l_O8Q9bwWHeosAGDNzrpuo89ZddirJyyRwmiax2YpSM","pdfSize":"1342223"}
