{"id":"2408.07295","title":"Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots","authors":"Pranay Dugar, Aayam Shrestha, Fangzhou Yu, Bart van Marum, Alan Fern","authorsParsed":[["Dugar","Pranay",""],["Shrestha","Aayam",""],["Yu","Fangzhou",""],["van Marum","Bart",""],["Fern","Alan",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 09:10:24 GMT"},{"version":"v2","created":"Mon, 16 Sep 2024 19:41:39 GMT"}],"updateDate":"2024-09-18","timestamp":1722330624000,"abstract":"  The foundational capabilities of humanoid robots should include robustly\nstanding, walking, and mimicry of whole and partial-body motions. This work\nintroduces the Masked Humanoid Controller (MHC), which supports all of these\ncapabilities by tracking target trajectories over selected subsets of humanoid\nstate variables while ensuring balance and robustness against disturbances. The\nMHC is trained in simulation using a carefully designed curriculum that\nimitates partially masked motions from a library of behaviors spanning\nstanding, walking, optimized reference trajectories, re-targeted video clips,\nand human motion capture data. It also allows for combining joystick-based\ncontrol with partial-body motion mimicry. We showcase simulation experiments\nvalidating the MHC's ability to execute a wide variety of behaviors from\npartially-specified target motions. Moreover, we demonstrate sim-to-real\ntransfer on the real-world Digit V3 humanoid robot. To our knowledge, this is\nthe first instance of a learned controller that can realize whole-body control\nof a real-world humanoid for such diverse multi-modal targets.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jkt2EAm8zQdnaG8u924-FBqFiD463SJSiTM57nPOcu8","pdfSize":"4672950"}
