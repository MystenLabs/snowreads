{"id":"2407.02253","title":"Parameter-Selective Continual Test-Time Adaptation","authors":"Jiaxu Tian, Fan Lyu","authorsParsed":[["Tian","Jiaxu",""],["Lyu","Fan",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 13:18:15 GMT"}],"updateDate":"2024-07-03","timestamp":1719926295000,"abstract":"  Continual Test-Time Adaptation (CTTA) aims to adapt a pretrained model to\never-changing environments during the test time under continuous domain shifts.\nMost existing CTTA approaches are based on the Mean Teacher (MT) structure,\nwhich contains a student and a teacher model, where the student is updated\nusing the pseudo-labels from the teacher model, and the teacher is then updated\nby exponential moving average strategy. However, these methods update the MT\nmodel indiscriminately on all parameters of the model. That is, some critical\nparameters involving sharing knowledge across different domains may be erased,\nintensifying error accumulation and catastrophic forgetting. In this paper, we\nintroduce Parameter-Selective Mean Teacher (PSMT) method, which is capable of\neffectively updating the critical parameters within the MT network under domain\nshifts. First, we introduce a selective distillation mechanism in the student\nmodel, which utilizes past knowledge to regularize novel knowledge, thereby\nmitigating the impact of error accumulation. Second, to avoid catastrophic\nforgetting, in the teacher model, we create a mask through Fisher information\nto selectively update parameters via exponential moving average, with\npreservation measures applied to crucial parameters. Extensive experimental\nresults verify that PSMT outperforms state-of-the-art methods across multiple\nbenchmark datasets. Our code is available at\n\\url{https://github.com/JiaxuTian/PSMT}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}