{"id":"2408.12110","title":"Pareto Inverse Reinforcement Learning for Diverse Expert Policy\n  Generation","authors":"Woo Kyung Kim and Minjong Yoo and Honguk Woo","authorsParsed":[["Kim","Woo Kyung",""],["Yoo","Minjong",""],["Woo","Honguk",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 03:51:39 GMT"}],"updateDate":"2024-08-23","timestamp":1724298699000,"abstract":"  Data-driven offline reinforcement learning and imitation learning approaches\nhave been gaining popularity in addressing sequential decision-making problems.\nYet, these approaches rarely consider learning Pareto-optimal policies from a\nlimited pool of expert datasets. This becomes particularly marked due to\npractical limitations in obtaining comprehensive datasets for all preferences,\nwhere multiple conflicting objectives exist and each expert might hold a unique\noptimization preference for these objectives. In this paper, we adapt inverse\nreinforcement learning (IRL) by using reward distance estimates for\nregularizing the discriminator. This enables progressive generation of a set of\npolicies that accommodate diverse preferences on the multiple objectives, while\nusing only two distinct datasets, each associated with a different expert\npreference. In doing so, we present a Pareto IRL framework (ParIRL) that\nestablishes a Pareto policy set from these limited datasets. In the framework,\nthe Pareto policy set is then distilled into a single, preference-conditioned\ndiffusion model, thus allowing users to immediately specify which expert's\npatterns they prefer. Through experiments, we show that ParIRL outperforms\nother IRL algorithms for various multi-objective control tasks, achieving the\ndense approximation of the Pareto frontier. We also demonstrate the\napplicability of ParIRL with autonomous driving in CARLA.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jauiTm5J-tAeYkNlhNxGBBVsPVLPCxlpEGmNejKP6bU","pdfSize":"6523974"}
