{"id":"2408.07089","title":"InfinityMATH: A Scalable Instruction Tuning Dataset in Programmatic\n  Mathematical Reasoning","authors":"Bo-Wen Zhang, Yan Yan, Lin Li, Guang Liu","authorsParsed":[["Zhang","Bo-Wen",""],["Yan","Yan",""],["Li","Lin",""],["Liu","Guang",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 08:18:20 GMT"}],"updateDate":"2024-08-15","timestamp":1723191500000,"abstract":"  Recent advancements in Chain-of-Thoughts (CoT) and Program-of-Thoughts (PoT)\nmethods have greatly enhanced language models' mathematical reasoning\ncapabilities, facilitating their integration into instruction tuning datasets\nwith LLMs. However, existing methods for large-scale dataset creation require\nsubstantial seed data and high computational costs for data synthesis, posing\nsignificant challenges for scalability. We introduce InfinityMATH, a scalable\ninstruction tuning dataset for programmatic mathematical reasoning. The\nconstruction pipeline emphasizes decoupling numbers from mathematical problems\nto synthesize number-independent programs, enabling efficient and flexible\nscaling while minimizing dependency on specific numerical values. Fine-tuning\nexperiments with open-source language and code models, such as Llama2 and\nCodeLlama, demonstrate the practical benefits of InfinityMATH. These fine-tuned\nmodels, showed significant relative improvements on both in-domain and\nout-of-domain benchmarks, ranging from 184.7% to 514.3% on average.\nAdditionally, these models exhibited high robustness on the GSM8K+ and MATH+\nbenchmarks, which are enhanced version of test sets with simply the number\nvariations. InfinityMATH ensures that models are more versatile and effective\nacross a broader range of mathematical problems. The data is available at\nhttps://huggingface.co/datasets/flagopen/InfinityMATH.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"8BhV7bUpYVFUS1h8JxezKOcznQTowDZxydRXdPxOReI","pdfSize":"1033694"}
