{"id":"2407.08609","title":"BiasPruner: Debiased Continual Learning for Medical Image Classification","authors":"Nourhan Bayasi, Jamil Fayyad, Alceu Bissoto, Ghassan Hamarneh, Rafeef\n  Garbi","authorsParsed":[["Bayasi","Nourhan",""],["Fayyad","Jamil",""],["Bissoto","Alceu",""],["Hamarneh","Ghassan",""],["Garbi","Rafeef",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 15:45:57 GMT"}],"updateDate":"2024-07-12","timestamp":1720712757000,"abstract":"  Continual Learning (CL) is crucial for enabling networks to dynamically adapt\nas they learn new tasks sequentially, accommodating new data and classes\nwithout catastrophic forgetting. Diverging from conventional perspectives on\nCL, our paper introduces a new perspective wherein forgetting could actually\nbenefit the sequential learning paradigm. Specifically, we present BiasPruner,\na CL framework that intentionally forgets spurious correlations in the training\ndata that could lead to shortcut learning. Utilizing a new bias score that\nmeasures the contribution of each unit in the network to learning spurious\nfeatures, BiasPruner prunes those units with the highest bias scores to form a\ndebiased subnetwork preserved for a given task. As BiasPruner learns a new\ntask, it constructs a new debiased subnetwork, potentially incorporating units\nfrom previous subnetworks, which improves adaptation and performance on the new\ntask. During inference, BiasPruner employs a simple task-agnostic approach to\nselect the best debiased subnetwork for predictions. We conduct experiments on\nthree medical datasets for skin lesion classification and chest X-Ray\nclassification and demonstrate that BiasPruner consistently outperforms SOTA CL\nmethods in terms of classification performance and fairness. Our code is\navailable here.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}