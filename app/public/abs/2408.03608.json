{"id":"2408.03608","title":"Mixstyle-Entropy: Domain Generalization with Causal Intervention and\n  Perturbation","authors":"Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Xinghao Ding, Yue Huang","authorsParsed":[["Tang","Luyao",""],["Yuan","Yuxuan",""],["Chen","Chaoqi",""],["Ding","Xinghao",""],["Huang","Yue",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 07:54:19 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 13:13:56 GMT"}],"updateDate":"2024-08-23","timestamp":1723017259000,"abstract":"  Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition","Statistics/Methodology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}