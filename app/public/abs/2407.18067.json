{"id":"2407.18067","title":"HVM-1: Large-scale video models pretrained with nearly 5000 hours of\n  human-like video data","authors":"A. Emin Orhan","authorsParsed":[["Orhan","A. Emin",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 14:21:50 GMT"}],"updateDate":"2024-07-26","timestamp":1721917310000,"abstract":"  We introduce Human-like Video Models (HVM-1), large-scale video models\npretrained with nearly 5000 hours of curated human-like video data (mostly\negocentric, temporally extended, continuous video recordings), using the\nspatiotemporal masked autoencoder (ST-MAE) algorithm. We release two 633M\nparameter models trained at spatial resolutions of 224x224 and 448x448 pixels.\nWe evaluate the performance of these models in downstream few-shot video and\nimage recognition tasks and compare them against a model pretrained with 1330\nhours of short action-oriented video clips from YouTube (Kinetics-700). HVM-1\nmodels perform competitively against the Kinetics-700 pretrained model in\ndownstream evaluations despite substantial qualitative differences between the\nspatiotemporal characteristics of the corresponding pretraining datasets. HVM-1\nmodels also learn more accurate and more robust object representations compared\nto models pretrained with the image-based MAE algorithm on the same data,\ndemonstrating the potential benefits of learning to predict temporal\nregularities in natural videos for learning better object representations.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing","Quantitative Biology/Neurons and Cognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}