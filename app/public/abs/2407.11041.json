{"id":"2407.11041","title":"Integer-only Quantized Transformers for Embedded FPGA-based Time-series\n  Forecasting in AIoT","authors":"Tianheng Ling, Chao Qian, Gregor Schiele","authorsParsed":[["Ling","Tianheng",""],["Qian","Chao",""],["Schiele","Gregor",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 15:03:40 GMT"},{"version":"v2","created":"Fri, 6 Sep 2024 13:33:34 GMT"}],"updateDate":"2024-09-09","timestamp":1720278220000,"abstract":"  This paper presents the design of a hardware accelerator for Transformers,\noptimized for on-device time-series forecasting in AIoT systems. It integrates\ninteger-only quantization and Quantization-Aware Training with optimized\nhardware designs to realize 6-bit and 4-bit quantized Transformer models, which\nachieved precision comparable to 8-bit quantized models from related research.\nUtilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7\nXC7S15), we examine the feasibility of deploying Transformer models on embedded\nIoT devices. This includes a thorough analysis of achievable precision,\nresource utilization, timing, power, and energy consumption for on-device\ninference. Our results indicate that while sufficient performance can be\nattained, the optimization process is not trivial. For instance, reducing the\nquantization bitwidth does not consistently result in decreased latency or\nenergy consumption, underscoring the necessity of systematically exploring\nvarious optimization combinations. Compared to an 8-bit quantized Transformer\nmodel in related studies, our 4-bit quantized Transformer model increases test\nloss by only 0.63%, operates up to 132.33x faster, and consumes 48.19x less\nenergy.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}