{"id":"2408.06544","title":"Variance-Reduced Cascade Q-learning: Algorithms and Sample Complexity","authors":"Mohammad Boveiri and Peyman Mohajerin Esfahani","authorsParsed":[["Boveiri","Mohammad",""],["Esfahani","Peyman Mohajerin",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 00:34:33 GMT"}],"updateDate":"2024-08-14","timestamp":1723509273000,"abstract":"  We study the problem of estimating the optimal Q-function of\n$\\gamma$-discounted Markov decision processes (MDPs) under the synchronous\nsetting, where independent samples for all state-action pairs are drawn from a\ngenerative model at each iteration. We introduce and analyze a novel model-free\nalgorithm called Variance-Reduced Cascade Q-learning (VRCQ). VRCQ comprises two\nkey building blocks: (i) the established direct variance reduction technique\nand (ii) our proposed variance reduction scheme, Cascade Q-learning. By\nleveraging these techniques, VRCQ provides superior guarantees in the\n$\\ell_\\infty$-norm compared with the existing model-free stochastic\napproximation-type algorithms. Specifically, we demonstrate that VRCQ is\nminimax optimal. Additionally, when the action set is a singleton (so that the\nQ-learning problem reduces to policy evaluation), it achieves non-asymptotic\ninstance optimality while requiring the minimum number of samples theoretically\npossible. Our theoretical results and their practical implications are\nsupported by numerical experiments.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}