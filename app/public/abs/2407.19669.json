{"id":"2407.19669","title":"mGTE: Generalized Long-Context Text Representation and Reranking Models\n  for Multilingual Text Retrieval","authors":"Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong\n  Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie\n  Li, Min Zhang","authorsParsed":[["Zhang","Xin",""],["Zhang","Yanzhao",""],["Long","Dingkun",""],["Xie","Wen",""],["Dai","Ziqi",""],["Tang","Jialong",""],["Lin","Huan",""],["Yang","Baosong",""],["Xie","Pengjun",""],["Huang","Fei",""],["Zhang","Meishan",""],["Li","Wenjie",""],["Zhang","Min",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 03:12:28 GMT"}],"updateDate":"2024-07-30","timestamp":1722222748000,"abstract":"  We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}