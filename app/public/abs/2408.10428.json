{"id":"2408.10428","title":"Are LLMs Any Good for High-Level Synthesis?","authors":"Yuchao Liao, Tosiron Adegbija, Roman Lysecky","authorsParsed":[["Liao","Yuchao",""],["Adegbija","Tosiron",""],["Lysecky","Roman",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 21:40:28 GMT"}],"updateDate":"2024-08-21","timestamp":1724103628000,"abstract":"  The increasing complexity and demand for faster, energy-efficient hardware\ndesigns necessitate innovative High-Level Synthesis (HLS) methodologies. This\npaper explores the potential of Large Language Models (LLMs) to streamline or\nreplace the HLS process, leveraging their ability to understand natural\nlanguage specifications and refactor code. We survey the current research and\nconduct experiments comparing Verilog designs generated by a standard HLS tool\n(Vitis HLS) with those produced by LLMs translating C code or natural language\nspecifications. Our evaluation focuses on quantifying the impact on\nperformance, power, and resource utilization, providing an assessment of the\nefficiency of LLM-based approaches. This study aims to illuminate the role of\nLLMs in HLS, identifying promising directions for optimized hardware design in\napplications such as AI acceleration, embedded systems, and high-performance\ncomputing.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}