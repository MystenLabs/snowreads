{"id":"2408.04638","title":"Affective Computing in the Era of Large Language Models: A Survey from\n  the NLP Perspective","authors":"Yiqun Zhang, Xiaocui Yang, Xingle Xu, Zeran Gao, Yijie Huang, Shiyi\n  Mu, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song, Ge Yu","authorsParsed":[["Zhang","Yiqun",""],["Yang","Xiaocui",""],["Xu","Xingle",""],["Gao","Zeran",""],["Huang","Yijie",""],["Mu","Shiyi",""],["Feng","Shi",""],["Wang","Daling",""],["Zhang","Yifei",""],["Song","Kaisong",""],["Yu","Ge",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 08:12:04 GMT"}],"updateDate":"2024-08-12","timestamp":1722327124000,"abstract":"  Affective Computing (AC), integrating computer science, psychology, and\ncognitive science knowledge, aims to enable machines to recognize, interpret,\nand simulate human emotions.To create more value, AC can be applied to diverse\nscenarios, including social media, finance, healthcare, education, etc.\nAffective Computing (AC) includes two mainstream tasks, i.e., Affective\nUnderstanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained\nLanguage Models (PLMs) for AU tasks has succeeded considerably. However, these\nmodels lack generalization ability, requiring specialized models for specific\ntasks. Additionally, traditional PLMs face challenges in AG, particularly in\ngenerating diverse and emotionally rich responses. The emergence of Large\nLanguage Models (LLMs), such as the ChatGPT series and LLaMA models, brings new\nopportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess\ncapabilities of in-context learning, common sense reasoning, and advanced\nsequence generation, which present unprecedented opportunities for AU. To\nprovide a comprehensive overview of AC in the LLMs era from an NLP perspective,\nwe summarize the development of LLMs research in this field, aiming to offer\nnew insights. Specifically, we first summarize the traditional tasks related to\nAC and introduce the preliminary study based on LLMs. Subsequently, we outline\nthe relevant techniques of popular LLMs to improve AC tasks, including\nInstruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss\nfull parameter fine-tuning and parameter-efficient methods such as LoRA,\nP-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,\nFew-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To\nclearly understand the performance of LLMs on different Affective Computing\ntasks, we further summarize the existing benchmarks and evaluation methods.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}