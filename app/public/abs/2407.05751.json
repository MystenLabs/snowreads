{"id":"2407.05751","title":"TransformerPayne: enhancing spectral emulation accuracy and data\n  efficiency by capturing long-range correlations","authors":"Tomasz R\\'o\\.za\\'nski, Yuan-Sen Ting, and Maja Jab{\\l}o\\'nska","authorsParsed":[["Różański","Tomasz",""],["Ting","Yuan-Sen",""],["Jabłońska","Maja",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 09:03:49 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 08:48:51 GMT"},{"version":"v3","created":"Mon, 12 Aug 2024 02:15:55 GMT"}],"updateDate":"2024-08-13","timestamp":1720429429000,"abstract":"  Stellar spectra emulators often rely on large grids and tend to reach a\nplateau in emulation accuracy, leading to significant systematic errors when\ninferring stellar properties. Our study explores the use of Transformer models\nto capture long-range information in spectra, comparing their performance to\nThe Payne emulator (a fully connected multilayer perceptron), an expanded\nversion of The Payne, and a convolutional-based emulator. We tested these\nmodels on synthetic spectra grids, evaluating their performance by analyzing\nemulation residuals and assessing the quality of spectral parameter inference.\nThe newly introduced TransformerPayne emulator outperformed all other tested\nmodels, achieving a mean absolute error (MAE) of approximately 0.15% when\ntrained on the full grid. The most significant improvements were observed in\ngrids containing between 1000 and 10,000 spectra, with TransformerPayne showing\n2 to 5 times better performance than the scaled-up version of The Payne.\nAdditionally, TransformerPayne demonstrated superior fine-tuning capabilities,\nallowing for pretraining on one spectral model grid before transferring to\nanother. This fine-tuning approach enabled up to a tenfold reduction in\ntraining grid size compared to models trained from scratch. Analysis of\nTransformerPayne's attention maps revealed that they encode interpretable\nfeatures common across many spectral lines of chosen elements. While scaling up\nThe Payne to a larger network reduced its MAE from 1.2% to 0.3% when trained on\nthe full dataset, TransformerPayne consistently achieved the lowest MAE across\nall tests. The inductive biases of the TransformerPayne emulator enhance\naccuracy, data efficiency, and interpretability for spectral emulation compared\nto existing methods.\n","subjects":["Astrophysics/Instrumentation and Methods for Astrophysics","Astrophysics/Solar and Stellar Astrophysics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}