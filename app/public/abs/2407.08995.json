{"id":"2407.08995","title":"Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs","authors":"Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin\n  Zhou, Jiaming Zhou, Haoqin Sun","authorsParsed":[["Kong","Aobo",""],["Zhao","Shiwan",""],["Chen","Hao",""],["Li","Qicheng",""],["Qin","Yong",""],["Sun","Ruiqi",""],["Zhou","Xin",""],["Zhou","Jiaming",""],["Sun","Haoqin",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 05:26:24 GMT"}],"updateDate":"2024-07-15","timestamp":1720761984000,"abstract":"  Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}