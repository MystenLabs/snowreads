{"id":"2407.20534","title":"BERT and LLMs-Based avGFP Brightness Prediction and Mutation Design","authors":"X. Guo, W.Che","authorsParsed":[["Guo","X.",""],["Che","W.",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 04:27:21 GMT"}],"updateDate":"2024-07-31","timestamp":1722313641000,"abstract":"  This study aims to utilize Transformer models and large language models (such\nas GPT and Claude) to predict the brightness of Aequorea victoria green\nfluorescent protein (avGFP) and design mutants with higher brightness.\nConsidering the time and cost associated with traditional experimental\nscreening methods, this study employs machine learning techniques to enhance\nresearch efficiency. We first read and preprocess a proprietary dataset\ncontaining approximately 140,000 protein sequences, including about 30,000\navGFP sequences. Subsequently, we constructed and trained a Transformer-based\nprediction model to screen and design new avGFP mutants that are expected to\nexhibit higher brightness.\n  Our methodology consists of two primary stages: first, the construction of a\nscoring model using BERT, and second, the screening and generation of mutants\nusing mutation site statistics and large language models. Through the analysis\nof predictive results, we designed and screened 10 new high-brightness avGFP\nsequences. This study not only demonstrates the potential of deep learning in\nprotein design but also provides new perspectives and methodologies for future\nresearch by integrating prior knowledge from large language models.\n","subjects":["Quantitative Biology/Other Quantitative Biology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}