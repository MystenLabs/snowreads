{"id":"2408.02379","title":"The Contribution of XAI for the Safe Development and Certification of\n  AI: An Expert-Based Analysis","authors":"Benjamin Fresz, Vincent Philipp G\\\"obels, Safa Omri, Danilo Brajovic,\n  Andreas Aichele, Janika Kutz, Jens Neuh\\\"uttler, Marco F. Huber","authorsParsed":[["Fresz","Benjamin",""],["Göbels","Vincent Philipp",""],["Omri","Safa",""],["Brajovic","Danilo",""],["Aichele","Andreas",""],["Kutz","Janika",""],["Neuhüttler","Jens",""],["Huber","Marco F.",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 16:08:21 GMT"}],"updateDate":"2024-08-06","timestamp":1721664501000,"abstract":"  Developing and certifying safe - or so-called trustworthy - AI has become an\nincreasingly salient issue, especially in light of upcoming regulation such as\nthe EU AI Act. In this context, the black-box nature of machine learning models\nlimits the use of conventional avenues of approach towards certifying complex\ntechnical systems. As a potential solution, methods to give insights into this\nblack-box - devised in the field of eXplainable AI (XAI) - could be used. In\nthis study, the potential and shortcomings of such methods for the purpose of\nsafe AI development and certification are discussed in 15 qualitative\ninterviews with experts out of the areas of (X)AI and certification. We find\nthat XAI methods can be a helpful asset for safe AI development, as they can\nshow biases and failures of ML-models, but since certification relies on\ncomprehensive and correct information about technical systems, their impact is\nexpected to be limited.\n","subjects":["Computing Research Repository/Computers and Society","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"12K5-zTnhFp6YcFLq1p5qIW5_GNa4yCSpo0BT9XjMvk","pdfSize":"151556"}
