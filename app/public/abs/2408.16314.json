{"id":"2408.16314","title":"ResVG: Enhancing Relation and Semantic Understanding in Multiple\n  Instances for Visual Grounding","authors":"Minghang Zheng, Jiahua Zhang, Qingchao Chen, Yuxin Peng, Yang Liu","authorsParsed":[["Zheng","Minghang",""],["Zhang","Jiahua",""],["Chen","Qingchao",""],["Peng","Yuxin",""],["Liu","Yang",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 07:32:01 GMT"}],"updateDate":"2024-08-30","timestamp":1724916721000,"abstract":"  Visual grounding aims to localize the object referred to in an image based on\na natural language query. Although progress has been made recently, accurately\nlocalizing target objects within multiple-instance distractions (multiple\nobjects of the same category as the target) remains a significant challenge.\nExisting methods demonstrate a significant performance drop when there are\nmultiple distractions in an image, indicating an insufficient understanding of\nthe fine-grained semantics and spatial relationships between objects. In this\npaper, we propose a novel approach, the Relation and Semantic-sensitive Visual\nGrounding (ResVG) model, to address this issue. Firstly, we enhance the model's\nunderstanding of fine-grained semantics by injecting semantic prior information\nderived from text queries into the model. This is achieved by leveraging\ntext-to-image generation models to produce images representing the semantic\nattributes of target objects described in queries. Secondly, we tackle the lack\nof training samples with multiple distractions by introducing a\nrelation-sensitive data augmentation method. This method generates additional\ntraining data by synthesizing images containing multiple objects of the same\ncategory and pseudo queries based on their spatial relationships. The proposed\nReSVG model significantly improves the model's ability to comprehend both\nobject semantics and spatial relations, leading to enhanced performance in\nvisual grounding tasks, particularly in scenarios with multiple-instance\ndistractions. We conduct extensive experiments to validate the effectiveness of\nour methods on five datasets. Code is available at\nhttps://github.com/minghangz/ResVG.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}