{"id":"2407.06677","title":"Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of\n  Modules","authors":"Zhuocheng Gong, Ang Lv, Jian Guan, Junxi Yan, Wei Wu, Huishuai Zhang,\n  Minlie Huang, Dongyan Zhao, Rui Yan","authorsParsed":[["Gong","Zhuocheng",""],["Lv","Ang",""],["Guan","Jian",""],["Yan","Junxi",""],["Wu","Wei",""],["Zhang","Huishuai",""],["Huang","Minlie",""],["Zhao","Dongyan",""],["Yan","Rui",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 08:50:18 GMT"}],"updateDate":"2024-07-10","timestamp":1720515018000,"abstract":"  Is it always necessary to compute tokens from shallow to deep layers in\nTransformers? The continued success of vanilla Transformers and their variants\nsuggests an undoubted \"yes\". In this work, however, we attempt to break the\ndepth-ordered convention by proposing a novel architecture dubbed\nmixture-of-modules (MoM), which is motivated by an intuition that any layer,\nregardless of its position, can be used to compute a token as long as it\npossesses the needed processing capabilities. The construction of MoM starts\nfrom a finite set of modules defined by multi-head attention and feed-forward\nnetworks, each distinguished by its unique parameterization. Two routers then\niteratively select attention modules and feed-forward modules from the set to\nprocess a token. The selection dynamically expands the computation graph in the\nforward pass of the token, culminating in an assembly of modules. We show that\nMoM provides not only a unified framework for Transformers and their numerous\nvariants but also a flexible and learnable approach for reducing redundancy in\nTransformer parameterization. We pre-train various MoMs using OpenWebText.\nEmpirical results demonstrate that MoMs, of different parameter counts,\nconsistently outperform vanilla transformers on both GLUE and XSUM benchmarks.\nMore interestingly, with a fixed parameter budget, MoM-large enables an over\n38% increase in depth for computation graphs compared to GPT-2-large, resulting\nin absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large\nalso enables an over 60% reduction in depth while involving more modules per\nlayer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage\ncompared to GPT-2-large, while maintaining comparable performance.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"npddd_Ta0L_ZCR9GupJdvoOy5MQYvX1PUiwTxkbqWlo","pdfSize":"538497"}
