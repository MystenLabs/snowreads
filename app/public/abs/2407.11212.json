{"id":"2407.11212","title":"Automated essay scoring in Arabic: a dataset and analysis of a\n  BERT-based system","authors":"Rayed Ghazawi, Edwin Simpson","authorsParsed":[["Ghazawi","Rayed",""],["Simpson","Edwin",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 19:55:37 GMT"}],"updateDate":"2024-07-17","timestamp":1721073337000,"abstract":"  Automated Essay Scoring (AES) holds significant promise in the field of\neducation, helping educators to mark larger volumes of essays and provide\ntimely feedback. However, Arabic AES research has been limited by the lack of\npublicly available essay data. This study introduces AR-AES, an Arabic AES\nbenchmark dataset comprising 2046 undergraduate essays, including gender\ninformation, scores, and transparent rubric-based evaluation guidelines,\nproviding comprehensive insights into the scoring process. These essays come\nfrom four diverse courses, covering both traditional and online exams.\nAdditionally, we pioneer the use of AraBERT for AES, exploring its performance\non different question types. We find encouraging results, particularly for\nEnvironmental Chemistry and source-dependent essay questions. For the first\ntime, we examine the scale of errors made by a BERT-based AES system, observing\nthat 96.15 percent of the errors are within one point of the first human\nmarker's prediction, on a scale of one to five, with 79.49 percent of\npredictions matching exactly. In contrast, additional human markers did not\nexceed 30 percent exact matches with the first marker, with 62.9 percent within\none mark. These findings highlight the subjectivity inherent in essay grading,\nand underscore the potential for current AES technology to assist human markers\nto grade consistently across large classes.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CHba16IG6dEuTfWDkvMTzUEcoDjGhNSnssY8INFc09I","pdfSize":"773275"}
