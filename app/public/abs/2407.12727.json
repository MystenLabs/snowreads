{"id":"2407.12727","title":"NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with\n  Diffusion Model","authors":"Zhongqun Zhang, Hengfei Wang, Ziwei Yu, Yihua Cheng, Angela Yao, Hyung\n  Jin Chang","authorsParsed":[["Zhang","Zhongqun",""],["Wang","Hengfei",""],["Yu","Ziwei",""],["Cheng","Yihua",""],["Yao","Angela",""],["Chang","Hyung Jin",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 16:46:40 GMT"}],"updateDate":"2024-07-18","timestamp":1721234800000,"abstract":"  Modeling the physical contacts between the hand and object is standard for\nrefining inaccurate hand poses and generating novel human grasp in 3D\nhand-object reconstruction. However, existing methods rely on geometric\nconstraints that cannot be specified or controlled. This paper introduces a\nnovel task of controllable 3D hand-object contact modeling with natural\nlanguage descriptions. Challenges include i) the complexity of cross-modal\nmodeling from language to contact, and ii) a lack of descriptive text for\ncontact patterns. To address these issues, we propose NL2Contact, a model that\ngenerates controllable contacts by leveraging staged diffusion models. Given a\nlanguage description of the hand and contact, NL2Contact generates realistic\nand faithful 3D hand-object contacts. To train the model, we build\n\\textit{ContactDescribe}, the first dataset with hand-centered contact\ndescriptions. It contains multi-level and diverse descriptions generated by\nlarge language models based on carefully designed prompts (e.g., grasp action,\ngrasp type, contact location, free finger status). We show applications of our\nmodel to grasp pose optimization and novel human grasp generation, both based\non a textual contact description.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mIMfEqeq6WSyv3KJIi7zSQ0Wd4FtI2osR1fcQtTTe4Q","pdfSize":"1825854"}
