{"id":"2408.07205","title":"Deep Index Policy for Multi-Resource Restless Matching Bandit and Its\n  Application in Multi-Channel Scheduling","authors":"Nida Zamir, I-Hong Hou","authorsParsed":[["Zamir","Nida",""],["Hou","I-Hong",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 21:24:14 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 07:20:10 GMT"}],"updateDate":"2024-08-21","timestamp":1723584254000,"abstract":"  Scheduling in multi-channel wireless communication system presents formidable\nchallenges in effectively allocating resources. To address these challenges, we\ninvestigate a multi-resource restless matching bandit (MR-RMB) model for\nheterogeneous resource systems with an objective of maximizing long-term\ndiscounted total rewards while respecting resource constraints. We have also\ngeneralized to applications beyond multi-channel wireless. We discuss the\nMax-Weight Index Matching algorithm, which optimizes resource allocation based\non learned partial indexes. We have derived the policy gradient theorem for\nindex learning. Our main contribution is the introduction of a new Deep Index\nPolicy (DIP), an online learning algorithm tailored for MR-RMB. DIP learns the\npartial index by leveraging the policy gradient theorem for restless arms with\nconvoluted and unknown transition kernels of heterogeneous resources. We\ndemonstrate the utility of DIP by evaluating its performance for three\ndifferent MR-RMB problems. Our simulation results show that DIP indeed learns\nthe partial indexes efficiently.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}