{"id":"2407.02770","title":"Large language models, physics-based modeling, experimental\n  measurements: the trinity of data-scarce learning of polymer properties","authors":"Ning Liu, Siavash Jafarzadeh, Brian Y. Lattimer, Shuna Ni, Jim Lua,\n  Yue Yu","authorsParsed":[["Liu","Ning",""],["Jafarzadeh","Siavash",""],["Lattimer","Brian Y.",""],["Ni","Shuna",""],["Lua","Jim",""],["Yu","Yue",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 02:57:40 GMT"}],"updateDate":"2024-07-04","timestamp":1719975460000,"abstract":"  Large language models (LLMs) bear promise as a fast and accurate material\nmodeling paradigm for evaluation, analysis, and design. Their vast number of\ntrainable parameters necessitates a wealth of data to achieve accuracy and\nmitigate overfitting. However, experimental measurements are often limited and\ncostly to obtain in sufficient quantities for finetuning. To this end, we\npresent a physics-based training pipeline that tackles the pathology of data\nscarcity. The core enabler is a physics-based modeling framework that generates\na multitude of synthetic data to align the LLM to a physically consistent\ninitial state before finetuning. Our framework features a two-phase training\nstrategy: (1) utilizing the large-in-amount while less accurate synthetic data\nfor supervised pretraining, and (2) finetuning the phase-1 model with limited\nexperimental data. We empirically demonstrate that supervised pretraining is\nvital to obtaining accurate finetuned LLMs, via the lens of learning polymer\nflammability metrics where cone calorimeter data is sparse.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computational Engineering, Finance, and Science"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"tqpnsLHkUOzlPRcIfSmBVFMhkna8i2ZjGoiZIVjLq8k","pdfSize":"1228083"}
