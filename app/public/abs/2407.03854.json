{"id":"2407.03854","title":"Risk Bounds on MDL Estimators for Linear Regression Models with\n  Application to Simple ReLU Neural Networks","authors":"Yoshinari Takeishi and Jun'ichi Takeuchi","authorsParsed":[["Takeishi","Yoshinari",""],["Takeuchi","Jun'ichi",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 11:39:32 GMT"}],"updateDate":"2024-07-08","timestamp":1720093172000,"abstract":"  To investigate the theoretical foundations of deep learning from the\nviewpoint of the minimum description length (MDL) principle, we analyse risk\nbounds of MDL estimators based on two-stage codes for simple two-layers neural\nnetworks (NNs) with ReLU activation. For that purpose, we propose a method to\ndesign two-stage codes for linear regression models and establish an upper\nbound on the risk of the corresponding MDL estimators based on the theory on\nMDL estimators originated by Barron and Cover (1991). Then, we apply this\nresult to the simple two-layers NNs with ReLU activation which consist of $d$\nnodes in the input layer, $m$ nodes in the hidden layer and one output node.\nSince the object of estimation is only the $m$ weights from the hidden layer to\nthe output node in our setting, this is an example of linear regression models.\nAs a result, we show that the redundancy of the obtained two-stage codes is\nsmall owing to the fact that the eigenvalue distribution of the Fisher\ninformation matrix of the NNs is strongly biased, which was recently shown by\nTakeishi et al. (2023). That is, we establish a tight upper bound on the risk\nof our MDL estimators. Note that our risk bound, of which the leading term is\n$O(d^2 \\log n /n)$, is independent of the number of parameters $m$.\n","subjects":["Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}