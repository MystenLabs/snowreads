{"id":"2407.14923","title":"RayFormer: Improving Query-Based Multi-Camera 3D Object Detection via\n  Ray-Centric Strategies","authors":"Xiaomeng Chu, Jiajun Deng, Guoliang You, Yifan Duan, Yao Li, Yanyong\n  Zhang","authorsParsed":[["Chu","Xiaomeng",""],["Deng","Jiajun",""],["You","Guoliang",""],["Duan","Yifan",""],["Li","Yao",""],["Zhang","Yanyong",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 16:23:57 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 08:27:39 GMT"},{"version":"v3","created":"Sat, 27 Jul 2024 08:48:05 GMT"}],"updateDate":"2024-07-30","timestamp":1721492637000,"abstract":"  The recent advances in query-based multi-camera 3D object detection are\nfeatured by initializing object queries in the 3D space, and then sampling\nfeatures from perspective-view images to perform multi-round query refinement.\nIn such a framework, query points near the same camera ray are likely to sample\nsimilar features from very close pixels, resulting in ambiguous query features\nand degraded detection accuracy. To this end, we introduce RayFormer, a\ncamera-ray-inspired query-based 3D object detector that aligns the\ninitialization and feature extraction of object queries with the optical\ncharacteristics of cameras. Specifically, RayFormer transforms perspective-view\nimage features into bird's eye view (BEV) via the lift-splat-shoot method and\nsegments the BEV map to sectors based on the camera rays. Object queries are\nuniformly and sparsely initialized along each camera ray, facilitating the\nprojection of different queries onto different areas in the image to extract\ndistinct features. Besides, we leverage the instance information of images to\nsupplement the uniformly initialized object queries by further involving\nadditional queries along the ray from 2D object detection boxes. To extract\nunique object-level features that cater to distinct queries, we design a ray\nsampling method that suitably organizes the distribution of feature sampling\npoints on both images and bird's eye view. Extensive experiments are conducted\non the nuScenes dataset to validate our proposed ray-inspired model design. The\nproposed RayFormer achieves 55.5% mAP and 63.3% NDS, respectively. Our codes\nwill be made available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}