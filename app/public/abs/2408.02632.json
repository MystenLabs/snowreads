{"id":"2408.02632","title":"SEAS: Self-Evolving Adversarial Safety Optimization for Large Language\n  Models","authors":"Muxi Diao, Rumei Li, Shiyang Liu, Guogang Liao, Jingang Wang, Xunliang\n  Cai, Weiran Xu","authorsParsed":[["Diao","Muxi",""],["Li","Rumei",""],["Liu","Shiyang",""],["Liao","Guogang",""],["Wang","Jingang",""],["Cai","Xunliang",""],["Xu","Weiran",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 16:55:06 GMT"}],"updateDate":"2024-08-06","timestamp":1722876906000,"abstract":"  As large language models (LLMs) continue to advance in capability and\ninfluence, ensuring their security and preventing harmful outputs has become\ncrucial. A promising approach to address these concerns involves training\nmodels to automatically generate adversarial prompts for red teaming. However,\nthe evolving subtlety of vulnerabilities in LLMs challenges the effectiveness\nof current adversarial methods, which struggle to specifically target and\nexplore the weaknesses of these models. To tackle these challenges, we\nintroduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving\n}\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$\noptimization framework, which enhances security by leveraging data generated by\nthe model itself. SEAS operates through three iterative stages: Initialization,\nAttack, and Adversarial Optimization, refining both the Red Team and Target\nmodels to improve robustness and safety. This framework reduces reliance on\nmanual testing and significantly enhances the security capabilities of LLMs.\nOur contributions include a novel adversarial framework, a comprehensive safety\ndataset, and after three iterations, the Target model achieves a security level\ncomparable to GPT-4, while the Red Team model shows a marked increase in attack\nsuccess rate (ASR) against advanced models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}