{"id":"2408.17396","title":"Fairness-Aware Estimation of Graphical Models","authors":"Zhuoping Zhou, Davoud Ataee Tarzanagh, Bojian Hou, Qi Long, Li Shen","authorsParsed":[["Zhou","Zhuoping",""],["Tarzanagh","Davoud Ataee",""],["Hou","Bojian",""],["Long","Qi",""],["Shen","Li",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 16:30:00 GMT"}],"updateDate":"2024-09-02","timestamp":1725035400000,"abstract":"  This paper examines the issue of fairness in the estimation of graphical\nmodels (GMs), particularly Gaussian, Covariance, and Ising models. These models\nplay a vital role in understanding complex relationships in high-dimensional\ndata. However, standard GMs can result in biased outcomes, especially when the\nunderlying data involves sensitive characteristics or protected groups. To\naddress this, we introduce a comprehensive framework designed to reduce bias in\nthe estimation of GMs related to protected attributes. Our approach involves\nthe integration of the pairwise graph disparity error and a tailored loss\nfunction into a nonsmooth multi-objective optimization problem, striving to\nachieve fairness across different sensitive groups while maintaining the\neffectiveness of the GMs. Experimental evaluations on synthetic and real-world\ndatasets demonstrate that our framework effectively mitigates bias without\nundermining GMs' performance.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dpUr1W0k_Vg3m20XgCbY_HM_0sD_IqXl-Rt-RYikfrE","pdfSize":"5531356"}
