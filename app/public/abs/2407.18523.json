{"id":"2407.18523","title":"DTFormer: A Transformer-Based Method for Discrete-Time Dynamic Graph\n  Representation Learning","authors":"Xi Chen, Yun Xiong, Siwei Zhang, Jiawei Zhang, Yao Zhang, Shiyang\n  Zhou, Xixi Wu, Mingyang Zhang, Tengfei Liu, Weiqiang Wang","authorsParsed":[["Chen","Xi",""],["Xiong","Yun",""],["Zhang","Siwei",""],["Zhang","Jiawei",""],["Zhang","Yao",""],["Zhou","Shiyang",""],["Wu","Xixi",""],["Zhang","Mingyang",""],["Liu","Tengfei",""],["Wang","Weiqiang",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 05:46:23 GMT"}],"updateDate":"2024-07-29","timestamp":1721972783000,"abstract":"  Discrete-Time Dynamic Graphs (DTDGs), which are prevalent in real-world\nimplementations and notable for their ease of data acquisition, have garnered\nconsiderable attention from both academic researchers and industry\npractitioners. The representation learning of DTDGs has been extensively\napplied to model the dynamics of temporally changing entities and their\nevolving connections. Currently, DTDG representation learning predominantly\nrelies on GNN+RNN architectures, which manifest the inherent limitations of\nboth Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs). GNNs\nsuffer from the over-smoothing issue as the models architecture goes deeper,\nwhile RNNs struggle to capture long-term dependencies effectively. GNN+RNN\narchitectures also grapple with scaling to large graph sizes and long\nsequences. Additionally, these methods often compute node representations\nseparately and focus solely on individual node characteristics, thereby\noverlooking the behavior intersections between the two nodes whose link is\nbeing predicted, such as instances where the two nodes appear together in the\nsame context or share common neighbors.\n  This paper introduces a novel representation learning method DTFormer for\nDTDGs, pivoting from the traditional GNN+RNN framework to a Transformer-based\narchitecture. Our approach exploits the attention mechanism to concurrently\nprocess topological information within the graph at each timestamp and temporal\ndynamics of graphs along the timestamps, circumventing the aforementioned\nfundamental weakness of both GNNs and RNNs. Moreover, we enhance the model's\nexpressive capability by incorporating the intersection relationships among\nnodes and integrating a multi-patching module. Extensive experiments conducted\non six public dynamic graph benchmark datasets confirm our model's efficacy,\nachieving the SOTA performance.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"DKsEXJURlUrDzG7lW--H72PEUvXXG6_BUdLZf2z3Tno","pdfSize":"1955636","objectId":"0xa3a72d5397d332e301eff1c755311708673c5bbb3fbc26d89109d7752e0369eb","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
