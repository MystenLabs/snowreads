{"id":"2408.08087","title":"ColorMamba: Towards High-quality NIR-to-RGB Spectral Translation with\n  Mamba","authors":"Huiyu Zhai, Guang Jin, Xingxing Yang, Guosheng Kang","authorsParsed":[["Zhai","Huiyu",""],["Jin","Guang",""],["Yang","Xingxing",""],["Kang","Guosheng",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 11:29:13 GMT"}],"updateDate":"2024-08-16","timestamp":1723721353000,"abstract":"  Translating NIR to the visible spectrum is challenging due to cross-domain\ncomplexities. Current models struggle to balance a broad receptive field with\ncomputational efficiency, limiting practical use. Although the Selective\nStructured State Space Model, especially the improved version, Mamba, excels in\ngenerative tasks by capturing long-range dependencies with linear complexity,\nits default approach of converting 2D images into 1D sequences neglects local\ncontext. In this work, we propose a simple but effective backbone, dubbed\nColorMamba, which first introduces Mamba into spectral translation tasks. To\nexplore global long-range dependencies and local context for efficient spectral\ntranslation, we introduce learnable padding tokens to enhance the distinction\nof image boundaries and prevent potential confusion within the sequence model.\nFurthermore, local convolutional enhancement and agent attention are designed\nto improve the vanilla Mamba. Moreover, we exploit the HSV color to provide\nmulti-scale guidance in the reconstruction process for more accurate spectral\ntranslation. Extensive experiments show that our ColorMamba achieves a 1.02\nimprovement in terms of PSNR compared with the state-of-the-art method. Our\ncode is available at https://github.com/AlexYangxx/ColorMamba.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}