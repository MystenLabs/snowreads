{"id":"2407.03129","title":"Social Bias Evaluation for Large Language Models Requires Prompt\n  Variations","authors":"Rem Hida, Masahiro Kaneko, Naoaki Okazaki","authorsParsed":[["Hida","Rem",""],["Kaneko","Masahiro",""],["Okazaki","Naoaki",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:12:04 GMT"}],"updateDate":"2024-07-04","timestamp":1720015924000,"abstract":"  Warning: This paper contains examples of stereotypes and biases. Large\nLanguage Models (LLMs) exhibit considerable social biases, and various studies\nhave tried to evaluate and mitigate these biases accurately. Previous studies\nuse downstream tasks as prompts to examine the degree of social biases for\nevaluation and mitigation. While LLMs' output highly depends on prompts,\nprevious studies evaluating and mitigating bias have often relied on a limited\nvariety of prompts. In this paper, we investigate the sensitivity of LLMs when\nchanging prompt variations (task instruction and prompt, few-shot examples,\ndebias-prompt) by analyzing task performance and social bias of LLMs. Our\nexperimental results reveal that LLMs are highly sensitive to prompts to the\nextent that the ranking of LLMs fluctuates when comparing models for task\nperformance and social bias. Additionally, we show that LLMs have tradeoffs\nbetween performance and social bias caused by the prompts. Less bias from\nprompt setting may result in reduced performance. Moreover, the ambiguity of\ninstances is one of the reasons for this sensitivity to prompts in advanced\nLLMs, leading to various outputs. We recommend using diverse prompts, as in\nthis study, to compare the effects of prompts on social bias in LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}