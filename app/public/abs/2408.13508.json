{"id":"2408.13508","title":"G3DST: Generalizing 3D Style Transfer with Neural Radiance Fields across\n  Scenes and Styles","authors":"Adil Meric, Umut Kocasari, Matthias Nie{\\ss}ner and Barbara Roessle","authorsParsed":[["Meric","Adil",""],["Kocasari","Umut",""],["Nie√üner","Matthias",""],["Roessle","Barbara",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 08:04:19 GMT"}],"updateDate":"2024-08-27","timestamp":1724486659000,"abstract":"  Neural Radiance Fields (NeRF) have emerged as a powerful tool for creating\nhighly detailed and photorealistic scenes. Existing methods for NeRF-based 3D\nstyle transfer need extensive per-scene optimization for single or multiple\nstyles, limiting the applicability and efficiency of 3D style transfer. In this\nwork, we overcome the limitations of existing methods by rendering stylized\nnovel views from a NeRF without the need for per-scene or per-style\noptimization. To this end, we take advantage of a generalizable NeRF model to\nfacilitate style transfer in 3D, thereby enabling the use of a single learned\nmodel across various scenes. By incorporating a hypernetwork into a\ngeneralizable NeRF, our approach enables on-the-fly generation of stylized\nnovel views. Moreover, we introduce a novel flow-based multi-view consistency\nloss to preserve consistency across multiple views. We evaluate our method\nacross various scenes and artistic styles and show its performance in\ngenerating high-quality and multi-view consistent stylized images without the\nneed for a scene-specific implicit model. Our findings demonstrate that this\napproach not only achieves a good visual quality comparable to that of\nper-scene methods but also significantly enhances efficiency and applicability,\nmarking a notable advancement in the field of 3D style transfer.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}