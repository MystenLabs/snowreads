{"id":"2408.00675","title":"Leveraging Entailment Judgements in Cross-Lingual Summarisation","authors":"Huajian Zhang, Laura Perez-Beltrachini","authorsParsed":[["Zhang","Huajian",""],["Perez-Beltrachini","Laura",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 16:18:09 GMT"}],"updateDate":"2024-08-02","timestamp":1722529089000,"abstract":"  Synthetically created Cross-Lingual Summarisation (CLS) datasets are prone to\ninclude document-summary pairs where the reference summary is unfaithful to the\ncorresponding document as it contains content not supported by the document\n(i.e., hallucinated content). This low data quality misleads model learning and\nobscures evaluation results. Automatic ways to assess hallucinations and\nimprove training have been proposed for monolingual summarisation,\npredominantly in English. For CLS, we propose to use off-the-shelf\ncross-lingual Natural Language Inference (X-NLI) to evaluate faithfulness of\nreference and model generated summaries. Then, we study training approaches\nthat are aware of faithfulness issues in the training data and propose an\napproach that uses unlikelihood loss to teach a model about unfaithful summary\nsequences. Our results show that it is possible to train CLS models that yield\nmore faithful summaries while maintaining comparable or better informativess.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"yY4hhe8Pg5ei9e6ZGvXZRzp1-UOctcKv0PJwVypJ9RE","pdfSize":"720873"}
