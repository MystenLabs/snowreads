{"id":"2408.11745","title":"FocusLLM: Scaling LLM's Context by Parallel Decoding","authors":"Zhenyu Li, Yike Zhang, Tengyu Pan, Yutao Sun, Zhichao Duan, Junjie\n  Fang, Rong Han, Zixuan Wang, Jianyong Wang","authorsParsed":[["Li","Zhenyu",""],["Zhang","Yike",""],["Pan","Tengyu",""],["Sun","Yutao",""],["Duan","Zhichao",""],["Fang","Junjie",""],["Han","Rong",""],["Wang","Zixuan",""],["Wang","Jianyong",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 16:11:59 GMT"}],"updateDate":"2024-08-22","timestamp":1724256719000,"abstract":"  Empowering LLMs with the ability to utilize useful information from a long\ncontext is crucial for many downstream applications. However, achieving long\ncontext lengths with the conventional transformer architecture requires\nsubstantial training and inference resources. In this paper, we present\nFocusLLM, a framework designed to extend the context length of any decoder-only\nLLM, enabling the model to focus on relevant information from very long\nsequences. FocusLLM processes long text inputs by dividing them into chunks\nbased on the model's original context length to alleviate the issue of\nattention distraction. Then, it appends the local context to each chunk as a\nprompt to extract essential information from each chunk based on a novel\nparallel decoding mechanism, and ultimately integrates the extracted\ninformation into the local context. FocusLLM stands out for great training\nefficiency and versatility: trained with an 8K input length with much less\ntraining cost than previous methods, FocusLLM exhibits superior performance\nacross downstream long-context tasks and maintains strong language modeling\nability when handling extensive long texts, even up to 400K tokens. Our code is\navailable at https://github.com/leezythu/FocusLLM.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}