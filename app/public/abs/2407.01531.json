{"id":"2407.01531","title":"Sparse Diffusion Policy: A Sparse, Reusable, and Flexible Policy for\n  Robot Learning","authors":"Yixiao Wang, Yifei Zhang, Mingxiao Huo, Ran Tian, Xiang Zhang, Yichen\n  Xie, Chenfeng Xu, Pengliang Ji, Wei Zhan, Mingyu Ding, Masayoshi Tomizuka","authorsParsed":[["Wang","Yixiao",""],["Zhang","Yifei",""],["Huo","Mingxiao",""],["Tian","Ran",""],["Zhang","Xiang",""],["Xie","Yichen",""],["Xu","Chenfeng",""],["Ji","Pengliang",""],["Zhan","Wei",""],["Ding","Mingyu",""],["Tomizuka","Masayoshi",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:59:56 GMT"}],"updateDate":"2024-07-02","timestamp":1719856796000,"abstract":"  The increasing complexity of tasks in robotics demands efficient strategies\nfor multitask and continual learning. Traditional models typically rely on a\nuniversal policy for all tasks, facing challenges such as high computational\ncosts and catastrophic forgetting when learning new tasks. To address these\nissues, we introduce a sparse, reusable, and flexible policy, Sparse Diffusion\nPolicy (SDP). By adopting Mixture of Experts (MoE) within a transformer-based\ndiffusion policy, SDP selectively activates experts and skills, enabling\nefficient and task-specific learning without retraining the entire model. SDP\nnot only reduces the burden of active parameters but also facilitates the\nseamless integration and reuse of experts across various tasks. Extensive\nexperiments on diverse tasks in both simulations and real world show that SDP\n1) excels in multitask scenarios with negligible increases in active\nparameters, 2) prevents forgetting in continual learning of new tasks, and 3)\nenables efficient task transfer, offering a promising solution for advanced\nrobotic applications. Demos and codes can be found in\nhttps://forrest-110.github.io/sparse_diffusion_policy/.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}