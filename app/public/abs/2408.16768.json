{"id":"2408.16768","title":"SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners","authors":"Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao,\n  Chunyuan Li, Pheng-Ann Heng","authorsParsed":[["Guo","Ziyu",""],["Zhang","Renrui",""],["Zhu","Xiangyang",""],["Tong","Chengzhuo",""],["Gao","Peng",""],["Li","Chunyuan",""],["Heng","Pheng-Ann",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 17:59:45 GMT"}],"updateDate":"2024-08-30","timestamp":1724954385000,"abstract":"  We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}