{"id":"2407.01155","title":"CPT: Consistent Proxy Tuning for Black-box Optimization","authors":"Yuanyang He, Zitong Huang, Xinxing Xu, Rick Siow Mong Goh, Salman\n  Khan, Wangmeng Zuo, Yong Liu, Chun-Mei Feng","authorsParsed":[["He","Yuanyang",""],["Huang","Zitong",""],["Xu","Xinxing",""],["Goh","Rick Siow Mong",""],["Khan","Salman",""],["Zuo","Wangmeng",""],["Liu","Yong",""],["Feng","Chun-Mei",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 10:23:14 GMT"}],"updateDate":"2024-07-02","timestamp":1719829394000,"abstract":"  Black-box tuning has attracted recent attention due to that the structure or\ninner parameters of advanced proprietary models are not accessible.\nProxy-tuning provides a test-time output adjustment for tuning black-box\nlanguage models. It applies the difference of the output logits before and\nafter tuning a smaller white-box \"proxy\" model to improve the black-box model.\nHowever, this technique serves only as a decoding-time algorithm, leading to an\ninconsistency between training and testing which potentially limits overall\nperformance. To address this problem, we introduce Consistent Proxy Tuning\n(CPT), a simple yet effective black-box tuning method. Different from\nProxy-tuning, CPT additionally exploits the frozen large black-box model and\nanother frozen small white-box model, ensuring consistency between\ntraining-stage optimization objective and test-time proxies. This consistency\nbenefits Proxy-tuning and enhances model performance. Note that our method\nfocuses solely on logit-level computation, which makes it model-agnostic and\napplicable to any task involving logit classification. Extensive experimental\nresults demonstrate the superiority of our CPT in both black-box tuning of\nLarge Language Models (LLMs) and Vision-Language Models (VLMs) across various\ndatasets. The code is available at https://github.com/chunmeifeng/CPT.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}