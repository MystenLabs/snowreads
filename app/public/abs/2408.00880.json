{"id":"2408.00880","title":"Annotator in the Loop: A Case Study of In-Depth Rater Engagement to\n  Create a Bridging Benchmark Dataset","authors":"Sonja Schmer-Galunder, Ruta Wheelock, Scott Friedman, Alyssa Chvasta,\n  Zaria Jalan, Emily Saltz","authorsParsed":[["Schmer-Galunder","Sonja",""],["Wheelock","Ruta",""],["Friedman","Scott",""],["Chvasta","Alyssa",""],["Jalan","Zaria",""],["Saltz","Emily",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 19:11:08 GMT"}],"updateDate":"2024-08-05","timestamp":1722539468000,"abstract":"  With the growing prevalence of large language models, it is increasingly\ncommon to annotate datasets for machine learning using pools of crowd raters.\nHowever, these raters often work in isolation as individual crowdworkers. In\nthis work, we regard annotation not merely as inexpensive, scalable labor, but\nrather as a nuanced interpretative effort to discern the meaning of what is\nbeing said in a text. We describe a novel, collaborative, and iterative\nannotator-in-the-loop methodology for annotation, resulting in a 'Bridging\nBenchmark Dataset' of comments relevant to bridging divides, annotated from\n11,973 textual posts in the Civil Comments dataset. The methodology differs\nfrom popular anonymous crowd-rating annotation processes due to its use of an\nin-depth, iterative engagement with seven US-based raters to (1)\ncollaboratively refine the definitions of the to-be-annotated concepts and then\n(2) iteratively annotate complex social concepts, with check-in meetings and\ndiscussions. This approach addresses some shortcomings of current anonymous\ncrowd-based annotation work, and we present empirical evidence of the\nperformance of our annotation process in the form of inter-rater reliability.\nOur findings indicate that collaborative engagement with annotators can enhance\nannotation methods, as opposed to relying solely on isolated work conducted\nremotely. We provide an overview of the input texts, attributes, and annotation\nprocess, along with the empirical results and the resulting benchmark dataset,\ncategorized according to the following attributes: Alienation, Compassion,\nReasoning, Curiosity, Moral Outrage, and Respect.\n","subjects":["Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XTAJD6T7QBTKp29axxvFAn5RcUo7Mahn-amBe96Ch40","pdfSize":"138924","txDigest":"765vEEYsX3NrekStJ47BHfmFtb1ry2sgHNJ5wixex1gB","endEpoch":"1","status":"CERTIFIED"}
