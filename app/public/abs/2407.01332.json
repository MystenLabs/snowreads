{"id":"2407.01332","title":"AdaDistill: Adaptive Knowledge Distillation for Deep Face Recognition","authors":"Fadi Boutros, Vitomir \\v{S}truc and Naser Damer","authorsParsed":[["Boutros","Fadi",""],["Å truc","Vitomir",""],["Damer","Naser",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 14:39:55 GMT"}],"updateDate":"2024-07-02","timestamp":1719844795000,"abstract":"  Knowledge distillation (KD) aims at improving the performance of a compact\nstudent model by distilling the knowledge from a high-performing teacher model.\nIn this paper, we present an adaptive KD approach, namely AdaDistill, for deep\nface recognition. The proposed AdaDistill embeds the KD concept into the\nsoftmax loss by training the student using a margin penalty softmax loss with\ndistilled class centers from the teacher. Being aware of the relatively low\ncapacity of the compact student model, we propose to distill less complex\nknowledge at an early stage of training and more complex one at a later stage\nof training. This relative adjustment of the distilled knowledge is controlled\nby the progression of the learning capability of the student over the training\niterations without the need to tune any hyper-parameters. Extensive experiments\nand ablation studies show that AdaDistill can enhance the discriminative\nlearning capability of the student and demonstrate superiority over various\nstate-of-the-art competitors on several challenging benchmarks, such as IJB-B,\nIJB-C, and ICCV2021-MFR\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"XsxITDu8jAlRSbu9KHh_aQLbqeWCcvFoEVICawaCBLY","pdfSize":"1243819"}
