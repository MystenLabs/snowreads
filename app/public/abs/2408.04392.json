{"id":"2408.04392","title":"Open-domain Implicit Format Control for Large Language Model Generation","authors":"Yiqun Yao, Wenjia Ma, Xuezhi Fang, Xin Jiang, Xiang Li, Xuying Meng,\n  Peng Han, Jing Li, Aixin Sun, Yequan Wang","authorsParsed":[["Yao","Yiqun",""],["Ma","Wenjia",""],["Fang","Xuezhi",""],["Jiang","Xin",""],["Li","Xiang",""],["Meng","Xuying",""],["Han","Peng",""],["Li","Jing",""],["Sun","Aixin",""],["Wang","Yequan",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 11:51:45 GMT"}],"updateDate":"2024-08-09","timestamp":1723117905000,"abstract":"  Controlling the format of outputs generated by large language models (LLMs)\nis a critical functionality in various applications. Current methods typically\nemploy constrained decoding with rule-based automata or fine-tuning with\nmanually crafted format instructions, both of which struggle with open-domain\nformat requirements. To address this limitation, we introduce a novel framework\nfor controlled generation in LLMs, leveraging user-provided, one-shot QA pairs.\nThis study investigates LLMs' capabilities to follow open-domain, one-shot\nconstraints and replicate the format of the example answers. We observe that\nthis is a non-trivial problem for current LLMs. We also develop a dataset\ncollection methodology for supervised fine-tuning that enhances the open-domain\nformat control of LLMs without degrading output quality, as well as a benchmark\non which we evaluate both the helpfulness and format correctness of LLM\noutputs. The resulting datasets, named OIFC-SFT, along with the related code,\nwill be made publicly available at https://github.com/cofe-ai/OIFC.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}