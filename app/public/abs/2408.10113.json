{"id":"2408.10113","title":"Enhancing Reinforcement Learning Through Guided Search","authors":"J\\'er\\^ome Arjonilla, Abdallah Saffidine, Tristan Cazenave","authorsParsed":[["Arjonilla","Jérôme",""],["Saffidine","Abdallah",""],["Cazenave","Tristan",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 16:00:02 GMT"}],"updateDate":"2024-08-20","timestamp":1724083202000,"abstract":"  With the aim of improving performance in Markov Decision Problem in an\nOff-Policy setting, we suggest taking inspiration from what is done in Offline\nReinforcement Learning (RL). In Offline RL, it is a common practice during\npolicy learning to maintain proximity to a reference policy to mitigate\nuncertainty, reduce potential policy errors, and help improve performance. We\nfind ourselves in a different setting, yet it raises questions about whether a\nsimilar concept can be applied to enhance performance ie, whether it is\npossible to find a guiding policy capable of contributing to performance\nimprovement, and how to incorporate it into our RL agent. Our attention is\nparticularly focused on algorithms based on Monte Carlo Tree Search (MCTS) as a\nguide.MCTS renowned for its state-of-the-art capabilities across various\ndomains, catches our interest due to its ability to converge to equilibrium in\nsingle-player and two-player contexts. By harnessing the power of MCTS as a\nguide for our RL agent, we observed a significant performance improvement,\nsurpassing the outcomes achieved by utilizing each method in isolation. Our\nexperiments were carried out on the Atari 100k benchmark.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}