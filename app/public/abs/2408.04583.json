{"id":"2408.04583","title":"Unveiling the Power of Sparse Neural Networks for Feature Selection","authors":"Zahra Atashgahi, Tennison Liu, Mykola Pechenizkiy, Raymond Veldhuis,\n  Decebal Constantin Mocanu, Mihaela van der Schaar","authorsParsed":[["Atashgahi","Zahra",""],["Liu","Tennison",""],["Pechenizkiy","Mykola",""],["Veldhuis","Raymond",""],["Mocanu","Decebal Constantin",""],["van der Schaar","Mihaela",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 16:48:33 GMT"}],"updateDate":"2024-08-09","timestamp":1723135713000,"abstract":"  Sparse Neural Networks (SNNs) have emerged as powerful tools for efficient\nfeature selection. Leveraging the dynamic sparse training (DST) algorithms\nwithin SNNs has demonstrated promising feature selection capabilities while\ndrastically reducing computational overheads. Despite these advancements,\nseveral critical aspects remain insufficiently explored for feature selection.\nQuestions persist regarding the choice of the DST algorithm for network\ntraining, the choice of metric for ranking features/neurons, and the\ncomparative performance of these methods across diverse datasets when compared\nto dense networks. This paper addresses these gaps by presenting a\ncomprehensive systematic analysis of feature selection with sparse neural\nnetworks. Moreover, we introduce a novel metric considering sparse neural\nnetwork characteristics, which is designed to quantify feature importance\nwithin the context of SNNs. Our findings show that feature selection with SNNs\ntrained with DST algorithms can achieve, on average, more than $50\\%$ memory\nand $55\\%$ FLOPs reduction compared to the dense networks, while outperforming\nthem in terms of the quality of the selected features. Our code and the\nsupplementary material are available on GitHub\n(\\url{https://github.com/zahraatashgahi/Neuron-Attribution}).\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}