{"id":"2408.04909","title":"Surveying the Landscape of Image Captioning Evaluation: A Comprehensive\n  Taxonomy and Novel Ensemble Method","authors":"Uri Berger and Gabriel Stanovsky and Omri Abend and Lea Frermann","authorsParsed":[["Berger","Uri",""],["Stanovsky","Gabriel",""],["Abend","Omri",""],["Frermann","Lea",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 07:31:06 GMT"}],"updateDate":"2024-08-12","timestamp":1723188666000,"abstract":"  The task of image captioning has recently been gaining popularity, and with\nit the complex task of evaluating the quality of image captioning models. In\nthis work, we present the first survey and taxonomy of over 70 different image\ncaptioning metrics and their usage in hundreds of papers. We find that despite\nthe diversity of proposed metrics, the vast majority of studies rely on only\nfive popular metrics, which we show to be weakly correlated with human\njudgements. Instead, we propose EnsembEval -- an ensemble of evaluation methods\nachieving the highest reported correlation with human judgements across 5 image\ncaptioning datasets, showing there is a lot of room for improvement by\nleveraging a diverse set of metrics.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}