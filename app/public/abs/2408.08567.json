{"id":"2408.08567","title":"S$^3$Attention: Improving Long Sequence Attention with Smoothed Skeleton\n  Sketching","authors":"Xue Wang, Tian Zhou, Jianqing Zhu, Jialin Liu, Kun Yuan, Tao Yao,\n  Wotao Yin, Rong Jin, HanQin Cai","authorsParsed":[["Wang","Xue",""],["Zhou","Tian",""],["Zhu","Jianqing",""],["Liu","Jialin",""],["Yuan","Kun",""],["Yao","Tao",""],["Yin","Wotao",""],["Jin","Rong",""],["Cai","HanQin",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 07:01:46 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 04:53:11 GMT"},{"version":"v3","created":"Tue, 17 Sep 2024 17:30:46 GMT"}],"updateDate":"2024-09-18","timestamp":1723791706000,"abstract":"  Attention based models have achieved many remarkable breakthroughs in\nnumerous applications. However, the quadratic complexity of Attention makes the\nvanilla Attention based models hard to apply to long sequence tasks. Various\nimproved Attention structures are proposed to reduce the computation cost by\ninducing low rankness and approximating the whole sequence by sub-sequences.\nThe most challenging part of those approaches is maintaining the proper balance\nbetween information preservation and computation reduction: the longer\nsub-sequences used, the better information is preserved, but at the price of\nintroducing more noise and computational costs. In this paper, we propose a\nsmoothed skeleton sketching based Attention structure, coined S$^3$Attention,\nwhich significantly improves upon the previous attempts to negotiate this\ntrade-off. S$^3$Attention has two mechanisms to effectively minimize the impact\nof noise while keeping the linear complexity to the sequence length: a\nsmoothing block to mix information over long sequences and a matrix sketching\nmethod that simultaneously selects columns and rows from the input matrix. We\nverify the effectiveness of S$^3$Attention both theoretically and empirically.\nExtensive studies over Long Range Arena (LRA) datasets and six time-series\nforecasting show that S$^3$Attention significantly outperforms both vanilla\nAttention and other state-of-the-art variants of Attention structures.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Image and Video Processing","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}