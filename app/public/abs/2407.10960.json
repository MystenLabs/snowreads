{"id":"2407.10960","title":"Fast Matrix Multiplications for Lookup Table-Quantized LLMs","authors":"Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley,\n  Eric P. Xing, Yoon Kim","authorsParsed":[["Guo","Han",""],["Brandon","William",""],["Cholakov","Radostin",""],["Ragan-Kelley","Jonathan",""],["Xing","Eric P.",""],["Kim","Yoon",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 17:55:42 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 00:27:12 GMT"}],"updateDate":"2024-08-28","timestamp":1721066142000,"abstract":"  The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}