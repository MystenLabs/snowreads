{"id":"2407.21267","title":"DEF-oriCORN: efficient 3D scene understanding for robust\n  language-directed manipulation without demonstrations","authors":"Dongwon Son, Sanghyeon Son, Jaehyung Kim, Beomjoon Kim","authorsParsed":[["Son","Dongwon",""],["Son","Sanghyeon",""],["Kim","Jaehyung",""],["Kim","Beomjoon",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 01:13:25 GMT"}],"updateDate":"2024-08-01","timestamp":1722388405000,"abstract":"  We present DEF-oriCORN, a framework for language-directed manipulation tasks.\nBy leveraging a novel object-based scene representation and\ndiffusion-model-based state estimation algorithm, our framework enables\nefficient and robust manipulation planning in response to verbal commands, even\nin tightly packed environments with sparse camera views without any\ndemonstrations. Unlike traditional representations, our representation affords\nefficient collision checking and language grounding. Compared to\nstate-of-the-art baselines, our framework achieves superior estimation and\nmotion planning performance from sparse RGB images and zero-shot generalizes to\nreal-world scenarios with diverse materials, including transparent and\nreflective objects, despite being trained exclusively in simulation. Our code\nfor data generation, training, inference, and pre-trained weights are publicly\navailable at: https://sites.google.com/view/def-oricorn/home.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"tcuRe5vk05D0CuSU6AOEbpgDhDv6FlbEpHKw_vvEsf4","pdfSize":"13085750"}
