{"id":"2408.00129","title":"Vera Verto: Multimodal Hijacking Attack","authors":"Minxing Zhang, Ahmed Salem, Michael Backes, Yang Zhang","authorsParsed":[["Zhang","Minxing",""],["Salem","Ahmed",""],["Backes","Michael",""],["Zhang","Yang",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 19:37:06 GMT"}],"updateDate":"2024-08-02","timestamp":1722454626000,"abstract":"  The increasing cost of training machine learning (ML) models has led to the\ninclusion of new parties to the training pipeline, such as users who contribute\ntraining data and companies that provide computing resources. This involvement\nof such new parties in the ML training process has introduced new attack\nsurfaces for an adversary to exploit. A recent attack in this domain is the\nmodel hijacking attack, whereby an adversary hijacks a victim model to\nimplement their own -- possibly malicious -- hijacking tasks. However, the\nscope of the model hijacking attack is so far limited to the\nhomogeneous-modality tasks. In this paper, we transform the model hijacking\nattack into a more general multimodal setting, where the hijacking and original\ntasks are performed on data of different modalities. Specifically, we focus on\nthe setting where an adversary implements a natural language processing (NLP)\nhijacking task into an image classification model. To mount the attack, we\npropose a novel encoder-decoder based framework, namely the Blender, which\nrelies on advanced image and language models. Experimental results show that\nour modal hijacking attack achieves strong performances in different settings.\nFor instance, our attack achieves 94%, 94%, and 95% attack success rate when\nusing the Sogou news dataset to hijack STL10, CIFAR-10, and MNIST classifiers.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}