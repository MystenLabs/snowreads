{"id":"2407.01960","title":"Zero-shot Video Restoration and Enhancement Using Pre-Trained Image\n  Diffusion Model","authors":"Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang","authorsParsed":[["Cao","Cong",""],["Yue","Huanjing",""],["Liu","Xin",""],["Yang","Jingyu",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 05:31:59 GMT"}],"updateDate":"2024-07-03","timestamp":1719898319000,"abstract":"  Diffusion-based zero-shot image restoration and enhancement models have\nachieved great success in various image restoration and enhancement tasks\nwithout training. However, directly applying them to video restoration and\nenhancement results in severe temporal flickering artifacts. In this paper, we\npropose the first framework for zero-shot video restoration and enhancement\nbased on a pre-trained image diffusion model. By replacing the self-attention\nlayer with the proposed cross-previous-frame attention layer, the pre-trained\nimage diffusion model can take advantage of the temporal correlation between\nneighboring frames. We further propose temporal consistency guidance,\nspatial-temporal noise sharing, and an early stopping sampling strategy for\nbetter temporally consistent sampling. Our method is a plug-and-play module\nthat can be inserted into any diffusion-based zero-shot image restoration or\nenhancement methods to further improve their performance. Experimental results\ndemonstrate the superiority of our proposed method in producing temporally\nconsistent videos with better fidelity.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}