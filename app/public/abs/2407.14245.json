{"id":"2407.14245","title":"Dataset Distillation by Automatic Training Trajectories","authors":"Dai Liu, Jindong Gu, Hu Cao, Carsten Trinitis, Martin Schulz","authorsParsed":[["Liu","Dai",""],["Gu","Jindong",""],["Cao","Hu",""],["Trinitis","Carsten",""],["Schulz","Martin",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 12:27:11 GMT"}],"updateDate":"2024-07-22","timestamp":1721392031000,"abstract":"  Dataset Distillation is used to create a concise, yet informative, synthetic\ndataset that can replace the original dataset for training purposes. Some\nleading methods in this domain prioritize long-range matching, involving the\nunrolling of training trajectories with a fixed number of steps (NS) on the\nsynthetic dataset to align with various expert training trajectories. However,\ntraditional long-range matching methods possess an overfitting-like problem,\nthe fixed step size NS forces synthetic dataset to distortedly conform seen\nexpert training trajectories, resulting in a loss of generality-especially to\nthose from unencountered architecture. We refer to this as the Accumulated\nMismatching Problem (AMP), and propose a new approach, Automatic Training\nTrajectories (ATT), which dynamically and adaptively adjusts trajectory length\nNS to address the AMP. Our method outperforms existing methods particularly in\ntests involving cross-architectures. Moreover, owing to its adaptive nature, it\nexhibits enhanced stability in the face of parameter variations.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"oUgv7vs6vRE1Be9r20p9ka7T-1WcXLEuhTmTiBkYN-s","pdfSize":"7746015"}
