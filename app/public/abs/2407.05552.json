{"id":"2407.05552","title":"Ada-adapter:Fast Few-shot Style Personlization of Diffusion Model with\n  Pre-trained Image Encoder","authors":"Jia Liu and Changlin Li and Qirui Sun and Jiahui Ming and Chen Fang\n  and Jue Wang and Bing Zeng and Shuaicheng Liu","authorsParsed":[["Liu","Jia",""],["Li","Changlin",""],["Sun","Qirui",""],["Ming","Jiahui",""],["Fang","Chen",""],["Wang","Jue",""],["Zeng","Bing",""],["Liu","Shuaicheng",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 02:00:17 GMT"}],"updateDate":"2024-07-09","timestamp":1720404017000,"abstract":"  Fine-tuning advanced diffusion models for high-quality image stylization\nusually requires large training datasets and substantial computational\nresources, hindering their practical applicability. We propose Ada-Adapter, a\nnovel framework for few-shot style personalization of diffusion models.\nAda-Adapter leverages off-the-shelf diffusion models and pre-trained image\nfeature encoders to learn a compact style representation from a limited set of\nsource images. Our method enables efficient zero-shot style transfer utilizing\na single reference image. Furthermore, with a small number of source images\n(three to five are sufficient) and a few minutes of fine-tuning, our method can\ncapture intricate style details and conceptual characteristics, generating\nhigh-fidelity stylized images that align well with the provided text prompts.\nWe demonstrate the effectiveness of our approach on various artistic styles,\nincluding flat art, 3D rendering, and logo design. Our experimental results\nshow that Ada-Adapter outperforms existing zero-shot and few-shot stylization\nmethods in terms of output quality, diversity, and training efficiency.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}