{"id":"2407.16154","title":"DDK: Distilling Domain Knowledge for Efficient Large Language Models","authors":"Jiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang, Haoran Que,\n  Ken Deng, Zhiqi Bai, Jie Liu, Ge Zhang, Jiakai Wang, Yanan Wu, Congnan Liu,\n  Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng","authorsParsed":[["Liu","Jiaheng",""],["Zhang","Chenchen",""],["Guo","Jinyang",""],["Zhang","Yuanxing",""],["Que","Haoran",""],["Deng","Ken",""],["Bai","Zhiqi",""],["Liu","Jie",""],["Zhang","Ge",""],["Wang","Jiakai",""],["Wu","Yanan",""],["Liu","Congnan",""],["Su","Wenbo",""],["Wang","Jiamang",""],["Qu","Lin",""],["Zheng","Bo",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 03:47:28 GMT"}],"updateDate":"2024-07-24","timestamp":1721706448000,"abstract":"  Despite the advanced intelligence abilities of large language models (LLMs)\nin various applications, they still face significant computational and storage\ndemands. Knowledge Distillation (KD) has emerged as an effective strategy to\nimprove the performance of a smaller LLM (i.e., the student model) by\ntransferring knowledge from a high-performing LLM (i.e., the teacher model).\nPrevailing techniques in LLM distillation typically use a black-box model API\nto generate high-quality pretrained and aligned datasets, or utilize white-box\ndistillation by altering the loss function to better transfer knowledge from\nthe teacher LLM. However, these methods ignore the knowledge differences\nbetween the student and teacher LLMs across domains. This results in excessive\nfocus on domains with minimal performance gaps and insufficient attention to\ndomains with large gaps, reducing overall performance. In this paper, we\nintroduce a new LLM distillation framework called DDK, which dynamically\nadjusts the composition of the distillation dataset in a smooth manner\naccording to the domain performance differences between the teacher and student\nmodels, making the distillation process more stable and effective. Extensive\nevaluations show that DDK significantly improves the performance of student\nmodels, outperforming both continuously pretrained baselines and existing\nknowledge distillation methods by a large margin.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"Jc08Zqvd9RWQv5yERwip3XUecpsAGa1RJszvUcNrhw0","pdfSize":"960597"}
