{"id":"2408.12186","title":"Transformers are Minimax Optimal Nonparametric In-Context Learners","authors":"Juno Kim, Tai Nakamaki, Taiji Suzuki","authorsParsed":[["Kim","Juno",""],["Nakamaki","Tai",""],["Suzuki","Taiji",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 08:02:10 GMT"}],"updateDate":"2024-08-23","timestamp":1724313730000,"abstract":"  In-context learning (ICL) of large language models has proven to be a\nsurprisingly effective method of learning a new task from only a few\ndemonstrative examples. In this paper, we study the efficacy of ICL from the\nviewpoint of statistical learning theory. We develop approximation and\ngeneralization error bounds for a transformer composed of a deep neural network\nand one linear attention layer, pretrained on nonparametric regression tasks\nsampled from general function spaces including the Besov space and piecewise\n$\\gamma$-smooth class. We show that sufficiently trained transformers can\nachieve -- and even improve upon -- the minimax optimal estimation risk in\ncontext by encoding the most relevant basis representations during pretraining.\nOur analysis extends to high-dimensional or sequential data and distinguishes\nthe \\emph{pretraining} and \\emph{in-context} generalization gaps. Furthermore,\nwe establish information-theoretic lower bounds for meta-learners w.r.t. both\nthe number of tasks and in-context examples. These findings shed light on the\nroles of task diversity and representation learning for ICL.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}