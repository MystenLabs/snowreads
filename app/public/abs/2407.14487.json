{"id":"2407.14487","title":"Evaluating the Reliability of Self-Explanations in Large Language Models","authors":"Korbinian Randl, John Pavlopoulos, Aron Henriksson, and Tony Lindgren","authorsParsed":[["Randl","Korbinian",""],["Pavlopoulos","John",""],["Henriksson","Aron",""],["Lindgren","Tony",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:41:08 GMT"}],"updateDate":"2024-07-22","timestamp":1721410868000,"abstract":"  This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"KdJNVvJQ9XnEQ312txH03m8iAu5DVJzeMJO1KiODbew","pdfSize":"913520","objectId":"0x0b178baa7ab9b1a6362c4bc6022c6da3c102384accbca8464a250838ca6ea898","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
