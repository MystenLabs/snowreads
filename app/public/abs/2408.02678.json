{"id":"2408.02678","title":"Convergence rates of stochastic gradient method with independent\n  sequences of step-size and momentum weight","authors":"Wen-Liang Hwang","authorsParsed":[["Hwang","Wen-Liang",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 04:25:39 GMT"}],"updateDate":"2024-08-07","timestamp":1722399939000,"abstract":"  In large-scale learning algorithms, the momentum term is usually included in\nthe stochastic sub-gradient method to improve the learning speed because it can\nnavigate ravines efficiently to reach a local minimum. However, step-size and\nmomentum weight hyper-parameters must be appropriately tuned to optimize\nconvergence. We thus analyze the convergence rate using stochastic programming\nwith Polyak's acceleration of two commonly used step-size learning rates:\n``diminishing-to-zero\" and ``constant-and-drop\" (where the sequence is divided\ninto stages and a constant step-size is applied at each stage) under strongly\nconvex functions over a compact convex set with bounded sub-gradients. For the\nformer, we show that the convergence rate can be written as a product of\nexponential in step-size and polynomial in momentum weight. Our analysis\njustifies the convergence of using the default momentum weight setting and the\ndiminishing-to-zero step-size sequence in large-scale machine learning\nsoftware. For the latter, we present the condition for the momentum weight\nsequence to converge at each stage.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}