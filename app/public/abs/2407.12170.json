{"id":"2407.12170","title":"Neural Passage Quality Estimation for Static Pruning","authors":"Xuejun Chang, Debabrata Mishra, Craig Macdonald, Sean MacAvaney","authorsParsed":[["Chang","Xuejun",""],["Mishra","Debabrata",""],["Macdonald","Craig",""],["MacAvaney","Sean",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 20:47:54 GMT"}],"updateDate":"2024-07-18","timestamp":1721162874000,"abstract":"  Neural networks -- especially those that use large, pre-trained language\nmodels -- have improved search engines in various ways. Most prominently, they\ncan estimate the relevance of a passage or document to a user's query. In this\nwork, we depart from this direction by exploring whether neural networks can\neffectively predict which of a document's passages are unlikely to be relevant\nto any query submitted to the search engine. We refer to this query-agnostic\nestimation of passage relevance as a passage's quality. We find that our novel\nmethods for estimating passage quality allow passage corpora to be pruned\nconsiderably while maintaining statistically equivalent effectiveness; our best\nmethods can consistently prune >25% of passages in a corpora, across various\nretrieval pipelines. Such substantial pruning reduces the operating costs of\nneural search engines in terms of computing resources, power usage, and carbon\nfootprint -- both when processing queries (thanks to a smaller index size) and\nwhen indexing (lightweight models can prune low-quality passages prior to the\ncostly dense or learned sparse encoding step). This work sets the stage for\ndeveloping more advanced neural \"learning-what-to-index\" methods.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}