{"id":"2408.02128","title":"Table Transformers for Imputing Textual Attributes","authors":"Ting-Ruen Wei, Yuan Wang, Yoshitaka Inoue, Hsin-Tai Wu and Yi Fang","authorsParsed":[["Wei","Ting-Ruen",""],["Wang","Yuan",""],["Inoue","Yoshitaka",""],["Wu","Hsin-Tai",""],["Fang","Yi",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 19:54:12 GMT"}],"updateDate":"2024-08-06","timestamp":1722801252000,"abstract":"  Missing data in tabular dataset is a common issue as the performance of\ndownstream tasks usually depends on the completeness of the training dataset.\nPrevious missing data imputation methods focus on numeric and categorical\ncolumns, but we propose a novel end-to-end approach called Table Transformers\nfor Imputing Textual Attributes (TTITA) based on the transformer to impute\nunstructured textual columns using other columns in the table. We conduct\nextensive experiments on two Amazon Reviews datasets, and our approach shows\ncompetitive performance outperforming baseline models such as recurrent neural\nnetworks and Llama2. The performance improvement is more significant when the\ntarget sequence has a longer length. Additionally, we incorporated multi-task\nlearning to simultaneously impute for heterogeneous columns, boosting the\nperformance for text imputation. We also qualitatively compare with ChatGPT for\nrealistic applications.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6xyXvceUV12HUVHtO-JpqAByG8ZZQayGI6HPyCNx40o","pdfSize":"539805","txDigest":"9ZMwWRYM46hTZzbbSNHiyMDmeTk1ULkhhxMH2faVB2En","endEpoch":"1","status":"CERTIFIED"}
