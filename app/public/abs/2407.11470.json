{"id":"2407.11470","title":"Beyond Correctness: Benchmarking Multi-dimensional Code Generation for\n  Large Language Models","authors":"Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin,\n  Yaojie Lu, Xianpei Han, Le Sun","authorsParsed":[["Zheng","Jiasheng",""],["Cao","Boxi",""],["Ma","Zhengzhao",""],["Pan","Ruotong",""],["Lin","Hongyu",""],["Lu","Yaojie",""],["Han","Xianpei",""],["Sun","Le",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 08:08:48 GMT"}],"updateDate":"2024-07-17","timestamp":1721117328000,"abstract":"  In recent years, researchers have proposed numerous benchmarks to evaluate\nthe impressive coding capabilities of large language models (LLMs). However,\nexisting benchmarks primarily focus on assessing the correctness of code\ngenerated by LLMs, while neglecting other critical dimensions that also\nsignificantly impact code quality. Therefore, this paper proposes the RACE\nbenchmark, which comprehensively evaluates the quality of code generated by\nLLMs across 4 dimensions: Readability, mAintainability, Correctness, and\nEfficiency. Specifically, considering the demand-dependent nature of dimensions\nbeyond correctness, we design various types of user requirements for each\ndimension to assess the model's ability to generate correct code that also\nmeets user demands. We evaluate 18 representative LLMs on RACE and find that:\n1) the current LLMs' ability to generate high-quality code on demand does not\nyet meet the requirements of software development; 2) readability serves as a\ncritical indicator of the overall quality of generated code; 3) most LLMs\nexhibit an inherent preference for specific coding style. These findings can\nhelp researchers gain a deeper understanding of the coding capabilities of\ncurrent LLMs and shed light on future directions for model improvement.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}