{"id":"2407.11372","title":"UNIT: Backdoor Mitigation via Automated Neural Distribution Tightening","authors":"Siyuan Cheng, Guangyu Shen, Kaiyuan Zhang, Guanhong Tao, Shengwei An,\n  Hanxi Guo, Shiqing Ma, Xiangyu Zhang","authorsParsed":[["Cheng","Siyuan",""],["Shen","Guangyu",""],["Zhang","Kaiyuan",""],["Tao","Guanhong",""],["An","Shengwei",""],["Guo","Hanxi",""],["Ma","Shiqing",""],["Zhang","Xiangyu",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 04:33:05 GMT"}],"updateDate":"2024-07-17","timestamp":1721104385000,"abstract":"  Deep neural networks (DNNs) have demonstrated effectiveness in various\nfields. However, DNNs are vulnerable to backdoor attacks, which inject a unique\npattern, called trigger, into the input to cause misclassification to an\nattack-chosen target label. While existing works have proposed various methods\nto mitigate backdoor effects in poisoned models, they tend to be less effective\nagainst recent advanced attacks. In this paper, we introduce a novel\npost-training defense technique UNIT that can effectively eliminate backdoor\neffects for a variety of attacks. In specific, UNIT approximates a unique and\ntight activation distribution for each neuron in the model. It then proactively\ndispels substantially large activation values that exceed the approximated\nboundaries. Our experimental results demonstrate that UNIT outperforms 7\npopular defense methods against 14 existing backdoor attacks, including 2\nadvanced attacks, using only 5\\% of clean training data. UNIT is also cost\nefficient. The code is accessible at https://github.com/Megum1/UNIT.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}