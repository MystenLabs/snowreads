{"id":"2407.07024","title":"Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization","authors":"Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim","authorsParsed":[["Hyun","Jeongseok",""],["Han","Su Ho",""],["Kang","Hyolim",""],["Lee","Joon-Young",""],["Kim","Seon Joo",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 16:44:04 GMT"}],"updateDate":"2024-07-10","timestamp":1720543444000,"abstract":"  The vocabulary size in temporal action localization (TAL) is constrained by\nthe scarcity of large-scale annotated datasets. To address this, recent works\nincorporate powerful pre-trained vision-language models (VLMs), such as CLIP,\nto perform open-vocabulary TAL (OV-TAL). However, unlike VLMs trained on\nextensive image/video-text pairs, existing OV-TAL methods still rely on small,\nfully labeled TAL datasets for training an action localizer. In this paper, we\nexplore the scalability of self-training with unlabeled YouTube videos for\nOV-TAL. Our self-training approach consists of two stages. First, a\nclass-agnostic action localizer is trained on a human-labeled TAL dataset and\nused to generate pseudo-labels for unlabeled videos. Second, the large-scale\npseudo-labeled dataset is combined with the human-labeled dataset to train the\nlocalizer. Extensive experiments demonstrate that leveraging web-scale videos\nin self-training significantly enhances the generalizability of an action\nlocalizer. Additionally, we highlighted issues with existing OV-TAL evaluation\nschemes and proposed a new evaluation protocol. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"vopulTlOn-TLT0DDPiA91mGspigfo25rEnqFm5pwDug","pdfSize":"1041483"}
