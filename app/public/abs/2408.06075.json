{"id":"2408.06075","title":"Five Pitfalls When Assessing Synthetic Medical Images with Reference\n  Metrics","authors":"Melanie Dohmen, Tuan Truong, Ivo M. Baltruschat, Matthias Lenga","authorsParsed":[["Dohmen","Melanie",""],["Truong","Tuan",""],["Baltruschat","Ivo M.",""],["Lenga","Matthias",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 11:48:57 GMT"}],"updateDate":"2024-08-13","timestamp":1723463337000,"abstract":"  Reference metrics have been developed to objectively and quantitatively\ncompare two images. Especially for evaluating the quality of reconstructed or\ncompressed images, these metrics have shown very useful. Extensive tests of\nsuch metrics on benchmarks of artificially distorted natural images have\nrevealed which metric best correlate with human perception of quality. Direct\ntransfer of these metrics to the evaluation of generative models in medical\nimaging, however, can easily lead to pitfalls, because assumptions about image\ncontent, image data format and image interpretation are often very different.\nAlso, the correlation of reference metrics and human perception of quality can\nvary strongly for different kinds of distortions and commonly used metrics,\nsuch as SSIM, PSNR and MAE are not the best choice for all situations. We\nselected five pitfalls that showcase unexpected and probably undesired\nreference metric scores and discuss strategies to avoid them.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"9uSTIWDIpfvMkUFESuFLr6DnEEGRq99uX4cMC04is00","pdfSize":"570433"}
