{"id":"2407.16434","title":"Enhancing LLM's Cognition via Structurization","authors":"Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou,\n  Yaowu Chen, Yue Wu, Jieping Ye","authorsParsed":[["Liu","Kai",""],["Fu","Zhihang",""],["Chen","Chao",""],["Zhang","Wei",""],["Jiang","Rongxin",""],["Zhou","Fan",""],["Chen","Yaowu",""],["Wu","Yue",""],["Ye","Jieping",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 12:33:58 GMT"}],"updateDate":"2024-07-24","timestamp":1721738038000,"abstract":"  When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}