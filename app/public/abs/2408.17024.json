{"id":"2408.17024","title":"InkubaLM: A small language model for low-resource African languages","authors":"Atnafu Lambebo Tonja, Bonaventure F. P. Dossou, Jessica Ojo, Jenalea\n  Rajab, Fadel Thior, Eric Peter Wairagala, Anuoluwapo Aremu, Pelonomi Moiloa,\n  Jade Abbott, Vukosi Marivate, Benjamin Rosman","authorsParsed":[["Tonja","Atnafu Lambebo",""],["Dossou","Bonaventure F. P.",""],["Ojo","Jessica",""],["Rajab","Jenalea",""],["Thior","Fadel",""],["Wairagala","Eric Peter",""],["Aremu","Anuoluwapo",""],["Moiloa","Pelonomi",""],["Abbott","Jade",""],["Marivate","Vukosi",""],["Rosman","Benjamin",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 05:42:31 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 13:55:01 GMT"}],"updateDate":"2024-09-04","timestamp":1724996551000,"abstract":"  High-resource language models often fall short in the African context, where\nthere is a critical need for models that are efficient, accessible, and locally\nrelevant, even amidst significant computing and data constraints. This paper\nintroduces InkubaLM, a small language model with 0.4 billion parameters, which\nachieves performance comparable to models with significantly larger parameter\ncounts and more extensive training data on tasks such as machine translation,\nquestion-answering, AfriMMLU, and the AfriXnli task. Notably, InkubaLM\noutperforms many larger models in sentiment analysis and demonstrates\nremarkable consistency across multiple languages. This work represents a\npivotal advancement in challenging the conventional paradigm that effective\nlanguage models must rely on substantial resources. Our model and datasets are\npublicly available at https://huggingface.co/lelapa to encourage research and\ndevelopment on low-resource languages.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}