{"id":"2408.15091","title":"Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models","authors":"Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Wanli Ma, Ji Xiang,\n  Weiping Wang","authorsParsed":[["Liu","Xiyu",""],["Liu","Zhengxiao",""],["Gu","Naibin",""],["Lin","Zheng",""],["Ma","Wanli",""],["Xiang","Ji",""],["Wang","Weiping",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 14:22:02 GMT"}],"updateDate":"2024-08-28","timestamp":1724768522000,"abstract":"  The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on knowledge editing to avoid\nover-generalizing. Experimental results on the dataset supplemented with a new\nR-Specificity criterion demonstrate that our editing approach significantly\nalleviates over-generalizing while remaining competitive on other criteria,\nbreaking the domination of subject-focused editing for future research.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}