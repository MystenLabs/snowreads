{"id":"2408.07260","title":"MorphFader: Enabling Fine-grained Controllable Morphing with\n  Text-to-Audio Models","authors":"Purnima Kamath, Chitralekha Gupta, Suranga Nanayakkara","authorsParsed":[["Kamath","Purnima",""],["Gupta","Chitralekha",""],["Nanayakkara","Suranga",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 02:29:26 GMT"}],"updateDate":"2024-08-15","timestamp":1723602566000,"abstract":"  Sound morphing is the process of gradually and smoothly transforming one\nsound into another to generate novel and perceptually hybrid sounds that\nsimultaneously resemble both. Recently, diffusion-based text-to-audio models\nhave produced high-quality sounds using text prompts. However, granularly\ncontrolling the semantics of the sound, which is necessary for morphing, can be\nchallenging using text. In this paper, we propose \\textit{MorphFader}, a\ncontrollable method for morphing sounds generated by disparate prompts using\ntext-to-audio models. By intercepting and interpolating the components of the\ncross-attention layers within the diffusion process, we can create smooth\nmorphs between sounds generated by different text prompts. Using both objective\nmetrics and perceptual listening tests, we demonstrate the ability of our\nmethod to granularly control the semantics in the sound and generate smooth\nmorphs.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"aO7n2491374YVQUjdxWzBimEBNGnwzcMvORDnFWS37s","pdfSize":"1825543"}
