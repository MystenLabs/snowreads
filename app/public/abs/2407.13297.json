{"id":"2407.13297","title":"SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning","authors":"Joseph Marvin Imperial, Harish Tayyar Madabushi","authorsParsed":[["Imperial","Joseph Marvin",""],["Madabushi","Harish Tayyar",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 08:56:02 GMT"}],"updateDate":"2024-07-19","timestamp":1721292962000,"abstract":"  Specialized lexicons are collections of words with associated constraints\nsuch as special definitions, specific roles, and intended target audiences.\nThese constraints are necessary for content generation and documentation tasks\n(e.g., writing technical manuals or children's books), where the goal is to\nreduce the ambiguity of text content and increase its overall readability for a\nspecific group of audience. Understanding how large language models can capture\nthese constraints can help researchers build better, more impactful tools for\nwider use beyond the NLP community. Towards this end, we introduce SpeciaLex, a\nbenchmark for evaluating a language model's ability to follow specialized\nlexicon-based constraints across 18 diverse subtasks with 1,285 test instances\ncovering core tasks of Checking, Identification, Rewriting, and Open\nGeneration. We present an empirical evaluation of 15 open and closed-source\nLLMs and discuss insights on how factors such as model scale, openness, setup,\nand recency affect performance upon evaluating with the benchmark.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"Lpay2En62WRVU5PJ4B2tIps6QdgBIchgzWBxN76Kr7k","pdfSize":"538563"}
