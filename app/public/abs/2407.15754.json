{"id":"2407.15754","title":"LongVideoBench: A Benchmark for Long-context Interleaved Video-Language\n  Understanding","authors":"Haoning Wu, Dongxu Li, Bei Chen, Junnan Li","authorsParsed":[["Wu","Haoning",""],["Li","Dongxu",""],["Chen","Bei",""],["Li","Junnan",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 16:00:55 GMT"}],"updateDate":"2024-07-23","timestamp":1721664055000,"abstract":"  Large multimodal models (LMMs) are processing increasingly longer and richer\ninputs. Albeit the progress, few public benchmark is available to measure such\ndevelopment. To mitigate this gap, we introduce LongVideoBench, a\nquestion-answering benchmark that features video-language interleaved inputs up\nto an hour long. Our benchmark includes 3,763 varying-length web-collected\nvideos with their subtitles across diverse themes, designed to comprehensively\nevaluate LMMs on long-term multimodal understanding. To achieve this, we\ninterpret the primary challenge as to accurately retrieve and reason over\ndetailed multimodal information from long inputs. As such, we formulate a novel\nvideo question-answering task termed referring reasoning. Specifically, as part\nof the question, it contains a referring query that references related video\ncontexts, called referred context. The model is then required to reason over\nrelevant video details from the referred context. Following the paradigm of\nreferring reasoning, we curate 6,678 human-annotated multiple-choice questions\nin 17 fine-grained categories, establishing one of the most comprehensive\nbenchmarks for long-form video understanding. Evaluations suggest that the\nLongVideoBench presents significant challenges even for the most advanced\nproprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their\nopen-source counterparts show an even larger performance gap. In addition, our\nresults indicate that model performance on the benchmark improves only when\nthey are capable of processing more frames, positioning LongVideoBench as a\nvaluable benchmark for evaluating future-generation long-context LMMs.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}