{"id":"2408.14153","title":"Explaining Vision-Language Similarities in Dual Encoders with\n  Feature-Pair Attributions","authors":"Lucas M\\\"oller, Pascal Tilli, Ngoc Thang Vu, Sebastian Pad\\'o","authorsParsed":[["Möller","Lucas",""],["Tilli","Pascal",""],["Vu","Ngoc Thang",""],["Padó","Sebastian",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 09:55:34 GMT"}],"updateDate":"2024-08-27","timestamp":1724666134000,"abstract":"  Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and learn similarities between them. However, it is not\nunderstood how such models compare two inputs. Here, we address this research\ngap with two contributions. First, we derive a method to attribute predictions\nof any differentiable dual encoder onto feature-pair interactions between its\ninputs. Second, we apply our method to CLIP-type models and show that they\nlearn fine-grained correspondences between parts of captions and regions in\nimages. They match objects across input modes and also account for mismatches.\nHowever, this visual-linguistic grounding ability heavily varies between object\nclasses, depends on the training data distribution, and largely improves after\nin-domain training. Using our method we can identify knowledge gaps about\nspecific object classes in individual models and can monitor their improvement\nupon fine-tuning.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"j0MMynRlqPfh5-WaccGxVIOln3c9s-3NQWbrg6LKdFw","pdfSize":"7012056"}
