{"id":"2408.10124","title":"Molecular Graph Representation Learning Integrating Large Language\n  Models with Domain-specific Small Models","authors":"Tianyu Zhang and Yuxiang Ren and Chengbin Hou and Hairong Lv and\n  Xuegong Zhang","authorsParsed":[["Zhang","Tianyu",""],["Ren","Yuxiang",""],["Hou","Chengbin",""],["Lv","Hairong",""],["Zhang","Xuegong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 16:11:59 GMT"}],"updateDate":"2024-08-20","timestamp":1724083919000,"abstract":"  Molecular property prediction is a crucial foundation for drug discovery. In\nrecent years, pre-trained deep learning models have been widely applied to this\ntask. Some approaches that incorporate prior biological domain knowledge into\nthe pre-training framework have achieved impressive results. However, these\nmethods heavily rely on biochemical experts, and retrieving and summarizing\nvast amounts of domain knowledge literature is both time-consuming and\nexpensive. Large Language Models (LLMs) have demonstrated remarkable\nperformance in understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack precision in\ngenerating domain-specific knowledge. Conversely, Domain-specific Small Models\n(DSMs) possess rich domain knowledge and can accurately calculate molecular\ndomain-related metrics. However, due to their limited model size and singular\nfunctionality, they lack the breadth of knowledge necessary for comprehensive\nrepresentation learning. To leverage the advantages of both approaches in\nmolecular property prediction, we propose a novel Molecular Graph\nrepresentation learning framework that integrates Large language models and\nDomain-specific small models (MolGraph-LarDo). Technically, we design a\ntwo-stage prompt strategy where DSMs are introduced to calibrate the knowledge\nprovided by LLMs, enhancing the accuracy of domain-specific information and\nthus enabling LLMs to generate more precise textual descriptions for molecular\nsamples. Subsequently, we employ a multi-modal alignment method to coordinate\nvarious modalities, including molecular graphs and their corresponding\ndescriptive texts, to guide the pre-training of molecular representations.\nExtensive experiments demonstrate the effectiveness of the proposed method.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval","Physics/Chemical Physics","Quantitative Biology/Biomolecules"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}