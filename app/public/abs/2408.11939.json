{"id":"2408.11939","title":"Matmul or No Matmul in the Era of 1-bit LLMs","authors":"Jinendra Malekar, Mohammed E. Elbtity, Ramtin Zand","authorsParsed":[["Malekar","Jinendra",""],["Elbtity","Mohammed E.",""],["Zand","Ramtin",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 18:44:21 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 19:51:04 GMT"}],"updateDate":"2024-08-30","timestamp":1724265861000,"abstract":"  The advent of 1-bit large language models (LLMs) has attracted considerable\nattention and opened up new research opportunities. However, 1-bit LLMs only\nimprove a fraction of models by applying extreme quantization to the projection\nlayers while leaving attention heads unchanged. Therefore, to avoid\nfundamentally wrong choices of goals in future research, it is crucial to\nunderstand the actual improvements in computation and memory usage that 1-bit\nLLMs can deliver. In this work, we present an adaptation of Amdahl's Law\ntailored for the 1-bit LLM context, which illustrates how partial improvements\nin 1-bit LLMs impact overall model performance. Through extensive experiments,\nwe uncover key nuances across different model architectures and hardware\nconfigurations, offering a roadmap for future research in the era of 1-bit\nLLMs.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}