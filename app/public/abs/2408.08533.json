{"id":"2408.08533","title":"Unsupervised Transfer Learning via Adversarial Contrastive Training","authors":"Chenguang Duan, Yuling Jiao, Huazhen Lin, Wensen Ma, Jerry Zhijian\n  Yang","authorsParsed":[["Duan","Chenguang",""],["Jiao","Yuling",""],["Lin","Huazhen",""],["Ma","Wensen",""],["Yang","Jerry Zhijian",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 05:11:52 GMT"}],"updateDate":"2024-08-19","timestamp":1723785112000,"abstract":"  Learning a data representation for downstream supervised learning tasks under\nunlabeled scenario is both critical and challenging. In this paper, we propose\na novel unsupervised transfer learning approach using adversarial contrastive\ntraining (ACT). Our experimental results demonstrate outstanding classification\naccuracy with both fine-tuned linear probe and K-NN protocol across various\ndatasets, showing competitiveness with existing state-of-the-art\nself-supervised learning methods. Moreover, we provide an end-to-end\ntheoretical guarantee for downstream classification tasks in a misspecified,\nover-parameterized setting, highlighting how a large amount of unlabeled data\ncontributes to prediction accuracy. Our theoretical findings suggest that the\ntesting error of downstream tasks depends solely on the efficiency of data\naugmentation used in ACT when the unlabeled sample size is sufficiently large.\nThis offers a theoretical understanding of learning downstream tasks with a\nsmall sample size.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}