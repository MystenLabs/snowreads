{"id":"2408.04981","title":"Early Exit Strategies for Approximate k-NN Search in Dense Retrieval","authors":"Francesco Busolin, Claudio Lucchese, Franco Maria Nardini, Salvatore\n  Orlando, Raffaele Perego, Salvatore Trani","authorsParsed":[["Busolin","Francesco",""],["Lucchese","Claudio",""],["Nardini","Franco Maria",""],["Orlando","Salvatore",""],["Perego","Raffaele",""],["Trani","Salvatore",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 10:17:07 GMT"}],"updateDate":"2024-08-12","timestamp":1723198627000,"abstract":"  Learned dense representations are a popular family of techniques for encoding\nqueries and documents using high-dimensional embeddings, which enable retrieval\nby performing approximate k nearest-neighbors search (A-kNN). A popular\ntechnique for making A-kNN search efficient is based on a two-level index,\nwhere the embeddings of documents are clustered offline and, at query\nprocessing, a fixed number N of clusters closest to the query is visited\nexhaustively to compute the result set. In this paper, we build upon\nstate-of-the-art for early exit A-kNN and propose an unsupervised method based\non the notion of patience, which can reach competitive effectiveness with large\nefficiency gains. Moreover, we discuss a cascade approach where we first\nidentify queries that find their nearest neighbor within the closest t << N\nclusters, and then we decide how many more to visit based on our patience\napproach or other state-of-the-art strategies. Reproducible experiments\nemploying state-of-the-art dense retrieval models and publicly available\nresources show that our techniques improve the A-kNN efficiency with up to 5x\nspeedups while achieving negligible effectiveness losses. All the code used is\navailable at https://github.com/francescobusolin/faiss_pEE\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}