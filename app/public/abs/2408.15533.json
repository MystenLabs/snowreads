{"id":"2408.15533","title":"LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation","authors":"Haichuan Hu, Yuhan Sun, Quanjun Zhang","authorsParsed":[["Hu","Haichuan",""],["Sun","Yuhan",""],["Zhang","Quanjun",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 04:44:43 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 08:45:30 GMT"}],"updateDate":"2024-08-30","timestamp":1724820283000,"abstract":"  Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"vm8786E8h6zw35WSZw9iQg2Ye-VNXAcXXpKacsjplxA","pdfSize":"499366"}
