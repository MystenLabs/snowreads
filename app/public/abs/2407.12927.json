{"id":"2407.12927","title":"Textualized and Feature-based Models for Compound Multimodal Emotion\n  Recognition in the Wild","authors":"Nicolas Richet, Soufiane Belharbi, Haseeb Aslam, Meike Emilie Schadt,\n  Manuela Gonz\\'alez-Gonz\\'alez, Gustave Cortal, Alessandro Lameiras Koerich,\n  Marco Pedersoli, Alain Finkel, Simon Bacon, Eric Granger","authorsParsed":[["Richet","Nicolas",""],["Belharbi","Soufiane",""],["Aslam","Haseeb",""],["Schadt","Meike Emilie",""],["González-González","Manuela",""],["Cortal","Gustave",""],["Koerich","Alessandro Lameiras",""],["Pedersoli","Marco",""],["Finkel","Alain",""],["Bacon","Simon",""],["Granger","Eric",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 18:01:25 GMT"},{"version":"v2","created":"Thu, 1 Aug 2024 13:51:26 GMT"}],"updateDate":"2024-08-02","timestamp":1721239285000,"abstract":"  Systems for multimodal emotion recognition (ER) are commonly trained to\nextract features from different modalities (e.g., visual, audio, and textual)\nthat are combined to predict individual basic emotions. However, compound\nemotions often occur in real-world scenarios, and the uncertainty of\nrecognizing such complex emotions over diverse modalities is challenging for\nfeature-based models As an alternative, emerging multimodal large language\nmodels (LLMs) like BERT and LLaMA rely on explicit non-verbal cues that may be\ntranslated from different non-textual modalities (e.g., audio and visual) into\ntext. Textualization of modalities augments data with emotional cues to help\nthe LLM encode the interconnections between all modalities in a shared text\nspace. In such text-based models, prior knowledge of ER tasks is leveraged to\ntextualize relevant nonverbal cues such as audio tone from vocal expressions,\nand action unit intensity from facial expressions. Since the pre-trained\nweights are publicly available for many LLMs, training on large-scale datasets\nis unnecessary, allowing fine-tuning for downstream tasks such as compound ER\n(CER). This paper compares the potential of text- and feature-based approaches\nfor compound multimodal ER in videos. Experiments were conducted on the\nchallenging C-EXPR-DB dataset in the wild for CER, and contrasted with results\non the MELD dataset for basic ER. Our results indicate that multimodal\ntextualization provides lower accuracy than feature-based models on C-EXPR-DB,\nwhere text transcripts are captured in the wild. However, higher accuracy can\nbe achieved when the video data has rich transcripts. Our code is available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}