{"id":"2407.03856","title":"Q-Adapter: Training Your LLM Adapter as a Residual Q-Function","authors":"Yi-Chen Li, Fuxiang Zhang, Wenjie Qiu, Lei Yuan, Chengxing Jia,\n  Zongzhang Zhang, Yang Yu","authorsParsed":[["Li","Yi-Chen",""],["Zhang","Fuxiang",""],["Qiu","Wenjie",""],["Yuan","Lei",""],["Jia","Chengxing",""],["Zhang","Zongzhang",""],["Yu","Yang",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 11:42:36 GMT"}],"updateDate":"2024-07-08","timestamp":1720093356000,"abstract":"  We consider the problem of adapting Large Language Models (LLMs) pre-trained\nwith Reinforcement Learning from Human Feedback (RLHF) to downstream preference\ndata. Naive approaches to achieve this could be supervised fine-tuning on\npreferred responses or reinforcement learning with a learned reward model.\nHowever, the LLM runs the risk of forgetting its initial knowledge as the\nfine-tuning progresses. To customize the LLM while preserving its existing\ncapabilities, this paper proposes a novel method, named as Q-Adapter. We start\nby formalizing LLM adaptation as a problem of maximizing the linear combination\nof two rewards, one of which corresponds to the reward optimized by the\npre-trained LLM and the other to the downstream preference data. Although both\nrewards are unknown, we show that this can be solved by directly learning a new\nmodule from the preference data that approximates the \\emph{residual\nQ-function}. We consider this module to be an adapter because the original\npre-trained LLM, together with it, can form the optimal customised LLM.\nEmpirically, experiments on a range of domain-specific tasks and safety\nalignment tasks illustrate the superiority of Q-Adapter in both anti-forgetting\nand learning from new preferences.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"jiBWHY2uGun2zCE954tmDuEOqtM2WwujHzhuR-dHnQA","pdfSize":"527173"}
