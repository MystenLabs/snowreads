{"id":"2408.11974","title":"Two-Timescale Gradient Descent Ascent Algorithms for Nonconvex Minimax\n  Optimization","authors":"Tianyi Lin, Chi Jin and Michael. I. Jordan","authorsParsed":[["Lin","Tianyi",""],["Jin","Chi",""],["Jordan","Michael. I.",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 20:14:54 GMT"}],"updateDate":"2024-08-23","timestamp":1724271294000,"abstract":"  We provide a unified analysis of two-timescale gradient descent ascent\n(TTGDA) for solving structured nonconvex minimax optimization problems in the\nform of $\\min_\\textbf{x} \\max_{\\textbf{y} \\in Y} f(\\textbf{x}, \\textbf{y})$,\nwhere the objective function $f(\\textbf{x}, \\textbf{y})$ is nonconvex in\n$\\textbf{x}$ and concave in $\\textbf{y}$, and the constraint set $Y \\subseteq\n\\mathbb{R}^n$ is convex and bounded. In the convex-concave setting, the\nsingle-timescale GDA achieves strong convergence guarantees and has been used\nfor solving application problems arising from operations research and computer\nscience. However, it can fail to converge in more general settings. Our\ncontribution in this paper is to design the simple deterministic and stochastic\nTTGDA algorithms that efficiently find one stationary point of the function\n$\\Phi(\\cdot) := \\max_{\\textbf{y} \\in Y} f(\\cdot, \\textbf{y})$. Specifically, we\nprove the theoretical bounds on the complexity of solving both smooth and\nnonsmooth nonconvex-concave minimax optimization problems. To our knowledge,\nthis is the first systematic analysis of TTGDA for nonconvex minimax\noptimization, shedding light on its superior performance in training generative\nadversarial networks (GANs) and in solving other real-world application\nproblems.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}