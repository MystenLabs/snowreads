{"id":"2408.01147","title":"Actra: Optimized Transformer Architecture for Vision-Language-Action\n  Models in Robot Learning","authors":"Yueen Ma, Dafeng Chi, Shiguang Wu, Yuecheng Liu, Yuzheng Zhuang,\n  Jianye Hao, Irwin King","authorsParsed":[["Ma","Yueen",""],["Chi","Dafeng",""],["Wu","Shiguang",""],["Liu","Yuecheng",""],["Zhuang","Yuzheng",""],["Hao","Jianye",""],["King","Irwin",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 09:55:56 GMT"}],"updateDate":"2024-08-05","timestamp":1722592556000,"abstract":"  Vision-language-action models have gained significant attention for their\nability to model trajectories in robot learning. However, most existing models\nrely on Transformer models with vanilla causal attention, which we find\nsuboptimal for processing segmented multi-modal sequences. Additionally, the\nautoregressive generation approach falls short in generating multi-dimensional\nactions. In this paper, we introduce Actra, an optimized Transformer\narchitecture featuring trajectory attention and learnable action queries,\ndesigned for effective encoding and decoding of segmented\nvision-language-action trajectories in robot imitation learning. Furthermore,\nwe devise a multi-modal contrastive learning objective to explicitly align\ndifferent modalities, complementing the primary behavior cloning objective.\nThrough extensive experiments conducted across various environments, Actra\nexhibits substantial performance improvement when compared to state-of-the-art\nmodels in terms of generalizability, dexterity, and precision.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}