{"id":"2407.00023","title":"Preble: Efficient Distributed Prompt Scheduling for LLM Serving","authors":"Vikranth Srivatsa, Zijian He, Reyna Abhyankar, Dongming Li, Yiying\n  Zhang","authorsParsed":[["Srivatsa","Vikranth",""],["He","Zijian",""],["Abhyankar","Reyna",""],["Li","Dongming",""],["Zhang","Yiying",""]],"versions":[{"version":"v1","created":"Wed, 8 May 2024 06:30:58 GMT"}],"updateDate":"2024-07-02","timestamp":1715149858000,"abstract":"  Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices include\ndomain-specific instructions, illustration of tool usages, and long context,\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests, and their attention computation results can be\nreused. However, today's LLM serving systems treat every request in isolation,\nmissing the opportunity of computation reuse.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We perform a study on five popular\nLLM workloads. Based on our study results, we designed a distributed scheduling\nsystem that co-optimizes computation reuse and load balancing. Our evaluation\nof Preble on two to 8 GPUs with real workloads and request arrival patterns on\ntwo open-source LLM models shows that Preble outperforms the state-of-the-art\naverage latency by 1.5X to 14.5X and p99 by 2X to 10X.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1EgaT_5__1TL4oK90OtLUfB3-zuysoHdDWI3Thu31K8","pdfSize":"650834"}
