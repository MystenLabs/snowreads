{"id":"2408.01367","title":"Transformers are Universal In-context Learners","authors":"Takashi Furuya, Maarten V. de Hoop, Gabriel Peyr\\'e","authorsParsed":[["Furuya","Takashi",""],["de Hoop","Maarten V.",""],["Peyr√©","Gabriel",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 16:21:48 GMT"}],"updateDate":"2024-08-05","timestamp":1722615708000,"abstract":"  Transformers are deep architectures that define \"in-context mappings\" which\nenable predicting new tokens based on a given set of tokens (such as a prompt\nin NLP applications or a set of patches for vision transformers). This work\nstudies in particular the ability of these architectures to handle an\narbitrarily large number of context tokens. To mathematically and uniformly\naddress the expressivity of these architectures, we consider the case that the\nmappings are conditioned on a context represented by a probability distribution\nof tokens (discrete for a finite number of tokens). The related notion of\nsmoothness corresponds to continuity in terms of the Wasserstein distance\nbetween these contexts. We demonstrate that deep transformers are universal and\ncan approximate continuous in-context mappings to arbitrary precision,\nuniformly over compact token domains. A key aspect of our results, compared to\nexisting findings, is that for a fixed precision, a single transformer can\noperate on an arbitrary (even infinite) number of tokens. Additionally, it\noperates with a fixed embedding dimension of tokens (this dimension does not\nincrease with precision) and a fixed number of heads (proportional to the\ndimension). The use of MLP layers between multi-head attention layers is also\nexplicitly controlled.\n","subjects":["Computing Research Repository/Computation and Language","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}