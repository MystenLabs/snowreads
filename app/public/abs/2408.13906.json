{"id":"2408.13906","title":"ConVis: Contrastive Decoding with Hallucination Visualization for\n  Mitigating Hallucinations in Multimodal Large Language Models","authors":"Yeji Park, Deokyeong Lee, Junsuk Choe, Buru Chang","authorsParsed":[["Park","Yeji",""],["Lee","Deokyeong",""],["Choe","Junsuk",""],["Chang","Buru",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 18:02:36 GMT"}],"updateDate":"2024-08-27","timestamp":1724608956000,"abstract":"  Hallucinations in Multimodal Large Language Models (MLLMs) where generated\nresponses fail to accurately reflect the given image pose a significant\nchallenge to their reliability. To address this, we introduce ConVis, a novel\ntraining-free contrastive decoding method. ConVis leverages a text-to-image\n(T2I) generation model to semantically reconstruct the given image from\nhallucinated captions. By comparing the contrasting probability distributions\nproduced by the original and reconstructed images, ConVis enables MLLMs to\ncapture visual contrastive signals that penalize hallucination generation.\nNotably, this method operates purely within the decoding process, eliminating\nthe need for additional data or model updates. Our extensive experiments on\nfive popular benchmarks demonstrate that ConVis effectively reduces\nhallucinations across various MLLMs, highlighting its potential to enhance\nmodel reliability.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}