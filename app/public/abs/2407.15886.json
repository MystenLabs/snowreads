{"id":"2407.15886","title":"CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion\n  Models","authors":"Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang,\n  Xujie Zhang, Hanqing Zhao and Xiaodan Liang","authorsParsed":[["Chong","Zheng",""],["Dong","Xiao",""],["Li","Haoxiang",""],["Zhang","Shiyue",""],["Zhang","Wenqing",""],["Zhang","Xujie",""],["Zhao","Hanqing",""],["Liang","Xiaodan",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 11:58:53 GMT"}],"updateDate":"2024-07-24","timestamp":1721563133000,"abstract":"  Virtual try-on methods based on diffusion models achieve realistic try-on\neffects but often replicate the backbone network as a ReferenceNet or use\nadditional image encoders to process condition inputs, leading to high training\nand inference costs. In this work, we rethink the necessity of ReferenceNet and\nimage encoders and innovate the interaction between garment and person by\nproposing CatVTON, a simple and efficient virtual try-on diffusion model.\nCatVTON facilitates the seamless transfer of in-shop or worn garments of any\ncategory to target persons by simply concatenating them in spatial dimensions\nas inputs. The efficiency of our model is demonstrated in three aspects: (1)\nLightweight network: Only the original diffusion modules are used, without\nadditional network modules. The text encoder and cross-attentions for text\ninjection in the backbone are removed, reducing the parameters by 167.02M. (2)\nParameter-efficient training: We identified the try-on relevant modules through\nexperiments and achieved high-quality try-on effects by training only 49.57M\nparameters, approximately 5.51 percent of the backbone network's parameters.\n(3) Simplified inference: CatVTON eliminates all unnecessary conditions and\npreprocessing steps, including pose estimation, human parsing, and text input,\nrequiring only a garment reference, target person image, and mask for the\nvirtual try-on process. Extensive experiments demonstrate that CatVTON achieves\nsuperior qualitative and quantitative results with fewer prerequisites and\ntrainable parameters than baseline methods. Furthermore, CatVTON shows good\ngeneralization in in-the-wild scenarios despite using open-source datasets with\nonly 73K samples.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"Hg8XE9zptpSlFsRu3c_-XssXJGM8-rBCei12QB7Mrwc","pdfSize":"14963972"}
