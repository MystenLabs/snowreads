{"id":"2407.16714","title":"Masked Graph Learning with Recurrent Alignment for Multimodal Emotion\n  Recognition in Conversation","authors":"Tao Meng, Fuchen Zhang, Yuntao Shou, Hongen Shao, Wei Ai and Keqin Li","authorsParsed":[["Meng","Tao",""],["Zhang","Fuchen",""],["Shou","Yuntao",""],["Shao","Hongen",""],["Ai","Wei",""],["Li","Keqin",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 02:23:51 GMT"}],"updateDate":"2024-07-25","timestamp":1721701431000,"abstract":"  Since Multimodal Emotion Recognition in Conversation (MERC) can be applied to\npublic opinion monitoring, intelligent dialogue robots, and other fields, it\nhas received extensive research attention in recent years. Unlike traditional\nunimodal emotion recognition, MERC can fuse complementary semantic information\nbetween multiple modalities (e.g., text, audio, and vision) to improve emotion\nrecognition. However, previous work ignored the inter-modal alignment process\nand the intra-modal noise information before multimodal fusion but directly\nfuses multimodal features, which will hinder the model for representation\nlearning. In this study, we have developed a novel approach called Masked Graph\nLearning with Recursive Alignment (MGLRA) to tackle this problem, which uses a\nrecurrent iterative module with memory to align multimodal features, and then\nuses the masked GCN for multimodal feature fusion. First, we employ LSTM to\ncapture contextual information and use a graph attention-filtering mechanism to\neliminate noise effectively within the modality. Second, we build a recurrent\niteration module with a memory function, which can use communication between\ndifferent modalities to eliminate the gap between modalities and achieve the\npreliminary alignment of features between modalities. Then, a cross-modal\nmulti-head attention mechanism is introduced to achieve feature alignment\nbetween modalities and construct a masked GCN for multimodal feature fusion,\nwhich can perform random mask reconstruction on the nodes in the graph to\nobtain better node feature representation. Finally, we utilize a multilayer\nperceptron (MLP) for emotion recognition. Extensive experiments on two\nbenchmark datasets (i.e., IEMOCAP and MELD) demonstrate that {MGLRA}\noutperforms state-of-the-art methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"hanW4FhvspllNmgprF2aaQuq1LMSiAzrWlx7oOwzNAQ","pdfSize":"4260599"}
