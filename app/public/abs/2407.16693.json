{"id":"2407.16693","title":"Explanation Regularisation through the Lens of Attributions","authors":"Pedro Ferreira and Wilker Aziz and Ivan Titov","authorsParsed":[["Ferreira","Pedro",""],["Aziz","Wilker",""],["Titov","Ivan",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 17:56:32 GMT"}],"updateDate":"2024-07-24","timestamp":1721757392000,"abstract":"  Explanation regularisation (ER) has been introduced as a way to guide models\nto make their predictions in a manner more akin to humans, i.e., making their\nattributions \"plausible\". This is achieved by introducing an auxiliary\nexplanation loss, that measures how well the output of an input attribution\ntechnique for the model agrees with relevant human-annotated rationales. One\npositive outcome of using ER appears to be improved performance in\nout-of-domain (OOD) settings, presumably due to an increased reliance on\n\"plausible\" tokens. However, previous work has under-explored the impact of the\nER objective on model attributions, in particular when obtained with techniques\nother than the one used to train ER. In this work, we contribute a study of\nER's effectiveness at informing classification decisions on plausible tokens,\nand the relationship between increased plausibility and robustness to OOD\nconditions. Through a series of analyses, we find that the connection between\nER and the ability of a classifier to rely on plausible features has been\noverstated and that a stronger reliance on plausible tokens does not seem to be\nthe cause for any perceived OOD improvements.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}