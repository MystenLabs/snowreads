{"id":"2407.18581","title":"Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech\n  Recognition with Hierarchical Routing","authors":"Hukai Huang, Shenghui Lu, Yahui Shan, He Qu, Wenhao Guan, Qingyang\n  Hong, Lin Li","authorsParsed":[["Huang","Hukai",""],["Lu","Shenghui",""],["Shan","Yahui",""],["Qu","He",""],["Guan","Wenhao",""],["Hong","Qingyang",""],["Li","Lin",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 08:03:07 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 14:19:00 GMT"}],"updateDate":"2024-08-08","timestamp":1721980987000,"abstract":"  The Mixture of Experts (MoE) approach is well-suited for multilingual and\ncode-switching (CS) tasks due to its multi-expert architecture. This work\nintroduces the DLG-MoE, a Dynamic Language Group-based MoE optimized for\nbilingual and CS scenarios. DLG-MoE operates based on a hierarchical routing\nmechanism. First, the language router explicitly models the language and\ndispatches the representations to the corresponding language expert groups.\nSubsequently, the unsupervised router within each language group implicitly\nmodels attributes beyond language, and coordinates expert routing and\ncollaboration. The model achieves state-of-the-art (SOTA) performance while\nalso having unparalleled flexibility. It supports different top-k inference and\nstreaming capabilities, and can also prune the model parameters to obtain a\nmonolingual sub-model. The Code will be released.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}