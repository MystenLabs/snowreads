{"id":"2407.11510","title":"VoxBlink2: A 100K+ Speaker Recognition Corpus and the Open-Set\n  Speaker-Identification Benchmark","authors":"Yuke Lin, Ming Cheng, Fulin Zhang, Yingying Gao, Shilei Zhang, Ming Li","authorsParsed":[["Lin","Yuke",""],["Cheng","Ming",""],["Zhang","Fulin",""],["Gao","Yingying",""],["Zhang","Shilei",""],["Li","Ming",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 08:49:30 GMT"}],"updateDate":"2024-07-17","timestamp":1721119770000,"abstract":"  In this paper, we provide a large audio-visual speaker recognition dataset,\nVoxBlink2, which includes approximately 10M utterances with videos from 110K+\nspeakers in the wild. This dataset represents a significant expansion over the\nVoxBlink dataset, encompassing a broader diversity of speakers and scenarios by\nthe grace of an optimized data collection pipeline. Afterward, we explore the\nimpact of training strategies, data scale, and model complexity on speaker\nverification and finally establish a new single-model state-of-the-art EER at\n0.170% and minDCF at 0.006% on the VoxCeleb1-O test set. Such remarkable\nresults motivate us to explore speaker recognition from a new challenging\nperspective. We raise the Open-Set Speaker-Identification task, which is\ndesigned to either match a probe utterance with a known gallery speaker or\ncategorize it as an unknown query. Associated with this task, we design\nconcrete benchmark and evaluation protocols. The data and model resources can\nbe found in http://voxblink2.github.io.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}