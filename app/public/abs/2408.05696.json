{"id":"2408.05696","title":"SMILES-Mamba: Chemical Mamba Foundation Models for Drug ADMET Prediction","authors":"Bohao Xu, Yingzhou Lu, Chenhao Li, Ling Yue, Xiao Wang, Nan Hao,\n  Tianfan Fu, Jim Chen","authorsParsed":[["Xu","Bohao",""],["Lu","Yingzhou",""],["Li","Chenhao",""],["Yue","Ling",""],["Wang","Xiao",""],["Hao","Nan",""],["Fu","Tianfan",""],["Chen","Jim",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 04:53:12 GMT"}],"updateDate":"2024-08-13","timestamp":1723351992000,"abstract":"  In drug discovery, predicting the absorption, distribution, metabolism,\nexcretion, and toxicity (ADMET) properties of small-molecule drugs is critical\nfor ensuring safety and efficacy. However, the process of accurately predicting\nthese properties is often resource-intensive and requires extensive\nexperimental data. To address this challenge, we propose SMILES-Mamba, a\ntwo-stage model that leverages both unlabeled and labeled data through a\ncombination of self-supervised pretraining and fine-tuning strategies. The\nmodel first pre-trains on a large corpus of unlabeled SMILES strings to capture\nthe underlying chemical structure and relationships, before being fine-tuned on\nsmaller, labeled datasets specific to ADMET tasks. Our results demonstrate that\nSMILES-Mamba exhibits competitive performance across 22 ADMET datasets,\nachieving the highest score in 14 tasks, highlighting the potential of\nself-supervised learning in improving molecular property prediction. This\napproach not only enhances prediction accuracy but also reduces the dependence\non large, labeled datasets, offering a promising direction for future research\nin drug discovery.\n","subjects":["Computing Research Repository/Machine Learning","Quantitative Biology/Quantitative Methods"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}