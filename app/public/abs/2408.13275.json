{"id":"2408.13275","title":"An Information-Theoretic Approach to Generalization Theory","authors":"Borja Rodr\\'iguez-G\\'alvez, Ragnar Thobaben, Mikael Skoglund","authorsParsed":[["Rodríguez-Gálvez","Borja",""],["Thobaben","Ragnar",""],["Skoglund","Mikael",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 10:08:21 GMT"}],"updateDate":"2024-08-27","timestamp":1724148501000,"abstract":"  We investigate the in-distribution generalization of machine learning\nalgorithms. We depart from traditional complexity-based approaches by analyzing\ninformation-theoretic bounds that quantify the dependence between a learning\nalgorithm and the training data. We consider two categories of generalization\nguarantees:\n  1) Guarantees in expectation: These bounds measure performance in the average\ncase. Here, the dependence between the algorithm and the data is often captured\nby information measures. While these measures offer an intuitive\ninterpretation, they overlook the geometry of the algorithm's hypothesis class.\nHere, we introduce bounds using the Wasserstein distance to incorporate\ngeometry, and a structured, systematic method to derive bounds capturing the\ndependence between the algorithm and an individual datum, and between the\nalgorithm and subsets of the training data.\n  2) PAC-Bayesian guarantees: These bounds measure the performance level with\nhigh probability. Here, the dependence between the algorithm and the data is\noften measured by the relative entropy. We establish connections between the\nSeeger--Langford and Catoni's bounds, revealing that the former is optimized by\nthe Gibbs posterior. We introduce novel, tighter bounds for various types of\nloss functions. To achieve this, we introduce a new technique to optimize\nparameters in probabilistic statements.\n  To study the limitations of these approaches, we present a counter-example\nwhere most of the information-theoretic bounds fail while traditional\napproaches do not. Finally, we explore the relationship between privacy and\ngeneralization. We show that algorithms with a bounded maximal leakage\ngeneralize. For discrete data, we derive new bounds for differentially private\nalgorithms that guarantee generalization even with a constant privacy\nparameter, which is in contrast to previous bounds in the literature.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}