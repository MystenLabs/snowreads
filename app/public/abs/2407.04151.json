{"id":"2407.04151","title":"Securing Multi-turn Conversational Language Models Against Distributed\n  Backdoor Triggers","authors":"Terry Tong, Jiashu Xu, Qin Liu, Muhao Chen","authorsParsed":[["Tong","Terry",""],["Xu","Jiashu",""],["Liu","Qin",""],["Chen","Muhao",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 20:57:06 GMT"}],"updateDate":"2024-07-08","timestamp":1720126626000,"abstract":"  The security of multi-turn conversational large language models (LLMs) is\nunderstudied despite it being one of the most popular LLM utilization.\nSpecifically, LLMs are vulnerable to data poisoning backdoor attacks, where an\nadversary manipulates the training data to cause the model to output malicious\nresponses to predefined triggers. Specific to the multi-turn dialogue setting,\nLLMs are at the risk of even more harmful and stealthy backdoor attacks where\nthe backdoor triggers may span across multiple utterances, giving lee-way to\ncontext-driven attacks. In this paper, we explore a novel distributed backdoor\ntrigger attack that serves to be an extra tool in an adversary's toolbox that\ncan interface with other single-turn attack strategies in a plug and play\nmanner. Results on two representative defense mechanisms indicate that\ndistributed backdoor triggers are robust against existing defense strategies\nwhich are designed for single-turn user-model interactions, motivating us to\npropose a new defense strategy for the multi-turn dialogue setting that is more\nchallenging. To this end, we also explore a novel contrastive decoding based\ndefense that is able to mitigate the backdoor with a low computational\ntradeoff.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}