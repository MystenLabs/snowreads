{"id":"2407.07092","title":"V-VIPE: Variational View Invariant Pose Embedding","authors":"Mara Levy and Abhinav Shrivastava","authorsParsed":[["Levy","Mara",""],["Shrivastava","Abhinav",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 17:59:47 GMT"}],"updateDate":"2024-07-10","timestamp":1720547987000,"abstract":"  Learning to represent three dimensional (3D) human pose given a two\ndimensional (2D) image of a person, is a challenging problem. In order to make\nthe problem less ambiguous it has become common practice to estimate 3D pose in\nthe camera coordinate space. However, this makes the task of comparing two 3D\nposes difficult. In this paper, we address this challenge by separating the\nproblem of estimating 3D pose from 2D images into two steps. We use a\nvariational autoencoder (VAE) to find an embedding that represents 3D poses in\ncanonical coordinate space. We refer to this embedding as variational\nview-invariant pose embedding V-VIPE. Using V-VIPE we can encode 2D and 3D\nposes and use the embedding for downstream tasks, like retrieval and\nclassification. We can estimate 3D poses from these embeddings using the\ndecoder as well as generate unseen 3D poses. The variability of our encoding\nallows it to generalize well to unseen camera views when mapping from 2D space.\nTo the best of our knowledge, V-VIPE is the only representation to offer this\ndiversity of applications. Code and more information can be found at\nhttps://v-vipe.github.io/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3N_QY-TMrfNuiZjIeC5zb7Uht5qCMmRtYuJYKxpaON0","pdfSize":"4505078"}
