{"id":"2408.17003","title":"Safety Layers of Aligned Large Language Models: The Key to LLM Security","authors":"Shen Li, Liuyi Yao, Lan Zhang, Yaliang Li","authorsParsed":[["Li","Shen",""],["Yao","Liuyi",""],["Zhang","Lan",""],["Li","Yaliang",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 04:35:59 GMT"}],"updateDate":"2024-09-02","timestamp":1724992559000,"abstract":"  Aligned LLMs are highly secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nthis security is not well understood, further these models are vulnerable to\nsecurity degradation when fine-tuned with non-malicious backdoor data or normal\ndata. To address these challenges, our work uncovers the mechanism behind\nsecurity in aligned LLMs at the parameter level, identifying a small set of\ncontiguous layers in the middle of the model that are crucial for\ndistinguishing malicious queries from normal ones, referred to as \"safety\nlayers.\" We first confirm the existence of these safety layers by analyzing\nvariations in input vectors within the model's internal layers. Additionally,\nwe leverage the over-rejection phenomenon and parameters scaling analysis to\nprecisely locate the safety layers. Building on this understanding, we propose\na novel fine-tuning approach, Safely Partial-Parameter Fine-Tuning (SPPFT),\nthat fixes the gradient of the safety layers during fine-tuning to address the\nsecurity degradation. Our experiments demonstrate that this approach\nsignificantly preserves model security while maintaining performance and\nreducing computational resources compared to full fine-tuning.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}