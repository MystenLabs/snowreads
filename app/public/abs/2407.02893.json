{"id":"2407.02893","title":"An Uncertainty-guided Tiered Self-training Framework for Active\n  Source-free Domain Adaptation in Prostate Segmentation","authors":"Zihao Luo, Xiangde Luo, Zijun Gao and Guotai Wang","authorsParsed":[["Luo","Zihao",""],["Luo","Xiangde",""],["Gao","Zijun",""],["Wang","Guotai",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 08:13:16 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 08:11:49 GMT"}],"updateDate":"2024-07-08","timestamp":1719994396000,"abstract":"  Deep learning models have exhibited remarkable efficacy in accurately\ndelineating the prostate for diagnosis and treatment of prostate diseases, but\nchallenges persist in achieving robust generalization across different medical\ncenters. Source-free Domain Adaptation (SFDA) is a promising technique to adapt\ndeep segmentation models to address privacy and security concerns while\nreducing domain shifts between source and target domains. However, recent\nliterature indicates that the performance of SFDA remains far from satisfactory\ndue to unpredictable domain gaps. Annotating a few target domain samples is\nacceptable, as it can lead to significant performance improvement with a low\nannotation cost. Nevertheless, due to extremely limited annotation budgets,\ncareful consideration is needed in selecting samples for annotation. Inspired\nby this, our goal is to develop Active Source-free Domain Adaptation (ASFDA)\nfor medical image segmentation. Specifically, we propose a novel\nUncertainty-guided Tiered Self-training (UGTST) framework, consisting of\nefficient active sample selection via entropy-based primary local peak\nfiltering to aggregate global uncertainty and diversity-aware redundancy\nfilter, coupled with a tiered self-learning strategy, achieves stable domain\nadaptation. Experimental results on cross-center prostate MRI segmentation\ndatasets revealed that our method yielded marked advancements, with a mere 5%\nannotation, exhibiting an average Dice score enhancement of 9.78% and 7.58% in\ntwo target domains compared with state-of-the-art methods, on par with fully\nsupervised learning. Code is available at:https://github.com/HiLab-git/UGTST\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}