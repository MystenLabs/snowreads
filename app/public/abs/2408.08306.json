{"id":"2408.08306","title":"Accelerated Image-Aware Generative Diffusion Modeling","authors":"Tanmay Asthana and Yufang Bao and Hamid Krim","authorsParsed":[["Asthana","Tanmay",""],["Bao","Yufang",""],["Krim","Hamid",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:58:55 GMT"}],"updateDate":"2024-08-16","timestamp":1723744735000,"abstract":"  We propose in this paper an analytically new construct of a diffusion model\nwhose drift and diffusion parameters yield an exponentially time-decaying\nSignal to Noise Ratio in the forward process. In reverse, the construct\ncleverly carries out the learning of the diffusion coefficients on the\nstructure of clean images using an autoencoder. The proposed methodology\nsignificantly accelerates the diffusion process, reducing the required\ndiffusion time steps from around 1000 seen in conventional models to 200-500\nwithout compromising image quality in the reverse-time diffusion. In a\ndeparture from conventional models which typically use time-consuming multiple\nruns, we introduce a parallel data-driven model to generate a reverse-time\ndiffusion trajectory in a single run of the model. The resulting collective\nblock-sequential generative model eliminates the need for MCMC-based\nsub-sampling correction for safeguarding and improving image quality, to\nfurther improve the acceleration of image generation. Collectively, these\nadvancements yield a generative model that is an order of magnitude faster than\nconventional approaches, while maintaining high fidelity and diversity in\ngenerated images, hence promising widespread applicability in rapid image\nsynthesis tasks.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}