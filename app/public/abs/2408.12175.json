{"id":"2408.12175","title":"How disentangled are your classification uncertainties?","authors":"Ivo Pascal de Jong, Andreea Ioana Sburlea, Matias Valdenegro-Toro","authorsParsed":[["de Jong","Ivo Pascal",""],["Sburlea","Andreea Ioana",""],["Valdenegro-Toro","Matias",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 07:42:43 GMT"}],"updateDate":"2024-08-23","timestamp":1724312563000,"abstract":"  Uncertainty Quantification in Machine Learning has progressed to predicting\nthe source of uncertainty in a prediction: Uncertainty from stochasticity in\nthe data (aleatoric), or uncertainty from limitations of the model (epistemic).\nGenerally, each uncertainty is evaluated in isolation, but this obscures the\nfact that they are often not truly disentangled. This work proposes a set of\nexperiments to evaluate disentanglement of aleatoric and epistemic uncertainty,\nand uses these methods to compare two competing formulations for\ndisentanglement (the Information Theoretic approach, and the Gaussian Logits\napproach). The results suggest that the Information Theoretic approach gives\nbetter disentanglement, but that either predicted source of uncertainty is\nstill largely contaminated by the other for both methods. We conclude that with\nthe current methods for disentangling, aleatoric and epistemic uncertainty are\nnot reliably separated, and we provide a clear set of experimental criteria\nthat good uncertainty disentanglement should follow.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}