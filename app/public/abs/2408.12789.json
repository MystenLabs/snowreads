{"id":"2408.12789","title":"Context-Aware Temporal Embedding of Objects in Video Data","authors":"Ahnaf Farhan and M. Shahriar Hossain","authorsParsed":[["Farhan","Ahnaf",""],["Hossain","M. Shahriar",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 01:44:10 GMT"}],"updateDate":"2024-08-26","timestamp":1724377450000,"abstract":"  In video analysis, understanding the temporal context is crucial for\nrecognizing object interactions, event patterns, and contextual changes over\ntime. The proposed model leverages adjacency and semantic similarities between\nobjects from neighboring video frames to construct context-aware temporal\nobject embeddings. Unlike traditional methods that rely solely on visual\nappearance, our temporal embedding model considers the contextual relationships\nbetween objects, creating a meaningful embedding space where temporally\nconnected object's vectors are positioned in proximity. Empirical studies\ndemonstrate that our context-aware temporal embeddings can be used in\nconjunction with conventional visual embeddings to enhance the effectiveness of\ndownstream applications. Moreover, the embeddings can be used to narrate a\nvideo using a Large Language Model (LLM). This paper describes the intricate\ndetails of the proposed objective function to generate context-aware temporal\nobject embeddings for video data and showcases the potential applications of\nthe generated embeddings in video analysis and object classification tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}