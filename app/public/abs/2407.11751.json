{"id":"2407.11751","title":"Why long model-based rollouts are no reason for bad Q-value estimates","authors":"Philipp Wissmann, Daniel Hein, Steffen Udluft, Volker Tresp","authorsParsed":[["Wissmann","Philipp",""],["Hein","Daniel",""],["Udluft","Steffen",""],["Tresp","Volker",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:17:00 GMT"}],"updateDate":"2024-07-17","timestamp":1721139420000,"abstract":"  This paper explores the use of model-based offline reinforcement learning\nwith long model rollouts. While some literature criticizes this approach due to\ncompounding errors, many practitioners have found success in real-world\napplications. The paper aims to demonstrate that long rollouts do not\nnecessarily result in exponentially growing errors and can actually produce\nbetter Q-value estimates than model-free methods. These findings can\npotentially enhance reinforcement learning techniques.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}