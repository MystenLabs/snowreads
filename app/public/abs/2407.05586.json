{"id":"2407.05586","title":"Dynamic Neural Radiance Field From Defocused Monocular Video","authors":"Xianrui Luo, Huiqiang Sun, Juewen Peng, Zhiguo Cao","authorsParsed":[["Luo","Xianrui",""],["Sun","Huiqiang",""],["Peng","Juewen",""],["Cao","Zhiguo",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 03:46:56 GMT"},{"version":"v2","created":"Wed, 31 Jul 2024 06:04:25 GMT"}],"updateDate":"2024-08-01","timestamp":1720410416000,"abstract":"  Dynamic Neural Radiance Field (NeRF) from monocular videos has recently been\nexplored for space-time novel view synthesis and achieved excellent results.\nHowever, defocus blur caused by depth variation often occurs in video capture,\ncompromising the quality of dynamic reconstruction because the lack of sharp\ndetails interferes with modeling temporal consistency between input views. To\ntackle this issue, we propose D2RF, the first dynamic NeRF method designed to\nrestore sharp novel views from defocused monocular videos. We introduce layered\nDepth-of-Field (DoF) volume rendering to model the defocus blur and reconstruct\na sharp NeRF supervised by defocused views. The blur model is inspired by the\nconnection between DoF rendering and volume rendering. The opacity in volume\nrendering aligns with the layer visibility in DoF rendering. To execute the\nblurring, we modify the layered blur kernel to the ray-based kernel and employ\nan optimized sparse kernel to gather the input rays efficiently and render the\noptimized rays with our layered DoF volume rendering. We synthesize a dataset\nwith defocused dynamic scenes for our task, and extensive experiments on our\ndataset show that our method outperforms existing approaches in synthesizing\nall-in-focus novel views from defocus blur while maintaining spatial-temporal\nconsistency in the scene.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}