{"id":"2407.18541","title":"Towards Improving NAM-to-Speech Synthesis Intelligibility using\n  Self-Supervised Speech Models","authors":"Neil Shah, Shirish Karande, Vineet Gandhi","authorsParsed":[["Shah","Neil",""],["Karande","Shirish",""],["Gandhi","Vineet",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 06:44:01 GMT"}],"updateDate":"2024-07-29","timestamp":1721976241000,"abstract":"  We propose a novel approach to significantly improve the intelligibility in\nthe Non-Audible Murmur (NAM)-to-speech conversion task, leveraging\nself-supervision and sequence-to-sequence (Seq2Seq) learning techniques. Unlike\nconventional methods that explicitly record ground-truth speech, our\nmethodology relies on self-supervision and speech-to-speech synthesis to\nsimulate ground-truth speech. Despite utilizing simulated speech, our method\nsurpasses the current state-of-the-art (SOTA) by 29.08% improvement in the\nMel-Cepstral Distortion (MCD) metric. Additionally, we present error rates and\ndemonstrate our model's proficiency to synthesize speech in novel voices of\ninterest. Moreover, we present a methodology for augmenting the existing CSTR\nNAM TIMIT Plus corpus, setting a benchmark with a Word Error Rate (WER) of\n42.57% to gauge the intelligibility of the synthesized speech. Speech samples\ncan be found at https://nam2speech.github.io/NAM2Speech/\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"j9OUNPbTuBzXoEW60F2RSl-QgzG8SgKVY8QTgBcqAqc","pdfSize":"320502","objectId":"0x85f152a25beef4c18246d65a781c1089acc99881342b5d2cb80f069908d6bb5c","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
