{"id":"2408.00147","title":"Formal Ethical Obligations in Reinforcement Learning Agents:\n  Verification and Policy Updates","authors":"Colin Shea-Blymyer, Houssam Abbas","authorsParsed":[["Shea-Blymyer","Colin",""],["Abbas","Houssam",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 20:21:15 GMT"}],"updateDate":"2024-08-02","timestamp":1722457275000,"abstract":"  When designing agents for operation in uncertain environments, designers need\ntools to automatically reason about what agents ought to do, how that conflicts\nwith what is actually happening, and how a policy might be modified to remove\nthe conflict. These obligations include ethical and social obligations,\npermissions and prohibitions, which constrain how the agent achieves its\nmission and executes its policy. We propose a new deontic logic, Expected Act\nUtilitarian deontic logic, for enabling this reasoning at design time: for\nspecifying and verifying the agent's strategic obligations, then modifying its\npolicy from a reference policy to meet those obligations. Unlike approaches\nthat work at the reward level, working at the logical level increases the\ntransparency of the trade-offs. We introduce two algorithms: one for\nmodel-checking whether an RL agent has the right strategic obligations, and one\nfor modifying a reference decision policy to make it meet obligations expressed\nin our logic. We illustrate our algorithms on DAC-MDPs which accurately\nabstract neural decision policies, and on toy gridworld environments.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Logic in Computer Science"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"UevchmS-VcOJdAcCD4vI6HwvMrv11S-bRFhFZwZEjdU","pdfSize":"624570"}
