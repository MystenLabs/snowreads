{"id":"2408.11382","title":"On the Interchangeability of Positional Embeddings in Multilingual\n  Neural Machine Translation Models","authors":"Varun Gumma and Pranjal A. Chitale and Kalika Bali","authorsParsed":[["Gumma","Varun",""],["Chitale","Pranjal A.",""],["Bali","Kalika",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 07:23:34 GMT"}],"updateDate":"2024-08-22","timestamp":1724225014000,"abstract":"  Standard Neural Machine Translation (NMT) models have traditionally been\ntrained with Sinusoidal Positional Embeddings (PEs), which are inadequate for\ncapturing long-range dependencies and are inefficient for long-context or\ndocument-level translation. In contrast, state-of-the-art large language models\n(LLMs) employ relative PEs, demonstrating superior length generalization. This\nwork explores the potential for efficiently switching the Positional Embeddings\nof pre-trained NMT models from absolute sinusoidal PEs to relative approaches\nsuch as RoPE and ALiBi. Our findings reveal that sinusoidal PEs can be\neffectively replaced with RoPE and ALiBi with negligible or no performance\nloss, achieved by fine-tuning on a small fraction of high-quality data.\nAdditionally, models trained without Positional Embeddings (NoPE) are not a\nviable solution for Encoder-Decoder architectures, as they consistently\nunder-perform compared to models utilizing any form of Positional Embedding.\nFurthermore, even a model trained from scratch with these relative PEs slightly\nunder-performs a fine-tuned model, underscoring the efficiency and validity of\nour hypothesis.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"Xjlc8jB5Umqcx_5QNBiya_gBkMM-GHSW7gba-WFHtJQ","pdfSize":"296392"}
