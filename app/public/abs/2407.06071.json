{"id":"2407.06071","title":"From Loops to Oops: Fallback Behaviors of Language Models Under\n  Uncertainty","authors":"Maor Ivgi, Ori Yoran, Jonathan Berant, Mor Geva","authorsParsed":[["Ivgi","Maor",""],["Yoran","Ori",""],["Berant","Jonathan",""],["Geva","Mor",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 16:13:42 GMT"}],"updateDate":"2024-07-09","timestamp":1720455222000,"abstract":"  Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under uncertainty, and investigate the connection\nbetween them. We categorize fallback behaviors -- sequence repetitions,\ndegenerate text, and hallucinations -- and extensively analyze them in models\nfrom the same family that differ by the amount of pretraining tokens, parameter\ncount, or the inclusion of instruction-following training. Our experiments\nreveal a clear and consistent ordering of fallback behaviors, across all these\naxes: the more advanced an LLM is (i.e., trained on more tokens, has more\nparameters, or instruction-tuned), its fallback behavior shifts from sequence\nrepetitions, to degenerate text, and then to hallucinations. Moreover, the same\nordering is observed throughout a single generation, even for the\nbest-performing models; as uncertainty increases, models shift from generating\nhallucinations to producing degenerate text and then sequence repetitions.\nLastly, we demonstrate that while common decoding techniques, such as random\nsampling, might alleviate some unwanted behaviors like sequence repetitions,\nthey increase harder-to-detect hallucinations.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}