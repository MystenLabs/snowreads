{"id":"2408.10491","title":"Achieving the Tightest Relaxation of Sigmoids for Formal Verification","authors":"Samuel Chevalier, Duncan Starkenburg, Krishnamurthy Dvijotham","authorsParsed":[["Chevalier","Samuel",""],["Starkenburg","Duncan",""],["Dvijotham","Krishnamurthy",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 02:22:27 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 00:10:24 GMT"}],"updateDate":"2024-08-23","timestamp":1724120547000,"abstract":"  In the field of formal verification, Neural Networks (NNs) are typically\nreformulated into equivalent mathematical programs which are optimized over. To\novercome the inherent non-convexity of these reformulations, convex relaxations\nof nonlinear activation functions are typically utilized. Common relaxations\n(i.e., static linear cuts) of \"S-shaped\" activation functions, however, can be\noverly loose, slowing down the overall verification process. In this paper, we\nderive tuneable hyperplanes which upper and lower bound the sigmoid activation\nfunction. When tuned in the dual space, these affine bounds smoothly rotate\naround the nonlinear manifold of the sigmoid activation function. This\napproach, termed $\\alpha$-sig, allows us to tractably incorporate the tightest\npossible, element-wise convex relaxation of the sigmoid activation function\ninto a formal verification framework. We embed these relaxations inside of\nlarge verification tasks and compare their performance to LiRPA and\n$\\alpha$-CROWN, a state-of-the-art verification duo.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}