{"id":"2407.05420","title":"Towards Bridging the Cross-modal Semantic Gap for Multi-modal\n  Recommendation","authors":"Xinglong Wu, Anfeng Huang, Hongwei Yang, Hui He, Yu Tai, Weizhe Zhang","authorsParsed":[["Wu","Xinglong",""],["Huang","Anfeng",""],["Yang","Hongwei",""],["He","Hui",""],["Tai","Yu",""],["Zhang","Weizhe",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 15:56:03 GMT"}],"updateDate":"2024-07-09","timestamp":1720367763000,"abstract":"  Multi-modal recommendation greatly enhances the performance of recommender\nsystems by modeling the auxiliary information from multi-modality contents.\nMost existing multi-modal recommendation models primarily exploit multimedia\ninformation propagation processes to enrich item representations and directly\nutilize modal-specific embedding vectors independently obtained from upstream\npre-trained models. However, this might be inappropriate since the abundant\ntask-specific semantics remain unexplored, and the cross-modality semantic gap\nhinders the recommendation performance.\n  Inspired by the recent progress of the cross-modal alignment model CLIP, in\nthis paper, we propose a novel \\textbf{CLIP} \\textbf{E}nhanced\n\\textbf{R}ecommender (\\textbf{CLIPER}) framework to bridge the semantic gap\nbetween modalities and extract fine-grained multi-view semantic information.\nSpecifically, we introduce a multi-view modality-alignment approach for\nrepresentation extraction and measure the semantic similarity between\nmodalities. Furthermore, we integrate the multi-view multimedia representations\ninto downstream recommendation models. Extensive experiments conducted on three\npublic datasets demonstrate the consistent superiority of our model over\nstate-of-the-art multi-modal recommendation models.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}