{"id":"2408.16440","title":"Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain","authors":"Miguel Rios","authorsParsed":[["Rios","Miguel",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 11:05:54 GMT"}],"updateDate":"2024-08-30","timestamp":1724929554000,"abstract":"  Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DLWThTTHixZfI5g9CPQomdYi89ldBboWhxzDudfX-sU","pdfSize":"137563"}
