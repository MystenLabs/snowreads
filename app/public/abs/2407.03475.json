{"id":"2407.03475","title":"How JEPA Avoids Noisy Features: The Implicit Bias of Deep Linear Self\n  Distillation Networks","authors":"Etai Littwin, Omid Saremi, Madhu Advani, Vimal Thilak, Preetum\n  Nakkiran, Chen Huang, Joshua Susskind","authorsParsed":[["Littwin","Etai",""],["Saremi","Omid",""],["Advani","Madhu",""],["Thilak","Vimal",""],["Nakkiran","Preetum",""],["Huang","Chen",""],["Susskind","Joshua",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 19:43:12 GMT"}],"updateDate":"2024-07-08","timestamp":1720035792000,"abstract":"  Two competing paradigms exist for self-supervised learning of data\nrepresentations. Joint Embedding Predictive Architecture (JEPA) is a class of\narchitectures in which semantically similar inputs are encoded into\nrepresentations that are predictive of each other. A recent successful approach\nthat falls under the JEPA framework is self-distillation, where an online\nencoder is trained to predict the output of the target encoder, sometimes using\na lightweight predictor network. This is contrasted with the Masked AutoEncoder\n(MAE) paradigm, where an encoder and decoder are trained to reconstruct missing\nparts of the input in the data space rather, than its latent representation. A\ncommon motivation for using the JEPA approach over MAE is that the JEPA\nobjective prioritizes abstract features over fine-grained pixel information\n(which can be unpredictable and uninformative). In this work, we seek to\nunderstand the mechanism behind this empirical observation by analyzing the\ntraining dynamics of deep linear models. We uncover a surprising mechanism: in\na simplified linear setting where both approaches learn similar\nrepresentations, JEPAs are biased to learn high-influence features, i.e.,\nfeatures characterized by having high regression coefficients. Our results\npoint to a distinct implicit bias of predicting in latent space that may shed\nlight on its success in practice.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}