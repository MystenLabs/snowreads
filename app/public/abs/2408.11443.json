{"id":"2408.11443","title":"Distributional Properties of Subword Regularization","authors":"Marco Cognetta, Vil\\'em Zouhar, Naoaki Okazaki","authorsParsed":[["Cognetta","Marco",""],["Zouhar","Vil√©m",""],["Okazaki","Naoaki",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 08:53:35 GMT"}],"updateDate":"2024-08-22","timestamp":1724230415000,"abstract":"  Subword regularization, used widely in NLP, improves model performance by\nreducing the dependency on exact tokenizations, augmenting the training corpus,\nand exposing the model to more unique contexts during training. BPE and\nMaxMatch, two popular subword tokenization schemes, have stochastic dropout\nregularization variants. However, there has not been an analysis of the\ndistributions formed by them. We show that these stochastic variants are\nheavily biased towards a small set of tokenizations per word. If the benefits\nof subword regularization are as mentioned, we hypothesize that biasedness\nartificially limits the effectiveness of these schemes. Thus, we propose an\nalgorithm to uniformly sample tokenizations that we use as a drop-in\nreplacement for the stochastic aspects of existing tokenizers, and find that it\nimproves machine translation quality.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}