{"id":"2407.03848","title":"Bias of Stochastic Gradient Descent or the Architecture: Disentangling\n  the Effects of Overparameterization of Neural Networks","authors":"Amit Peleg and Matthias Hein","authorsParsed":[["Peleg","Amit",""],["Hein","Matthias",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 11:29:50 GMT"}],"updateDate":"2024-07-08","timestamp":1720092590000,"abstract":"  Neural networks typically generalize well when fitting the data perfectly,\neven though they are heavily overparameterized. Many factors have been pointed\nout as the reason for this phenomenon, including an implicit bias of stochastic\ngradient descent (SGD) and a possible simplicity bias arising from the neural\nnetwork architecture. The goal of this paper is to disentangle the factors that\ninfluence generalization stemming from optimization and architectural choices\nby studying random and SGD-optimized networks that achieve zero training error.\nWe experimentally show, in the low sample regime, that overparameterization in\nterms of increasing width is beneficial for generalization, and this benefit is\ndue to the bias of SGD and not due to an architectural bias. In contrast, for\nincreasing depth, overparameterization is detrimental for generalization, but\nrandom and SGD-optimized networks behave similarly, so this can be attributed\nto an architectural bias. For more information, see\nhttps://bias-sgd-or-architecture.github.io .\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}