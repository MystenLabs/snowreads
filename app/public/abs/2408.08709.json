{"id":"2408.08709","title":"Multimodal Relational Triple Extraction with Query-based Entity Object\n  Transformer","authors":"Lei Hei, Ning An, Tingjing Liao, Qi Ma, Jiaqi Wang, Feiliang Ren","authorsParsed":[["Hei","Lei",""],["An","Ning",""],["Liao","Tingjing",""],["Ma","Qi",""],["Wang","Jiaqi",""],["Ren","Feiliang",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 12:43:38 GMT"}],"updateDate":"2024-08-19","timestamp":1723812218000,"abstract":"  Multimodal Relation Extraction is crucial for constructing flexible and\nrealistic knowledge graphs. Recent studies focus on extracting the relation\ntype with entity pairs present in different modalities, such as one entity in\nthe text and another in the image. However, existing approaches require\nentities and objects given beforehand, which is costly and impractical. To\naddress the limitation, we propose a novel task, Multimodal Entity-Object\nRelational Triple Extraction, which aims to extract all triples (entity span,\nrelation, object region) from image-text pairs. To facilitate this study, we\nmodified a multimodal relation extraction dataset MORE, which includes 21\nrelation types, to create a new dataset containing 20,264 triples, averaging\n5.75 triples per image-text pair. Moreover, we propose QEOT, a query-based\nmodel with a selective attention mechanism, to dynamically explore the\ninteraction and fusion of textual and visual information. In particular, the\nproposed method can simultaneously accomplish entity extraction, relation\nclassification, and object detection with a set of queries. Our method is\nsuitable for downstream applications and reduces error accumulation due to the\npipeline-style approaches. Extensive experimental results demonstrate that our\nproposed method outperforms the existing baselines by 8.06% and achieves\nstate-of-the-art performance.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}