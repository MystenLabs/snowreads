{"id":"2408.07802","title":"Kraken: Inherently Parallel Transformers For Efficient Multi-Device\n  Inference","authors":"Rohan Baskar Prabhakar, Hengrui Zhang, David Wentzlaff","authorsParsed":[["Prabhakar","Rohan Baskar",""],["Zhang","Hengrui",""],["Wentzlaff","David",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 20:24:03 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 00:58:10 GMT"}],"updateDate":"2024-08-20","timestamp":1723667043000,"abstract":"  Large Transformer networks are increasingly used in settings where low\ninference latency can improve the end-user experience and enable new\napplications. However, autoregressive inference is resource intensive and\nrequires parallelism for efficiency. Parallelism introduces collective\ncommunication that is both expensive and represents a phase when hardware\nresources are underutilized. Towards mitigating this, Kraken is an evolution of\nthe standard Transformer architecture that is designed to complement existing\ntensor parallelism schemes for efficient inference on multi-device systems. By\nintroducing a fixed degree of intra-layer model parallelism, the architecture\nallows collective operations to be overlapped with compute, decreasing latency\nand increasing hardware utilization. When trained on OpenWebText, Kraken models\nreach a similar perplexity as standard Transformers while also preserving their\nlanguage modeling capabilities when evaluated on the SuperGLUE benchmark.\nImportantly, when tested on multi-GPU systems using TensorRT-LLM engines,\nKraken speeds up Time To First Token by a mean of 35.6% across a range of model\nsizes, context lengths, and degrees of tensor parallelism.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}