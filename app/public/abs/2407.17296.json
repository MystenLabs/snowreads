{"id":"2407.17296","title":"Enhanced SMC$^2$: Leveraging Gradient Information from Differentiable\n  Particle Filters Within Langevin Proposals","authors":"Conor Rosato, Joshua Murphy, Alessandro Varsi, Paul Horridge, Simon\n  Maskell","authorsParsed":[["Rosato","Conor",""],["Murphy","Joshua",""],["Varsi","Alessandro",""],["Horridge","Paul",""],["Maskell","Simon",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 14:05:44 GMT"}],"updateDate":"2024-07-25","timestamp":1721829944000,"abstract":"  Sequential Monte Carlo Squared (SMC$^2$) is a Bayesian method which can infer\nthe states and parameters of non-linear, non-Gaussian state-space models. The\nstandard random-walk proposal in SMC$^2$ faces challenges, particularly with\nhigh-dimensional parameter spaces. This study outlines a novel approach by\nharnessing first-order gradients derived from a Common Random Numbers -\nParticle Filter (CRN-PF) using PyTorch. The resulting gradients can be\nleveraged within a Langevin proposal without accept/reject. Including Langevin\ndynamics within the proposal can result in a higher effective sample size and\nmore accurate parameter estimates when compared with the random-walk. The\nresulting algorithm is parallelized on distributed memory using Message Passing\nInterface (MPI) and runs in $\\mathcal{O}(\\log_2N)$ time complexity. Utilizing\n64 computational cores we obtain a 51x speed-up when compared to a single core.\nA GitHub link is given which provides access to the code.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Statistics/Applications"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}