{"id":"2407.19795","title":"VolDoGer: LLM-assisted Datasets for Domain Generalization in\n  Vision-Language Tasks","authors":"Juhwan Choi, Junehyoung Kwon, JungMin Yun, Seunguk Yu, YoungBin Kim","authorsParsed":[["Choi","Juhwan",""],["Kwon","Junehyoung",""],["Yun","JungMin",""],["Yu","Seunguk",""],["Kim","YoungBin",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 08:38:46 GMT"}],"updateDate":"2024-07-30","timestamp":1722242326000,"abstract":"  Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"Mo8P5r-2msphNln1iduxubO32uqYm_2TwCUqJ2rFlS4","pdfSize":"3205011"}
