{"id":"2408.15664","title":"Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts","authors":"Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, Damai Dai","authorsParsed":[["Wang","Lean",""],["Gao","Huazuo",""],["Zhao","Chenggang",""],["Sun","Xu",""],["Dai","Damai",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 09:31:09 GMT"}],"updateDate":"2024-08-29","timestamp":1724837469000,"abstract":"  For Mixture-of-Experts (MoE) models, an unbalanced expert load will lead to\nrouting collapse or increased computational overhead. Existing methods commonly\nemploy an auxiliary loss to encourage load balance, but a large auxiliary loss\nwill introduce non-negligible interference gradients into training and thus\nimpair the model performance. In order to control load balance while not\nproducing undesired gradients during training, we propose Loss-Free Balancing,\nfeatured by an auxiliary-loss-free load balancing strategy. To be specific,\nbefore the top-K routing decision, Loss-Free Balancing will first apply an\nexpert-wise bias to the routing scores of each expert. By dynamically updating\nthe bias of each expert according to its recent load, Loss-Free Balancing can\nconsistently maintain a balanced distribution of expert load. In addition,\nsince Loss-Free Balancing does not produce any interference gradients, it also\nelevates the upper bound of model performance gained from MoE training. We\nvalidate the performance of Loss-Free Balancing on MoE models with up to 3B\nparameters trained on up to 200B tokens. Experimental results show that\nLoss-Free Balancing achieves both better performance and better load balance\ncompared with traditional auxiliary-loss-controlled load balancing strategies.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}