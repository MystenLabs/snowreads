{"id":"2408.05107","title":"Depth Helps: Improving Pre-trained RGB-based Policy with Depth\n  Information Injection","authors":"Xincheng Pang, Wenke Xia, Zhigang Wang, Bin Zhao, Di Hu, Dong Wang,\n  Xuelong Li","authorsParsed":[["Pang","Xincheng",""],["Xia","Wenke",""],["Wang","Zhigang",""],["Zhao","Bin",""],["Hu","Di",""],["Wang","Dong",""],["Li","Xuelong",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 14:58:03 GMT"}],"updateDate":"2024-08-12","timestamp":1723215483000,"abstract":"  3D perception ability is crucial for generalizable robotic manipulation.\nWhile recent foundation models have made significant strides in perception and\ndecision-making with RGB-based input, their lack of 3D perception limits their\neffectiveness in fine-grained robotic manipulation tasks. To address these\nlimitations, we propose a Depth Information Injection ($\\bold{DI}^{\\bold{2}}$)\nframework that leverages the RGB-Depth modality for policy fine-tuning, while\nrelying solely on RGB images for robust and efficient deployment. Concretely,\nwe introduce the Depth Completion Module (DCM) to extract the spatial prior\nknowledge related to depth information and generate virtual depth information\nfrom RGB inputs to aid policy deployment. Further, we propose the Depth-Aware\nCodebook (DAC) to eliminate noise and reduce the cumulative error from the\ndepth prediction. In the inference phase, this framework employs RGB inputs and\naccurately predicted depth data to generate the manipulation action. We conduct\nexperiments on simulated LIBERO environments and real-world scenarios, and the\nexperiment results prove that our method could effectively enhance the\npre-trained RGB-based policy with 3D perception ability for robotic\nmanipulation. The website is released at\nhttps://gewu-lab.github.io/DepthHelps-IROS2024.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}