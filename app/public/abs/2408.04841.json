{"id":"2408.04841","title":"Kolmogorov-Arnold Network for Online Reinforcement Learning","authors":"Victor Augusto Kich, Jair Augusto Bottega, Raul Steinmetz, Ricardo\n  Bedin Grando, Ayano Yorozu, Akihisa Ohya","authorsParsed":[["Kich","Victor Augusto",""],["Bottega","Jair Augusto",""],["Steinmetz","Raul",""],["Grando","Ricardo Bedin",""],["Yorozu","Ayano",""],["Ohya","Akihisa",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 03:32:37 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 12:23:18 GMT"},{"version":"v3","created":"Sat, 31 Aug 2024 21:01:06 GMT"}],"updateDate":"2024-09-04","timestamp":1723174357000,"abstract":"  Kolmogorov-Arnold Networks (KANs) have shown potential as an alternative to\nMulti-Layer Perceptrons (MLPs) in neural networks, providing universal function\napproximation with fewer parameters and reduced memory usage. In this paper, we\nexplore the use of KANs as function approximators within the Proximal Policy\nOptimization (PPO) algorithm. We evaluate this approach by comparing its\nperformance to the original MLP-based PPO using the DeepMind Control Proprio\nRobotics benchmark. Our results indicate that the KAN-based reinforcement\nlearning algorithm can achieve comparable performance to its MLP-based\ncounterpart, often with fewer parameters. These findings suggest that KANs may\noffer a more efficient option for reinforcement learning models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}