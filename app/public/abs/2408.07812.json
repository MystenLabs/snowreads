{"id":"2408.07812","title":"Differentiating Policies for Non-Myopic Bayesian Optimization","authors":"Darian Nwankwo, David Bindel","authorsParsed":[["Nwankwo","Darian",""],["Bindel","David",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 21:00:58 GMT"}],"updateDate":"2024-08-16","timestamp":1723669258000,"abstract":"  Bayesian optimization (BO) methods choose sample points by optimizing an\nacquisition function derived from a statistical model of the objective. These\nacquisition functions are chosen to balance sampling regions with predicted\ngood objective values against exploring regions where the objective is\nuncertain. Standard acquisition functions are myopic, considering only the\nimpact of the next sample, but non-myopic acquisition functions may be more\neffective. In principle, one could model the sampling by a Markov decision\nprocess, and optimally choose the next sample by maximizing an expected reward\ncomputed by dynamic programming; however, this is infeasibly expensive. More\npractical approaches, such as rollout, consider a parametric family of sampling\npolicies. In this paper, we show how to efficiently estimate rollout\nacquisition functions and their gradients, enabling stochastic gradient-based\noptimization of sampling policies.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}