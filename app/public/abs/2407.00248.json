{"id":"2407.00248","title":"DiffuseDef: Improved Robustness to Adversarial Attacks","authors":"Zhenhao Li, Marek Rei, Lucia Specia","authorsParsed":[["Li","Zhenhao",""],["Rei","Marek",""],["Specia","Lucia",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 22:36:17 GMT"}],"updateDate":"2024-07-02","timestamp":1719614177000,"abstract":"  Pretrained language models have significantly advanced performance across\nvarious natural language processing tasks. However, adversarial attacks\ncontinue to pose a critical challenge to system built using these models, as\nthey can be exploited with carefully crafted adversarial texts. Inspired by the\nability of diffusion models to predict and reduce noise in computer vision, we\npropose a novel and flexible adversarial defense method for language\nclassification tasks, DiffuseDef, which incorporates a diffusion layer as a\ndenoiser between the encoder and the classifier. During inference, the\nadversarial hidden state is first combined with sampled noise, then denoised\niteratively and finally ensembled to produce a robust text representation. By\nintegrating adversarial training, denoising, and ensembling techniques, we show\nthat DiffuseDef improves over different existing adversarial defense methods\nand achieves state-of-the-art performance against common adversarial attacks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}