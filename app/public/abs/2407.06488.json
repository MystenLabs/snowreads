{"id":"2407.06488","title":"Towards Understanding Multi-Task Learning (Generalization) of LLMs via\n  Detecting and Exploring Task-Specific Neurons","authors":"Yongqi Leng and Deyi Xiong","authorsParsed":[["Leng","Yongqi",""],["Xiong","Deyi",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 01:27:35 GMT"}],"updateDate":"2024-07-10","timestamp":1720488455000,"abstract":"  While large language models (LLMs) have demonstrated superior multi-task\ncapabilities, understanding the learning mechanisms behind this is still a\nchallenging problem. In this paper, we attempt to understand such mechanisms\nfrom the perspective of neurons. Specifically, we detect task-sensitive neurons\nin LLMs via gradient attribution on task-specific data. Through extensive\ndeactivation and fine-tuning experiments, we demonstrate that the detected\nneurons are highly correlated with the given task, which we term as\ntask-specific neurons. With these identified task-specific neurons, we delve\ninto two common problems in multi-task learning and continuous learning:\nGeneralization and Catastrophic Forgetting. We find that the overlap of\ntask-specific neurons is strongly associated with generalization and\nspecialization across tasks. Interestingly, at certain layers of LLMs, there is\na high similarity in the parameters of different task-specific neurons, and\nsuch similarity is highly correlated with the generalization performance.\nInspired by these findings, we propose a neuron-level continuous fine-tuning\nmethod that only fine-tunes the current task-specific neurons during continuous\nlearning, and extensive experiments demonstrate the effectiveness of the\nproposed method. Our study provides insights into the interpretability of LLMs\nin multi-task learning.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}