{"id":"2408.01866","title":"Efficient Solutions For An Intriguing Failure of LLMs: Long Context\n  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly","authors":"Peyman Hosseini, Ignacio Castro, Iacopo Ghinassi, Matthew Purver","authorsParsed":[["Hosseini","Peyman",""],["Castro","Ignacio",""],["Ghinassi","Iacopo",""],["Purver","Matthew",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 21:31:34 GMT"}],"updateDate":"2024-08-06","timestamp":1722720694000,"abstract":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and analyzing lengthy sequential inputs, owing to their extensive\ncontext windows that allow processing millions of tokens in a single forward\npass. However, this paper uncovers a surprising limitation: LLMs fall short\nwhen handling long input sequences. We investigate this issue using three\ndatasets and two tasks (sentiment analysis and news categorization) across\nvarious LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,\nand Mistral Instruct models. To address this limitation, we propose and\nevaluate ad-hoc solutions that substantially enhance LLMs' performance on long\ninput sequences by up to 50%, while reducing API cost and latency by up to 93%\nand 50%, respectively.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}