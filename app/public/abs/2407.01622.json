{"id":"2407.01622","title":"Addressing Prediction Delays in Time Series Forecasting: A Continuous\n  GRU Approach with Derivative Regularization","authors":"Sheo Yon Jhin, Seojin Kim, Noseong Park","authorsParsed":[["Jhin","Sheo Yon",""],["Kim","Seojin",""],["Park","Noseong",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 05:36:04 GMT"}],"updateDate":"2024-07-03","timestamp":1719639364000,"abstract":"  Time series forecasting has been an essential field in many different\napplication areas, including economic analysis, meteorology, and so forth. The\nmajority of time series forecasting models are trained using the mean squared\nerror (MSE). However, this training based on MSE causes a limitation known as\nprediction delay. The prediction delay, which implies the ground-truth precedes\nthe prediction, can cause serious problems in a variety of fields, e.g.,\nfinance and weather forecasting -- as a matter of fact, predictions succeeding\nground-truth observations are not practically meaningful although their MSEs\ncan be low. This paper proposes a new perspective on traditional time series\nforecasting tasks and introduces a new solution to mitigate the prediction\ndelay. We introduce a continuous-time gated recurrent unit (GRU) based on the\nneural ordinary differential equation (NODE) which can supervise explicit\ntime-derivatives. We generalize the GRU architecture in a continuous-time\nmanner and minimize the prediction delay through our time-derivative\nregularization. Our method outperforms in metrics such as MSE, Dynamic Time\nWarping (DTW) and Time Distortion Index (TDI). In addition, we demonstrate the\nlow prediction delay of our method in a variety of datasets.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}