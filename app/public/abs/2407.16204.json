{"id":"2407.16204","title":"CLII: Visual-Text Inpainting via Cross-Modal Predictive Interaction","authors":"Liang Zhao, Qing Guo, Xiaoguang Li, and Song Wang","authorsParsed":[["Zhao","Liang",""],["Guo","Qing",""],["Li","Xiaoguang",""],["Wang","Song",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 06:12:19 GMT"}],"updateDate":"2024-07-24","timestamp":1721715139000,"abstract":"  Image inpainting aims to fill missing pixels in damaged images and has\nachieved significant progress with cut-edging learning techniques.\nNevertheless, state-of-the-art inpainting methods are mainly designed for\nnature images and cannot correctly recover text within scene text images, and\ntraining existing models on the scene text images cannot fix the issues. In\nthis work, we identify the visual-text inpainting task to achieve high-quality\nscene text image restoration and text completion: Given a scene text image with\nunknown missing regions and the corresponding text with unknown missing\ncharacters, we aim to complete the missing information in both images and text\nby leveraging their complementary information. Intuitively, the input text,\neven if damaged, contains language priors of the contents within the images and\ncan guide the image inpainting. Meanwhile, the scene text image includes the\nappearance cues of the characters that could benefit text recovery. To this\nend, we design the cross-modal predictive interaction (CLII) model containing\ntwo branches, i.e., ImgBranch and TxtBranch, for scene text inpainting and text\ncompletion, respectively while leveraging their complementary effectively.\nMoreover, we propose to embed our model into the SOTA scene text spotting\nmethod and significantly enhance its robustness against missing pixels, which\ndemonstrates the practicality of the newly developed task. To validate the\neffectiveness of our method, we construct three real datasets based on existing\ntext-related datasets, containing 1838 images and covering three scenarios with\ncurved, incidental, and styled texts, and conduct extensive experiments to show\nthat our method outperforms baselines significantly.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}