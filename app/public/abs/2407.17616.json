{"id":"2407.17616","title":"Pretraining a Neural Operator in Lower Dimensions","authors":"AmirPouya Hemmasian, Amir Barati Farimani","authorsParsed":[["Hemmasian","AmirPouya",""],["Farimani","Amir Barati",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 20:06:12 GMT"}],"updateDate":"2024-07-26","timestamp":1721851572000,"abstract":"  There has recently been increasing attention towards developing foundational\nneural Partial Differential Equation (PDE) solvers and neural operators through\nlarge-scale pretraining. However, unlike vision and language models that make\nuse of abundant and inexpensive (unlabeled) data for pretraining, these neural\nsolvers usually rely on simulated PDE data, which can be costly to obtain,\nespecially for high-dimensional PDEs. In this work, we aim to Pretrain neural\nPDE solvers on Lower Dimensional PDEs (PreLowD) where data collection is the\nleast expensive. We evaluated the effectiveness of this pretraining strategy in\nsimilar PDEs in higher dimensions. We use the Factorized Fourier Neural\nOperator (FFNO) due to having the necessary flexibility to be applied to PDE\ndata of arbitrary spatial dimensions and reuse trained parameters in lower\ndimensions. In addition, our work sheds light on the effect of the fine-tuning\nconfiguration to make the most of this pretraining strategy.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}