{"id":"2408.06970","title":"Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2","authors":"Osher Rafaeli, Tal Svoray, Roni Blushtein-Livnon and Ariel Nahlieli","authorsParsed":[["Rafaeli","Osher",""],["Svoray","Tal",""],["Blushtein-Livnon","Roni",""],["Nahlieli","Ariel",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 15:27:43 GMT"},{"version":"v2","created":"Thu, 15 Aug 2024 13:43:10 GMT"}],"updateDate":"2024-08-16","timestamp":1723562863000,"abstract":"  This paper provides insight into the effectiveness of zero-shot,\nprompt-based, Segment Anything Model (SAM), and its updated version, SAM 2, and\nthe non-promptable, conventional convolutional network (CNN), in segmenting\nsolar panels, in RGB aerial imagery, across lighting conditions, spatial\nresolutions, and prompt strategies. SAM 2 demonstrates improvements over SAM,\nparticularly in sub-optimal lighting conditions when prompted by points. Both\nSAMs, prompted by user-box, outperformed CNN, in all scenarios. Additionally,\nYOLOv9 prompting outperformed user points prompting. In high-resolution\nimagery, both in optimal and sub-optimal lighting conditions, Eff-UNet\noutperformed both SAM models prompted by YOLOv9 boxes, positioning Eff-UNet as\nthe appropriate model for automatic segmentation in high-resolution data. In\nlow-resolution data, user box prompts were found crucial to achieve a\nreasonable performance. This paper provides details on strengths and\nlimitations of each model and outlines robustness of user prompted image\nsegmentation models in inconsistent resolution and lighting conditions of\nremotely sensed data.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}