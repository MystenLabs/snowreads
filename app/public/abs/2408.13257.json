{"id":"2408.13257","title":"MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?","authors":"Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing\n  Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang,\n  Rong Jin, Tieniu Tan","authorsParsed":[["Zhang","Yi-Fan",""],["Zhang","Huanyu",""],["Tian","Haochen",""],["Fu","Chaoyou",""],["Zhang","Shuangqing",""],["Wu","Junfei",""],["Li","Feng",""],["Wang","Kun",""],["Wen","Qingsong",""],["Zhang","Zhang",""],["Wang","Liang",""],["Jin","Rong",""],["Tan","Tieniu",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 17:59:51 GMT"},{"version":"v2","created":"Wed, 11 Sep 2024 07:42:11 GMT"}],"updateDate":"2024-09-12","timestamp":1724435991000,"abstract":"  Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}