{"id":"2407.12588","title":"Benchmarking Robust Self-Supervised Learning Across Diverse Downstream\n  Tasks","authors":"Antoni Kowalczuk, Jan Dubi\\'nski, Atiyeh Ashari Ghomi, Yi Sui, George\n  Stein, Jiapeng Wu, Jesse C. Cresswell, Franziska Boenisch, Adam Dziedzic","authorsParsed":[["Kowalczuk","Antoni",""],["Dubi≈Ñski","Jan",""],["Ghomi","Atiyeh Ashari",""],["Sui","Yi",""],["Stein","George",""],["Wu","Jiapeng",""],["Cresswell","Jesse C.",""],["Boenisch","Franziska",""],["Dziedzic","Adam",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 14:12:34 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 06:55:33 GMT"}],"updateDate":"2024-07-19","timestamp":1721225554000,"abstract":"  Large-scale vision models have become integral in many applications due to\ntheir unprecedented performance and versatility across downstream tasks.\nHowever, the robustness of these foundation models has primarily been explored\nfor a single task, namely image classification. The vulnerability of other\ncommon vision tasks, such as semantic segmentation and depth estimation,\nremains largely unknown. We present a comprehensive empirical evaluation of the\nadversarial robustness of self-supervised vision encoders across multiple\ndownstream tasks. Our attacks operate in the encoder embedding space and at the\ndownstream task output level. In both cases, current state-of-the-art\nadversarial fine-tuning techniques tested only for classification significantly\ndegrade clean and robust performance on other tasks. Since the purpose of a\nfoundation model is to cater to multiple applications at once, our findings\nreveal the need to enhance encoder robustness more broadly. Our code is\navailable at ${github.com/layer6ai-labs/ssl-robustness}$.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}