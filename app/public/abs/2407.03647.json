{"id":"2407.03647","title":"WANCO: Weak Adversarial Networks for Constrained Optimization problems","authors":"Gang Bao and Dong Wang and Boyi Zou","authorsParsed":[["Bao","Gang",""],["Wang","Dong",""],["Zou","Boyi",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 05:37:48 GMT"}],"updateDate":"2024-07-08","timestamp":1720071468000,"abstract":"  This paper focuses on integrating the networks and adversarial training into\nconstrained optimization problems to develop a framework algorithm for\nconstrained optimization problems. For such problems, we first transform them\ninto minimax problems using the augmented Lagrangian method and then use two\n(or several) deep neural networks(DNNs) to represent the primal and dual\nvariables respectively. The parameters in the neural networks are then trained\nby an adversarial process. The proposed architecture is relatively insensitive\nto the scale of values of different constraints when compared to penalty based\ndeep learning methods. Through this type of training, the constraints are\nimposed better based on the augmented Lagrangian multipliers. Extensive\nexamples for optimization problems with scalar constraints, nonlinear\nconstraints, partial differential equation constraints, and inequality\nconstraints are considered to show the capability and robustness of the\nproposed method, with applications ranging from Ginzburg--Landau energy\nminimization problems, partition problems, fluid-solid topology optimization,\nto obstacle problems.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}