{"id":"2407.16698","title":"Diffusion Models for Monocular Depth Estimation: Overcoming Challenging\n  Conditions","authors":"Fabio Tosi, Pierluigi Zama Ramirez, Matteo Poggi","authorsParsed":[["Tosi","Fabio",""],["Ramirez","Pierluigi Zama",""],["Poggi","Matteo",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 17:59:59 GMT"}],"updateDate":"2024-07-24","timestamp":1721757599000,"abstract":"  We present a novel approach designed to address the complexities posed by\nchallenging, out-of-distribution data in the single-image depth estimation\ntask. Starting with images that facilitate depth prediction due to the absence\nof unfavorable factors, we systematically generate new, user-defined scenes\nwith a comprehensive set of challenges and associated depth information. This\nis achieved by leveraging cutting-edge text-to-image diffusion models with\ndepth-aware control, known for synthesizing high-quality image content from\ntextual prompts while preserving the coherence of 3D structure between\ngenerated and source imagery. Subsequent fine-tuning of any monocular depth\nnetwork is carried out through a self-distillation protocol that takes into\naccount images generated using our strategy and its own depth predictions on\nsimple, unchallenging scenes. Experiments on benchmarks tailored for our\npurposes demonstrate the effectiveness and versatility of our proposal.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}