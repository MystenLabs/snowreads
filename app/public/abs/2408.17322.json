{"id":"2408.17322","title":"Investigating Neuron Ablation in Attention Heads: The Case for Peak\n  Activation Centering","authors":"Nicholas Pochinkov, Ben Pasero, Skylar Shibayama","authorsParsed":[["Pochinkov","Nicholas",""],["Pasero","Ben",""],["Shibayama","Skylar",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 14:32:25 GMT"}],"updateDate":"2024-09-02","timestamp":1725028345000,"abstract":"  The use of transformer-based models is growing rapidly throughout society.\nWith this growth, it is important to understand how they work, and in\nparticular, how the attention mechanisms represent concepts. Though there are\nmany interpretability methods, many look at models through their neuronal\nactivations, which are poorly understood. We describe different lenses through\nwhich to view neuron activations, and investigate the effectiveness in language\nmodels and vision transformers through various methods of neural ablation: zero\nablation, mean ablation, activation resampling, and a novel approach we term\n'peak ablation'. Through experimental analysis, we find that in different\nregimes and models, each method can offer the lowest degradation of model\nperformance compared to other methods, with resampling usually causing the most\nsignificant performance deterioration. We make our code available at\nhttps://github.com/nickypro/investigating-ablation.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"K7ZAiuq2gi7CqzRfNMfm7LsYXUwQcHjZxNJrpNYIWHM","pdfSize":"1552716"}
