{"id":"2407.12260","title":"HuBar: A Visual Analytics Tool to Explore Human Behaviour based on fNIRS\n  in AR guidance systems","authors":"Sonia Castelo, Joao Rulff, Parikshit Solunke, Erin McGowan, Guande Wu,\n  Iran Roman, Roque Lopez, Bea Steers, Qi Sun, Juan Bello, Bradley Feest,\n  Michael Middleton, Ryan Mckendrick, Claudio Silva","authorsParsed":[["Castelo","Sonia",""],["Rulff","Joao",""],["Solunke","Parikshit",""],["McGowan","Erin",""],["Wu","Guande",""],["Roman","Iran",""],["Lopez","Roque",""],["Steers","Bea",""],["Sun","Qi",""],["Bello","Juan",""],["Feest","Bradley",""],["Middleton","Michael",""],["Mckendrick","Ryan",""],["Silva","Claudio",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 02:13:20 GMT"}],"updateDate":"2024-07-18","timestamp":1721182400000,"abstract":"  The concept of an intelligent augmented reality (AR) assistant has\nsignificant, wide-ranging applications, with potential uses in medicine,\nmilitary, and mechanics domains. Such an assistant must be able to perceive the\nenvironment and actions, reason about the environment state in relation to a\ngiven task, and seamlessly interact with the task performer. These interactions\ntypically involve an AR headset equipped with sensors which capture video,\naudio, and haptic feedback. Previous works have sought to facilitate the\ndevelopment of intelligent AR assistants by visualizing these sensor data\nstreams in conjunction with the assistant's perception and reasoning model\noutputs. However, existing visual analytics systems do not focus on user\nmodeling or include biometric data, and are only capable of visualizing a\nsingle task session for a single performer at a time. Moreover, they typically\nassume a task involves linear progression from one step to the next. We propose\na visual analytics system that allows users to compare performance during\nmultiple task sessions, focusing on non-linear tasks where different step\nsequences can lead to success. In particular, we design visualizations for\nunderstanding user behavior through functional near-infrared spectroscopy\n(fNIRS) data as a proxy for perception, attention, and memory as well as\ncorresponding motion data (acceleration, angular velocity, and gaze). We\ndistill these insights into embedding representations that allow users to\neasily select groups of sessions with similar behaviors. We provide two case\nstudies that demonstrate how to use these visualizations to gain insights about\ntask performance using data collected during helicopter copilot training tasks.\nFinally, we evaluate our approach by conducting an in-depth examination of a\nthink-aloud experiment with five domain experts.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}