{"id":"2408.05827","title":"Divergence Maximizing Linear Projection for Supervised Dimension\n  Reduction","authors":"Biao Chen and Joshua Kortje","authorsParsed":[["Chen","Biao",""],["Kortje","Joshua",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 17:00:37 GMT"}],"updateDate":"2024-08-13","timestamp":1723395637000,"abstract":"  This paper proposes two linear projection methods for supervised dimension\nreduction using only the first and second-order statistics. The methods, each\ncatering to a different parameter regime, are derived under the general\nGaussian model by maximizing the Kullback-Leibler divergence between the two\nclasses in the projected sample for a binary classification problem. They\nsubsume existing linear projection approaches developed under simplifying\nassumptions of Gaussian distributions, such as these distributions might share\nan equal mean or covariance matrix. As a by-product, we establish that the\nmulti-class linear discriminant analysis, a celebrated method for\nclassification and supervised dimension reduction, is provably optimal for\nmaximizing pairwise Kullback-Leibler divergence when the Gaussian populations\nshare an identical covariance matrix. For the case when the Gaussian\ndistributions share an equal mean, we establish conditions under which the\noptimal subspace remains invariant regardless of how the Kullback-Leibler\ndivergence is defined, despite the asymmetry of the divergence measure itself.\nSuch conditions encompass the classical case of signal plus noise, where both\nthe signal and noise have zero mean and arbitrary covariance matrices.\nExperiments are conducted to validate the proposed solutions, demonstrate their\nsuperior performance over existing alternatives, and illustrate the procedure\nfor selecting the appropriate linear projection solution.\n","subjects":["Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}