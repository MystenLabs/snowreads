{"id":"2407.00699","title":"Tackling Long-Horizon Tasks with Model-based Offline Reinforcement\n  Learning","authors":"Kwanyoung Park and Youngwoon Lee","authorsParsed":[["Park","Kwanyoung",""],["Lee","Youngwoon",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 13:44:59 GMT"}],"updateDate":"2024-07-02","timestamp":1719755099000,"abstract":"  Model-based offline reinforcement learning (RL) is a compelling approach that\naddresses the challenge of learning from limited, static data by generating\nimaginary trajectories using learned models. However, it falls short in solving\nlong-horizon tasks due to high bias in value estimation from model rollouts. In\nthis paper, we introduce a novel model-based offline RL method, Lower Expectile\nQ-learning (LEQ), which enhances long-horizon task performance by mitigating\nthe high bias in model-based value estimation via expectile regression of\n$\\lambda$-returns. Our empirical results show that LEQ significantly\noutperforms previous model-based offline RL methods on long-horizon tasks, such\nas the D4RL AntMaze tasks, matching or surpassing the performance of model-free\napproaches. Our experiments demonstrate that expectile regression,\n$\\lambda$-returns, and critic training on offline data are all crucial for\naddressing long-horizon tasks. Additionally, LEQ achieves performance\ncomparable to the state-of-the-art model-based and model-free offline RL\nmethods on the NeoRL benchmark and the D4RL MuJoCo Gym tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9ZhwDvDX4sWuYPkDd5ZyIumvPrXarWrFCr1iefZWN-Y","pdfSize":"1368476"}
