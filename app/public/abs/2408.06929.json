{"id":"2408.06929","title":"Evaluating Cultural Adaptability of a Large Language Model via\n  Simulation of Synthetic Personas","authors":"Louis Kwok, Michal Bravansky, Lewis D. Griffin","authorsParsed":[["Kwok","Louis",""],["Bravansky","Michal",""],["Griffin","Lewis D.",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 14:32:43 GMT"}],"updateDate":"2024-08-14","timestamp":1723559563000,"abstract":"  The success of Large Language Models (LLMs) in multicultural environments\nhinges on their ability to understand users' diverse cultural backgrounds. We\nmeasure this capability by having an LLM simulate human profiles representing\nvarious nationalities within the scope of a questionnaire-style psychological\nexperiment. Specifically, we employ GPT-3.5 to reproduce reactions to\npersuasive news articles of 7,286 participants from 15 countries; comparing the\nresults with a dataset of real participants sharing the same demographic\ntraits. Our analysis shows that specifying a person's country of residence\nimproves GPT-3.5's alignment with their responses. In contrast, using native\nlanguage prompting introduces shifts that significantly reduce overall\nalignment, with some languages particularly impairing performance. These\nfindings suggest that while direct nationality information enhances the model's\ncultural adaptability, native language cues do not reliably improve simulation\nfidelity and can detract from the model's effectiveness.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3x6TbvEGOpiWWRUZ2Tzj7uwg9s-XisRVJmeeykAOdWQ","pdfSize":"1266067"}
