{"id":"2407.02779","title":"Croppable Knowledge Graph Embedding","authors":"Yushan Zhu, Wen Zhang, Zhiqiang Liu, Mingyang Chen, Lei Liang, Huajun\n  Chen","authorsParsed":[["Zhu","Yushan",""],["Zhang","Wen",""],["Liu","Zhiqiang",""],["Chen","Mingyang",""],["Liang","Lei",""],["Chen","Huajun",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 03:10:25 GMT"}],"updateDate":"2024-07-04","timestamp":1719976225000,"abstract":"  Knowledge Graph Embedding (KGE) is a common method for Knowledge Graphs (KGs)\nto serve various artificial intelligence tasks. The suitable dimensions of the\nembeddings depend on the storage and computing conditions of the specific\napplication scenarios. Once a new dimension is required, a new KGE model needs\nto be trained from scratch, which greatly increases the training cost and\nlimits the efficiency and flexibility of KGE in serving various scenarios. In\nthis work, we propose a novel KGE training framework MED, through which we\ncould train once to get a croppable KGE model applicable to multiple scenarios\nwith different dimensional requirements, sub-models of the required dimensions\ncan be cropped out of it and used directly without any additional training. In\nMED, we propose a mutual learning mechanism to improve the low-dimensional\nsub-models performance and make the high-dimensional sub-models retain the\ncapacity that low-dimensional sub-models have, an evolutionary improvement\nmechanism to promote the high-dimensional sub-models to master the knowledge\nthat the low-dimensional sub-models can not learn, and a dynamic loss weight to\nbalance the multiple losses adaptively. Experiments on 3 KGE models over 4\nstandard KG completion datasets, 3 real application scenarios over a real-world\nlarge-scale KG, and the experiments of extending MED to the language model BERT\nshow the effectiveness, high efficiency, and flexible extensibility of MED.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}