{"id":"2407.11876","title":"Simplifying the Theory on Over-Smoothing","authors":"Andreas Roth","authorsParsed":[["Roth","Andreas",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 16:00:42 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 09:49:49 GMT"}],"updateDate":"2024-09-04","timestamp":1721145642000,"abstract":"  Graph convolutions have gained popularity due to their ability to efficiently\noperate on data with an irregular geometric structure. However, graph\nconvolutions cause over-smoothing, which refers to representations becoming\nmore similar with increased depth. However, many different definitions and\nintuitions currently coexist, leading to research efforts focusing on\nincompatible directions. This paper attempts to align these directions by\nshowing that over-smoothing is merely a special case of power iteration. This\ngreatly simplifies the existing theory on over-smoothing, making it more\naccessible. Based on the theory, we provide a novel comprehensive definition of\nrank collapse as a generalized form of over-smoothing and introduce the\nrank-one distance as a corresponding metric. Our empirical evaluation of 14\ncommonly used methods shows that more models than were previously known suffer\nfrom this issue.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}