{"id":"2408.15633","title":"Comparison of Model Predictive Control and Proximal Policy Optimization\n  for a 1-DOF Helicopter System","authors":"Georg Sch\\\"afer and Jakob Rehrl and Stefan Huber and Simon Hirlaender","authorsParsed":[["Sch√§fer","Georg",""],["Rehrl","Jakob",""],["Huber","Stefan",""],["Hirlaender","Simon",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 08:35:34 GMT"}],"updateDate":"2024-08-29","timestamp":1724834134000,"abstract":"  This study conducts a comparative analysis of Model Predictive Control (MPC)\nand Proximal Policy Optimization (PPO), a Deep Reinforcement Learning (DRL)\nalgorithm, applied to a 1-Degree of Freedom (DOF) Quanser Aero 2 system.\nClassical control techniques such as MPC and Linear Quadratic Regulator (LQR)\nare widely used due to their theoretical foundation and practical\neffectiveness. However, with advancements in computational techniques and\nmachine learning, DRL approaches like PPO have gained traction in solving\noptimal control problems through environment interaction. This paper\nsystematically evaluates the dynamic response characteristics of PPO and MPC,\ncomparing their performance, computational resource consumption, and\nimplementation complexity. Experimental results show that while LQR achieves\nthe best steady-state accuracy, PPO excels in rise-time and adaptability,\nmaking it a promising approach for applications requiring rapid response and\nadaptability. Additionally, we have established a baseline for future\nRL-related research on this specific testbed. We also discuss the strengths and\nlimitations of each control strategy, providing recommendations for selecting\nappropriate controllers for real-world scenarios.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Machine Learning","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}