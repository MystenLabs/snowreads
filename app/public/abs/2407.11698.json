{"id":"2407.11698","title":"NITRO-D: Native Integer-only Training of Deep Convolutional Neural\n  Networks","authors":"Alberto Pirillo, Luca Colombo, Manuel Roveri","authorsParsed":[["Pirillo","Alberto",""],["Colombo","Luca",""],["Roveri","Manuel",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 13:16:49 GMT"},{"version":"v2","created":"Thu, 12 Sep 2024 14:18:22 GMT"}],"updateDate":"2024-09-13","timestamp":1721135809000,"abstract":"  Quantization has become increasingly pivotal in addressing the steadily\nincreasing computational and memory requirements of Deep Neural Networks\n(DNNs). By reducing the number of bits used to represent weights and\nactivations (typically from 32-bit floating-point to 16-bit or 8-bit integers),\nquantization reduces the memory footprint, energy consumption, and execution\ntime of DNN models. However, traditional quantization methods typically focus\non the inference of DNNs, while the training process still relies on\nfloating-point operations. To date, only one work in the literature has\naddressed integer-only training for Multi-Layer Perceptron (MLP) architectures.\nThis work introduces NITRO-D, a new framework for training arbitrarily deep\ninteger-only Convolutional Neural Networks (CNNs) that operate entirely in the\ninteger-only domain for both training and inference. NITRO-D is the first\nframework in the literature enabling the training of integer-only CNNs without\nthe need to introduce a quantization scheme. Specifically, NITRO-D introduces a\nnovel architecture integrating multiple integer local-loss blocks, which\ninclude the proposed NITRO Scaling Layer and the NITRO-ReLU activation\nfunction. Additionally, it introduces a novel integer-only learning algorithm\nderived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer\nspecifically designed to operate in an integer-only context. NITRO-D is\nimplemented in an open-source Python library. Extensive experimental\nevaluations demonstrate its effectiveness across several state-of-the-art image\nrecognition datasets. Results show significant performance improvements from\n2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art\nsolution, and the capability of training integer-only CNN architectures with\nminimal accuracy degradation from -0.15% to -4.22% compared to floating-point\nLES.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}