{"id":"2408.14909","title":"SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models","authors":"Shuaijie Shen, Chao Wang, Renzhuo Huang, Yan Zhong, Qinghai Guo,\n  Zhichao Lu, Jianguo Zhang, Luziwei Leng","authorsParsed":[["Shen","Shuaijie",""],["Wang","Chao",""],["Huang","Renzhuo",""],["Zhong","Yan",""],["Guo","Qinghai",""],["Lu","Zhichao",""],["Zhang","Jianguo",""],["Leng","Luziwei",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 09:35:49 GMT"}],"updateDate":"2024-08-28","timestamp":1724751349000,"abstract":"  Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}