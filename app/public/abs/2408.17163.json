{"id":"2408.17163","title":"The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by\n  Leveraging Second-Order Information","authors":"Diyuan Wu, Ionut-Vlad Modoranu, Mher Safaryan, Denis Kuznedelev, Dan\n  Alistarh","authorsParsed":[["Wu","Diyuan",""],["Modoranu","Ionut-Vlad",""],["Safaryan","Mher",""],["Kuznedelev","Denis",""],["Alistarh","Dan",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 10:06:26 GMT"}],"updateDate":"2024-09-02","timestamp":1725012386000,"abstract":"  The rising footprint of machine learning has led to a focus on imposing\n\\emph{model sparsity} as a means of reducing computational and memory costs.\nFor deep neural networks (DNNs), the state-of-the-art accuracy-vs-sparsity is\nachieved by heuristics inspired by the classical Optimal Brain Surgeon (OBS)\nframework~\\citep{lecun90brain, hassibi1992second, hassibi1993optimal}, which\nleverages loss curvature information to make better pruning decisions. Yet,\nthese results still lack a solid theoretical understanding, and it is unclear\nwhether they can be improved by leveraging connections to the wealth of work on\nsparse recovery algorithms. In this paper, we draw new connections between\nthese two areas and present new sparse recovery algorithms inspired by the OBS\nframework that comes with theoretical guarantees under reasonable assumptions\nand have strong practical performance. Specifically, our work starts from the\nobservation that we can leverage curvature information in OBS-like fashion upon\nthe projection step of classic iterative sparse recovery algorithms such as\nIHT. We show for the first time that this leads both to improved convergence\nbounds under standard assumptions. Furthermore, we present extensions of this\napproach to the practical task of obtaining accurate sparse DNNs, and validate\nit experimentally at scale for Transformer-based models on vision and language\ntasks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}