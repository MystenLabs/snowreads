{"id":"2407.11385","title":"Grasping Diverse Objects with Simulated Humanoids","authors":"Zhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris\n  Kitani, Weipeng Xu","authorsParsed":[["Luo","Zhengyi",""],["Cao","Jinkun",""],["Christen","Sammy",""],["Winkler","Alexander",""],["Kitani","Kris",""],["Xu","Weipeng",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 05:05:02 GMT"}],"updateDate":"2024-07-17","timestamp":1721106302000,"abstract":"  We present a method for controlling a simulated humanoid to grasp an object\nand move it to follow an object trajectory. Due to the challenges in\ncontrolling a humanoid with dexterous hands, prior methods often use a\ndisembodied hand and only consider vertical lifts or short trajectories. This\nlimited scope hampers their applicability for object manipulation required for\nanimation and simulation. To close this gap, we learn a controller that can\npick up a large number (>1200) of objects and carry them to follow randomly\ngenerated trajectories. Our key insight is to leverage a humanoid motion\nrepresentation that provides human-like motor skills and significantly speeds\nup training. Using only simplistic reward, state, and object representations,\nour method shows favorable scalability on diverse object and trajectories. For\ntraining, we do not need dataset of paired full-body motion and object\ntrajectories. At test time, we only require the object mesh and desired\ntrajectories for grasping and transporting. To demonstrate the capabilities of\nour method, we show state-of-the-art success rates in following object\ntrajectories and generalizing to unseen objects. Code and models will be\nreleased.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Graphics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"G-_35Vw8_wKSbTi5aVZjxW7q0nrAgwtDNxx7dJmbJXM","pdfSize":"9075531"}
