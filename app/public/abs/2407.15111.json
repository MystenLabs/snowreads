{"id":"2407.15111","title":"D$^4$-VTON: Dynamic Semantics Disentangling for Differential Diffusion\n  based Virtual Try-On","authors":"Zhaotong Yang, Zicheng Jiang, Xinzhe Li, Huiyu Zhou, Junyu Dong,\n  Huaidong Zhang, Yong Du","authorsParsed":[["Yang","Zhaotong",""],["Jiang","Zicheng",""],["Li","Xinzhe",""],["Zhou","Huiyu",""],["Dong","Junyu",""],["Zhang","Huaidong",""],["Du","Yong",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 10:40:53 GMT"}],"updateDate":"2024-07-23","timestamp":1721558453000,"abstract":"  In this paper, we introduce D$^4$-VTON, an innovative solution for\nimage-based virtual try-on. We address challenges from previous studies, such\nas semantic inconsistencies before and after garment warping, and reliance on\nstatic, annotation-driven clothing parsers. Additionally, we tackle the\ncomplexities in diffusion-based VTON models when handling simultaneous tasks\nlike inpainting and denoising. Our approach utilizes two key technologies:\nFirstly, Dynamic Semantics Disentangling Modules (DSDMs) extract abstract\nsemantic information from garments to create distinct local flows, improving\nprecise garment warping in a self-discovered manner. Secondly, by integrating a\nDifferential Information Tracking Path (DITP), we establish a novel\ndiffusion-based VTON paradigm. This path captures differential information\nbetween incomplete try-on inputs and their complete versions, enabling the\nnetwork to handle multiple degradations independently, thereby minimizing\nlearning ambiguities and achieving realistic results with minimal overhead.\nExtensive experiments demonstrate that D$^4$-VTON significantly outperforms\nexisting methods in both quantitative metrics and qualitative evaluations,\ndemonstrating its capability in generating realistic images and ensuring\nsemantic consistency.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}