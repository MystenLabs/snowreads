{"id":"2407.10454","title":"Deflated Dynamics Value Iteration","authors":"Jongmin Lee, Amin Rakhsha, Ernest K. Ryu, Amir-massoud Farahmand","authorsParsed":[["Lee","Jongmin",""],["Rakhsha","Amin",""],["Ryu","Ernest K.",""],["Farahmand","Amir-massoud",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 06:07:05 GMT"}],"updateDate":"2024-07-16","timestamp":1721023625000,"abstract":"  The Value Iteration (VI) algorithm is an iterative procedure to compute the\nvalue function of a Markov decision process, and is the basis of many\nreinforcement learning (RL) algorithms as well. As the error convergence rate\nof VI as a function of iteration $k$ is $O(\\gamma^k)$, it is slow when the\ndiscount factor $\\gamma$ is close to $1$. To accelerate the computation of the\nvalue function, we propose Deflated Dynamics Value Iteration (DDVI). DDVI uses\nmatrix splitting and matrix deflation techniques to effectively remove\n(deflate) the top $s$ dominant eigen-structure of the transition matrix\n$\\mathcal{P}^{\\pi}$. We prove that this leads to a $\\tilde{O}(\\gamma^k\n|\\lambda_{s+1}|^k)$ convergence rate, where $\\lambda_{s+1}$is $(s+1)$-th\nlargest eigenvalue of the dynamics matrix. We then extend DDVI to the RL\nsetting and present Deflated Dynamics Temporal Difference (DDTD) algorithm. We\nempirically show the effectiveness of the proposed algorithms.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/"}