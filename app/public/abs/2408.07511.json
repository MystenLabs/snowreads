{"id":"2408.07511","title":"Protected Test-Time Adaptation via Online Entropy Matching: A Betting\n  Approach","authors":"Yarin Bar, Shalev Shaer, and Yaniv Romano","authorsParsed":[["Bar","Yarin",""],["Shaer","Shalev",""],["Romano","Yaniv",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 12:40:57 GMT"}],"updateDate":"2024-08-15","timestamp":1723639257000,"abstract":"  We present a novel approach for test-time adaptation via online\nself-training, consisting of two components. First, we introduce a statistical\nframework that detects distribution shifts in the classifier's entropy values\nobtained on a stream of unlabeled samples. Second, we devise an online\nadaptation mechanism that utilizes the evidence of distribution shifts captured\nby the detection tool to dynamically update the classifier's parameters. The\nresulting adaptation process drives the distribution of test entropy values\nobtained from the self-trained classifier to match those of the source domain,\nbuilding invariance to distribution shifts. This approach departs from the\nconventional self-training method, which focuses on minimizing the classifier's\nentropy. Our approach combines concepts in betting martingales and online\nlearning to form a detection tool capable of quickly reacting to distribution\nshifts. We then reveal a tight relation between our adaptation scheme and\noptimal transport, which forms the basis of our novel self-supervised loss.\nExperimental results demonstrate that our approach improves test-time accuracy\nunder distribution shifts while maintaining accuracy and calibration in their\nabsence, outperforming leading entropy minimization methods across various\nscenarios.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}