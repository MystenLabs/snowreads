{"id":"2408.00008","title":"ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing\n  End-to-End Efficiency","authors":"Yuhang Yao, Han Jin, Alay Dilipbhai Shah, Shanshan Han, Zijian Hu,\n  Yide Ran, Dimitris Stripelis, Zhaozhuo Xu, Salman Avestimehr, Chaoyang He","authorsParsed":[["Yao","Yuhang",""],["Jin","Han",""],["Shah","Alay Dilipbhai",""],["Han","Shanshan",""],["Hu","Zijian",""],["Ran","Yide",""],["Stripelis","Dimitris",""],["Xu","Zhaozhuo",""],["Avestimehr","Salman",""],["He","Chaoyang",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 23:37:29 GMT"},{"version":"v2","created":"Tue, 10 Sep 2024 22:35:13 GMT"}],"updateDate":"2024-09-12","timestamp":1721777849000,"abstract":"  Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}