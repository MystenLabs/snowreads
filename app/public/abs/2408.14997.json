{"id":"2408.14997","title":"Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot\n  Handover","authors":"Ran Yu, Haixin Yu, Shoujie Li, Huang Yan, Ziwu Song, Wenbo Ding","authorsParsed":[["Yu","Ran",""],["Yu","Haixin",""],["Li","Shoujie",""],["Yan","Huang",""],["Song","Ziwu",""],["Ding","Wenbo",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 12:25:12 GMT"},{"version":"v2","created":"Mon, 16 Sep 2024 07:05:56 GMT"}],"updateDate":"2024-09-17","timestamp":1724761512000,"abstract":"  Transparent objects are common in daily life, while their optical properties\npose challenges for RGB-D cameras to capture accurate depth information. This\nissue is further amplified when these objects are hand-held, as hand occlusions\nfurther complicate depth estimation. For assistant robots, however, accurately\nperceiving hand-held transparent objects is critical to effective human-robot\ninteraction. This paper presents a Hand-Aware Depth Restoration (HADR) method\nbased on creating an implicit neural representation function from a single\nRGB-D image. The proposed method utilizes hand posture as an important guidance\nto leverage semantic and geometric information of hand-object interaction. To\ntrain and evaluate the proposed method, we create a high-fidelity synthetic\ndataset named TransHand-14K with a real-to-sim data generation scheme.\nExperiments show that our method has better performance and generalization\nability compared with existing methods. We further develop a real-world\nhuman-to-robot handover system based on HADR, demonstrating its potential in\nhuman-robot interaction applications.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}