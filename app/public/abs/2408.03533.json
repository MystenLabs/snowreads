{"id":"2408.03533","title":"Lifelong Personalized Low-Rank Adaptation of Large Language Models for\n  Recommendation","authors":"Jiachen Zhu, Jianghao Lin, Xinyi Dai, Bo Chen, Rong Shan, Jieming Zhu,\n  Ruiming Tang, Yong Yu, Weinan Zhang","authorsParsed":[["Zhu","Jiachen",""],["Lin","Jianghao",""],["Dai","Xinyi",""],["Chen","Bo",""],["Shan","Rong",""],["Zhu","Jieming",""],["Tang","Ruiming",""],["Yu","Yong",""],["Zhang","Weinan",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 04:20:28 GMT"},{"version":"v2","created":"Sun, 11 Aug 2024 09:08:59 GMT"}],"updateDate":"2024-08-13","timestamp":1723004428000,"abstract":"  We primarily focus on the field of large language models (LLMs) for\nrecommendation, which has been actively explored recently and poses a\nsignificant challenge in effectively enhancing recommender systems with logical\nreasoning abilities and open-world knowledge. Current mainstream efforts mainly\ncenter around injecting personalized information from recommendation models\ninto LLMs by customizing input templates or aligning representations between\nsemantic and recommendation spaces at the prediction layer. However, they face\nthree significant limitations: (1) LoRA is mostly used as a core component in\nexisting works, but personalization is not well established in LoRA parameters\nas the LoRA matrix shared by every user may not cater to different users'\ncharacteristics, leading to suboptimal performance. (2) Although lifelong\npersonalized behavior sequences are ideal for personalization, their use raises\neffectiveness and efficiency issues since LLMs require escalating training and\ninference time to extend text lengths. (3) Existing approaches aren't scalable\nfor large datasets due to training efficiency constraints. Thus, LLMs only see\na small fraction of the datasets (e.g., less than 10%) instead of the whole\ndatasets, limiting their exposure to the full training space. To address these\nproblems, we propose RecLoRA. This model incorporates a Personalized LoRA\nmodule that maintains independent LoRAs for different users and a Long-Short\nModality Retriever that retrieves different history lengths for different\nmodalities, significantly improving performance while adding minimal time cost.\nFurthermore, we design a Few2Many Learning Strategy, using a conventional\nrecommendation model as a lens to magnify small training spaces to full spaces.\nExtensive experiments on public datasets demonstrate the efficacy of our\nRecLoRA compared to existing baseline models.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}