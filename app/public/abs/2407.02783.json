{"id":"2407.02783","title":"52B to 1T: Lessons Learned via Tele-FLM Series","authors":"Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Chao Wang, Xinzhang Liu,\n  Zihan Wang, Yu Zhao, Xin Wang, Yuyao Huang, Shuangyong Song, Yongxiang Li,\n  Zheng Zhang, Bo Zhao, Aixin Sun, Yequan Wang, Zhongjiang He, Zhongyuan Wang,\n  Xuelong Li, Tiejun Huang","authorsParsed":[["Li","Xiang",""],["Yao","Yiqun",""],["Jiang","Xin",""],["Fang","Xuezhi",""],["Wang","Chao",""],["Liu","Xinzhang",""],["Wang","Zihan",""],["Zhao","Yu",""],["Wang","Xin",""],["Huang","Yuyao",""],["Song","Shuangyong",""],["Li","Yongxiang",""],["Zhang","Zheng",""],["Zhao","Bo",""],["Sun","Aixin",""],["Wang","Yequan",""],["He","Zhongjiang",""],["Wang","Zhongyuan",""],["Li","Xuelong",""],["Huang","Tiejun",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 03:21:02 GMT"}],"updateDate":"2024-07-04","timestamp":1719976862000,"abstract":"  Large Language Models (LLMs) represent a significant stride toward Artificial\nGeneral Intelligence. As scaling laws underscore the potential of increasing\nmodel sizes, the academic community has intensified its investigations into\nLLMs with capacities exceeding 50 billion parameters. This technical report\nbuilds on our prior work with Tele-FLM (also known as FLM-2), a publicly\navailable 52-billion-parameter model. We delve into two primary areas: we first\ndiscuss our observation of Supervised Fine-tuning (SFT) on Tele-FLM-52B, which\nsupports the \"less is more\" approach for SFT data construction; second, we\ndemonstrate our experiments and analyses on the best practices for\nprogressively growing a model from 52 billion to 102 billion, and subsequently\nto 1 trillion parameters. We will open-source a 1T model checkpoint, namely\nTele-FLM-1T, to advance further training and research.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}