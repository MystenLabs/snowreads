{"id":"2407.16890","title":"Why Machines Can't Be Moral: Turing's Halting Problem and the Moral\n  Limits of Artificial Intelligence","authors":"Massimo Passamonti","authorsParsed":[["Passamonti","Massimo",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:50:24 GMT"}],"updateDate":"2024-07-25","timestamp":1721843424000,"abstract":"  In this essay, I argue that explicit ethical machines, whose moral principles\nare inferred through a bottom-up approach, are unable to replicate human-like\nmoral reasoning and cannot be considered moral agents. By utilizing Alan\nTuring's theory of computation, I demonstrate that moral reasoning is\ncomputationally intractable by these machines due to the halting problem. I\naddress the frontiers of machine ethics by formalizing moral problems into\n'algorithmic moral questions' and by exploring moral psychology's dual-process\nmodel. While the nature of Turing Machines theoretically allows artificial\nagents to engage in recursive moral reasoning, critical limitations are\nintroduced by the halting problem, which states that it is impossible to\npredict with certainty whether a computational process will halt. A thought\nexperiment involving a military drone illustrates this issue, showing that an\nartificial agent might fail to decide between actions due to the halting\nproblem, which limits the agent's ability to make decisions in all instances,\nundermining its moral agency.\n","subjects":["Computing Research Repository/Computers and Society","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}