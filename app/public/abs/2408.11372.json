{"id":"2408.11372","title":"Denoising Pre-Training and Customized Prompt Learning for Efficient\n  Multi-Behavior Sequential Recommendation","authors":"Hao Wang, Yongqiang Han, Kefan Wang, Kai Cheng, Zhen Wang, Wei Guo,\n  Yong Liu, Defu Lian, and Enhong Chen","authorsParsed":[["Wang","Hao",""],["Han","Yongqiang",""],["Wang","Kefan",""],["Cheng","Kai",""],["Wang","Zhen",""],["Guo","Wei",""],["Liu","Yong",""],["Lian","Defu",""],["Chen","Enhong",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 06:48:38 GMT"}],"updateDate":"2024-08-22","timestamp":1724222918000,"abstract":"  In the realm of recommendation systems, users exhibit a diverse array of\nbehaviors when interacting with items. This phenomenon has spurred research\ninto learning the implicit semantic relationships between these behaviors to\nenhance recommendation performance. However, these methods often entail high\ncomputational complexity. To address concerns regarding efficiency,\npre-training presents a viable solution. Its objective is to extract knowledge\nfrom extensive pre-training data and fine-tune the model for downstream tasks.\nNevertheless, previous pre-training methods have primarily focused on\nsingle-behavior data, while multi-behavior data contains significant noise.\nAdditionally, the fully fine-tuning strategy adopted by these methods still\nimposes a considerable computational burden. In response to this challenge, we\npropose DPCPL, the first pre-training and prompt-tuning paradigm tailored for\nMulti-Behavior Sequential Recommendation. Specifically, in the pre-training\nstage, we commence by proposing a novel Efficient Behavior Miner (EBM) to\nfilter out the noise at multiple time scales, thereby facilitating the\ncomprehension of the contextual semantics of multi-behavior sequences.\nSubsequently, we propose to tune the pre-trained model in a highly efficient\nmanner with the proposed Customized Prompt Learning (CPL) module, which\ngenerates personalized, progressive, and diverse prompts to fully exploit the\npotential of the pre-trained model effectively. Extensive experiments on three\nreal-world datasets have unequivocally demonstrated that DPCPL not only\nexhibits high efficiency and effectiveness, requiring minimal parameter\nadjustments but also surpasses the state-of-the-art performance across a\ndiverse range of downstream tasks.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}