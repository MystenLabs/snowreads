{"id":"2408.14809","title":"GSIFN: A Graph-Structured and Interlaced-Masked Multimodal\n  Transformer-based Fusion Network for Multimodal Sentiment Analysis","authors":"Yijie Jin","authorsParsed":[["Jin","Yijie",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 06:44:28 GMT"},{"version":"v2","created":"Thu, 12 Sep 2024 16:11:41 GMT"}],"updateDate":"2024-09-13","timestamp":1724741068000,"abstract":"  Multimodal Sentiment Analysis (MSA) leverages multiple data modals to analyze\nhuman sentiment. Existing MSA models generally employ cutting-edge multimodal\nfusion and representation learning-based methods to promote MSA capability.\nHowever, there are two key challenges: (i) in existing multimodal fusion\nmethods, the decoupling of modal combinations and tremendous parameter\nredundancy, lead to insufficient fusion performance and efficiency; (ii) a\nchallenging trade-off exists between representation capability and\ncomputational overhead in unimodal feature extractors and encoders. Our\nproposed GSIFN incorporates two main components to solve these problems: (i) a\ngraph-structured and interlaced-masked multimodal Transformer. It adopts the\nInterlaced Mask mechanism to construct robust multimodal graph embedding,\nachieve all-modal-in-one Transformer-based fusion, and greatly reduce the\ncomputational overhead; (ii) a self-supervised learning framework with low\ncomputational overhead and high performance, which utilizes a parallelized LSTM\nwith matrix memory to enhance non-verbal modal features for unimodal label\ngeneration. Evaluated on the MSA datasets CMU-MOSI, CMU-MOSEI, and CH-SIMS,\nGSIFN demonstrates superior performance with significantly lower computational\noverhead compared with previous state-of-the-art models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}