{"id":"2407.12841","title":"What to do if language models disagree? Black-box model ensembling for\n  textual and visual question answering","authors":"Yuxi Xia, Kilm Zaporojets, Benjamin Roth","authorsParsed":[["Xia","Yuxi",""],["Zaporojets","Kilm",""],["Roth","Benjamin",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 12:59:10 GMT"}],"updateDate":"2024-07-19","timestamp":1720097950000,"abstract":"  A diverse range of large language models (LLMs), e.g., ChatGPT, and visual\nquestion answering (VQA) models, e.g., BLIP, have been developed for solving\ntextual and visual question answering tasks. However, both LLMs and VQA models\nencounter challenges when applied to task-specific datasets. Fine-tuning these\nmodels is either difficult, as it requires access via APIs, rendering them as\nblack-boxes, or costly due to the need of tuning a large number of parameters.\nTo address this, we introduce InfoSel, a data-efficient and lightweight\nensemble method that learns to dynamically pick the winner from existing\nblack-box models for predictions on both textual and multimodal visual question\nanswering tasks. Unlike traditional ensemble models, InfoSel does not rely on\nprediction probabilities or confidences, which typically are not available in\nblack-box models. Experimental results on four datasets demonstrate that our\napproach achieves an absolute increase of up to +5.27% in the F1-score compared\nto standalone LLMs. Remarkably, this improvement is achieved by utilizing only\n1K training instances and 110M model parameters for training task-specific\nensemble models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GQREUpYtv5xxTHfKUIrDWVgpeJc4ulLzJIikXrd_6rE","pdfSize":"2743637"}
