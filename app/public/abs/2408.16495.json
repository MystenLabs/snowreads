{"id":"2408.16495","title":"On-device AI: Quantization-aware Training of Transformers in Time-Series","authors":"Tianheng Ling, Gregor Schiele","authorsParsed":[["Ling","Tianheng",""],["Schiele","Gregor",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 12:49:22 GMT"}],"updateDate":"2024-08-30","timestamp":1724935762000,"abstract":"  Artificial Intelligence (AI) models for time-series in pervasive computing\nkeep getting larger and more complicated. The Transformer model is by far the\nmost compelling of these AI models. However, it is difficult to obtain the\ndesired performance when deploying such a massive model on a sensor device with\nlimited resources. My research focuses on optimizing the Transformer model for\ntime-series forecasting tasks. The optimized model will be deployed as hardware\naccelerators on embedded Field Programmable Gate Arrays (FPGAs). I will\ninvestigate the impact of applying Quantization-aware Training to the\nTransformer model to reduce its size and runtime memory footprint while\nmaximizing the advantages of FPGAs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}