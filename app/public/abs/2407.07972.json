{"id":"2407.07972","title":"Deconstructing What Makes a Good Optimizer for Language Models","authors":"Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, Sham\n  Kakade","authorsParsed":[["Zhao","Rosie",""],["Morwani","Depen",""],["Brandfonbrener","David",""],["Vyas","Nikhil",""],["Kakade","Sham",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 18:11:40 GMT"}],"updateDate":"2024-07-12","timestamp":1720635100000,"abstract":"  Training language models becomes increasingly expensive with scale, prompting\nnumerous attempts to improve optimization efficiency. Despite these efforts,\nthe Adam optimizer remains the most widely used, due to a prevailing view that\nit is the most effective approach. We aim to compare several optimization\nalgorithms, including SGD, Adafactor, Adam, and Lion, in the context of\nautoregressive language modeling across a range of model sizes,\nhyperparameters, and architecture variants. Our findings indicate that, except\nfor SGD, these algorithms all perform comparably both in their optimal\nperformance and also in terms of how they fare across a wide range of\nhyperparameter choices. Our results suggest to practitioners that the choice of\noptimizer can be guided by practical considerations like memory constraints and\nease of implementation, as no single algorithm emerged as a clear winner in\nterms of performance or stability to hyperparameter misspecification.\n  Given our findings, we further dissect these approaches, examining two\nsimplified versions of Adam: a) signed momentum (Signum) which we see recovers\nboth the performance and hyperparameter stability of Adam and b) Adalayer, a\nlayerwise variant of Adam which we introduce to study Adam's preconditioning.\nExamining Adalayer leads us to the conclusion that the largest impact of Adam's\npreconditioning is restricted to the last layer and LayerNorm parameters, and,\nperhaps surprisingly, the remaining layers can be trained with SGD.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}