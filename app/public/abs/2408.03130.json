{"id":"2408.03130","title":"Inference Optimizations for Large Language Models: Effects, Challenges,\n  and Practical Considerations","authors":"Leo Donisch, Sigurd Schacht, Carsten Lanquillon","authorsParsed":[["Donisch","Leo",""],["Schacht","Sigurd",""],["Lanquillon","Carsten",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 12:07:32 GMT"}],"updateDate":"2024-08-07","timestamp":1722946052000,"abstract":"  Large language models are ubiquitous in natural language processing because\nthey can adapt to new tasks without retraining. However, their sheer scale and\ncomplexity present unique challenges and opportunities, prompting researchers\nand practitioners to explore novel model training, optimization, and deployment\nmethods. This literature review focuses on various techniques for reducing\nresource requirements and compressing large language models, including\nquantization, pruning, knowledge distillation, and architectural optimizations.\nThe primary objective is to explore each method in-depth and highlight its\nunique challenges and practical applications. The discussed methods are\ncategorized into a taxonomy that presents an overview of the optimization\nlandscape and helps navigate it to understand the research trajectory better.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"wDzaZgutgfAFJ3JbIRL1pugSMXOs0ZU1GBdy63XcbjU","pdfSize":"459518"}
