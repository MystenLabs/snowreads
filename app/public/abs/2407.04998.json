{"id":"2407.04998","title":"The Solution for the 5th GCAIAC Zero-shot Referring Expression\n  Comprehension Challenge","authors":"Longfei Huang, Feng Yu, Zhihao Guan, Zhonghua Wan, Yang Yang","authorsParsed":[["Huang","Longfei",""],["Yu","Feng",""],["Guan","Zhihao",""],["Wan","Zhonghua",""],["Yang","Yang",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 08:31:33 GMT"}],"updateDate":"2024-07-09","timestamp":1720254693000,"abstract":"  This report presents a solution for the zero-shot referring expression\ncomprehension task. Visual-language multimodal base models (such as CLIP, SAM)\nhave gained significant attention in recent years as a cornerstone of\nmainstream research. One of the key applications of multimodal base models lies\nin their ability to generalize to zero-shot downstream tasks. Unlike\ntraditional referring expression comprehension, zero-shot referring expression\ncomprehension aims to apply pre-trained visual-language models directly to the\ntask without specific training. Recent studies have enhanced the zero-shot\nperformance of multimodal base models in referring expression comprehension\ntasks by introducing visual prompts. To address the zero-shot referring\nexpression comprehension challenge, we introduced a combination of visual\nprompts and considered the influence of textual prompts, employing joint\nprediction tailored to the data characteristics. Ultimately, our approach\nachieved accuracy rates of 84.825 on the A leaderboard and 71.460 on the B\nleaderboard, securing the first position.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}