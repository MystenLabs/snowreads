{"id":"2407.15913","title":"Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot\n  Generalization of Vision-Language Models","authors":"Raza Imam, Hanan Gani, Muhammad Huzaifa, Karthik Nandakumar","authorsParsed":[["Imam","Raza",""],["Gani","Hanan",""],["Huzaifa","Muhammad",""],["Nandakumar","Karthik",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 17:59:19 GMT"}],"updateDate":"2024-07-24","timestamp":1721671159000,"abstract":"  The conventional modus operandi for adapting pre-trained vision-language\nmodels (VLMs) during test-time involves tuning learnable prompts, ie, test-time\nprompt tuning. This paper introduces Test-Time Low-rank adaptation (TTL) as an\nalternative to prompt tuning for zero-shot generalization of large-scale VLMs.\nTaking inspiration from recent advancements in efficiently fine-tuning large\nlanguage models, TTL offers a test-time parameter-efficient adaptation approach\nthat updates the attention weights of the transformer encoder by maximizing\nprediction confidence. The self-supervised confidence maximization objective is\nspecified using a weighted entropy loss that enforces consistency among\npredictions of augmented samples. TTL introduces only a small amount of\ntrainable parameters for low-rank adapters in the model space while keeping the\nprompts and backbone frozen. Extensive experiments on a variety of natural\ndistribution and cross-domain tasks show that TTL can outperform other\ntechniques for test-time optimization of VLMs in strict zero-shot settings.\nSpecifically, TTL outperforms test-time prompt tuning baselines with a\nsignificant improvement on average. Our code is available at at\nhttps://github.com/Razaimam45/TTL-Test-Time-Low-Rank-Adaptation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"tKFOietB25SeNoTCGOu9kCxjw7A0Fz4yZjXd9YY1XQI","pdfSize":"12849629","objectId":"0x5ae5f10676b041012484756a83f2e384a630672dcd959de312dd7dee9cd7c979","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
