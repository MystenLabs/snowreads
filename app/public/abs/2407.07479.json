{"id":"2407.07479","title":"How to Make Cross Encoder a Good Teacher for Efficient Image-Text\n  Retrieval?","authors":"Yuxin Chen, Zongyang Ma, Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Bing\n  Li, Junfu Pu, Ying Shan, Xiaojuan Qi, Weiming Hu","authorsParsed":[["Chen","Yuxin",""],["Ma","Zongyang",""],["Zhang","Ziqi",""],["Qi","Zhongang",""],["Yuan","Chunfeng",""],["Li","Bing",""],["Pu","Junfu",""],["Shan","Ying",""],["Qi","Xiaojuan",""],["Hu","Weiming",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 09:10:01 GMT"}],"updateDate":"2024-07-11","timestamp":1720602601000,"abstract":"  Dominant dual-encoder models enable efficient image-text retrieval but suffer\nfrom limited accuracy while the cross-encoder models offer higher accuracy at\nthe expense of efficiency. Distilling cross-modality matching knowledge from\ncross-encoder to dual-encoder provides a natural approach to harness their\nstrengths. Thus we investigate the following valuable question: how to make\ncross-encoder a good teacher for dual-encoder? Our findings are threefold:(1)\nCross-modal similarity score distribution of cross-encoder is more concentrated\nwhile the result of dual-encoder is nearly normal making vanilla logit\ndistillation less effective. However ranking distillation remains practical as\nit is not affected by the score distribution.(2) Only the relative order\nbetween hard negatives conveys valid knowledge while the order information\nbetween easy negatives has little significance.(3) Maintaining the coordination\nbetween distillation loss and dual-encoder training loss is beneficial for\nknowledge transfer. Based on these findings we propose a novel Contrastive\nPartial Ranking Distillation (CPRD) method which implements the objective of\nmimicking relative order between hard negative samples with contrastive\nlearning. This approach coordinates with the training of the dual-encoder\neffectively transferring valid knowledge from the cross-encoder to the\ndual-encoder. Extensive experiments on image-text retrieval and ranking tasks\nshow that our method surpasses other distillation methods and significantly\nimproves the accuracy of dual-encoder.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}