{"id":"2407.09190","title":"Zeroth-Order Katyusha: An Accelerated Derivative-Free Method for\n  Composite Convex Optimization","authors":"Silan Zhang, Yujie Tang","authorsParsed":[["Zhang","Silan",""],["Tang","Yujie",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 11:44:56 GMT"}],"updateDate":"2024-07-15","timestamp":1720784696000,"abstract":"  We investigate accelerated zeroth-order algorithms for smooth composite\nconvex optimization problems. While for unconstrained optimization, existing\nmethods that merge 2-point zeroth-order gradient estimators with first-order\nframeworks usually lead to satisfactory performance, for constrained/composite\nproblems, there is still a gap in the complexity bound that is related to the\nnon-vanishing variance of the 2-point gradient estimator near an optimal point.\nTo bridge this gap, we propose the Zeroth-Order Loopless Katyusha\n(ZO-L-Katyusha) algorithm, leveraging the variance reduction as well as\nacceleration techniques from the first-order loopless Katyusha algorithm. We\nshow that ZO-L-Katyusha is able to achieve accelerated linear convergence for\ncompositve smooth and strongly convex problems, and has the same oracle\ncomplexity as the unconstrained case. Moreover, the number of function queries\nto construct a zeroth-order gradient estimator in ZO-L-Katyusha can be made to\nbe O(1) on average. These results suggest that ZO-L-Katyusha provides a\npromising approach towards bridging the gap in the complexity bound for\nzeroth-order composite optimization.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}