{"id":"2407.14212","title":"Braille-to-Speech Generator: Audio Generation Based on Joint Fine-Tuning\n  of CLIP and Fastspeech2","authors":"Chun Xu, En-Wei Sun","authorsParsed":[["Xu","Chun",""],["Sun","En-Wei",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 11:18:44 GMT"}],"updateDate":"2024-07-22","timestamp":1721387924000,"abstract":"  An increasing number of Chinese people are troubled by different degrees of\nvisual impairment, which has made the modal conversion between a single image\nor video frame in the visual field and the audio expressing the same\ninformation a research hotspot. Deep learning technologies such as OCR+Vocoder\nand Im2Wav enable English audio synthesis or image-to-sound matching in a\nself-supervised manner. However, the audio data used for training is limited\nand English is not universal for visually impaired people with different\neducational levels. Therefore, for the sake of solving the problems of data\nvolume and language applicability to improve the reading efficiency of visually\nimpaired people, a set of image-to-speech framework CLIP-KNN-Fastspeech2 based\non the Chinese context was constructed. The framework integrates multiple basic\nmodels and adopts the strategy of independent pre-training and joint\nfine-tuning. First, the Chinese CLIP and Fastspeech2 text-to-speech models were\npre-trained on two public datasets, MUGE and Baker, respectively, and their\nconvergence was verified. Subsequently, joint fine-tuning was performed using a\nself-built Braille image dataset. Experimental results on multiple public\ndatasets such as VGGSound, Flickr8k, ImageHear, and the self-built Braille\ndataset BIT-DP show that the model has improved objective indicators such as\nBLEU4,FAD(Fr\\'echet Audio Distance), WER(Word Error Ratio), and even inference\nspeed. This verifies that the constructed model still has the ability to\nsynthesize high-quality speech under limited data, and also proves the\neffectiveness of the joint training strategy that integrates multiple basic\nmodels.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}