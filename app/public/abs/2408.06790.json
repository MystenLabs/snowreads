{"id":"2408.06790","title":"Residual Deep Reinforcement Learning for Inverter-based Volt-Var Control","authors":"Qiong Liu, Ye Guo, Lirong Deng, Haotian Liu, Dongyu Li, and Hongbin\n  Sun","authorsParsed":[["Liu","Qiong",""],["Guo","Ye",""],["Deng","Lirong",""],["Liu","Haotian",""],["Li","Dongyu",""],["Sun","Hongbin",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 10:17:20 GMT"}],"updateDate":"2024-08-14","timestamp":1723544240000,"abstract":"  A residual deep reinforcement learning (RDRL) approach is proposed by\nintegrating DRL with model-based optimization for inverter-based volt-var\ncontrol in active distribution networks when the accurate power flow model is\nunknown. RDRL learns a residual action with a reduced residual action space,\nbased on the action of the model-based approach with an approximate model. RDRL\ninherits the control capability of the approximate-model-based optimization and\nenhances the policy optimization capability by residual policy learning.\nAdditionally, it improves the approximation accuracy of the critic and reduces\nthe search difficulties of the actor by reducing residual action space. To\naddress the issues of \"too small\" or \"too large\" residual action space of RDRL\nand further improve the optimization performance, we extend RDRL to a boosting\nRDRL approach. It selects a much smaller residual action space and learns a\nresidual policy by using the policy of RDRL as a base policy. Simulations\ndemonstrate that RDRL and boosting RDRL improve the optimization performance\nconsiderably throughout the learning stage and verify their rationales\npoint-by-point, including 1) inheriting the capability of the approximate\nmodel-based optimization, 2) residual policy learning, and 3) learning in a\nreduced action space.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Systems and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}