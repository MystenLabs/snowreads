{"id":"2408.04998","title":"ProFuser: Progressive Fusion of Large Language Models","authors":"Tianyuan Shi, Fanqi Wan, Canbin Huang, Xiaojun Quan, Chenliang Li,\n  Ming Yan, Ji Zhang","authorsParsed":[["Shi","Tianyuan",""],["Wan","Fanqi",""],["Huang","Canbin",""],["Quan","Xiaojun",""],["Li","Chenliang",""],["Yan","Ming",""],["Zhang","Ji",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 11:18:29 GMT"}],"updateDate":"2024-08-12","timestamp":1723202309000,"abstract":"  While fusing the capacities and advantages of various large language models\n(LLMs) offers a pathway to construct more powerful and versatile models, a\nfundamental challenge is to properly select advantageous model during the\ntraining. Existing fusion methods primarily focus on the training mode that\nuses cross entropy on ground truth in a teacher-forcing setup to measure a\nmodel's advantage, which may provide limited insight towards model advantage.\nIn this paper, we introduce a novel approach that enhances the fusion process\nby incorporating both the training and inference modes. Our method evaluates\nmodel advantage not only through cross entropy during training but also by\nconsidering inference outputs, providing a more comprehensive assessment. To\ncombine the two modes effectively, we introduce ProFuser to progressively\ntransition from inference mode to training mode. To validate ProFuser's\neffectiveness, we fused three models, including vicuna-7b-v1.5,\nLlama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance\nin knowledge, reasoning, and safety compared to baseline methods.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}