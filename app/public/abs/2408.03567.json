{"id":"2408.03567","title":"Unlocking Exocentric Video-Language Data for Egocentric Video\n  Representation Learning","authors":"Zi-Yi Dou, Xitong Yang, Tushar Nagarajan, Huiyu Wang, Jing Huang,\n  Nanyun Peng, Kris Kitani, Fu-Jen Chu","authorsParsed":[["Dou","Zi-Yi",""],["Yang","Xitong",""],["Nagarajan","Tushar",""],["Wang","Huiyu",""],["Huang","Jing",""],["Peng","Nanyun",""],["Kitani","Kris",""],["Chu","Fu-Jen",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 06:10:45 GMT"}],"updateDate":"2024-08-08","timestamp":1723011045000,"abstract":"  We present EMBED (Egocentric Models Built with Exocentric Data), a method\ndesigned to transform exocentric video-language data for egocentric video\nrepresentation learning. Large-scale exocentric data covers diverse activities\nwith significant potential for egocentric learning, but inherent disparities\nbetween egocentric and exocentric data pose challenges in utilizing one view\nfor the other seamlessly. Egocentric videos predominantly feature close-up\nhand-object interactions, whereas exocentric videos offer a broader perspective\non human activities. Additionally, narratives in egocentric datasets are\ntypically more action-centric and closely linked with the visual content, in\ncontrast to the narrative styles found in exocentric datasets. To address these\nchallenges, we employ a data transformation framework to adapt exocentric data\nfor egocentric training, focusing on identifying specific video clips that\nemphasize hand-object interactions and transforming narration styles to align\nwith egocentric perspectives. By applying both vision and language style\ntransfer, our framework creates a new egocentric dataset derived from\nexocentric video-language data. Through extensive evaluations, we demonstrate\nthe effectiveness of EMBED, achieving state-of-the-art results across various\negocentric downstream tasks, including an absolute improvement of 4.7% on the\nEpic-Kitchens-100 multi-instance retrieval and 6.2% on the EGTEA classification\nbenchmarks in zero-shot settings. Furthermore, EMBED enables egocentric\nvideo-language models to perform competitively in exocentric tasks. Finally, we\nshowcase EMBED's application across various exocentric datasets, exhibiting\nstrong generalization capabilities when applied to different exocentric\ndatasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"tV9aVXjBIDhK7_PeEr3f_1b7wkMYYjkgXr_-FXA_TNw","pdfSize":"7044563"}
