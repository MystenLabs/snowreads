{"id":"2408.15034","title":"MONAS: Efficient Zero-Shot Neural Architecture Search for MCUs","authors":"Ye Qiao, Haocheng Xu, Yifan Zhang, Sitao Huang","authorsParsed":[["Qiao","Ye",""],["Xu","Haocheng",""],["Zhang","Yifan",""],["Huang","Sitao",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 10:24:45 GMT"}],"updateDate":"2024-08-28","timestamp":1724667885000,"abstract":"  Neural Architecture Search (NAS) has proven effective in discovering new\nConvolutional Neural Network (CNN) architectures, particularly for scenarios\nwith well-defined accuracy optimization goals. However, previous approaches\noften involve time-consuming training on super networks or intensive\narchitecture sampling and evaluations. Although various zero-cost proxies\ncorrelated with CNN model accuracy have been proposed for efficient\narchitecture search without training, their lack of hardware consideration\nmakes it challenging to target highly resource-constrained edge devices such as\nmicrocontroller units (MCUs). To address these challenges, we introduce MONAS,\na novel hardware-aware zero-shot NAS framework specifically designed for MCUs\nin edge computing. MONAS incorporates hardware optimality considerations into\nthe search process through our proposed MCU hardware latency estimation model.\nBy combining this with specialized performance indicators (proxies), MONAS\nidentifies optimal neural architectures without incurring heavy training and\nevaluation costs, optimizing for both hardware latency and accuracy under\nresource constraints. MONAS achieves up to a 1104x improvement in search\nefficiency over previous work targeting MCUs and can discover CNN models with\nover 3.23x faster inference on MCUs while maintaining similar accuracy compared\nto more general NAS approaches.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}