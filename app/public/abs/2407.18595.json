{"id":"2407.18595","title":"LinguaLinker: Audio-Driven Portraits Animation with Implicit Facial\n  Control Enhancement","authors":"Rui Zhang, Yixiao Fang, Zhengnan Lu, Pei Cheng, Zebiao Huang, Bin Fu","authorsParsed":[["Zhang","Rui",""],["Fang","Yixiao",""],["Lu","Zhengnan",""],["Cheng","Pei",""],["Huang","Zebiao",""],["Fu","Bin",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 08:30:06 GMT"}],"updateDate":"2024-07-29","timestamp":1721982606000,"abstract":"  This study delves into the intricacies of synchronizing facial dynamics with\nmultilingual audio inputs, focusing on the creation of visually compelling,\ntime-synchronized animations through diffusion-based techniques. Diverging from\ntraditional parametric models for facial animation, our approach, termed\nLinguaLinker, adopts a holistic diffusion-based framework that integrates\naudio-driven visual synthesis to enhance the synergy between auditory stimuli\nand visual responses. We process audio features separately and derive the\ncorresponding control gates, which implicitly govern the movements in the\nmouth, eyes, and head, irrespective of the portrait's origin. The advanced\naudio-driven visual synthesis mechanism provides nuanced control but keeps the\ncompatibility of output video and input audio, allowing for a more tailored and\neffective portrayal of distinct personas across different languages. The\nsignificant improvements in the fidelity of animated portraits, the accuracy of\nlip-syncing, and the appropriate motion variations achieved by our method\nrender it a versatile tool for animating any portrait in any language.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}