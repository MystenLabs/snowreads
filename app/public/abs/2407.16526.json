{"id":"2407.16526","title":"Imperfect Vision Encoders: Efficient and Robust Tuning for\n  Vision-Language Models","authors":"Aristeidis Panos, Rahaf Aljundi, Daniel Olmeda Reino and Richard E\n  Turner","authorsParsed":[["Panos","Aristeidis",""],["Aljundi","Rahaf",""],["Reino","Daniel Olmeda",""],["Turner","Richard E",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 14:39:40 GMT"}],"updateDate":"2024-07-24","timestamp":1721745580000,"abstract":"  Vision language models (VLMs) demonstrate impressive capabilities in visual\nquestion answering and image captioning, acting as a crucial link between\nvisual and language models. However, existing open-source VLMs heavily rely on\npretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness\nacross diverse domains, it still exhibits non-negligible image understanding\nerrors. These errors propagate to the VLM responses, resulting in sub-optimal\nperformance. In our work, we propose an efficient and robust method for\nupdating vision encoders within VLMs. Our approach selectively and locally\nupdates encoders, leading to substantial performance improvements on data where\nprevious mistakes occurred, while maintaining overall robustness. Furthermore,\nwe demonstrate the effectiveness of our method during continual few-shot\nupdates. Theoretical grounding, generality, and computational efficiency\ncharacterize our approach.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"QMAt4De--lluxrRZCCQCaRt_1x3n18vsRlZnemmGXkA","pdfSize":"15848771"}
