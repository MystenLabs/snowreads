{"id":"2407.14814","title":"FMamba: Mamba based on Fast-attention for Multivariate Time-series\n  Forecasting","authors":"Shusen Ma, Yu Kang, Peng Bai, and Yun-Bo Zhao","authorsParsed":[["Ma","Shusen",""],["Kang","Yu",""],["Bai","Peng",""],["Zhao","Yun-Bo",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 09:14:05 GMT"}],"updateDate":"2024-07-23","timestamp":1721466845000,"abstract":"  In multivariate time-series forecasting (MTSF), extracting the temporal\ncorrelations of the input sequences is crucial. While popular Transformer-based\npredictive models can perform well, their quadratic computational complexity\nresults in inefficiency and high overhead. The recently emerged Mamba, a\nselective state space model, has shown promising results in many fields due to\nits strong temporal feature extraction capabilities and linear computational\ncomplexity. However, due to the unilateral nature of Mamba, channel-independent\npredictive models based on Mamba cannot attend to the relationships among all\nvariables in the manner of Transformer-based models. To address this issue, we\ncombine fast-attention with Mamba to introduce a novel framework named FMamba\nfor MTSF. Technically, we first extract the temporal features of the input\nvariables through an embedding layer, then compute the dependencies among input\nvariables via the fast-attention module. Subsequently, we use Mamba to\nselectively deal with the input features and further extract the temporal\ndependencies of the variables through the multi-layer perceptron block\n(MLP-block). Finally, FMamba obtains the predictive results through the\nprojector, a linear layer. Experimental results on eight public datasets\ndemonstrate that FMamba can achieve state-of-the-art performance while\nmaintaining low computational overhead.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}