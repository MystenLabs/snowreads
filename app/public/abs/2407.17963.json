{"id":"2407.17963","title":"Relating the Seemingly Unrelated: Principled Understanding of\n  Generalization for Generative Models in Arithmetic Reasoning Tasks","authors":"Xingcheng Xu, Zibo Zhao, Haipeng Zhang, Yanqing Yang","authorsParsed":[["Xu","Xingcheng",""],["Zhao","Zibo",""],["Zhang","Haipeng",""],["Yang","Yanqing",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 11:35:22 GMT"}],"updateDate":"2024-07-26","timestamp":1721907322000,"abstract":"  Large language models (LLMs) have demonstrated impressive versatility across\nnumerous tasks, yet their generalization capabilities remain poorly understood.\nTo investigate these behaviors, arithmetic tasks serve as important venues. In\nprevious studies, seemingly unrelated mysteries still exist -- (1) models with\nappropriate positional embeddings can correctly perform longer unseen\narithmetic operations such as addition, but their effectiveness varies in more\ncomplex tasks like multiplication; (2) models perform well for longer unseen\ncases in modular addition under specific moduli (e.g., modulo 100) but struggle\nunder very close moduli (e.g., modulo 101), regardless of the positional\nencoding used. We believe previous studies have been treating the symptoms\nrather than addressing the root cause -- they have paid excessive attention to\nimproving model components, while overlooking the differences in task\nproperties that may be the real drivers. This is confirmed by our unified\ntheoretical framework for different arithmetic scenarios. For example, unlike\nmultiplication, the digital addition task has the property of translation\ninvariance which naturally aligns with the relative positional encoding, and\nthis combination leads to successful generalization of addition to unseen\nlonger domains. The discrepancy in operations modulo 100 and 101 arises from\nthe base. Modulo 100, unlike 101, is compatible with the decimal system (base\n10), such that unseen information in digits beyond the units digit and the tens\ndigit is actually not needed for the task. Extensive experiments with GPT-like\nmodels validate our theoretical predictions. These findings deepen our\nunderstanding of the generalization mechanisms, and facilitate more\ndata-efficient model training and objective-oriented AI alignment.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"BTcBlx18G0oDaqVP0eqldQx3ryxFF0-x3Ri-ynaFu5Y","pdfSize":"6194146","objectId":"0xb4b26998dc4168c2ed604b898126493f12f3d6c97832816c29049aecf3cfe0fc","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
