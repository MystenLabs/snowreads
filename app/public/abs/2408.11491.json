{"id":"2408.11491","title":"Nothing in Excess: Mitigating the Exaggerated Safety for LLMs via\n  Safety-Conscious Activation Steering","authors":"Zouying Cao, Yifei Yang, Hai Zhao","authorsParsed":[["Cao","Zouying",""],["Yang","Yifei",""],["Zhao","Hai",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 10:01:34 GMT"}],"updateDate":"2024-08-22","timestamp":1724234494000,"abstract":"  Safety alignment is indispensable for Large language models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}