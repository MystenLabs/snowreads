{"id":"2407.08892","title":"Characterizing Prompt Compression Methods for Long Context Inference","authors":"Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Kurt Keutzer, Amir\n  Gholami","authorsParsed":[["Jha","Siddharth",""],["Erdogan","Lutfi Eren",""],["Kim","Sehoon",""],["Keutzer","Kurt",""],["Gholami","Amir",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 23:34:32 GMT"}],"updateDate":"2024-07-15","timestamp":1720740872000,"abstract":"  Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}