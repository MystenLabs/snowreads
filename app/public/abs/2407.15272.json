{"id":"2407.15272","title":"MIBench: Evaluating Multimodal Large Language Models over Multiple\n  Images","authors":"Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya Jiang, Ming Yan, Ji\n  Zhang, Fei Huang, Chunfeng Yuan, Bing Li, Weiming Hu","authorsParsed":[["Liu","Haowei",""],["Zhang","Xi",""],["Xu","Haiyang",""],["Shi","Yaya",""],["Jiang","Chaoya",""],["Yan","Ming",""],["Zhang","Ji",""],["Huang","Fei",""],["Yuan","Chunfeng",""],["Li","Bing",""],["Hu","Weiming",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 21:22:58 GMT"}],"updateDate":"2024-07-23","timestamp":1721596978000,"abstract":"  Built on the power of LLMs, numerous multimodal large language models (MLLMs)\nhave recently achieved remarkable performance on various vision-language tasks\nacross multiple benchmarks. However, most existing MLLMs and benchmarks\nprimarily focus on single-image input scenarios, leaving the performance of\nMLLMs when handling realistic multiple images remain underexplored. Although a\nfew benchmarks consider multiple images, their evaluation dimensions and\nsamples are very limited. Therefore, in this paper, we propose a new benchmark\nMIBench, to comprehensively evaluate fine-grained abilities of MLLMs in\nmulti-image scenarios. Specifically, MIBench categorizes the multi-image\nabilities into three scenarios: multi-image instruction (MII), multimodal\nknowledge-seeking (MKS) and multimodal in-context learning (MIC), and\nconstructs 13 tasks with a total of 13K annotated samples. During data\nconstruction, for MII and MKS, we extract correct options from manual\nannotations and create challenging distractors to obtain multiple-choice\nquestions. For MIC, to enable an in-depth evaluation, we set four sub-tasks and\ntransform the original datasets into in-context learning formats. We evaluate\nseveral open-source MLLMs and close-source MLLMs on the proposed MIBench. The\nresults reveal that although current models excel in single-image tasks, they\nexhibit significant shortcomings when faced with multi-image inputs, such as\nconfused fine-grained perception, limited multi-image reasoning, and unstable\nin-context learning. The annotated data in MIBench is available at\nhttps://huggingface.co/datasets/StarBottle/MIBench.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gsVrIbHJ0CPWEPGVGDfpdzg3iLkpOdp_8Q8xl86XdO4","pdfSize":"3615527"}
