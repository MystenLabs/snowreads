{"id":"2408.10711","title":"Investigating Context Effects in Similarity Judgements in Large Language\n  Models","authors":"Sagar Uprety, Amit Kumar Jaiswal, Haiming Liu, Dawei Song","authorsParsed":[["Uprety","Sagar",""],["Jaiswal","Amit Kumar",""],["Liu","Haiming",""],["Song","Dawei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 10:26:02 GMT"}],"updateDate":"2024-08-21","timestamp":1724149562000,"abstract":"  Large Language Models (LLMs) have revolutionised the capability of AI models\nin comprehending and generating natural language text. They are increasingly\nbeing used to empower and deploy agents in real-world scenarios, which make\ndecisions and take actions based on their understanding of the context.\nTherefore researchers, policy makers and enterprises alike are working towards\nensuring that the decisions made by these agents align with human values and\nuser expectations. That being said, human values and decisions are not always\nstraightforward to measure and are subject to different cognitive biases. There\nis a vast section of literature in Behavioural Science which studies biases in\nhuman judgements. In this work we report an ongoing investigation on alignment\nof LLMs with human judgements affected by order bias. Specifically, we focus on\na famous human study which showed evidence of order effects in similarity\njudgements, and replicate it with various popular LLMs. We report the different\nsettings where LLMs exhibit human-like order effect bias and discuss the\nimplications of these findings to inform the design and development of LLM\nbased applications.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}