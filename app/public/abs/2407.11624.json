{"id":"2407.11624","title":"Rethinking Fair Graph Neural Networks from Re-balancing","authors":"Zhixun Li, Yushun Dong, Qiang Liu, Jeffrey Xu Yu","authorsParsed":[["Li","Zhixun",""],["Dong","Yushun",""],["Liu","Qiang",""],["Yu","Jeffrey Xu",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 11:39:27 GMT"}],"updateDate":"2024-07-17","timestamp":1721129967000,"abstract":"  Driven by the powerful representation ability of Graph Neural Networks\n(GNNs), plentiful GNN models have been widely deployed in many real-world\napplications. Nevertheless, due to distribution disparities between different\ndemographic groups, fairness in high-stake decision-making systems is receiving\nincreasing attention. Although lots of recent works devoted to improving the\nfairness of GNNs and achieved considerable success, they all require\nsignificant architectural changes or additional loss functions requiring more\nhyper-parameter tuning. Surprisingly, we find that simple re-balancing methods\ncan easily match or surpass existing fair GNN methods. We claim that the\nimbalance across different demographic groups is a significant source of\nunfairness, resulting in imbalanced contributions from each group to the\nparameters updating. However, these simple re-balancing methods have their own\nshortcomings during training. In this paper, we propose FairGB, Fair Graph\nNeural Network via re-Balancing, which mitigates the unfairness of GNNs by\ngroup balancing. Technically, FairGB consists of two modules: counterfactual\nnode mixup and contribution alignment loss. Firstly, we select counterfactual\npairs across inter-domain and inter-class, and interpolate the ego-networks to\ngenerate new samples. Guided by analysis, we can reveal the debiasing mechanism\nof our model by the causal view and prove that our strategy can make sensitive\nattributes statistically independent from target labels. Secondly, we reweigh\nthe contribution of each group according to gradients. By combining these two\nmodules, they can mutually promote each other. Experimental results on\nbenchmark datasets show that our method can achieve state-of-the-art results\nconcerning both utility and fairness metrics. Code is available at\nhttps://github.com/ZhixunLEE/FairGB.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}