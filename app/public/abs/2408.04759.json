{"id":"2408.04759","title":"Confident magnitude-based neural network pruning","authors":"Joaquin Alvarez","authorsParsed":[["Alvarez","Joaquin",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 21:29:20 GMT"}],"updateDate":"2024-08-12","timestamp":1723152560000,"abstract":"  Pruning neural networks has proven to be a successful approach to increase\nthe efficiency and reduce the memory storage of deep learning models without\ncompromising performance. Previous literature has shown that it is possible to\nachieve a sizable reduction in the number of parameters of a deep neural\nnetwork without deteriorating its predictive capacity in one-shot pruning\nregimes. Our work builds beyond this background in order to provide rigorous\nuncertainty quantification for pruning neural networks reliably, which has not\nbeen addressed to a great extent in previous literature focusing on pruning\nmethods in computer vision settings. We leverage recent techniques on\ndistribution-free uncertainty quantification to provide finite-sample\nstatistical guarantees to compress deep neural networks, while maintaining high\nperformance. Moreover, this work presents experiments in computer vision tasks\nto illustrate how uncertainty-aware pruning is a useful approach to deploy\nsparse neural networks safely.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rxjQahd8uYOYqR7YJCYHut_nhC2Au6STZgaiqsiEmBU","pdfSize":"2335712"}
