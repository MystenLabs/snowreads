{"id":"2408.01031","title":"POA: Pre-training Once for Models of All Sizes","authors":"Yingying Zhang and Xin Guo and Jiangwei Lao and Lei Yu and Lixiang Ru\n  and Jian Wang and Guo Ye and Huimei He and Jingdong Chen and Ming Yang","authorsParsed":[["Zhang","Yingying",""],["Guo","Xin",""],["Lao","Jiangwei",""],["Yu","Lei",""],["Ru","Lixiang",""],["Wang","Jian",""],["Ye","Guo",""],["He","Huimei",""],["Chen","Jingdong",""],["Yang","Ming",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 06:13:29 GMT"}],"updateDate":"2024-08-05","timestamp":1722579209000,"abstract":"  Large-scale self-supervised pre-training has paved the way for one foundation\nmodel to handle many different vision tasks. Most pre-training methodologies\ntrain a single model of a certain size at one time. Nevertheless, various\ncomputation or storage constraints in real-world scenarios require substantial\nefforts to develop a series of models with different sizes to deploy. Thus, in\nthis study, we propose a novel tri-branch self-supervised training framework,\ntermed as POA (Pre-training Once for All), to tackle this aforementioned issue.\nOur approach introduces an innovative elastic student branch into a modern\nself-distillation paradigm. At each pre-training step, we randomly sample a\nsub-network from the original student to form the elastic student and train all\nbranches in a self-distilling fashion. Once pre-trained, POA allows the\nextraction of pre-trained models of diverse sizes for downstream tasks.\nRemarkably, the elastic student facilitates the simultaneous pre-training of\nmultiple models with different sizes, which also acts as an additional ensemble\nof models of various sizes to enhance representation learning. Extensive\nexperiments, including k-nearest neighbors, linear probing evaluation and\nassessments on multiple downstream tasks demonstrate the effectiveness and\nadvantages of our POA. It achieves state-of-the-art performance using ViT, Swin\nTransformer and ResNet backbones, producing around a hundred models with\ndifferent sizes through a single pre-training session. The code is available\nat: https://github.com/Qichuzyy/POA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}