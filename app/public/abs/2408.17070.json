{"id":"2408.17070","title":"Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using\n  Prefix-Tuning","authors":"Maxime M\\'eloux and Christophe Cerisara","authorsParsed":[["MÃ©loux","Maxime",""],["Cerisara","Christophe",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 07:54:50 GMT"}],"updateDate":"2024-09-02","timestamp":1725004490000,"abstract":"  Teaching new information to pre-trained large language models (PLM) is a\ncrucial but challenging task. Model adaptation techniques, such as fine-tuning\nand parameter-efficient training have been shown to store new facts at a slow\nrate; continual learning is an option but is costly and prone to catastrophic\nforgetting. This work studies and quantifies how PLM may learn and remember new\nworld knowledge facts that do not occur in their pre-training corpus, which\nonly contains world knowledge up to a certain date. To that purpose, we first\npropose Novel-WD, a new dataset consisting of sentences containing novel facts\nextracted from recent Wikidata updates, along with two evaluation tasks in the\nform of causal language modeling and multiple choice questions (MCQ). We make\nthis dataset freely available to the community, and release a procedure to\nlater build new versions of similar datasets with up-to-date information. We\nalso explore the use of prefix-tuning for novel information learning, and\nanalyze how much information can be stored within a given prefix. We show that\na single fact can reliably be encoded within a single prefix, and that the\nprefix capacity increases with its length and with the base model size.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}