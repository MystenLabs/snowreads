{"id":"2408.05382","title":"Optimizing Portfolio with Two-Sided Transactions and Lending: A\n  Reinforcement Learning Framework","authors":"Ali Habibnia and Mahdi Soltanzadeh","authorsParsed":[["Habibnia","Ali",""],["Soltanzadeh","Mahdi",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 23:36:58 GMT"}],"updateDate":"2024-08-13","timestamp":1723246618000,"abstract":"  This study presents a Reinforcement Learning (RL)-based portfolio management\nmodel tailored for high-risk environments, addressing the limitations of\ntraditional RL models and exploiting market opportunities through two-sided\ntransactions and lending. Our approach integrates a new environmental\nformulation with a Profit and Loss (PnL)-based reward function, enhancing the\nRL agent's ability in downside risk management and capital optimization. We\nimplemented the model using the Soft Actor-Critic (SAC) agent with a\nConvolutional Neural Network with Multi-Head Attention (CNN-MHA). This setup\neffectively manages a diversified 12-crypto asset portfolio in the Binance\nperpetual futures market, leveraging USDT for both granting and receiving loans\nand rebalancing every 4 hours, utilizing market data from the preceding 48\nhours. Tested over two 16-month periods of varying market volatility, the model\nsignificantly outperformed benchmarks, particularly in high-volatility\nscenarios, achieving higher return-to-risk ratios and demonstrating robust\nprofitability. These results confirm the model's effectiveness in leveraging\nmarket dynamics and managing risks in volatile environments like the\ncryptocurrency market.\n","subjects":["Quantitative Finance/Portfolio Management","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}