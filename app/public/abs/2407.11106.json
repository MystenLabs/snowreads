{"id":"2407.11106","title":"Deep Learning Evidence for Global Optimality of Gerver's Sofa","authors":"Kuangdai Leng, Jia Bi, Jaehoon Cha, Samuel Pinilla and Jeyan\n  Thiyagalingam","authorsParsed":[["Leng","Kuangdai",""],["Bi","Jia",""],["Cha","Jaehoon",""],["Pinilla","Samuel",""],["Thiyagalingam","Jeyan",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 14:59:32 GMT"}],"updateDate":"2024-07-17","timestamp":1721055572000,"abstract":"  The Moving Sofa Problem, formally proposed by Leo Moser in 1966, seeks to\ndetermine the largest area of a two-dimensional shape that can navigate through\nan $L$-shaped corridor with unit width. The current best lower bound is about\n2.2195, achieved by Joseph Gerver in 1992, though its global optimality remains\nunproven. In this paper, we investigate this problem by leveraging the\nuniversal approximation strength and computational efficiency of neural\nnetworks. We report two approaches, both supporting Gerver's conjecture that\nhis shape is the unique global maximum. Our first approach is continuous\nfunction learning. We drop Gerver's assumptions that i) the rotation of the\ncorridor is monotonic and symmetric and, ii) the trajectory of its corner as a\nfunction of rotation is continuously differentiable. We parameterize rotation\nand trajectory by independent piecewise linear neural networks (with input\nbeing some pseudo time), allowing for rich movements such as backward rotation\nand pure translation. We then compute the sofa area as a differentiable\nfunction of rotation and trajectory using our \"waterfall\" algorithm. Our final\nloss function includes differential terms and initial conditions, leveraging\nthe principles of physics-informed machine learning. Under such settings,\nextensive training starting from diverse function initialization and\nhyperparameters is conducted, unexceptionally showing rapid convergence to\nGerver's solution. Our second approach is via discrete optimization of the\nKallus-Romik upper bound, which converges to the maximum sofa area from above\nas the number of rotation angles increases. We uplift this number to 10000 to\nreveal its asymptotic behavior. It turns out that the upper bound yielded by\nour models does converge to Gerver's area (within an error of 0.01% when the\nnumber of angles reaches 2100). We also improve their five-angle upper bound\nfrom 2.37 to 2.3337.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DVujZC7c_SW4y90iqm5vtdjVjLuIGx1t1WNnBcmG004","pdfSize":"5281766"}
