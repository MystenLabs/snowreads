{"id":"2408.03006","title":"Dual-path Collaborative Generation Network for Emotional Video\n  Captioning","authors":"Cheng Ye, Weidong Chen, Jingyu Li, Lei Zhang, Zhendong Mao","authorsParsed":[["Ye","Cheng",""],["Chen","Weidong",""],["Li","Jingyu",""],["Zhang","Lei",""],["Mao","Zhendong",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 07:30:53 GMT"}],"updateDate":"2024-08-07","timestamp":1722929453000,"abstract":"  Emotional Video Captioning is an emerging task that aims to describe factual\ncontent with the intrinsic emotions expressed in videos. The essential of the\nEVC task is to effectively perceive subtle and ambiguous visual emotional cues\nduring the caption generation, which is neglected by the traditional video\ncaptioning. Existing emotional video captioning methods perceive global visual\nemotional cues at first, and then combine them with the video features to guide\nthe emotional caption generation, which neglects two characteristics of the EVC\ntask. Firstly, their methods neglect the dynamic subtle changes in the\nintrinsic emotions of the video, which makes it difficult to meet the needs of\ncommon scenes with diverse and changeable emotions. Secondly, as their methods\nincorporate emotional cues into each step, the guidance role of emotion is\noveremphasized, which makes factual content more or less ignored during\ngeneration. To this end, we propose a dual-path collaborative generation\nnetwork, which dynamically perceives visual emotional cues evolutions while\ngenerating emotional captions by collaborative learning. Specifically, in the\ndynamic emotion perception path, we propose a dynamic emotion evolution module,\nwhich first aggregates visual features and historical caption features to\nsummarize the global visual emotional cues, and then dynamically selects\nemotional cues required to be re-composed at each stage. Besides, in the\nadaptive caption generation path, to balance the description of factual content\nand emotional cues, we propose an emotion adaptive decoder. Thus, our methods\ncan generate emotion-related words at the necessary time step, and our caption\ngeneration balances the guidance of factual content and emotional cues well.\nExtensive experiments on three challenging datasets demonstrate the superiority\nof our approach and each proposed module.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}