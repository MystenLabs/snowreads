{"id":"2408.06699","title":"Information Geometry and Beta Link for Optimizing Sparse Variational\n  Student-t Processes","authors":"Jian Xu, Delu Zeng, John Paisley","authorsParsed":[["Xu","Jian",""],["Zeng","Delu",""],["Paisley","John",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 07:53:39 GMT"}],"updateDate":"2024-08-14","timestamp":1723535619000,"abstract":"  Recently, a sparse version of Student-t Processes, termed sparse variational\nStudent-t Processes, has been proposed to enhance computational efficiency and\nflexibility for real-world datasets using stochastic gradient descent. However,\ntraditional gradient descent methods like Adam may not fully exploit the\nparameter space geometry, potentially leading to slower convergence and\nsuboptimal performance. To mitigate these issues, we adopt natural gradient\nmethods from information geometry for variational parameter optimization of\nStudent-t Processes. This approach leverages the curvature and structure of the\nparameter space, utilizing tools such as the Fisher information matrix which is\nlinked to the Beta function in our model. This method provides robust\nmathematical support for the natural gradient algorithm when using Student's\nt-distribution as the variational distribution. Additionally, we present a\nmini-batch algorithm for efficiently computing natural gradients. Experimental\nresults across four benchmark datasets demonstrate that our method consistently\naccelerates convergence speed.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KjltX9CpDS3ucYdp7cOgw4vFfqHMKd8bPBRuZh55m9Y","pdfSize":"584704"}
