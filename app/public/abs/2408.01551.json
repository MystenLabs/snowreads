{"id":"2408.01551","title":"PiCoGen2: Piano cover generation with transfer learning approach and\n  weakly aligned data","authors":"Chih-Pin Tan, Hsin Ai, Yi-Hsin Chang, Shuen-Huei Guan, Yi-Hsuan Yang","authorsParsed":[["Tan","Chih-Pin",""],["Ai","Hsin",""],["Chang","Yi-Hsin",""],["Guan","Shuen-Huei",""],["Yang","Yi-Hsuan",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 19:45:18 GMT"}],"updateDate":"2024-08-06","timestamp":1722627918000,"abstract":"  Piano cover generation aims to create a piano cover from a pop song. Existing\napproaches mainly employ supervised learning and the training demands\nstrongly-aligned and paired song-to-piano data, which is built by remapping\npiano notes to song audio. This would, however, result in the loss of piano\ninformation and accordingly cause inconsistencies between the original and\nremapped piano versions. To overcome this limitation, we propose a transfer\nlearning approach that pre-trains our model on piano-only data and fine-tunes\nit on weakly-aligned paired data constructed without note remapping. During\npre-training, to guide the model to learn piano composition concepts instead of\nmerely transcribing audio, we use an existing lead sheet transcription model as\nthe encoder to extract high-level features from the piano recordings. The\npre-trained model is then fine-tuned on the paired song-piano data to transfer\nthe learned composition knowledge to the pop song domain. Our evaluation shows\nthat this training strategy enables our model, named PiCoGen2, to attain\nhigh-quality results, outperforming baselines on both objective and subjective\nmetrics across five pop genres.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}