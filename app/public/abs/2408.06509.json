{"id":"2408.06509","title":"Fooling SHAP with Output Shuffling Attacks","authors":"Jun Yuan, Aritra Dasgupta","authorsParsed":[["Yuan","Jun",""],["Dasgupta","Aritra",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 21:57:18 GMT"}],"updateDate":"2024-08-14","timestamp":1723499838000,"abstract":"  Explainable AI~(XAI) methods such as SHAP can help discover feature\nattributions in black-box models. If the method reveals a significant\nattribution from a ``protected feature'' (e.g., gender, race) on the model\noutput, the model is considered unfair. However, adversarial attacks can\nsubvert the detection of XAI methods. Previous approaches to constructing such\nan adversarial model require access to underlying data distribution, which may\nnot be possible in many practical scenarios. We relax this constraint and\npropose a novel family of attacks, called shuffling attacks, that are\ndata-agnostic. The proposed attack strategies can adapt any trained machine\nlearning model to fool Shapley value-based explanations. We prove that Shapley\nvalues cannot detect shuffling attacks. However, algorithms that estimate\nShapley values, such as linear SHAP and SHAP, can detect these attacks with\nvarying degrees of effectiveness. We demonstrate the efficacy of the attack\nstrategies by comparing the performance of linear SHAP and SHAP using\nreal-world datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}