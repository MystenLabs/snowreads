{"id":"2407.00740","title":"Locate&Edit: Energy-based Text Editing for Efficient, Flexible, and\n  Faithful Controlled Text Generation","authors":"Hye Ryung Son and Jay-Yoon Lee","authorsParsed":[["Son","Hye Ryung",""],["Lee","Jay-Yoon",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 16:04:29 GMT"}],"updateDate":"2024-07-02","timestamp":1719763469000,"abstract":"  Recent approaches to controlled text generation (CTG) often involve\nmanipulating the weights or logits of base language models (LMs) at decoding\ntime. However, these methods are inapplicable to latest black-box LMs and\nineffective at preserving the core semantics of the base LM's original\ngenerations. In this work, we propose Locate&Edit(L&E), an efficient and\nflexible energy-based approach to CTG, which edits text outputs from a base LM\nusing off-the-shelf energy models. Given text outputs from the base LM, L&E\nfirst locates spans that are most relevant to constraints (e.g., toxicity)\nutilizing energy models, and then edits these spans by replacing them with more\nsuitable alternatives. Importantly, our method is compatible with black-box\nLMs, as it requires only the text outputs. Also, since L&E doesn't mandate\nspecific architecture for its component models, it can work with a diverse\ncombination of available off-the-shelf models. Moreover, L&E preserves the base\nLM's original generations, by selectively modifying constraint-related aspects\nof the texts and leaving others unchanged. These targeted edits also ensure\nthat L&E operates efficiently. Our experiments confirm that L&E achieves\nsuperior semantic preservation of the base LM generations and speed, while\nsimultaneously obtaining competitive or improved constraint satisfaction.\nFurthermore, we analyze how the granularity of energy distribution impacts CTG\nperformance and find that fine-grained, regression-based energy models improve\nconstraint satisfaction, compared to conventional binary classifier energy\nmodels.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}