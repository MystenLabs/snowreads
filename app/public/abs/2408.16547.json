{"id":"2408.16547","title":"OP-Align: Object-level and Part-level Alignment for Self-supervised\n  Category-level Articulated Object Pose Estimation","authors":"Yuchen Che, Ryo Furukawa, Asako Kanezaki","authorsParsed":[["Che","Yuchen",""],["Furukawa","Ryo",""],["Kanezaki","Asako",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 14:10:14 GMT"}],"updateDate":"2024-08-30","timestamp":1724940614000,"abstract":"  Category-level articulated object pose estimation focuses on the pose\nestimation of unknown articulated objects within known categories. Despite its\nsignificance, this task remains challenging due to the varying shapes and poses\nof objects, expensive dataset annotation costs, and complex real-world\nenvironments. In this paper, we propose a novel self-supervised approach that\nleverages a single-frame point cloud to solve this task. Our model consistently\ngenerates reconstruction with a canonical pose and joint state for the entire\ninput object, and it estimates object-level poses that reduce overall pose\nvariance and part-level poses that align each part of the input with its\ncorresponding part of the reconstruction. Experimental results demonstrate that\nour approach significantly outperforms previous self-supervised methods and is\ncomparable to the state-of-the-art supervised methods. To assess the\nperformance of our model in real-world scenarios, we also introduce a new\nreal-world articulated object benchmark dataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CtBoI4bj9bN_jVyBNn_IwtR_imwIwYhjzCiHMBTxM50","pdfSize":"4906359"}
