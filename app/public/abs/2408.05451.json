{"id":"2408.05451","title":"Mathematical Models of Computation in Superposition","authors":"Kaarel H\\\"anni, Jake Mendel, Dmitry Vaintrob, Lawrence Chan","authorsParsed":[["HÃ¤nni","Kaarel",""],["Mendel","Jake",""],["Vaintrob","Dmitry",""],["Chan","Lawrence",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 06:11:48 GMT"}],"updateDate":"2024-08-13","timestamp":1723270308000,"abstract":"  Superposition -- when a neural network represents more ``features'' than it\nhas dimensions -- seems to pose a serious challenge to mechanistically\ninterpreting current AI systems. Existing theory work studies\n\\emph{representational} superposition, where superposition is only used when\npassing information through bottlenecks. In this work, we present mathematical\nmodels of \\emph{computation} in superposition, where superposition is actively\nhelpful for efficiently accomplishing the task.\n  We first construct a task of efficiently emulating a circuit that takes the\nAND of the $\\binom{m}{2}$ pairs of each of $m$ features. We construct a 1-layer\nMLP that uses superposition to perform this task up to $\\varepsilon$-error,\nwhere the network only requires $\\tilde{O}(m^{\\frac{2}{3}})$ neurons, even when\nthe input features are \\emph{themselves in superposition}. We generalize this\nconstruction to arbitrary sparse boolean circuits of low depth, and then\nconstruct ``error correction'' layers that allow deep fully-connected networks\nof width $d$ to emulate circuits of width $\\tilde{O}(d^{1.5})$ and \\emph{any}\npolynomial depth. We conclude by providing some potential applications of our\nwork for interpreting neural networks that implement computation in\nsuperposition.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"yRyppJnR_CNmIOBbGziUclTPB-5enYYuBh6tF22a8u8","pdfSize":"1335316"}
