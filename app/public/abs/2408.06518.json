{"id":"2408.06518","title":"Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in\n  Language Models","authors":"Hila Gonen, Terra Blevins, Alisa Liu, Luke Zettlemoyer, Noah A. Smith","authorsParsed":[["Gonen","Hila",""],["Blevins","Terra",""],["Liu","Alisa",""],["Zettlemoyer","Luke",""],["Smith","Noah A.",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 22:30:55 GMT"},{"version":"v2","created":"Thu, 12 Sep 2024 18:33:33 GMT"}],"updateDate":"2024-09-16","timestamp":1723501855000,"abstract":"  Despite their wide adoption, the biases and unintended behaviors of language\nmodels remain poorly understood. In this paper, we identify and characterize a\nphenomenon never discussed before, which we call semantic leakage, where models\nleak irrelevant information from the prompt into the generation in unexpected\nways. We propose an evaluation setting to detect semantic leakage both by\nhumans and automatically, curate a diverse test suite for diagnosing this\nbehavior, and measure significant semantic leakage in 13 flagship models. We\nalso show that models exhibit semantic leakage in languages besides English and\nacross different settings and generation scenarios. This discovery highlights\nyet another type of bias in language models that affects their generation\npatterns and behavior.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}