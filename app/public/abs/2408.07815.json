{"id":"2408.07815","title":"Algebraic Representations for Faster Predictions in Convolutional Neural\n  Networks","authors":"Johnny Joyce and Jan Verschelde","authorsParsed":[["Joyce","Johnny",""],["Verschelde","Jan",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 21:10:05 GMT"}],"updateDate":"2024-08-16","timestamp":1723669805000,"abstract":"  Convolutional neural networks (CNNs) are a popular choice of model for tasks\nin computer vision. When CNNs are made with many layers, resulting in a deep\nneural network, skip connections may be added to create an easier gradient\noptimization problem while retaining model expressiveness. In this paper, we\nshow that arbitrarily complex, trained, linear CNNs with skip connections can\nbe simplified into a single-layer model, resulting in greatly reduced\ncomputational requirements during prediction time. We also present a method for\ntraining nonlinear models with skip connections that are gradually removed\nthroughout training, giving the benefits of skip connections without requiring\ncomputational overhead during during prediction time. These results are\ndemonstrated with practical examples on Residual Networks (ResNet)\narchitecture.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Symbolic Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}