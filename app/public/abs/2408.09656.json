{"id":"2408.09656","title":"A Comparison of Large Language Model and Human Performance on Random\n  Number Generation Tasks","authors":"Rachel M. Harrison","authorsParsed":[["Harrison","Rachel M.",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 02:34:15 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 02:05:46 GMT"}],"updateDate":"2024-08-21","timestamp":1724034855000,"abstract":"  Random Number Generation Tasks (RNGTs) are used in psychology for examining\nhow humans generate sequences devoid of predictable patterns. By adapting an\nexisting human RNGT for an LLM-compatible environment, this preliminary study\ntests whether ChatGPT-3.5, a large language model (LLM) trained on\nhuman-generated text, exhibits human-like cognitive biases when generating\nrandom number sequences. Initial findings indicate that ChatGPT-3.5 more\neffectively avoids repetitive and sequential patterns compared to humans, with\nnotably lower repeat frequencies and adjacent number frequencies. Continued\nresearch into different models, parameters, and prompting methodologies will\ndeepen our understanding of how LLMs can more closely mimic human random\ngeneration behaviors, while also broadening their applications in cognitive and\nbehavioral science research.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Quantitative Biology/Neurons and Cognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}