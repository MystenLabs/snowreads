{"id":"2408.08994","title":"Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order\n  Bounds","authors":"Zhiyong Wang, Dongruo Zhou, John C.S. Lui, Wen Sun","authorsParsed":[["Wang","Zhiyong",""],["Zhou","Dongruo",""],["Lui","John C. S.",""],["Sun","Wen",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 19:52:53 GMT"},{"version":"v2","created":"Fri, 30 Aug 2024 02:26:29 GMT"}],"updateDate":"2024-09-02","timestamp":1723837973000,"abstract":"  Learning a transition model via Maximum Likelihood Estimation (MLE) followed\nby planning inside the learned model is perhaps the most standard and simplest\nModel-based Reinforcement Learning (RL) framework. In this work, we show that\nsuch a simple Model-based RL scheme, when equipped with optimistic and\npessimistic planning procedures, achieves strong regret and sample complexity\nbounds in online and offline RL settings. Particularly, we demonstrate that\nunder the conditions where the trajectory-wise reward is normalized between\nzero and one and the transition is time-homogenous, it achieves horizon-free\nand second-order bounds. Horizon-free means that our bounds have no polynomial\ndependence on the horizon of the Markov Decision Process. A second-order bound\nis a type of instance-dependent bound that scales with respect to the variances\nof the returns of the policies which can be small when the system is nearly\ndeterministic and (or) the optimal policy has small values. We highlight that\nour algorithms are simple, fairly standard, and indeed have been extensively\nstudied in the RL literature: they learn a model via MLE, build a version space\naround the MLE solution, and perform optimistic or pessimistic planning\ndepending on whether operating in the online or offline mode. These algorithms\ndo not rely on additional specialized algorithmic designs such as learning\nvariances and performing variance-weighted learning and thus can leverage rich\nfunction approximations that are significantly beyond linear or tabular\nstructures. The simplicity of the algorithms also implies that our horizon-free\nand second-order regret analysis is actually standard and mainly follows the\ngeneral framework of optimism/pessimism in the face of uncertainty.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}