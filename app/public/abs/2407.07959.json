{"id":"2407.07959","title":"Source Code Summarization in the Era of Large Language Models","authors":"Weisong Sun and Yun Miao and Yuekang Li and Hongyu Zhang and Chunrong\n  Fang and Yi Liu and Gelei Deng and Yang Liu and Zhenyu Chen","authorsParsed":[["Sun","Weisong",""],["Miao","Yun",""],["Li","Yuekang",""],["Zhang","Hongyu",""],["Fang","Chunrong",""],["Liu","Yi",""],["Deng","Gelei",""],["Liu","Yang",""],["Chen","Zhenyu",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 05:48:42 GMT"}],"updateDate":"2024-07-12","timestamp":1720504122000,"abstract":"  To support software developers in understanding and maintaining programs,\nvarious automatic (source) code summarization techniques have been proposed to\ngenerate a concise natural language summary (i.e., comment) for a given code\nsnippet. Recently, the emergence of large language models (LLMs) has led to a\ngreat boost in the performance of code-related tasks. In this paper, we\nundertake a systematic and comprehensive study on code summarization in the era\nof LLMs, which covers multiple aspects involved in the workflow of LLM-based\ncode summarization. Specifically, we begin by examining prevalent automated\nevaluation methods for assessing the quality of summaries generated by LLMs and\nfind that the results of the GPT-4 evaluation method are most closely aligned\nwith human evaluation. Then, we explore the effectiveness of five prompting\ntechniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in\nadapting LLMs to code summarization tasks. Contrary to expectations, advanced\nprompting techniques may not outperform simple zero-shot prompting. Next, we\ninvestigate the impact of LLMs' model settings (including top\\_p and\ntemperature parameters) on the quality of generated summaries. We find the\nimpact of the two parameters on summary quality varies by the base LLM and\nprogramming language, but their impacts are similar. Moreover, we canvass LLMs'\nabilities to summarize code snippets in distinct types of programming\nlanguages. The results reveal that LLMs perform suboptimally when summarizing\ncode written in logic programming languages compared to other language types.\nFinally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can\noutperform advanced GPT-4 in generating summaries describing code\nimplementation details and asserting code properties. We hope that our findings\ncan provide a comprehensive understanding of code summarization in the era of\nLLMs.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}