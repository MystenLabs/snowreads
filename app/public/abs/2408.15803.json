{"id":"2408.15803","title":"ModalityMirror: Improving Audio Classification in Modality Heterogeneity\n  Federated Learning with Multimodal Distillation","authors":"Tiantian Feng, Tuo Zhang, Salman Avestimehr, Shrikanth S. Narayanan","authorsParsed":[["Feng","Tiantian",""],["Zhang","Tuo",""],["Avestimehr","Salman",""],["Narayanan","Shrikanth S.",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 13:56:22 GMT"}],"updateDate":"2024-08-29","timestamp":1724853382000,"abstract":"  Multimodal Federated Learning frequently encounters challenges of client\nmodality heterogeneity, leading to undesired performances for secondary\nmodality in multimodal learning. It is particularly prevalent in audiovisual\nlearning, with audio is often assumed to be the weaker modality in recognition\ntasks. To address this challenge, we introduce ModalityMirror to improve audio\nmodel performance by leveraging knowledge distillation from an audiovisual\nfederated learning model. ModalityMirror involves two phases: a modality-wise\nFL stage to aggregate uni-modal encoders; and a federated knowledge\ndistillation stage on multi-modality clients to train an unimodal student\nmodel. Our results demonstrate that ModalityMirror significantly improves the\naudio classification compared to the state-of-the-art FL methods such as\nHarmony, particularly in audiovisual FL facing video missing. Our approach\nunlocks the potential for exploiting the diverse modality spectrum inherent in\nmulti-modal FL.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by/4.0/"}