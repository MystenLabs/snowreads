{"id":"2408.15600","title":"Exploring Selective Layer Fine-Tuning in Federated Learning","authors":"Yuchang Sun and Yuexiang Xie and Bolin Ding and Yaliang Li and Jun\n  Zhang","authorsParsed":[["Sun","Yuchang",""],["Xie","Yuexiang",""],["Ding","Bolin",""],["Li","Yaliang",""],["Zhang","Jun",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 07:48:39 GMT"}],"updateDate":"2024-08-29","timestamp":1724831319000,"abstract":"  Federated learning (FL) has emerged as a promising paradigm for fine-tuning\nfoundation models using distributed data in a privacy-preserving manner. Under\nlimited computational resources, clients often find it more practical to\nfine-tune a selected subset of layers, rather than the entire model, based on\ntheir task-specific data. In this study, we provide a thorough theoretical\nexploration of selective layer fine-tuning in FL, emphasizing a flexible\napproach that allows the clients to adjust their selected layers according to\ntheir local data and resources. We theoretically demonstrate that the layer\nselection strategy has a significant impact on model convergence in two\ncritical aspects: the importance of selected layers and the heterogeneous\nchoices across clients. Drawing from these insights, we further propose a\nstrategic layer selection method that utilizes local gradients and regulates\nlayer selections across clients. The extensive experiments on both image and\ntext datasets demonstrate the effectiveness of the proposed strategy compared\nwith several baselines, highlighting its advances in identifying critical\nlayers that adapt to the client heterogeneity and training dynamics in FL.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}