{"id":"2407.01640","title":"BADM: Batch ADMM for Deep Learning","authors":"Ouya Wang, Shenglong Zhou and Geoffrey Ye Li","authorsParsed":[["Wang","Ouya",""],["Zhou","Shenglong",""],["Li","Geoffrey Ye",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 20:47:15 GMT"}],"updateDate":"2024-07-03","timestamp":1719780435000,"abstract":"  Stochastic gradient descent-based algorithms are widely used for training\ndeep neural networks but often suffer from slow convergence. To address the\nchallenge, we leverage the framework of the alternating direction method of\nmultipliers (ADMM) to develop a novel data-driven algorithm, called batch ADMM\n(BADM). The fundamental idea of the proposed algorithm is to split the training\ndata into batches, which is further divided into sub-batches where primal and\ndual variables are updated to generate global parameters through aggregation.\nWe evaluate the performance of BADM across various deep learning tasks,\nincluding graph modelling, computer vision, image generation, and natural\nlanguage processing. Extensive numerical experiments demonstrate that BADM\nachieves faster convergence and superior testing accuracy compared to other\nstate-of-the-art optimizers.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"3K6gVM_uX_ZMBGxOYb5FAifMVGbyr1ok3DPq-V0xMFA","pdfSize":"6729219"}
