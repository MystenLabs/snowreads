{"id":"2407.05302","title":"Mamba Hawkes Process","authors":"Anningzhe Gao, Shan Dai, Yan Hu","authorsParsed":[["Gao","Anningzhe",""],["Dai","Shan",""],["Hu","Yan",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 08:37:43 GMT"}],"updateDate":"2024-07-09","timestamp":1720341463000,"abstract":"  Irregular and asynchronous event sequences are prevalent in many domains,\nsuch as social media, finance, and healthcare. Traditional temporal point\nprocesses (TPPs), like Hawkes processes, often struggle to model mutual\ninhibition and nonlinearity effectively. While recent neural network models,\nincluding RNNs and Transformers, address some of these issues, they still face\nchallenges with long-term dependencies and computational efficiency. In this\npaper, we introduce the Mamba Hawkes Process (MHP), which leverages the Mamba\nstate space architecture to capture long-range dependencies and dynamic event\ninteractions. Our results show that MHP outperforms existing models across\nvarious datasets. Additionally, we propose the Mamba Hawkes Process Extension\n(MHP-E), which combines Mamba and Transformer models to enhance predictive\ncapabilities. We present the novel application of the Mamba architecture to\nHawkes processes, a flexible and extensible model structure, and a theoretical\nanalysis of the synergy between state space models and Hawkes processes.\nExperimental results demonstrate the superior performance of both MHP and\nMHP-E, advancing the field of temporal point process modeling.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}