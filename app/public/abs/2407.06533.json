{"id":"2407.06533","title":"LETS-C: Leveraging Language Embedding for Time Series Classification","authors":"Rachneet Kaur, Zhen Zeng, Tucker Balch, Manuela Veloso","authorsParsed":[["Kaur","Rachneet",""],["Zeng","Zhen",""],["Balch","Tucker",""],["Veloso","Manuela",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 04:07:57 GMT"}],"updateDate":"2024-07-10","timestamp":1720498077000,"abstract":"  Recent advancements in language modeling have shown promising results when\napplied to time series data. In particular, fine-tuning pre-trained large\nlanguage models (LLMs) for time series classification tasks has achieved\nstate-of-the-art (SOTA) performance on standard benchmarks. However, these\nLLM-based models have a significant drawback due to the large model size, with\nthe number of trainable parameters in the millions. In this paper, we propose\nan alternative approach to leveraging the success of language modeling in the\ntime series domain. Instead of fine-tuning LLMs, we utilize a language\nembedding model to embed time series and then pair the embeddings with a simple\nclassification head composed of convolutional neural networks (CNN) and\nmultilayer perceptron (MLP). We conducted extensive experiments on\nwell-established time series classification benchmark datasets. We demonstrated\nLETS-C not only outperforms the current SOTA in classification accuracy but\nalso offers a lightweight solution, using only 14.5% of the trainable\nparameters on average compared to the SOTA model. Our findings suggest that\nleveraging language encoders to embed time series data, combined with a simple\nyet effective classification head, offers a promising direction for achieving\nhigh-performance time series classification while maintaining a lightweight\nmodel architecture.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computational Engineering, Finance, and Science","Computing Research Repository/Computation and Language","Statistics/Methodology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}