{"id":"2407.02936","title":"GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large\n  Language Models","authors":"Zike Yuan, Ming Liu, Hui Wang, Bing Qin","authorsParsed":[["Yuan","Zike",""],["Liu","Ming",""],["Wang","Hui",""],["Qin","Bing",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 09:12:38 GMT"}],"updateDate":"2024-07-04","timestamp":1719997958000,"abstract":"  Evaluating the graph comprehension and reasoning abilities of Large Language\nModels (LLMs) is challenging and often incomplete. Existing benchmarks focus\nprimarily on pure graph understanding, lacking a comprehensive evaluation\nacross all graph types and detailed capability definitions. This paper presents\nGraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and\nreasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and\ntest models on pure graph and heterogeneous graphs, subdividing capabilities\ninto 10 distinct areas tested through 19 tasks. Our benchmark includes 11\ndatasets with 5,140 graphs of varying complexity. We evaluated three\nclosed-source and seven open-source LLMs, conducting thorough analyses from\nboth ability and task perspectives. Key findings reveal that semantic\nenrichment enhances reasoning performance, node ordering impacts task success,\nand the ability to process longer texts does not necessarily improve graph\ncomprehension or reasoning. GraCoRe is open-sourced at\nhttps://github.com/ZIKEYUAN/GraCoRe\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}