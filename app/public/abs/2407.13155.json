{"id":"2407.13155","title":"Real-Time 3D Occupancy Prediction via Geometric-Semantic Disentanglement","authors":"Yulin He, Wei Chen, Tianci Xun, Yusong Tan","authorsParsed":[["He","Yulin",""],["Chen","Wei",""],["Xun","Tianci",""],["Tan","Yusong",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 04:46:13 GMT"},{"version":"v2","created":"Sun, 21 Jul 2024 07:28:19 GMT"}],"updateDate":"2024-07-23","timestamp":1721277973000,"abstract":"  Occupancy prediction plays a pivotal role in autonomous driving (AD) due to\nthe fine-grained geometric perception and general object recognition\ncapabilities. However, existing methods often incur high computational costs,\nwhich contradicts the real-time demands of AD. To this end, we first evaluate\nthe speed and memory usage of most public available methods, aiming to redirect\nthe focus from solely prioritizing accuracy to also considering efficiency. We\nthen identify a core challenge in achieving both fast and accurate performance:\n\\textbf{the strong coupling between geometry and semantic}. To address this\nissue, 1) we propose a Geometric-Semantic Dual-Branch Network (GSDBN) with a\nhybrid BEV-Voxel representation. In the BEV branch, a BEV-level temporal fusion\nmodule and a U-Net encoder is introduced to extract dense semantic features. In\nthe voxel branch, a large-kernel re-parameterized 3D convolution is proposed to\nrefine sparse 3D geometry and reduce computation. Moreover, we propose a novel\nBEV-Voxel lifting module that projects BEV features into voxel space for\nfeature fusion of the two branches. In addition to the network design, 2) we\nalso propose a Geometric-Semantic Decoupled Learning (GSDL) strategy. This\nstrategy initially learns semantics with accurate geometry using ground-truth\ndepth, and then gradually mixes predicted depth to adapt the model to the\npredicted geometry. Extensive experiments on the widely-used Occ3D-nuScenes\nbenchmark demonstrate the superiority of our method, which achieves a 39.4 mIoU\nwith 20.0 FPS. This result is $\\sim 3 \\times$ faster and +1.9 mIoU higher\ncompared to FB-OCC, the winner of CVPR2023 3D Occupancy Prediction Challenge.\nOur code will be made open-source.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}