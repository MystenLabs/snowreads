{"id":"2408.07106","title":"\"You still have to study\" -- On the Security of LLM generated code","authors":"Stefan Goetz, Andreas Schaad","authorsParsed":[["Goetz","Stefan",""],["Schaad","Andreas",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 07:57:53 GMT"}],"updateDate":"2024-08-15","timestamp":1723535873000,"abstract":"  We witness an increasing usage of AI-assistants even for routine (classroom)\nprogramming tasks. However, the code generated on basis of a so called \"prompt\"\nby the programmer does not always meet accepted security standards. On the one\nhand, this may be due to lack of best-practice examples in the training data.\nOn the other hand, the actual quality of the programmers prompt appears to\ninfluence whether generated code contains weaknesses or not. In this paper we\nanalyse 4 major LLMs with respect to the security of generated code. We do this\non basis of a case study for the Python and Javascript language, using the\nMITRE CWE catalogue as the guiding security definition. Our results show that\nusing different prompting techniques, some LLMs initially generate 65% code\nwhich is deemed insecure by a trained security engineer. On the other hand\nalmost all analysed LLMs will eventually generate code being close to 100%\nsecure with increasing manual guidance of a skilled engineer.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}