{"id":"2408.08696","title":"Turning Trash into Treasure: Accelerating Inference of Large Language\n  Models with Token Recycling","authors":"Xianzhen Luo, Yixuan Wang, Qingfu Zhu, Zhiming Zhang, Xuanyu Zhang,\n  Qing Yang, Dongliang Xu, Wanxiang Che","authorsParsed":[["Luo","Xianzhen",""],["Wang","Yixuan",""],["Zhu","Qingfu",""],["Zhang","Zhiming",""],["Zhang","Xuanyu",""],["Yang","Qing",""],["Xu","Dongliang",""],["Che","Wanxiang",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 12:20:56 GMT"}],"updateDate":"2024-08-19","timestamp":1723810856000,"abstract":"  The rapid growth in the parameters of large language models (LLMs) has made\ninference latency a fundamental bottleneck, limiting broader application of\nLLMs. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm, leveraging the parallel\ncapabilities of modern hardware. Some speculative decoding methods rely on\nadditional structures to guess draft tokens, such as small models or\nparameter-efficient architectures, which need extra training before use.\nAlternatively, retrieval-based train-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nThis approach stores candidate tokens in an adjacency matrix and employs a\nbreadth-first search (BFS)-like algorithm on the matrix to construct a draft\ntree. The tree is then validated through tree attention. New candidate tokens\nfrom the decoding process are then used to update the matrix. Token Recycling\nrequires \\textless2MB of additional storage and achieves approximately 2x\nspeedup across all sizes of LLMs. It significantly outperforms existing\ntrain-free methods by 30\\% and even a training method by 25\\%. It can be\ndirectly applied to any existing LLMs and tasks without the need for\nadaptation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}