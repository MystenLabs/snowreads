{"id":"2407.07521","title":"CHILLI: A data context-aware perturbation method for XAI","authors":"Saif Anwar, Nathan Griffiths, Abhir Bhalerao, Thomas Popham","authorsParsed":[["Anwar","Saif",""],["Griffiths","Nathan",""],["Bhalerao","Abhir",""],["Popham","Thomas",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 10:18:07 GMT"}],"updateDate":"2024-07-11","timestamp":1720606687000,"abstract":"  The trustworthiness of Machine Learning (ML) models can be difficult to\nassess, but is critical in high-risk or ethically sensitive applications. Many\nmodels are treated as a `black-box' where the reasoning or criteria for a final\ndecision is opaque to the user. To address this, some existing Explainable AI\n(XAI) approaches approximate model behaviour using perturbed data. However,\nsuch methods have been criticised for ignoring feature dependencies, with\nexplanations being based on potentially unrealistic data. We propose a novel\nframework, CHILLI, for incorporating data context into XAI by generating\ncontextually aware perturbations, which are faithful to the training data of\nthe base model being explained. This is shown to improve both the soundness and\naccuracy of the explanations.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"qYZaZSdGYfY24QC6bW4-aNfR8Nrk9IsLmQinkxDWrS8","pdfSize":"2045236"}
