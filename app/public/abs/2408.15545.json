{"id":"2408.15545","title":"SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding","authors":"Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun\n  Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai","authorsParsed":[["Li","Sihang",""],["Huang","Jin",""],["Zhuang","Jiaxi",""],["Shi","Yaorui",""],["Cai","Xiaochen",""],["Xu","Mingjun",""],["Wang","Xiang",""],["Zhang","Linfeng",""],["Ke","Guolin",""],["Cai","Hengxing",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 05:41:52 GMT"},{"version":"v2","created":"Fri, 30 Aug 2024 06:42:36 GMT"}],"updateDate":"2024-09-02","timestamp":1724823712000,"abstract":"  Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}