{"id":"2407.07825","title":"RT-LA-VocE: Real-Time Low-SNR Audio-Visual Speech Enhancement","authors":"Honglie Chen and Rodrigo Mira and Stavros Petridis and Maja Pantic","authorsParsed":[["Chen","Honglie",""],["Mira","Rodrigo",""],["Petridis","Stavros",""],["Pantic","Maja",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:49:23 GMT"}],"updateDate":"2024-07-11","timestamp":1720630163000,"abstract":"  In this paper, we aim to generate clean speech frame by frame from a live\nvideo stream and a noisy audio stream without relying on future inputs. To this\nend, we propose RT-LA-VocE, which completely re-designs every component of\nLA-VocE, a state-of-the-art non-causal audio-visual speech enhancement model,\nto perform causal real-time inference with a 40ms input frame. We do so by\ndevising new visual and audio encoders that rely solely on past frames,\nreplacing the Transformer encoder with the Emformer, and designing a new causal\nneural vocoder C-HiFi-GAN. On the popular AVSpeech dataset, we show that our\nalgorithm achieves state-of-the-art results in all real-time scenarios. More\nimportantly, each component is carefully tuned to minimize the algorithm\nlatency to the theoretical minimum (40ms) while maintaining a low end-to-end\nprocessing latency of 28.15ms per frame, enabling real-time frame-by-frame\nenhancement with minimal delay.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}