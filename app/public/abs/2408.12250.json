{"id":"2408.12250","title":"Can Artificial Intelligence Embody Moral Values?","authors":"Torben Swoboda and Lode Lauwaert","authorsParsed":[["Swoboda","Torben",""],["Lauwaert","Lode",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 09:39:16 GMT"}],"updateDate":"2024-08-23","timestamp":1724319556000,"abstract":"  The neutrality thesis holds that technology cannot be laden with values. This\nlong-standing view has faced critiques, but much of the argumentation against\nneutrality has focused on traditional, non-smart technologies like bridges and\nrazors. In contrast, AI is a smart technology increasingly used in high-stakes\ndomains like healthcare, finance, and policing, where its decisions can cause\nmoral harm. In this paper, we argue that artificial intelligence, particularly\nartificial agents that autonomously make decisions to pursue their goals,\nchallenge the neutrality thesis. Our central claim is that the computational\nmodels underlying artificial agents can integrate representations of moral\nvalues such as fairness, honesty and avoiding harm. We provide a conceptual\nframework discussing the neutrality thesis, values, and AI. Moreover, we\nexamine two approaches to designing computational models of morality,\nartificial conscience and ethical prompting, and present empirical evidence\nfrom text-based game environments that artificial agents with such models\nexhibit more ethical behavior compared to agents without these models. The\nfindings support that AI can embody moral values, which contradicts the claim\nthat all technologies are necessarily value-neutral.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"spJMo4R506GGlpgryoIniqwpkuptSSUewtipC-PNfKg","pdfSize":"126176"}
