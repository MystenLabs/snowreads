{"id":"2408.06875","title":"Advancing Interactive Explainable AI via Belief Change Theory","authors":"Antonio Rago and Maria Vanina Martinez","authorsParsed":[["Rago","Antonio",""],["Martinez","Maria Vanina",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 13:11:56 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 11:23:43 GMT"}],"updateDate":"2024-08-15","timestamp":1723554716000,"abstract":"  As AI models become ever more complex and intertwined in humans' daily lives,\ngreater levels of interactivity of explainable AI (XAI) methods are needed. In\nthis paper, we propose the use of belief change theory as a formal foundation\nfor operators that model the incorporation of new information, i.e. user\nfeedback in interactive XAI, to logical representations of data-driven\nclassifiers. We argue that this type of formalisation provides a framework and\na methodology to develop interactive explanations in a principled manner,\nproviding warranted behaviour and favouring transparency and accountability of\nsuch interactions. Concretely, we first define a novel, logic-based formalism\nto represent explanatory information shared between humans and machines. We\nthen consider real world scenarios for interactive XAI, with different\nprioritisations of new and existing knowledge, where our formalism may be\ninstantiated. Finally, we analyse a core set of belief change postulates,\ndiscussing their suitability for our real world settings and pointing to\nparticular challenges that may require the relaxation or reinterpretation of\nsome of the theoretical assumptions underlying existing operators.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}