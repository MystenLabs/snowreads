{"id":"2407.10347","title":"MambaForGCN: Enhancing Long-Range Dependency with State Space Model and\n  Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis","authors":"Adamu Lawan, Juhua Pu, Haruna Yunusa, Aliyu Umar, Muhammad Lawan","authorsParsed":[["Lawan","Adamu",""],["Pu","Juhua",""],["Yunusa","Haruna",""],["Umar","Aliyu",""],["Lawan","Muhammad",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 22:23:07 GMT"}],"updateDate":"2024-07-16","timestamp":1720995787000,"abstract":"  Aspect-based sentiment Analysis (ABSA) identifies and evaluates sentiments\ntoward specific aspects of entities within text, providing detailed insights\nbeyond overall sentiment. However, Attention mechanisms and neural network\nmodels struggle with syntactic constraints, and the quadratic complexity of\nattention mechanisms hinders their adoption for capturing long-range\ndependencies between aspect and opinion words in ABSA. This complexity can lead\nto the misinterpretation of irrelevant con-textual words, restricting their\neffectiveness to short-range dependencies. Some studies have investigated\nmerging semantic and syntactic approaches but face challenges in effectively\nintegrating these methods. To address the above problems, we present\nMambaForGCN, a novel approach to enhance short and long-range dependencies\nbetween aspect and opinion words in ABSA. This innovative approach incorporates\nsyntax-based Graph Convolutional Network (SynGCN) and MambaFormer\n(Mamba-Transformer) modules to encode input with dependency relations and\nsemantic information. The Multihead Attention (MHA) and Mamba blocks in the\nMambaFormer module serve as channels to enhance the model with short and\nlong-range dependencies between aspect and opinion words. We also introduce the\nKolmogorov-Arnold Networks (KANs) gated fusion, an adaptively integrated\nfeature representation system combining SynGCN and MambaFormer representations.\nExperimental results on three benchmark datasets demonstrate MambaForGCN's\neffectiveness, outperforming state-of-the-art (SOTA) baseline models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}