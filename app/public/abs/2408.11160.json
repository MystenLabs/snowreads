{"id":"2408.11160","title":"Statistical Challenges with Dataset Construction: Why You Will Never\n  Have Enough Images","authors":"Josh Goldman, John K. Tsotsos","authorsParsed":[["Goldman","Josh",""],["Tsotsos","John K.",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 19:33:24 GMT"}],"updateDate":"2024-08-22","timestamp":1724182404000,"abstract":"  Deep neural networks have achieved impressive performance on many computer\nvision benchmarks in recent years. However, can we be confident that impressive\nperformance on benchmarks will translate to strong performance in real-world\nenvironments? Many environments in the real world are safety critical, and even\nslight model failures can be catastrophic. Therefore, it is crucial to test\nmodels rigorously before deployment. We argue, through both statistical theory\nand empirical evidence, that selecting representative image datasets for\ntesting a model is likely implausible in many domains. Furthermore, performance\nstatistics calculated with non-representative image datasets are highly\nunreliable. As a consequence, we cannot guarantee that models which perform\nwell on withheld test images will also perform well in the real world. Creating\nlarger and larger datasets will not help, and bias aware datasets cannot solve\nthis problem either. Ultimately, there is little statistical foundation for\nevaluating models using withheld test sets. We recommend that future evaluation\nmethodologies focus on assessing a model's decision-making process, rather than\nmetrics such as accuracy.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Xlmo1mOmAxUQ-mMjcDhi8V-3poXNKa7pPWRSgIRunHg","pdfSize":"301773"}
