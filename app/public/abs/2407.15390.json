{"id":"2407.15390","title":"ALLaM: Large Language Models for Arabic and English","authors":"M Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi,\n  Hisham A. Alyahya, Sultan AlRashed, Faisal A. Mirza, Shaykhah Z. Alsubaie,\n  Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef\n  Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Majed\n  Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen Al-Thubaity, Ahmed\n  Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb\n  Alowisheq, Haidar Khan","authorsParsed":[["Bari","M Saiful",""],["Alnumay","Yazeed",""],["Alzahrani","Norah A.",""],["Alotaibi","Nouf M.",""],["Alyahya","Hisham A.",""],["AlRashed","Sultan",""],["Mirza","Faisal A.",""],["Alsubaie","Shaykhah Z.",""],["Alahmed","Hassan A.",""],["Alabduljabbar","Ghadah",""],["Alkhathran","Raghad",""],["Almushayqih","Yousef",""],["Alnajim","Raneem",""],["Alsubaihi","Salman",""],["Mansour","Maryam Al",""],["Alrubaian","Majed",""],["Alammari","Ali",""],["Alawami","Zaki",""],["Al-Thubaity","Abdulmohsen",""],["Abdelali","Ahmed",""],["Kuriakose","Jeril",""],["Abujabal","Abdalghani",""],["Al-Twairesh","Nora",""],["Alowisheq","Areeb",""],["Khan","Haidar",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 05:35:17 GMT"}],"updateDate":"2024-07-23","timestamp":1721626517000,"abstract":"  We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}