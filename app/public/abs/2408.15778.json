{"id":"2408.15778","title":"LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models","authors":"Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning\n  Wang, Yuxiao Dong, Jie Tang, Minlie Huang","authorsParsed":[["Gui","Jiayi",""],["Liu","Yiming",""],["Cheng","Jiale",""],["Gu","Xiaotao",""],["Liu","Xiao",""],["Wang","Hongning",""],["Dong","Yuxiao",""],["Tang","Jie",""],["Huang","Minlie",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 13:16:41 GMT"},{"version":"v2","created":"Wed, 4 Sep 2024 15:35:15 GMT"},{"version":"v3","created":"Thu, 5 Sep 2024 10:30:39 GMT"}],"updateDate":"2024-09-06","timestamp":1724851001000,"abstract":"  Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}