{"id":"2408.09429","title":"Reefknot: A Comprehensive Benchmark for Relation Hallucination\n  Evaluation, Analysis and Mitigation in Multimodal Large Language Models","authors":"Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu","authorsParsed":[["Zheng","Kening",""],["Chen","Junkai",""],["Yan","Yibo",""],["Zou","Xin",""],["Hu","Xuming",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 10:07:02 GMT"}],"updateDate":"2024-08-20","timestamp":1723975622000,"abstract":"  Hallucination issues persistently plagued current multimodal large language\nmodels (MLLMs). While existing research primarily focuses on object-level or\nattribute-level hallucinations, sidelining the more sophisticated relation\nhallucinations that necessitate advanced reasoning abilities from MLLMs.\nBesides, recent benchmarks regarding relation hallucinations lack in-depth\nevaluation and effective mitigation. Moreover, their datasets are typically\nderived from a systematic annotation process, which could introduce inherent\nbiases due to the predefined process. To handle the aforementioned challenges,\nwe introduce Reefknot, a comprehensive benchmark specifically targeting\nrelation hallucinations, consisting of over 20,000 samples derived from\nreal-world scenarios. Specifically, we first provide a systematic definition of\nrelation hallucinations, integrating perspectives from perceptive and cognitive\ndomains. Furthermore, we construct the relation-based corpus utilizing the\nrepresentative scene graph dataset Visual Genome (VG), from which semantic\ntriplets follow real-world distributions. Our comparative evaluation across\nthree distinct tasks revealed a substantial shortcoming in the capabilities of\ncurrent MLLMs to mitigate relation hallucinations. Finally, we advance a novel\nconfidence-based mitigation strategy tailored to tackle the relation\nhallucinations problem. Across three datasets, including Reefknot, we observed\nan average reduction of 9.75% in the hallucination rate. We believe our paper\nsheds valuable insights into achieving trustworthy multimodal intelligence. Our\ndataset and code will be released upon paper acceptance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"lLylxHcdpnfQdjLtNZmY36Ki_0lHE7MDBFVmVsMIIrQ","pdfSize":"6176745"}
