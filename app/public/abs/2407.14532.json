{"id":"2407.14532","title":"A Scenario-Oriented Benchmark for Assessing AIOps Algorithms in\n  Microservice Management","authors":"Yongqian Sun, Jiaju Wang, Zhengdan Li, Xiaohui Nie, Minghua Ma,\n  Shenglin Zhang, Yuhe Ji, Lu Zhang, Wen Long, Hengmao Chen, Yongnan Luo, Dan\n  Pei","authorsParsed":[["Sun","Yongqian",""],["Wang","Jiaju",""],["Li","Zhengdan",""],["Nie","Xiaohui",""],["Ma","Minghua",""],["Zhang","Shenglin",""],["Ji","Yuhe",""],["Zhang","Lu",""],["Long","Wen",""],["Chen","Hengmao",""],["Luo","Yongnan",""],["Pei","Dan",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 12:41:43 GMT"}],"updateDate":"2024-07-23","timestamp":1720528903000,"abstract":"  AIOps algorithms play a crucial role in the maintenance of microservice\nsystems. Many previous benchmarks' performance leaderboard provides valuable\nguidance for selecting appropriate algorithms. However, existing AIOps\nbenchmarks mainly utilize offline datasets to evaluate algorithms. They cannot\nconsistently evaluate the performance of algorithms using real-time datasets,\nand the operation scenarios for evaluation are static, which is insufficient\nfor effective algorithm selection. To address these issues, we propose an\nevaluation-consistent and scenario-oriented evaluation framework named\nMicroServo. The core idea is to build a live microservice benchmark to generate\nreal-time datasets and consistently simulate the specific operation scenarios\non it. MicroServo supports different leaderboards by selecting specific\nalgorithms and datasets according to the operation scenarios. It also supports\nthe deployment of various types of algorithms, enabling algorithms\nhot-plugging. At last, we test MicroServo with three typical microservice\noperation scenarios to demonstrate its efficiency and usability.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}