{"id":"2408.17214","title":"Efficient Multi-task Prompt Tuning for Recommendation","authors":"Ting Bai, Le Huang, Yue Yu, Cheng Yang, Cheng Hou, Zhe Zhao, Chuan Shi","authorsParsed":[["Bai","Ting",""],["Huang","Le",""],["Yu","Yue",""],["Yang","Cheng",""],["Hou","Cheng",""],["Zhao","Zhe",""],["Shi","Chuan",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 11:38:51 GMT"}],"updateDate":"2024-09-02","timestamp":1725017931000,"abstract":"  With the expansion of business scenarios, real recommender systems are facing\nchallenges in dealing with the constantly emerging new tasks in multi-task\nlearning frameworks. In this paper, we attempt to improve the generalization\nability of multi-task recommendations when dealing with new tasks. We find that\njoint training will enhance the performance of the new task but always\nnegatively impact existing tasks in most multi-task learning methods. Besides,\nsuch a re-training mechanism with new tasks increases the training costs,\nlimiting the generalization ability of multi-task recommendation models. Based\non this consideration, we aim to design a suitable sharing mechanism among\ndifferent tasks while maintaining joint optimization efficiency in new task\nlearning. A novel two-stage prompt-tuning MTL framework (MPT-Rec) is proposed\nto address task irrelevance and training efficiency problems in multi-task\nrecommender systems. Specifically, we disentangle the task-specific and\ntask-sharing information in the multi-task pre-training stage, then use\ntask-aware prompts to transfer knowledge from other tasks to the new task\neffectively. By freezing parameters in the pre-training tasks, MPT-Rec solves\nthe negative impacts that may be brought by the new task and greatly reduces\nthe training costs. Extensive experiments on three real-world datasets show the\neffectiveness of our proposed multi-task learning framework. MPT-Rec achieves\nthe best performance compared to the SOTA multi-task learning method. Besides,\nit maintains comparable model performance but vastly improves the training\nefficiency (i.e., with up to 10% parameters in the full training way) in the\nnew task learning.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}