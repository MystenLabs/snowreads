{"id":"2408.06631","title":"IFShip: A Large Vision-Language Model for Interpretable Fine-grained\n  Ship Classification via Domain Knowledge-Enhanced Instruction Tuning","authors":"Mingning Guo, Mengwei Wu, Yuxiang Shen, Haifeng Li and Chao Tao","authorsParsed":[["Guo","Mingning",""],["Wu","Mengwei",""],["Shen","Yuxiang",""],["Li","Haifeng",""],["Tao","Chao",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 04:36:18 GMT"}],"updateDate":"2024-08-14","timestamp":1723523778000,"abstract":"  End-to-end interpretation is currently the prevailing paradigm for remote\nsensing fine-grained ship classification (RS-FGSC) task. However, its inference\nprocess is uninterpretable, leading to criticism as a black box model. To\naddress this issue, we propose a large vision-language model (LVLM) named\nIFShip for interpretable fine-grained ship classification. Unlike traditional\nmethods, IFShip excels in interpretability by accurately conveying the\nreasoning process of FGSC in natural language. Specifically, we first design a\ndomain knowledge-enhanced Chain-of-Thought (COT) prompt generation mechanism.\nThis mechanism is used to semi-automatically construct a task-specific\ninstruction-following dataset named TITANIC-FGS, which emulates human-like\nlogical decision-making. We then train the IFShip model using task instructions\ntuned with the TITANIC-FGS dataset. Building on IFShip, we develop an FGSC\nvisual chatbot that redefines the FGSC problem as a step-by-step reasoning task\nand conveys the reasoning process in natural language. Experimental results\nreveal that the proposed method surpasses state-of-the-art FGSC algorithms in\nboth classification interpretability and accuracy. Moreover, compared to LVLMs\nlike LLaVA and MiniGPT-4, our approach demonstrates superior expertise in the\nFGSC task. It provides an accurate chain of reasoning when fine-grained ship\ntypes are recognizable to the human eye and offers interpretable explanations\nwhen they are not.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}