{"id":"2407.11764","title":"Relaxing Graph Transformers for Adversarial Attacks","authors":"Philipp Foth, Lukas Gosch, Simon Geisler, Leo Schwinn, Stephan\n  G\\\"unnemann","authorsParsed":[["Foth","Philipp",""],["Gosch","Lukas",""],["Geisler","Simon",""],["Schwinn","Leo",""],["GÃ¼nnemann","Stephan",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:24:58 GMT"}],"updateDate":"2024-07-17","timestamp":1721139898000,"abstract":"  Existing studies have shown that Graph Neural Networks (GNNs) are vulnerable\nto adversarial attacks. Even though Graph Transformers (GTs) surpassed\nMessage-Passing GNNs on several benchmarks, their adversarial robustness\nproperties are unexplored. However, attacking GTs is challenging due to their\nPositional Encodings (PEs) and special attention mechanisms which can be\ndifficult to differentiate. We overcome these challenges by targeting three\nrepresentative architectures based on (1) random-walk PEs, (2)\npair-wise-shortest-path PEs, and (3) spectral PEs - and propose the first\nadaptive attacks for GTs. We leverage our attacks to evaluate robustness to (a)\nstructure perturbations on node classification; and (b) node injection attacks\nfor (fake-news) graph classification. Our evaluation reveals that they can be\ncatastrophically fragile and underlines our work's importance and the necessity\nfor adaptive attacks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LYsH9WQhLQtzOFMdpLjUM8S5qHOVA252rQNLcQZhRo0","pdfSize":"697732"}
