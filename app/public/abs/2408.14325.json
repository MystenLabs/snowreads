{"id":"2408.14325","title":"Function-Space MCMC for Bayesian Wide Neural Networks","authors":"Lucia Pezzetti, Stefano Favaro and Stefano Peluchetti","authorsParsed":[["Pezzetti","Lucia",""],["Favaro","Stefano",""],["Peluchetti","Stefano",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 14:54:13 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 20:17:18 GMT"}],"updateDate":"2024-08-30","timestamp":1724684053000,"abstract":"  Bayesian Neural Networks represent a fascinating confluence of deep learning\nand probabilistic reasoning, offering a compelling framework for understanding\nuncertainty in complex predictive models. In this paper, we investigate the use\nof the preconditioned Crank-Nicolson algorithm and its Langevin version to\nsample from the reparametrised posterior distribution of the weights as the\nwidths of Bayesian Neural Networks grow larger. In addition to being robust in\nthe infinite-dimensional setting, we prove that the acceptance probabilities of\nthe proposed methods approach 1 as the width of the network increases,\nindependently of any stepsize tuning. Moreover, we examine and compare how the\nmixing speeds of the underdamped Langevin Monte Carlo, the preconditioned\nCrank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are\ninfluenced by changes in the network width in some real-world cases. Our\nfindings suggest that, in wide Bayesian Neural Networks configurations, the\npreconditioned Crank-Nicolson method allows for more efficient sampling of the\nreparametrised posterior distribution, as evidenced by a higher effective\nsample size and improved diagnostic results compared with the other analysed\nalgorithms.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}