{"id":"2408.16871","title":"GSTAM: Efficient Graph Distillation with Structural Attention-Matching","authors":"Arash Rasti-Meymandi, Ahmad Sajedi, Zhaopan Xu, Konstantinos N.\n  Plataniotis","authorsParsed":[["Rasti-Meymandi","Arash",""],["Sajedi","Ahmad",""],["Xu","Zhaopan",""],["Plataniotis","Konstantinos N.",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 19:40:04 GMT"}],"updateDate":"2024-09-02","timestamp":1724960404000,"abstract":"  Graph distillation has emerged as a solution for reducing large graph\ndatasets to smaller, more manageable, and informative ones. Existing methods\nprimarily target node classification, involve computationally intensive\nprocesses, and fail to capture the true distribution of the full graph dataset.\nTo address these issues, we introduce Graph Distillation with Structural\nAttention Matching (GSTAM), a novel method for condensing graph classification\ndatasets. GSTAM leverages the attention maps of GNNs to distill structural\ninformation from the original dataset into synthetic graphs. The structural\nattention-matching mechanism exploits the areas of the input graph that GNNs\nprioritize for classification, effectively distilling such information into the\nsynthetic graphs and improving overall distillation performance. Comprehensive\nexperiments demonstrate GSTAM's superiority over existing methods, achieving\n0.45% to 6.5% better performance in extreme condensation ratios, highlighting\nits potential use in advancing distillation for graph classification tasks\n(Code available at https://github.com/arashrasti96/GSTAM).\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}