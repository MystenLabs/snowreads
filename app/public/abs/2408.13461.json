{"id":"2408.13461","title":"Probing the Robustness of Vision-Language Pretrained Models: A\n  Multimodal Adversarial Attack Approach","authors":"Jiwei Guan, Tianyu Ding, Longbing Cao, Lei Pan, Chen Wang, Xi Zheng","authorsParsed":[["Guan","Jiwei",""],["Ding","Tianyu",""],["Cao","Longbing",""],["Pan","Lei",""],["Wang","Chen",""],["Zheng","Xi",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 04:31:37 GMT"}],"updateDate":"2024-08-27","timestamp":1724473897000,"abstract":"  Vision-language pretraining (VLP) with transformers has demonstrated\nexceptional performance across numerous multimodal tasks. However, the\nadversarial robustness of these models has not been thoroughly investigated.\nExisting multimodal attack methods have largely overlooked cross-modal\ninteractions between visual and textual modalities, particularly in the context\nof cross-attention mechanisms. In this paper, we study the adversarial\nvulnerability of recent VLP transformers and design a novel Joint Multimodal\nTransformer Feature Attack (JMTFA) that concurrently introduces adversarial\nperturbations in both visual and textual modalities under white-box settings.\nJMTFA strategically targets attention relevance scores to disrupt important\nfeatures within each modality, generating adversarial samples by fusing\nperturbations and leading to erroneous model predictions. Experimental results\nindicate that the proposed approach achieves high attack success rates on\nvision-language understanding and reasoning downstream tasks compared to\nexisting baselines. Notably, our findings reveal that the textual modality\nsignificantly influences the complex fusion processes within VLP transformers.\nMoreover, we observe no apparent relationship between model size and\nadversarial robustness under our proposed attacks. These insights emphasize a\nnew dimension of adversarial robustness and underscore potential risks in the\nreliable deployment of multimodal AI systems.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}