{"id":"2408.09335","title":"Exploratory Optimal Stopping: A Singular Control Formulation","authors":"Jodi Dianetti, Giorgio Ferrari and Renyuan Xu","authorsParsed":[["Dianetti","Jodi",""],["Ferrari","Giorgio",""],["Xu","Renyuan",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 02:31:55 GMT"}],"updateDate":"2024-08-20","timestamp":1723948315000,"abstract":"  This paper explores continuous-time and state-space optimal stopping problems\nfrom a reinforcement learning perspective. We begin by formulating the stopping\nproblem using randomized stopping times, where the decision maker's control is\nrepresented by the probability of stopping within a given time--specifically, a\nbounded, non-decreasing, c\\`adl\\`ag control process. To encourage exploration\nand facilitate learning, we introduce a regularized version of the problem by\npenalizing it with the cumulative residual entropy of the randomized stopping\ntime. The regularized problem takes the form of an (n+1)-dimensional degenerate\nsingular stochastic control with finite-fuel. We address this through the\ndynamic programming principle, which enables us to identify the unique optimal\nexploratory strategy. For the specific case of a real option problem, we derive\na semi-explicit solution to the regularized problem, allowing us to assess the\nimpact of entropy regularization and analyze the vanishing entropy limit.\nFinally, we propose a reinforcement learning algorithm based on policy\niteration. We show both policy improvement and policy convergence results for\nour proposed algorithm.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning","Quantitative Finance/Mathematical Finance","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OsJQt_ZO-kd0xoPWr3dz442ZQm2Ly1tRAl_6jgvOIOw","pdfSize":"1277720"}
