{"id":"2408.12237","title":"Weight Scope Alignment: A Frustratingly Easy Method for Model Merging","authors":"Yichu Xu, Xin-Chun Li, Le Gan and De-Chuan Zhan","authorsParsed":[["Xu","Yichu",""],["Li","Xin-Chun",""],["Gan","Le",""],["Zhan","De-Chuan",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 09:13:27 GMT"}],"updateDate":"2024-08-23","timestamp":1724318007000,"abstract":"  Merging models becomes a fundamental procedure in some applications that\nconsider model efficiency and robustness. The training randomness or Non-I.I.D.\ndata poses a huge challenge for averaging-based model fusion. Previous research\nefforts focus on element-wise regularization or neural permutations to enhance\nmodel averaging while overlooking weight scope variations among models, which\ncan significantly affect merging effectiveness. In this paper, we reveal\nvariations in weight scope under different training conditions, shedding light\non its influence on model merging. Fortunately, the parameters in each layer\nbasically follow the Gaussian distribution, which inspires a novel and simple\nregularization approach named Weight Scope Alignment (WSA). It contains two key\ncomponents: 1) leveraging a target weight scope to guide the model training\nprocess for ensuring weight scope matching in the subsequent model merging. 2)\nfusing the weight scope of two or more models into a unified one for\nmulti-stage model fusion. We extend the WSA regularization to two different\nscenarios, including Mode Connectivity and Federated Learning. Abundant\nexperimental studies validate the effectiveness of our approach.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Bm4maSGYs7zvfCDU2zxwF6iQQ8tm9SxsHneR__7aBpo","pdfSize":"2160529"}
