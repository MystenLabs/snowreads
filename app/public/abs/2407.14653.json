{"id":"2407.14653","title":"OASIS: Conditional Distribution Shaping for Offline Safe Reinforcement\n  Learning","authors":"Yihang Yao, Zhepeng Cen, Wenhao Ding, Haohong Lin, Shiqi Liu, Tingnan\n  Zhang, Wenhao Yu, Ding Zhao","authorsParsed":[["Yao","Yihang",""],["Cen","Zhepeng",""],["Ding","Wenhao",""],["Lin","Haohong",""],["Liu","Shiqi",""],["Zhang","Tingnan",""],["Yu","Wenhao",""],["Zhao","Ding",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 20:15:00 GMT"}],"updateDate":"2024-07-23","timestamp":1721420100000,"abstract":"  Offline safe reinforcement learning (RL) aims to train a policy that\nsatisfies constraints using a pre-collected dataset. Most current methods\nstruggle with the mismatch between imperfect demonstrations and the desired\nsafe and rewarding performance. In this paper, we introduce OASIS (cOnditionAl\ndiStributIon Shaping), a new paradigm in offline safe RL designed to overcome\nthese critical limitations. OASIS utilizes a conditional diffusion model to\nsynthesize offline datasets, thus shaping the data distribution toward a\nbeneficial target domain. Our approach makes compliance with safety constraints\nthrough effective data utilization and regularization techniques to benefit\noffline safe RL training. Comprehensive evaluations on public benchmarks and\nvarying datasets showcase OASIS's superiority in benefiting offline safe RL\nagents to achieve high-reward behavior while satisfying the safety constraints,\noutperforming established baselines. Furthermore, OASIS exhibits high data\nefficiency and robustness, making it suitable for real-world applications,\nparticularly in tasks where safety is imperative and high-quality\ndemonstrations are scarce.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}