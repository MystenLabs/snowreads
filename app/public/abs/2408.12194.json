{"id":"2408.12194","title":"Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment","authors":"Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, Kang Liu","authorsParsed":[["Luo","Kun",""],["Qin","Minghao",""],["Liu","Zheng",""],["Xiao","Shitao",""],["Zhao","Jun",""],["Liu","Kang",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 08:16:07 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 06:46:41 GMT"}],"updateDate":"2024-08-26","timestamp":1724314567000,"abstract":"  Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}