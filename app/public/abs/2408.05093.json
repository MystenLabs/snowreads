{"id":"2408.05093","title":"Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models","authors":"Zikai Xie","authorsParsed":[["Xie","Zikai",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 14:34:32 GMT"},{"version":"v2","created":"Fri, 16 Aug 2024 22:58:20 GMT"}],"updateDate":"2024-08-20","timestamp":1723214072000,"abstract":"  Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}