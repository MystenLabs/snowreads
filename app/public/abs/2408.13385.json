{"id":"2408.13385","title":"MICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning","authors":"Zhenyu Zhang, Guangyao Chen, Yixiong Zou, Zhimeng Huang, Yuhua Li,\n  Ruixuan Li","authorsParsed":[["Zhang","Zhenyu",""],["Chen","Guangyao",""],["Zou","Yixiong",""],["Huang","Zhimeng",""],["Li","Yuhua",""],["Li","Ruixuan",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 21:32:53 GMT"}],"updateDate":"2024-08-27","timestamp":1724448773000,"abstract":"  Humans exhibit a remarkable ability to learn quickly from a limited number of\nlabeled samples, a capability that starkly contrasts with that of current\nmachine learning systems. Unsupervised Few-Shot Learning (U-FSL) seeks to\nbridge this divide by reducing reliance on annotated datasets during initial\ntraining phases. In this work, we first quantitatively assess the impacts of\nMasked Image Modeling (MIM) and Contrastive Learning (CL) on few-shot learning\ntasks. Our findings highlight the respective limitations of MIM and CL in terms\nof discriminative and generalization abilities, which contribute to their\nunderperformance in U-FSL contexts. To address these trade-offs between\ngeneralization and discriminability in unsupervised pretraining, we introduce a\nnovel paradigm named Masked Image Contrastive Modeling (MICM). MICM creatively\ncombines the targeted object learning strength of CL with the generalized\nvisual feature learning capability of MIM, significantly enhancing its efficacy\nin downstream few-shot learning inference. Extensive experimental analyses\nconfirm the advantages of MICM, demonstrating significant improvements in both\ngeneralization and discrimination capabilities for few-shot learning. Our\ncomprehensive quantitative evaluations further substantiate the superiority of\nMICM, showing that our two-stage U-FSL framework based on MICM markedly\noutperforms existing leading baselines.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}