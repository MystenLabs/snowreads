{"id":"2407.13911","title":"Continual Distillation Learning","authors":"Qifan Zhang, Yunhui Guo, Yu Xiang","authorsParsed":[["Zhang","Qifan",""],["Guo","Yunhui",""],["Xiang","Yu",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 21:52:57 GMT"}],"updateDate":"2024-07-22","timestamp":1721339577000,"abstract":"  We study the problem of Continual Distillation Learning (CDL) that considers\nKnowledge Distillation (KD) in the Continual Learning (CL) setup. A teacher\nmodel and a student model need to learn a sequence of tasks, and the knowledge\nof the teacher model will be distilled to the student to improve the student\nmodel. We introduce a novel method named CDL-Prompt that utilizes prompt-based\ncontinual learning models to build the teacher-student model. We investigate\nhow to utilize the prompts of the teacher model in the student model for\nknowledge distillation, and propose an attention-based prompt mapping scheme to\nuse the teacher prompts for the student. We demonstrate that our method can be\napplied to different prompt-based continual learning models such as L2P,\nDualPrompt and CODA-Prompt to improve their performance using powerful teacher\nmodels. Although recent CL methods focus on prompt learning, we show that our\nmethod can be utilized to build efficient CL models using prompt-based\nknowledge distillation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}