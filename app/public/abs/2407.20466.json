{"id":"2407.20466","title":"A Method for Fast Autonomy Transfer in Reinforcement Learning","authors":"Dinuka Sahabandu, Bhaskar Ramasubramanian, Michail Alexiou, J. Sukarno\n  Mertoguno, Linda Bushnell, Radha Poovendran","authorsParsed":[["Sahabandu","Dinuka",""],["Ramasubramanian","Bhaskar",""],["Alexiou","Michail",""],["Mertoguno","J. Sukarno",""],["Bushnell","Linda",""],["Poovendran","Radha",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 23:48:07 GMT"}],"updateDate":"2024-07-31","timestamp":1722296887000,"abstract":"  This paper introduces a novel reinforcement learning (RL) strategy designed\nto facilitate rapid autonomy transfer by utilizing pre-trained critic value\nfunctions from multiple environments. Unlike traditional methods that require\nextensive retraining or fine-tuning, our approach integrates existing\nknowledge, enabling an RL agent to adapt swiftly to new settings without\nrequiring extensive computational resources. Our contributions include\ndevelopment of the Multi-Critic Actor-Critic (MCAC) algorithm, establishing its\nconvergence, and empirical evidence demonstrating its efficacy. Our\nexperimental results show that MCAC significantly outperforms the baseline\nactor-critic algorithm, achieving up to 22.76x faster autonomy transfer and\nhigher reward accumulation. This advancement underscores the potential of\nleveraging accumulated knowledge for efficient adaptation in RL applications.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}