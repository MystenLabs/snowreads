{"id":"2407.20743","title":"Meltemi: The first open Large Language Model for Greek","authors":"Leon Voukoutis, Dimitris Roussis, Georgios Paraskevopoulos, Sokratis\n  Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios\n  Katsamanis, Stelios Piperidis, Vassilis Katsouros","authorsParsed":[["Voukoutis","Leon",""],["Roussis","Dimitris",""],["Paraskevopoulos","Georgios",""],["Sofianopoulos","Sokratis",""],["Prokopidis","Prokopis",""],["Papavasileiou","Vassilis",""],["Katsamanis","Athanasios",""],["Piperidis","Stelios",""],["Katsouros","Vassilis",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 11:22:52 GMT"}],"updateDate":"2024-07-31","timestamp":1722338572000,"abstract":"  We describe the development and capabilities of Meltemi 7B, the first open\nLarge Language Model for the Greek language. Meltemi 7B has 7 billion\nparameters and is trained on a 40 billion token Greek corpus. For the\ndevelopment of Meltemi 7B, we adapt Mistral, by continuous pretraining on the\nGreek Corpus. Meltemi 7B contains up-to-date information up to September 2023.\nFurthermore, we have translated and curated a Greek instruction corpus, which\nhas been used for the instruction-tuning of a chat model, named Meltemi 7B\nInstruct. Special care has been given to the alignment and the removal of toxic\ncontent for the Meltemi 7B Instruct. The developed models are evaluated on a\nbroad set of collected evaluation corpora, and examples of prompts and\nresponses are presented. Both Meltemi 7B and Meltemi 7B Instruct are available\nat https://huggingface.co/ilsp under the Apache 2.0 license.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Uf3hpTEa0XuIvHAEBT3vimOXlBF0Js-hkp4W9ORWsqk","pdfSize":"166422"}
