{"id":"2408.02288","title":"Spin glass model of in-context learning","authors":"Yuhao Li, Ruoran Bai, Haiping Huang","authorsParsed":[["Li","Yuhao",""],["Bai","Ruoran",""],["Huang","Haiping",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 07:54:01 GMT"}],"updateDate":"2024-08-06","timestamp":1722844441000,"abstract":"  Large language models show a surprising in-context learning ability -- being\nable to use a prompt to form a prediction for a query, yet without additional\ntraining, in stark contrast to old-fashioned supervised learning. Providing a\nmechanistic interpretation and linking the empirical phenomenon to physics are\nthus challenging and remain unsolved. We study a simple yet expressive\ntransformer with linear attention, and map this structure to a spin glass model\nwith real-valued spins, where the couplings and fields explain the intrinsic\ndisorder in data. The spin glass model explains how the weight parameters\ninteract with each other during pre-training, and most importantly why an\nunseen function can be predicted by providing only a prompt yet without\ntraining. Our theory reveals that for single instance learning, increasing the\ntask diversity leads to the emergence of the in-context learning, by allowing\nthe Boltzmann distribution to converge to a unique correct solution of weight\nparameters. Therefore the pre-trained transformer displays a prediction power\nin a novel prompt setting. The proposed spin glass model thus establishes a\nfoundation to understand the empirical success of large language models.\n","subjects":["Condensed Matter/Disordered Systems and Neural Networks","Condensed Matter/Statistical Mechanics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"5WatLvMk3qpshGSu8S-OopaXQw-GqlVUCAYzeLnyvyA","pdfSize":"1479806","txDigest":"D2GgZnr1YQCmALaje7Db6fFEBUbdtyWs1zTDzs1fB6a7","endEpoch":"1","status":"CERTIFIED"}
