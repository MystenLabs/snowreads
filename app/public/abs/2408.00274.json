{"id":"2408.00274","title":"QUITO: Accelerating Long-Context Reasoning through Query-Guided Context\n  Compression","authors":"Wenshan Wang, Yihang Wang, Yixing Fan, Huaming Liao, and Jiafeng Guo","authorsParsed":[["Wang","Wenshan",""],["Wang","Yihang",""],["Fan","Yixing",""],["Liao","Huaming",""],["Guo","Jiafeng",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 04:28:38 GMT"}],"updateDate":"2024-08-02","timestamp":1722486518000,"abstract":"  In-context learning (ICL) capabilities are foundational to the success of\nlarge language models (LLMs). Recently, context compression has attracted\ngrowing interest since it can largely reduce reasoning complexities and\ncomputation costs of LLMs. In this paper, we introduce a novel Query-gUIded\naTtention cOmpression (QUITO) method, which leverages attention of the question\nover the contexts to filter useless information. Specifically, we take a\ntrigger token to calculate the attention distribution of the context in\nresponse to the question. Based on the distribution, we propose three different\nfiltering methods to satisfy the budget constraints of the context length. We\nevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and\nASQA. Experimental results demonstrate that QUITO significantly outperforms\nestablished baselines across various datasets and downstream LLMs, underscoring\nits effectiveness. Our code is available at\nhttps://github.com/Wenshansilvia/attention_compressor.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}