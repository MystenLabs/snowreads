{"id":"2408.14340","title":"Foundation Models for Music: A Survey","authors":"Yinghao Ma, Anders {\\O}land, Anton Ragni, Bleiz MacSen Del Sette,\n  Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras,\n  Emmanouil Benetos, Elona Shatri, Fabio Morreale, Ge Zhang, Gy\\\"orgy Fazekas,\n  Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin,\n  Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B.\n  Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin\n  Kang, Simon Dixon, Wenhu Chen, Wenhao Huang, Xingjian Du, Xingwei Qu, Xu Tan,\n  Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang","authorsParsed":[["Ma","Yinghao",""],["Øland","Anders",""],["Ragni","Anton",""],["Del Sette","Bleiz MacSen",""],["Saitis","Charalampos",""],["Donahue","Chris",""],["Lin","Chenghua",""],["Plachouras","Christos",""],["Benetos","Emmanouil",""],["Shatri","Elona",""],["Morreale","Fabio",""],["Zhang","Ge",""],["Fazekas","György",""],["Xia","Gus",""],["Zhang","Huan",""],["Manco","Ilaria",""],["Huang","Jiawen",""],["Guinot","Julien",""],["Lin","Liwei",""],["Marinelli","Luca",""],["Lam","Max W. Y.",""],["Sharma","Megha",""],["Kong","Qiuqiang",""],["Dannenberg","Roger B.",""],["Yuan","Ruibin",""],["Wu","Shangda",""],["Wu","Shih-Lun",""],["Dai","Shuqi",""],["Lei","Shun",""],["Kang","Shiyin",""],["Dixon","Simon",""],["Chen","Wenhu",""],["Huang","Wenhao",""],["Du","Xingjian",""],["Qu","Xingwei",""],["Tan","Xu",""],["Li","Yizhi",""],["Tian","Zeyue",""],["Wu","Zhiyong",""],["Wu","Zhizheng",""],["Ma","Ziyang",""],["Wang","Ziyu",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 15:13:14 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 14:09:44 GMT"},{"version":"v3","created":"Tue, 3 Sep 2024 14:53:34 GMT"}],"updateDate":"2024-09-04","timestamp":1724685194000,"abstract":"  In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"qrDWidlyJRSGTts9Iv2IVs3o_YS0dfjfUAh6C0IPUEY","pdfSize":"5109009"}
