{"id":"2407.09286","title":"Adaptive Bayesian Regression on Data with Low Intrinsic Dimensionality","authors":"Tao Tang, Nan Wu, Xiuyuan Cheng, David Dunson","authorsParsed":[["Tang","Tao",""],["Wu","Nan",""],["Cheng","Xiuyuan",""],["Dunson","David",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 14:19:31 GMT"},{"version":"v2","created":"Thu, 5 Sep 2024 21:19:06 GMT"}],"updateDate":"2024-09-09","timestamp":1720793971000,"abstract":"  We study how the posterior contraction rate under a Gaussian process (GP)\nprior depends on the intrinsic dimension of the predictors and smoothness of\nthe regression function. An open question is whether a generic GP prior that\ndoes not incorporate knowledge of the intrinsic lower-dimensional structure of\nthe predictors can attain an adaptive rate for a broad class of such\nstructures. We show that this is indeed the case, establishing conditions under\nwhich the posterior contraction rates become adaptive to the intrinsic\ndimension $\\varrho$ in terms of the covering number of the data domain $X$ (the\nMinkowski dimension), and prove the optimal posterior contraction rate\n$O(n^{-s/(2s +\\varrho)})$, up to a logarithmic factor, assuming an\napproximation order $s$ of the reproducing kernel Hilbert space (RKHS) on\n${X}$. When ${X}$ is a $\\varrho$-dimensional compact smooth manifold, we study\nRKHS approximations to intrinsically defined $s$-order H\\\"older functions on\nthe manifold for any positive $s$ by a novel analysis of kernel approximations\non manifolds, leading to the optimal adaptive posterior contraction rate. We\npropose an empirical Bayes prior on the kernel bandwidth using kernel affinity\nand $k$-nearest neighbor statistics, eliminating the need for prior knowledge\nof the intrinsic dimension. The efficiency of the proposed Bayesian regression\napproach is demonstrated on various numerical experiments.\n","subjects":["Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}