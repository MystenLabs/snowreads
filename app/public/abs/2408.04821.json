{"id":"2408.04821","title":"VLM-MPC: Vision Language Foundation Model (VLM)-Guided Model Predictive\n  Controller (MPC) for Autonomous Driving","authors":"Keke Long, Haotian Shi, Jiaxi Liu, Xiaopeng Li","authorsParsed":[["Long","Keke",""],["Shi","Haotian",""],["Liu","Jiaxi",""],["Li","Xiaopeng",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 02:27:25 GMT"}],"updateDate":"2024-08-12","timestamp":1723170445000,"abstract":"  Motivated by the emergent reasoning capabilities of Vision Language Models\n(VLMs) and its potential to improve the comprehensibility of autonomous driving\nsystems, this paper introduces a closed-loop autonomous driving controller\ncalled VLM-MPC, which combines a VLM for high-level decision-making and a Model\nPredictive Controller (MPC) for low-level vehicle control. The proposed VLM-MPC\nsystem is structurally divided into two asynchronous components: an upper-level\nVLM and a lower-level MPC. The upper layer VLM generates driving parameters for\nlower-level control based on front camera images, ego vehicle state, traffic\nenvironment conditions, and reference memory. The lower-level MPC controls the\nvehicle in real-time using these parameters, considering engine lag and\nproviding state feedback to the entire system. Experiments based on the\nnuScenes dataset validated the effectiveness of the proposed VLM-MPC system\nacross various scenarios (e.g., night, rain, intersections). Results showed\nthat the VLM-MPC system consistently outperformed baseline models in terms of\nsafety and driving comfort. By comparing behaviors under different weather\nconditions and scenarios, we demonstrated the VLM's ability to understand the\nenvironment and make reasonable inferences.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}