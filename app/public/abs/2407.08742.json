{"id":"2407.08742","title":"Improved Robustness and Hyperparameter Selection in the Dense\n  Associative Memory","authors":"Hayden McAlister, Anthony Robins, Lech Szymanski","authorsParsed":[["McAlister","Hayden",""],["Robins","Anthony",""],["Szymanski","Lech",""]],"versions":[{"version":"v1","created":"Wed, 29 May 2024 01:23:19 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 03:53:32 GMT"},{"version":"v3","created":"Sat, 14 Sep 2024 04:59:03 GMT"}],"updateDate":"2024-09-17","timestamp":1716945799000,"abstract":"  The Dense Associative Memory generalizes the Hopfield network by allowing for\nsharper interaction functions. This increases the capacity of the network as an\nautoassociative memory as nearby learned attractors will not interfere with one\nanother. However, the implementation of the network relies on applying large\nexponents to the dot product of memory vectors and probe vectors. If the\ndimension of the data is large the calculation can be very large and result in\nimprecisions and overflow when using floating point numbers in a practical\nimplementation. We describe the computational issues in detail, modify the\noriginal network description to mitigate the problem, and show the modification\nwill not alter the networks' dynamics during update or training. We also show\nour modification greatly improves hyperparameter selection for the Dense\nAssociative Memory, removing dependence on the interaction vertex and resulting\nin an optimal region of hyperparameters that does not significantly change with\nthe interaction vertex as it does in the original network.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}