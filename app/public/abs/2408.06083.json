{"id":"2408.06083","title":"Towards Robust Monocular Depth Estimation in Non-Lambertian Surfaces","authors":"Junrui Zhang, Jiaqi Li, Yachuan Huang, Yiran Wang, Jinghong Zheng,\n  Liao Shen, and Zhiguo Cao","authorsParsed":[["Zhang","Junrui",""],["Li","Jiaqi",""],["Huang","Yachuan",""],["Wang","Yiran",""],["Zheng","Jinghong",""],["Shen","Liao",""],["Cao","Zhiguo",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 11:58:45 GMT"}],"updateDate":"2024-08-13","timestamp":1723463925000,"abstract":"  In the field of monocular depth estimation (MDE), many models with excellent\nzero-shot performance in general scenes emerge recently. However, these methods\noften fail in predicting non-Lambertian surfaces, such as transparent or mirror\n(ToM) surfaces, due to the unique reflective properties of these regions.\nPrevious methods utilize externally provided ToM masks and aim to obtain\ncorrect depth maps through direct in-painting of RGB images. These methods\nhighly depend on the accuracy of additional input masks, and the use of random\ncolors during in-painting makes them insufficiently robust. We are committed to\nincrementally enabling the baseline model to directly learn the uniqueness of\nnon-Lambertian surface regions for depth estimation through a well-designed\ntraining framework. Therefore, we propose non-Lambertian surface regional\nguidance, which constrains the predictions of MDE model from the gradient\ndomain to enhance its robustness. Noting the significant impact of lighting on\nthis task, we employ the random tone-mapping augmentation during training to\nensure the network can predict correct results for varying lighting inputs.\nAdditionally, we propose an optional novel lighting fusion module, which uses\nVariational Autoencoders to fuse multiple images and obtain the most\nadvantageous input RGB image for depth estimation when multi-exposure images\nare available. Our method achieves accuracy improvements of 33.39% and 5.21% in\nzero-shot testing on the Booster and Mirror3D dataset for non-Lambertian\nsurfaces, respectively, compared to the Depth Anything V2. The state-of-the-art\nperformance of 90.75 in delta1.05 within the ToM regions on the TRICKY2024\ncompetition test set demonstrates the effectiveness of our approach.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}