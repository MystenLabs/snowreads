{"id":"2408.13242","title":"Improving Equivariant Model Training via Constraint Relaxation","authors":"Stefanos Pertigkiozoglou, Evangelos Chatzipantazis, Shubhendu Trivedi,\n  Kostas Daniilidis","authorsParsed":[["Pertigkiozoglou","Stefanos",""],["Chatzipantazis","Evangelos",""],["Trivedi","Shubhendu",""],["Daniilidis","Kostas",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 17:35:08 GMT"}],"updateDate":"2024-08-26","timestamp":1724434508000,"abstract":"  Equivariant neural networks have been widely used in a variety of\napplications due to their ability to generalize well in tasks where the\nunderlying data symmetries are known. Despite their successes, such networks\ncan be difficult to optimize and require careful hyperparameter tuning to train\nsuccessfully. In this work, we propose a novel framework for improving the\noptimization of such models by relaxing the hard equivariance constraint during\ntraining: We relax the equivariance constraint of the network's intermediate\nlayers by introducing an additional non-equivariance term that we progressively\nconstrain until we arrive at an equivariant solution. By controlling the\nmagnitude of the activation of the additional relaxation term, we allow the\nmodel to optimize over a larger hypothesis space containing approximate\nequivariant networks and converge back to an equivariant solution at the end of\ntraining. We provide experimental results on different state-of-the-art network\narchitectures, demonstrating how this training framework can result in\nequivariant models with improved generalization performance.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mjfXkrDurG1zOK0Q4SPwhUpaH_WRuErjGIjnFiLoLm0","pdfSize":"1061695"}
