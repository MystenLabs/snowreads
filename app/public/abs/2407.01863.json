{"id":"2407.01863","title":"VSP: Assessing the dual challenges of perception and reasoning in\n  spatial planning tasks for VLMs","authors":"Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang\n  Wang, Yang Zhang, Shiyu Chang","authorsParsed":[["Wu","Qiucheng",""],["Zhao","Handong",""],["Saxon","Michael",""],["Bui","Trung",""],["Wang","William Yang",""],["Zhang","Yang",""],["Chang","Shiyu",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 00:24:01 GMT"}],"updateDate":"2024-07-03","timestamp":1719879841000,"abstract":"  Vision language models (VLMs) are an exciting emerging class of language\nmodels (LMs) that have merged classic LM capabilities with those of image\nprocessing systems. However, the ways that these capabilities combine are not\nalways intuitive and warrant direct investigation. One understudied capability\nin VLMs is visual spatial planning -- the ability to comprehend the spatial\narrangements of objects and devise action plans to achieve desired outcomes in\nvisual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates\nthe spatial planning capability in these models in general, and 2) breaks down\nthe visual planning task into finer-grained sub-tasks, including perception and\nreasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation\nshows that both open-source and private VLMs fail to generate effective plans\nfor even simple spatial planning tasks. Evaluations on the fine-grained\nanalytical tasks further reveal fundamental deficiencies in the models' visual\nperception and bottlenecks in reasoning abilities, explaining their worse\nperformance in the general spatial planning tasks. Our work illuminates future\ndirections for improving VLMs' abilities in spatial planning. Our benchmark is\npublicly available at\nhttps://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}