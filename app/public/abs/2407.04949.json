{"id":"2407.04949","title":"Beyond the Federation: Topology-aware Federated Learning for\n  Generalization to Unseen Clients","authors":"Mengmeng Ma and Tang Li and Xi Peng","authorsParsed":[["Ma","Mengmeng",""],["Li","Tang",""],["Peng","Xi",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 03:57:05 GMT"}],"updateDate":"2024-07-09","timestamp":1720238225000,"abstract":"  Federated Learning is widely employed to tackle distributed sensitive data.\nExisting methods primarily focus on addressing in-federation data\nheterogeneity. However, we observed that they suffer from significant\nperformance degradation when applied to unseen clients for out-of-federation\n(OOF) generalization. The recent attempts to address generalization to unseen\nclients generally struggle to scale up to large-scale distributed settings due\nto high communication or computation costs. Moreover, methods that scale well\noften demonstrate poor generalization capability. To achieve OOF-resiliency in\na scalable manner, we propose Topology-aware Federated Learning (TFL) that\nleverages client topology - a graph representing client relationships - to\neffectively train robust models against OOF data. We formulate a novel\noptimization problem for TFL, consisting of two key modules: Client Topology\nLearning, which infers the client relationships in a privacy-preserving manner,\nand Learning on Client Topology, which leverages the learned topology to\nidentify influential clients and harness this information into the FL\noptimization process to efficiently build robust models. Empirical evaluation\non a variety of real-world datasets verifies TFL's superior OOF robustness and\nscalability.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}