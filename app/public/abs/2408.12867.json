{"id":"2408.12867","title":"Semantic Alignment for Multimodal Large Language Models","authors":"Tao Wu, Mengze Li, Jingyuan Chen, Wei Ji, Wang Lin, Jinyang Gao, Kun\n  Kuang, Zhou Zhao, Fei Wu","authorsParsed":[["Wu","Tao",""],["Li","Mengze",""],["Chen","Jingyuan",""],["Ji","Wei",""],["Lin","Wang",""],["Gao","Jinyang",""],["Kuang","Kun",""],["Zhao","Zhou",""],["Wu","Fei",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 06:48:46 GMT"}],"updateDate":"2024-08-26","timestamp":1724395726000,"abstract":"  Research on Multi-modal Large Language Models (MLLMs) towards the multi-image\ncross-modal instruction has received increasing attention and made significant\nprogress, particularly in scenarios involving closely resembling images (e.g.,\nchange captioning). Existing MLLMs typically follow a two-step process in their\npipelines: first, extracting visual tokens independently for each input image,\nand then aligning these visual tokens from different images with the Large\nLanguage Model (LLM) in its textual feature space. However, the independent\nextraction of visual tokens for each image may result in different semantics\nbeing prioritized for different images in the first step, leading to a lack of\npreservation of linking information among images for subsequent LLM analysis.\nThis issue becomes more serious in scenarios where significant variations exist\namong the images (e.g., visual storytelling). To address this challenge, we\nintroduce Semantic Alignment for Multi-modal large language models (SAM). By\ninvolving the bidirectional semantic guidance between different images in the\nvisual-token extraction process, SAM aims to enhance the preservation of\nlinking information for coherent analysis and align the semantics of different\nimages before feeding them into LLM. As the test bed, we propose a large-scale\ndataset named MmLINK consisting of 69K samples. Different from most existing\ndatasets for MLLMs fine-tuning, our MmLINK dataset comprises multi-modal\ninstructions with significantly diverse images. Extensive experiments on the\ngroup captioning task and the storytelling task prove the effectiveness of our\nSAM model, surpassing the state-of-the-art methods by a large margin (+37% for\ngroup captioning and +22% for storytelling on CIDEr score). Project page:\nhttps://mccartney01.github.io/SAM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}