{"id":"2407.04001","title":"PaSE: Parallelization Strategies for Efficient DNN Training","authors":"Venmugil Elango","authorsParsed":[["Elango","Venmugil",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 15:21:20 GMT"}],"updateDate":"2024-07-08","timestamp":1720106480000,"abstract":"  Training a deep neural network (DNN) requires substantial computational and\nmemory requirements. It is common to use multiple devices to train a DNN to\nreduce the overall training time. There are several choices to parallelize each\nlayer in a DNN. Exhaustively searching this list to find an optimal\nparallelization strategy is prohibitively time consuming and impractical. The\nstandard practice is to use data parallelism because of its simplicity.\nHowever, data parallelism is often sub-optimal, and suffers from poor\nperformance and high memory requirement. Expert-designed strategies have been\nproposed on a case-by-case basis using domain specific knowledge. These\nexpert-designed strategies do not generalize well to DNNs other than the ones\nfor which they were designed, and are not always necessarily the best choice.\n  In this paper, we propose an approach to automatically find efficient\nparallelization strategies for DNNs from their computation graphs. We present\nan efficient algorithm to compute these strategies within a reasonable time in\npractice. We evaluate the effectiveness of our approach on various DNNs. We\nalso compare the performance of the strategies identified by our approach\nagainst data parallelism, expert-designed strategies, and the state-of-the-art\napproaches. Our results show that the strategies found using our approach\noutperform the baseline data parallelism strategy in all the cases. In\naddition, our strategies achieve better performance than the expert-designed\nstrategies and the state-of-the-art approaches.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}