{"id":"2407.07810","title":"Transformer Alignment in Large Language Models","authors":"Murdock Aubry, Haoming Meng, Anton Sugolov, Vardan Papyan","authorsParsed":[["Aubry","Murdock",""],["Meng","Haoming",""],["Sugolov","Anton",""],["Papyan","Vardan",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:30:27 GMT"}],"updateDate":"2024-07-11","timestamp":1720629027000,"abstract":"  Large Language Models (LLMs) have made significant strides in natural\nlanguage processing, and a precise understanding of the internal mechanisms\ndriving their success is essential. We regard LLMs as transforming embeddings\nvia a discrete, coupled, nonlinear, dynamical system in high dimensions. This\nperspective motivates tracing the trajectories of individual tokens as they\npass through transformer blocks, and linearizing the system along these\ntrajectories through their Jacobian matrices. In our analysis of 38 openly\navailable LLMs, we uncover the alignment of top left and right singular vectors\nof Residual Jacobians, as well as the emergence of linearity and layer-wise\nexponential growth. Notably, we discover that increased alignment\n$\\textit{positively correlates}$ with model performance. Metrics evaluated\npost-training show significant improvement in comparison to measurements made\nwith randomly initialized weights, highlighting the significant effects of\ntraining in transformers. These findings reveal a remarkable level of\nregularity that has previously been overlooked, reinforcing the dynamical\ninterpretation and paving the way for deeper understanding and optimization of\nLLM architectures.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JIUvX0IbYQScOuqQP3XccPnBBb32A3k0CFMf8fCHA_k","pdfSize":"5756930"}
