{"id":"2407.11930","title":"Fine-grained Hallucination Detection and Mitigation in Long-form\n  Question Answering","authors":"Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych","authorsParsed":[["Sachdeva","Rachneet",""],["Song","Yixiao",""],["Iyyer","Mohit",""],["Gurevych","Iryna",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:23:16 GMT"}],"updateDate":"2024-07-17","timestamp":1721150596000,"abstract":"  Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nhallucination and improves answer quality. Furthermore, humans find answers\ngenerated by our approach comprehensive and highly prefer them (84%) over the\nbaseline answers.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"v9Ynk5ka8_XimDWwDFJc-djJv9KqL9eXc54uvJ1Tz2w","pdfSize":"2558378"}
