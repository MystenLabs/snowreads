{"id":"2408.13849","title":"Sample-Independent Federated Learning Backdoor Attack","authors":"Weida Xu, Yang Xu, Sicong Zhang","authorsParsed":[["Xu","Weida",""],["Xu","Yang",""],["Zhang","Sicong",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 14:38:13 GMT"}],"updateDate":"2024-08-27","timestamp":1724596693000,"abstract":"  In federated learning, backdoor attacks embed triggers in the adversarial\nclient's data to inject a backdoor into the model. To evade detection through\nsample analysis, non-sample-modifying backdoor attack methods based on dropout\nhave been developed. However, these methods struggle to covertly utilize\ndropout in evaluation mode, thus hindering their deployment in real-world\nscenarios. To address these, this paper introduces GhostB, a novel approach to\nfederated learning backdoor attacks that neither alters samples nor relies on\ndropout. This method employs the behavior of neurons producing specific values\nas triggers. By mapping these neuronal values to categories specified by the\nadversary, the backdoor is implanted and activated when particular feature\nvalues are detected at designated neurons. Our experiments conducted on TIMIT,\nLibriSpeech, and VoxCeleb2 databases in both Closed Set Identification (CSI)\nand Open Set Identification (OSI) scenarios demonstrate that GhostB achieves a\n100% success rate upon activation, with this rate maintained across experiments\ninvolving 1 to 50 ghost neurons. This paper investigates how the dispersion of\nneurons and their depth within hidden layers affect the success rate, revealing\nthat increased dispersion and positioning of neurons can significantly decrease\neffectiveness, potentially rendering the attack unsuccessful.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}