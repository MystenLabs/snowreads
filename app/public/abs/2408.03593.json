{"id":"2408.03593","title":"Bridging the Gap between Audio and Text using Parallel-attention for\n  User-defined Keyword Spotting","authors":"Youkyum Kim, Jaemin Jung, Jihwan Park, Byeong-Yeol Kim, Joon Son Chung","authorsParsed":[["Kim","Youkyum",""],["Jung","Jaemin",""],["Park","Jihwan",""],["Kim","Byeong-Yeol",""],["Chung","Joon Son",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 07:13:55 GMT"}],"updateDate":"2024-08-08","timestamp":1723014835000,"abstract":"  This paper proposes a novel user-defined keyword spotting framework that\naccurately detects audio keywords based on text enrollment. Since audio data\npossesses additional acoustic information compared to text, there are\ndiscrepancies between these two modalities. To address this challenge, we\npresent ParallelKWS, which utilises self- and cross-attention in a parallel\narchitecture to effectively capture information both within and across the two\nmodalities. We further propose a phoneme duration-based alignment loss that\nenforces the sequential correspondence between audio and text features.\nExtensive experimental results demonstrate that our proposed method achieves\nstate-of-the-art performance on several benchmark datasets in both seen and\nunseen domains, without incorporating extra data beyond the dataset used in\nprevious studies.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}