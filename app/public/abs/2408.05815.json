{"id":"2408.05815","title":"HySparK: Hybrid Sparse Masking for Large Scale Medical Image\n  Pre-Training","authors":"Fenghe Tang, Ronghao Xu, Qingsong Yao, Xueming Fu, Quan Quan, Heqin\n  Zhu, Zaiyi Liu, S. Kevin Zhou","authorsParsed":[["Tang","Fenghe",""],["Xu","Ronghao",""],["Yao","Qingsong",""],["Fu","Xueming",""],["Quan","Quan",""],["Zhu","Heqin",""],["Liu","Zaiyi",""],["Zhou","S. Kevin",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 16:31:39 GMT"}],"updateDate":"2024-08-13","timestamp":1723393899000,"abstract":"  The generative self-supervised learning strategy exhibits remarkable learning\nrepresentational capabilities. However, there is limited attention to\nend-to-end pre-training methods based on a hybrid architecture of CNN and\nTransformer, which can learn strong local and global representations\nsimultaneously. To address this issue, we propose a generative pre-training\nstrategy called Hybrid Sparse masKing (HySparK) based on masked image modeling\nand apply it to large-scale pre-training on medical images. First, we perform a\nbottom-up 3D hybrid masking strategy on the encoder to keep consistency\nmasking. Then we utilize sparse convolution for the top CNNs and encode\nunmasked patches for the bottom vision Transformers. Second, we employ a simple\nhierarchical decoder with skip-connections to achieve dense multi-scale feature\nreconstruction. Third, we implement our pre-training method on a collection of\nmultiple large-scale 3D medical imaging datasets. Extensive experiments\nindicate that our proposed pre-training strategy demonstrates robust\ntransfer-ability in supervised downstream tasks and sheds light on HySparK's\npromising prospects. The code is available at\nhttps://github.com/FengheTan9/HySparK\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}