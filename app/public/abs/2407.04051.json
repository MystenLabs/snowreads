{"id":"2407.04051","title":"FunAudioLLM: Voice Understanding and Generation Foundation Models for\n  Natural Interaction Between Humans and LLMs","authors":"Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao,\n  Yue Gu, Ting He, Hangrui Hu, Kai Hu, Shengpeng Ji, Yabin Li, Zerui Li, Heng\n  Lu, Haoneng Luo, Xiang Lv, Bin Ma, Ziyang Ma, Chongjia Ni, Changhe Song,\n  Jiaqi Shi, Xian Shi, Hao Wang, Wen Wang, Yuxuan Wang, Zhangyu Xiao, Zhijie\n  Yan, Yexin Yang, Bin Zhang, Qinglin Zhang, Shiliang Zhang, Nan Zhao, Siqi\n  Zheng","authorsParsed":[["An","Keyu",""],["Chen","Qian",""],["Deng","Chong",""],["Du","Zhihao",""],["Gao","Changfeng",""],["Gao","Zhifu",""],["Gu","Yue",""],["He","Ting",""],["Hu","Hangrui",""],["Hu","Kai",""],["Ji","Shengpeng",""],["Li","Yabin",""],["Li","Zerui",""],["Lu","Heng",""],["Luo","Haoneng",""],["Lv","Xiang",""],["Ma","Bin",""],["Ma","Ziyang",""],["Ni","Chongjia",""],["Song","Changhe",""],["Shi","Jiaqi",""],["Shi","Xian",""],["Wang","Hao",""],["Wang","Wen",""],["Wang","Yuxuan",""],["Xiao","Zhangyu",""],["Yan","Zhijie",""],["Yang","Yexin",""],["Zhang","Bin",""],["Zhang","Qinglin",""],["Zhang","Shiliang",""],["Zhao","Nan",""],["Zheng","Siqi",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 16:49:02 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 07:08:30 GMT"},{"version":"v3","created":"Thu, 11 Jul 2024 02:08:35 GMT"}],"updateDate":"2024-07-12","timestamp":1720111742000,"abstract":"  This report introduces FunAudioLLM, a model family designed to enhance\nnatural voice interactions between humans and large language models (LLMs). At\nits core are two innovative models: SenseVoice, which handles multilingual\nspeech recognition, emotion recognition, and audio event detection; and\nCosyVoice, which facilitates natural speech generation with control over\nmultiple languages, timbre, speaking style, and speaker identity.\nSenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and\nSenseVoice-Large supports high-precision ASR for over 50 languages, while\nCosyVoice excels in multi-lingual voice generation, zero-shot in-context\nlearning, cross-lingual voice cloning, and instruction-following capabilities.\nThe models related to SenseVoice and CosyVoice have been open-sourced on\nModelscope and Huggingface, along with the corresponding training, inference,\nand fine-tuning codes released on GitHub. By integrating these models with\nLLMs, FunAudioLLM enables applications such as speech-to-speech translation,\nemotional voice chat, interactive podcasts, and expressive audiobook narration,\nthereby pushing the boundaries of voice interaction technology. Demos are\navailable at https://fun-audio-llm.github.io, and the code can be accessed at\nhttps://github.com/FunAudioLLM.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}