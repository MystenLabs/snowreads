{"id":"2408.05375","title":"Enhancing Representation Learning of EEG Data with Masked Autoencoders","authors":"Yifei Zhou, Sitong Liu","authorsParsed":[["Zhou","Yifei",""],["Liu","Sitong",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 22:56:36 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 03:13:33 GMT"}],"updateDate":"2024-09-04","timestamp":1723244196000,"abstract":"  Self-supervised learning has been a powerful training paradigm to facilitate\nrepresentation learning. In this study, we design a masked autoencoder (MAE) to\nguide deep learning models to learn electroencephalography (EEG) signal\nrepresentation. Our MAE includes an encoder and a decoder. A certain proportion\nof input EEG signals are randomly masked and sent to our MAE. The goal is to\nrecover these masked signals. After this self-supervised pre-training, the\nencoder is fine-tuned on downstream tasks. We evaluate our MAE on EEGEyeNet\ngaze estimation task. We find that the MAE is an effective brain signal\nlearner. It also significantly improves learning efficiency. Compared to the\nmodel without MAE pre-training, the pre-trained one achieves equal performance\nwith 1/3 the time of training and outperforms it in half the training time. Our\nstudy shows that self-supervised learning is a promising research direction for\nEEG-based applications as other fields (natural language processing, computer\nvision, robotics, etc.), and thus we expect foundation models to be successful\nin EEG domain.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/"}