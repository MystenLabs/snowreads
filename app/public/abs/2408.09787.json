{"id":"2408.09787","title":"Anim-Director: A Large Multimodal Model Powered Agent for Controllable\n  Animation Video Generation","authors":"Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi\n  Xu, Zhen Zhao, Min Zhang","authorsParsed":[["Li","Yunxin",""],["Shi","Haoyuan",""],["Hu","Baotian",""],["Wang","Longyue",""],["Zhu","Jiashun",""],["Xu","Jinyi",""],["Zhao","Zhen",""],["Zhang","Min",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 08:27:31 GMT"}],"updateDate":"2024-08-20","timestamp":1724056051000,"abstract":"  Traditional animation generation methods depend on training generative models\nwith human-labelled data, entailing a sophisticated multi-stage pipeline that\ndemands substantial human effort and incurs high training costs. Due to limited\nprompting plans, these methods typically produce brief, information-poor, and\ncontext-incoherent animations. To overcome these limitations and automate the\nanimation process, we pioneer the introduction of large multimodal models\n(LMMs) as the core processor to build an autonomous animation-making agent,\nnamed Anim-Director. This agent mainly harnesses the advanced understanding and\nreasoning capabilities of LMMs and generative AI tools to create animated\nvideos from concise narratives or simple instructions. Specifically, it\noperates in three main stages: Firstly, the Anim-Director generates a coherent\nstoryline from user inputs, followed by a detailed director's script that\nencompasses settings of character profiles and interior/exterior descriptions,\nand context-coherent scene descriptions that include appearing characters,\ninteriors or exteriors, and scene events. Secondly, we employ LMMs with the\nimage generation tool to produce visual images of settings and scenes. These\nimages are designed to maintain visual consistency across different scenes\nusing a visual-language prompting method that combines scene descriptions and\nimages of the appearing character and setting. Thirdly, scene images serve as\nthe foundation for producing animated videos, with LMMs generating prompts to\nguide this process. The whole process is notably autonomous without manual\nintervention, as the LMMs interact seamlessly with generative tools to generate\nprompts, evaluate visual quality, and select the best one to optimize the final\noutput.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"j8AUMSxhm5UjfmrS_YpEsyYSGQaOVoBR7_rIx6fT5og","pdfSize":"17927129"}
