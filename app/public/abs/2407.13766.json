{"id":"2407.13766","title":"Visual Haystacks: Answering Harder Questions About Sets of Images","authors":"Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E.\n  Gonzalez, Trevor Darrell, David M. Chan","authorsParsed":[["Wu","Tsung-Han",""],["Biamby","Giscard",""],["Quenum","Jerome",""],["Gupta","Ritwik",""],["Gonzalez","Joseph E.",""],["Darrell","Trevor",""],["Chan","David M.",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:59:30 GMT"}],"updateDate":"2024-07-19","timestamp":1721325570000,"abstract":"  Recent advancements in Large Multimodal Models (LMMs) have made significant\nprogress in the field of single-image visual question answering. However, these\nmodels face substantial challenges when tasked with queries that span extensive\ncollections of images, similar to real-world scenarios like searching through\nlarge photo albums, finding specific information across the internet, or\nmonitoring environmental changes through satellite imagery. This paper explores\nthe task of Multi-Image Visual Question Answering (MIQA): given a large set of\nimages and a natural language query, the task is to generate a relevant and\ngrounded response. We propose a new public benchmark, dubbed \"Visual Haystacks\n(VHs),\" specifically designed to evaluate LMMs' capabilities in visual\nretrieval and reasoning over sets of unrelated images, where we perform\ncomprehensive evaluations demonstrating that even robust closed-source models\nstruggle significantly. Towards addressing these shortcomings, we introduce\nMIRAGE (Multi-Image Retrieval Augmented Generation), a novel retrieval/QA\nframework tailored for LMMs that confronts the challenges of MIQA with marked\nefficiency and accuracy improvements over baseline methods. Our evaluation\nshows that MIRAGE surpasses closed-source GPT-4o models by up to 11% on the VHs\nbenchmark and offers up to 3.4x improvements in efficiency over text-focused\nmulti-stage approaches.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}