{"id":"2407.19034","title":"MangaUB: A Manga Understanding Benchmark for Large Multimodal Models","authors":"Hikaru Ikuta, Leslie W\\\"ohler, Kiyoharu Aizawa","authorsParsed":[["Ikuta","Hikaru",""],["WÃ¶hler","Leslie",""],["Aizawa","Kiyoharu",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 18:21:30 GMT"}],"updateDate":"2024-07-30","timestamp":1722018090000,"abstract":"  Manga is a popular medium that combines stylized drawings and text to convey\nstories. As manga panels differ from natural images, computational systems\ntraditionally had to be designed specifically for manga. Recently, the adaptive\nnature of modern large multimodal models (LMMs) shows possibilities for more\ngeneral approaches. To provide an analysis of the current capability of LMMs\nfor manga understanding tasks and identifying areas for their improvement, we\ndesign and evaluate MangaUB, a novel manga understanding benchmark for LMMs.\nMangaUB is designed to assess the recognition and understanding of content\nshown in a single panel as well as conveyed across multiple panels, allowing\nfor a fine-grained analysis of a model's various capabilities required for\nmanga understanding. Our results show strong performance on the recognition of\nimage content, while understanding the emotion and information conveyed across\nmultiple panels is still challenging, highlighting future work towards LMMs for\nmanga understanding.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}