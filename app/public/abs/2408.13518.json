{"id":"2408.13518","title":"Selective Preference Optimization via Token-Level Reward Function\n  Estimation","authors":"Kailai Yang, Zhiwei Liu, Qianqian Xie, Jimin Huang, Erxue Min, Sophia\n  Ananiadou","authorsParsed":[["Yang","Kailai",""],["Liu","Zhiwei",""],["Xie","Qianqian",""],["Huang","Jimin",""],["Min","Erxue",""],["Ananiadou","Sophia",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 08:44:04 GMT"}],"updateDate":"2024-08-27","timestamp":1724489044000,"abstract":"  Recent advancements in large language model alignment leverage token-level\nsupervisions to perform fine-grained preference optimization. However, existing\ntoken-level alignment methods either optimize on all available tokens, which\ncan be noisy and inefficient, or perform selective training with complex and\nexpensive key token selection strategies. In this work, we propose Selective\nPreference Optimization (SePO), a novel selective alignment strategy that\ncenters on efficient key token selection. SePO proposes the first token\nselection method based on Direct Preference Optimization (DPO), which trains an\noracle model to estimate a token-level reward function on the target data. This\nmethod applies to any existing alignment datasets with response-level\nannotations and enables cost-efficient token selection with small-scale oracle\nmodels and training data. The estimated reward function is then utilized to\nscore all tokens within the target dataset, where only the key tokens are\nselected to supervise the target policy model with a reference model-free\ncontrastive objective function. Extensive experiments on three public\nevaluation benchmarks show that SePO significantly outperforms competitive\nbaseline methods by only optimizing 30% key tokens on the target dataset. SePO\napplications on weak-to-strong generalization show that weak oracle models\neffectively supervise strong policy models with up to 16.8x more parameters.\nSePO also effectively selects key tokens from out-of-distribution data to\nenhance strong policy models and alleviate the over-optimization problem.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}