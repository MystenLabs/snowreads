{"id":"2407.03994","title":"Unlocking the Potential of Model Merging for Low-Resource Languages","authors":"Mingxu Tao, Chen Zhang, Quzhe Huang, Tianyao Ma, Songfang Huang,\n  Dongyan Zhao, Yansong Feng","authorsParsed":[["Tao","Mingxu",""],["Zhang","Chen",""],["Huang","Quzhe",""],["Ma","Tianyao",""],["Huang","Songfang",""],["Zhao","Dongyan",""],["Feng","Yansong",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 15:14:17 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 11:09:19 GMT"}],"updateDate":"2024-07-10","timestamp":1720106057000,"abstract":"  Adapting large language models (LLMs) to new languages typically involves\ncontinual pre-training (CT) followed by supervised fine-tuning (SFT). However,\nthis CT-then-SFT approach struggles with limited data in the context of\nlow-resource languages, failing to balance language modeling and task-solving\ncapabilities. We thus propose model merging as an alternative for low-resource\nlanguages, combining models with distinct capabilities into a single model\nwithout additional training. We use model merging to develop task-solving LLMs\nfor low-resource languages without SFT data in the target languages. Our\nexperiments based on Llama-2-7B demonstrate that model merging effectively\nendows LLMs for low-resource languages with task-solving abilities,\noutperforming CT-then-SFT in scenarios with extremely scarce data. Observing\nperformance saturation in model merging with more training tokens, we further\nanalyze the merging process and introduce a slack variable to the model merging\nalgorithm to mitigate the loss of important parameters, thereby enhancing\nperformance. We hope that model merging can benefit more human languages\nsuffering from data scarcity with its higher data efficiency.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}