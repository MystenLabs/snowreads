{"id":"2408.10085","title":"MASALA: Model-Agnostic Surrogate Explanations by Locality Adaptation","authors":"Saif Anwar, Nathan Griffiths, Abhir Bhalerao, Thomas Popham","authorsParsed":[["Anwar","Saif",""],["Griffiths","Nathan",""],["Bhalerao","Abhir",""],["Popham","Thomas",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 15:26:45 GMT"}],"updateDate":"2024-08-20","timestamp":1724081205000,"abstract":"  Existing local Explainable AI (XAI) methods, such as LIME, select a region of\nthe input space in the vicinity of a given input instance, for which they\napproximate the behaviour of a model using a simpler and more interpretable\nsurrogate model. The size of this region is often controlled by a user-defined\nlocality hyperparameter. In this paper, we demonstrate the difficulties\nassociated with defining a suitable locality size to capture impactful model\nbehaviour, as well as the inadequacy of using a single locality size to explain\nall predictions. We propose a novel method, MASALA, for generating\nexplanations, which automatically determines the appropriate local region of\nimpactful model behaviour for each individual instance being explained. MASALA\napproximates the local behaviour used by a complex model to make a prediction\nby fitting a linear surrogate model to a set of points which experience similar\nmodel behaviour. These points are found by clustering the input space into\nregions of linear behavioural trends exhibited by the model. We compare the\nfidelity and consistency of explanations generated by our method with existing\nlocal XAI methods, namely LIME and CHILLI. Experiments on the PHM08 and MIDAS\ndatasets show that our method produces more faithful and consistent\nexplanations than existing methods, without the need to define any sensitive\nlocality hyperparameters.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"lhIk0exhW7UV8ysBvjD3z1Pwe6WlszjBAkH5mZugGn8","pdfSize":"8445110"}
