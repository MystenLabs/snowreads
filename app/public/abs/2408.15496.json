{"id":"2408.15496","title":"ReMamba: Equip Mamba with Effective Long-Sequence Modeling","authors":"Danlong Yuan, Jiahao Liu, Bei Li, Huishuai Zhang, Jingang Wang,\n  Xunliang Cai, Dongyan Zhao","authorsParsed":[["Yuan","Danlong",""],["Liu","Jiahao",""],["Li","Bei",""],["Zhang","Huishuai",""],["Wang","Jingang",""],["Cai","Xunliang",""],["Zhao","Dongyan",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 02:47:27 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 10:35:52 GMT"},{"version":"v3","created":"Sun, 1 Sep 2024 06:03:46 GMT"}],"updateDate":"2024-09-04","timestamp":1724813247000,"abstract":"  While the Mamba architecture demonstrates superior inference efficiency and\ncompetitive performance on short-context natural language processing (NLP)\ntasks, empirical evidence suggests its capacity to comprehend long contexts is\nlimited compared to transformer-based models. In this study, we investigate the\nlong-context efficiency issues of the Mamba models and propose ReMamba, which\nenhances Mamba's ability to comprehend long contexts. ReMamba incorporates\nselective compression and adaptation techniques within a two-stage re-forward\nprocess, incurring minimal additional inference costs overhead. Experimental\nresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,\nimproving over the baselines by 3.2 and 1.6 points, respectively, and attaining\nperformance almost on par with same-size transformer models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}