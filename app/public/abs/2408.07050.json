{"id":"2408.07050","title":"PSM: Learning Probabilistic Embeddings for Multi-scale Zero-Shot\n  Soundscape Mapping","authors":"Subash Khanal, Eric Xing, Srikumar Sastry, Aayush Dhakal, Zhexiao\n  Xiong, Adeel Ahmad, Nathan Jacobs","authorsParsed":[["Khanal","Subash",""],["Xing","Eric",""],["Sastry","Srikumar",""],["Dhakal","Aayush",""],["Xiong","Zhexiao",""],["Ahmad","Adeel",""],["Jacobs","Nathan",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 17:37:40 GMT"}],"updateDate":"2024-08-14","timestamp":1723570660000,"abstract":"  A soundscape is defined by the acoustic environment a person perceives at a\nlocation. In this work, we propose a framework for mapping soundscapes across\nthe Earth. Since soundscapes involve sound distributions that span varying\nspatial scales, we represent locations with multi-scale satellite imagery and\nlearn a joint representation among this imagery, audio, and text. To capture\nthe inherent uncertainty in the soundscape of a location, we design the\nrepresentation space to be probabilistic. We also fuse ubiquitous metadata\n(including geolocation, time, and data source) to enable learning of spatially\nand temporally dynamic representations of soundscapes. We demonstrate the\nutility of our framework by creating large-scale soundscape maps integrating\nboth audio and text with temporal control. To facilitate future research on\nthis task, we also introduce a large-scale dataset, GeoSound, containing over\n$300k$ geotagged audio samples paired with both low- and high-resolution\nsatellite imagery. We demonstrate that our method outperforms the existing\nstate-of-the-art on both GeoSound and the existing SoundingEarth dataset. Our\ndataset and code is available at https://github.com/mvrl/PSM.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}