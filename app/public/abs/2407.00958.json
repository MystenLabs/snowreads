{"id":"2407.00958","title":"Universal Approximation Theory: The Basic Theory for Transformer-based\n  Large Language Models","authors":"Wei Wang, Qing Li","authorsParsed":[["Wang","Wei",""],["Li","Qing",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 04:29:35 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 02:08:03 GMT"},{"version":"v3","created":"Mon, 19 Aug 2024 04:02:44 GMT"}],"updateDate":"2024-08-20","timestamp":1719808175000,"abstract":"  Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uaPj4qZWds9yxe4Mk-p1Nb07SPInohDHHoOpUQASWQE","pdfSize":"2524563","objectId":"0x23bfbddfeb2479069afd2c95af6e410602ac0204b425812e21528acca046b54a","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
