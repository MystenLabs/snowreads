{"id":"2407.06690","title":"Hierarchical Average-Reward Linearly-solvable Markov Decision Processes","authors":"Guillermo Infante, Anders Jonsson, Vicen\\c{c} G\\'omez","authorsParsed":[["Infante","Guillermo",""],["Jonsson","Anders",""],["Gómez","Vicenç",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 09:06:44 GMT"}],"updateDate":"2024-07-10","timestamp":1720516004000,"abstract":"  We introduce a novel approach to hierarchical reinforcement learning for\nLinearly-solvable Markov Decision Processes (LMDPs) in the infinite-horizon\naverage-reward setting. Unlike previous work, our approach allows learning\nlow-level and high-level tasks simultaneously, without imposing limiting\nrestrictions on the low-level tasks. Our method relies on partitions of the\nstate space that create smaller subtasks that are easier to solve, and the\nequivalence between such partitions to learn more efficiently. We then exploit\nthe compositionality of low-level tasks to exactly represent the value function\nof the high-level task. Experiments show that our approach can outperform flat\naverage-reward reinforcement learning by one or several orders of magnitude.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"sVd54T92b3-keOVYgjobs34PJdJm_PqJ2V-63rXyACg","pdfSize":"1503858"}
