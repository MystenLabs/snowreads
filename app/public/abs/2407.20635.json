{"id":"2407.20635","title":"Autonomous Improvement of Instruction Following Skills via Foundation\n  Models","authors":"Zhiyuan Zhou, Pranav Atreya, Abraham Lee, Homer Walke, Oier Mees,\n  Sergey Levine","authorsParsed":[["Zhou","Zhiyuan",""],["Atreya","Pranav",""],["Lee","Abraham",""],["Walke","Homer",""],["Mees","Oier",""],["Levine","Sergey",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 08:26:44 GMT"}],"updateDate":"2024-07-31","timestamp":1722328004000,"abstract":"  Intelligent instruction-following robots capable of improving from\nautonomously collected experience have the potential to transform robot\nlearning: instead of collecting costly teleoperated demonstration data,\nlarge-scale deployment of fleets of robots can quickly collect larger\nquantities of autonomous data that can collectively improve their performance.\nHowever, autonomous improvement requires solving two key problems: (i) fully\nautomating a scalable data collection procedure that can collect diverse and\nsemantically meaningful robot data and (ii) learning from non-optimal,\nautonomous data with no human annotations. To this end, we propose a novel\napproach that addresses these challenges, allowing instruction-following\npolicies to improve from autonomously collected data without human supervision.\nOur framework leverages vision-language models to collect and evaluate\nsemantically meaningful experiences in new environments, and then utilizes a\ndecomposition of instruction following tasks into (semantic)\nlanguage-conditioned image generation and (non-semantic) goal reaching, which\nmakes it significantly more practical to improve from this autonomously\ncollected data without any human annotations. We carry out extensive\nexperiments in the real world to demonstrate the effectiveness of our approach,\nand find that in a suite of unseen environments, the robot policy can be\nimproved significantly with autonomously collected data. We open-source the\ncode for our semantic autonomous improvement pipeline, as well as our\nautonomous dataset of 30.5K trajectories collected across five tabletop\nenvironments.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}