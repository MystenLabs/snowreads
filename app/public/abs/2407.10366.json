{"id":"2407.10366","title":"Accessing Vision Foundation Models at ImageNet-level Costs","authors":"Yitian Zhang, Xu Ma, Yue Bai, Huan Wang, Yun Fu","authorsParsed":[["Zhang","Yitian",""],["Ma","Xu",""],["Bai","Yue",""],["Wang","Huan",""],["Fu","Yun",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 00:13:53 GMT"}],"updateDate":"2024-07-16","timestamp":1721002433000,"abstract":"  Vision foundation models are renowned for their generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could advance research in this field.\nIn this work, we offer a very simple and general solution, named Proteus, to\ndistill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nLeveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the performance of\nthe Oracle method DINOv2-L/14 (142M training data) across 15 benchmarks and\noutperforms other vision foundation models including CLIP-L/14 (400M),\nOpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M).\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}