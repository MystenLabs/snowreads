{"id":"2407.08935","title":"Distributed Backdoor Attacks on Federated Graph Learning and Certified\n  Defenses","authors":"Yuxin Yang (1 and 2), Qiang Li (1), Jinyuan Jia (3), Yuan Hong (4),\n  Binghui Wang (2) ((1) College of Computer Science and Technology, Jilin\n  University, (2) Illinois Institute of Technology, (3) The Pennsylvania State\n  University, (4) University of Connecticut)","authorsParsed":[["Yang","Yuxin","","1 and 2"],["Li","Qiang",""],["Jia","Jinyuan",""],["Hong","Yuan",""],["Wang","Binghui",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 02:43:44 GMT"}],"updateDate":"2024-07-15","timestamp":1720752224000,"abstract":"  Federated graph learning (FedGL) is an emerging federated learning (FL)\nframework that extends FL to learn graph data from diverse sources. FL for\nnon-graph data has shown to be vulnerable to backdoor attacks, which inject a\nshared backdoor trigger into the training data such that the trained backdoored\nFL model can predict the testing data containing the trigger as the attacker\ndesires. However, FedGL against backdoor attacks is largely unexplored, and no\neffective defense exists.\n  In this paper, we aim to address such significant deficiency. First, we\npropose an effective, stealthy, and persistent backdoor attack on FedGL. Our\nattack uses a subgraph as the trigger and designs an adaptive trigger generator\nthat can derive the effective trigger location and shape for each graph. Our\nattack shows that empirical defenses are hard to detect/remove our generated\ntriggers. To mitigate it, we further develop a certified defense for any\nbackdoored FedGL model against the trigger with any shape at any location. Our\ndefense involves carefully dividing a testing graph into multiple subgraphs and\ndesigning a majority vote-based ensemble classifier on these subgraphs. We then\nderive the deterministic certified robustness based on the ensemble classifier\nand prove its tightness. We extensively evaluate our attack and defense on six\ngraph datasets. Our attack results show our attack can obtain > 90% backdoor\naccuracy in almost all datasets. Our defense results show, in certain cases,\nthe certified accuracy for clean testing graphs against an arbitrary trigger\nwith size 20 can be close to the normal accuracy under no attack, while there\nis a moderate gap in other cases. Moreover, the certified backdoor accuracy is\nalways 0 for backdoored testing graphs generated by our attack, implying our\ndefense can fully mitigate the attack. Source code is available at:\nhttps://github.com/Yuxin104/Opt-GDBA.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}