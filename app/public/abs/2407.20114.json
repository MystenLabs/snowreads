{"id":"2407.20114","title":"FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval\n  for comparative performance analysis","authors":"Mikel Williams-Lekuona and Georgina Cosma","authorsParsed":[["Williams-Lekuona","Mikel",""],["Cosma","Georgina",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 15:44:22 GMT"}],"updateDate":"2024-07-30","timestamp":1722267862000,"abstract":"  In the field of Image-Text Retrieval (ITR), recent advancements have\nleveraged large-scale Vision-Language Pretraining (VLP) for Fine-Grained (FG)\ninstance-level retrieval, achieving high accuracy at the cost of increased\ncomputational complexity. For Coarse-Grained (CG) category-level retrieval,\nprominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency,\nalbeit at the cost of retrieval performance. Due to differences in\nmethodologies, FG and CG models are rarely compared directly within evaluations\nin the literature, resulting in a lack of empirical data quantifying the\nretrieval performance-efficiency tradeoffs between the two. This paper\naddresses this gap by introducing the \\texttt{FiCo-ITR} library, which\nstandardises evaluation methodologies for both FG and CG models, facilitating\ndirect comparisons. We conduct empirical evaluations of representative models\nfrom both subfields, analysing precision, recall, and computational complexity\nacross varying data scales. Our findings offer new insights into the\nperformance-efficiency trade-offs between recent representative FG and CG\nmodels, highlighting their respective strengths and limitations. These findings\nprovide the foundation necessary to make more informed decisions regarding\nmodel selection for specific retrieval tasks and highlight avenues for future\nresearch into hybrid systems that leverage the strengths of both FG and CG\napproaches.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}