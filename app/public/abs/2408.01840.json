{"id":"2408.01840","title":"E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry\n  Images","authors":"Yunshan Qi, Jia Li, Yifan Zhao, Yu Zhang, and Lin Zhu","authorsParsed":[["Qi","Yunshan",""],["Li","Jia",""],["Zhao","Yifan",""],["Zhang","Yu",""],["Zhu","Lin",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 18:47:31 GMT"}],"updateDate":"2024-08-06","timestamp":1722710851000,"abstract":"  Neural Radiance Fields (NeRF) achieve impressive rendering performance by\nlearning volumetric 3D representation from several images of different views.\nHowever, it is difficult to reconstruct a sharp NeRF from blurry input as it\noften occurs in the wild. To solve this problem, we propose a novel Efficient\nEvent-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and\nevent streams. To effectively introduce event streams into the neural\nvolumetric representation learning process, we propose an event-enhanced blur\nrendering loss and an event rendering loss, which guide the network via\nmodeling the real blur process and event generation process, respectively.\nSpecifically, we leverage spatial-temporal information from the event stream to\nevenly distribute learning attention over temporal blur while simultaneously\nfocusing on blurry texture through the spatial attention. Moreover, a camera\npose estimation framework for real-world data is built with the guidance of the\nevents to generalize the method to practical applications. Compared to previous\nimage-based or event-based NeRF, our framework makes more profound use of the\ninternal relationship between events and images. Extensive experiments on both\nsynthetic data and real-world data demonstrate that E$^3$NeRF can effectively\nlearn a sharp NeRF from blurry images, especially in non-uniform motion and\nlow-light scenes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}