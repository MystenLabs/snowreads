{"id":"2408.05772","title":"An analysis of HOI: using a training-free method with multimodal visual\n  foundation models when only the test set is available, without the training\n  set","authors":"Chaoyi Ai","authorsParsed":[["Ai","Chaoyi",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 13:40:02 GMT"}],"updateDate":"2024-08-13","timestamp":1723383602000,"abstract":"  Human-Object Interaction (HOI) aims to identify the pairs of humans and\nobjects in images and to recognize their relationships, ultimately forming\n$\\langle human, object, verb \\rangle$ triplets. Under default settings, HOI\nperformance is nearly saturated, with many studies focusing on long-tail\ndistribution and zero-shot/few-shot scenarios. Let us consider an intriguing\nproblem:``What if there is only test dataset without training dataset, using\nmultimodal visual foundation model in a training-free manner? '' This study\nuses two experimental settings: grounding truth and random arbitrary\ncombinations. We get some interesting conclusion and find that the open\nvocabulary capabilities of the multimodal visual foundation model are not yet\nfully realized. Additionally, replacing the feature extraction with grounding\nDINO further confirms these findings.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}