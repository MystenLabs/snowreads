{"id":"2408.10115","title":"GLIMMER: Incorporating Graph and Lexical Features in Unsupervised\n  Multi-Document Summarization","authors":"Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan\n  Li, Xiang Meng, Weiqing Huang","authorsParsed":[["Liu","Ran",""],["Liu","Ming",""],["Yu","Min",""],["Jiang","Jianguo",""],["Li","Gang",""],["Zhang","Dan",""],["Li","Jingyuan",""],["Meng","Xiang",""],["Huang","Weiqing",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 16:01:48 GMT"}],"updateDate":"2024-08-20","timestamp":1724083308000,"abstract":"  Pre-trained language models are increasingly being used in multi-document\nsummarization tasks. However, these models need large-scale corpora for\npre-training and are domain-dependent. Other non-neural unsupervised\nsummarization approaches mostly rely on key sentence extraction, which can lead\nto information loss. To address these challenges, we propose a lightweight yet\neffective unsupervised approach called GLIMMER: a Graph and LexIcal features\nbased unsupervised Multi-docuMEnt summaRization approach. It first constructs a\nsentence graph from the source documents, then automatically identifies\nsemantic clusters by mining low-level features from raw texts, thereby\nimproving intra-cluster correlation and the fluency of generated sentences.\nFinally, it summarizes clusters into natural sentences. Experiments conducted\non Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach\noutperforms existing unsupervised approaches. Furthermore, it surpasses\nstate-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS\nand PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,\nhuman evaluations indicate that summaries generated by GLIMMER achieve high\nreadability and informativeness scores. Our code is available at\nhttps://github.com/Oswald1997/GLIMMER.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}