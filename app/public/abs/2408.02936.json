{"id":"2408.02936","title":"Achieving More with Less: A Tensor-Optimization-Powered Ensemble Method","authors":"Jinghui Yuan, Weijin Jiang, Zhe Cao, Fangyuan Xie, Rong Wang, Feiping\n  Nie, Yuan Yuan","authorsParsed":[["Yuan","Jinghui",""],["Jiang","Weijin",""],["Cao","Zhe",""],["Xie","Fangyuan",""],["Wang","Rong",""],["Nie","Feiping",""],["Yuan","Yuan",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 03:42:38 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 09:46:30 GMT"}],"updateDate":"2024-08-13","timestamp":1722915758000,"abstract":"  Ensemble learning is a method that leverages weak learners to produce a\nstrong learner. However, obtaining a large number of base learners requires\nsubstantial time and computational resources. Therefore, it is meaningful to\nstudy how to achieve the performance typically obtained with many base learners\nusing only a few. We argue that to achieve this, it is essential to enhance\nboth classification performance and generalization ability during the ensemble\nprocess. To increase model accuracy, each weak base learner needs to be more\nefficiently integrated. It is observed that different base learners exhibit\nvarying levels of accuracy in predicting different classes. To capitalize on\nthis, we introduce confidence tensors $\\tilde{\\mathbf{\\Theta}}$ and\n$\\tilde{\\mathbf{\\Theta}}_{rst}$ signifies the degree of confidence that the\n$t$-th base classifier assigns the sample to class $r$ while it actually\nbelongs to class $s$. To the best of our knowledge, this is the first time an\nevaluation of the performance of base classifiers across different classes has\nbeen proposed. The proposed confidence tensor compensates for the strengths and\nweaknesses of each base classifier in different classes, enabling the method to\nachieve superior results with a smaller number of base learners. To enhance\ngeneralization performance, we design a smooth and convex objective function\nthat leverages the concept of margin, making the strong learner more\ndiscriminative. Furthermore, it is proved that in gradient matrix of the loss\nfunction, the sum of each column's elements is zero, allowing us to solve a\nconstrained optimization problem using gradient-based methods. We then compare\nour algorithm with random forests of ten times the size and other classical\nmethods across numerous datasets, demonstrating the superiority of our\napproach.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}