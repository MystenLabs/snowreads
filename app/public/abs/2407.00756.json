{"id":"2407.00756","title":"Less Forgetting for Better Generalization: Exploring Continual-learning\n  Fine-tuning Methods for Speech Self-supervised Representations","authors":"Salah Zaiem, Titouan Parcollet, Slim Essid","authorsParsed":[["Zaiem","Salah",""],["Parcollet","Titouan",""],["Essid","Slim",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 16:41:33 GMT"}],"updateDate":"2024-07-02","timestamp":1719765693000,"abstract":"  Despite being trained on massive and diverse datasets, speech self-supervised\nencoders are generally used for downstream purposes as mere frozen feature\nextractors or model initializers before fine-tuning. The former severely limits\nthe exploitation of large encoders, while the latter hurts the robustness\nacquired during pretraining, especially in low-resource scenarios. This work\nexplores middle-ground solutions, conjecturing that reducing the forgetting of\nthe self-supervised task during the downstream fine-tuning leads to better\ngeneralization. To prove this, focusing on speech recognition, we benchmark\ndifferent continual-learning approaches during fine-tuning and show that they\nimprove both in-domain and out-of-domain generalization abilities. Relative\nperformance gains reach 15.7% and 22.5% with XLSR used as the encoder on two\nEnglish and Danish speech recognition tasks. Further probing experiments show\nthat these gains are indeed linked to less forgetting.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}