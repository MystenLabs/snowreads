{"id":"2407.14175","title":"On Policy Evaluation Algorithms in Distributional Reinforcement Learning","authors":"Julian Gerstenberg, Ralph Neininger, Denis Spiegel","authorsParsed":[["Gerstenberg","Julian",""],["Neininger","Ralph",""],["Spiegel","Denis",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 10:06:01 GMT"}],"updateDate":"2024-07-22","timestamp":1721383561000,"abstract":"  We introduce a novel class of algorithms to efficiently approximate the\nunknown return distributions in policy evaluation problems from distributional\nreinforcement learning (DRL). The proposed distributional dynamic programming\nalgorithms are suitable for underlying Markov decision processes (MDPs) having\nan arbitrary probabilistic reward mechanism, including continuous reward\ndistributions with unbounded support being potentially heavy-tailed.\n  For a plain instance of our proposed class of algorithms we prove error\nbounds, both within Wasserstein and Kolmogorov--Smirnov distances. Furthermore,\nfor return distributions having probability density functions the algorithms\nyield approximations for these densities; error bounds are given within\nsupremum norm. We introduce the concept of quantile-spline discretizations to\ncome up with algorithms showing promising results in simulation experiments.\n  While the performance of our algorithms can rigorously be analysed they can\nbe seen as universal black box algorithms applicable to a large class of MDPs.\nWe also derive new properties of probability metrics commonly used in DRL on\nwhich our quantitative analysis is based.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Probability"],"license":"http://creativecommons.org/licenses/by/4.0/"}