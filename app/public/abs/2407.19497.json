{"id":"2407.19497","title":"Skeleton-based Group Activity Recognition via Spatial-Temporal Panoramic\n  Graph","authors":"Zhengcen Li, Xinle Chang, Yueran Li, Jingyong Su","authorsParsed":[["Li","Zhengcen",""],["Chang","Xinle",""],["Li","Yueran",""],["Su","Jingyong",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 13:57:03 GMT"}],"updateDate":"2024-07-30","timestamp":1722175023000,"abstract":"  Group Activity Recognition aims to understand collective activities from\nvideos. Existing solutions primarily rely on the RGB modality, which encounters\nchallenges such as background variations, occlusions, motion blurs, and\nsignificant computational overhead. Meanwhile, current keypoint-based methods\noffer a lightweight and informative representation of human motions but\nnecessitate accurate individual annotations and specialized interaction\nreasoning modules. To address these limitations, we design a panoramic graph\nthat incorporates multi-person skeletons and objects to encapsulate group\nactivity, offering an effective alternative to RGB video. This panoramic graph\nenables Graph Convolutional Network (GCN) to unify intra-person, inter-person,\nand person-object interactive modeling through spatial-temporal graph\nconvolutions. In practice, we develop a novel pipeline that extracts skeleton\ncoordinates using pose estimation and tracking algorithms and employ\nMulti-person Panoramic GCN (MP-GCN) to predict group activities. Extensive\nexperiments on Volleyball and NBA datasets demonstrate that the MP-GCN achieves\nstate-of-the-art performance in both accuracy and efficiency. Notably, our\nmethod outperforms RGB-based approaches by using only estimated 2D keypoints as\ninput. Code is available at https://github.com/mgiant/MP-GCN\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}