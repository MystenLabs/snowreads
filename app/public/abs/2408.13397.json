{"id":"2408.13397","title":"Perturbation on Feature Coalition: Towards Interpretable Deep Neural\n  Networks","authors":"Xuran Hu, Mingzhe Zhu, Zhenpeng Feng, Milo\\v{s} Dakovi\\'c, Ljubi\\v{s}a\n  Stankovi\\'c","authorsParsed":[["Hu","Xuran",""],["Zhu","Mingzhe",""],["Feng","Zhenpeng",""],["Daković","Miloš",""],["Stanković","Ljubiša",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 22:44:21 GMT"}],"updateDate":"2024-08-27","timestamp":1724453061000,"abstract":"  The inherent \"black box\" nature of deep neural networks (DNNs) compromises\ntheir transparency and reliability. Recently, explainable AI (XAI) has garnered\nincreasing attention from researchers. Several perturbation-based\ninterpretations have emerged. However, these methods often fail to adequately\nconsider feature dependencies. To solve this problem, we introduce a\nperturbation-based interpretation guided by feature coalitions, which leverages\ndeep information of network to extract correlated features. Then, we proposed a\ncarefully-designed consistency loss to guide network interpretation. Both\nquantitative and qualitative experiments are conducted to validate the\neffectiveness of our proposed method. Code is available at\ngithub.com/Teriri1999/Perturebation-on-Feature-Coalition.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}