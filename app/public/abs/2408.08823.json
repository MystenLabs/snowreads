{"id":"2408.08823","title":"Optimal Symmetries in Binary Classification","authors":"Vishal S. Ngairangbam, and Michael Spannowsky","authorsParsed":[["Ngairangbam","Vishal S.",""],["Spannowsky","Michael",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 16:15:18 GMT"}],"updateDate":"2024-08-19","timestamp":1723824918000,"abstract":"  We explore the role of group symmetries in binary classification tasks,\npresenting a novel framework that leverages the principles of Neyman-Pearson\noptimality. Contrary to the common intuition that larger symmetry groups lead\nto improved classification performance, our findings show that selecting the\nappropriate group symmetries is crucial for optimising generalisation and\nsample efficiency. We develop a theoretical foundation for designing group\nequivariant neural networks that align the choice of symmetries with the\nunderlying probability distributions of the data. Our approach provides a\nunified methodology for improving classification accuracy across a broad range\nof applications by carefully tailoring the symmetry group to the specific\ncharacteristics of the problem. Theoretical analysis and experimental results\ndemonstrate that optimal classification performance is not always associated\nwith the largest equivariant groups possible in the domain, even when the\nlikelihood ratio is invariant under one of its proper subgroups, but rather\nwith those subgroups themselves. This work offers insights and practical\nguidelines for constructing more effective group equivariant architectures in\ndiverse machine-learning contexts.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Physics/Data Analysis, Statistics and Probability","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}