{"id":"2408.13359","title":"Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate\n  Scheduler","authors":"Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn\n  Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, Rameswar Panda","authorsParsed":[["Shen","Yikang",""],["Stallone","Matthew",""],["Mishra","Mayank",""],["Zhang","Gaoyuan",""],["Tan","Shawn",""],["Prasad","Aditya",""],["Soria","Adriana Meza",""],["Cox","David D.",""],["Panda","Rameswar",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 20:22:20 GMT"},{"version":"v2","created":"Wed, 11 Sep 2024 20:48:05 GMT"}],"updateDate":"2024-09-13","timestamp":1724444540000,"abstract":"  Finding the optimal learning rate for language model pretraining is a\nchallenging task. This is not only because there is a complicated correlation\nbetween learning rate, batch size, number of training tokens, model size, and\nother hyperparameters but also because it is prohibitively expensive to perform\na hyperparameter search for large language models with Billions or Trillions of\nparameters. Recent studies propose using small proxy models and small corpus to\nperform hyperparameter searches and transposing the optimal parameters to large\nmodels and large corpus. While the zero-shot transferability is theoretically\nand empirically proven for model size related hyperparameters, like depth and\nwidth, the zero-shot transfer from small corpus to large corpus is\nunderexplored. In this paper, we study the correlation between optimal learning\nrate, batch size, and number of training tokens for the recently proposed WSD\nscheduler. After thousands of small experiments, we found a power-law\nrelationship between variables and demonstrated its transferability across\nmodel sizes. Based on the observation, we propose a new learning rate\nscheduler, Power scheduler, that is agnostic about the number of training\ntokens and batch size. The experiment shows that combining the Power scheduler\nwith Maximum Update Parameterization (muP) can consistently achieve impressive\nperformance with one set of hyperparameters regardless of the number of\ntraining tokens, batch size, model size, and even model architecture. Our 3B\ndense and MoE models trained with the Power scheduler achieve comparable\nperformance as state-of-the-art small language models. We open-source these\npretrained models at https://ibm.biz/BdKhLa.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}