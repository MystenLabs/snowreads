{"id":"2407.05417","title":"See Further for Parameter Efficient Fine-tuning by Standing on the\n  Shoulders of Decomposition","authors":"Chongjie Si, Xiaokang Yang, Wei Shen","authorsParsed":[["Si","Chongjie",""],["Yang","Xiaokang",""],["Shen","Wei",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 15:44:42 GMT"}],"updateDate":"2024-07-09","timestamp":1720367082000,"abstract":"  The rapid expansion of large foundation models within the pre-training and\nfine-tuning framework has underscored that larger models often yield better\nresults. However, the scaling up of large foundation models has led to soaring\ncosts in fine-tuning and parameter storage, rendering extensive adaptations\nimpractical. This challenge has sparked the development of parameter-efficient\nfine-tuning (PEFT), which focuses on optimizing a select subset of parameters\nwhile keeping the rest fixed, significantly lowering computational and storage\noverheads. While recent years have witnessed a significant success in PEFT, a\ndeep understanding of the fundamental principles behind these methods remains\nunexplored. To this end, here we take the first step to unify all approaches by\ndissecting them from a decomposition perspective. We initiate a comprehensive\nmathematical analysis of these methods, allowing us to delve deeply into their\nunderlying mechanisms, and we explore the reasons behind the variations in\nperformance among different techniques. Furthermore, inspired by our\ntheoretical analysis, we introduce two novel PEFT methods alongside a simple\nyet effective framework designed to enhance the performance of PEFT techniques\nacross various applications. Our empirical validations, conducted across\nmultiple datasets, demonstrate the efficacy of these methods, showcasing both\ntheoretical validity and practical performance improvements under the guidance\nof our analytical findings. We believe our work will deepen researchers'\nunderstanding of PEFT and other techniques, prompting further contemplation and\nadvancing the research across the whole community.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}