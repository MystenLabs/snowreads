{"id":"2408.13705","title":"Cross-Modal Denoising: A Novel Training Paradigm for Enhancing\n  Speech-Image Retrieval","authors":"Lifeng Zhou, Yuke Li, Rui Deng, Yuting Yang, Haoqi Zhu","authorsParsed":[["Zhou","Lifeng",""],["Li","Yuke",""],["Deng","Rui",""],["Yang","Yuting",""],["Zhu","Haoqi",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 02:42:05 GMT"},{"version":"v2","created":"Wed, 11 Sep 2024 07:22:58 GMT"}],"updateDate":"2024-09-12","timestamp":1723689725000,"abstract":"  The success of speech-image retrieval relies on establishing an effective\nalignment between speech and image. Existing methods often model cross-modal\ninteraction through simple cosine similarity of the global feature of each\nmodality, which fall short in capturing fine-grained details within modalities.\nTo address this issue, we introduce an effective framework and a novel learning\ntask named cross-modal denoising (CMD) to enhance cross-modal interaction to\nachieve finer-level cross-modal alignment. Specifically, CMD is a denoising\ntask designed to reconstruct semantic features from noisy features within one\nmodality by interacting features from another modality. Notably, CMD operates\nexclusively during model training and can be removed during inference without\nadding extra inference time. The experimental results demonstrate that our\nframework outperforms the state-of-the-art method by 2.0% in mean R@1 on the\nFlickr8k dataset and by 1.7% in mean R@1 on the SpokenCOCO dataset for the\nspeech-image retrieval tasks, respectively. These experimental results validate\nthe efficiency and effectiveness of our framework.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}