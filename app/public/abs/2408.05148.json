{"id":"2408.05148","title":"Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications","authors":"Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Oscar\n  Hernandez, Mark Coletti and Ada Sedova","authorsParsed":[["Shanmugavelu","Sanjif",""],["Taillefumier","Mathieu",""],["Culver","Christopher",""],["Hernandez","Oscar",""],["Coletti","Mark",""],["Sedova","Ada",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 16:07:37 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 17:40:15 GMT"}],"updateDate":"2024-08-26","timestamp":1723219657000,"abstract":"  Run-by-run variability in parallel programs caused by floating-point\nnon-associativity (FPNA) has been known to significantly affect reproducibility\nin iterative algorithms, due to accumulating errors. Non-reproducibility\nnegatively affects efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning (DL) training\nand inference pipelines to FPNA have been found to be extreme, and can prevent\ncertification for commercial applications, accurate assessment of robustness\nand sensitivity, and bug detection. New approaches in scientific computing\napplications have coupled DL models with high-performance computing (HPC)\nsimulations, leading to an aggravation of debugging and testing challenges.\nHere we perform an investigation of the statistical properties of FPNA within\nmodern parallel programming models, analyze performance and productivity\nimpacts of replacing atomic operations with deterministic alternatives on GPUs,\nand examine the recently-added deterministic options within the PyTorch\nframework within the context of GPU deployment, uncovering and quantifying the\nimpacts of input parameters triggering run-by-run variability and reporting on\nthe reliability and completeness of the documentation. Finally, we evaluate the\nstrategy of exploiting automatic determinism provided by deterministic\nhardware, using the Groq LPU$^{TM}$ accelerator for inference portions of the\nDL pipeline. We demonstrate the benefits that this strategy can provide within\nreproducibility and correctness efforts.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}