{"id":"2408.17354","title":"Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language\n  Models for Privacy Leakage","authors":"Md Rafi Ur Rashid, Jing Liu, Toshiaki Koike-Akino, Shagufta Mehnaz, Ye\n  Wang","authorsParsed":[["Rashid","Md Rafi Ur",""],["Liu","Jing",""],["Koike-Akino","Toshiaki",""],["Mehnaz","Shagufta",""],["Wang","Ye",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 15:35:09 GMT"}],"updateDate":"2024-09-02","timestamp":1725032109000,"abstract":"  Fine-tuning large language models on private data for downstream applications\nposes significant privacy risks in potentially exposing sensitive information.\nSeveral popular community platforms now offer convenient distribution of a\nlarge variety of pre-trained models, allowing anyone to publish without\nrigorous verification. This scenario creates a privacy threat, as pre-trained\nmodels can be intentionally crafted to compromise the privacy of fine-tuning\ndatasets. In this study, we introduce a novel poisoning technique that uses\nmodel-unlearning as an attack tool. This approach manipulates a pre-trained\nlanguage model to increase the leakage of private data during the fine-tuning\nprocess. Our method enhances both membership inference and data extraction\nattacks while preserving model utility. Experimental results across different\nmodels, datasets, and fine-tuning setups demonstrate that our attacks\nsignificantly surpass baseline performance. This work serves as a cautionary\nnote for users who download pre-trained models from unverified sources,\nhighlighting the potential risks involved.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}