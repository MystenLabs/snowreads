{"id":"2408.10996","title":"Approximation Rates for Shallow ReLU$^k$ Neural Networks on Sobolev\n  Spaces via the Radon Transform","authors":"Tong Mao, Jonathan W. Siegel, Jinchao Xu","authorsParsed":[["Mao","Tong",""],["Siegel","Jonathan W.",""],["Xu","Jinchao",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 16:43:45 GMT"}],"updateDate":"2024-08-21","timestamp":1724172225000,"abstract":"  Let $\\Omega\\subset \\mathbb{R}^d$ be a bounded domain. We consider the problem\nof how efficiently shallow neural networks with the ReLU$^k$ activation\nfunction can approximate functions from Sobolev spaces $W^s(L_p(\\Omega))$ with\nerror measured in the $L_q(\\Omega)$-norm. Utilizing the Radon transform and\nrecent results from discrepancy theory, we provide a simple proof of nearly\noptimal approximation rates in a variety of cases, including when $q\\leq p$,\n$p\\geq 2$, and $s \\leq k + (d+1)/2$. The rates we derive are optimal up to\nlogarithmic factors, and significantly generalize existing results. An\ninteresting consequence is that the adaptivity of shallow ReLU$^k$ neural\nnetworks enables them to obtain optimal approximation rates for smoothness up\nto order $s = k + (d+1)/2$, even though they represent piecewise polynomials of\nfixed degree $k$.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}