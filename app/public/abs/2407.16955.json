{"id":"2407.16955","title":"DVPE: Divided View Position Embedding for Multi-View 3D Object Detection","authors":"Jiasen Wang, Zhenglin Li, Ke Sun, Xianyuan Liu and Yang Zhou","authorsParsed":[["Wang","Jiasen",""],["Li","Zhenglin",""],["Sun","Ke",""],["Liu","Xianyuan",""],["Zhou","Yang",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 02:44:41 GMT"}],"updateDate":"2024-07-25","timestamp":1721789081000,"abstract":"  Sparse query-based paradigms have achieved significant success in multi-view\n3D detection for autonomous vehicles. Current research faces challenges in\nbalancing between enlarging receptive fields and reducing interference when\naggregating multi-view features. Moreover, different poses of cameras present\nchallenges in training global attention models. To address these problems, this\npaper proposes a divided view method, in which features are modeled globally\nvia the visibility crossattention mechanism, but interact only with partial\nfeatures in a divided local virtual space. This effectively reduces\ninterference from other irrelevant features and alleviates the training\ndifficulties of the transformer by decoupling the position embedding from\ncamera poses. Additionally, 2D historical RoI features are incorporated into\nthe object-centric temporal modeling to utilize highlevel visual semantic\ninformation. The model is trained using a one-to-many assignment strategy to\nfacilitate stability. Our framework, named DVPE, achieves state-of-the-art\nperformance (57.2% mAP and 64.5% NDS) on the nuScenes test set. Codes will be\navailable at https://github.com/dop0/DVPE.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}