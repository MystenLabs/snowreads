{"id":"2408.01839","title":"Complexity of Minimizing Projected-Gradient-Dominated Functions with\n  Stochastic First-order Oracles","authors":"Saeed Masiha, Saber Salehkaleybar, Niao He, Negar Kiyavash, and\n  Patrick Thiran","authorsParsed":[["Masiha","Saeed",""],["Salehkaleybar","Saber",""],["He","Niao",""],["Kiyavash","Negar",""],["Thiran","Patrick",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 18:34:23 GMT"}],"updateDate":"2024-08-06","timestamp":1722710063000,"abstract":"  This work investigates the performance limits of projected stochastic\nfirst-order methods for minimizing functions under the\n$(\\alpha,\\tau,\\mathcal{X})$-projected-gradient-dominance property, that asserts\nthe sub-optimality gap $F(\\mathbf{x})-\\min_{\\mathbf{x}'\\in\n\\mathcal{X}}F(\\mathbf{x}')$ is upper-bounded by\n$\\tau\\cdot\\|\\mathcal{G}_{\\eta,\\mathcal{X}}(\\mathbf{x})\\|^{\\alpha}$ for some\n$\\alpha\\in[1,2)$ and $\\tau>0$ and $\\mathcal{G}_{\\eta,\\mathcal{X}}(\\mathbf{x})$\nis the projected-gradient mapping with $\\eta>0$ as a parameter. For non-convex\nfunctions, we show that the complexity lower bound of querying a batch smooth\nfirst-order stochastic oracle to obtain an $\\epsilon$-global-optimum point is\n$\\Omega(\\epsilon^{-{2}/{\\alpha}})$. Furthermore, we show that a projected\nvariance-reduced first-order algorithm can obtain the upper complexity bound of\n$\\mathcal{O}(\\epsilon^{-{2}/{\\alpha}})$, matching the lower bound. For convex\nfunctions, we establish a complexity lower bound of\n$\\Omega(\\log(1/\\epsilon)\\cdot\\epsilon^{-{2}/{\\alpha}})$ for minimizing\nfunctions under a local version of gradient-dominance property, which also\nmatches the upper complexity bound of accelerated stochastic subgradient\nmethods.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZnFfz8QQTuS1NQXa2FMHNj-StgFXY1o3ZWYa81Gh4YI","pdfSize":"498964"}
