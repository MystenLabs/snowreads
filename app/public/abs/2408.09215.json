{"id":"2408.09215","title":"Generating Data with Text-to-Speech and Large-Language Models for\n  Conversational Speech Recognition","authors":"Samuele Cornell and Jordan Darefsky and Zhiyao Duan and Shinji\n  Watanabe","authorsParsed":[["Cornell","Samuele",""],["Darefsky","Jordan",""],["Duan","Zhiyao",""],["Watanabe","Shinji",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 14:47:05 GMT"}],"updateDate":"2024-08-20","timestamp":1723906025000,"abstract":"  Currently, a common approach in many speech processing tasks is to leverage\nlarge scale pre-trained models by fine-tuning them on in-domain data for a\nparticular application. Yet obtaining even a small amount of such data can be\nproblematic, especially for sensitive domains and conversational speech\nscenarios, due to both privacy issues and annotation costs. To address this,\nsynthetic data generation using single speaker datasets has been employed. Yet,\nfor multi-speaker cases, such an approach often requires extensive manual\neffort and is prone to domain mismatches. In this work, we propose a synthetic\ndata generation pipeline for multi-speaker conversational ASR, leveraging a\nlarge language model (LLM) for content creation and a conversational\nmulti-speaker text-to-speech (TTS) model for speech synthesis. We conduct\nevaluation by fine-tuning the Whisper ASR model for telephone and distant\nconversational speech settings, using both in-domain data and generated\nsynthetic data. Our results show that the proposed method is able to\nsignificantly outperform classical multi-speaker generation approaches that use\nexternal, non-conversational speech datasets.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}