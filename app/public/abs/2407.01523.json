{"id":"2407.01523","title":"MMLongBench-Doc: Benchmarking Long-context Document Understanding with\n  Visualizations","authors":"Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li,\n  Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang\n  Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun","authorsParsed":[["Ma","Yubo",""],["Zang","Yuhang",""],["Chen","Liangyu",""],["Chen","Meiqi",""],["Jiao","Yizhu",""],["Li","Xinze",""],["Lu","Xinyuan",""],["Liu","Ziyu",""],["Ma","Yan",""],["Dong","Xiaoyi",""],["Zhang","Pan",""],["Pan","Liangming",""],["Jiang","Yu-Gang",""],["Wang","Jiaqi",""],["Cao","Yixin",""],["Sun","Aixin",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:59:26 GMT"},{"version":"v2","created":"Wed, 10 Jul 2024 15:31:09 GMT"}],"updateDate":"2024-07-11","timestamp":1719856766000,"abstract":"  Understanding documents with rich layouts and multi-modal components is a\nlong-standing and practical task. Recent Large Vision-Language Models (LVLMs)\nhave made remarkable strides in various tasks, particularly in single-page\ndocument understanding (DU). However, their abilities on long-context DU remain\nan open problem. This work presents MMLongBench-Doc, a long-context,\nmulti-modal benchmark comprising 1,062 expert-annotated questions. Distinct\nfrom previous datasets, it is constructed upon 130 lengthy PDF-formatted\ndocuments with an average of 49.4 pages and 20,971 textual tokens. Towards\ncomprehensive evaluation, answers to these questions rely on pieces of evidence\nfrom (1) different sources (text, image, chart, table, and layout structure)\nand (2) various locations (i.e. page number). Moreover, 33.2% of the questions\nare cross-page questions requiring evidence across multiple pages. 22.8% of the\nquestions are designed to be unanswerable for detecting potential\nhallucinations. Experiments on 14 LVLMs demonstrate that long-context DU\ngreatly challenges current models. Notably, the best-performing model, GPT-4o,\nachieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores\n31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse\nperformance than their LLM counterparts which are fed with lossy-parsed OCR\ndocuments. These results validate the necessity of future research toward more\ncapable long-context LVLMs. Project Page:\nhttps://mayubo2333.github.io/MMLongBench-Doc\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"Ad3QwGdDlIelW3ME9AHvsHMQgs4W73obHYumGaPNUFc","pdfSize":"23874166"}
