{"id":"2408.03408","title":"LLM-Aided Compilation for Tensor Accelerators","authors":"Charles Hong, Sahil Bhatia, Altan Haan, Shengjun Kris Dong, Dima\n  Nikiforov, Alvin Cheung, Yakun Sophia Shao","authorsParsed":[["Hong","Charles",""],["Bhatia","Sahil",""],["Haan","Altan",""],["Dong","Shengjun Kris",""],["Nikiforov","Dima",""],["Cheung","Alvin",""],["Shao","Yakun Sophia",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 19:10:25 GMT"}],"updateDate":"2024-08-08","timestamp":1722971425000,"abstract":"  Hardware accelerators, in particular accelerators for tensor processing, have\nmany potential application domains. However, they currently lack the software\ninfrastructure to support the majority of domains outside of deep learning.\nFurthermore, a compiler that can easily be updated to reflect changes at both\napplication and hardware levels would enable more agile development and design\nspace exploration of accelerators, allowing hardware designers to realize\ncloser-to-optimal performance. In this work, we discuss how large language\nmodels (LLMs) could be leveraged to build such a compiler. Specifically, we\ndemonstrate the ability of GPT-4 to achieve high pass rates in translating code\nto the Gemmini accelerator, and prototype a technique for decomposing\ntranslation into smaller, more LLM-friendly steps. Additionally, we propose a\n2-phase workflow for utilizing LLMs to generate hardware-optimized code.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Machine Learning","Computing Research Repository/Programming Languages"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mVlP0g0K3Mql4TYq7005Y60brY4JgIO1zadcR0WTKcw","pdfSize":"327333"}
