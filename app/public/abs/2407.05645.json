{"id":"2407.05645","title":"OneDiff: A Generalist Model for Image Difference Captioning","authors":"Erdong Hu, Longteng Guo, Tongtian Yue, Zijia Zhao, Shuning Xue and\n  Jing Liu","authorsParsed":[["Hu","Erdong",""],["Guo","Longteng",""],["Yue","Tongtian",""],["Zhao","Zijia",""],["Xue","Shuning",""],["Liu","Jing",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 06:14:37 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 07:50:44 GMT"}],"updateDate":"2024-07-17","timestamp":1720419277000,"abstract":"  In computer vision, Image Difference Captioning (IDC) is crucial for\naccurately describing variations between closely related images. Traditional\nIDC methods often rely on specialist models, which restrict their applicability\nacross varied contexts. This paper introduces the OneDiff model, a novel\ngeneralist approach that utilizes a robust vision-language model architecture,\nintegrating a siamese image encoder with a Visual Delta Module. This innovative\nconfiguration allows for the precise detection and articulation of fine-grained\ndifferences between image pairs. OneDiff is trained through a dual-phase\nstrategy, encompassing Coupled Sample Training and multi-task learning across a\ndiverse array of data types, supported by our newly developed DiffCap Dataset.\nThis dataset merges real-world and synthetic data, enhancing the training\nprocess and bolstering the model's robustness. Extensive testing on diverse IDC\nbenchmarks, such as Spot-the-Diff, CLEVR-Change, and Birds-to-Words, shows that\nOneDiff consistently outperforms existing state-of-the-art models in accuracy\nand adaptability, achieving improvements of up to 85\\% CIDEr points in average.\nBy setting a new benchmark in IDC, OneDiff paves the way for more versatile and\neffective applications in detecting and describing visual differences. The\ncode, models, and data will be made publicly available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}