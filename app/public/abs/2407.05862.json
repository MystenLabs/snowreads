{"id":"2407.05862","title":"Bringing Masked Autoencoders Explicit Contrastive Properties for Point\n  Cloud Self-Supervised Learning","authors":"Bin Ren, Guofeng Mei, Danda Pani Paudel, Weijie Wang, Yawei Li,\n  Mengyuan Liu, Rita Cucchiara, Luc Van Gool, Nicu Sebe","authorsParsed":[["Ren","Bin",""],["Mei","Guofeng",""],["Paudel","Danda Pani",""],["Wang","Weijie",""],["Li","Yawei",""],["Liu","Mengyuan",""],["Cucchiara","Rita",""],["Van Gool","Luc",""],["Sebe","Nicu",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 12:28:56 GMT"}],"updateDate":"2024-07-09","timestamp":1720441736000,"abstract":"  Contrastive learning (CL) for Vision Transformers (ViTs) in image domains has\nachieved performance comparable to CL for traditional convolutional backbones.\nHowever, in 3D point cloud pretraining with ViTs, masked autoencoder (MAE)\nmodeling remains dominant. This raises the question: Can we take the best of\nboth worlds? To answer this question, we first empirically validate that\nintegrating MAE-based point cloud pre-training with the standard contrastive\nlearning paradigm, even with meticulous design, can lead to a decrease in\nperformance. To address this limitation, we reintroduce CL into the MAE-based\npoint cloud pre-training paradigm by leveraging the inherent contrastive\nproperties of MAE. Specifically, rather than relying on extensive data\naugmentation as commonly used in the image domain, we randomly mask the input\ntokens twice to generate contrastive input pairs. Subsequently, a\nweight-sharing encoder and two identically structured decoders are utilized to\nperform masked token reconstruction. Additionally, we propose that for an input\ntoken masked by both masks simultaneously, the reconstructed features should be\nas similar as possible. This naturally establishes an explicit contrastive\nconstraint within the generative MAE-based pre-training paradigm, resulting in\nour proposed method, Point-CMAE. Consequently, Point-CMAE effectively enhances\nthe representation quality and transfer performance compared to its MAE\ncounterpart. Experimental evaluations across various downstream applications,\nincluding classification, part segmentation, and few-shot learning, demonstrate\nthe efficacy of our framework in surpassing state-of-the-art techniques under\nstandard ViTs and single-modal settings. The source code and trained models are\navailable at: https://github.com/Amazingren/Point-CMAE.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}