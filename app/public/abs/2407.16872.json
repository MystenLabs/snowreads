{"id":"2407.16872","title":"How Can Deep Neural Networks Fail Even With Global Optima?","authors":"Qingguang Guan","authorsParsed":[["Guan","Qingguang",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 22:44:57 GMT"}],"updateDate":"2024-07-25","timestamp":1721774697000,"abstract":"  Fully connected deep neural networks are successfully applied to\nclassification and function approximation problems. By minimizing the cost\nfunction, i.e., finding the proper weights and biases, models can be built for\naccurate predictions. The ideal optimization process can achieve global optima.\nHowever, do global optima always perform well? If not, how bad can it be? In\nthis work, we aim to: 1) extend the expressive power of shallow neural networks\nto networks of any depth using a simple trick, 2) construct extremely\noverfitting deep neural networks that, despite having global optima, still fail\nto perform well on classification and function approximation problems.\nDifferent types of activation functions are considered, including ReLU,\nParametric ReLU, and Sigmoid functions. Extensive theoretical analysis has been\nconducted, ranging from one-dimensional models to models of any dimensionality.\nNumerical results illustrate our theoretical findings.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}