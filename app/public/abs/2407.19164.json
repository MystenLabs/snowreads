{"id":"2407.19164","title":"Addressing Topic Leakage in Cross-Topic Evaluation for Authorship\n  Verification","authors":"Jitkapat Sawatphol, Can Udomcharoenchaikit, Sarana Nutanong","authorsParsed":[["Sawatphol","Jitkapat",""],["Udomcharoenchaikit","Can",""],["Nutanong","Sarana",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 04:16:11 GMT"}],"updateDate":"2024-07-30","timestamp":1722053771000,"abstract":"  Authorship verification (AV) aims to identify whether a pair of texts has the\nsame author. We address the challenge of evaluating AV models' robustness\nagainst topic shifts. The conventional evaluation assumes minimal topic overlap\nbetween training and test data. However, we argue that there can still be topic\nleakage in test data, causing misleading model performance and unstable\nrankings. To address this, we propose an evaluation method called\nHeterogeneity-Informed Topic Sampling (HITS), which creates a smaller dataset\nwith a heterogeneously distributed topic set. Our experimental results\ndemonstrate that HITS-sampled datasets yield a more stable ranking of models\nacross random seeds and evaluation splits. Our contributions include: 1. An\nanalysis of causes and effects of topic leakage. 2. A demonstration of the HITS\nin reducing the effects of topic leakage, and 3. The Robust Authorship\nVerification bENchmark (RAVEN) that allows topic shortcut test to uncover AV\nmodels' reliance on topic-specific features.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Ap1dBomegxazKBTaXWe0b9BpIDUZXgCBgQPs55BEqTc","pdfSize":"524869"}
