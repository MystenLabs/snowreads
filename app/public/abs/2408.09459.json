{"id":"2408.09459","title":"WPN: An Unlearning Method Based on N-pair Contrastive Learning in\n  Language Models","authors":"Guitao Chen, Yunshen Wang, Hongye Sun, Guang Chen","authorsParsed":[["Chen","Guitao",""],["Wang","Yunshen",""],["Sun","Hongye",""],["Chen","Guang",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 12:37:03 GMT"}],"updateDate":"2024-08-20","timestamp":1723984623000,"abstract":"  Generative language models (LMs) offer numerous advantages but may produce\ninappropriate or harmful outputs due to the harmful knowledge acquired during\npre-training. This knowledge often manifests as undesirable correspondences,\nsuch as \"harmful prompts\" leading to \"harmful outputs,\" which our research aims\nto mitigate through unlearning techniques.However, existing unlearning methods\nbased on gradient ascent can significantly impair the performance of LMs. To\naddress this issue, we propose a novel approach called Weighted Positional\nN-pair (WPN) Learning, which leverages position-weighted mean pooling within an\nn-pair contrastive learning framework. WPN is designed to modify the output\ndistribution of LMs by eliminating specific harmful outputs (e.g., replacing\ntoxic responses with neutral ones), thereby transforming the model's behavior\nfrom \"harmful prompt-harmful output\" to \"harmful prompt-harmless\nresponse\".Experiments on OPT and GPT-NEO LMs show that WPN effectively reduces\nthe proportion of harmful responses, achieving a harmless rate of up to 95.8\\%\nwhile maintaining stable performance on nine common benchmarks (with less than\n2\\% degradation on average). Moreover, we provide empirical evidence to\ndemonstrate WPN's ability to weaken the harmful correspondences in terms of\ngeneralizability and robustness, as evaluated on out-of-distribution test sets\nand under adversarial attacks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}