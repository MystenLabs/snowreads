{"id":"2407.09985","title":"A Training Data Recipe to Accelerate A* Search with Language Models","authors":"Devaansh Gupta, Boyang Li","authorsParsed":[["Gupta","Devaansh",""],["Li","Boyang",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 19:21:44 GMT"}],"updateDate":"2024-07-16","timestamp":1720898504000,"abstract":"  Recent works in AI planning have proposed to combine LLMs with iterative\ntree-search algorithms like A* and MCTS, where LLMs are typically used to\ncalculate the heuristic, guiding the planner towards the goal. However,\ncombining these techniques is not trivial : LM-based heuristics are quite weak,\nincurring a high computational cost without a significant performance\nimprovement. Existing methods to learn these heuristics do not consider the\nrequirements of the planner, and typically need a lot of compute. Thus, in this\nwork, we propose a distribution to downsample training data by identifying\nrelevant data points to learn a performant heuristic, while constraining\ncomputational costs. To arrive at this model, we disentangle the requirements\nof the planner, in our case A* search, from that of the language model to\ngeneralise on this task. Surprisingly, we find an overlap between their\nrequirements; A* requires more accurate predictions on nodes near the goal, and\nLMs need the same set of nodes for effective generalisation. With these\ninsights, we can quantify the contribution of each node towards accelerating A*\nsearch, and subsequently derive a training distribution for learning LM-based\nheuristics. Following a recent work, we conduct our experiments on two\nclassical planning domains, maze navigation and sokoban, with two test splits\nper domain, and two conventional loss functions. We reduce the number of\niterations required to find the solutions by upto 13x, with a wall-clock\nspeed-up of upto 5x.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}