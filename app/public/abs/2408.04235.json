{"id":"2408.04235","title":"LLDif: Diffusion Models for Low-light Emotion Recognition","authors":"Zhifeng Wang and Kaihao Zhang and Ramesh Sankaranarayana","authorsParsed":[["Wang","Zhifeng",""],["Zhang","Kaihao",""],["Sankaranarayana","Ramesh",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 05:41:09 GMT"}],"updateDate":"2024-08-09","timestamp":1723095669000,"abstract":"  This paper introduces LLDif, a novel diffusion-based facial expression\nrecognition (FER) framework tailored for extremely low-light (LL) environments.\nImages captured under such conditions often suffer from low brightness and\nsignificantly reduced contrast, presenting challenges to conventional methods.\nThese challenges include poor image quality that can significantly reduce the\naccuracy of emotion recognition. LLDif addresses these issues with a novel\ntwo-stage training process that combines a Label-aware CLIP (LA-CLIP), an\nembedding prior network (PNET), and a transformer-based network adept at\nhandling the noise of low-light images. The first stage involves LA-CLIP\ngenerating a joint embedding prior distribution (EPD) to guide the LLformer in\nlabel recovery. In the second stage, the diffusion model (DM) refines the EPD\ninference, ultilising the compactness of EPD for precise predictions.\nExperimental evaluations on various LL-FER datasets have shown that LLDif\nachieves competitive performance, underscoring its potential to enhance FER\napplications in challenging lighting conditions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Jk4d7i3-dUtvvOTjubNC3dsNuzbk24YPjHdcUkYUMN8","pdfSize":"1931938"}
