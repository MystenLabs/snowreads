{"id":"2407.01976","title":"A Bounding Box is Worth One Token: Interleaving Layout and Text in a\n  Large Language Model for Document Understanding","authors":"Jinghui Lu, Haiyang Yu, Yanjie Wang, Yongjie Ye, Jingqun Tang, Ziwei\n  Yang, Binghong Wu, Qi Liu, Hao Feng, Han Wang, Hao Liu, Can Huang","authorsParsed":[["Lu","Jinghui",""],["Yu","Haiyang",""],["Wang","Yanjie",""],["Ye","Yongjie",""],["Tang","Jingqun",""],["Yang","Ziwei",""],["Wu","Binghong",""],["Liu","Qi",""],["Feng","Hao",""],["Wang","Han",""],["Liu","Hao",""],["Huang","Can",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 06:29:05 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 11:45:48 GMT"}],"updateDate":"2024-07-25","timestamp":1719901745000,"abstract":"  Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. In particular,\nLayTextLLM projects each bounding box to a single embedding and interleaves it\nwith text, efficiently avoiding long sequence issues while leveraging\nautoregressive traits of LLMs. LayTextLLM not only streamlines the interaction\nof layout and textual data but also shows enhanced performance in Key\nInformation Extraction (KIE) and Visual Question Answering (VQA). Comprehensive\nbenchmark evaluations reveal significant improvements, with a 27.2% increase on\nKIE tasks and 12.0% on VQA tasks compared to previous state-of-the-art document\nunderstanding MLLMs, as well as a 15.1% improvement over other SOTA OCR-based\nLLMs on KIE tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"hEngbB7scEiIt_KiPGFm0KdQVO0ffGRGAykW1mwfVsY","pdfSize":"4505654","objectId":"0x79775680b22bcfb83a2edadc34e07c59bcfef681791c8b81540dda189e4864d1","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
