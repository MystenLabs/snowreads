{"id":"2408.03001","title":"Multitask and Multimodal Neural Tuning for Large Models","authors":"Hao Sun, Yu Song, Jihong Hu, Yen-Wei Chen, and Lanfen Lin","authorsParsed":[["Sun","Hao",""],["Song","Yu",""],["Hu","Jihong",""],["Chen","Yen-Wei",""],["Lin","Lanfen",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 07:19:51 GMT"}],"updateDate":"2024-08-07","timestamp":1722928791000,"abstract":"  In recent years, large-scale multimodal models have demonstrated impressive\ncapabilities across various domains. However, enabling these models to\neffectively perform multiple multimodal tasks simultaneously remains a\nsignificant challenge. To address this, we introduce a novel tuning method\ncalled neural tuning, designed to handle diverse multimodal tasks concurrently,\nincluding reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. Neural tuning emulates sparse distributed\nrepresentation in human brain, where only specific subsets of neurons are\nactivated for each task. Additionally, we present a new benchmark, MMUD, where\neach sample is annotated with multiple task labels. By applying neural tuning\nto pretrained large models on the MMUD benchmark, we achieve simultaneous task\nhandling in a streamlined and efficient manner. All models, code, and datasets\nwill be publicly available after publication, facilitating further research and\ndevelopment in this field.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/"}