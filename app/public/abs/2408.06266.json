{"id":"2408.06266","title":"Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment","authors":"Karel D'Oosterlinck, Winnie Xu, Chris Develder, Thomas Demeester,\n  Amanpreet Singh, Christopher Potts, Douwe Kiela, and Shikib Mehri","authorsParsed":[["D'Oosterlinck","Karel",""],["Xu","Winnie",""],["Develder","Chris",""],["Demeester","Thomas",""],["Singh","Amanpreet",""],["Potts","Christopher",""],["Kiela","Douwe",""],["Mehri","Shikib",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 16:24:51 GMT"},{"version":"v2","created":"Sat, 24 Aug 2024 03:19:13 GMT"},{"version":"v3","created":"Thu, 29 Aug 2024 20:26:19 GMT"},{"version":"v4","created":"Wed, 4 Sep 2024 00:22:45 GMT"},{"version":"v5","created":"Sat, 14 Sep 2024 23:09:07 GMT"}],"updateDate":"2024-09-17","timestamp":1723479891000,"abstract":"  Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}