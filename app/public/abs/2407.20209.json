{"id":"2407.20209","title":"Characterizing Dynamical Stability of Stochastic Gradient Descent in\n  Overparameterized Learning","authors":"Dennis Chemnitz, Maximilian Engel","authorsParsed":[["Chemnitz","Dennis",""],["Engel","Maximilian",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 17:40:04 GMT"},{"version":"v2","created":"Wed, 18 Sep 2024 17:44:48 GMT"}],"updateDate":"2024-09-19","timestamp":1722274804000,"abstract":"  For overparameterized optimization tasks, such as the ones found in modern\nmachine learning, global minima are generally not unique. In order to\nunderstand generalization in these settings, it is vital to study to which\nminimum an optimization algorithm converges. The possibility of having minima\nthat are unstable under the dynamics imposed by the optimization algorithm\nlimits the potential minima that the algorithm can find. In this paper, we\ncharacterize the global minima that are dynamically stable/unstable for both\ndeterministic and stochastic gradient descent (SGD). In particular, we\nintroduce a characteristic Lyapunov exponent which depends on the local\ndynamics around a global minimum and rigorously prove that the sign of this\nLyapunov exponent determines whether SGD can accumulate at the respective\nglobal minimum.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Dynamical Systems","Mathematics/Probability"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}