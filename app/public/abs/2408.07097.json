{"id":"2408.07097","title":"Attention Please: What Transformer Models Really Learn for Process\n  Prediction","authors":"Martin K\\\"appel, Lars Ackermann, Stefan Jablonski, Simon H\\\"artl","authorsParsed":[["Käppel","Martin",""],["Ackermann","Lars",""],["Jablonski","Stefan",""],["Härtl","Simon",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 08:20:38 GMT"}],"updateDate":"2024-08-15","timestamp":1723450838000,"abstract":"  Predictive process monitoring aims to support the execution of a process\nduring runtime with various predictions about the further evolution of a\nprocess instance. In the last years a plethora of deep learning architectures\nhave been established as state-of-the-art for different prediction targets,\namong others the transformer architecture. The transformer architecture is\nequipped with a powerful attention mechanism, assigning attention scores to\neach input part that allows to prioritize most relevant information leading to\nmore accurate and contextual output. However, deep learning models largely\nrepresent a black box, i.e., their reasoning or decision-making process cannot\nbe understood in detail. This paper examines whether the attention scores of a\ntransformer based next-activity prediction model can serve as an explanation\nfor its decision-making. We find that attention scores in next-activity\nprediction models can serve as explainers and exploit this fact in two proposed\ngraph-based explanation approaches. The gained insights could inspire future\nwork on the improvement of predictive business process models as well as\nenabling a neural network based mining of process models from event logs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"MEBwoUccySstbktRGyHT0tacUAdFtZDJT5I7JGjVBac","pdfSize":"1073577"}
