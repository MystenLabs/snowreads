{"id":"2408.00307","title":"ABC Align: Large Language Model Alignment for Safety & Accuracy","authors":"Gareth Seneque, Lap-Hang Ho, Ariel Kuperman, Nafise Erfanian Saeedi,\n  and Jeffrey Molendijk","authorsParsed":[["Seneque","Gareth",""],["Ho","Lap-Hang",""],["Kuperman","Ariel",""],["Saeedi","Nafise Erfanian",""],["Molendijk","Jeffrey",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 06:06:25 GMT"}],"updateDate":"2024-08-02","timestamp":1722492385000,"abstract":"  Alignment of Large Language Models (LLMs) remains an unsolved problem. Human\npreferences are highly distributed and can be captured at multiple levels of\nabstraction, from the individual to diverse populations. Organisational\npreferences, represented by standards and principles, are defined to mitigate\nreputational risk or meet legislative obligations. In this paper, we present\nABC Align, a novel alignment methodology for LLMs that enables integration of\nthe standards and preferences of a large media organisation into the LLM\nitself. We combine a set of data and methods that build on recent breakthroughs\nin synthetic data generation, preference optimisation, and post-training model\nquantisation. Our unified approach mitigates bias and improves accuracy, while\npreserving reasoning capability, as measured against standard benchmarks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OjgsDt3KD7PfqGCQNiMRxA9s5OQNEkeE1uIus4cgJXQ","pdfSize":"452912","txDigest":"EQ94qrUD3211WKHdsQFPZTSEUucxQYpZ7XhwNPqCTgCD","endEpoch":"1","status":"CERTIFIED"}
