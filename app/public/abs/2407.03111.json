{"id":"2407.03111","title":"Compressed Latent Replays for Lightweight Continual Learning on Spiking\n  Neural Networks","authors":"Alberto Dequino, Alessio Carpegna, Davide Nadalini, Alessandro Savino,\n  Luca Benini, Stefano Di Carlo, Francesco Conti","authorsParsed":[["Dequino","Alberto",""],["Carpegna","Alessio",""],["Nadalini","Davide",""],["Savino","Alessandro",""],["Benini","Luca",""],["Di Carlo","Stefano",""],["Conti","Francesco",""]],"versions":[{"version":"v1","created":"Wed, 8 May 2024 09:03:17 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 08:07:18 GMT"}],"updateDate":"2024-07-08","timestamp":1715158997000,"abstract":"  Rehearsal-based Continual Learning (CL) has been intensely investigated in\nDeep Neural Networks (DNNs). However, its application in Spiking Neural\nNetworks (SNNs) has not been explored in depth. In this paper we introduce the\nfirst memory-efficient implementation of Latent Replay (LR)-based CL for SNNs,\ndesigned to seamlessly integrate with resource-constrained devices. LRs combine\nnew samples with latent representations of previously learned data, to mitigate\nforgetting. Experiments on the Heidelberg SHD dataset with Sample and\nClass-Incremental tasks reach a Top-1 accuracy of 92.5% and 92%, respectively,\nwithout forgetting the previously learned information. Furthermore, we minimize\nthe LRs' requirements by applying a time-domain compression, reducing by two\norders of magnitude their memory requirement, with respect to a naive rehearsal\nsetup, with a maximum accuracy drop of 4%. On a Multi-Class-Incremental task,\nour SNN learns 10 new classes from an initial set of 10, reaching a Top-1\naccuracy of 78.4% on the full test set.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Emerging Technologies","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}