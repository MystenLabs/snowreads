{"id":"2408.09240","title":"RepControlNet: ControlNet Reparameterization","authors":"Zhaoli Deng, Kaibin Zhou, Fanyi Wang, Zhenpeng Mi","authorsParsed":[["Deng","Zhaoli",""],["Zhou","Kaibin",""],["Wang","Fanyi",""],["Mi","Zhenpeng",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 16:21:51 GMT"}],"updateDate":"2024-08-20","timestamp":1723911711000,"abstract":"  With the wide application of diffusion model, the high cost of inference\nresources has became an important bottleneck for its universal application.\nControllable generation, such as ControlNet, is one of the key research\ndirections of diffusion model, and the research related to inference\nacceleration and model compression is more important. In order to solve this\nproblem, this paper proposes a modal reparameterization method, RepControlNet,\nto realize the controllable generation of diffusion models without increasing\ncomputation. In the training process, RepControlNet uses the adapter to\nmodulate the modal information into the feature space, copy the CNN and MLP\nlearnable layers of the original diffusion model as the modal network, and\ninitialize these weights based on the original weights and coefficients. The\ntraining process only optimizes the parameters of the modal network. In the\ninference process, the weights of the neutralization original diffusion model\nin the modal network are reparameterized, which can be compared with or even\nsurpass the methods such as ControlNet, which use additional parameters and\ncomputational quantities, without increasing the number of parameters. We have\ncarried out a large number of experiments on both SD1.5 and SDXL, and the\nexperimental results show the effectiveness and efficiency of the proposed\nRepControlNet.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}