{"id":"2407.16171","title":"Learning Trimodal Relation for Audio-Visual Question Answering with\n  Missing Modality","authors":"Kyu Ri Park, Hong Joo Lee, Jung Uk Kim","authorsParsed":[["Park","Kyu Ri",""],["Lee","Hong Joo",""],["Kim","Jung Uk",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 04:35:56 GMT"}],"updateDate":"2024-07-25","timestamp":1721709356000,"abstract":"  Recent Audio-Visual Question Answering (AVQA) methods rely on complete visual\nand audio input to answer questions accurately. However, in real-world\nscenarios, issues such as device malfunctions and data transmission errors\nfrequently result in missing audio or visual modality. In such cases, existing\nAVQA methods suffer significant performance degradation. In this paper, we\npropose a framework that ensures robust AVQA performance even when a modality\nis missing. First, we propose a Relation-aware Missing Modal (RMM) generator\nwith Relation-aware Missing Modal Recalling (RMMR) loss to enhance the ability\nof the generator to recall missing modal information by understanding the\nrelationships and context among the available modalities. Second, we design an\nAudio-Visual Relation-aware (AVR) diffusion model with Audio-Visual Enhancing\n(AVE) loss to further enhance audio-visual features by leveraging the\nrelationships and shared cues between the audio-visual modalities. As a result,\nour method can provide accurate answers by effectively utilizing available\ninformation even when input modalities are missing. We believe our method holds\npotential applications not only in AVQA research but also in various\nmulti-modal scenarios.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YnFwtOq5Y_9jVuJ1GOYbn4dViggzoQOPRxsw7-0IAvE","pdfSize":"1311892"}
