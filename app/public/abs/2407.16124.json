{"id":"2407.16124","title":"Fr\\'echet Video Motion Distance: A Metric for Evaluating Motion\n  Consistency in Videos","authors":"Jiahe Liu, Youran Qu, Qi Yan, Xiaohui Zeng, Lele Wang, Renjie Liao","authorsParsed":[["Liu","Jiahe",""],["Qu","Youran",""],["Yan","Qi",""],["Zeng","Xiaohui",""],["Wang","Lele",""],["Liao","Renjie",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 02:10:50 GMT"}],"updateDate":"2024-07-24","timestamp":1721700650000,"abstract":"  Significant advancements have been made in video generative models recently.\nUnlike image generation, video generation presents greater challenges,\nrequiring not only generating high-quality frames but also ensuring temporal\nconsistency across these frames. Despite the impressive progress, research on\nmetrics for evaluating the quality of generated videos, especially concerning\ntemporal and motion consistency, remains underexplored. To bridge this research\ngap, we propose Fr\\'echet Video Motion Distance (FVMD) metric, which focuses on\nevaluating motion consistency in video generation. Specifically, we design\nexplicit motion features based on key point tracking, and then measure the\nsimilarity between these features via the Fr\\'echet distance. We conduct\nsensitivity analysis by injecting noise into real videos to verify the\neffectiveness of FVMD. Further, we carry out a large-scale human study,\ndemonstrating that our metric effectively detects temporal noise and aligns\nbetter with human perceptions of generated video quality than existing metrics.\nAdditionally, our motion features can consistently improve the performance of\nVideo Quality Assessment (VQA) models, indicating that our approach is also\napplicable to unary video quality evaluation. Code is available at\nhttps://github.com/ljh0v0/FMD-frechet-motion-distance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}