{"id":"2407.06084","title":"3D Vision and Language Pretraining with Large-Scale Synthetic Data","authors":"Dejie Yang and Zhu Xu and Wentao Mo and Qingchao Chen and Siyuan Huang\n  and Yang Liu","authorsParsed":[["Yang","Dejie",""],["Xu","Zhu",""],["Mo","Wentao",""],["Chen","Qingchao",""],["Huang","Siyuan",""],["Liu","Yang",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 16:26:52 GMT"}],"updateDate":"2024-07-09","timestamp":1720456012000,"abstract":"  3D Vision-Language Pre-training (3D-VLP) aims to provide a pre-train model\nwhich can bridge 3D scenes with natural language, which is an important\ntechnique for embodied intelligence. However, current 3D-VLP datasets are\nhindered by limited scene-level diversity and insufficient fine-grained\nannotations (only 1.2K scenes and 280K textual annotations in ScanScribe),\nprimarily due to the labor-intensive of collecting and annotating 3D scenes. To\novercome these obstacles, we construct SynVL3D, a comprehensive synthetic\nscene-text corpus with 10K indoor scenes and 1M descriptions at object, view,\nand room levels, which has the advantages of diverse scene data, rich textual\ndescriptions, multi-grained 3D-text associations, and low collection cost.\nUtilizing the rich annotations in SynVL3D, we pre-train a simple and unified\nTransformer for aligning 3D and language with multi-grained pretraining tasks.\nMoreover, we propose a synthetic-to-real domain adaptation in downstream task\nfine-tuning process to address the domain shift. Through extensive experiments,\nwe verify the effectiveness of our model design by achieving state-of-the-art\nperformance on downstream tasks including visual grounding, dense captioning,\nand question answering.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}