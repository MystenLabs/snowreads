{"id":"2408.05955","title":"Probabilistic Vision-Language Representation for Weakly Supervised\n  Temporal Action Localization","authors":"Geuntaek Lim, Hyunwoo Kim, Joonsoo Kim, Yukyung Choi","authorsParsed":[["Lim","Geuntaek",""],["Kim","Hyunwoo",""],["Kim","Joonsoo",""],["Choi","Yukyung",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 07:09:12 GMT"}],"updateDate":"2024-08-13","timestamp":1723446552000,"abstract":"  Weakly supervised temporal action localization (WTAL) aims to detect action\ninstances in untrimmed videos using only video-level annotations. Since many\nexisting works optimize WTAL models based on action classification labels, they\nencounter the task discrepancy problem (i.e., localization-by-classification).\nTo tackle this issue, recent studies have attempted to utilize action category\nnames as auxiliary semantic knowledge through vision-language pre-training\n(VLP). However, there are still areas where existing research falls short.\nPrevious approaches primarily focused on leveraging textual information from\nlanguage models but overlooked the alignment of dynamic human action and VLP\nknowledge in a joint space. Furthermore, the deterministic representation\nemployed in previous studies struggles to capture fine-grained human motions.\nTo address these problems, we propose a novel framework that aligns human\naction knowledge and VLP knowledge in a probabilistic embedding space.\nMoreover, we propose intra- and inter-distribution contrastive learning to\nenhance the probabilistic embedding space based on statistical similarities.\nExtensive experiments and ablation studies reveal that our method significantly\noutperforms all previous state-of-the-art methods. Code is available at\nhttps://github.com/sejong-rcv/PVLR.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"J5tGFMF9mpI4FUSlsvYMYMg-G_TOh3OEF-g_tUqGWzs","pdfSize":"3859337"}
