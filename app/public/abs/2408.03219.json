{"id":"2408.03219","title":"Learning to Learn without Forgetting using Attention","authors":"Anna Vettoruzzo, Joaquin Vanschoren, Mohamed-Rafik Bouguelia,\n  Thorsteinn R\\\"ognvaldsson","authorsParsed":[["Vettoruzzo","Anna",""],["Vanschoren","Joaquin",""],["Bouguelia","Mohamed-Rafik",""],["RÃ¶gnvaldsson","Thorsteinn",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 14:25:23 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 06:25:50 GMT"}],"updateDate":"2024-08-15","timestamp":1722954323000,"abstract":"  Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}