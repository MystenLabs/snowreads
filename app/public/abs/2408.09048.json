{"id":"2408.09048","title":"mRNA2vec: mRNA Embedding with Language Model in the 5'UTR-CDS for mRNA\n  Design","authors":"Honggen Zhang, Xiangrui Gao, June Zhang, Lipeng Lai","authorsParsed":[["Zhang","Honggen",""],["Gao","Xiangrui",""],["Zhang","June",""],["Lai","Lipeng",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 23:23:40 GMT"}],"updateDate":"2024-08-20","timestamp":1723850620000,"abstract":"  Messenger RNA (mRNA)-based vaccines are accelerating the discovery of new\ndrugs and revolutionizing the pharmaceutical industry. However, selecting\nparticular mRNA sequences for vaccines and therapeutics from extensive mRNA\nlibraries is costly. Effective mRNA therapeutics require carefully designed\nsequences with optimized expression levels and stability. This paper proposes a\nnovel contextual language model (LM)-based embedding method: mRNA2vec. In\ncontrast to existing mRNA embedding approaches, our method is based on the\nself-supervised teacher-student learning framework of data2vec. We jointly use\nthe 5' untranslated region (UTR) and coding sequence (CDS) region as the input\nsequences. We adapt our LM-based approach specifically to mRNA by 1)\nconsidering the importance of location on the mRNA sequence with probabilistic\nmasking, 2) using Minimum Free Energy (MFE) prediction and Secondary Structure\n(SS) classification as additional pretext tasks. mRNA2vec demonstrates\nsignificant improvements in translation efficiency (TE) and expression level\n(EL) prediction tasks in UTR compared to SOTA methods such as UTR-LM. It also\ngives a competitive performance in mRNA stability and protein production level\ntasks in CDS such as CodonBERT.\n","subjects":["Quantitative Biology/Quantitative Methods","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}