{"id":"2408.04746","title":"More Questions than Answers? Lessons from Integrating Explainable AI\n  into a Cyber-AI Tool","authors":"Ashley Suh, Harry Li, Caitlin Kenney, Kenneth Alperin, Steven R. Gomez","authorsParsed":[["Suh","Ashley",""],["Li","Harry",""],["Kenney","Caitlin",""],["Alperin","Kenneth",""],["Gomez","Steven R.",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 20:09:31 GMT"}],"updateDate":"2024-08-12","timestamp":1723147771000,"abstract":"  We share observations and challenges from an ongoing effort to implement\nExplainable AI (XAI) in a domain-specific workflow for cybersecurity analysts.\nSpecifically, we briefly describe a preliminary case study on the use of XAI\nfor source code classification, where accurate assessment and timeliness are\nparamount. We find that the outputs of state-of-the-art saliency explanation\ntechniques (e.g., SHAP or LIME) are lost in translation when interpreted by\npeople with little AI expertise, despite these techniques being marketed for\nnon-technical users. Moreover, we find that popular XAI techniques offer fewer\ninsights for real-time human-AI workflows when they are post hoc and too\nlocalized in their explanations. Instead, we observe that cyber analysts need\nhigher-level, easy-to-digest explanations that can offer as little disruption\nas possible to their workflows. We outline unaddressed gaps in practical and\neffective XAI, then touch on how emerging technologies like Large Language\nModels (LLMs) could mitigate these existing obstacles.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}