{"id":"2407.09731","title":"Sliding Window Bi-Objective Evolutionary Algorithms for Optimizing\n  Chance-Constrained Monotone Submodular Functions","authors":"Xiankun Yan, Aneta Neumann, Frank Neumann","authorsParsed":[["Yan","Xiankun",""],["Neumann","Aneta",""],["Neumann","Frank",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 00:28:29 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 05:13:09 GMT"}],"updateDate":"2024-08-08","timestamp":1720830509000,"abstract":"  Variants of the GSEMO algorithm using multi-objective formulations have been\nsuccessfully analyzed and applied to optimize chance-constrained submodular\nfunctions. However, due to the effect of the increasing population size of the\nGSEMO algorithm considered in these studies from the algorithms, the approach\nbecomes ineffective if the number of trade-offs obtained grows quickly during\nthe optimization run. In this paper, we apply the sliding-selection approach\nintroduced in [21] to the optimization of chance-constrained monotone\nsubmodular functions. We theoretically analyze the resulting SW-GSEMO algorithm\nwhich successfully limits the population size as a key factor that impacts the\nruntime and show that this allows it to obtain better runtime guarantees than\nthe best ones currently known for the GSEMO. In our experimental study, we\ncompare the performance of the SW-GSEMO to the GSEMO and NSGA-II on the maximum\ncoverage problem under the chance constraint and show that the SW-GSEMO\noutperforms the other two approaches in most cases. In order to get additional\ninsights into the optimization behavior of SW-GSEMO, we visualize the selection\nbehavior of SW-GSEMO during its optimization process and show it beats other\nalgorithms to obtain the highest quality of solution in variable instances.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}