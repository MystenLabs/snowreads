{"id":"2407.13515","title":"CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool\n  Interactions for People with Low Vision","authors":"Jaewook Lee, Andrew D. Tjahjadi, Jiho Kim, Junpu Yu, Minji Park,\n  Jiawen Zhang, Jon E. Froehlich, Yapeng Tian, Yuhang Zhao","authorsParsed":[["Lee","Jaewook",""],["Tjahjadi","Andrew D.",""],["Kim","Jiho",""],["Yu","Junpu",""],["Park","Minji",""],["Zhang","Jiawen",""],["Froehlich","Jon E.",""],["Tian","Yapeng",""],["Zhao","Yuhang",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 13:46:15 GMT"},{"version":"v2","created":"Sun, 28 Jul 2024 00:00:10 GMT"}],"updateDate":"2024-07-30","timestamp":1721310375000,"abstract":"  Cooking is a central activity of daily living, supporting independence as\nwell as mental and physical health. However, prior work has highlighted key\nbarriers for people with low vision (LV) to cook, particularly around safely\ninteracting with tools, such as sharp knives or hot pans. Drawing on recent\nadvancements in computer vision (CV), we present CookAR, a head-mounted AR\nsystem with real-time object affordance augmentations to support safe and\nefficient interactions with kitchen tools. To design and implement CookAR, we\ncollected and annotated the first egocentric dataset of kitchen tool\naffordances, fine-tuned an affordance segmentation model, and developed an AR\nsystem with a stereo camera to generate visual augmentations. To validate\nCookAR, we conducted a technical evaluation of our fine-tuned model as well as\na qualitative lab study with 10 LV participants for suitable augmentation\ndesign. Our technical evaluation demonstrates that our model outperforms the\nbaseline on our tool affordance dataset, while our user study indicates a\npreference for affordance augmentations over the traditional whole object\naugmentations.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/"}