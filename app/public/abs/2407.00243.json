{"id":"2407.00243","title":"Improving Locality in Sparse and Dense Matrix Multiplications","authors":"Mohammad Mahdi Salehi Dezfuli and Kazem Cheshmi","authorsParsed":[["Dezfuli","Mohammad Mahdi Salehi",""],["Cheshmi","Kazem",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 21:50:37 GMT"}],"updateDate":"2024-07-02","timestamp":1719611437000,"abstract":"  Consecutive matrix multiplications are commonly used in graph neural networks\nand sparse linear solvers. These operations frequently access the same matrices\nfor both reading and writing. While reusing these matrices improves data\nlocality, it presents a challenge due to the irregular dependencies between\niterations across the two multiplication operations. Existing fusion methods\noften introduce excessive synchronization overhead or overlapped computations\nwith limited benefits. This paper proposes tile fusion, a runtime approach that\nfuses tiles of the two matrix-matrix multiplications, where at least one of the\ninvolved matrices is sparse. Tile fusion aims to improve data locality while\nproviding sufficient workload for cores in shared-memory multi-core processors.\nFor a pair of matrix-matrix multiplications, tile fusion outperforms unfused\nbaseline and MKL implementations with a geometric mean speedup of 1.97$\\times$\n1.64$\\times$, respectively, on multi-core CPUs.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}