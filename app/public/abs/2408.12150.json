{"id":"2408.12150","title":"DeepHQ: Learned Hierarchical Quantizer for Progressive Deep Image Coding","authors":"Jooyoung Lee, Se Yoon Jeong and Munchurl Kim","authorsParsed":[["Lee","Jooyoung",""],["Jeong","Se Yoon",""],["Kim","Munchurl",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 06:32:53 GMT"}],"updateDate":"2024-08-23","timestamp":1724308373000,"abstract":"  Unlike fixed- or variable-rate image coding, progressive image coding (PIC)\naims to compress various qualities of images into a single bitstream,\nincreasing the versatility of bitstream utilization and providing high\ncompression efficiency compared to simulcast compression. Research on neural\nnetwork (NN)-based PIC is in its early stages, mainly focusing on applying\nvarying quantization step sizes to the transformed latent representations in a\nhierarchical manner. These approaches are designed to compress only the\nprogressively added information as the quality improves, considering that a\nwider quantization interval for lower-quality compression includes multiple\nnarrower sub-intervals for higher-quality compression. However, the existing\nmethods are based on handcrafted quantization hierarchies, resulting in\nsub-optimal compression efficiency. In this paper, we propose an NN-based\nprogressive coding method that firstly utilizes learned quantization step sizes\nvia learning for each quantization layer. We also incorporate selective\ncompression with which only the essential representation components are\ncompressed for each quantization layer. We demonstrate that our method achieves\nsignificantly higher coding efficiency than the existing approaches with\ndecreased decoding time and reduced model size.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}