{"id":"2408.05421","title":"EPAM-Net: An Efficient Pose-driven Attention-guided Multimodal Network\n  for Video Action Recognition","authors":"Ahmed Abdelkawy, Asem Ali, and Aly Farag","authorsParsed":[["Abdelkawy","Ahmed",""],["Ali","Asem",""],["Farag","Aly",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 03:15:24 GMT"}],"updateDate":"2024-08-13","timestamp":1723259724000,"abstract":"  Existing multimodal-based human action recognition approaches are either\ncomputationally expensive, which limits their applicability in real-time\nscenarios, or fail to exploit the spatial temporal information of multiple data\nmodalities. In this work, we present an efficient pose-driven attention-guided\nmultimodal network (EPAM-Net) for action recognition in videos. Specifically,\nwe adapted X3D networks for both RGB and pose streams to capture\nspatio-temporal features from RGB videos and their skeleton sequences. Then\nskeleton features are utilized to help the visual network stream focusing on\nkey frames and their salient spatial regions using a spatial temporal attention\nblock. Finally, the scores of the two streams of the proposed network are fused\nfor final classification. The experimental results show that our method\nachieves competitive performance on NTU-D 60 and NTU RGB-D 120 benchmark\ndatasets. Moreover, our model provides a 6.2--9.9x reduction in FLOPs\n(floating-point operation, in number of multiply-adds) and a 9--9.6x reduction\nin the number of network parameters. The code will be available at\nhttps://github.com/ahmed-nady/Multimodal-Action-Recognition.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}