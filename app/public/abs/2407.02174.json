{"id":"2407.02174","title":"BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event\n  Stream","authors":"Wenpu Li, Pian Wan, Peng Wang, Jinghang Li, Yi Zhou, Peidong Liu","authorsParsed":[["Li","Wenpu",""],["Wan","Pian",""],["Wang","Peng",""],["Li","Jinghang",""],["Zhou","Yi",""],["Liu","Peidong",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 11:28:22 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 13:17:05 GMT"},{"version":"v3","created":"Wed, 11 Sep 2024 15:25:18 GMT"}],"updateDate":"2024-09-12","timestamp":1719919702000,"abstract":"  Neural implicit representation of visual scenes has attracted a lot of\nattention in recent research of computer vision and graphics. Most prior\nmethods focus on how to reconstruct 3D scene representation from a set of\nimages. In this work, we demonstrate the possibility to recover the neural\nradiance fields (NeRF) from a single blurry image and its corresponding event\nstream. We model the camera motion with a cubic B-Spline in SE(3) space. Both\nthe blurry image and the brightness change within a time interval, can then be\nsynthesized from the 3D scene representation given the 6-DoF poses interpolated\nfrom the cubic B-Spline. Our method can jointly learn both the implicit neural\nscene representation and recover the camera motion by minimizing the\ndifferences between the synthesized data and the real measurements without\npre-computed camera poses from COLMAP. We evaluate the proposed method with\nboth synthetic and real datasets. The experimental results demonstrate that we\nare able to render view-consistent latent sharp images from the learned NeRF\nand bring a blurry image alive in high quality. Code and data are available at\nhttps://github.com/wu-cvgl/BeNeRF.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}