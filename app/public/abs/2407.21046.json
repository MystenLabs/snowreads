{"id":"2407.21046","title":"Promises and Pitfalls of Generative Masked Language Modeling:\n  Theoretical Framework and Practical Guidelines","authors":"Yuchen Li, Alexandre Kirchmeyer, Aashay Mehta, Yilong Qin, Boris\n  Dadachev, Kishore Papineni, Sanjiv Kumar, Andrej Risteski","authorsParsed":[["Li","Yuchen",""],["Kirchmeyer","Alexandre",""],["Mehta","Aashay",""],["Qin","Yilong",""],["Dadachev","Boris",""],["Papineni","Kishore",""],["Kumar","Sanjiv",""],["Risteski","Andrej",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 18:00:00 GMT"}],"updateDate":"2024-08-01","timestamp":1721671200000,"abstract":"  Autoregressive language models are the currently dominant paradigm for text\ngeneration, but they have some fundamental limitations that cannot be remedied\nby scale-for example inherently sequential and unidirectional generation. While\nalternate classes of models have been explored, we have limited mathematical\nunderstanding of their fundamental power and limitations. In this paper we\nfocus on Generative Masked Language Models (GMLMs), a non-autoregressive\nparadigm in which we train a model to fit conditional probabilities of the data\ndistribution via masking, which are subsequently used as inputs to a Markov\nChain to draw samples from the model, These models empirically strike a\npromising speed-quality trade-off as each step can be typically parallelized by\ndecoding the entire sequence in parallel. We develop a mathematical framework\nfor analyzing and improving such models which sheds light on questions of\nsample complexity and inference speed and quality. Empirically, we adapt the T5\nmodel for iteratively-refined parallel decoding, achieving 2-3x speedup in\nmachine translation with minimal sacrifice in quality compared with\nautoregressive models. We run careful ablation experiments to give\nrecommendations on key design choices, and make fine-grained observations on\nthe common error modes in connection with our theory. Our mathematical analyses\nand empirical observations characterize both potentials and limitations of this\napproach, and can be applied to future works on improving understanding and\nperformance of GMLMs. Our codes are released at\nhttps://github.com/google-research/google-research/tree/master/padir\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}