{"id":"2408.05723","title":"Deep Learning with Data Privacy via Residual Perturbation","authors":"Wenqi Tao, Huaming Ling, Zuoqiang Shi, Bao Wang","authorsParsed":[["Tao","Wenqi",""],["Ling","Huaming",""],["Shi","Zuoqiang",""],["Wang","Bao",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 08:26:43 GMT"}],"updateDate":"2024-08-13","timestamp":1723364803000,"abstract":"  Protecting data privacy in deep learning (DL) is of crucial importance.\nSeveral celebrated privacy notions have been established and used for\nprivacy-preserving DL. However, many existing mechanisms achieve privacy at the\ncost of significant utility degradation and computational overhead. In this\npaper, we propose a stochastic differential equation-based residual\nperturbation for privacy-preserving DL, which injects Gaussian noise into each\nresidual mapping of ResNets. Theoretically, we prove that residual perturbation\nguarantees differential privacy (DP) and reduces the generalization gap of DL.\nEmpirically, we show that residual perturbation is computationally efficient\nand outperforms the state-of-the-art differentially private stochastic gradient\ndescent (DPSGD) in utility maintenance without sacrificing membership privacy.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}