{"id":"2408.14381","title":"Learning Tree-Structured Composition of Data Augmentation","authors":"Dongyue Li, Kailai Chen, Predrag Radivojac, and Hongyang R. Zhang","authorsParsed":[["Li","Dongyue",""],["Chen","Kailai",""],["Radivojac","Predrag",""],["Zhang","Hongyang R.",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 16:04:13 GMT"}],"updateDate":"2024-08-27","timestamp":1724688253000,"abstract":"  Data augmentation is widely used for training a neural network given little\nlabeled data. A common practice of augmentation training is applying a\ncomposition of multiple transformations sequentially to the data. Existing\naugmentation methods such as RandAugment randomly sample from a list of\npre-selected transformations, while methods such as AutoAugment apply advanced\nsearch to optimize over an augmentation set of size $k^d$, which is the number\nof transformation sequences of length $d$, given a list of $k$ transformations.\n  In this paper, we design efficient algorithms whose running time complexity\nis much faster than the worst-case complexity of $O(k^d)$, provably. We propose\na new algorithm to search for a binary tree-structured composition of $k$\ntransformations, where each tree node corresponds to one transformation. The\nbinary tree generalizes sequential augmentations, such as the SimCLR\naugmentation scheme for contrastive learning. Using a top-down, recursive\nsearch procedure, our algorithm achieves a runtime complexity of $O(2^d k)$,\nwhich is much faster than $O(k^d)$ as $k$ increases above $2$. We apply our\nalgorithm to tackle data distributions with heterogeneous subpopulations by\nsearching for one tree in each subpopulation and then learning a weighted\ncombination, resulting in a forest of trees.\n  We validate our proposed algorithms on numerous graph and image datasets,\nincluding a multi-label graph classification dataset we collected. The dataset\nexhibits significant variations in the sizes of graphs and their average\ndegrees, making it ideal for studying data augmentation. We show that our\napproach can reduce the computation cost by 43% over existing search methods\nwhile improving performance by 4.3%. The tree structures can be used to\ninterpret the relative importance of each transformation, such as identifying\nthe important transformations on small vs. large graphs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Data Structures and Algorithms"],"license":"http://creativecommons.org/licenses/by/4.0/"}