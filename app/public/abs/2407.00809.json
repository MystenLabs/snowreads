{"id":"2407.00809","title":"Kernel Neural Operators (KNOs) for Scalable, Memory-efficient,\n  Geometrically-flexible Operator Learning","authors":"Matthew Lowery, John Turnage, Zachary Morrow, John D. Jakeman, Akil\n  Narayan, Shandian Zhe, Varun Shankar","authorsParsed":[["Lowery","Matthew",""],["Turnage","John",""],["Morrow","Zachary",""],["Jakeman","John D.",""],["Narayan","Akil",""],["Zhe","Shandian",""],["Shankar","Varun",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 19:28:12 GMT"}],"updateDate":"2024-07-02","timestamp":1719775692000,"abstract":"  This paper introduces the Kernel Neural Operator (KNO), a novel operator\nlearning technique that uses deep kernel-based integral operators in\nconjunction with quadrature for function-space approximation of operators (maps\nfrom functions to functions). KNOs use parameterized, closed-form,\nfinitely-smooth, and compactly-supported kernels with trainable sparsity\nparameters within the integral operators to significantly reduce the number of\nparameters that must be learned relative to existing neural operators.\nMoreover, the use of quadrature for numerical integration endows the KNO with\ngeometric flexibility that enables operator learning on irregular geometries.\nNumerical results demonstrate that on existing benchmarks the training and test\naccuracy of KNOs is higher than popular operator learning techniques while\nusing at least an order of magnitude fewer trainable parameters. KNOs thus\nrepresent a new paradigm of low-memory, geometrically-flexible, deep operator\nlearning, while retaining the implementation simplicity and transparency of\ntraditional kernel methods from both scientific computing and machine learning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}