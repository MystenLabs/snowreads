{"id":"2407.09152","title":"The Two Sides of the Coin: Hallucination Generation and Detection with\n  LLMs as Evaluators for LLMs","authors":"Anh Thu Maria Bui, Saskia Felizitas Brech, Natalie Hu{\\ss}feldt,\n  Tobias Jennert, Melanie Ullrich, Timo Breuer, Narjes Nikzad Khasmakhi,\n  Philipp Schaer","authorsParsed":[["Bui","Anh Thu Maria",""],["Brech","Saskia Felizitas",""],["Hu√üfeldt","Natalie",""],["Jennert","Tobias",""],["Ullrich","Melanie",""],["Breuer","Timo",""],["Khasmakhi","Narjes Nikzad",""],["Schaer","Philipp",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 10:34:46 GMT"}],"updateDate":"2024-07-15","timestamp":1720780486000,"abstract":"  Hallucination detection in Large Language Models (LLMs) is crucial for\nensuring their reliability. This work presents our participation in the CLEF\nELOQUENT HalluciGen shared task, where the goal is to develop evaluators for\nboth generating and detecting hallucinated content. We explored the\ncapabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this\npurpose. We also employed ensemble majority voting to incorporate all four\nmodels for the detection task. The results provide valuable insights into the\nstrengths and weaknesses of these LLMs in handling hallucination generation and\ndetection tasks.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}