{"id":"2407.01079","title":"On Statistical Rates and Provably Efficient Criteria of Latent Diffusion\n  Transformers (DiTs)","authors":"Jerry Yao-Chieh Hu, Weimin Wu, Zhao Song, Han Liu","authorsParsed":[["Hu","Jerry Yao-Chieh",""],["Wu","Weimin",""],["Song","Zhao",""],["Liu","Han",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 08:34:40 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 06:25:19 GMT"}],"updateDate":"2024-08-23","timestamp":1719822880000,"abstract":"  We investigate the statistical and computational limits of latent\n\\textbf{Di}ffusion \\textbf{T}ransformers (\\textbf{DiT}s) under the\nlow-dimensional linear latent space assumption. Statistically, we study the\nuniversal approximation and sample complexity of the DiTs score function, as\nwell as the distribution recovery property of the initial data. Specifically,\nunder mild data assumptions, we derive an approximation error bound for the\nscore network of latent DiTs, which is sub-linear in the latent space\ndimension. Additionally, we derive the corresponding sample complexity bound\nand show that the data distribution generated from the estimated score function\nconverges toward a proximate area of the original one. Computationally, we\ncharacterize the hardness of both forward inference and backward computation of\nlatent DiTs, assuming the Strong Exponential Time Hypothesis (SETH). For\nforward inference, we identify efficient criteria for all possible latent DiTs\ninference algorithms and showcase our theory by pushing the efficiency toward\nalmost-linear time inference. For backward computation, we leverage the\nlow-rank structure within the gradient computation of DiTs training for\npossible algorithmic speedup. Specifically, we show that such speedup achieves\nalmost-linear time latent DiTs training by casting the DiTs gradient as a\nseries of chained low-rank approximations with bounded error. Under the\nlow-dimensional assumption, we show that the convergence rate and the\ncomputational efficiency are both dominated by the dimension of the subspace,\nsuggesting that latent DiTs have the potential to bypass the challenges\nassociated with the high dimensionality of initial data.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"egO4zIaR_j7yIMD0pumacquOZFDtaTl0T5lTTmigDrE","pdfSize":"1161664"}
