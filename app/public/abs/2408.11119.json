{"id":"2408.11119","title":"Mistral-SPLADE: LLMs for better Learned Sparse Retrieval","authors":"Meet Doshi, Vishwajeet Kumar, Rudra Murthy, Vignesh P, and Jaydeep Sen","authorsParsed":[["Doshi","Meet",""],["Kumar","Vishwajeet",""],["Murthy","Rudra",""],["P","Vignesh",""],["Sen","Jaydeep",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 18:21:54 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 03:46:25 GMT"}],"updateDate":"2024-08-23","timestamp":1724178114000,"abstract":"  Learned Sparse Retrievers (LSR) have evolved into an effective retrieval\nstrategy that can bridge the gap between traditional keyword-based sparse\nretrievers and embedding-based dense retrievers. At its core, learned sparse\nretrievers try to learn the most important semantic keyword expansions from a\nquery and/or document which can facilitate better retrieval with overlapping\nkeyword expansions. LSR like SPLADE has typically been using encoder only\nmodels with MLM (masked language modeling) style objective in conjunction with\nknown ways of retrieval performance improvement such as hard negative mining,\ndistillation, etc. In this work, we propose to use decoder-only model for\nlearning semantic keyword expansion. We posit, decoder only models that have\nseen much higher magnitudes of data are better equipped to learn keyword\nexpansions needed for improved retrieval. We use Mistral as the backbone to\ndevelop our Learned Sparse Retriever similar to SPLADE and train it on a subset\nof sentence-transformer data which is often used for training text embedding\nmodels. Our experiments support the hypothesis that a sparse retrieval model\nbased on decoder only large language model (LLM) surpasses the performance of\nexisting LSR systems, including SPLADE and all its variants. The LLM based\nmodel (Echo-Mistral-SPLADE) now stands as a state-of-the-art learned sparse\nretrieval model on the BEIR text retrieval benchmark.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"SPefYYWGPEE3gohAe9W11gHwHijAxtycIOQT_UZhrq4","pdfSize":"4737820"}
