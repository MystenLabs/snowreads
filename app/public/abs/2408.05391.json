{"id":"2408.05391","title":"SAMSA: Efficient Transformer for Many Data Modalities","authors":"Minh Lenhat, Viet Anh Nguyen, Khoa Nguyen, Duong Duc Hieu, Dao Huu\n  Hung, Truong Son Hy","authorsParsed":[["Lenhat","Minh",""],["Nguyen","Viet Anh",""],["Nguyen","Khoa",""],["Hieu","Duong Duc",""],["Hung","Dao Huu",""],["Hy","Truong Son",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 00:09:06 GMT"},{"version":"v2","created":"Sun, 18 Aug 2024 13:22:05 GMT"}],"updateDate":"2024-08-20","timestamp":1723248546000,"abstract":"  The versatility of self-attention mechanism earned transformers great success\nin almost all data modalities, with limitations on the quadratic complexity and\ndifficulty of training. Efficient transformers, on the other hand, often rely\non clever data-modality-dependent construction to get over the quadratic\ncomplexity of transformers. This greatly hinders their applications on\ndifferent data modalities, which is one of the pillars of contemporary\nfoundational modeling. In this paper, we lay the groundwork for efficient\nfoundational modeling by proposing SAMSA - SAMpling-Self-Attention, a\ncontext-aware linear complexity self-attention mechanism that works well on\nmultiple data modalities. Our mechanism is based on a differentiable sampling\nwithout replacement method we discovered. This enables the self-attention\nmodule to attend to the most important token set, where the importance is\ndefined by data. Moreover, as differentiability is not needed in inference, the\nsparse formulation of our method costs little time overhead, further lowering\ncomputational costs. In short, SAMSA achieved competitive or even SOTA results\non many benchmarks, while being faster in inference, compared to other very\nspecialized models. Against full self-attention, real inference time\nsignificantly decreases while performance ranges from negligible degradation to\noutperformance. We release our source code in the repository:\nhttps://github.com/HySonLab/SAMSA\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}