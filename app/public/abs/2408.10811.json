{"id":"2408.10811","title":"Beyond English-Centric LLMs: What Language Do Multilingual Language\n  Models Think in?","authors":"Chengzhi Zhong, Fei Cheng, Qianying Liu, Junfeng Jiang, Zhen Wan,\n  Chenhui Chu, Yugo Murawaki, Sadao Kurohashi","authorsParsed":[["Zhong","Chengzhi",""],["Cheng","Fei",""],["Liu","Qianying",""],["Jiang","Junfeng",""],["Wan","Zhen",""],["Chu","Chenhui",""],["Murawaki","Yugo",""],["Kurohashi","Sadao",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 13:05:41 GMT"}],"updateDate":"2024-08-21","timestamp":1724159141000,"abstract":"  In this study, we investigate whether non-English-centric LLMs, despite their\nstrong performance, `think' in their respective dominant language: more\nprecisely, `think' refers to how the representations of intermediate layers,\nwhen un-embedded into the vocabulary space, exhibit higher probabilities for\ncertain dominant languages during generation. We term such languages as\ninternal $\\textbf{latent languages}$.\n  We examine the latent language of three typical categories of models for\nJapanese processing: Llama2, an English-centric model; Swallow, an\nEnglish-centric model with continued pre-training in Japanese; and LLM-jp, a\nmodel pre-trained on balanced English and Japanese corpora. Our empirical\nfindings reveal that, unlike Llama2 which relies exclusively on English as the\ninternal latent language, Japanese-specific Swallow and LLM-jp employ both\nJapanese and English, exhibiting dual internal latent languages. For any given\ntarget language, the model preferentially activates the latent language most\nclosely related to it. In addition, we explore how intermediate layers respond\nto questions involving cultural conflicts between latent internal and target\noutput languages. We further explore how the language identity shifts across\nlayers while keeping consistent semantic meaning reflected in the intermediate\nlayer representations.\n  This study deepens the understanding of non-English-centric large language\nmodels, highlighting the intricate dynamics of language representation within\ntheir intermediate layers.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}