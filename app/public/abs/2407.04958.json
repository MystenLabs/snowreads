{"id":"2407.04958","title":"Entropy-Informed Weighting Channel Normalizing Flow","authors":"Wei Chen, Shian Du, Shigui Li, Delu Zeng, John Paisley","authorsParsed":[["Chen","Wei",""],["Du","Shian",""],["Li","Shigui",""],["Zeng","Delu",""],["Paisley","John",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 04:46:41 GMT"}],"updateDate":"2024-07-09","timestamp":1720241201000,"abstract":"  Normalizing Flows (NFs) have gained popularity among deep generative models\ndue to their ability to provide exact likelihood estimation and efficient\nsampling. However, a crucial limitation of NFs is their substantial memory\nrequirements, arising from maintaining the dimension of the latent space equal\nto that of the input space. Multi-scale architectures bypass this limitation by\nprogressively reducing the dimension of latent variables while ensuring\nreversibility. Existing multi-scale architectures split the latent variables in\na simple, static manner at the channel level, compromising NFs' expressive\npower. To address this issue, we propose a regularized and feature-dependent\n$\\mathtt{Shuffle}$ operation and integrate it into vanilla multi-scale\narchitecture. This operation heuristically generates channel-wise weights and\nadaptively shuffles latent variables before splitting them with these weights.\nWe observe that such operation guides the variables to evolve in the direction\nof entropy increase, hence we refer to NFs with the $\\mathtt{Shuffle}$\noperation as \\emph{Entropy-Informed Weighting Channel Normalizing Flow}\n(EIW-Flow). Experimental results indicate that the EIW-Flow achieves\nstate-of-the-art density estimation results and comparable sample quality on\nCIFAR-10, CelebA and ImageNet datasets, with negligible additional\ncomputational overhead.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}