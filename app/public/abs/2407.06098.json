{"id":"2407.06098","title":"Epistemological Bias As a Means for the Automated Detection of\n  Injustices in Text","authors":"Kenya Andrews, Lamogha Chiazor","authorsParsed":[["Andrews","Kenya",""],["Chiazor","Lamogha",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 16:38:31 GMT"}],"updateDate":"2024-07-09","timestamp":1720456711000,"abstract":"  Injustice occurs when someone experiences unfair treatment or their rights\nare violated and is often due to the presence of implicit biases and prejudice\nsuch as stereotypes. The automated identification of injustice in text has\nreceived little attention, due in part to the fact that underlying implicit\nbiases or stereotypes are rarely explicitly stated and that instances often\noccur unconsciously due to the pervasive nature of prejudice in society. Here,\nwe describe a novel framework that combines the use of a fine-tuned BERT-based\nbias detection model, two stereotype detection models, and a lexicon-based\napproach to show that epistemological biases (i.e., words, which presupposes,\nentails, asserts, hedges, or boosts text to erode or assert a person's capacity\nas a knower) can assist with the automatic detection of injustice in text. The\nnews media has many instances of injustice (i.e. discriminatory narratives),\nthus it is our use case here. We conduct and discuss an empirical qualitative\nresearch study which shows how the framework can be applied to detect\ninjustices, even at higher volumes of data.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"yrd8C61zVkYYa0QiFt0fep98Dy1vhw6XI3cPyM5taY8","pdfSize":"1792581"}
