{"id":"2407.12616","title":"Missing Modality Prediction for Unpaired Multimodal Learning via Joint\n  Embedding of Unimodal Models","authors":"Donggeun Kim and Taesup Kim","authorsParsed":[["Kim","Donggeun",""],["Kim","Taesup",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 14:44:25 GMT"}],"updateDate":"2024-07-18","timestamp":1721227465000,"abstract":"  Multimodal learning typically relies on the assumption that all modalities\nare fully available during both the training and inference phases. However, in\nreal-world scenarios, consistently acquiring complete multimodal data presents\nsignificant challenges due to various factors. This often leads to the issue of\nmissing modalities, where data for certain modalities are absent, posing\nconsiderable obstacles not only for the availability of multimodal pretrained\nmodels but also for their fine-tuning and the preservation of robustness in\ndownstream tasks. To address these challenges, we propose a novel framework\nintegrating parameter-efficient fine-tuning of unimodal pretrained models with\na self-supervised joint-embedding learning method. This framework enables the\nmodel to predict the embedding of a missing modality in the representation\nspace during inference. Our method effectively predicts the missing embedding\nthrough prompt tuning, leveraging information from available modalities. We\nevaluate our approach on several multimodal benchmark datasets and demonstrate\nits effectiveness and robustness across various scenarios of missing\nmodalities.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}