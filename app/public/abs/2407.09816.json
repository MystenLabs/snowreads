{"id":"2407.09816","title":"MaskMoE: Boosting Token-Level Learning via Routing Mask in\n  Mixture-of-Experts","authors":"Zhenpeng Su, Zijia Lin, Xue Bai, Xing Wu, Yizhe Xiong, Haoran Lian,\n  Guangyuan Ma, Hui Chen, Guiguang Ding, Wei Zhou, Songlin Hu","authorsParsed":[["Su","Zhenpeng",""],["Lin","Zijia",""],["Bai","Xue",""],["Wu","Xing",""],["Xiong","Yizhe",""],["Lian","Haoran",""],["Ma","Guangyuan",""],["Chen","Hui",""],["Ding","Guiguang",""],["Zhou","Wei",""],["Hu","Songlin",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 09:22:33 GMT"},{"version":"v2","created":"Sun, 28 Jul 2024 04:00:52 GMT"},{"version":"v3","created":"Mon, 19 Aug 2024 13:16:16 GMT"},{"version":"v4","created":"Thu, 29 Aug 2024 08:45:58 GMT"}],"updateDate":"2024-08-30","timestamp":1720862553000,"abstract":"  Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. In MoE, there is an important module called the\nrouter, which is used to distribute each token to the experts. Currently, the\nmainstream routing methods include dynamic routing and fixed routing. Despite\ntheir promising results, MoE models encounter several challenges. Primarily,\nfor dynamic routing methods, the dispersion of training tokens across multiple\nexperts can lead to underfitting, particularly for infrequent tokens.\nAdditionally, though fixed routing methods can mitigate that issue, they\ncompromise on the diversity of representations. In this paper, we propose\n\\textbf{MaskMoE}, a method designed to enhance token-level learning by\nemploying a routing \\textbf{mask}ing technique within the\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"A8C-DCvMtG7_44B_0ue8tODGzaICqss4XLCK2vbXffo","pdfSize":"539163"}
