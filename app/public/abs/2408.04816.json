{"id":"2408.04816","title":"FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt\n  Optimization Across Tokenizers","authors":"Joshua Nathaniel Williams, J. Zico Kolter","authorsParsed":[["Williams","Joshua Nathaniel",""],["Kolter","J. Zico",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 02:16:37 GMT"}],"updateDate":"2024-08-12","timestamp":1723169797000,"abstract":"  The widespread use of large language models has resulted in a multitude of\ntokenizers and embedding spaces, making knowledge transfer in prompt discovery\ntasks difficult. In this work, we propose FUSE (Flexible Unification of\nSemantic Embeddings), an inexpensive approach to approximating an adapter layer\nthat maps from one model's textual embedding space to another, even across\ndifferent tokenizers. We introduce a third-order tensor-based representation of\na model's embedding space that aligns semantic embeddings that have been split\napart by different tokenizers, and use this representation to derive an\napproximation of the gradient of one model's outputs with respect to another\nmodel's embedding space. We show the efficacy of our approach via\nmulti-objective optimization over vision-language and causal language models\nfor image captioning and sentiment-based image captioning.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}