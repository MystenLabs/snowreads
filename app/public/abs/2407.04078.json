{"id":"2407.04078","title":"DotaMath: Decomposition of Thought with Code Assistance and\n  Self-correction for Mathematical Reasoning","authors":"Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang,\n  Dayiheng Liu","authorsParsed":[["Li","Chengpeng",""],["Dong","Guanting",""],["Xue","Mingfeng",""],["Peng","Ru",""],["Wang","Xiang",""],["Liu","Dayiheng",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 17:39:16 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 15:29:03 GMT"},{"version":"v3","created":"Wed, 17 Jul 2024 13:13:05 GMT"}],"updateDate":"2024-07-18","timestamp":1720114756000,"abstract":"  Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}