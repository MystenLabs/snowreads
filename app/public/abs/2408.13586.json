{"id":"2408.13586","title":"Balancing Diversity and Risk in LLM Sampling: How to Select Your Method\n  and Parameter for Open-Ended Text Generation","authors":"Yuxuan Zhou, Margret Keuper, Mario Fritz","authorsParsed":[["Zhou","Yuxuan",""],["Keuper","Margret",""],["Fritz","Mario",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 14:14:32 GMT"}],"updateDate":"2024-08-27","timestamp":1724508872000,"abstract":"  Sampling-based decoding strategies have been widely adopted for Large\nLanguage Models (LLMs) in numerous applications, which target a balance between\ndiversity and quality via temperature tuning and tail truncation (e.g., top-k\nand top-p sampling). Considering the high dynamic range of the candidate\nnext-token given different prefixes, recent studies propose to adaptively\ntruncate the tail of LLM's predicted distribution. Although improved results\nhaven been reported with these methods on open-ended text generation tasks, the\nresults are highly dependent on the curated truncation parameters and exemplar\ntext. In this paper, we propose a systematic way to estimate the intrinsic\ncapacity of a truncation sampling method by considering the trade-off between\ndiversity and risk at each decoding step, based on our collected prefix tree\nwhich preserves the context of a full sentence. Our work provides a\ncomprehensive comparison between existing truncation sampling methods, as well\nas their recommended parameters as a guideline for users.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZcJPXGPNpP0WT87pHsPnaloTb_G7NgqSFFuSZ976A04","pdfSize":"934097"}
