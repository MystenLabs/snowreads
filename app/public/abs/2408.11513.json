{"id":"2408.11513","title":"Last-Iterate Convergence of General Parameterized Policies in\n  Constrained MDPs","authors":"Washim Uddin Mondal and Vaneet Aggarwal","authorsParsed":[["Mondal","Washim Uddin",""],["Aggarwal","Vaneet",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 10:44:57 GMT"}],"updateDate":"2024-08-22","timestamp":1724237097000,"abstract":"  We consider the problem of learning a Constrained Markov Decision Process\n(CMDP) via general parameterization. Our proposed Primal-Dual based Regularized\nAccelerated Natural Policy Gradient (PDR-ANPG) algorithm uses entropy and\nquadratic regularizers to reach this goal. For a parameterized policy class\nwith transferred compatibility approximation error, $\\epsilon_{\\mathrm{bias}}$,\nPDR-ANPG achieves a last-iterate $\\epsilon$ optimality gap and $\\epsilon$\nconstraint violation (up to some additive factor of $\\epsilon_{\\mathrm{bias}}$)\nwith a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2}\\min\\{\\epsilon^{-2},\\epsilon_{\\mathrm{bias}}^{-\\frac{1}{3}}\\})$.\nIf the class is incomplete ($\\epsilon_{\\mathrm{bias}}>0$), then the sample\ncomplexity reduces to $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for\n$\\epsilon<(\\epsilon_{\\mathrm{bias}})^{\\frac{1}{6}}$. Moreover, for complete\npolicies with $\\epsilon_{\\mathrm{bias}}=0$, our algorithm achieves a\nlast-iterate $\\epsilon$ optimality gap and $\\epsilon$ constraint violation with\n$\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ sample complexity. It is a significant\nimprovement of the state-of-the-art last-iterate guarantees of general\nparameterized CMDPs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}