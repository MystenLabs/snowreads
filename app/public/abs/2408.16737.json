{"id":"2408.16737","title":"Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling","authors":"Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, Mehran\n  Kazemi","authorsParsed":[["Bansal","Hritik",""],["Hosseini","Arian",""],["Agarwal","Rishabh",""],["Tran","Vinh Q.",""],["Kazemi","Mehran",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 17:32:35 GMT"}],"updateDate":"2024-08-30","timestamp":1724952755000,"abstract":"  Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CGji2kzGeIJMu65Pt5cl_DsBwPgrlDv5deYmmWbm3Hw","pdfSize":"1169064"}
