{"id":"2407.18930","title":"Dynamic Encoder Size Based on Data-Driven Layer-wise Pruning for Speech\n  Recognition","authors":"Jingjing Xu, Wei Zhou, Zijian Yang, Eugen Beck, Ralf Schlueter","authorsParsed":[["Xu","Jingjing",""],["Zhou","Wei",""],["Yang","Zijian",""],["Beck","Eugen",""],["Schlueter","Ralf",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 08:35:21 GMT"}],"updateDate":"2024-07-30","timestamp":1720600521000,"abstract":"  Varying-size models are often required to deploy ASR systems under different\nhardware and/or application constraints such as memory and latency. To avoid\nredundant training and optimization efforts for individual models of different\nsizes, we present the dynamic encoder size approach, which jointly trains\nmultiple performant models within one supernet from scratch. These subnets of\nvarious sizes are layer-wise pruned from the supernet, and thus, enjoy full\nparameter sharing. By combining score-based pruning with supernet training, we\npropose two novel methods, Simple-Top-k and Iterative-Zero-Out, to\nautomatically select the best-performing subnets in a data-driven manner,\navoiding resource-intensive search efforts. Our experiments using CTC on both\nLibrispeech and TED-LIUM-v2 corpora show that our methods can achieve on-par\nperformance as individually trained models of each size category. Also, our\napproach consistently brings small performance improvements for the full-size\nsupernet.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}