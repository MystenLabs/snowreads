{"id":"2407.02665","title":"SMILe: Leveraging Submodular Mutual Information For Robust Few-Shot\n  Object Detection","authors":"Anay Majee, Ryan Sharp, Rishabh Iyer","authorsParsed":[["Majee","Anay",""],["Sharp","Ryan",""],["Iyer","Rishabh",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 20:53:43 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 15:52:12 GMT"}],"updateDate":"2024-09-18","timestamp":1719953623000,"abstract":"  Confusion and forgetting of object classes have been challenges of prime\ninterest in Few-Shot Object Detection (FSOD). To overcome these pitfalls in\nmetric learning based FSOD techniques, we introduce a novel Submodular Mutual\nInformation Learning (SMILe) framework which adopts combinatorial mutual\ninformation functions to enforce the creation of tighter and discriminative\nfeature clusters in FSOD. Our proposed approach generalizes to several existing\napproaches in FSOD, agnostic of the backbone architecture demonstrating\nelevated performance gains. A paradigm shift from instance based objective\nfunctions to combinatorial objectives in SMILe naturally preserves the\ndiversity within an object class resulting in reduced forgetting when subjected\nto few training examples. Furthermore, the application of mutual information\nbetween the already learnt (base) and newly added (novel) objects ensures\nsufficient separation between base and novel classes, minimizing the effect of\nclass confusion. Experiments on popular FSOD benchmarks, PASCAL-VOC and MS-COCO\nshow that our approach generalizes to State-of-the-Art (SoTA) approaches\nimproving their novel class performance by up to 5.7% (3.3 mAP points) and 5.4%\n(2.6 mAP points) on the 10-shot setting of VOC (split 3) and 30-shot setting of\nCOCO datasets respectively. Our experiments also demonstrate better retention\nof base class performance and up to 2x faster convergence over existing\napproaches agnostic of the underlying architecture.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}