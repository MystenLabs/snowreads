{"id":"2408.09380","title":"ELASTIC: Efficient Linear Attention for Sequential Interest Compression","authors":"Jiaxin Deng, Shiyao Wang, Song Lu, Yinfeng Li, Xinchen Luo, Yuanjun\n  Liu, Peixing Xu, Guorui Zhou","authorsParsed":[["Deng","Jiaxin",""],["Wang","Shiyao",""],["Lu","Song",""],["Li","Yinfeng",""],["Luo","Xinchen",""],["Liu","Yuanjun",""],["Xu","Peixing",""],["Zhou","Guorui",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 06:41:46 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 13:24:50 GMT"}],"updateDate":"2024-08-21","timestamp":1723963306000,"abstract":"  State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}