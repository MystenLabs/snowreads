{"id":"2407.12878","title":"Do LLMs have Consistent Values?","authors":"Naama Rozen, Gal Elidan, Amir Globerson, Ella Daniel","authorsParsed":[["Rozen","Naama",""],["Elidan","Gal",""],["Globerson","Amir",""],["Daniel","Ella",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 08:58:00 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 14:24:47 GMT"}],"updateDate":"2024-07-22","timestamp":1721120280000,"abstract":"  Values are a basic driving force underlying human behavior. Large Language\nModels (LLM) technology is constantly improving towards human-like dialogue.\nHowever, little research has been done to study the values exhibited in text\ngenerated by LLMs. Here we study this question by turning to the rich\nliterature on value structure in psychology. We ask whether LLMs exhibit the\nsame value structure that has been demonstrated in humans, including the\nranking of values, and correlation between values. We show that the results of\nthis analysis strongly depend on how the LLM is prompted, and that under a\nparticular prompting strategy (referred to as 'Value Anchoring') the agreement\nwith human data is quite compelling. Our results serve both to improve our\nunderstanding of values in LLMs, as well as introduce novel methods for\nassessing consistency in LLM responses.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}