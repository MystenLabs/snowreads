{"id":"2407.06426","title":"DebUnc: Mitigating Hallucinations in Large Language Model Agent\n  Communication with Uncertainty Estimations","authors":"Luke Yoffe and Alfonso Amayuelas and William Yang Wang","authorsParsed":[["Yoffe","Luke",""],["Amayuelas","Alfonso",""],["Wang","William Yang",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 22:15:01 GMT"}],"updateDate":"2024-07-10","timestamp":1720476901000,"abstract":"  To enhance Large Language Model (LLM) capabilities, multi-agent debates have\nbeen introduced, where multiple LLMs discuss solutions to a problem over\nseveral rounds of debate. However, LLMs often produce incorrect responses that\nappear deceptively confident, which can mislead other agents. This is partly\nbecause agents do not express their confidence levels during standard debates.\nTo address this, we introduce DebUnc, a multi-agent debate framework that uses\nuncertainty metrics to assess agent confidence levels. We adapted the LLM\nattention mechanism to adjust token weights based on confidence levels and also\nexplored using textual prompts to convey confidence. Our evaluations across\nvarious benchmarks show that attention-based methods are particularly\neffective, and that as uncertainty metrics evolve, performance will continue to\nincrease. The code is available at https://github.com/lukeyoffe/debunc\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multiagent Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}