{"id":"2408.13777","title":"Towards Completeness: A Generalizable Action Proposal Generator for\n  Zero-Shot Temporal Action Localization","authors":"Jia-Run Du, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng","authorsParsed":[["Du","Jia-Run",""],["Lin","Kun-Yu",""],["Meng","Jingke",""],["Zheng","Wei-Shi",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 09:07:06 GMT"}],"updateDate":"2024-08-27","timestamp":1724576826000,"abstract":"  To address the zero-shot temporal action localization (ZSTAL) task, existing\nworks develop models that are generalizable to detect and classify actions from\nunseen categories. They typically develop a category-agnostic action detector\nand combine it with the Contrastive Language-Image Pre-training (CLIP) model to\nsolve ZSTAL. However, these methods suffer from incomplete action proposals\ngenerated for \\textit{unseen} categories, since they follow a frame-level\nprediction paradigm and require hand-crafted post-processing to generate action\nproposals. To address this problem, in this work, we propose a novel model\nnamed Generalizable Action Proposal generator (GAP), which can interface\nseamlessly with CLIP and generate action proposals in a holistic way. Our GAP\nis built in a query-based architecture and trained with a proposal-level\nobjective, enabling it to estimate proposal completeness and eliminate the\nhand-crafted post-processing. Based on this architecture, we propose an\nAction-aware Discrimination loss to enhance the category-agnostic dynamic\ninformation of actions. Besides, we introduce a Static-Dynamic Rectifying\nmodule that incorporates the generalizable static information from CLIP to\nrefine the predicted proposals, which improves proposal completeness in a\ngeneralizable manner. Our experiments show that our GAP achieves\nstate-of-the-art performance on two challenging ZSTAL benchmarks, i.e.,\nThumos14 and ActivityNet1.3. Specifically, our model obtains significant\nperformance improvement over previous works on the two benchmarks, i.e., +3.2%\nand +3.4% average mAP, respectively.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}