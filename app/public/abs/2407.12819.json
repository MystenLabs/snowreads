{"id":"2407.12819","title":"A Look Into Training Large Language Models on Next Generation\n  Datacenters","authors":"Alexandru M. Gherghescu, Vlad-Andrei B\\u{a}doiu, Alexandru Agache,\n  Mihai-Valentin Dumitru, Iuliu Vasilescu, Radu Mantu, Costin Raiciu","authorsParsed":[["Gherghescu","Alexandru M.",""],["BÄƒdoiu","Vlad-Andrei",""],["Agache","Alexandru",""],["Dumitru","Mihai-Valentin",""],["Vasilescu","Iuliu",""],["Mantu","Radu",""],["Raiciu","Costin",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 10:33:46 GMT"}],"updateDate":"2024-07-19","timestamp":1719830026000,"abstract":"  Is it still worth doing computer networking research? What are relevant\nproblems in this space given the supremacy of hyperscalers in deployed large\nnetworks? We take an unconventional approach to finding relevant research\ndirections, by starting from Microsoft's plans to build a $100 billion\ndatacenter for ML. Our goal is to understand what models could be trained in\nsuch a datacenter, as well as the high-level challenges one may encounter in\ndoing so.\n  We first examine the constraints imposed by cooling and power requirements\nfor our target datacenter and find that it is infeasible to build in a single\nlocation. We use LLM scaling laws to determine that we could train models of\n50T or 100T. Finally, we examine how distributed training might work for these\nmodels, and what the networking requirements are. We conclude that building the\ndatacenter and training such models is technically possible, but this requires\na novel NIC-based multipath transport along with a redesign of the entire\ntraining stack, outlining a research agenda for our community in the near\nfuture.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Computing Research Repository/Networking and Internet Architecture"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Nw-mc7gRLfszC3Sq3krbmrLJhSfLFJbBr12pZPyJ_V0","pdfSize":"752763"}
