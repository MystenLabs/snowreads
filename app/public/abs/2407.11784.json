{"id":"2407.11784","title":"Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model\n  Co-development","authors":"Daoyuan Chen, Haibin Wang, Yilun Huang, Ce Ge, Yaliang Li, Bolin Ding,\n  Jingren Zhou","authorsParsed":[["Chen","Daoyuan",""],["Wang","Haibin",""],["Huang","Yilun",""],["Ge","Ce",""],["Li","Yaliang",""],["Ding","Bolin",""],["Zhou","Jingren",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:40:07 GMT"}],"updateDate":"2024-07-17","timestamp":1721140807000,"abstract":"  The emergence of large-scale multi-modal generative models has drastically\nadvanced artificial intelligence, introducing unprecedented levels of\nperformance and functionality. However, optimizing these models remains\nchallenging due to historically isolated paths of model-centric and\ndata-centric developments, leading to suboptimal outcomes and inefficient\nresource utilization. In response, we present a novel sandbox suite tailored\nfor integrated data-model co-development. This sandbox provides a comprehensive\nexperimental platform, enabling rapid iteration and insight-driven refinement\nof both data and models. Our proposed \"Probe-Analyze-Refine\" workflow,\nvalidated through applications on state-of-the-art LLaVA-like and DiT based\nmodels, yields significant performance boosts, such as topping the VBench\nleaderboard. We also uncover fruitful insights gleaned from exhaustive\nbenchmarks, shedding light on the critical interplay between data quality,\ndiversity, and model behavior. With the hope of fostering deeper understanding\nand future progress in multi-modal data and generative modeling, our codes,\ndatasets, and models are maintained and accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}