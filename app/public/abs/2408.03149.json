{"id":"2408.03149","title":"Leveraging Entity Information for Cross-Modality Correlation Learning:\n  The Entity-Guided Multimodal Summarization","authors":"Yanghai Zhang, Ye Liu, Shiwei Wu, Kai Zhang, Xukai Liu, Qi Liu, Enhong\n  Chen","authorsParsed":[["Zhang","Yanghai",""],["Liu","Ye",""],["Wu","Shiwei",""],["Zhang","Kai",""],["Liu","Xukai",""],["Liu","Qi",""],["Chen","Enhong",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 12:45:56 GMT"}],"updateDate":"2024-08-07","timestamp":1722948356000,"abstract":"  The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}