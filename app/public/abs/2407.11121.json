{"id":"2407.11121","title":"Towards Adversarially Robust Vision-Language Models: Insights from\n  Design Choices and Prompt Formatting Techniques","authors":"Rishika Bhagwatkar, Shravan Nayak, Reza Bayat, Alexis Roger, Daniel Z\n  Kaplan, Pouya Bashivan, Irina Rish","authorsParsed":[["Bhagwatkar","Rishika",""],["Nayak","Shravan",""],["Bayat","Reza",""],["Roger","Alexis",""],["Kaplan","Daniel Z",""],["Bashivan","Pouya",""],["Rish","Irina",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 18:00:01 GMT"}],"updateDate":"2024-07-17","timestamp":1721066401000,"abstract":"  Vision-Language Models (VLMs) have witnessed a surge in both research and\nreal-world applications. However, as they are becoming increasingly prevalent,\nensuring their robustness against adversarial attacks is paramount. This work\nsystematically investigates the impact of model design choices on the\nadversarial robustness of VLMs against image-based attacks. Additionally, we\nintroduce novel, cost-effective approaches to enhance robustness through prompt\nformatting. By rephrasing questions and suggesting potential adversarial\nperturbations, we demonstrate substantial improvements in model robustness\nagainst strong image-based attacks such as Auto-PGD. Our findings provide\nimportant guidelines for developing more robust VLMs, particularly for\ndeployment in safety-critical environments.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}