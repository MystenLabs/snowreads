{"id":"2407.17940","title":"Positive Text Reframing under Multi-strategy Optimization","authors":"Shutong Jia, Biwei Cao, Qingqing Gao, Jiuxin Cao, Bo Liu","authorsParsed":[["Jia","Shutong",""],["Cao","Biwei",""],["Gao","Qingqing",""],["Cao","Jiuxin",""],["Liu","Bo",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 10:58:42 GMT"},{"version":"v2","created":"Sat, 27 Jul 2024 06:56:06 GMT"}],"updateDate":"2024-07-30","timestamp":1721905122000,"abstract":"  Differing from sentiment transfer, positive reframing seeks to substitute\nnegative perspectives with positive expressions while preserving the original\nmeaning. With the emergence of pre-trained language models (PLMs), it is\npossible to achieve acceptable results by fine-tuning PLMs. Nevertheless,\ngenerating fluent, diverse and task-constrained reframing text remains a\nsignificant challenge. To tackle this issue, a \\textbf{m}ulti-\\textbf{s}trategy\n\\textbf{o}ptimization \\textbf{f}ramework (MSOF) is proposed in this paper.\nStarting from the objective of positive reframing, we first design positive\nsentiment reward and content preservation reward to encourage the model to\ntransform the negative expressions of the original text while ensuring the\nintegrity and consistency of the semantics. Then, different decoding\noptimization approaches are introduced to improve the quality of text\ngeneration. Finally, based on the modeling formula of positive reframing, we\npropose a multi-dimensional re-ranking method that further selects candidate\nsentences from three dimensions: strategy consistency, text similarity and\nfluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate\nour framework achieves significant improvements on unconstrained and controlled\npositive reframing tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}