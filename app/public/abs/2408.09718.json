{"id":"2408.09718","title":"Confirmation Bias in Gaussian Mixture Models","authors":"Amnon Balanov, Tamir Bendory, and Wasim Huleihel","authorsParsed":[["Balanov","Amnon",""],["Bendory","Tamir",""],["Huleihel","Wasim",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 06:18:42 GMT"}],"updateDate":"2024-08-20","timestamp":1724048322000,"abstract":"  Confirmation bias, the tendency to interpret information in a way that aligns\nwith one's preconceptions, can profoundly impact scientific research, leading\nto conclusions that reflect the researcher's hypotheses even when the\nobservational data do not support them. This issue is especially critical in\nscientific fields involving highly noisy observations, such as cryo-electron\nmicroscopy.\n  This study investigates confirmation bias in Gaussian mixture models. We\nconsider the following experiment: A team of scientists assumes they are\nanalyzing data drawn from a Gaussian mixture model with known signals\n(hypotheses) as centroids. However, in reality, the observations consist\nentirely of noise without any informative structure. The researchers use a\nsingle iteration of the K-means or expectation-maximization algorithms, two\npopular algorithms to estimate the centroids. Despite the observations being\npure noise, we show that these algorithms yield biased estimates that resemble\nthe initial hypotheses, contradicting the unbiased expectation that averaging\nthese noise observations would converge to zero. Namely, the algorithms\ngenerate estimates that mirror the postulated model, although the hypotheses\n(the presumed centroids of the Gaussian mixture) are not evident in the\nobservations. Specifically, among other results, we prove a positive\ncorrelation between the estimates produced by the algorithms and the\ncorresponding hypotheses. We also derive explicit closed-form expressions of\nthe estimates for a finite and infinite number of hypotheses. This study\nunderscores the risks of confirmation bias in low signal-to-noise environments,\nprovides insights into potential pitfalls in scientific methodologies, and\nhighlights the importance of prudent data interpretation.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Information Theory","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Signal Processing","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}