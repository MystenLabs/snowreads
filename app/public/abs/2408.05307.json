{"id":"2408.05307","title":"Audio-visual cross-modality knowledge transfer for machine\n  learning-based in-situ monitoring in laser additive manufacturing","authors":"Jiarui Xie, Mutahar Safdar, Lequn Chen, Seung Ki Moon, Yaoyao Fiona\n  Zhao","authorsParsed":[["Xie","Jiarui",""],["Safdar","Mutahar",""],["Chen","Lequn",""],["Moon","Seung Ki",""],["Zhao","Yaoyao Fiona",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 19:06:38 GMT"}],"updateDate":"2024-08-13","timestamp":1723230398000,"abstract":"  Various machine learning (ML)-based in-situ monitoring systems have been\ndeveloped to detect laser additive manufacturing (LAM) process anomalies and\ndefects. Multimodal fusion can improve in-situ monitoring performance by\nacquiring and integrating data from multiple modalities, including visual and\naudio data. However, multimodal fusion employs multiple sensors of different\ntypes, which leads to higher hardware, computational, and operational costs.\nThis paper proposes a cross-modality knowledge transfer (CMKT) methodology that\ntransfers knowledge from a source to a target modality for LAM in-situ\nmonitoring. CMKT enhances the usefulness of the features extracted from the\ntarget modality during the training phase and removes the sensors of the source\nmodality during the prediction phase. This paper proposes three CMKT methods:\nsemantic alignment, fully supervised mapping, and semi-supervised mapping.\nSemantic alignment establishes a shared encoded space between modalities to\nfacilitate knowledge transfer. It utilizes a semantic alignment loss to align\nthe distributions of the same classes (e.g., visual defective and audio\ndefective classes) and a separation loss to separate the distributions of\ndifferent classes (e.g., visual defective and audio defect-free classes). The\ntwo mapping methods transfer knowledge by deriving the features of one modality\nfrom the other modality using fully supervised and semi-supervised learning.\nThe proposed CMKT methods were implemented and compared with multimodal\naudio-visual fusion in an LAM in-situ anomaly detection case study. The\nsemantic alignment method achieves a 98.4% accuracy while removing the audio\nmodality during the prediction phase, which is comparable to the accuracy of\nmultimodal fusion (98.2%).\n","subjects":["Computing Research Repository/Computational Engineering, Finance, and Science","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}