{"id":"2408.02266","title":"One-Shot Collaborative Data Distillation","authors":"William Holland, Chandra Thapa, Sarah Ali Siddiqui, Wei Shao, and\n  Seyit Camtepe","authorsParsed":[["Holland","William",""],["Thapa","Chandra",""],["Siddiqui","Sarah Ali",""],["Shao","Wei",""],["Camtepe","Seyit",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 06:47:32 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 06:08:35 GMT"}],"updateDate":"2024-08-13","timestamp":1722840452000,"abstract":"  Large machine-learning training datasets can be distilled into small\ncollections of informative synthetic data samples. These synthetic sets support\nefficient model learning and reduce the communication cost of data sharing.\nThus, high-fidelity distilled data can support the efficient deployment of\nmachine learning applications in distributed network environments. A naive way\nto construct a synthetic set in a distributed environment is to allow each\nclient to perform local data distillation and to merge local distillations at a\ncentral server. However, the quality of the resulting set is impaired by\nheterogeneity in the distributions of the local data held by clients. To\novercome this challenge, we introduce the first collaborative data distillation\ntechnique, called CollabDM, which captures the global distribution of the data\nand requires only a single round of communication between client and server.\nOur method outperforms the state-of-the-art one-shot learning method on skewed\ndata in distributed learning environments. We also show the promising practical\nbenefits of our method when applied to attack detection in 5G networks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}