{"id":"2407.19198","title":"Towards the Dynamics of a DNN Learning Symbolic Interactions","authors":"Qihan Ren, Yang Xu, Junpeng Zhang, Yue Xin, Dongrui Liu, Quanshi Zhang","authorsParsed":[["Ren","Qihan",""],["Xu","Yang",""],["Zhang","Junpeng",""],["Xin","Yue",""],["Liu","Dongrui",""],["Zhang","Quanshi",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 07:34:49 GMT"}],"updateDate":"2024-07-30","timestamp":1722065689000,"abstract":"  This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, in recent years, a series of theorems have\nbeen proven to show that given an input sample, a small number of interactions\nbetween input variables can be considered as primitive inference patterns,\nwhich can faithfully represent every detailed inference logic of the DNN on\nthis sample. Particularly, it has been observed that various DNNs all learn\ninteractions of different complexities with two-phase dynamics, and this well\nexplains how a DNN's generalization power changes from under-fitting to\nover-fitting. Therefore, in this study, we prove the dynamics of a DNN\ngradually encoding interactions of different complexities, which provides a\ntheoretically grounded mechanism for the over-fitting of a DNN. Experiments\nshow that our theory well predicts the real learning dynamics of various DNNs\non different tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}