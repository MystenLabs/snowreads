{"id":"2407.18961","title":"MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains","authors":"Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu,\n  Shen Ma, Jiarui Lu, Xiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe zhang,\n  Karsten Ahnert, Vik Kamath, Mathias Berglund, Dominic Walsh, Tobias Gindele,\n  Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng Cao, Ruoming\n  Pang, Zirui Wang","authorsParsed":[["Yin","Guoli",""],["Bai","Haoping",""],["Ma","Shuang",""],["Nan","Feng",""],["Sun","Yanchao",""],["Xu","Zhaoyang",""],["Ma","Shen",""],["Lu","Jiarui",""],["Kong","Xiang",""],["Zhang","Aonan",""],["Yap","Dian Ang",""],["zhang","Yizhe",""],["Ahnert","Karsten",""],["Kamath","Vik",""],["Berglund","Mathias",""],["Walsh","Dominic",""],["Gindele","Tobias",""],["Wiest","Juergen",""],["Lai","Zhengfeng",""],["Wang","Xiaoming",""],["Shan","Jiulong",""],["Cao","Meng",""],["Pang","Ruoming",""],["Wang","Zirui",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 00:58:41 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 09:15:38 GMT"},{"version":"v3","created":"Thu, 15 Aug 2024 21:32:57 GMT"}],"updateDate":"2024-08-19","timestamp":1721264321000,"abstract":"  Recent advances in large language models (LLMs) have increased the demand for\ncomprehensive benchmarks to evaluate their capabilities as human-like agents.\nExisting benchmarks, while useful, often focus on specific application\nscenarios, emphasizing task completion but failing to dissect the underlying\nskills that drive these outcomes. This lack of granularity makes it difficult\nto deeply discern where failures stem from. Additionally, setting up these\nenvironments requires considerable effort, and issues of unreliability and\nreproducibility sometimes arise, especially in interactive tasks. To address\nthese limitations, we introduce the Massive Multitask Agent Understanding\n(MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need\nfor complex environment setups. It evaluates models across five domains,\nincluding Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine\nLearning coding, Contest-level programming and Mathematics, and covers five\nessential capabilities: Understanding, Reasoning, Planning, Problem-solving,\nand Self-correction. With a total of 20 meticulously designed tasks\nencompassing over 3K distinct prompts, MMAU provides a comprehensive framework\nfor evaluating the strengths and limitations of LLM agents. By testing 18\nrepresentative models on MMAU, we provide deep and insightful analyses.\nUltimately, MMAU not only sheds light on the capabilities and limitations of\nLLM agents but also enhances the interpretability of their performance.\nDatasets and evaluation scripts of MMAU are released at\nhttps://github.com/apple/axlearn/tree/main/docs/research/mmau.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"UmNlqi_pq5OtKsnfWgG0Fmn7glJwAtDxTvT93whV5IA","pdfSize":"5813409"}
