{"id":"2407.04659","title":"Simulation-based Calibration of Uncertainty Intervals under Approximate\n  Bayesian Estimation","authors":"Terrance D. Savitsky, Julie Gershunskaya","authorsParsed":[["Savitsky","Terrance D.",""],["Gershunskaya","Julie",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 17:15:34 GMT"}],"updateDate":"2024-07-08","timestamp":1720199734000,"abstract":"  The mean field variational Bayes (VB) algorithm implemented in Stan is\nrelatively fast and efficient, making it feasible to produce model-estimated\nofficial statistics on a rapid timeline. Yet, while consistent point estimates\nof parameters are achieved for continuous data models, the mean field\napproximation often produces inaccurate uncertainty quantification to the\nextent that parameters are correlated a posteriori. In this paper, we propose a\nsimulation procedure that calibrates uncertainty intervals for model parameters\nestimated under approximate algorithms to achieve nominal coverages. Our\nprocedure detects and corrects biased estimation of both first and second\nmoments of approximate marginal posterior distributions induced by any\nestimation algorithm that produces consistent first moments under specification\nof the correct model. The method generates replicate datasets using parameters\nestimated in an initial model run. The model is subsequently re-estimated on\neach replicate dataset, and we use the empirical distribution over the\nre-samples to formulate calibrated confidence intervals of parameter estimates\nof the initial model run that are guaranteed to asymptotically achieve nominal\ncoverage. We demonstrate the performance of our procedure in Monte Carlo\nsimulation study and apply it to real data from the Current Employment\nStatistics survey.\n","subjects":["Statistics/Methodology","Statistics/Applications"],"license":"http://creativecommons.org/licenses/by/4.0/"}