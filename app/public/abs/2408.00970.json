{"id":"2408.00970","title":"Multimodal Fusion via Hypergraph Autoencoder and Contrastive Learning\n  for Emotion Recognition in Conversation","authors":"Zijian Yi, Ziming Zhao, Zhishu Shen, Tiehua Zhang","authorsParsed":[["Yi","Zijian",""],["Zhao","Ziming",""],["Shen","Zhishu",""],["Zhang","Tiehua",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 01:30:18 GMT"}],"updateDate":"2024-08-05","timestamp":1722562218000,"abstract":"  Multimodal emotion recognition in conversation (MERC) seeks to identify the\nspeakers' emotions expressed in each utterance, offering significant potential\nacross diverse fields. The challenge of MERC lies in balancing speaker modeling\nand context modeling, encompassing both long-distance and short-distance\ncontexts, as well as addressing the complexity of multimodal information\nfusion. Recent research adopts graph-based methods to model intricate\nconversational relationships effectively. Nevertheless, the majority of these\nmethods utilize a fixed fully connected structure to link all utterances,\nrelying on convolution to interpret complex context. This approach can\ninherently heighten the redundancy in contextual messages and excessive graph\nnetwork smoothing, particularly in the context of long-distance conversations.\nTo address this issue, we propose a framework that dynamically adjusts\nhypergraph connections by variational hypergraph autoencoder (VHGAE), and\nemploys contrastive learning to mitigate uncertainty factors during the\nreconstruction process. Experimental results demonstrate the effectiveness of\nour proposal against the state-of-the-art methods on IEMOCAP and MELD datasets.\nWe release the code to support the reproducibility of this work at\nhttps://github.com/yzjred/-HAUCL.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}