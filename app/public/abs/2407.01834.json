{"id":"2407.01834","title":"A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf\n  Affect-related Tweet Classifiers","authors":"Valentin Barriere, Sebastian Cifuentes","authorsParsed":[["Barriere","Valentin",""],["Cifuentes","Sebastian",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 22:17:17 GMT"}],"updateDate":"2024-07-03","timestamp":1719872237000,"abstract":"  In this paper, we apply a method to quantify biases associated with named\nentities from various countries. We create counterfactual examples with small\nperturbations on target-domain data instead of relying on templates or specific\ndatasets for bias detection. On widely used classifiers for subjectivity\nanalysis, including sentiment, emotion, hate speech, and offensive text using\nTwitter data, our results demonstrate positive biases related to the language\nspoken in a country across all classifiers studied. Notably, the presence of\ncertain country names in a sentence can strongly influence predictions, up to a\n23\\% change in hate speech detection and up to a 60\\% change in the prediction\nof negative emotions such as anger. We hypothesize that these biases stem from\nthe training data of pre-trained language models (PLMs) and find correlations\nbetween affect predictions and PLMs likelihood in English and unknown languages\nlike Basque and Maori, revealing distinct patterns with exacerbate\ncorrelations. Further, we followed these correlations in-between counterfactual\nexamples from a same sentence to remove the syntactical component, uncovering\ninteresting results suggesting the impact of the pre-training data was more\nimportant for English-speaking-country names. Our anonymized code is\n[https://anonymous.4open.science/r/biases_ppl-576B/README.md](available here).\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HM7wUoCySExL2_8Zz0yV3tUcEkq6Im5j49hwAw0ExGk","pdfSize":"829907"}
