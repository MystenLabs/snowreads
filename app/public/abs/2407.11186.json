{"id":"2407.11186","title":"FarsInstruct: Empowering Large Language Models for Persian Instruction\n  Understanding","authors":"Hojjat Mokhtarabadi, Ziba Zamani, Abbas Maazallahi, Hossein Manshaei","authorsParsed":[["Mokhtarabadi","Hojjat",""],["Zamani","Ziba",""],["Maazallahi","Abbas",""],["Manshaei","Hossein",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 19:17:31 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 20:03:55 GMT"}],"updateDate":"2024-07-19","timestamp":1721071051000,"abstract":"  Instruction-tuned large language models, such as T0, have demonstrated\nremarkable capabilities in following instructions across various domains.\nHowever, their proficiency remains notably deficient in many low-resource\nlanguages. To address this challenge, we introduce FarsInstruct: a\ncomprehensive instruction dataset designed to enhance the instruction-following\nability of large language models specifically for the Persian language, a\nsignificant yet underrepresented language globally. FarsInstruct encompasses a\nwide range of task types and datasets, each containing a mix of straightforward\nto complex manual written instructions, as well as translations from Public\nPool of Prompts, ensuring a rich linguistic and cultural representation.\nFurthermore, we introduce Co-CoLA, a framework designed to enhance the\nmulti-task adaptability of LoRA-tuned models. Through extensive experimental\nanalyses, our study showcases the effectiveness of FarsInstruct dataset coupled\nwith training by Co-CoLA framework, in improving the performance of large\nlanguage models within the Persian context. As of the current writing,\nFarsInstruct comprises more than 200 templates across 21 distinct datasets, and\nwe intend to update it consistently, thus augmenting its applicability.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}