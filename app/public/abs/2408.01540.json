{"id":"2408.01540","title":"Monotonic warpings for additive and deep Gaussian processes","authors":"Steven D. Barnett, Lauren J. Beesley, Annie S. Booth, Robert B.\n  Gramacy, and Dave Osthus","authorsParsed":[["Barnett","Steven D.",""],["Beesley","Lauren J.",""],["Booth","Annie S.",""],["Gramacy","Robert B.",""],["Osthus","Dave",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 19:01:35 GMT"}],"updateDate":"2024-08-06","timestamp":1722625295000,"abstract":"  Gaussian processes (GPs) are canonical as surrogates for computer experiments\nbecause they enjoy a degree of analytic tractability. But that breaks when the\nresponse surface is constrained, say to be monotonic. Here, we provide a\nmono-GP construction for a single input that is highly efficient even though\nthe calculations are non-analytic. Key ingredients include transformation of a\nreference process and elliptical slice sampling. We then show how mono-GP may\nbe deployed effectively in two ways. One is additive, extending monotonicity to\nmore inputs; the other is as a prior on injective latent warping variables in a\ndeep Gaussian process for (non-monotonic, multi-input) non-stationary surrogate\nmodeling. We provide illustrative and benchmarking examples throughout, showing\nthat our methods yield improved performance over the state-of-the-art on\nexamples from those two classes of problems.\n","subjects":["Statistics/Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}