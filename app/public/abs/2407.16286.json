{"id":"2407.16286","title":"A deeper look at depth pruning of LLMs","authors":"Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan\n  Kautz, David Krueger, Pavlo Molchanov","authorsParsed":[["Siddiqui","Shoaib Ahmed",""],["Dong","Xin",""],["Heinrich","Greg",""],["Breuel","Thomas",""],["Kautz","Jan",""],["Krueger","David",""],["Molchanov","Pavlo",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 08:40:27 GMT"}],"updateDate":"2024-07-24","timestamp":1721724027000,"abstract":"  Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"iHUIMdWv4i7JiKPW4d7uFqIeEDUU1YkHO5uvig6He2g","pdfSize":"1912476","objectId":"0x6cd7741830103363751e8b1302668b4a8d3aba21219a2212bc5f0efc53f0e61f","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
