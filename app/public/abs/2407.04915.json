{"id":"2407.04915","title":"Safe Generative Chats in a WhatsApp Intelligent Tutoring System","authors":"Zachary Levonian, Owen Henkel","authorsParsed":[["Levonian","Zachary",""],["Henkel","Owen",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 01:49:25 GMT"}],"updateDate":"2024-07-09","timestamp":1720230565000,"abstract":"  Large language models (LLMs) are flexible, personalizable, and available,\nwhich makes their use within Intelligent Tutoring Systems (ITSs) appealing.\nHowever, that flexibility creates risks: inaccuracies, harmful content, and\nnon-curricular material. Ethically deploying LLM-backed ITS systems requires\ndesigning safeguards that ensure positive experiences for students. We describe\nthe design of a conversational system integrated into an ITS, and our\nexperience evaluating its safety with red-teaming, an in-classroom usability\ntest, and field deployment. We present empirical data from more than 8,000\nstudent conversations with this system, finding that GPT-3.5 rarely generates\ninappropriate messages. Comparatively more common is inappropriate messages\nfrom students, which prompts us to reason about safeguarding as a content\nmoderation and classroom management problem. The student interaction behaviors\nwe observe provide implications for designers - to focus on student inputs as a\ncontent moderation problem - and implications for researchers - to focus on\nsubtle forms of bad content.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}