{"id":"2407.11681","title":"MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models","authors":"Hongrong Cheng and Miao Zhang and Javen Qinfeng Shi","authorsParsed":[["Cheng","Hongrong",""],["Zhang","Miao",""],["Shi","Javen Qinfeng",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 12:59:44 GMT"}],"updateDate":"2024-07-17","timestamp":1721134784000,"abstract":"  As Large Language Models (LLMs) grow dramatically in size, there is an\nincreasing trend in compressing and speeding up these models. Previous studies\nhave highlighted the usefulness of gradients for importance scoring in neural\nnetwork compressing, especially in pruning medium-size networks. However, the\nsubstantial memory requirements involved in calculating gradients with\nbackpropagation impede the utilization of gradients in guiding LLM pruning. As\na result, most pruning strategies for LLMs rely on gradient-free criteria, such\nas weight magnitudes or a mix of magnitudes and activations. In this paper, we\ndevise a hybrid pruning criterion, which appropriately integrates magnitude,\nactivation, and gradient to capitalize on feature map sensitivity for pruning\nLLMs. To overcome memory requirement barriers, we estimate gradients using only\nforward passes. Based on this, we propose a Memory-effIcieNt structured prunIng\nprocedure for LLMs (MINI-LLM) to remove no-critical channels and\nmulti-attention heads. Experimental results demonstrate the superior\nperformance of MINI-LLM over existing gradient-free methods on three LLMs:\nLLaMA, BLOOM, and OPT across various downstream tasks (classification,\nmultiple-choice, and generation), while MINI-LLM maintains a GPU memory\nfootprint akin to gradient-free methods.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}