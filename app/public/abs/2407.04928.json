{"id":"2407.04928","title":"CLIPVQA:Video Quality Assessment via CLIP","authors":"Fengchuang Xing, Mingjie Li, Yuan-Gen Wang, Guopu Zhu, and Xiaochun\n  Cao","authorsParsed":[["Xing","Fengchuang",""],["Li","Mingjie",""],["Wang","Yuan-Gen",""],["Zhu","Guopu",""],["Cao","Xiaochun",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 02:32:28 GMT"}],"updateDate":"2024-07-09","timestamp":1720233148000,"abstract":"  In learning vision-language representations from web-scale data, the\ncontrastive language-image pre-training (CLIP) mechanism has demonstrated a\nremarkable performance in many vision tasks. However, its application to the\nwidely studied video quality assessment (VQA) task is still an open issue. In\nthis paper, we propose an efficient and effective CLIP-based Transformer method\nfor the VQA problem (CLIPVQA). Specifically, we first design an effective video\nframe perception paradigm with the goal of extracting the rich spatiotemporal\nquality and content information among video frames. Then, the spatiotemporal\nquality features are adequately integrated together using a self-attention\nmechanism to yield video-level quality representation. To utilize the quality\nlanguage descriptions of videos for supervision, we develop a CLIP-based\nencoder for language embedding, which is then fully aggregated with the\ngenerated content information via a cross-attention module for producing\nvideo-language representation. Finally, the video-level quality and\nvideo-language representations are fused together for final video quality\nprediction, where a vectorized regression loss is employed for efficient\nend-to-end optimization. Comprehensive experiments are conducted on eight\nin-the-wild video datasets with diverse resolutions to evaluate the performance\nof CLIPVQA. The experimental results show that the proposed CLIPVQA achieves\nnew state-of-the-art VQA performance and up to 37% better generalizability than\nexisting benchmark VQA methods. A series of ablation studies are also performed\nto validate the effectiveness of each module in CLIPVQA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}