{"id":"2407.18010","title":"Stochastic Games with Minimally Bounded Action Costs","authors":"David Mguni","authorsParsed":[["Mguni","David",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 13:04:49 GMT"},{"version":"v2","created":"Thu, 1 Aug 2024 12:21:27 GMT"}],"updateDate":"2024-08-02","timestamp":1721912689000,"abstract":"  In many multi-player interactions, players incur strictly positive costs each\ntime they execute actions e.g. 'menu costs' or transaction costs in financial\nsystems. Since acting at each available opportunity would accumulate\nprohibitively large costs, the resulting decision problem is one in which\nplayers must make strategic decisions about when to execute actions in addition\nto their choice of action. This paper analyses a discrete-time stochastic game\n(SG) in which players face minimally bounded positive costs for each action and\ninfluence the system using impulse controls. We prove SGs of two-sided impulse\ncontrol have a unique value and characterise the saddle point equilibrium in\nwhich the players execute actions at strategically chosen times in accordance\nwith Markovian strategies. We prove the game respects a dynamic programming\nprinciple and that the Markov perfect equilibrium can be computed as a limit\npoint of a sequence of Bellman operations. We then introduce a new Q-learning\nvariant which we show converges almost surely to the value of the game enabling\nsolutions to be extracted in unknown settings. Lastly, we extend our results to\nsettings with budgetory constraints.\n","subjects":["Computing Research Repository/Multiagent Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}