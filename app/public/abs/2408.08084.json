{"id":"2408.08084","title":"An Efficient Replay for Class-Incremental Learning with Pre-trained\n  Models","authors":"Weimin Yin and Bin Chen adn Chunzhao Xie and Zhenhao Tan","authorsParsed":[["Yin","Weimin",""],["Xie","Bin Chen adn Chunzhao",""],["Tan","Zhenhao",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 11:26:28 GMT"}],"updateDate":"2024-08-16","timestamp":1723721188000,"abstract":"  In general class-incremental learning, researchers typically use sample sets\nas a tool to avoid catastrophic forgetting during continuous learning. At the\nsame time, researchers have also noted the differences between\nclass-incremental learning and Oracle training and have attempted to make\ncorrections. In recent years, researchers have begun to develop\nclass-incremental learning algorithms utilizing pre-trained models, achieving\nsignificant results. This paper observes that in class-incremental learning,\nthe steady state among the weight guided by each class center is disrupted,\nwhich is significantly correlated with catastrophic forgetting. Based on this,\nwe propose a new method to overcoming forgetting . In some cases, by retaining\nonly a single sample unit of each class in memory for replay and applying\nsimple gradient constraints, very good results can be achieved. Experimental\nresults indicate that under the condition of pre-trained models, our method can\nachieve competitive performance with very low computational cost and by simply\nusing the cross-entropy loss.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}