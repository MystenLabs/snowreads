{"id":"2407.03719","title":"Relative Difficulty Distillation for Semantic Segmentation","authors":"Dong Liang, Yue Sun, Yun Du, Songcan Chen, Sheng-Jun Huang","authorsParsed":[["Liang","Dong",""],["Sun","Yue",""],["Du","Yun",""],["Chen","Songcan",""],["Huang","Sheng-Jun",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 08:08:25 GMT"}],"updateDate":"2024-07-08","timestamp":1720080505000,"abstract":"  Current knowledge distillation (KD) methods primarily focus on transferring\nvarious structured knowledge and designing corresponding optimization goals to\nencourage the student network to imitate the output of the teacher network.\nHowever, introducing too many additional optimization objectives may lead to\nunstable training, such as gradient conflicts. Moreover, these methods ignored\nthe guidelines of relative learning difficulty between the teacher and student\nnetworks. Inspired by human cognitive science, in this paper, we redefine\nknowledge from a new perspective -- the student and teacher networks' relative\ndifficulty of samples, and propose a pixel-level KD paradigm for semantic\nsegmentation named Relative Difficulty Distillation (RDD). We propose a\ntwo-stage RDD framework: Teacher-Full Evaluated RDD (TFE-RDD) and\nTeacher-Student Evaluated RDD (TSE-RDD). RDD allows the teacher network to\nprovide effective guidance on learning focus without additional optimization\ngoals, thus avoiding adjusting learning weights for multiple losses. Extensive\nexperimental evaluations using a general distillation loss function on popular\ndatasets such as Cityscapes, CamVid, Pascal VOC, and ADE20k demonstrate the\neffectiveness of RDD against state-of-the-art KD methods. Additionally, our\nresearch showcases that RDD can integrate with existing KD methods to improve\ntheir upper performance bound.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}