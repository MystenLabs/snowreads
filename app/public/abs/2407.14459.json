{"id":"2407.14459","title":"PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer","authors":"Jiahong Ma, Mingguo He, Zhewei Wei","authorsParsed":[["Ma","Jiahong",""],["He","Mingguo",""],["Wei","Zhewei",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:01:41 GMT"}],"updateDate":"2024-07-22","timestamp":1721408501000,"abstract":"  Spectral Graph Neural Networks have demonstrated superior performance in\ngraph representation learning. However, many current methods focus on employing\nshared polynomial coefficients for all nodes, i.e., learning node-unified\nfilters, which limits the filters' flexibility for node-level tasks. The recent\nDSF attempts to overcome this limitation by learning node-wise coefficients\nbased on positional encoding. However, the initialization and updating process\nof the positional encoding are burdensome, hindering scalability on large-scale\ngraphs. In this work, we propose a scalable node-wise filter, PolyAttn.\nLeveraging the attention mechanism, PolyAttn can directly learn node-wise\nfilters in an efficient manner, offering powerful representation capabilities.\nBuilding on PolyAttn, we introduce the whole model, named PolyFormer. In the\nlens of Graph Transformer models, PolyFormer, which calculates attention scores\nwithin nodes, shows great scalability. Moreover, the model captures spectral\ninformation, enhancing expressiveness while maintaining efficiency. With these\nadvantages, PolyFormer offers a desirable balance between scalability and\nexpressiveness for node-level tasks. Extensive experiments demonstrate that our\nproposed methods excel at learning arbitrary node-wise filters, showing\nsuperior performance on both homophilic and heterophilic graphs, and handling\ngraphs containing up to 100 million nodes. The code is available at\nhttps://github.com/air029/PolyFormer.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}