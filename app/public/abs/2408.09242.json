{"id":"2408.09242","title":"Learning to Optimally Stop Diffusion Processes, with Financial\n  Applications","authors":"Min Dai, Yu Sun, Zuo Quan Xu, Xun Yu Zhou","authorsParsed":[["Dai","Min",""],["Sun","Yu",""],["Xu","Zuo Quan",""],["Zhou","Xun Yu",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 16:27:19 GMT"},{"version":"v2","created":"Sun, 8 Sep 2024 13:02:44 GMT"}],"updateDate":"2024-09-10","timestamp":1723912039000,"abstract":"  We study optimal stopping for diffusion processes with unknown model\nprimitives within the continuous-time reinforcement learning (RL) framework\ndeveloped by Wang et al. (2020), and present applications to option pricing and\nportfolio choice. By penalizing the corresponding variational inequality\nformulation, we transform the stopping problem into a stochastic optimal\ncontrol problem with two actions. We then randomize controls into Bernoulli\ndistributions and add an entropy regularizer to encourage exploration. We\nderive a semi-analytical optimal Bernoulli distribution, based on which we\ndevise RL algorithms using the martingale approach established in Jia and Zhou\n(2022a), and prove a policy improvement theorem. We demonstrate the\neffectiveness of the algorithms in pricing finite-horizon American put options\nand in solving Merton's problem with transaction costs, and show that both the\noffline and online algorithms achieve high accuracy in learning the value\nfunctions and characterizing the associated free boundaries.\n","subjects":["Mathematics/Optimization and Control","Quantitative Finance/Mathematical Finance","Quantitative Finance/Pricing of Securities"],"license":"http://creativecommons.org/licenses/by/4.0/"}