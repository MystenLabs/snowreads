{"id":"2408.06931","title":"The advantages of context specific language models: the case of the\n  Erasmian Language Model","authors":"Jo\\~ao Gon\\c{c}alves, Nick Jelicic, Michele Murgia, Evert Stamhuis","authorsParsed":[["Gonçalves","João",""],["Jelicic","Nick",""],["Murgia","Michele",""],["Stamhuis","Evert",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 14:34:59 GMT"}],"updateDate":"2024-08-14","timestamp":1723559699000,"abstract":"  The current trend to improve language model performance seems to be based on\nscaling up with the number of parameters (e.g. the state of the art GPT4 model\nhas approximately 1.7 trillion parameters) or the amount of training data fed\ninto the model. However this comes at significant costs in terms of\ncomputational resources and energy costs that compromise the sustainability of\nAI solutions, as well as risk relating to privacy and misuse. In this paper we\npresent the Erasmian Language Model (ELM) a small context specific, 900 million\nparameter model, pre-trained and fine-tuned by and for Erasmus University\nRotterdam. We show how the model performs adequately in a classroom context for\nessay writing, and how it achieves superior performance in subjects that are\npart of its context. This has implications for a wide range of institutions and\norganizations, showing that context specific language models may be a viable\nalternative for resource constrained, privacy sensitive use cases.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}