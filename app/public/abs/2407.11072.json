{"id":"2407.11072","title":"MaPPing Your Model: Assessing the Impact of Adversarial Attacks on\n  LLM-based Programming Assistants","authors":"John Heibel, Daniel Lowd","authorsParsed":[["Heibel","John",""],["Lowd","Daniel",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 22:30:35 GMT"}],"updateDate":"2024-07-17","timestamp":1720823435000,"abstract":"  LLM-based programming assistants offer the promise of programming faster but\nwith the risk of introducing more security vulnerabilities. Prior work has\nstudied how LLMs could be maliciously fine-tuned to suggest vulnerabilities\nmore often. With the rise of agentic LLMs, which may use results from an\nuntrusted third party, there is a growing risk of attacks on the model's\nprompt. We introduce the Malicious Programming Prompt (MaPP) attack, in which\nan attacker adds a small amount of text to a prompt for a programming task\n(under 500 bytes). We show that our prompt strategy can cause an LLM to add\nvulnerabilities while continuing to write otherwise correct code. We evaluate\nthree prompts on seven common LLMs, from basic to state-of-the-art commercial\nmodels. Using the HumanEval benchmark, we find that our prompts are broadly\neffective, with no customization required for different LLMs. Furthermore, the\nLLMs that are best at HumanEval are also best at following our malicious\ninstructions, suggesting that simply scaling language models will not prevent\nMaPP attacks. Using a dataset of eight CWEs in 16 scenarios, we find that MaPP\nattacks are also effective at implementing specific and targeted\nvulnerabilities across a range of models. Our work highlights the need to\nsecure LLM prompts against manipulation as well as rigorously auditing code\ngenerated with the help of LLMs.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}