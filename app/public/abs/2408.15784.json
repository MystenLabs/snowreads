{"id":"2408.15784","title":"Implicit Regularization Paths of Weighted Neural Representations","authors":"Jin-Hong Du, Pratik Patil","authorsParsed":[["Du","Jin-Hong",""],["Patil","Pratik",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 13:26:36 GMT"}],"updateDate":"2024-08-29","timestamp":1724851596000,"abstract":"  We study the implicit regularization effects induced by (observation)\nweighting of pretrained features. For weight and feature matrices of bounded\noperator norms that are infinitesimally free with respect to (normalized) trace\nfunctionals, we derive equivalence paths connecting different weighting\nmatrices and ridge regularization levels. Specifically, we show that ridge\nestimators trained on weighted features along the same path are asymptotically\nequivalent when evaluated against test vectors of bounded norms. These paths\ncan be interpreted as matching the effective degrees of freedom of ridge\nestimators fitted with weighted features. For the special case of subsampling\nwithout replacement, our results apply to independently sampled random features\nand kernel features and confirm recent conjectures (Conjectures 7 and 8) of the\nauthors on the existence of such paths in Patil et al. We also present an\nadditive risk decomposition for ensembles of weighted estimators and show that\nthe risks are equivalent along the paths when the ensemble size goes to\ninfinity. As a practical consequence of the path equivalences, we develop an\nefficient cross-validation method for tuning and apply it to subsampled\npretrained representations across several models (e.g., ResNet-50) and datasets\n(e.g., CIFAR-100).\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}