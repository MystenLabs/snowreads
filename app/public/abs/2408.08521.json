{"id":"2408.08521","title":"MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement\n  Framework for Multimodal Question Answering","authors":"Zhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree Harsha, Loic Feujio,\n  Akash Maharaj, Yunyao Li","authorsParsed":[["Zhu","Zhengyuan",""],["Lee","Daniel",""],["Zhang","Hong",""],["Harsha","Sai Sree",""],["Feujio","Loic",""],["Maharaj","Akash",""],["Li","Yunyao",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 04:32:10 GMT"}],"updateDate":"2024-08-19","timestamp":1723782730000,"abstract":"  Recent advancements in retrieval-augmented generation (RAG) have demonstrated\nimpressive performance in the question-answering (QA) task. However, most\nprevious works predominantly focus on text-based answers. While some studies\naddress multimodal data, they still fall short in generating comprehensive\nmultimodal answers, particularly for explaining concepts or providing\nstep-by-step tutorials on how to accomplish specific goals. This capability is\nespecially valuable for applications such as enterprise chatbots and settings\nsuch as customer service and educational systems, where the answers are sourced\nfrom multimodal data. In this paper, we introduce a simple and effective\nframework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR\nenhances text-based answers by retrieving relevant multimodal data and refining\nthe responses to create coherent multimodal answers. This framework can be\neasily extended to support multimodal answers in enterprise chatbots with\nminimal modifications. Human evaluation results indicate that multimodal\nanswers generated by MuRAR are more useful and readable compared to plain text\nanswers.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}