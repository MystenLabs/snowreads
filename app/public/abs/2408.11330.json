{"id":"2408.11330","title":"Design Principle Transfer in Neural Architecture Search via Large\n  Language Models","authors":"Xun Zhou, Liang Feng, Xingyu Wu, Zhichao Lu, Kay Chen Tan","authorsParsed":[["Zhou","Xun",""],["Feng","Liang",""],["Wu","Xingyu",""],["Lu","Zhichao",""],["Tan","Kay Chen",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 04:27:44 GMT"}],"updateDate":"2024-08-22","timestamp":1724214464000,"abstract":"  Transferable neural architecture search (TNAS) has been introduced to design\nefficient neural architectures for multiple tasks, to enhance the practical\napplicability of NAS in real-world scenarios. In TNAS, architectural knowledge\naccumulated in previous search processes is reused to warm up the architecture\nsearch for new tasks. However, existing TNAS methods still search in an\nextensive search space, necessitating the evaluation of numerous architectures.\nTo overcome this challenge, this work proposes a novel transfer paradigm, i.e.,\ndesign principle transfer. In this work, the linguistic description of various\nstructural components' effects on architectural performance is termed design\nprinciples. They are learned from established architectures and then can be\nreused to reduce the search space by discarding unpromising architectures.\nSearching in the refined search space can boost both the search performance and\nefficiency for new NAS tasks. To this end, a large language model\n(LLM)-assisted design principle transfer (LAPT) framework is devised. In LAPT,\nLLM is applied to automatically reason the design principles from a set of\ngiven architectures, and then a principle adaptation method is applied to\nrefine these principles progressively based on the new search results.\nExperimental results show that LAPT can beat the state-of-the-art TNAS methods\non most tasks and achieve comparable performance on others.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}