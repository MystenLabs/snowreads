{"id":"2407.04996","title":"The Solution for the sequential task continual learning track of the 2nd\n  Greater Bay Area International Algorithm Competition","authors":"Sishun Pan, Xixian Wu, Tingmin Li, Longfei Huang, Mingxu Feng,\n  Zhonghua Wan, Yang Yang","authorsParsed":[["Pan","Sishun",""],["Wu","Xixian",""],["Li","Tingmin",""],["Huang","Longfei",""],["Feng","Mingxu",""],["Wan","Zhonghua",""],["Yang","Yang",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 08:21:29 GMT"}],"updateDate":"2024-07-09","timestamp":1720254089000,"abstract":"  This paper presents a data-free, parameter-isolation-based continual learning\nalgorithm we developed for the sequential task continual learning track of the\n2nd Greater Bay Area International Algorithm Competition. The method learns an\nindependent parameter subspace for each task within the network's convolutional\nand linear layers and freezes the batch normalization layers after the first\ntask. Specifically, for domain incremental setting where all domains share a\nclassification head, we freeze the shared classification head after first task\nis completed, effectively solving the issue of catastrophic forgetting.\nAdditionally, facing the challenge of domain incremental settings without\nproviding a task identity, we designed an inference task identity strategy,\nselecting an appropriate mask matrix for each sample. Furthermore, we\nintroduced a gradient supplementation strategy to enhance the importance of\nunselected parameters for the current task, facilitating learning for new\ntasks. We also implemented an adaptive importance scoring strategy that\ndynamically adjusts the amount of parameters to optimize single-task\nperformance while reducing parameter usage. Moreover, considering the\nlimitations of storage space and inference time, we designed a mask matrix\ncompression strategy to save storage space and improve the speed of encryption\nand decryption of the mask matrix. Our approach does not require expanding the\ncore network or using external auxiliary networks or data, and performs well\nunder both task incremental and domain incremental settings. This solution\nultimately won a second-place prize in the competition.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}