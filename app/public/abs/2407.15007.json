{"id":"2407.15007","title":"Is Behavior Cloning All You Need? Understanding Horizon in Imitation\n  Learning","authors":"Dylan J. Foster and Adam Block and Dipendra Misra","authorsParsed":[["Foster","Dylan J.",""],["Block","Adam",""],["Misra","Dipendra",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 23:31:56 GMT"}],"updateDate":"2024-07-23","timestamp":1721518316000,"abstract":"  Imitation learning (IL) aims to mimic the behavior of an expert in a\nsequential decision making task by learning from demonstrations, and has been\nwidely applied to robotics, autonomous driving, and autoregressive text\ngeneration. The simplest approach to IL, behavior cloning (BC), is thought to\nincur sample complexity with unfavorable quadratic dependence on the problem\nhorizon, motivating a variety of different online algorithms that attain\nimproved linear horizon dependence under stronger assumptions on the data and\nthe learner's access to the expert.\n  We revisit the apparent gap between offline and online IL from a\nlearning-theoretic perspective, with a focus on general policy classes up to\nand including deep neural networks. Through a new analysis of behavior cloning\nwith the logarithmic loss, we show that it is possible to achieve\nhorizon-independent sample complexity in offline IL whenever (i) the range of\nthe cumulative payoffs is controlled, and (ii) an appropriate notion of\nsupervised learning complexity for the policy class is controlled. Specializing\nour results to deterministic, stationary policies, we show that the gap between\noffline and online IL is not fundamental: (i) it is possible to achieve linear\ndependence on horizon in offline IL under dense rewards (matching what was\npreviously only known to be achievable in online IL); and (ii) without further\nassumptions on the policy class, online IL cannot improve over offline IL with\nthe logarithmic loss, even in benign MDPs. We complement our theoretical\nresults with experiments on standard RL tasks and autoregressive language\ngeneration to validate the practical relevance of our findings.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}