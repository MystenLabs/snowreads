{"id":"2407.15498","title":"Refining Corpora from a Model Calibration Perspective for Chinese\n  Spelling Correction","authors":"Dingyao Yu, Yang An, Wei Ye, Xiongfeng Xiao, Shaoguang Mao, Tao Ge,\n  Shikun Zhang","authorsParsed":[["Yu","Dingyao",""],["An","Yang",""],["Ye","Wei",""],["Xiao","Xiongfeng",""],["Mao","Shaoguang",""],["Ge","Tao",""],["Zhang","Shikun",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 09:26:35 GMT"}],"updateDate":"2024-07-23","timestamp":1721640395000,"abstract":"  Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality\ncorpora, due to the labor-intensive labeling of spelling errors in real-life\nhuman writing or typing scenarios. Two data augmentation methods are widely\nadopted: (1) \\textit{Random Replacement} with the guidance of confusion sets\nand (2) \\textit{OCR/ASR-based Generation} that simulates character misusing.\nHowever, both methods inevitably introduce noisy data (e.g., false spelling\nerrors), potentially leading to over-correction. By carefully analyzing the two\ntypes of corpora, we find that though the latter achieves more robust\ngeneralization performance, the former yields better-calibrated CSC models. We\nthen provide a theoretical analysis of this empirical observation, based on\nwhich a corpus refining strategy is proposed. Specifically, OCR/ASR-based data\nsamples are fed into a well-calibrated CSC model trained on random\nreplacement-based corpora and then filtered based on prediction confidence. By\nlearning a simple BERT-based model on the refined OCR/ASR-based corpus, we set\nup impressive state-of-the-art performance on three widely-used benchmarks,\nwhile significantly alleviating over-correction (e.g., lowering false positive\npredictions).\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}