{"id":"2408.13704","title":"DHP Benchmark: Are LLMs Good NLG Evaluators?","authors":"Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu,\n  Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, Xia Hu","authorsParsed":[["Wang","Yicheng",""],["Yuan","Jiayi",""],["Chuang","Yu-Neng",""],["Wang","Zhuoer",""],["Liu","Yingchi",""],["Cusick","Mark",""],["Kulkarni","Param",""],["Ji","Zhengping",""],["Ibrahim","Yasser",""],["Hu","Xia",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 02:01:38 GMT"}],"updateDate":"2024-08-27","timestamp":1724551298000,"abstract":"  Large Language Models (LLMs) are increasingly serving as evaluators in\nNatural Language Generation (NLG) tasks. However, the capabilities of LLMs in\nscoring NLG quality remain inadequately explored. Current studies depend on\nhuman assessments and simple metrics that fail to capture the discernment of\nLLMs across diverse NLG tasks. To address this gap, we propose the Discernment\nof Hierarchical Perturbation (DHP) benchmarking framework, which provides\nquantitative discernment scores for LLMs utilizing hierarchically perturbed\ntext data and statistical tests to measure the NLG evaluation capabilities of\nLLMs systematically. We have re-established six evaluation datasets for this\nbenchmark, covering four NLG tasks: Summarization, Story Completion, Question\nAnswering, and Translation. Our comprehensive benchmarking of five major LLM\nseries provides critical insight into their strengths and limitations as NLG\nevaluators.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}