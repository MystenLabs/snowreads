{"id":"2408.04671","title":"Prompt and Prejudice","authors":"Lorenzo Berlincioni, Luca Cultrera, Federico Becattini, Marco Bertini,\n  Alberto Del Bimbo","authorsParsed":[["Berlincioni","Lorenzo",""],["Cultrera","Luca",""],["Becattini","Federico",""],["Bertini","Marco",""],["Del Bimbo","Alberto",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 14:11:33 GMT"}],"updateDate":"2024-08-12","timestamp":1723039893000,"abstract":"  This paper investigates the impact of using first names in Large Language\nModels (LLMs) and Vision Language Models (VLMs), particularly when prompted\nwith ethical decision-making tasks. We propose an approach that appends first\nnames to ethically annotated text scenarios to reveal demographic biases in\nmodel outputs. Our study involves a curated list of more than 300 names\nrepresenting diverse genders and ethnic backgrounds, tested across thousands of\nmoral scenarios. Following the auditing methodologies from social sciences we\npropose a detailed analysis involving popular LLMs/VLMs to contribute to the\nfield of responsible AI by emphasizing the importance of recognizing and\nmitigating biases in these systems. Furthermore, we introduce a novel\nbenchmark, the Pratical Scenarios Benchmark (PSB), designed to assess the\npresence of biases involving gender or demographic prejudices in everyday\ndecision-making scenarios as well as practical scenarios where an LLM might be\nused to make sensible decisions (e.g., granting mortgages or insurances). This\nbenchmark allows for a comprehensive comparison of model behaviors across\ndifferent demographic categories, highlighting the risks and biases that may\narise in practical applications of LLMs and VLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/"}