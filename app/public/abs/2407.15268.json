{"id":"2407.15268","title":"Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical\n  Radiology Report Generation","authors":"Liwen Sun, James Zhao, Megan Han, Chenyan Xiong","authorsParsed":[["Sun","Liwen",""],["Zhao","James",""],["Han","Megan",""],["Xiong","Chenyan",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 21:04:28 GMT"}],"updateDate":"2024-07-23","timestamp":1721595868000,"abstract":"  Multimodal foundation models hold significant potential for automating\nradiology report generation, thereby assisting clinicians in diagnosing cardiac\ndiseases. However, generated reports often suffer from serious factual\ninaccuracy. In this paper, we introduce a fact-aware multimodal\nretrieval-augmented pipeline in generating accurate radiology reports\n(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then\nintegrate factual knowledge to train a universal multimodal retriever. Given a\nradiology image, our retriever can identify high-quality reference reports to\naugment multimodal foundation models, thus enhancing the factual completeness\nand correctness of report generation. Experiments on two benchmark datasets\nshow that our multimodal retriever outperforms state-of-the-art retrievers on\nboth language generation and radiology-specific metrics, up to 6.5% and 2%\nscore in F1CheXbert and F1RadGraph. Further analysis indicates that employing\nour factually-informed training strategy imposes an effective supervision\nsignal, without relying on explicit diagnostic label guidance, and successfully\npropagates fact-aware capabilities from the multimodal retriever to the\nmultimodal foundation model in radiology report generation.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}