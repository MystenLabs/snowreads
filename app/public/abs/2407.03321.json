{"id":"2407.03321","title":"Planetarium: A Rigorous Benchmark for Translating Text to Structured\n  Planning Languages","authors":"Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael L. Littman,\n  Stephen H. Bach","authorsParsed":[["Zuo","Max",""],["Velez","Francisco Piedrahita",""],["Li","Xiaochen",""],["Littman","Michael L.",""],["Bach","Stephen H.",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 17:59:53 GMT"}],"updateDate":"2024-07-04","timestamp":1720029593000,"abstract":"  Many recent works have explored using language models for planning problems.\nOne line of research focuses on translating natural language descriptions of\nplanning tasks into structured planning languages, such as the planning domain\ndefinition language (PDDL). While this approach is promising, accurately\nmeasuring the quality of generated PDDL code continues to pose significant\nchallenges. First, generated PDDL code is typically evaluated using planning\nvalidators that check whether the problem can be solved with a planner. This\nmethod is insufficient because a language model might generate valid PDDL code\nthat does not align with the natural language description of the task. Second,\nexisting evaluation sets often have natural language descriptions of the\nplanning task that closely resemble the ground truth PDDL, reducing the\nchallenge of the task. To bridge this gap, we introduce \\benchmarkName, a\nbenchmark designed to evaluate language models' ability to generate PDDL code\nfrom natural language descriptions of planning tasks. We begin by creating a\nPDDL equivalence algorithm that rigorously evaluates the correctness of PDDL\ncode generated by language models by flexibly comparing it against a ground\ntruth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across\n13 different tasks, with varying levels of difficulty. Finally, we evaluate\nseveral API-access and open-weight language models that reveal this task's\ncomplexity. For example, $87.6\\%$ of the PDDL problem descriptions generated by\nGPT-4o are syntactically parseable, $82.2\\%$ are valid, solve-able problems,\nbut only $35.1\\%$ are semantically correct, highlighting the need for a more\nrigorous benchmark for this problem.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}