{"id":"2407.16366","title":"Robust Bayesian Model Averaging for Linear Regression Models With\n  Heavy-Tailed Errors","authors":"Shamriddha De and Joyee Ghosh","authorsParsed":[["De","Shamriddha",""],["Ghosh","Joyee",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 10:20:20 GMT"}],"updateDate":"2024-07-24","timestamp":1721730020000,"abstract":"  In this article, our goal is to develop a method for Bayesian model averaging\nin linear regression models to accommodate heavier tailed error distributions\nthan the normal distribution. Motivated by the use of the Huber loss function\nin presence of outliers, Park and Casella (2008) proposed the concept of the\nBayesian Huberized lasso, which has been recently developed and implemented by\nKawakami and Hashimoto (2023), with hyperbolic errors. Because the Huberized\nlasso cannot enforce regression coefficients to be exactly zero, we propose a\nfully Bayesian variable selection approach with spike and slab priors, that can\naddress sparsity more effectively. Furthermore, while the hyperbolic\ndistribution has heavier tails than a normal distribution, its tails are less\nheavy in comparison to a Cauchy distribution.Thus, we propose a regression\nmodel, with an error distribution that encompasses both hyperbolic and\nStudent-t distributions. Our model aims to capture the benefit of using Huber\nloss, but it can also adapt to heavier tails, and unknown levels of sparsity,\nas necessitated by the data. We develop an efficient Gibbs sampler with\nMetropolis Hastings steps for posterior computation. Through simulation\nstudies, and analyses of the benchmark Boston housing dataset and NBA player\nsalaries in the 2022-2023 season, we show that our method is competitive with\nvarious state-of-the-art methods.\n","subjects":["Statistics/Methodology","Statistics/Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}