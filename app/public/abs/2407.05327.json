{"id":"2407.05327","title":"Can Model Uncertainty Function as a Proxy for Multiple-Choice Question\n  Item Difficulty?","authors":"Leonidas Zotos, Hedderik van Rijn and Malvina Nissim","authorsParsed":[["Zotos","Leonidas",""],["van Rijn","Hedderik",""],["Nissim","Malvina",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 10:48:04 GMT"}],"updateDate":"2024-07-09","timestamp":1720349284000,"abstract":"  Estimating the difficulty of multiple-choice questions would be great help\nfor educators who must spend substantial time creating and piloting stimuli for\ntheir tests, and for learners who want to practice. Supervised approaches to\ndifficulty estimation have yielded to date mixed results. In this contribution\nwe leverage an aspect of generative large models which might be seen as a\nweakness when answering questions, namely their uncertainty, and exploit it\ntowards exploring correlations between two different metrics of uncertainty,\nand the actual student response distribution. While we observe some present but\nweak correlations, we also discover that the models' behaviour is different in\nthe case of correct vs wrong answers, and that correlations differ\nsubstantially according to the different question types which are included in\nour fine-grained, previously unused dataset of 451 questions from a\nBiopsychology course. In discussing our findings, we also suggest potential\navenues to further leverage model uncertainty as an additional proxy for item\ndifficulty.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}