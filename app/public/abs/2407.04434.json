{"id":"2407.04434","title":"From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive\n  Language for Bias Reduction in LLMs","authors":"Marion Bartl and Susan Leavy","authorsParsed":[["Bartl","Marion",""],["Leavy","Susan",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 11:31:30 GMT"}],"updateDate":"2024-07-08","timestamp":1720179090000,"abstract":"  Gender bias is not only prevalent in Large Language Models (LLMs) and their\ntraining data, but also firmly ingrained into the structural aspects of\nlanguage itself. Therefore, adapting linguistic structures within LLM training\ndata to promote gender-inclusivity can make gender representations within the\nmodel more inclusive. The focus of our work are gender-exclusive affixes in\nEnglish, such as in 'show-girl' or 'man-cave', which can perpetuate gender\nstereotypes and binary conceptions of gender. We use an LLM training dataset to\ncompile a catalogue of 692 gender-exclusive terms along with gender-neutral\nvariants and from this, develop a gender-inclusive fine-tuning dataset, the\n'Tiny Heap'. Fine-tuning three different LLMs with this dataset, we observe an\noverall reduction in gender-stereotyping tendencies across the models. Our\napproach provides a practical method for enhancing gender inclusivity in LLM\ntraining data and contributes to incorporating queer-feminist linguistic\nactivism in bias mitigation research in NLP.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}