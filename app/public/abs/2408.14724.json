{"id":"2408.14724","title":"GeoTransfer : Generalizable Few-Shot Multi-View Reconstruction via\n  Transfer Learning","authors":"Shubhendu Jena, Franck Multon and Adnane Boukhayma","authorsParsed":[["Jena","Shubhendu",""],["Multon","Franck",""],["Boukhayma","Adnane",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 01:28:15 GMT"}],"updateDate":"2024-08-28","timestamp":1724722095000,"abstract":"  This paper presents a novel approach for sparse 3D reconstruction by\nleveraging the expressive power of Neural Radiance Fields (NeRFs) and fast\ntransfer of their features to learn accurate occupancy fields. Existing 3D\nreconstruction methods from sparse inputs still struggle with capturing\nintricate geometric details and can suffer from limitations in handling\noccluded regions. On the other hand, NeRFs excel in modeling complex scenes but\ndo not offer means to extract meaningful geometry. Our proposed method offers\nthe best of both worlds by transferring the information encoded in NeRF\nfeatures to derive an accurate occupancy field representation. We utilize a\npre-trained, generalizable state-of-the-art NeRF network to capture detailed\nscene radiance information, and rapidly transfer this knowledge to train a\ngeneralizable implicit occupancy network. This process helps in leveraging the\nknowledge of the scene geometry encoded in the generalizable NeRF prior and\nrefining it to learn occupancy fields, facilitating a more precise\ngeneralizable representation of 3D space. The transfer learning approach leads\nto a dramatic reduction in training time, by orders of magnitude (i.e. from\nseveral days to 3.5 hrs), obviating the need to train generalizable sparse\nsurface reconstruction methods from scratch. Additionally, we introduce a novel\nloss on volumetric rendering weights that helps in the learning of accurate\noccupancy fields, along with a normal loss that helps in global smoothing of\nthe occupancy fields. We evaluate our approach on the DTU dataset and\ndemonstrate state-of-the-art performance in terms of reconstruction accuracy,\nespecially in challenging scenarios with sparse input data and occluded\nregions. We furthermore demonstrate the generalization capabilities of our\nmethod by showing qualitative results on the Blended MVS dataset without any\nretraining.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}