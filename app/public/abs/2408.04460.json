{"id":"2408.04460","title":"An experimental comparative study of backpropagation and alternatives\n  for training binary neural networks for image classification","authors":"Ben Crulis, Barthelemy Serres, Cyril de Runz, Gilles Venturini","authorsParsed":[["Crulis","Ben",""],["Serres","Barthelemy",""],["de Runz","Cyril",""],["Venturini","Gilles",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 13:39:09 GMT"}],"updateDate":"2024-08-09","timestamp":1723124349000,"abstract":"  Current artificial neural networks are trained with parameters encoded as\nfloating point numbers that occupy lots of memory space at inference time. Due\nto the increase in the size of deep learning models, it is becoming very\ndifficult to consider training and using artificial neural networks on edge\ndevices. Binary neural networks promise to reduce the size of deep neural\nnetwork models, as well as to increase inference speed while decreasing energy\nconsumption. Thus, they may allow the deployment of more powerful models on\nedge devices. However, binary neural networks are still proven to be difficult\nto train using the backpropagation-based gradient descent scheme. This paper\nextends the work of \\cite{crulis2023alternatives}, which proposed adapting to\nbinary neural networks two promising alternatives to backpropagation originally\ndesigned for continuous neural networks, and experimented with them on simple\nimage classification datasets. This paper proposes new experiments on the\nImageNette dataset, compares three different model architectures for image\nclassification, and adds two additional alternatives to backpropagation.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"AlQ9z2DFWr6Dez1xnVM9jDlZmqqGlPYSeIeMn9ce1zk","pdfSize":"663518"}
