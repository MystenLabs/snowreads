{"id":"2407.07329","title":"Probability of Differentiation Reveals Brittleness of Homogeneity Bias\n  in Large Language Models","authors":"Messi H.J. Lee, Calvin K. Lai","authorsParsed":[["Lee","Messi H. J.",""],["Lai","Calvin K.",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 02:56:55 GMT"}],"updateDate":"2024-07-11","timestamp":1720580215000,"abstract":"  Homogeneity bias in Large Language Models (LLMs) refers to their tendency to\nhomogenize the representations of some groups compared to others. Previous\nstudies documenting this bias have predominantly used encoder models, which may\nhave inadvertently introduced biases. To address this limitation, we prompted\nGPT-4 to generate single word/expression completions associated with 18\nsituation cues - specific, measurable elements of environments that influence\nhow individuals perceive situations and compared the variability of these\ncompletions using probability of differentiation. This approach directly\nassessed homogeneity bias from the model's outputs, bypassing encoder models.\nAcross five studies, we find that homogeneity bias is highly volatile across\nsituation cues and writing prompts, suggesting that the bias observed in past\nwork may reflect those within encoder models rather than LLMs. Furthermore,\nthese results suggest that homogeneity bias in LLMs is brittle, as even minor\nand arbitrary changes in prompts can significantly alter the expression of\nbiases. Future work should further explore how variations in syntactic features\nand topic choices in longer text generations influence homogeneity bias in\nLLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"Xkx_XkjgXp-qA-3gTcjnnsdzTIhkO713vSwPsVxkHKM","pdfSize":"581712"}
