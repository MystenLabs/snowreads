{"id":"2407.02473","title":"Open Scene Graphs for Open World Object-Goal Navigation","authors":"Joel Loo, Zhanxin Wu, David Hsu","authorsParsed":[["Loo","Joel",""],["Wu","Zhanxin",""],["Hsu","David",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:52:12 GMT"}],"updateDate":"2024-07-03","timestamp":1719942732000,"abstract":"  How can we build robots for open-world semantic navigation tasks, like\nsearching for target objects in novel scenes? While foundation models have the\nrich knowledge and generalisation needed for these tasks, a suitable scene\nrepresentation is needed to connect them into a complete robot system. We\naddress this with Open Scene Graphs (OSGs), a topo-semantic representation that\nretains and organises open-set scene information for these models, and has a\nstructure that can be configured for different environment types. We integrate\nfoundation models and OSGs into the OpenSearch system for Open World\nObject-Goal Navigation, which is capable of searching for open-set objects\nspecified in natural language, while generalising zero-shot across diverse\nenvironments and embodiments. Our OSGs enhance reasoning with Large Language\nModels (LLM), enabling robust object-goal navigation outperforming existing LLM\napproaches. Through simulation and real-world experiments, we validate\nOpenSearch's generalisation across varied environments, robots and novel\ninstructions.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}