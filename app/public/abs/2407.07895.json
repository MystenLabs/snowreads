{"id":"2407.07895","title":"LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large\n  Multimodal Models","authors":"Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun\n  Ma, Chunyuan Li","authorsParsed":[["Li","Feng",""],["Zhang","Renrui",""],["Zhang","Hao",""],["Zhang","Yuanhan",""],["Li","Bo",""],["Li","Wei",""],["Ma","Zejun",""],["Li","Chunyuan",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 17:59:43 GMT"},{"version":"v2","created":"Sun, 28 Jul 2024 19:58:08 GMT"}],"updateDate":"2024-07-30","timestamp":1720634383000,"abstract":"  Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}