{"id":"2407.00467","title":"VcLLM: Video Codecs are Secretly Tensor Codecs","authors":"Ceyu Xu, Yongji Wu, Xinyu Yang, Beidi Chen, Matthew Lentz, Danyang\n  Zhuo, Lisa Wu Wills","authorsParsed":[["Xu","Ceyu",""],["Wu","Yongji",""],["Yang","Xinyu",""],["Chen","Beidi",""],["Lentz","Matthew",""],["Zhuo","Danyang",""],["Wills","Lisa Wu",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 15:24:33 GMT"}],"updateDate":"2024-07-02","timestamp":1719674673000,"abstract":"  As the parameter size of large language models (LLMs) continues to expand,\nthe need for a large memory footprint and high communication bandwidth have\nbecome significant bottlenecks for the training and inference of LLMs. To\nmitigate these bottlenecks, various tensor compression techniques have been\nproposed to reduce the data size, thereby alleviating memory requirements and\ncommunication pressure.\n  Our research found that video codecs, despite being originally designed for\ncompressing videos, show excellent efficiency when compressing various types of\ntensors. We demonstrate that video codecs can be versatile and general-purpose\ntensor codecs while achieving the state-of-the-art compression efficiency in\nvarious tasks. We further make use of the hardware video encoding and decoding\nmodule available on GPUs to create a framework capable of both inference and\ntraining with video codecs repurposed as tensor codecs. This greatly reduces\nthe requirement for memory capacity and communication bandwidth, enabling\ntraining and inference of large models on consumer-grade GPUs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}