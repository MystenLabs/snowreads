{"id":"2407.12858","title":"Grounding and Evaluation for Large Language Models: Practical Challenges\n  and Lessons Learned (Survey)","authors":"Krishnaram Kenthapadi and Mehrnoosh Sameki and Ankur Taly","authorsParsed":[["Kenthapadi","Krishnaram",""],["Sameki","Mehrnoosh",""],["Taly","Ankur",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 01:23:10 GMT"}],"updateDate":"2024-07-19","timestamp":1720574590000,"abstract":"  With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems\nin high-stakes domains, ensuring the trustworthiness, safety, and observability\nof these systems has become crucial. It is essential to evaluate and monitor AI\nsystems not only for accuracy and quality-related metrics but also for\nrobustness, bias, security, interpretability, and other responsible AI\ndimensions. We focus on large language models (LLMs) and other generative AI\nmodels, which present additional challenges such as hallucinations, harmful and\nmanipulative content, and copyright infringement. In this survey article\naccompanying our KDD 2024 tutorial, we highlight a wide range of harms\nassociated with generative AI systems, and survey state of the art approaches\n(along with open challenges) to address these harms.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}