{"id":"2407.10065","title":"An Efficient High-dimensional Gradient Estimator for Stochastic\n  Differential Equations","authors":"Shengbo Wang, Jose Blanchet, Peter Glynn","authorsParsed":[["Wang","Shengbo",""],["Blanchet","Jose",""],["Glynn","Peter",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 03:33:10 GMT"}],"updateDate":"2024-07-16","timestamp":1720927990000,"abstract":"  Overparameterized stochastic differential equation (SDE) models have achieved\nremarkable success in various complex environments, such as PDE-constrained\noptimization, stochastic control and reinforcement learning, financial\nengineering, and neural SDEs. These models often feature system evolution\ncoefficients that are parameterized by a high-dimensional vector $\\theta \\in\n\\mathbb{R}^n$, aiming to optimize expectations of the SDE, such as a value\nfunction, through stochastic gradient ascent. Consequently, designing efficient\ngradient estimators for which the computational complexity scales well with $n$\nis of significant interest. This paper introduces a novel unbiased stochastic\ngradient estimator--the generator gradient estimator--for which the computation\ntime remains stable in $n$. In addition to establishing the validity of our\nmethodology for general SDEs with jumps, we also perform numerical experiments\nthat test our estimator in linear-quadratic control problems parameterized by\nhigh-dimensional neural networks. The results show a significant improvement in\nefficiency compared to the widely used pathwise differentiation method: Our\nestimator achieves near-constant computation times, increasingly outperforms\nits counterpart as $n$ increases, and does so without compromising estimation\nvariance. These empirical findings highlight the potential of our proposed\nmethodology for optimizing SDEs in contemporary applications.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}