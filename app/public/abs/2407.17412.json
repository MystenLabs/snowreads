{"id":"2407.17412","title":"(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent\n  HyperNetwork","authors":"Tianjin Huang, Fang Meng, Li Shen, Fan Liu, Yulong Pei, Mykola\n  Pechenizkiy, Shiwei Liu, Tianlong Chen","authorsParsed":[["Huang","Tianjin",""],["Meng","Fang",""],["Shen","Li",""],["Liu","Fan",""],["Pei","Yulong",""],["Pechenizkiy","Mykola",""],["Liu","Shiwei",""],["Chen","Tianlong",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 16:47:45 GMT"}],"updateDate":"2024-07-25","timestamp":1721839665000,"abstract":"  Large-scale neural networks have demonstrated remarkable performance in\ndifferent domains like vision and language processing, although at the cost of\nmassive computation resources. As illustrated by compression literature,\nstructural model pruning is a prominent algorithm to encourage model\nefficiency, thanks to its acceleration-friendly sparsity patterns. One of the\nkey questions of structural pruning is how to estimate the channel\nsignificance. In parallel, work on data-centric AI has shown that\nprompting-based techniques enable impressive generalization of large language\nmodels across diverse downstream tasks. In this paper, we investigate a\ncharming possibility - \\textit{leveraging visual prompts to capture the channel\nimportance and derive high-quality structural sparsity}. To this end, we\npropose a novel algorithmic framework, namely \\texttt{PASS}. It is a tailored\nhyper-network to take both visual prompts and network weight statistics as\ninput, and output layer-wise channel sparsity in a recurrent manner. Such\ndesigns consider the intrinsic channel dependency between layers. Comprehensive\nexperiments across multiple network architectures and six datasets demonstrate\nthe superiority of \\texttt{PASS} in locating good structural sparsity. For\nexample, at the same FLOPs level, \\texttt{PASS} subnetworks achieve $1\\%\\sim\n3\\%$ better accuracy on Food101 dataset; or with a similar performance of\n$80\\%$ accuracy, \\texttt{PASS} subnetworks obtain $0.35\\times$ more speedup\nthan the baselines.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}