{"id":"2407.10098","title":"Accelerator-as-a-Service in Public Clouds: An Intra-Host Traffic\n  Management View for Performance Isolation in the Wild","authors":"Jiechen Zhao, Ran Shu, Katie Lim, Zewen Fan, Thomas Anderson, Mingyu\n  Gao, Natalie Enright Jerger","authorsParsed":[["Zhao","Jiechen",""],["Shu","Ran",""],["Lim","Katie",""],["Fan","Zewen",""],["Anderson","Thomas",""],["Gao","Mingyu",""],["Jerger","Natalie Enright",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 06:36:19 GMT"}],"updateDate":"2024-07-16","timestamp":1720938979000,"abstract":"  I/O devices in public clouds have integrated increasing numbers of hardware\naccelerators, e.g., AWS Nitro, Azure FPGA and Nvidia BlueField. However, such\nspecialized compute (1) is not explicitly accessible to cloud users with\nperformance guarantee, (2) cannot be leveraged simultaneously by both providers\nand users, unlike general-purpose compute (e.g., CPUs). Through ten\nobservations, we present that the fundamental difficulty of democratizing\naccelerators is insufficient performance isolation support. The key obstacles\nto enforcing accelerator isolation are (1) too many unknown traffic patterns in\npublic clouds and (2) too many possible contention sources in the datapath. In\nthis work, instead of scheduling such complex traffic on-the-fly and augmenting\nisolation support on each system component, we propose to model traffic as\nnetwork flows and proactively re-shape the traffic to avoid unpredictable\ncontention. We discuss the implications of our findings on the design of future\nI/O management stacks and device interfaces.\n","subjects":["Computing Research Repository/Operating Systems","Computing Research Repository/Hardware Architecture","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Networking and Internet Architecture","Computing Research Repository/Performance"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}