{"id":"2408.13674","title":"GenCA: A Text-conditioned Generative Model for Realistic and Drivable\n  Codec Avatars","authors":"Keqiang Sun, Amin Jourabloo, Riddhish Bhalodia, Moustafa Meshry, Yu\n  Rong, Zhengyu Yang, Thu Nguyen-Phuoc, Christian Haene, Jiu Xu, Sam Johnson,\n  Hongsheng Li, Sofien Bouaziz","authorsParsed":[["Sun","Keqiang",""],["Jourabloo","Amin",""],["Bhalodia","Riddhish",""],["Meshry","Moustafa",""],["Rong","Yu",""],["Yang","Zhengyu",""],["Nguyen-Phuoc","Thu",""],["Haene","Christian",""],["Xu","Jiu",""],["Johnson","Sam",""],["Li","Hongsheng",""],["Bouaziz","Sofien",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 21:25:22 GMT"}],"updateDate":"2024-08-27","timestamp":1724534722000,"abstract":"  Photo-realistic and controllable 3D avatars are crucial for various\napplications such as virtual and mixed reality (VR/MR), telepresence, gaming,\nand film production. Traditional methods for avatar creation often involve\ntime-consuming scanning and reconstruction processes for each avatar, which\nlimits their scalability. Furthermore, these methods do not offer the\nflexibility to sample new identities or modify existing ones. On the other\nhand, by learning a strong prior from data, generative models provide a\npromising alternative to traditional reconstruction methods, easing the time\nconstraints for both data capture and processing. Additionally, generative\nmethods enable downstream applications beyond reconstruction, such as editing\nand stylization. Nonetheless, the research on generative 3D avatars is still in\nits infancy, and therefore current methods still have limitations such as\ncreating static avatars, lacking photo-realism, having incomplete facial\ndetails, or having limited drivability. To address this, we propose a\ntext-conditioned generative model that can generate photo-realistic facial\navatars of diverse identities, with more complete details like hair, eyes and\nmouth interior, and which can be driven through a powerful non-parametric\nlatent expression space. Specifically, we integrate the generative and editing\ncapabilities of latent diffusion models with a strong prior model for avatar\nexpression driving.\n  Our model can generate and control high-fidelity avatars, even those\nout-of-distribution. We also highlight its potential for downstream\napplications, including avatar editing and single-shot avatar reconstruction.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}