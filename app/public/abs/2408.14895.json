{"id":"2408.14895","title":"VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view\n  Videos of Daily Activities","authors":"Shusaku Egami, Takahiro Ugai, Swe Nwe Nwe Htun, Ken Fukuda","authorsParsed":[["Egami","Shusaku",""],["Ugai","Takahiro",""],["Htun","Swe Nwe Nwe",""],["Fukuda","Ken",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 09:18:57 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 01:56:33 GMT"}],"updateDate":"2024-08-29","timestamp":1724750337000,"abstract":"  Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data\n(e.g., images and videos) into symbols, have attracted attention as resources\nenabling knowledge processing and machine learning across modalities. However,\nthe construction of MMKGs for videos consisting of multiple events, such as\ndaily activities, is still in the early stages. In this paper, we construct an\nMMKG based on synchronized multi-view simulated videos of daily activities.\nBesides representing the content of daily life videos as event-centric\nknowledge, our MMKG also includes frame-by-frame fine-grained changes, such as\nbounding boxes within video frames. In addition, we provide support tools for\nquerying our MMKG. As an application example, we demonstrate that our MMKG\nfacilitates benchmarking vision-language models by providing the necessary\nvision-language datasets for a tailored task.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"hLrIRZGqYNJLuhMOx-FC_5oPqPGkRw_tXCGa5xiDLDE","pdfSize":"587103"}
