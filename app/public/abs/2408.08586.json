{"id":"2408.08586","title":"Rubick: Exploiting Job Reconfigurability for Deep Learning Cluster\n  Scheduling","authors":"Xinyi Zhang, Hanyu Zhao, Wencong Xiao, Xianyan Jia, Fei Xu, Yong Li,\n  Wei Lin, Fangming Liu","authorsParsed":[["Zhang","Xinyi",""],["Zhao","Hanyu",""],["Xiao","Wencong",""],["Jia","Xianyan",""],["Xu","Fei",""],["Li","Yong",""],["Lin","Wei",""],["Liu","Fangming",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 07:41:52 GMT"}],"updateDate":"2024-08-19","timestamp":1723794112000,"abstract":"  The era of large deep learning models has given rise to advanced training\nstrategies such as 3D parallelism and the ZeRO series. These strategies enable\nvarious (re-)configurable execution plans for a training job, which exhibit\nremarkably different requirements of multiple resource types. Existing cluster\nscheduling systems, however, treat such reconfigurable training jobs as black\nboxes: they rely on users to choose execution plans statically, and then make\nresource allocations without awareness of the chosen plans and their resource\nrequirements. This approach results in mismatches between execution plans and\nresources, making both training performance and cluster utilization far from\noptimal.\n  We introduce Rubick, a cluster scheduling system for deep learning training\nthat exploits the reconfigurability to improve job performance and cluster\nefficiency. Rubick incorporates the job execution planning as a new dimension\nin cluster scheduling, by continuously reconfiguring jobs' execution plans and\ntuning multi-resource allocations across jobs jointly. Such a co-optimization\nis navigated by a performance model that understands the diverse resource\nrequirements and performance characteristics of different jobs and execution\nplans. Rubick exploits such a model to make performance-aware scheduling\ndecisions to maximize cluster throughput while providing performance guarantees\nto individual jobs. Evaluations on a 64-GPU high-performance training cluster\nshow that Rubick improves average job completion time and makespan by up to\n3.2x and 1.4x, respectively, compared against state-of-the-art systems.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}