{"id":"2408.01228","title":"The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models","authors":"Simone Caldarella, Massimiliano Mancini, Elisa Ricci, Rahaf Aljundi","authorsParsed":[["Caldarella","Simone",""],["Mancini","Massimiliano",""],["Ricci","Elisa",""],["Aljundi","Rahaf",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 12:36:13 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 13:35:05 GMT"}],"updateDate":"2024-08-20","timestamp":1722602173000,"abstract":"  Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}