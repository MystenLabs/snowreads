{"id":"2408.12596","title":"Poplar: Efficient Scaling of Distributed DNN Training on Heterogeneous\n  GPU Clusters","authors":"WenZheng Zhang, Yang Hu, Jing Shi, Xiaoying Bai","authorsParsed":[["Zhang","WenZheng",""],["Hu","Yang",""],["Shi","Jing",""],["Bai","Xiaoying",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 17:58:06 GMT"}],"updateDate":"2024-08-23","timestamp":1724349486000,"abstract":"  Scaling Deep Neural Networks (DNNs) requires significant computational\nresources in terms of GPU quantity and compute capacity. In practice, there\nusually exists a large number of heterogeneous GPU devices due to the rapid\nrelease cycle of GPU products. It is highly needed to efficiently and\neconomically harness the power of heterogeneous GPUs, so that it can meet the\nrequirements of DNN research and development. The paper introduces Poplar, a\ndistributed training system that extends Zero Redundancy Optimizer (ZeRO) with\nheterogeneous-aware capabilities. We explore a broader spectrum of GPU\nheterogeneity, including compute capability, memory capacity, quantity and a\ncombination of them. In order to achieve high computational efficiency across\nall heterogeneous conditions, Poplar conducts fine-grained measurements of GPUs\nin each ZeRO stage. We propose a novel batch allocation method and a search\nalgorithm to optimize the utilization of heterogeneous GPUs clusters.\nFurthermore, Poplar implements fully automated parallelism, eliminating the\nneed for deploying heterogeneous hardware and finding suitable batch size.\nExtensive experiments on three heterogeneous clusters, comprising six different\ntypes of GPUs, demonstrate that Poplar achieves a training throughput\nimprovement of 1.02-3.92x over current state-of-the-art heterogeneous training\nsystems.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}