{"id":"2407.07653","title":"AffectGPT: Dataset and Framework for Explainable Multimodal Emotion\n  Recognition","authors":"Zheng Lian, Haiyang Sun, Licai Sun, Jiangyan Yi, Bin Liu, Jianhua Tao","authorsParsed":[["Lian","Zheng",""],["Sun","Haiyang",""],["Sun","Licai",""],["Yi","Jiangyan",""],["Liu","Bin",""],["Tao","Jianhua",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:34:14 GMT"}],"updateDate":"2024-07-11","timestamp":1720618454000,"abstract":"  Explainable Multimodal Emotion Recognition (EMER) is an emerging task that\naims to achieve reliable and accurate emotion recognition. However, due to the\nhigh annotation cost, the existing dataset (denoted as EMER-Fine) is small,\nmaking it difficult to perform supervised training. To reduce the annotation\ncost and expand the dataset size, this paper reviews the previous dataset\nconstruction process. Then, we simplify the annotation pipeline, avoid manual\nchecks, and replace the closed-source models with open-source models. Finally,\nwe build \\textbf{EMER-Coarse}, a coarsely-labeled dataset containing\nlarge-scale samples. Besides the dataset, we propose a two-stage training\nframework \\textbf{AffectGPT}. The first stage exploits EMER-Coarse to learn a\ncoarse mapping between multimodal inputs and emotion-related descriptions; the\nsecond stage uses EMER-Fine to better align with manually-checked results.\nExperimental results demonstrate the effectiveness of our proposed method on\nthe challenging EMER task. To facilitate further research, we will make the\ncode and dataset available at: https://github.com/zeroQiaoba/AffectGPT.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}