{"id":"2408.07978","title":"Coupling without Communication and Drafter-Invariant Speculative\n  Decoding","authors":"Majid Daliri, Christopher Musco, Ananda Theertha Suresh","authorsParsed":[["Daliri","Majid",""],["Musco","Christopher",""],["Suresh","Ananda Theertha",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 06:52:24 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 05:04:38 GMT"}],"updateDate":"2024-08-20","timestamp":1723704744000,"abstract":"  Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice\nwants to generate a sample $a\\sim P$ and Bob a sample $b \\sim Q$ such that $a =\nb$ with has as high of probability as possible. It is well-known that, by\nsampling from an optimal coupling between the distributions, Alice and Bob can\nachieve $Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total\nvariation distance. What if Alice and Bob must solve this same problem without\ncommunicating at all? Perhaps surprisingly, with access to public randomness,\nthey can still achieve $Pr[a=b] \\geq \\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)} \\geq\n1-2D_{TV}(P,Q)$. In fact, this bound can be obtained using a simple protocol\nbased on the Weighted MinHash algorithm. In this work, we explore the\ncommunication-free coupling problem in greater depth. First, we show that an\nequally simple protocol based on Gumbel sampling matches the worst-case\nguarantees of the Weighted MinHash approach, but tends to perform better in\npractice. Conversely, we prove that both approaches are actually sharp: no\ncommunication-free protocol can achieve\n$Pr[a=b]>\\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$ in the worst-case. Finally, we\nprove that, for distributions over $n$ items, there exists a scheme that uses\njust $O(\\log(n/\\epsilon))$ bits of communication to achieve $Pr[a = b] = 1 -\nD_{TV}(P,Q) - \\epsilon$, i.e. to essentially match optimal coupling. Beyond our\ntheoretical results, we demonstrate an application of communication-free\ncoupling to speculative decoding, a recent method for accelerating\nautoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We\nshow that communication-free protocols yield a variant of speculative decoding\nthat we call Drafter-Invariant Speculative Decoding, which has the desirable\nproperty that the output of the method is fixed given a fixed random seed,\nregardless of what drafter is used for speculation.\n","subjects":["Computing Research Repository/Data Structures and Algorithms","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}