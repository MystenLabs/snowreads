{"id":"2408.10433","title":"CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing\n  Hallucinations in LVLMs","authors":"Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios\n  Tzimiropoulos","authorsParsed":[["Ouali","Yassine",""],["Bulat","Adrian",""],["Martinez","Brais",""],["Tzimiropoulos","Georgios",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 21:56:20 GMT"}],"updateDate":"2024-08-21","timestamp":1724104580000,"abstract":"  Despite recent successes, LVLMs or Large Vision Language Models are prone to\nhallucinating details like objects and their properties or relations, limiting\ntheir real-world deployment. To address this and improve their robustness, we\npresent CLIP-DPO, a preference optimization method that leverages contrastively\npre-trained Vision-Language (VL) embedding models, such as CLIP, for DPO-based\noptimization of LVLMs. Unlike prior works tackling LVLM hallucinations, our\nmethod does not rely on paid-for APIs, and does not require additional training\ndata or the deployment of other external LVLMs. Instead, starting from the\ninitial pool of supervised fine-tuning data, we generate a diverse set of\npredictions, which are ranked based on their CLIP image-text similarities, and\nthen filtered using a robust rule-based approach to obtain a set of positive\nand negative pairs for DPO-based training. We applied CLIP-DPO fine-tuning to\nthe MobileVLM-v2 family of models and to LlaVA-1.5, in all cases observing\nsignificant improvements in terms of hallucination reduction over baseline\nmodels. We also observe better performance for zero-shot classification,\nsuggesting improved grounding capabilities, and verify that the original\nperformance on standard LVLM benchmarks is overall preserved.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}