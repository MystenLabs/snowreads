{"id":"2408.02954","title":"WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal\n  Deepfake Detection","authors":"Juho Jung, Sangyoun Lee, Jooeon Kang and Yunjin Na","authorsParsed":[["Jung","Juho",""],["Lee","Sangyoun",""],["Kang","Jooeon",""],["Na","Yunjin",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 04:44:10 GMT"}],"updateDate":"2024-08-07","timestamp":1722919450000,"abstract":"  All current benchmarks for multimodal deepfake detection manipulate entire\nframes using various generation techniques, resulting in oversaturated\ndetection accuracies exceeding 94% at the video-level classification. However,\nthese benchmarks struggle to detect dynamic deepfake attacks with challenging\nframe-by-frame alterations presented in real-world scenarios. To address this\nlimitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed\nat identifying manipulated segments within both video and audio, providing\ninsight into the origins of deepfakes. Furthermore, we propose novel evaluation\nmetrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to\nassess the robustness of deepfake detection models. Evaluating state-of-the-art\nmodels against diverse deepfake benchmarks, particularly FakeMix, demonstrates\nthe effectiveness of our approach comprehensively. Specifically, while\nachieving an Average Precision (AP) of 94.2% at the video-level, the evaluation\nof the existing models at the clip-level using the proposed metrics, TA and\nFDM, yielded sharp declines in accuracy to 53.1%, and 52.1%, respectively.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}