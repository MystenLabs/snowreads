{"id":"2407.14681","title":"Value Internalization: Learning and Generalizing from Social Reward","authors":"Frieda Rong and Max Kleiman-Weiner","authorsParsed":[["Rong","Frieda",""],["Kleiman-Weiner","Max",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 21:53:33 GMT"}],"updateDate":"2024-07-23","timestamp":1721426013000,"abstract":"  Social rewards shape human behavior. During development, a caregiver guides a\nlearner's behavior towards culturally aligned goals and values. How do these\nbehaviors persist and generalize when the caregiver is no longer present, and\nthe learner must continue autonomously? Here, we propose a model of value\ninternalization where social feedback trains an internal social reward (ISR)\nmodel that generates internal rewards when social rewards are unavailable.\nThrough empirical simulations, we show that an ISR model prevents agents from\nunlearning socialized behaviors and enables generalization in\nout-of-distribution tasks. We characterize the implications of incomplete\ninternalization, akin to \"reward hacking\" on the ISR. Additionally, we show\nthat our model internalizes prosocial behavior in a multi-agent environment.\nOur work provides a foundation for understanding how humans acquire and\ngeneralize values and offers insights for aligning AI with human values.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/"}