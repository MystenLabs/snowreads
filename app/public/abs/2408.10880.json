{"id":"2408.10880","title":"Open 3D World in Autonomous Driving","authors":"Xinlong Cheng, Lei Li","authorsParsed":[["Cheng","Xinlong",""],["Li","Lei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 14:10:44 GMT"}],"updateDate":"2024-08-21","timestamp":1724163044000,"abstract":"  The capability for open vocabulary perception represents a significant\nadvancement in autonomous driving systems, facilitating the comprehension and\ninterpretation of a wide array of textual inputs in real-time. Despite\nextensive research in open vocabulary tasks within 2D computer vision, the\napplication of such methodologies to 3D environments, particularly within\nlarge-scale outdoor contexts, remains relatively underdeveloped. This paper\npresents a novel approach that integrates 3D point cloud data, acquired from\nLIDAR sensors, with textual information. The primary focus is on the\nutilization of textual data to directly localize and identify objects within\nthe autonomous driving context. We introduce an efficient framework for the\nfusion of bird's-eye view (BEV) region features with textual features, thereby\nenabling the system to seamlessly adapt to novel textual inputs and enhancing\nthe robustness of open vocabulary detection tasks. The effectiveness of the\nproposed methodology is rigorously evaluated through extensive experimentation\non the newly introduced NuScenes-T dataset, with additional validation of its\nzero-shot performance on the Lyft Level 5 dataset. This research makes a\nsubstantive contribution to the advancement of autonomous driving technologies\nby leveraging multimodal data to enhance open vocabulary perception in 3D\nenvironments, thereby pushing the boundaries of what is achievable in\nautonomous navigation and perception.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}