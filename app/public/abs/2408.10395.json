{"id":"2408.10395","title":"Evaluating Image-Based Face and Eye Tracking with Event Cameras","authors":"Khadija Iddrisu, Waseem Shariff, Noel E.OConnor, Joseph Lemley,\n  Suzanne Little","authorsParsed":[["Iddrisu","Khadija",""],["Shariff","Waseem",""],["OConnor","Noel E.",""],["Lemley","Joseph",""],["Little","Suzanne",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 20:27:08 GMT"}],"updateDate":"2024-08-21","timestamp":1724099228000,"abstract":"  Event Cameras, also known as Neuromorphic sensors, capture changes in local\nlight intensity at the pixel level, producing asynchronously generated data\ntermed ``events''. This distinct data format mitigates common issues observed\nin conventional cameras, like under-sampling when capturing fast-moving\nobjects, thereby preserving critical information that might otherwise be lost.\nHowever, leveraging this data often necessitates the development of\nspecialized, handcrafted event representations that can integrate seamlessly\nwith conventional Convolutional Neural Networks (CNNs), considering the unique\nattributes of event data. In this study, We evaluate event-based Face and Eye\ntracking. The core objective of our study is to showcase the viability of\nintegrating conventional algorithms with event-based data, transformed into a\nframe format while preserving the unique benefits of event cameras. To validate\nour approach, we constructed a frame-based event dataset by simulating events\nbetween RGB frames derived from the publicly accessible Helen Dataset. We\nassess its utility for face and eye detection tasks through the application of\nGR-YOLO -- a pioneering technique derived from YOLOv3. This evaluation includes\na comparative analysis with results derived from training the dataset with\nYOLOv8. Subsequently, the trained models were tested on real event streams from\nvarious iterations of Prophesee's event cameras and further evaluated on the\nFaces in Event Stream (FES) benchmark dataset. The models trained on our\ndataset shows a good prediction performance across all the datasets obtained\nfor validation with the best results of a mean Average precision score of 0.91.\nAdditionally, The models trained demonstrated robust performance on real event\ncamera data under varying light conditions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}