{"id":"2407.21229","title":"Advancing Vietnamese Visual Question Answering with Transformer and\n  Convolutional Integration","authors":"Ngoc Son Nguyen, Van Son Nguyen, Tung Le","authorsParsed":[["Nguyen","Ngoc Son",""],["Nguyen","Van Son",""],["Le","Tung",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 22:32:50 GMT"}],"updateDate":"2024-08-01","timestamp":1722378770000,"abstract":"  Visual Question Answering (VQA) has recently emerged as a potential research\ndomain, captivating the interest of many in the field of artificial\nintelligence and computer vision. Despite the prevalence of approaches in\nEnglish, there is a notable lack of systems specifically developed for certain\nlanguages, particularly Vietnamese. This study aims to bridge this gap by\nconducting comprehensive experiments on the Vietnamese Visual Question\nAnswering (ViVQA) dataset, demonstrating the effectiveness of our proposed\nmodel. In response to community interest, we have developed a model that\nenhances image representation capabilities, thereby improving overall\nperformance in the ViVQA system. Specifically, our model integrates the\nBootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)\nand the convolutional neural network EfficientNet to extract and process both\nlocal and global features from images. This integration leverages the strengths\nof transformer-based architectures for capturing comprehensive contextual\ninformation and convolutional networks for detailed local features. By freezing\nthe parameters of these pre-trained models, we significantly reduce the\ncomputational cost and training time, while maintaining high performance. This\napproach significantly improves image representation and enhances the\nperformance of existing VQA systems. We then leverage a multi-modal fusion\nmodule based on a general-purpose multi-modal foundation model (BEiT-3) to fuse\nthe information between visual and textual features. Our experimental findings\ndemonstrate that our model surpasses competing baselines, achieving promising\nperformance. This is particularly evident in its accuracy of $71.04\\%$ on the\ntest set of the ViVQA dataset, marking a significant advancement in our\nresearch area. The code is available at https://github.com/nngocson2002/ViVQA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"_nkk8MLjXHC1dWmIpaa_8H0jT1ExGj_r8DyjpBBqqTo","pdfSize":"6118198","objectId":"0x12f2c2c74e648f9f1a5b3d0bfad2147c0d420c0b783152556d8f10b336ac718c","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
