{"id":"2408.08751","title":"Comparative Analysis of Generative Models: Enhancing Image Synthesis\n  with VAEs, GANs, and Stable Diffusion","authors":"Sanchayan Vivekananthan","authorsParsed":[["Vivekananthan","Sanchayan",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 13:50:50 GMT"}],"updateDate":"2024-08-19","timestamp":1723816250000,"abstract":"  This paper examines three major generative modelling frameworks: Variational\nAutoencoders (VAEs), Generative Adversarial Networks (GANs), and Stable\nDiffusion models. VAEs are effective at learning latent representations but\nfrequently yield blurry results. GANs can generate realistic images but face\nissues such as mode collapse. Stable Diffusion models, while producing\nhigh-quality images with strong semantic coherence, are demanding in terms of\ncomputational resources. Additionally, the paper explores how incorporating\nGrounding DINO and Grounded SAM with Stable Diffusion improves image accuracy\nby utilising sophisticated segmentation and inpainting techniques. The analysis\nguides on selecting suitable models for various applications and highlights\nareas for further research.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}