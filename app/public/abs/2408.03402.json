{"id":"2408.03402","title":"ULLME: A Unified Framework for Large Language Model Embeddings with\n  Generation-Augmented Learning","authors":"Hieu Man, Nghia Trung Ngo, Franck Dernoncourt and Thien Huu Nguyen","authorsParsed":[["Man","Hieu",""],["Ngo","Nghia Trung",""],["Dernoncourt","Franck",""],["Nguyen","Thien Huu",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 18:53:54 GMT"}],"updateDate":"2024-08-08","timestamp":1722970434000,"abstract":"  Large Language Models (LLMs) excel in various natural language processing\ntasks, but leveraging them for dense passage embedding remains challenging.\nThis is due to their causal attention mechanism and the misalignment between\ntheir pre-training objectives and the text ranking tasks. Despite some recent\nefforts to address these issues, existing frameworks for LLM-based text\nembeddings have been limited by their support for only a limited range of LLM\narchitectures and fine-tuning strategies, limiting their practical application\nand versatility. In this work, we introduce the Unified framework for Large\nLanguage Model Embedding (ULLME), a flexible, plug-and-play implementation that\nenables bidirectional attention across various LLMs and supports a range of\nfine-tuning strategies. We also propose Generation-augmented Representation\nLearning (GRL), a novel fine-tuning method to boost LLMs for text embedding\ntasks. GRL enforces consistency between representation-based and\ngeneration-based relevance scores, leveraging LLMs' powerful generative\nabilities for learning passage embeddings. To showcase our framework's\nflexibility and effectiveness, we release three pre-trained models from ULLME\nwith different backbone architectures, ranging from 1.5B to 8B parameters, all\nof which demonstrate strong performance on the Massive Text Embedding\nBenchmark. Our framework is publicly available at:\nhttps://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found\nat https://rb.gy/ws1ile.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HiVWVTRDcm9pB05lazdlHwjS3JHnRUcT5f7TuNx0fJo","pdfSize":"348484"}
