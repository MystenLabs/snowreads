{"id":"2407.11745","title":"Universal Sound Separation with Self-Supervised Audio Masked Autoencoder","authors":"Junqi Zhao, Xubo Liu, Jinzheng Zhao, Yi Yuan, Qiuqiang Kong, Mark D.\n  Plumbley, Wenwu Wang","authorsParsed":[["Zhao","Junqi",""],["Liu","Xubo",""],["Zhao","Jinzheng",""],["Yuan","Yi",""],["Kong","Qiuqiang",""],["Plumbley","Mark D.",""],["Wang","Wenwu",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:11:44 GMT"}],"updateDate":"2024-07-17","timestamp":1721139104000,"abstract":"  Universal sound separation (USS) is a task of separating mixtures of\narbitrary sound sources. Typically, universal separation models are trained\nfrom scratch in a supervised manner, using labeled data. Self-supervised\nlearning (SSL) is an emerging deep learning approach that leverages unlabeled\ndata to obtain task-agnostic representations, which can benefit many downstream\ntasks. In this paper, we propose integrating a self-supervised pre-trained\nmodel, namely the audio masked autoencoder (A-MAE), into a universal sound\nseparation system to enhance its separation performance. We employ two\nstrategies to utilize SSL embeddings: freezing or updating the parameters of\nA-MAE during fine-tuning. The SSL embeddings are concatenated with the\nshort-time Fourier transform (STFT) to serve as input features for the\nseparation model. We evaluate our methods on the AudioSet dataset, and the\nexperimental results indicate that the proposed methods successfully enhance\nthe separation performance of a state-of-the-art ResUNet-based USS model.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}