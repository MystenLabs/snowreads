{"id":"2408.05075","title":"DeepInteraction++: Multi-Modality Interaction for Autonomous Driving","authors":"Zeyu Yang, Nan Song, Wei Li, Xiatian Zhu, Li Zhang, Philip H.S. Torr","authorsParsed":[["Yang","Zeyu",""],["Song","Nan",""],["Li","Wei",""],["Zhu","Xiatian",""],["Zhang","Li",""],["Torr","Philip H. S.",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 14:04:21 GMT"},{"version":"v2","created":"Thu, 15 Aug 2024 11:03:41 GMT"}],"updateDate":"2024-08-16","timestamp":1723212261000,"abstract":"  Existing top-performance autonomous driving systems typically rely on the\nmulti-modal fusion strategy for reliable scene understanding. This design is\nhowever fundamentally restricted due to overlooking the modality-specific\nstrengths and finally hampering the model performance. To address this\nlimitation, in this work, we introduce a novel modality interaction strategy\nthat allows individual per-modality representations to be learned and\nmaintained throughout, enabling their unique characteristics to be exploited\nduring the whole perception pipeline. To demonstrate the effectiveness of the\nproposed strategy, we design DeepInteraction++, a multi-modal interaction\nframework characterized by a multi-modal representational interaction encoder\nand a multi-modal predictive interaction decoder. Specifically, the encoder is\nimplemented as a dual-stream Transformer with specialized attention operation\nfor information exchange and integration between separate modality-specific\nrepresentations. Our multi-modal representational learning incorporates both\nobject-centric, precise sampling-based feature alignment and global dense\ninformation spreading, essential for the more challenging planning task. The\ndecoder is designed to iteratively refine the predictions by alternately\naggregating information from separate representations in a unified\nmodality-agnostic manner, realizing multi-modal predictive interaction.\nExtensive experiments demonstrate the superior performance of the proposed\nframework on both 3D object detection and end-to-end autonomous driving tasks.\nOur code is available at https://github.com/fudan-zvg/DeepInteraction.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}