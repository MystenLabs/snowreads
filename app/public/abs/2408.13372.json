{"id":"2408.13372","title":"Understanding Defects in Generated Codes by Language Models","authors":"Ali Mohammadi Esfahani, Nafiseh Kahani, Samuel A. Ajila","authorsParsed":[["Esfahani","Ali Mohammadi",""],["Kahani","Nafiseh",""],["Ajila","Samuel A.",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 21:10:09 GMT"}],"updateDate":"2024-08-27","timestamp":1724447409000,"abstract":"  This study investigates the reliability of code generation by Large Language\nModels (LLMs), focusing on identifying and analyzing defects in the generated\ncode. Despite the advanced capabilities of LLMs in automating code generation,\nensuring the accuracy and functionality of the output remains a significant\nchallenge. By using a structured defect classification method to understand\ntheir nature and origins this study categorizes and analyzes 367 identified\ndefects from code snippets generated by LLMs, with a significant proportion\nbeing functionality and algorithm errors. These error categories indicate key\nareas where LLMs frequently fail, underscoring the need for targeted\nimprovements. To enhance the accuracy of code generation, this paper\nimplemented five prompt engineering techniques, including Scratchpad Prompting,\nProgram of Thoughts Prompting, Chain-of-Thought Prompting, Chain of Code\nPrompting, and Structured Chain-of-Thought Prompting. These techniques were\napplied to refine the input prompts, aiming to reduce ambiguities and improve\nthe models' accuracy rate. The research findings suggest that precise and\nstructured prompting significantly mitigates common defects, thereby increasing\nthe reliability of LLM-generated code.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}