{"id":"2407.02369","title":"Two-Step Q-Learning","authors":"Antony Vijesh, Shreyas S R","authorsParsed":[["Vijesh","Antony",""],["R","Shreyas S",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 15:39:00 GMT"}],"updateDate":"2024-07-03","timestamp":1719934740000,"abstract":"  Q-learning is a stochastic approximation version of the classic value\niteration. The literature has established that Q-learning suffers from both\nmaximization bias and slower convergence. Recently, multi-step algorithms have\nshown practical advantages over existing methods. This paper proposes a novel\noff-policy two-step Q-learning algorithms, without importance sampling. With\nsuitable assumption it was shown that, iterates in the proposed two-step\nQ-learning is bounded and converges almost surely to the optimal Q-values. This\nstudy also address the convergence analysis of the smooth version of two-step\nQ-learning, i.e., by replacing max function with the log-sum-exp function. The\nproposed algorithms are robust and easy to implement. Finally, we test the\nproposed algorithms on benchmark problems such as the roulette problem,\nmaximization bias problem, and randomly generated Markov decision processes and\ncompare it with the existing methods available in literature. Numerical\nexperiments demonstrate the superior performance of both the two-step\nQ-learning and its smooth variants.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}