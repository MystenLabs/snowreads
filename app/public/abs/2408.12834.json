{"id":"2408.12834","title":"CLLMFS: A Contrastive Learning enhanced Large Language Model Framework\n  for Few-Shot Named Entity Recognition","authors":"Yafeng Zhang, Zilan Yu, Yuang Huang and Jing Tang","authorsParsed":[["Zhang","Yafeng",""],["Yu","Zilan",""],["Huang","Yuang",""],["Tang","Jing",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 04:44:05 GMT"}],"updateDate":"2024-08-26","timestamp":1724388245000,"abstract":"  Few-shot Named Entity Recognition (NER), the task of identifying named\nentities with only a limited amount of labeled data, has gained increasing\nsignificance in natural language processing. While existing methodologies have\nshown some effectiveness, such as enriching label semantics through various\nprompting modes or employing metric learning techniques, their performance\nexhibits limited robustness across diverse domains due to the lack of rich\nknowledge in their pre-trained models. To address this issue, we propose\nCLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework\nfor Few-Shot Named Entity Recognition, achieving promising results with limited\ntraining data. Considering the impact of LLM's internal representations on\ndownstream tasks, CLLMFS integrates Low-Rank Adaptation (LoRA) and contrastive\nlearning mechanisms specifically tailored for few-shot NER. By enhancing the\nmodel's internal representations, CLLMFS effectively improves both entity\nboundary awareness ability and entity recognition accuracy. Our method has\nachieved state-of-the-art performance improvements on F1-score ranging from\n2.58\\% to 97.74\\% over existing best-performing methods across several\nrecognized benchmarks. Furthermore, through cross-domain NER experiments\nconducted on multiple datasets, we have further validated the robust\ngeneralization capability of our method. Our code will be released in the near\nfuture.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}