{"id":"2407.15904","title":"Comprehensive Study on Performance Evaluation and Optimization of Model\n  Compression: Bridging Traditional Deep Learning and Large Language Models","authors":"Aayush Saxena, Arit Kumar Bishwas, Ayush Ashok Mishra, Ryan Armstrong","authorsParsed":[["Saxena","Aayush",""],["Bishwas","Arit Kumar",""],["Mishra","Ayush Ashok",""],["Armstrong","Ryan",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 14:20:53 GMT"}],"updateDate":"2024-07-24","timestamp":1721658053000,"abstract":"  Deep learning models have achieved tremendous success in most of the\nindustries in recent years. The evolution of these models has also led to an\nincrease in the model size and energy requirement, making it difficult to\ndeploy in production on low compute devices. An increase in the number of\nconnected devices around the world warrants compressed models that can be\neasily deployed at the local devices with low compute capacity and power\naccessibility. A wide range of solutions have been proposed by different\nresearchers to reduce the size and complexity of such models, prominent among\nthem are, Weight Quantization, Parameter Pruning, Network Pruning, low-rank\nrepresentation, weights sharing, neural architecture search, knowledge\ndistillation etc. In this research work, we investigate the performance impacts\non various trained deep learning models, compressed using quantization and\npruning techniques. We implemented both, quantization and pruning, compression\ntechniques on popular deep learning models used in the image classification,\nobject detection, language models and generative models-based problem\nstatements. We also explored performance of various large language models\n(LLMs) after quantization and low rank adaptation. We used the standard\nevaluation metrics (model's size, accuracy, and inference time) for all the\nrelated problem statements and concluded this paper by discussing the\nchallenges and future work.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}