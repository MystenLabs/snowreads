{"id":"2407.09639","title":"Computation of Generalized Derivatives for Abs-Smooth Functions by\n  Backward Mode Algorithmic Differentiation and Implications to Deep Learning","authors":"Lukas Baumg\\\"artner, Franz Bethke","authorsParsed":[["Baumg√§rtner","Lukas",""],["Bethke","Franz",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 18:54:33 GMT"}],"updateDate":"2024-07-16","timestamp":1720810473000,"abstract":"  Algorithmic differentiation (AD) tools allow to obtain gradient information\nof a continuously differentiable objective function in a computationally cheap\nway using the so-called backward mode. It is common practice to use the same\ntools even in the absence of differentiability, although the resulting vectors\nmay not be generalized gradients in the sense of Clarke. The paper at hand\nfocuses on objectives in which the non-differentiability arises solely from the\nevaluation of the absolute value function. In that case, an algebraic condition\nbased on the evaluation procedure of the objective is identified, that\nguarantees that Clarke gradients are correctly computed without requiring any\nmodifications of the AD tool in question. The analysis allows to prove that any\nstandard AD tool is adequate to drive a stochastic generalized gradient descent\nmethod for training a dense neural network with ReLU activations. The same is\ntrue for generalized batch gradients or the full generalized gradient, provided\nthat the AD tool makes a deterministic and agnostic choice for the derivative\ninformation of the absolute value at 0.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}