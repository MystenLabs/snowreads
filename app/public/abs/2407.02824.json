{"id":"2407.02824","title":"Exploring the Capabilities of LLMs for Code Change Related Tasks","authors":"Lishui Fan, Jiakun Liu, Zhongxin Liu, David Lo, Xin Xia, Shanping Li","authorsParsed":[["Fan","Lishui",""],["Liu","Jiakun",""],["Liu","Zhongxin",""],["Lo","David",""],["Xia","Xin",""],["Li","Shanping",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 05:49:18 GMT"}],"updateDate":"2024-07-04","timestamp":1719985758000,"abstract":"  Developers deal with code-change-related tasks daily, e.g., reviewing code.\nPre-trained code and code-change-oriented models have been adapted to help\ndevelopers with such tasks. Recently, large language models (LLMs) have shown\ntheir effectiveness in code-related tasks. However, existing LLMs for code\nfocus on general code syntax and semantics rather than the differences between\ntwo code versions. Thus, it is an open question how LLMs perform on\ncode-change-related tasks.\n  To answer this question, we conduct an empirical study using \\textgreater 1B\nparameters LLMs on three code-change-related tasks, i.e., code review\ngeneration, commit message generation, and just-in-time comment update, with\nin-context learning (ICL) and parameter-efficient fine-tuning (PEFT, including\nLoRA and prefix-tuning). We observe that the performance of LLMs is poor\nwithout examples and generally improves with examples, but more examples do not\nalways lead to better performance. LLMs tuned with LoRA have comparable\nperformance to the state-of-the-art small pre-trained models. Larger models are\nnot always better, but \\textsc{Llama~2} and \\textsc{Code~Llama} families are\nalways the best. The best LLMs outperform small pre-trained models on the code\nchanges that only modify comments and perform comparably on other code changes.\nWe suggest future work should focus more on guiding LLMs to learn the knowledge\nspecific to the changes related to code rather than comments for\ncode-change-related tasks.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}