{"id":"2408.04645","title":"Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors\n  for a Robotics Course","authors":"Sebastian Kahl, Felix L\\\"offler, Martin Maciol, Fabian Ridder, Marius\n  Schmitz, Jennifer Spanagel, Jens Wienkamp, Christopher Burgahn, Malte\n  Schilling","authorsParsed":[["Kahl","Sebastian",""],["LÃ¶ffler","Felix",""],["Maciol","Martin",""],["Ridder","Fabian",""],["Schmitz","Marius",""],["Spanagel","Jennifer",""],["Wienkamp","Jens",""],["Burgahn","Christopher",""],["Schilling","Malte",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 19:49:19 GMT"}],"updateDate":"2024-08-12","timestamp":1722628159000,"abstract":"  This study evaluates the performance of Large Language Models (LLMs) as an\nArtificial Intelligence-based tutor for a university course. In particular,\ndifferent advanced techniques are utilized, such as prompt engineering,\nRetrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the\ndifferent models and applied techniques using common similarity metrics like\nBLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of\nhelpfulness and trustworthiness. Our findings indicate that RAG combined with\nprompt engineering significantly enhances model responses and produces better\nfactual answers. In the context of education, RAG appears as an ideal technique\nas it is based on enriching the input of the model with additional information\nand material which usually is already present for a university course.\nFine-tuning, on the other hand, can produce quite small, still strong expert\nmodels, but poses the danger of overfitting. Our study further asks how we\nmeasure performance of LLMs and how well current measurements represent\ncorrectness or relevance? We find high correlation on similarity metrics and a\nbias of most of these metrics towards shorter responses. Overall, our research\npoints to both the potential and challenges of integrating LLMs in educational\nsettings, suggesting a need for balanced training approaches and advanced\nevaluation frameworks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/"}