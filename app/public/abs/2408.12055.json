{"id":"2408.12055","title":"Aligning (Medical) LLMs for (Counterfactual) Fairness","authors":"Raphael Poulain, Hamed Fayyaz, Rahmatollah Beheshti","authorsParsed":[["Poulain","Raphael",""],["Fayyaz","Hamed",""],["Beheshti","Rahmatollah",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 01:11:27 GMT"}],"updateDate":"2024-08-23","timestamp":1724289087000,"abstract":"  Large Language Models (LLMs) have emerged as promising solutions for a\nvariety of medical and clinical decision support applications. However, LLMs\nare often subject to different types of biases, which can lead to unfair\ntreatment of individuals, worsening health disparities, and reducing trust in\nAI-augmented medical tools. Aiming to address this important issue, in this\nstudy, we present a new model alignment approach for aligning LLMs using a\npreference optimization method within a knowledge distillation framework. Prior\nto presenting our proposed method, we first use an evaluation framework to\nconduct a comprehensive (largest to our knowledge) empirical evaluation to\nreveal the type and nature of existing biases in LLMs used for medical\napplications. We then offer a bias mitigation technique to reduce the unfair\npatterns in LLM outputs across different subgroups identified by the protected\nattributes. We show that our mitigation method is effective in significantly\nreducing observed biased patterns. Our code is publicly available at\n\\url{https://github.com/healthylaife/FairAlignmentLLM}.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}