{"id":"2407.02350","title":"Conceptual Codebook Learning for Vision-Language Models","authors":"Yi Zhang, Ke Yu, Siqi Wu and Zhihai He","authorsParsed":[["Zhang","Yi",""],["Yu","Ke",""],["Wu","Siqi",""],["He","Zhihai",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 15:16:06 GMT"},{"version":"v2","created":"Fri, 5 Jul 2024 12:45:32 GMT"},{"version":"v3","created":"Mon, 15 Jul 2024 14:00:24 GMT"}],"updateDate":"2024-07-16","timestamp":1719933366000,"abstract":"  In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}