{"id":"2408.15710","title":"Conan-embedding: General Text Embedding with More and Better Negative\n  Samples","authors":"Shiyu Li, Yang Tang, Shizhe Chen, Xi Chen","authorsParsed":[["Li","Shiyu",""],["Tang","Yang",""],["Chen","Shizhe",""],["Chen","Xi",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 11:18:06 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 14:47:37 GMT"}],"updateDate":"2024-08-30","timestamp":1724843886000,"abstract":"  With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}