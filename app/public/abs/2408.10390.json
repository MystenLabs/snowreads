{"id":"2408.10390","title":"Self-Refined Generative Foundation Models for Wireless Traffic\n  Prediction","authors":"Chengming Hu, Hao Zhou, Di Wu, Xi Chen, Jun Yan, and Xue Liu","authorsParsed":[["Hu","Chengming",""],["Zhou","Hao",""],["Wu","Di",""],["Chen","Xi",""],["Yan","Jun",""],["Liu","Xue",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 20:19:00 GMT"}],"updateDate":"2024-08-21","timestamp":1724098740000,"abstract":"  With a broad range of emerging applications in 6G networks, wireless traffic\nprediction has become a critical component of network management. However, the\ndynamically shifting distribution of wireless traffic in non-stationary 6G\nnetworks presents significant challenges to achieving accurate and stable\npredictions. Motivated by recent advancements in Generative AI (GAI)-enabled 6G\nnetworks, this paper proposes a novel self-refined Large Language Model (LLM)\nfor wireless traffic prediction, namely TrafficLLM, through in-context learning\nwithout parameter fine-tuning or model training. The proposed TrafficLLM\nharnesses the powerful few-shot learning abilities of LLMs to enhance the\nscalability of traffic prediction in dynamically changing wireless\nenvironments. Specifically, our proposed TrafficLLM embraces an LLM to\niteratively refine its predictions through a three-step process: traffic\nprediction, feedback generation, and prediction refinement. Initially, the\nproposed TrafficLLM conducts traffic predictions using task-specific\ndemonstration prompts. Recognizing that LLMs may generate incorrect predictions\non the first attempt, we subsequently incorporate feedback demonstration\nprompts designed to provide multifaceted and valuable feedback related to these\ninitial predictions. Following this comprehensive feedback, our proposed\nTrafficLLM introduces refinement demonstration prompts, enabling the same LLM\nto further refine its predictions and thereby enhance prediction performance.\nThe evaluations on two realistic datasets demonstrate that the proposed\nTrafficLLM outperforms state-of-the-art methods with performance improvements\nof 23.17% and 17.09%, respectively.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"9QISbIR-fDHWTwov-LFG9fD6nphLt6ZIyTTQvdcdyEI","pdfSize":"999374"}
