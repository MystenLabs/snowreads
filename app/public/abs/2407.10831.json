{"id":"2407.10831","title":"Temporal Event Stereo via Joint Learning with Stereoscopic Flow","authors":"Hoonhee Cho, Jae-Young Kang, Kuk-Jin Yoon","authorsParsed":[["Cho","Hoonhee",""],["Kang","Jae-Young",""],["Yoon","Kuk-Jin",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:43:08 GMT"}],"updateDate":"2024-07-16","timestamp":1721058188000,"abstract":"  Event cameras are dynamic vision sensors inspired by the biological retina,\ncharacterized by their high dynamic range, high temporal resolution, and low\npower consumption. These features make them capable of perceiving 3D\nenvironments even in extreme conditions. Event data is continuous across the\ntime dimension, which allows a detailed description of each pixel's movements.\nTo fully utilize the temporally dense and continuous nature of event cameras,\nwe propose a novel temporal event stereo, a framework that continuously uses\ninformation from previous time steps. This is accomplished through the\nsimultaneous training of an event stereo matching network alongside\nstereoscopic flow, a new concept that captures all pixel movements from stereo\ncameras. Since obtaining ground truth for optical flow during training is\nchallenging, we propose a method that uses only disparity maps to train the\nstereoscopic flow. The performance of event-based stereo matching is enhanced\nby temporally aggregating information using the flows. We have achieved\nstate-of-the-art performance on the MVSEC and the DSEC datasets. The method is\ncomputationally efficient, as it stacks previous information in a cascading\nmanner. The code is available at\nhttps://github.com/mickeykang16/TemporalEventStereo.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}