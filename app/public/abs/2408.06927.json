{"id":"2408.06927","title":"Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class\n  Feature Compensator","authors":"Xin Zhang, Jiawei Du, Ping Liu, Joey Tianyi Zhou","authorsParsed":[["Zhang","Xin",""],["Du","Jiawei",""],["Liu","Ping",""],["Zhou","Joey Tianyi",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 14:29:00 GMT"}],"updateDate":"2024-08-14","timestamp":1723559340000,"abstract":"  Dataset distillation has emerged as a technique aiming to condense\ninformative features from large, natural datasets into a compact and synthetic\nform. While recent advancements have refined this technique, its performance is\nbottlenecked by the prevailing class-specific synthesis paradigm. Under this\nparadigm, synthetic data is optimized exclusively for a pre-assigned one-hot\nlabel, creating an implicit class barrier in feature condensation. This leads\nto inefficient utilization of the distillation budget and oversight of\ninter-class feature distributions, which ultimately limits the effectiveness\nand efficiency, as demonstrated in our analysis.\n  To overcome these constraints, this paper presents the Inter-class Feature\nCompensator (INFER), an innovative distillation approach that transcends the\nclass-specific data-label framework widely utilized in current dataset\ndistillation methods. Specifically, INFER leverages a Universal Feature\nCompensator (UFC) to enhance feature integration across classes, enabling the\ngeneration of multiple additional synthetic instances from a single UFC input.\nThis significantly improves the efficiency of the distillation budget.\n  Moreover, INFER enriches inter-class interactions during the distillation,\nthereby enhancing the effectiveness and generalizability of the distilled data.\nBy allowing for the linear interpolation of labels similar to those in the\noriginal dataset, INFER meticulously optimizes the synthetic data and\ndramatically reduces the size of soft labels in the synthetic dataset to almost\nzero, establishing a new benchmark for efficiency and effectiveness in dataset\ndistillation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}