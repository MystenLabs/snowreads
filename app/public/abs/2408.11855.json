{"id":"2408.11855","title":"FactorLLM: Factorizing Knowledge via Mixture of Experts for Large\n  Language Models","authors":"Zhongyu Zhao, Menghang Dong, Rongyu Zhang, Wenzhao Zheng, Yunpeng\n  Zhang, Huanrui Yang, Dalong Du, Kurt Keutzer, Shanghang Zhang","authorsParsed":[["Zhao","Zhongyu",""],["Dong","Menghang",""],["Zhang","Rongyu",""],["Zheng","Wenzhao",""],["Zhang","Yunpeng",""],["Yang","Huanrui",""],["Du","Dalong",""],["Keutzer","Kurt",""],["Zhang","Shanghang",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 16:45:16 GMT"}],"updateDate":"2024-08-23","timestamp":1723740316000,"abstract":"  Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large\nLanguage Models (LLMs) play a pivotal role in storing diverse linguistic and\nfactual knowledge. Conventional methods frequently face challenges due to\nknowledge confusion stemming from their monolithic and redundant architectures,\nwhich calls for more efficient solutions with minimal computational overhead,\nparticularly for LLMs. In this paper, we explore the FFN computation paradigm\nin LLMs and introduce FactorLLM, a novel approach that decomposes well-trained\ndense FFNs into sparse sub-networks without requiring any further\nmodifications, while maintaining the same level of performance. Furthermore, we\nembed a router from the Mixture-of-Experts (MoE), combined with our devised\nPrior-Approximate (PA) loss term that facilitates the dynamic activation of\nexperts and knowledge adaptation, thereby accelerating computational processes\nand enhancing performance using minimal training data and fine-tuning steps.\nFactorLLM thus enables efficient knowledge factorization and activates select\ngroups of experts specifically tailored to designated tasks, emulating the\ninteractive functional segmentation of the human brain. Extensive experiments\nacross various benchmarks demonstrate the effectiveness of our proposed\nFactorLLM which achieves comparable performance to the source model securing up\nto 85% model performance while obtaining over a 30% increase in inference\nspeed. Code: https://github.com/zhenwuweihe/FactorLLM.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}