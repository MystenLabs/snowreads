{"id":"2407.15130","title":"DOPRA: Decoding Over-accumulation Penalization and Re-allocation in\n  Specific Weighting Layer","authors":"Jinfeng Wei, Xiaofeng Zhang","authorsParsed":[["Wei","Jinfeng",""],["Zhang","Xiaofeng",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 11:54:49 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 09:30:57 GMT"}],"updateDate":"2024-07-24","timestamp":1721562889000,"abstract":"  In this work, we introduce DOPRA, a novel approach designed to mitigate\nhallucinations in multi-modal large language models (MLLMs). Unlike existing\nsolutions that typically involve costly supplementary training data or the\nintegration of external knowledge sources, DOPRA innovatively addresses\nhallucinations by decoding specific weighted layer penalties and\nredistribution, offering an economical and effective solution without\nadditional resources. DOPRA is grounded in unique insights into the intrinsic\nmechanisms controlling hallucinations within MLLMs, especially the models'\ntendency to over-rely on a subset of summary tokens in the self-attention\nmatrix, neglecting critical image-related information. This phenomenon is\nparticularly pronounced in certain strata. To counteract this over-reliance,\nDOPRA employs a strategy of weighted overlay penalties and redistribution in\nspecific layers, such as the 12th layer, during the decoding process.\nFurthermore, DOPRA includes a retrospective allocation process that re-examines\nthe sequence of generated tokens, allowing the algorithm to reallocate token\nselection to better align with the actual image content, thereby reducing the\nincidence of hallucinatory descriptions in auto-generated captions. Overall,\nDOPRA represents a significant step forward in improving the output quality of\nMLLMs by systematically reducing hallucinations through targeted adjustments\nduring the decoding process.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}