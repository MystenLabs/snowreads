{"id":"2407.05285","title":"Gradient Diffusion: A Perturbation-Resilient Gradient Leakage Attack","authors":"Xuan Liu, Siqi Cai, Qihua Zhou, Song Guo, Ruibin Li, Kaiwei Lin","authorsParsed":[["Liu","Xuan",""],["Cai","Siqi",""],["Zhou","Qihua",""],["Guo","Song",""],["Li","Ruibin",""],["Lin","Kaiwei",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 07:06:49 GMT"}],"updateDate":"2024-07-09","timestamp":1720336009000,"abstract":"  Recent years have witnessed the vulnerability of Federated Learning (FL)\nagainst gradient leakage attacks, where the private training data can be\nrecovered from the exchanged gradients, making gradient protection a critical\nissue for the FL training process. Existing solutions often resort to\nperturbation-based mechanisms, such as differential privacy, where each\nparticipating client injects a specific amount of noise into local gradients\nbefore aggregating to the server, and the global distribution variation finally\nconceals the gradient privacy. However, perturbation is not always the panacea\nfor gradient protection since the robustness heavily relies on the injected\nnoise. This intuition raises an interesting question: \\textit{is it possible to\ndeactivate existing protection mechanisms by removing the perturbation inside\nthe gradients?} In this paper, we present the answer: \\textit{yes} and propose\nthe Perturbation-resilient Gradient Leakage Attack (PGLA), the first attempt to\nrecover the perturbed gradients, without additional access to the original\nmodel structure or third-party data. Specifically, we leverage the inherent\ndiffusion property of gradient perturbation protection and construct a novel\ndiffusion-based denoising model to implement PGLA. Our insight is that\ncapturing the disturbance level of perturbation during the diffusion reverse\nprocess can release the gradient denoising capability, which promotes the\ndiffusion model to generate approximate gradients as the original clean version\nthrough adaptive sampling steps. Extensive experiments demonstrate that PGLA\neffectively recovers the protected gradients and exposes the FL training\nprocess to the threat of gradient leakage, achieving the best quality in\ngradient denoising and data recovery compared to existing models. We hope to\narouse public attention on PGLA and its defense.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}