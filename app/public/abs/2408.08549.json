{"id":"2408.08549","title":"Vulnerability Handling of AI-Generated Code -- Existing Solutions and\n  Open Challenges","authors":"Sabrina Kaniewski, Dieter Holstein, Fabian Schmidt, Tobias Heer","authorsParsed":[["Kaniewski","Sabrina",""],["Holstein","Dieter",""],["Schmidt","Fabian",""],["Heer","Tobias",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 06:31:44 GMT"}],"updateDate":"2024-08-19","timestamp":1723789904000,"abstract":"  The increasing use of generative Artificial Intelligence (AI) in modern\nsoftware engineering, particularly Large Language Models (LLMs) for code\ngeneration, has transformed professional software development by boosting\nproductivity and automating development processes. This adoption, however, has\nhighlighted a significant issue: the introduction of security vulnerabilities\ninto the code. These vulnerabilities result, e.g., from flaws in the training\ndata that propagate into the generated code, creating challenges in disclosing\nthem. Traditional vulnerability handling processes often involve extensive\nmanual review. Applying such traditional processes to AI-generated code is\nchallenging. AI-generated code may include several vulnerabilities, possibly in\nslightly different forms as developers might not build on already implemented\ncode but prompt similar tasks. In this work, we explore the current state of\nLLM-based approaches for vulnerability handling, focusing on approaches for\nvulnerability detection, localization, and repair. We provide an overview of\nrecent progress in this area and highlight open challenges that must be\naddressed in order to establish a reliable and scalable vulnerability handling\nprocess of AI-generated code.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}