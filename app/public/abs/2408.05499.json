{"id":"2408.05499","title":"LLMServingSim: A HW/SW Co-Simulation Infrastructure for LLM Inference\n  Serving at Scale","authors":"Jaehong Cho, Minsu Kim, Hyunmin Choi, Guseul Heo, Jongse Park","authorsParsed":[["Cho","Jaehong",""],["Kim","Minsu",""],["Choi","Hyunmin",""],["Heo","Guseul",""],["Park","Jongse",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 09:26:15 GMT"}],"updateDate":"2024-08-13","timestamp":1723281975000,"abstract":"  Recently, there has been an extensive research effort in building efficient\nlarge language model (LLM) inference serving systems. These efforts not only\ninclude innovations in the algorithm and software domains but also constitute\ndevelopments of various hardware acceleration techniques. Nevertheless, there\nis a lack of simulation infrastructure capable of accurately modeling versatile\nhardware-software behaviors in LLM serving systems without extensively\nextending the simulation time. This paper aims to develop an effective\nsimulation tool, called LLMServingSim, to support future research in LLM\nserving systems. In designing LLMServingSim, we focus on two limitations of\nexisting simulators: (1) they lack consideration of the dynamic workload\nvariations of LLM inference serving due to its autoregressive nature, and (2)\nthey incur repetitive simulations without leveraging algorithmic redundancies\nin LLMs. To address these limitations, LLMServingSim simulates the LLM serving\nin the granularity of iterations, leveraging the computation redundancies\nacross decoder blocks and reusing the simulation results from previous\niterations. Additionally, LLMServingSim provides a flexible framework that\nallows users to plug in any accelerator compiler-and-simulation stacks for\nexploring various system designs with heterogeneous processors. Our experiments\ndemonstrate that LLMServingSim produces simulation results closely following\nthe performance behaviors of real GPU-based LLM serving system with less than\n14.7% error rate, while offering 91.5x faster simulation speed compared to\nexisting accelerator simulators.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}