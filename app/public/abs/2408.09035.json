{"id":"2408.09035","title":"Multi Teacher Privileged Knowledge Distillation for Multimodal\n  Expression Recognition","authors":"Muhammad Haseeb Aslam, Marco Pedersoli, Alessandro Lameiras Koerich,\n  Eric Granger","authorsParsed":[["Aslam","Muhammad Haseeb",""],["Pedersoli","Marco",""],["Koerich","Alessandro Lameiras",""],["Granger","Eric",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 22:11:01 GMT"}],"updateDate":"2024-08-20","timestamp":1723846261000,"abstract":"  Human emotion is a complex phenomenon conveyed and perceived through facial\nexpressions, vocal tones, body language, and physiological signals. Multimodal\nemotion recognition systems can perform well because they can learn\ncomplementary and redundant semantic information from diverse sensors. In\nreal-world scenarios, only a subset of the modalities employed for training may\nbe available at test time. Learning privileged information allows a model to\nexploit data from additional modalities that are only available during\ntraining. SOTA methods for PKD have been proposed to distill information from a\nteacher model (with privileged modalities) to a student model (without\nprivileged modalities). However, such PKD methods utilize point-to-point\nmatching and do not explicitly capture the relational information. Recently,\nmethods have been proposed to distill the structural information. However, PKD\nmethods based on structural similarity are primarily confined to learning from\na single joint teacher representation, which limits their robustness, accuracy,\nand ability to learn from diverse multimodal sources. In this paper, a\nmulti-teacher PKD (MT-PKDOT) method with self-distillation is introduced to\nalign diverse teacher representations before distilling them to the student.\nMT-PKDOT employs a structural similarity KD mechanism based on a regularized\noptimal transport (OT) for distillation. The proposed MT-PKDOT method was\nvalidated on the Affwild2 and Biovid datasets. Results indicate that our\nproposed method can outperform SOTA PKD methods. It improves the visual-only\nbaseline on Biovid data by 5.5%. On the Affwild2 dataset, the proposed method\nimproves 3% and 5% over the visual-only baseline for valence and arousal\nrespectively. Allowing the student to learn from multiple diverse sources is\nshown to increase the accuracy and implicitly avoids negative transfer to the\nstudent model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}