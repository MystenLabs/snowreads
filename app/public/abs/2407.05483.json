{"id":"2407.05483","title":"Just read twice: closing the recall gap for recurrent language models","authors":"Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri\n  Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, and Christopher R\\'e","authorsParsed":[["Arora","Simran",""],["Timalsina","Aman",""],["Singhal","Aaryan",""],["Spector","Benjamin",""],["Eyuboglu","Sabri",""],["Zhao","Xinyi",""],["Rao","Ashish",""],["Rudra","Atri",""],["RÃ©","Christopher",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 19:55:09 GMT"}],"updateDate":"2024-07-09","timestamp":1720382109000,"abstract":"  Recurrent large language models that compete with Transformers in language\nmodeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV).\nExcitingly, these architectures use a constant amount of memory during\ninference. However, due to the limited memory, recurrent LMs cannot recall and\nuse all the information in long contexts leading to brittle in-context learning\n(ICL) quality. A key challenge for efficient LMs is selecting what information\nto store versus discard. In this work, we observe the order in which\ninformation is shown to the LM impacts the selection difficulty. To formalize\nthis, we show that the hardness of information recall reduces to the hardness\nof a problem called set disjointness (SD), a quintessential problem in\ncommunication complexity that requires a streaming algorithm (e.g., recurrent\nmodel) to decide whether inputted sets are disjoint. We empirically and\ntheoretically show that the recurrent memory required to solve SD changes with\nset order, i.e., whether the smaller set appears first in-context. Our analysis\nsuggests, to mitigate the reliance on data order, we can put information in the\nright order in-context or process prompts non-causally. Towards that end, we\npropose: (1) JRT-Prompt, where context gets repeated multiple times in the\nprompt, effectively showing the model all data orders. This gives $11.0 \\pm\n1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL\ntasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation\nprefill (length $32$k, batch size $16$, NVidia H100). We then propose (2)\nJRT-RNN, which uses non-causal prefix-linear-attention to process prompts and\nprovides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and\n$96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with\n$19.2\\times$ higher throughput for prefill than FA2.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}