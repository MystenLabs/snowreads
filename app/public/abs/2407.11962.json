{"id":"2407.11962","title":"Motion-Oriented Compositional Neural Radiance Fields for Monocular\n  Dynamic Human Modeling","authors":"Jaehyeok Kim, Dongyoon Wee, Dan Xu","authorsParsed":[["Kim","Jaehyeok",""],["Wee","Dongyoon",""],["Xu","Dan",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:59:01 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 08:44:16 GMT"}],"updateDate":"2024-07-19","timestamp":1721152741000,"abstract":"  This paper introduces Motion-oriented Compositional Neural Radiance Fields\n(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of\nmonocular human videos via novel non-rigid motion modeling approach. In the\ncontext of dynamic clothed humans, complex cloth dynamics generate non-rigid\nmotions that are intrinsically distinct from skeletal articulations and\ncritically important for the rendering quality. The conventional approach\nmodels non-rigid motions as spatial (3D) deviations in addition to skeletal\ntransformations. However, it is either time-consuming or challenging to achieve\noptimal quality due to its high learning complexity without a direct\nsupervision. To target this problem, we propose a novel approach of modeling\nnon-rigid motions as radiance residual fields to benefit from more direct color\nsupervision in the rendering and utilize the rigid radiance fields as a prior\nto reduce the complexity of the learning process. Our approach utilizes a\nsingle multiresolution hash encoding (MHE) to concurrently learn the canonical\nT-pose representation from rigid skeletal motions and the radiance residual\nfield for non-rigid motions. Additionally, to further improve both training\nefficiency and usability, we extend MoCo-NeRF to support simultaneous training\nof multiple subjects within a single framework, thanks to our effective design\nfor modeling non-rigid motions. This scalability is achieved through the\nintegration of a global MHE and learnable identity codes in addition to\nmultiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,\nclearly demonstrating state-of-the-art performance in both single- and\nmulti-subject settings. The code and model will be made publicly available at\nthe project page: https://stevejaehyeok.github.io/publications/moco-nerf.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}