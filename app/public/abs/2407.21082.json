{"id":"2407.21082","title":"Accelerating Large Language Model Inference with Self-Supervised Early\n  Exits","authors":"Florian Valade","authorsParsed":[["Valade","Florian",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 07:58:28 GMT"}],"updateDate":"2024-08-01","timestamp":1722326308000,"abstract":"  This paper presents a novel technique for accelerating inference in large,\npre-trained language models (LLMs) by introducing early exits during inference.\nThe computational demands of these models, used across a wide range of\napplications, can be substantial. By capitalizing on the inherent variability\nin token complexity, our approach enables selective acceleration of the\ninference process. Specifically, we propose the integration of early exit\n''heads'' atop existing transformer layers, which facilitate conditional\nterminations based on a confidence metric. These heads are trained in a\nself-supervised manner using the model's own predictions as training data,\nthereby eliminating the need for additional annotated data. The confidence\nmetric, established using a calibration set, ensures a desired level of\naccuracy while enabling early termination when confidence exceeds a\npredetermined threshold. Notably, our method preserves the original accuracy\nand reduces computational time on certain tasks, leveraging the existing\nknowledge of pre-trained LLMs without requiring extensive retraining. This\nlightweight, modular modification has the potential to greatly enhance the\npractical usability of LLMs, particularly in applications like real-time\nlanguage processing in resource-constrained environments.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}