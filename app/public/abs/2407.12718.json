{"id":"2407.12718","title":"SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow","authors":"Yuanzhi Zhu, Xingchao Liu, Qiang Liu","authorsParsed":[["Zhu","Yuanzhi",""],["Liu","Xingchao",""],["Liu","Qiang",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 16:38:45 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 03:23:13 GMT"}],"updateDate":"2024-07-19","timestamp":1721234325000,"abstract":"  Diffusion models excel in high-quality generation but suffer from slow\ninference due to iterative sampling. While recent methods have successfully\ntransformed diffusion models into one-step generators, they neglect model size\nreduction, limiting their applicability in compute-constrained scenarios. This\npaper aims to develop small, efficient one-step diffusion models based on the\npowerful rectified flow framework, by exploring joint compression of inference\nsteps and model size. The rectified flow framework trains one-step generative\nmodels using two operations, reflow and distillation. Compared with the\noriginal framework, squeezing the model size brings two new challenges: (1) the\ninitialization mismatch between large teachers and small students during\nreflow; (2) the underperformance of naive distillation on small student models.\nTo overcome these issues, we propose Annealing Reflow and Flow-Guided\nDistillation, which together comprise our SlimFlow framework. With our novel\nframework, we train a one-step diffusion model with an FID of 5.02 and 15.7M\nparameters, outperforming the previous state-of-the-art one-step diffusion\nmodel (FID=6.47, 19.4M parameters) on CIFAR10. On ImageNet 64$\\times$64 and\nFFHQ 64$\\times$64, our method yields small one-step diffusion models that are\ncomparable to larger models, showcasing the effectiveness of our method in\ncreating compact, efficient one-step diffusion models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}