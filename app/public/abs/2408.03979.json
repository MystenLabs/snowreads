{"id":"2408.03979","title":"Speaker Adaptation for Quantised End-to-End ASR Models","authors":"Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng","authorsParsed":[["Zhao","Qiuming",""],["Sun","Guangzhi",""],["Zhang","Chao",""],["Xu","Mingxing",""],["Zheng","Thomas Fang",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 15:20:53 GMT"}],"updateDate":"2024-08-09","timestamp":1723044053000,"abstract":"  End-to-end models have shown superior performance for automatic speech\nrecognition (ASR). However, such models are often very large in size and thus\nchallenging to deploy on resource-constrained edge devices. While quantisation\ncan reduce model sizes, it can lead to increased word error rates (WERs).\nAlthough improved quantisation methods were proposed to address the issue of\nperformance degradation, the fact that quantised models deployed on edge\ndevices often target only on a small group of users is under-explored. To this\nend, we propose personalisation for quantised models (P4Q), a novel strategy\nthat uses speaker adaptation (SA) to improve quantised end-to-end ASR models by\nfitting them to the characteristics of the target speakers. In this paper, we\nstudy the P4Q strategy based on Whisper and Conformer attention-based\nencoder-decoder (AED) end-to-end ASR models, which leverages a 4-bit block-wise\nNormalFloat4 (NF4) approach for quantisation and the low-rank adaptation (LoRA)\napproach for SA. Experimental results on the LibriSpeech and the TED-LIUM 3\ncorpora show that, with a 7-time reduction in model size and 1% extra\nspeaker-specific parameters, 15.1% and 23.3% relative WER reductions were\nachieved on quantised Whisper and Conformer AED models respectively, comparing\nto the full precision models.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}