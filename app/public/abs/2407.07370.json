{"id":"2407.07370","title":"LokiLM: Technical Report","authors":"Justin Kiefel and Shrey Shah","authorsParsed":[["Kiefel","Justin",""],["Shah","Shrey",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 05:05:47 GMT"}],"updateDate":"2024-07-11","timestamp":1720587947000,"abstract":"  In this work, we introduce LokiLM, a 1.4B parameter large language model\ntrained on 500B tokens. Our model performs strongly in natural language\nreasoning tasks and achieves state-of-the-art performance among models with\n1.5B parameters or less. LokiLM is trained using multi-teacher knowledge\ndistillation and high-quality training data to achieve benchmark results\ncompetitive with larger models trained on significantly more tokens. We support\nthese findings by introducing steps to avoid benchmark contamination and\noverfitting throughout our development process. Despite its promising\nperformance, LokiLM exhibits a concerning amount of hallucinations and scores\npoorly on the TruthfulQA benchmark, so we do not release the model publicly.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}