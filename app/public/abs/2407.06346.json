{"id":"2407.06346","title":"High-Dimensional Distributed Sparse Classification with Scalable\n  Communication-Efficient Global Updates","authors":"Fred Lu, Ryan R. Curtin, Edward Raff, Francis Ferraro, James Holt","authorsParsed":[["Lu","Fred",""],["Curtin","Ryan R.",""],["Raff","Edward",""],["Ferraro","Francis",""],["Holt","James",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 19:34:39 GMT"}],"updateDate":"2024-07-10","timestamp":1720467279000,"abstract":"  As the size of datasets used in statistical learning continues to grow,\ndistributed training of models has attracted increasing attention. These\nmethods partition the data and exploit parallelism to reduce memory and\nruntime, but suffer increasingly from communication costs as the data size or\nthe number of iterations grows. Recent work on linear models has shown that a\nsurrogate likelihood can be optimized locally to iteratively improve on an\ninitial solution in a communication-efficient manner. However, existing\nversions of these methods experience multiple shortcomings as the data size\nbecomes massive, including diverging updates and efficiently handling sparsity.\nIn this work we develop solutions to these problems which enable us to learn a\ncommunication-efficient distributed logistic regression model even beyond\nmillions of features. In our experiments we demonstrate a large improvement in\naccuracy over distributed algorithms with only a few distributed update steps\nneeded, and similar or faster runtimes. Our code is available at\n\\url{https://github.com/FutureComputing4AI/ProxCSL}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}