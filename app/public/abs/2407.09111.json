{"id":"2407.09111","title":"Inference Optimization of Foundation Models on AI Accelerators","authors":"Youngsuk Park and Kailash Budhathoki and Liangfu Chen and Jonas\n  K\\\"ubler and Jiaji Huang and Matth\\\"aus Kleindessner and Jun Huan and Volkan\n  Cevher and Yida Wang and George Karypis","authorsParsed":[["Park","Youngsuk",""],["Budhathoki","Kailash",""],["Chen","Liangfu",""],["Kübler","Jonas",""],["Huang","Jiaji",""],["Kleindessner","Matthäus",""],["Huan","Jun",""],["Cevher","Volkan",""],["Wang","Yida",""],["Karypis","George",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 09:24:34 GMT"}],"updateDate":"2024-07-15","timestamp":1720776274000,"abstract":"  Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}