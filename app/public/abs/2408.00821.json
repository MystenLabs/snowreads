{"id":"2408.00821","title":"An FDA for AI? Pitfalls and Plausibility of Approval Regulation for\n  Frontier Artificial Intelligence","authors":"Daniel Carpenter, Carson Ezell","authorsParsed":[["Carpenter","Daniel",""],["Ezell","Carson",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:54:57 GMT"}],"updateDate":"2024-08-05","timestamp":1722534897000,"abstract":"  Observers and practitioners of artificial intelligence (AI) have proposed an\nFDA-style licensing regime for the most advanced AI models, or 'frontier'\nmodels. In this paper, we explore the applicability of approval regulation --\nthat is, regulation of a product that combines experimental minima with\ngovernment licensure conditioned partially or fully upon that experimentation\n-- to the regulation of frontier AI. There are a number of reasons to believe\nthat approval regulation, simplistically applied, would be inapposite for\nfrontier AI risks. Domains of weak fit include the difficulty of defining the\nregulated product, the presence of Knightian uncertainty or deep ambiguity\nabout harms from AI, the potentially transmissible nature of risks, and\ndistributed activities among actors involved in the AI lifecycle. We conclude\nby highlighting the role of policy learning and experimentation in regulatory\ndevelopment, describing how learning from other forms of AI regulation and\nimprovements in evaluation and testing methods can help to overcome some of the\nchallenges we identify.\n","subjects":["Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"e-ni_mOQPgWsDwyI8SAArO8O16mG5JQEiFkkkCdVRiQ","pdfSize":"659696"}
