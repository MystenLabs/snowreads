{"id":"2407.21579","title":"A Performance Study of LLM-Generated Code on Leetcode","authors":"Tristan Coignion, Cl\\'ement Quinton, Romain Rouvoy","authorsParsed":[["Coignion","Tristan",""],["Quinton","Cl√©ment",""],["Rouvoy","Romain",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 13:10:03 GMT"}],"updateDate":"2024-08-01","timestamp":1722431403000,"abstract":"  This study evaluates the efficiency of code generation by Large Language\nModels (LLMs) and measures their performance against human-crafted solutions\nusing a dataset from Leetcode. We compare 18 LLMs, considering factors such as\nmodel temperature and success rate, and their impact on code performance. This\nresearch introduces a novel method for measuring and comparing the speed of\nLLM-generated code, revealing that LLMs produce code with comparable\nperformance, irrespective of the adopted LLM. We also find that LLMs are\ncapable of generating code that is, on average, more efficient than the code\nwritten by humans. The paper further discusses the use of Leetcode as a\nbenchmarking dataset, the limitations imposed by potential data contamination,\nand the platform's measurement reliability. We believe that our findings\ncontribute to a better understanding of LLM capabilities in code generation and\nset the stage for future optimizations in the field.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}