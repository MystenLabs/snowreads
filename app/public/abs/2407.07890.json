{"id":"2407.07890","title":"Training on the Test Task Confounds Evaluation and Emergence","authors":"Ricardo Dominguez-Olmedo, Florian E. Dorner, Moritz Hardt","authorsParsed":[["Dominguez-Olmedo","Ricardo",""],["Dorner","Florian E.",""],["Hardt","Moritz",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 17:57:58 GMT"}],"updateDate":"2024-07-11","timestamp":1720634278000,"abstract":"  We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"CBetRvuiL2zJsdGoVkfP1XCrP25KfODSoxGgRAMu5Ho","pdfSize":"1148874"}
