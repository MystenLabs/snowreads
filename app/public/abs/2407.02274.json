{"id":"2407.02274","title":"DextrAH-G: Pixels-to-Action Dexterous Arm-Hand Grasping with Geometric\n  Fabrics","authors":"Tyler Ga Wei Lum, Martin Matak, Viktor Makoviychuk, Ankur Handa,\n  Arthur Allshire, Tucker Hermans, Nathan D. Ratliff, Karl Van Wyk","authorsParsed":[["Lum","Tyler Ga Wei",""],["Matak","Martin",""],["Makoviychuk","Viktor",""],["Handa","Ankur",""],["Allshire","Arthur",""],["Hermans","Tucker",""],["Ratliff","Nathan D.",""],["Van Wyk","Karl",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 14:03:49 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 02:45:21 GMT"}],"updateDate":"2024-07-08","timestamp":1719929029000,"abstract":"  A pivotal challenge in robotics is achieving fast, safe, and robust dexterous\ngrasping across a diverse range of objects, an important goal within industrial\napplications. However, existing methods often have very limited speed,\ndexterity, and generality, along with limited or no hardware safety guarantees.\nIn this work, we introduce DextrAH-G, a depth-based dexterous grasping policy\ntrained entirely in simulation that combines reinforcement learning, geometric\nfabrics, and teacher-student distillation. We address key challenges in joint\narm-hand policy learning, such as high-dimensional observation and action\nspaces, the sim2real gap, collision avoidance, and hardware constraints.\nDextrAH-G enables a 23 motor arm-hand robot to safely and continuously grasp\nand transport a large variety of objects at high speed using multi-modal inputs\nincluding depth images, allowing generalization across object geometry. Videos\nat https://sites.google.com/view/dextrah-g.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"IiDwRPmrUC47ET3N_DOJV28HLZY057W9n4NCdQ3Ho5Q","pdfSize":"29914866"}
