{"id":"2408.11804","title":"Approaching Deep Learning through the Spectral Dynamics of Weights","authors":"David Yunis, Kumar Kshitij Patel, Samuel Wheeler, Pedro Savarese, Gal\n  Vardi, Karen Livescu, Michael Maire, Matthew R. Walter","authorsParsed":[["Yunis","David",""],["Patel","Kumar Kshitij",""],["Wheeler","Samuel",""],["Savarese","Pedro",""],["Vardi","Gal",""],["Livescu","Karen",""],["Maire","Michael",""],["Walter","Matthew R.",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:48:01 GMT"}],"updateDate":"2024-08-22","timestamp":1724262481000,"abstract":"  We propose an empirical approach centered on the spectral dynamics of weights\n-- the behavior of singular values and vectors during optimization -- to unify\nand clarify several phenomena in deep learning. We identify a consistent bias\nin optimization across various experiments, from small-scale ``grokking'' to\nlarge-scale tasks like image classification with ConvNets, image generation\nwith UNets, speech recognition with LSTMs, and language modeling with\nTransformers. We also demonstrate that weight decay enhances this bias beyond\nits role as a norm regularizer, even in practical systems. Moreover, we show\nthat these spectral dynamics distinguish memorizing networks from generalizing\nones, offering a novel perspective on this longstanding conundrum.\nAdditionally, we leverage spectral dynamics to explore the emergence of\nwell-performing sparse subnetworks (lottery tickets) and the structure of the\nloss surface through linear mode connectivity. Our findings suggest that\nspectral dynamics provide a coherent framework to better understand the\nbehavior of neural networks across diverse settings.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}