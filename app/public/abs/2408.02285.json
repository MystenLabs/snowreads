{"id":"2408.02285","title":"Joint-Motion Mutual Learning for Pose Estimation in Videos","authors":"Sifan Wu, Haipeng Chen, Yifang Yin, Sihao Hu, Runyang Feng, Yingying\n  Jiao, Ziqi Yang, Zhenguang Liu","authorsParsed":[["Wu","Sifan",""],["Chen","Haipeng",""],["Yin","Yifang",""],["Hu","Sihao",""],["Feng","Runyang",""],["Jiao","Yingying",""],["Yang","Ziqi",""],["Liu","Zhenguang",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 07:37:55 GMT"}],"updateDate":"2024-08-06","timestamp":1722843475000,"abstract":"  Human pose estimation in videos has long been a compelling yet challenging\ntask within the realm of computer vision. Nevertheless, this task remains\ndifficult because of the complex video scenes, such as video defocus and\nself-occlusion. Recent methods strive to integrate multi-frame visual features\ngenerated by a backbone network for pose estimation. However, they often ignore\nthe useful joint information encoded in the initial heatmap, which is a\nby-product of the backbone generation. Comparatively, methods that attempt to\nrefine the initial heatmap fail to consider any spatio-temporal motion\nfeatures. As a result, the performance of existing methods for pose estimation\nfalls short due to the lack of ability to leverage both local joint (heatmap)\ninformation and global motion (feature) dynamics.\n  To address this problem, we propose a novel joint-motion mutual learning\nframework for pose estimation, which effectively concentrates on both local\njoint dependency and global pixel-level motion dynamics. Specifically, we\nintroduce a context-aware joint learner that adaptively leverages initial\nheatmaps and motion flow to retrieve robust local joint feature. Given that\nlocal joint feature and global motion flow are complementary, we further\npropose a progressive joint-motion mutual learning that synergistically\nexchanges information and interactively learns between joint feature and motion\nflow to improve the capability of the model. More importantly, to capture more\ndiverse joint and motion cues, we theoretically analyze and propose an\ninformation orthogonality objective to avoid learning redundant information\nfrom multi-cues. Empirical experiments show our method outperforms prior arts\non three challenging benchmarks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}