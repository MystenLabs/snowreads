{"id":"2407.13808","title":"CoAPT: Context Attribute words for Prompt Tuning","authors":"Gun Lee, Subin An, Sungyong Baik, Soochahn Lee","authorsParsed":[["Lee","Gun",""],["An","Subin",""],["Baik","Sungyong",""],["Lee","Soochahn",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 08:58:01 GMT"}],"updateDate":"2024-07-22","timestamp":1721293081000,"abstract":"  We propose a novel prompt tuning method called CoAPT(Context Attribute words\nin Prompt Tuning) for few/zero-shot image classification. The core motivation\nis that attributes are descriptive words with rich information about a given\nconcept. Thus, we aim to enrich text queries of existing prompt tuning methods,\nimproving alignment between text and image embeddings in CLIP embedding space.\nTo do so, CoAPT integrates attribute words as additional prompts within\nlearnable prompt tuning and can be easily incorporated into various existing\nprompt tuning methods. To facilitate the incorporation of attributes into text\nembeddings and the alignment with image embeddings, soft prompts are trained\ntogether with an additional meta-network that generates input-image-wise\nfeature biases from the concatenated feature encodings of the image-text\ncombined queries. Our experiments demonstrate that CoAPT leads to considerable\nimprovements for existing baseline methods on several few/zero-shot image\nclassification tasks, including base-to-novel generalization, cross-dataset\ntransfer, and domain generalization. Our findings highlight the importance of\ncombining hard and soft prompts and pave the way for future research on the\ninterplay between text and image latent spaces in pre-trained models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}