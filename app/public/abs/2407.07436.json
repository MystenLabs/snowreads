{"id":"2407.07436","title":"Alternating Subspace Approximate Message Passing","authors":"Xu Zhu, Yufei Ma, Xiaoguang Li, and Tiejun Li","authorsParsed":[["Zhu","Xu",""],["Ma","Yufei",""],["Li","Xiaoguang",""],["Li","Tiejun",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 07:45:31 GMT"}],"updateDate":"2024-07-11","timestamp":1720597531000,"abstract":"  Numerous renowned algorithms for tackling the compressed sensing problem\nemploy an alternating strategy, which typically involves data matching in one\nmodule and denoising in another. Based on an in-depth analysis of the\nconnection between the message passing and operator splitting, we present a\nnovel approach, the Alternating Subspace Method (ASM), which intuitively\ncombines the principles of the greedy methods (e.g., the orthogonal matching\npursuit type methods) and the splitting methods (e.g., the approximate message\npassing type methods). Essentially, ASM modifies the splitting method by\nachieving fidelity in a subspace-restricted fashion. We reveal that such\nconfining strategy still yields a consistent fixed point iteration and\nestablish its local geometric convergence on the lasso problem. Numerical\nexperiments on both the lasso and channel estimation problems demonstrate its\nhigh convergence rate and its capacity to incorporate different prior\ndistributions. Further theoretical analysis also demonstrates the advantage of\nthe motivated message-passing splitting by incorporating quasi-variance degree\nof freedom even for the classical lasso optimization problem. Overall, the\nproposed method is promising in efficiency, accuracy and flexibility, which has\nthe potential to be competitive in different sparse recovery applications.\n","subjects":["Computing Research Repository/Information Theory","Mathematics/Information Theory","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}