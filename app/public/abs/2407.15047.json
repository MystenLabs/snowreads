{"id":"2407.15047","title":"End-to-End Video Question Answering with Frame Scoring Mechanisms and\n  Adaptive Sampling","authors":"Jianxin Liang, Xiaojun Meng, Yueqian Wang, Chang Liu, Qun Liu, Dongyan\n  Zhao","authorsParsed":[["Liang","Jianxin",""],["Meng","Xiaojun",""],["Wang","Yueqian",""],["Liu","Chang",""],["Liu","Qun",""],["Zhao","Dongyan",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 04:09:37 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 14:56:22 GMT"}],"updateDate":"2024-07-24","timestamp":1721534977000,"abstract":"  Video Question Answering (VideoQA) has emerged as a challenging frontier in\nthe field of multimedia processing, requiring intricate interactions between\nvisual and textual modalities. Simply uniformly sampling frames or\nindiscriminately aggregating frame-level visual features often falls short in\ncapturing the nuanced and relevant contexts of videos to well perform VideoQA.\nTo mitigate these issues, we propose VidF4, a novel VideoQA framework equipped\nwith tailored frame selection strategy for effective and efficient VideoQA. We\npropose three frame-scoring mechanisms that consider both question relevance\nand inter-frame similarity to evaluate the importance of each frame for a given\nquestion on the video. Furthermore, we design a differentiable adaptive frame\nsampling mechanism to facilitate end-to-end training for the frame selector and\nanswer generator. The experimental results across three widely adopted\nbenchmarks demonstrate that our model consistently outperforms existing VideoQA\nmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA\n(+1.0%). Furthermore, through both quantitative and qualitative analyses, we\nvalidate the effectiveness of each design choice.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}