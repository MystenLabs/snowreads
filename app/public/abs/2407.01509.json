{"id":"2407.01509","title":"MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal\n  LLMs","authors":"Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei\n  Yang, Zhe Gan","authorsParsed":[["Qian","Yusu",""],["Ye","Hanrong",""],["Fauconnier","Jean-Philippe",""],["Grasch","Peter",""],["Yang","Yinfei",""],["Gan","Zhe",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:53:35 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 18:11:45 GMT"},{"version":"v3","created":"Thu, 25 Jul 2024 19:50:32 GMT"}],"updateDate":"2024-07-29","timestamp":1719856415000,"abstract":"  We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large\nlanguage models (MLLMs) on their ability to strictly adhere to complex\ninstructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,\neach crafted to challenge the models' compliance with layered instructions in\ngenerating accurate responses that satisfy specific requested patterns.\nEvaluation results from a wide array of state-of-the-art MLLMs reveal\nsignificant variations in performance, highlighting areas for improvement in\ninstruction fidelity. Additionally, we create extra training data and explore\nsupervised fine-tuning to enhance the models' ability to strictly follow\ninstructions without compromising performance on other tasks. We hope this\nbenchmark not only serves as a tool for measuring MLLM adherence to\ninstructions, but also guides future developments in MLLM training methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}