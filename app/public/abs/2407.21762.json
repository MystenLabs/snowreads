{"id":"2407.21762","title":"ReplanVLM: Replanning Robotic Tasks with Visual Language Models","authors":"Aoran Mei, Guo-Niu Zhu, Huaxiang Zhang, and Zhongxue Gan","authorsParsed":[["Mei","Aoran",""],["Zhu","Guo-Niu",""],["Zhang","Huaxiang",""],["Gan","Zhongxue",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 17:31:01 GMT"}],"updateDate":"2024-08-01","timestamp":1722447061000,"abstract":"  Large language models (LLMs) have gained increasing popularity in robotic\ntask planning due to their exceptional abilities in text analytics and\ngeneration, as well as their broad knowledge of the world. However, they fall\nshort in decoding visual cues. LLMs have limited direct perception of the\nworld, which leads to a deficient grasp of the current state of the world. By\ncontrast, the emergence of visual language models (VLMs) fills this gap by\nintegrating visual perception modules, which can enhance the autonomy of\nrobotic task planning. Despite these advancements, VLMs still face challenges,\nsuch as the potential for task execution errors, even when provided with\naccurate instructions. To address such issues, this paper proposes a ReplanVLM\nframework for robotic task planning. In this study, we focus on error\ncorrection interventions. An internal error correction mechanism and an\nexternal error correction mechanism are presented to correct errors under\ncorresponding phases. A replan strategy is developed to replan tasks or correct\nerror codes when task execution fails. Experimental results on real robots and\nin simulation environments have demonstrated the superiority of the proposed\nframework, with higher success rates and robust error correction capabilities\nin open-world tasks. Videos of our experiments are available at\nhttps://youtu.be/NPk2pWKazJc.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}