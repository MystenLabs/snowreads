{"id":"2408.01620","title":"MedUHIP: Towards Human-In-the-Loop Medical Segmentation","authors":"Jiayuan Zhu, Junde Wu","authorsParsed":[["Zhu","Jiayuan",""],["Wu","Junde",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 01:06:02 GMT"}],"updateDate":"2024-08-06","timestamp":1722647162000,"abstract":"  Although segmenting natural images has shown impressive performance, these\ntechniques cannot be directly applied to medical image segmentation. Medical\nimage segmentation is particularly complicated by inherent uncertainties. For\ninstance, the ambiguous boundaries of tissues can lead to diverse but plausible\nannotations from different clinicians. These uncertainties cause significant\ndiscrepancies in clinical interpretations and impact subsequent medical\ninterventions. Therefore, achieving quantitative segmentations from uncertain\nmedical images becomes crucial in clinical practice. To address this, we\npropose a novel approach that integrates an \\textbf{uncertainty-aware model}\nwith \\textbf{human-in-the-loop interaction}. The uncertainty-aware model\nproposes several plausible segmentations to address the uncertainties inherent\nin medical images, while the human-in-the-loop interaction iteratively modifies\nthe segmentation under clinician supervision. This collaborative model ensures\nthat segmentation is not solely dependent on automated techniques but is also\nrefined through clinician expertise. As a result, our approach represents a\nsignificant advancement in the field which enhances the safety of medical image\nsegmentation. It not only offers a comprehensive solution to produce\nquantitative segmentation from inherent uncertain medical images, but also\nestablishes a synergistic balance between algorithmic precision and clincian\nknowledge. We evaluated our method on various publicly available\nmulti-clinician annotated datasets: REFUGE2, LIDC-IDRI and QUBIQ. Our method\nshowcases superior segmentation capabilities, outperforming a wide range of\ndeterministic and uncertainty-aware models. We also demonstrated that our model\nproduced significantly better results with fewer interactions compared to\nprevious interactive models. We will release the code to foster further\nresearch in this area.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}