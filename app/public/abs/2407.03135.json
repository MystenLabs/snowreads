{"id":"2407.03135","title":"GMM-ResNext: Combining Generative and Discriminative Models for Speaker\n  Verification","authors":"Hui Yan, Zhenchun Lei, Changhong Liu, Yong Zhou","authorsParsed":[["Yan","Hui",""],["Lei","Zhenchun",""],["Liu","Changhong",""],["Zhou","Yong",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:14:18 GMT"}],"updateDate":"2024-07-04","timestamp":1720016058000,"abstract":"  With the development of deep learning, many different network architectures\nhave been explored in speaker verification. However, most network architectures\nrely on a single deep learning architecture, and hybrid networks combining\ndifferent architectures have been little studied in ASV tasks. In this paper,\nwe propose the GMM-ResNext model for speaker verification. Conventional GMM\ndoes not consider the score distribution of each frame feature over all\nGaussian components and ignores the relationship between neighboring speech\nframes. So, we extract the log Gaussian probability features based on the raw\nacoustic features and use ResNext-based network as the backbone to extract the\nspeaker embedding. GMM-ResNext combines Generative and Discriminative Models to\nimprove the generalization ability of deep learning models and allows one to\nmore easily specify meaningful priors on model parameters. A two-path\nGMM-ResNext model based on two gender-related GMMs has also been proposed. The\nExperimental results show that the proposed GMM-ResNext achieves relative\nimprovements of 48.1\\% and 11.3\\% in EER compared with ResNet34 and ECAPA-TDNN\non VoxCeleb1-O test set.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}