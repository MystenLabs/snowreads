{"id":"2407.13139","title":"Image Inpainting Models are Effective Tools for Instruction-guided Image\n  Editing","authors":"Xuan Ju, Junhao Zhuang, Zhaoyang Zhang, Yuxuan Bian, Qiang Xu, Ying\n  Shan","authorsParsed":[["Ju","Xuan",""],["Zhuang","Junhao",""],["Zhang","Zhaoyang",""],["Bian","Yuxuan",""],["Xu","Qiang",""],["Shan","Ying",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 03:55:33 GMT"}],"updateDate":"2024-07-19","timestamp":1721274933000,"abstract":"  This is the technique report for the winning solution of the CVPR2024 GenAI\nMedia Generation Challenge Workshop's Instruction-guided Image Editing track.\nInstruction-guided image editing has been largely studied in recent years. The\nmost advanced methods, such as SmartEdit and MGIE, usually combine large\nlanguage models with diffusion models through joint training, where the former\nprovides text understanding ability, and the latter provides image generation\nability. However, in our experiments, we find that simply connecting large\nlanguage models and image generation models through intermediary guidance such\nas masks instead of joint fine-tuning leads to a better editing performance and\nsuccess rate. We use a 4-step process IIIE (Inpainting-based Instruction-guided\nImage Editing): editing category classification, main editing object\nidentification, editing mask acquisition, and image inpainting. Results show\nthat through proper combinations of language models and image inpainting\nmodels, our pipeline can reach a high success rate with satisfying visual\nquality.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"djMSsW1AxT1Gnve9Q1NvK6A1trHwck_5zxEBheelMw8","pdfSize":"2631162"}
