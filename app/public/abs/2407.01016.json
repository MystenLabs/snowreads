{"id":"2407.01016","title":"SOOD++: Leveraging Unlabeled Data to Boost Oriented Object Detection","authors":"Dingkang Liang, Wei Hua, Chunsheng Shi, Zhikang Zou, Xiaoqing Ye,\n  Xiang Bai","authorsParsed":[["Liang","Dingkang",""],["Hua","Wei",""],["Shi","Chunsheng",""],["Zou","Zhikang",""],["Ye","Xiaoqing",""],["Bai","Xiang",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 07:03:51 GMT"}],"updateDate":"2024-07-02","timestamp":1719817431000,"abstract":"  Semi-supervised object detection (SSOD), leveraging unlabeled data to boost\nobject detectors, has become a hot topic recently. However, existing SSOD\napproaches mainly focus on horizontal objects, leaving multi-oriented objects\ncommon in aerial images unexplored. At the same time, the annotation cost of\nmulti-oriented objects is significantly higher than that of their horizontal\ncounterparts. Therefore, in this paper, we propose a simple yet effective\nSemi-supervised Oriented Object Detection method termed SOOD++. Specifically,\nwe observe that objects from aerial images are usually arbitrary orientations,\nsmall scales, and aggregation, which inspires the following core designs: a\nSimple Instance-aware Dense Sampling (SIDS) strategy is used to generate\ncomprehensive dense pseudo-labels; the Geometry-aware Adaptive Weighting (GAW)\nloss dynamically modulates the importance of each pair between pseudo-label and\ncorresponding prediction by leveraging the intricate geometric information of\naerial objects; we treat aerial images as global layouts and explicitly build\nthe many-to-many relationship between the sets of pseudo-labels and predictions\nvia the proposed Noise-driven Global Consistency (NGC). Extensive experiments\nconducted on various multi-oriented object datasets under various labeled\nsettings demonstrate the effectiveness of our method. For example, on the\nDOTA-V1.5 benchmark, the proposed method outperforms previous state-of-the-art\n(SOTA) by a large margin (+2.92, +2.39, and +2.57 mAP under 10%, 20%, and 30%\nlabeled data settings, respectively) with single-scale training and testing.\nMore importantly, it still improves upon a strong supervised baseline with\n70.66 mAP, trained using the full DOTA-V1.5 train-val set, by +1.82 mAP,\nresulting in a 72.48 mAP, pushing the new state-of-the-art. The code will be\nmade available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}