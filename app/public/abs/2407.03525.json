{"id":"2407.03525","title":"UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs'\n  Memorization","authors":"Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son,\n  Eduardo Blanco, Steven R. Corman, Chitta Baral","authorsParsed":[["Uddin","Md Nayem",""],["Saeidi","Amir",""],["Handa","Divij",""],["Seth","Agastya",""],["Son","Tran Cao",""],["Blanco","Eduardo",""],["Corman","Steven R.",""],["Baral","Chitta",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 22:02:07 GMT"}],"updateDate":"2024-07-08","timestamp":1720044127000,"abstract":"  This paper introduces UnSeenTimeQA, a novel time-sensitive question-answering\n(TSQA) benchmark that diverges from traditional TSQA benchmarks by avoiding\nfactual and web-searchable queries. We present a series of time-sensitive event\nscenarios decoupled from real-world factual information. It requires large\nlanguage models (LLMs) to engage in genuine temporal reasoning, disassociating\nfrom the knowledge acquired during the pre-training phase. Our evaluation of\nsix open-source LLMs (ranging from 2B to 70B in size) and three closed-source\nLLMs reveal that the questions from the UnSeenTimeQA present substantial\nchallenges. This indicates the models' difficulties in handling complex\ntemporal reasoning scenarios. Additionally, we present several analyses\nshedding light on the models' performance in answering time-sensitive\nquestions.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}