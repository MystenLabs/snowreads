{"id":"2407.03479","title":"Human-Centered Design Recommendations for LLM-as-a-Judge","authors":"Qian Pan, Zahra Ashktorab, Michael Desmond, Martin Santillan Cooper,\n  James Johnson, Rahul Nair, Elizabeth Daly, Werner Geyer","authorsParsed":[["Pan","Qian",""],["Ashktorab","Zahra",""],["Desmond","Michael",""],["Cooper","Martin Santillan",""],["Johnson","James",""],["Nair","Rahul",""],["Daly","Elizabeth",""],["Geyer","Werner",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 19:53:47 GMT"}],"updateDate":"2024-07-08","timestamp":1720036427000,"abstract":"  Traditional reference-based metrics, such as BLEU and ROUGE, are less\neffective for assessing outputs from Large Language Models (LLMs) that produce\nhighly creative or superior-quality text, or in situations where reference\noutputs are unavailable. While human evaluation remains an option, it is costly\nand difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge)\nis promising, but trust and reliability remain a significant concern.\nIntegrating human input is crucial to ensure criteria used to evaluate are\naligned with the human's intent, and evaluations are robust and consistent.\nThis paper presents a user study of a design exploration called EvaluLLM, that\nenables users to leverage LLMs as customizable judges, promoting human\ninvolvement to balance trust and cost-saving potential with caution. Through\ninterviews with eight domain experts, we identified the need for assistance in\ndeveloping effective evaluation criteria aligning the LLM-as-a-judge with\npractitioners' preferences and expectations. We offer findings and design\nrecommendations to optimize human-assisted LLM-as-judge systems.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}