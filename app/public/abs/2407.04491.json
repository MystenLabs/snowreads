{"id":"2407.04491","title":"Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular\n  Data","authors":"David Holzm\\\"uller, L\\'eo Grinsztajn, Ingo Steinwart","authorsParsed":[["Holzmüller","David",""],["Grinsztajn","Léo",""],["Steinwart","Ingo",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 13:29:30 GMT"}],"updateDate":"2024-07-08","timestamp":1720186170000,"abstract":"  For classification and regression on tabular data, the dominance of\ngradient-boosted decision trees (GBDTs) has recently been challenged by often\nmuch slower deep learning methods with extensive hyperparameter tuning. We\naddress this discrepancy by introducing (a) RealMLP, an improved multilayer\nperceptron (MLP), and (b) improved default parameters for GBDTs and RealMLP. We\ntune RealMLP and the default parameters on a meta-train benchmark with 71\nclassification and 47 regression datasets and compare them to\nhyperparameter-optimized versions on a disjoint meta-test benchmark with 48\nclassification and 42 regression datasets, as well as the GBDT-friendly\nbenchmark by Grinsztajn et al. (2022). Our benchmark results show that RealMLP\noffers a better time-accuracy tradeoff than other neural nets and is\ncompetitive with GBDTs. Moreover, a combination of RealMLP and GBDTs with\nimproved default parameters can achieve excellent results on medium-sized\ntabular datasets (1K--500K samples) without hyperparameter tuning.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}