{"id":"2408.00203","title":"OmniParser for Pure Vision Based GUI Agent","authors":"Yadong Lu, Jianwei Yang, Yelong Shen, Ahmed Awadallah","authorsParsed":[["Lu","Yadong",""],["Yang","Jianwei",""],["Shen","Yelong",""],["Awadallah","Ahmed",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 00:00:43 GMT"}],"updateDate":"2024-08-02","timestamp":1722470443000,"abstract":"  The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"v_VECG1aQl7QY1W6DpgD2woNLT4MdS3f5N-oFssPI70","pdfSize":"6010380","txDigest":"GQU3BCq9QezuFRTNu5ysTnQkbj5JmhKHsUPTpncNcaXL","endEpoch":"1","status":"CERTIFIED"}
