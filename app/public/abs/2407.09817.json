{"id":"2407.09817","title":"Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech\n  Recognition System","authors":"Lingwei Meng, Jiawen Kang, Yuejiao Wang, Zengrui Jin, Xixin Wu,\n  Xunying Liu, Helen Meng","authorsParsed":[["Meng","Lingwei",""],["Kang","Jiawen",""],["Wang","Yuejiao",""],["Jin","Zengrui",""],["Wu","Xixin",""],["Liu","Xunying",""],["Meng","Helen",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 09:28:24 GMT"},{"version":"v2","created":"Sat, 24 Aug 2024 17:01:19 GMT"}],"updateDate":"2024-08-27","timestamp":1720862904000,"abstract":"  Multi-talker speech recognition and target-talker speech recognition, both\ninvolve transcription in multi-talker contexts, remain significant challenges.\nHowever, existing methods rarely attempt to simultaneously address both tasks.\nIn this study, we propose a pioneering approach to empower Whisper, which is a\nspeech foundation model, to tackle joint multi-talker and target-talker speech\nrecognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar\nseparator into its encoder to separate mixed embedding for multiple talkers;\n(ii) a Target Talker Identifier is introduced to identify the embedding flow of\nthe target talker on the fly, requiring only three-second enrollment speech as\na cue; (iii) soft prompt tuning for decoder is explored for better task\nadaptation. Our method outperforms previous methods on two- and three-talker\nLibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable\nzero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}