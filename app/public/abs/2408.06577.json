{"id":"2408.06577","title":"Prompt Tuning as User Inherent Profile Inference Machine","authors":"Yusheng Lu, Zhaocheng Du, Xiangyang Li, Xiangyu Zhao, Weiwen Liu,\n  Yichao Wang, Huifeng Guo, Ruiming Tang, Zhenhua Dong, Yongrui Duan","authorsParsed":[["Lu","Yusheng",""],["Du","Zhaocheng",""],["Li","Xiangyang",""],["Zhao","Xiangyu",""],["Liu","Weiwen",""],["Wang","Yichao",""],["Guo","Huifeng",""],["Tang","Ruiming",""],["Dong","Zhenhua",""],["Duan","Yongrui",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 02:25:46 GMT"}],"updateDate":"2024-08-14","timestamp":1723515946000,"abstract":"  Large Language Models (LLMs) have exhibited significant promise in\nrecommender systems by empowering user profiles with their extensive world\nknowledge and superior reasoning capabilities. However, LLMs face challenges\nlike unstable instruction compliance, modality gaps, and high inference\nlatency, leading to textual noise and limiting their effectiveness in\nrecommender systems. To address these challenges, we propose UserIP-Tuning,\nwhich uses prompt-tuning to infer user profiles. It integrates the causal\nrelationship between user profiles and behavior sequences into LLMs' prompts.\nAnd employs expectation maximization to infer the embedded latent profile,\nminimizing textual noise by fixing the prompt template. Furthermore, A profile\nquantization codebook bridges the modality gap by categorizing profile\nembeddings into collaborative IDs, which are pre-stored for online deployment.\nThis improves time efficiency and reduces memory usage. Experiments on four\npublic datasets show that UserIP-Tuning outperforms state-of-the-art\nrecommendation algorithms. Additional tests and case studies confirm its\neffectiveness, robustness, and transferability.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}