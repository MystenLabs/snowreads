{"id":"2408.11053","title":"Revisiting VerilogEval: Newer LLMs, In-Context Learning, and\n  Specification-to-RTL Tasks","authors":"Nathaniel Pinckney, Christopher Batten, Mingjie Liu, Haoxing Ren, and\n  Brucek Khailany","authorsParsed":[["Pinckney","Nathaniel",""],["Batten","Christopher",""],["Liu","Mingjie",""],["Ren","Haoxing",""],["Khailany","Brucek",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 17:58:56 GMT"}],"updateDate":"2024-09-04","timestamp":1724176736000,"abstract":"  The application of large-language models (LLMs) to digital hardware code\ngeneration is an emerging field. Most LLMs are primarily trained on natural\nlanguage and software code. Hardware code, such as Verilog, represents only a\nsmall portion of the training data and few hardware benchmarks exist. To\naddress this gap, the open-source VerilogEval benchmark was released in 2023,\nproviding a consistent evaluation framework for LLMs on code completion tasks.\nIt was tested on state-of-the-art models at the time including GPT-4. However,\nVerilogEval and other Verilog generation benchmarks lack failure analysis and,\nin present form, are not conducive to exploring prompting techniques. Also,\nsince VerilogEval's release, both commercial and open-source models have seen\ncontinued development.\n  In this work, we evaluate new commercial and open-source models of varying\nsizes against an improved VerilogEval benchmark suite. We enhance VerilogEval's\ninfrastructure and dataset by automatically classifying failures, introduce new\nprompts for supporting in-context learning (ICL) examples, and extend the\nsupported tasks to specification-to-RTL translation. We find a measurable\nimprovement in commercial state-of-the-art models, with GPT-4 Turbo achieving a\n59% pass rate on spec-to-RTL tasks. We also study the performance of\nopen-source and domain-specific models that have emerged, and demonstrate that\nmodels can benefit substantially from ICL. We find that recently-released Llama\n3.1 405B achieves a pass rate of 58%, effectively matching that of GPT-4 Turbo,\nand that the much smaller domain-specific RTL-Coder 6.7B models achieve an\nimpressive 37% pass rate. However, prompt engineering is key to achieving good\npass rates, and varies widely with model and task. A benchmark infrastructure\nthat allows for prompt engineering and failure analysis is key to continued\nmodel development and deployment.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nyoENhwrUxwqNqEZ_dOdMEhKnx_lwLFhI5OjM_l1GaY","pdfSize":"422451"}
