{"id":"2407.13436","title":"An Algorithm for Computing the Capacity of Symmetrized KL Information\n  for Discrete Channels","authors":"Haobo Chen, Gholamali Aminian, Yuheng Bu","authorsParsed":[["Chen","Haobo",""],["Aminian","Gholamali",""],["Bu","Yuheng",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 12:03:19 GMT"}],"updateDate":"2024-07-19","timestamp":1721304199000,"abstract":"  Symmetrized Kullback-Leibler (KL) information (\\(I_{\\mathrm{SKL}}\\)), which\nsymmetrizes the traditional mutual information by integrating Lautum\ninformation, has been shown as a critical quantity in\ncommunication~\\cite{aminian2015capacity} and learning\ntheory~\\cite{aminian2023information}. This paper considers the problem of\ncomputing the capacity in terms of \\(I_{\\mathrm{SKL}}\\) for a fixed discrete\nchannel. Such a maximization problem is reformulated into a discrete quadratic\noptimization with a simplex constraint. One major challenge here is the\nnon-concavity of Lautum information, which complicates the optimization\nproblem. Our method involves symmetrizing the KL divergence matrix and applying\niterative updates to ensure a non-decreasing update while maintaining a valid\nprobability distribution. We validate our algorithm on Binary symmetric\nChannels and Binomial Channels, demonstrating its consistency with theoretical\nvalues. Additionally, we explore its application in machine learning through\nthe Gibbs channel, showcasing the effectiveness of our algorithm in finding the\nworst-case data distributions.\n","subjects":["Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}