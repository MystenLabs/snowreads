{"id":"2407.05750","title":"Large Language Models Understand Layout","authors":"Weiming Li, Manni Duan, Dong An, Yan Shao","authorsParsed":[["Li","Weiming",""],["Duan","Manni",""],["An","Dong",""],["Shao","Yan",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 09:03:12 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 09:17:21 GMT"},{"version":"v3","created":"Wed, 28 Aug 2024 02:04:24 GMT"}],"updateDate":"2024-08-29","timestamp":1720429392000,"abstract":"  Large language models (LLMs) demonstrate extraordinary abilities in a wide\nrange of natural language processing (NLP) tasks. In this paper, we show that,\nbeyond text understanding capability, LLMs are capable of processing text\nlayouts that are denoted by spatial markers. They are able to answer questions\nthat require explicit spatial perceiving and reasoning, while a drastic\nperformance drop is observed when the spatial markers from the original data\nare excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,\nLlama2 and ChatGLM3 models on various types of layout-sensitive datasets for\nfurther analysis. The experimental results reveal that the layout understanding\nability of LLMs is mainly introduced by the coding data for pretraining, which\nis further enhanced at the instruction-tuning stage. In addition, layout\nunderstanding can be enhanced by integrating low-cost, auto-generated data\napproached by a novel text game. Finally, we show that layout understanding\nability is beneficial for building efficient visual question-answering (VQA)\nsystems.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}