{"id":"2407.06794","title":"ERQ: Error Reduction for Post-Training Quantization of Vision\n  Transformers","authors":"Yunshan Zhong, Jiawei Hu, You Huang, Yuxin Zhang, Rongrong Ji","authorsParsed":[["Zhong","Yunshan",""],["Hu","Jiawei",""],["Huang","You",""],["Zhang","Yuxin",""],["Ji","Rongrong",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 12:06:03 GMT"}],"updateDate":"2024-07-10","timestamp":1720526763000,"abstract":"  Post-training quantization (PTQ) for vision transformers (ViTs) has garnered\nsignificant attention due to its efficiency in compressing models. However,\nexisting methods typically overlook the intricate interdependence between\nquantized weight and activation, leading to considerable quantization error. In\nthis paper, we propose ERQ, a two-step PTQ approach meticulously crafted to\nsequentially reduce the quantization error arising from activation and weight\nquantization. ERQ first introduces Activation quantization error reduction\n(Aqer) that strategically formulates the minimization of activation\nquantization error as a Ridge Regression problem, tackling it by updating\nweights with full-precision. Subsequently, ERQ introduces Weight quantization\nerror reduction (Wqer) that adopts an iterative approach to mitigate the\nquantization error induced by weight quantization. In each iteration, an\nempirically derived, efficient proxy is employed to refine the rounding\ndirections of quantized weights, coupled with a Ridge Regression solver to\ncurtail weight quantization error. Experimental results attest to the\neffectiveness of our approach. Notably, ERQ surpasses the state-of-the-art GPTQ\nby 22.36% in accuracy for W3A4 ViT-S.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}