{"id":"2407.08152","title":"Privacy-Preserving Data Deduplication for Enhancing Federated Learning\n  of Language Models","authors":"Aydin Abadi, Vishnu Asutosh Dasu, Sumanta Sarkar","authorsParsed":[["Abadi","Aydin",""],["Dasu","Vishnu Asutosh",""],["Sarkar","Sumanta",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 03:10:27 GMT"}],"updateDate":"2024-07-12","timestamp":1720667427000,"abstract":"  Deduplication is a vital preprocessing step that enhances machine learning\nmodel performance and saves training time and energy. However, enhancing\nfederated learning through deduplication poses challenges, especially regarding\nscalability and potential privacy violations if deduplication involves sharing\nall clients' data. In this paper, we address the problem of deduplication in a\nfederated setup by introducing a pioneering protocol, Efficient\nPrivacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes\nduplicates from multiple clients' datasets without compromising data privacy.\nEP-MPD is constructed in a modular fashion, utilizing two novel variants of the\nPrivate Set Intersection protocol. Our extensive experiments demonstrate the\nsignificant benefits of deduplication in federated learning of large language\nmodels. For instance, we observe up to 19.61% improvement in perplexity and up\nto 27.95% reduction in running time. EP-MPD effectively balances privacy and\nperformance in federated learning, making it a valuable solution for\nlarge-scale applications.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}