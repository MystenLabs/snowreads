{"id":"2408.11918","title":"Neural Symbolic Logical Rule Learner for Interpretable Learning","authors":"Bowen Wei and Ziwei Zhu","authorsParsed":[["Wei","Bowen",""],["Zhu","Ziwei",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 18:09:12 GMT"}],"updateDate":"2024-08-23","timestamp":1724263752000,"abstract":"  Rule-based neural networks stand out for enabling interpretable\nclassification by learning logical rules for both prediction and\ninterpretation. However, existing models often lack flexibility due to the\nfixed model structure. Addressing this, we introduce the Normal Form Rule\nLearner (NFRL) algorithm, leveraging a selective discrete neural network, that\ntreat weight parameters as hard selectors, to learn rules in both Conjunctive\nNormal Form (CNF) and Disjunctive Normal Form (DNF) for enhanced accuracy and\ninterpretability. Instead of adopting a deep, complex structure, the NFRL\nincorporates two specialized Normal Form Layers (NFLs) with adaptable AND/OR\nneurons, a Negation Layer for input negations, and a Normal Form Constraint\n(NFC) to streamline neuron connections. We also show the novel network\narchitecture can be optimized using adaptive gradient update together with\nStraight-Through Estimator to overcome the gradient vanishing challenge.\nThrough extensive experiments on 11 datasets, NFRL demonstrates superior\nclassification performance, quality of learned rules, efficiency and\ninterpretability compared to 12 state-of-the-art alternatives. Code and data\nare available at \\url{https://anonymous.4open.science/r/NFRL-27B4/}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}