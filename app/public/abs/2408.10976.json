{"id":"2408.10976","title":"Kernel-Based Differentiable Learning of Non-Parametric Directed Acyclic\n  Graphical Models","authors":"Yurou Liang, Oleksandr Zadorozhnyi and Mathias Drton","authorsParsed":[["Liang","Yurou",""],["Zadorozhnyi","Oleksandr",""],["Drton","Mathias",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 16:09:40 GMT"}],"updateDate":"2024-08-21","timestamp":1724170180000,"abstract":"  Causal discovery amounts to learning a directed acyclic graph (DAG) that\nencodes a causal model. This model selection problem can be challenging due to\nits large combinatorial search space, particularly when dealing with\nnon-parametric causal models. Recent research has sought to bypass the\ncombinatorial search by reformulating causal discovery as a continuous\noptimization problem, employing constraints that ensure the acyclicity of the\ngraph. In non-parametric settings, existing approaches typically rely on\nfinite-dimensional approximations of the relationships between nodes, resulting\nin a score-based continuous optimization problem with a smooth acyclicity\nconstraint. In this work, we develop an alternative approximation method by\nutilizing reproducing kernel Hilbert spaces (RKHS) and applying general\nsparsity-inducing regularization terms based on partial derivatives. Within\nthis framework, we introduce an extended RKHS representer theorem. To enforce\nacyclicity, we advocate the log-determinant formulation of the acyclicity\nconstraint and show its stability. Finally, we assess the performance of our\nproposed RKHS-DAGMA procedure through simulations and illustrative data\nanalyses.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}