{"id":"2408.10284","title":"AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference","authors":"Shuzhang Zhong, Ling Liang, Yuan Wang, Runsheng Wang, Ru Huang, Meng\n  Li","authorsParsed":[["Zhong","Shuzhang",""],["Liang","Ling",""],["Wang","Yuan",""],["Wang","Runsheng",""],["Huang","Ru",""],["Li","Meng",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 03:27:15 GMT"}],"updateDate":"2024-08-21","timestamp":1724038035000,"abstract":"  Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}