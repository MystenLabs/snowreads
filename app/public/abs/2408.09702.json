{"id":"2408.09702","title":"Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering","authors":"Ruofan Liang, Zan Gojcic, Merlin Nimier-David, David Acuna, Nandita\n  Vijaykumar, Sanja Fidler, Zian Wang","authorsParsed":[["Liang","Ruofan",""],["Gojcic","Zan",""],["Nimier-David","Merlin",""],["Acuna","David",""],["Vijaykumar","Nandita",""],["Fidler","Sanja",""],["Wang","Zian",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 05:15:45 GMT"}],"updateDate":"2024-08-20","timestamp":1724044545000,"abstract":"  The correct insertion of virtual objects in images of real-world scenes\nrequires a deep understanding of the scene's lighting, geometry and materials,\nas well as the image formation process. While recent large-scale diffusion\nmodels have shown strong generative and inpainting capabilities, we find that\ncurrent models do not sufficiently \"understand\" the scene shown in a single\npicture to generate consistent lighting effects (shadows, bright reflections,\netc.) while preserving the identity and details of the composited object. We\npropose using a personalized large diffusion model as guidance to a physically\nbased inverse rendering process. Our method recovers scene lighting and\ntone-mapping parameters, allowing the photorealistic composition of arbitrary\nvirtual objects in single frames or videos of indoor or outdoor scenes. Our\nphysically based pipeline further enables automatic materials and tone-mapping\nrefinement.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Graphics"],"license":"http://creativecommons.org/licenses/by/4.0/"}