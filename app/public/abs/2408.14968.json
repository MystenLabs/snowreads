{"id":"2408.14968","title":"MRSE: An Efficient Multi-modality Retrieval System for Large Scale\n  E-commerce","authors":"Hao Jiang, Haoxiang Zhang, Qingshan Hou, Chaofeng Chen, Weisi Lin,\n  Jingchang Zhang, Annan Wang","authorsParsed":[["Jiang","Hao",""],["Zhang","Haoxiang",""],["Hou","Qingshan",""],["Chen","Chaofeng",""],["Lin","Weisi",""],["Zhang","Jingchang",""],["Wang","Annan",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 11:21:19 GMT"}],"updateDate":"2024-08-28","timestamp":1724757679000,"abstract":"  Providing high-quality item recall for text queries is crucial in large-scale\ne-commerce search systems. Current Embedding-based Retrieval Systems (ERS)\nembed queries and items into a shared low-dimensional space, but uni-modality\nERS rely too heavily on textual features, making them unreliable in complex\ncontexts. While multi-modality ERS incorporate various data sources, they often\noverlook individual preferences for different modalities, leading to suboptimal\nresults. To address these issues, we propose MRSE, a Multi-modality Retrieval\nSystem that integrates text, item images, and user preferences through\nlightweight mixture-of-expert (LMoE) modules to better align features across\nand within modalities. MRSE also builds user profiles at a multi-modality level\nand introduces a novel hybrid loss function that enhances consistency and\nrobustness using hard negative sampling. Experiments on a large-scale dataset\nfrom Shopee and online A/B testing show that MRSE achieves an 18.9% improvement\nin offline relevance and a 3.7% gain in online core metrics compared to\nShopee's state-of-the-art uni-modality system.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}