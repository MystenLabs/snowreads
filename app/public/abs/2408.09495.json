{"id":"2408.09495","title":"Directed Exploration in Reinforcement Learning from Linear Temporal\n  Logic","authors":"Marco Bagatella, Andreas Krause, Georg Martius","authorsParsed":[["Bagatella","Marco",""],["Krause","Andreas",""],["Martius","Georg",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 14:25:44 GMT"}],"updateDate":"2024-08-20","timestamp":1723991144000,"abstract":"  Linear temporal logic (LTL) is a powerful language for task specification in\nreinforcement learning, as it allows describing objectives beyond the\nexpressivity of conventional discounted return formulations. Nonetheless,\nrecent works have shown that LTL formulas can be translated into a variable\nrewarding and discounting scheme, whose optimization produces a policy\nmaximizing a lower bound on the probability of formula satisfaction. However,\nthe synthesized reward signal remains fundamentally sparse, making exploration\nchallenging. We aim to overcome this limitation, which can prevent current\nalgorithms from scaling beyond low-dimensional, short-horizon problems. We show\nhow better exploration can be achieved by further leveraging the LTL\nspecification and casting its corresponding Limit Deterministic B\\\"uchi\nAutomaton (LDBA) as a Markov reward process, thus enabling a form of high-level\nvalue estimation. By taking a Bayesian perspective over LDBA dynamics and\nproposing a suitable prior distribution, we show that the values estimated\nthrough this procedure can be treated as a shaping potential and mapped to\ninformative intrinsic rewards. Empirically, we demonstrate applications of our\nmethod from tabular settings to high-dimensional continuous systems, which have\nso far represented a significant challenge for LTL-based reinforcement learning\nalgorithms.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}