{"id":"2408.11194","title":"Compress Guidance in Conditional Diffusion Sampling","authors":"Anh-Dung Dinh, Daochang Liu and Chang Xu","authorsParsed":[["Dinh","Anh-Dung",""],["Liu","Daochang",""],["Xu","Chang",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 21:02:54 GMT"}],"updateDate":"2024-08-22","timestamp":1724187774000,"abstract":"  Enforcing guidance throughout the entire sampling process often proves\ncounterproductive due to the model-fitting issue., where samples are generated\nto match the classifier's parameters rather than generalizing the expected\ncondition. This work identifies and quantifies the problem, demonstrating that\nreducing or excluding guidance at numerous timesteps can mitigate this issue.\nBy distributing the guidance densely in the early stages of the process, we\nobserve a significant improvement in image quality and diversity while also\nreducing the required guidance timesteps by nearly 40%. This approach addresses\na major challenge in applying guidance effectively to generative tasks.\nConsequently, our proposed method, termed Compress Guidance, allows for the\nexclusion of a substantial number of guidance timesteps while still surpassing\nbaseline models in image quality. We validate our approach through benchmarks\non label conditional and text-to-image generative tasks across various datasets\nand models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_iDNYpv7VV5HaF2Yv5tGKuJ3AvFyNvbARWA8CBO8284","pdfSize":"11247968"}
