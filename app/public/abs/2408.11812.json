{"id":"2408.11812","title":"Scaling Cross-Embodied Learning: One Policy for Manipulation,\n  Navigation, Locomotion and Aviation","authors":"Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, Sergey Levine","authorsParsed":[["Doshi","Ria",""],["Walke","Homer",""],["Mees","Oier",""],["Dasari","Sudeep",""],["Levine","Sergey",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:57:51 GMT"}],"updateDate":"2024-08-22","timestamp":1724263071000,"abstract":"  Modern machine learning systems rely on large datasets to attain broad\ngeneralization, and this often poses a challenge in robot learning, where each\nrobotic platform and task might have only a small dataset. By training a single\npolicy across many different kinds of robots, a robot learning method can\nleverage much broader and more diverse datasets, which in turn can lead to\nbetter generalization and robustness. However, training a single policy on\nmulti-robot data is challenging because robots can have widely varying sensors,\nactuators, and control frequencies. We propose CrossFormer, a scalable and\nflexible transformer-based policy that can consume data from any embodiment. We\ntrain CrossFormer on the largest and most diverse dataset to date, 900K\ntrajectories across 20 different robot embodiments. We demonstrate that the\nsame network weights can control vastly different robots, including single and\ndual arm manipulation systems, wheeled robots, quadcopters, and quadrupeds.\nUnlike prior work, our model does not require manual alignment of the\nobservation or action spaces. Extensive experiments in the real world show that\nour method matches the performance of specialist policies tailored for each\nembodiment, while also significantly outperforming the prior state of the art\nin cross-embodiment learning.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}