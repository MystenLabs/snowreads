{"id":"2408.03119","title":"Evaluating the Translation Performance of Large Language Models Based on\n  Euas-20","authors":"Yan Huang, Wei Liu","authorsParsed":[["Huang","Yan",""],["Liu","Wei",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 11:49:11 GMT"}],"updateDate":"2024-08-07","timestamp":1722944951000,"abstract":"  In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}