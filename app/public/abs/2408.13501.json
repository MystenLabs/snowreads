{"id":"2408.13501","title":"Utilizing Large Language Models for Named Entity Recognition in\n  Traditional Chinese Medicine against COVID-19 Literature: Comparative Study","authors":"Xu Tong, Nina Smirnova, Sharmila Upadhyaya, Ran Yu, Jack H. Culbert,\n  Chao Sun, Wolfgang Otto, Philipp Mayr","authorsParsed":[["Tong","Xu",""],["Smirnova","Nina",""],["Upadhyaya","Sharmila",""],["Yu","Ran",""],["Culbert","Jack H.",""],["Sun","Chao",""],["Otto","Wolfgang",""],["Mayr","Philipp",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 06:59:55 GMT"}],"updateDate":"2024-08-27","timestamp":1724482795000,"abstract":"  Objective: To explore and compare the performance of ChatGPT and other\nstate-of-the-art LLMs on domain-specific NER tasks covering different entity\ntypes and domains in TCM against COVID-19 literature. Methods: We established a\ndataset of 389 articles on TCM against COVID-19, and manually annotated 48 of\nthem with 6 types of entities belonging to 3 domains as the ground truth,\nagainst which the NER performance of LLMs can be assessed. We then performed\nNER tasks for the 6 entity types using ChatGPT (GPT-3.5 and GPT-4) and 4\nstate-of-the-art BERT-based question-answering (QA) models (RoBERTa, MiniLM,\nPubMedBERT and SciBERT) without prior training on the specific task. A domain\nfine-tuned model (GSAP-NER) was also applied for a comprehensive comparison.\nResults: The overall performance of LLMs varied significantly in exact match\nand fuzzy match. In the fuzzy match, ChatGPT surpassed BERT-based QA models in\n5 out of 6 tasks, while in exact match, BERT-based QA models outperformed\nChatGPT in 5 out of 6 tasks but with a smaller F-1 difference. GPT-4 showed a\nsignificant advantage over other models in fuzzy match, especially on the\nentity type of TCM formula and the Chinese patent drug (TFD) and ingredient\n(IG). Although GPT-4 outperformed BERT-based models on entity type of herb,\ntarget, and research method, none of the F-1 scores exceeded 0.5. GSAP-NER,\noutperformed GPT-4 in terms of F-1 by a slight margin on RM. ChatGPT achieved\nconsiderably higher recalls than precisions, particularly in the fuzzy match.\nConclusions: The NER performance of LLMs is highly dependent on the entity\ntype, and their performance varies across application scenarios. ChatGPT could\nbe a good choice for scenarios where high recall is favored. However, for\nknowledge acquisition in rigorous scenarios, neither ChatGPT nor BERT-based QA\nmodels are off-the-shelf tools for professional practitioners.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}