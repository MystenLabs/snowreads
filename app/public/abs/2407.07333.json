{"id":"2407.07333","title":"Mitigating Partial Observability in Sequential Decision Processes via\n  the Lambda Discrepancy","authors":"Cameron Allen, Aaron Kirtland, Ruo Yu Tao, Sam Lobel, Daniel Scott,\n  Nicholas Petrocelli, Omer Gottesman, Ronald Parr, Michael L. Littman, George\n  Konidaris","authorsParsed":[["Allen","Cameron",""],["Kirtland","Aaron",""],["Tao","Ruo Yu",""],["Lobel","Sam",""],["Scott","Daniel",""],["Petrocelli","Nicholas",""],["Gottesman","Omer",""],["Parr","Ronald",""],["Littman","Michael L.",""],["Konidaris","George",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 03:04:20 GMT"},{"version":"v2","created":"Sun, 21 Jul 2024 06:43:18 GMT"}],"updateDate":"2024-07-23","timestamp":1720580660000,"abstract":"  Reinforcement learning algorithms typically rely on the assumption that the\nenvironment dynamics and value function can be expressed in terms of a\nMarkovian state representation. However, when state information is only\npartially observable, how can an agent learn such a state representation, and\nhow can it detect when it has found one? We introduce a metric that can\naccomplish both objectives, without requiring access to--or knowledge of--an\nunderlying, unobservable state space. Our metric, the $\\lambda$-discrepancy, is\nthe difference between two distinct temporal difference (TD) value estimates,\neach computed using TD($\\lambda$) with a different value of $\\lambda$. Since\nTD($\\lambda$=0) makes an implicit Markov assumption and TD($\\lambda$=1) does\nnot, a discrepancy between these estimates is a potential indicator of a\nnon-Markovian state representation. Indeed, we prove that the\n$\\lambda$-discrepancy is exactly zero for all Markov decision processes and\nalmost always non-zero for a broad class of partially observable environments.\nWe also demonstrate empirically that, once detected, minimizing the\n$\\lambda$-discrepancy can help with learning a memory function to mitigate the\ncorresponding partial observability. We then train a reinforcement learning\nagent that simultaneously constructs two recurrent value networks with\ndifferent $\\lambda$ parameters and minimizes the difference between them as an\nauxiliary loss. The approach scales to challenging partially observable\ndomains, where the resulting agent frequently performs significantly better\n(and never performs worse) than a baseline recurrent agent with only a single\nvalue network.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}