{"id":"2407.01656","title":"Statistical signatures of abstraction in deep neural networks","authors":"Carlo Orientale Caputo and Matteo Marsili","authorsParsed":[["Caputo","Carlo Orientale",""],["Marsili","Matteo",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 14:13:11 GMT"}],"updateDate":"2024-07-03","timestamp":1719843191000,"abstract":"  We study how abstract representations emerge in a Deep Belief Network (DBN)\ntrained on benchmark datasets. Our analysis targets the principles of learning\nin the early stages of information processing, starting from the \"primordial\nsoup\" of the under-sampling regime. As the data is processed by deeper and\ndeeper layers, features are detected and removed, transferring more and more\n\"context-invariant\" information to deeper layers. We show that the\nrepresentation approaches an universal model -- the Hierarchical Feature Model\n(HFM) -- determined by the principle of maximal relevance. Relevance quantifies\nthe uncertainty on the model of the data, thus suggesting that \"meaning\" --\ni.e. syntactic information -- is that part of the data which is not yet\ncaptured by a model. Our analysis shows that shallow layers are well described\nby pairwise Ising models, which provide a representation of the data in terms\nof generic, low order features. We also show that plasticity increases with\ndepth, in a similar way as it does in the brain. These findings suggest that\nDBNs are capable of extracting a hierarchy of features from the data which is\nconsistent with the principle of maximal relevance.\n","subjects":["Computing Research Repository/Machine Learning","Condensed Matter/Disordered Systems and Neural Networks","Physics/Data Analysis, Statistics and Probability","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}