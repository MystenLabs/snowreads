{"id":"2407.20650","title":"No learning rates needed: Introducing SALSA -- Stable Armijo Line Search\n  Adaptation","authors":"Philip Kenneweg, Tristan Kenneweg, Fabian Fumagalli, Barbara Hammer","authorsParsed":[["Kenneweg","Philip",""],["Kenneweg","Tristan",""],["Fumagalli","Fabian",""],["Hammer","Barbara",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 08:47:02 GMT"}],"updateDate":"2024-07-31","timestamp":1722329222000,"abstract":"  In recent studies, line search methods have been demonstrated to\nsignificantly enhance the performance of conventional stochastic gradient\ndescent techniques across various datasets and architectures, while making an\notherwise critical choice of learning rate schedule superfluous. In this paper,\nwe identify problems of current state-of-the-art of line search methods,\npropose enhancements, and rigorously assess their effectiveness. Furthermore,\nwe evaluate these methods on orders of magnitude larger datasets and more\ncomplex data domains than previously done. More specifically, we enhance the\nArmijo line search method by speeding up its computation and incorporating a\nmomentum term into the Armijo criterion, making it better suited for stochastic\nmini-batching. Our optimization approach outperforms both the previous Armijo\nimplementation and a tuned learning rate schedule for the Adam and SGD\noptimizers. Our evaluation covers a diverse range of architectures, such as\nTransformers, CNNs, and MLPs, as well as data domains, including NLP and image\ndata.\n  Our work is publicly available as a Python package, which provides a simple\nPytorch optimizer.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ITqeKPdw2g08B0SJAa_gxf9iE6S1VwgG5awSmE6xL_4","pdfSize":"3929305"}
