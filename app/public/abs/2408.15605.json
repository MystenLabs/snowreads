{"id":"2408.15605","title":"ES-PTAM: Event-based Stereo Parallel Tracking and Mapping","authors":"Suman Ghosh, Valentina Cavinato, Guillermo Gallego","authorsParsed":[["Ghosh","Suman",""],["Cavinato","Valentina",""],["Gallego","Guillermo",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 07:56:28 GMT"}],"updateDate":"2024-08-29","timestamp":1724831788000,"abstract":"  Visual Odometry (VO) and SLAM are fundamental components for spatial\nperception in mobile robots. Despite enormous progress in the field, current\nVO/SLAM systems are limited by their sensors' capability. Event cameras are\nnovel visual sensors that offer advantages to overcome the limitations of\nstandard cameras, enabling robots to expand their operating range to\nchallenging scenarios, such as high-speed motion and high dynamic range\nillumination. We propose a novel event-based stereo VO system by combining two\nideas: a correspondence-free mapping module that estimates depth by maximizing\nray density fusion and a tracking module that estimates camera poses by\nmaximizing edge-map alignment. We evaluate the system comprehensively on five\nreal-world datasets, spanning a variety of camera types (manufacturers and\nspatial resolutions) and scenarios (driving, flying drone, hand-held,\negocentric, etc). The quantitative and qualitative results demonstrate that our\nmethod outperforms the state of the art in majority of the test sequences by a\nmargin, e.g., trajectory error reduction of 45% on RPG dataset, 61% on DSEC\ndataset, and 21% on TUM-VIE dataset. To benefit the community and foster\nresearch on event-based perception systems, we release the source code and\nresults: https://github.com/tub-rip/ES-PTAM\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}