{"id":"2408.05840","title":"Iterative Improvement of an Additively Regularized Topic Model","authors":"Alex Gorbulev, Vasiliy Alekseev, Konstantin Vorontsov","authorsParsed":[["Gorbulev","Alex",""],["Alekseev","Vasiliy",""],["Vorontsov","Konstantin",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 18:22:12 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 11:07:17 GMT"}],"updateDate":"2024-08-15","timestamp":1723400532000,"abstract":"  Topic modelling is fundamentally a soft clustering problem (of known objects\n-- documents, over unknown clusters -- topics). That is, the task is\nincorrectly posed. In particular, the topic models are unstable and incomplete.\nAll this leads to the fact that the process of finding a good topic model\n(repeated hyperparameter selection, model training, and topic quality\nassessment) can be particularly long and labor-intensive. We aim to simplify\nthe process, to make it more deterministic and provable. To this end, we\npresent a method for iterative training of a topic model. The essence of the\nmethod is that a series of related topic models are trained so that each\nsubsequent model is at least as good as the previous one, i.e., that it retains\nall the good topics found earlier. The connection between the models is\nachieved by additive regularization. The result of this iterative training is\nthe last topic model in the series, which we call the iteratively updated\nadditively regularized topic model (ITAR). Experiments conducted on several\ncollections of natural language texts show that the proposed ITAR model\nperforms better than other popular topic models (LDA, ARTM, BERTopic), its\ntopics are diverse, and its perplexity (ability to \"explain\" the underlying\ndata) is moderate.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval","Mathematics/Probability"],"license":"http://creativecommons.org/licenses/by/4.0/"}