{"id":"2407.13432","title":"The Art of Imitation: Learning Long-Horizon Manipulation Tasks from Few\n  Demonstrations","authors":"Jan Ole von Hartz, Tim Welschehold, Abhinav Valada, Joschka Boedecker","authorsParsed":[["von Hartz","Jan Ole",""],["Welschehold","Tim",""],["Valada","Abhinav",""],["Boedecker","Joschka",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 12:01:09 GMT"}],"updateDate":"2024-07-19","timestamp":1721304069000,"abstract":"  Task Parametrized Gaussian Mixture Models (TP-GMM) are a sample-efficient\nmethod for learning object-centric robot manipulation tasks. However, there are\nseveral open challenges to applying TP-GMMs in the wild. In this work, we\ntackle three crucial challenges synergistically. First, end-effector velocities\nare non-Euclidean and thus hard to model using standard GMMs. We thus propose\nto factorize the robot's end-effector velocity into its direction and\nmagnitude, and model them using Riemannian GMMs. Second, we leverage the\nfactorized velocities to segment and sequence skills from complex demonstration\ntrajectories. Through the segmentation, we further align skill trajectories and\nhence leverage time as a powerful inductive bias. Third, we present a method to\nautomatically detect relevant task parameters per skill from visual\nobservations. Our approach enables learning complex manipulation tasks from\njust five demonstrations while using only RGB-D observations. Extensive\nexperimental evaluations on RLBench demonstrate that our approach achieves\nstate-of-the-art performance with 20-fold improved sample efficiency. Our\npolicies generalize across different environments, object instances, and object\npositions, while the learned skills are reusable.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}