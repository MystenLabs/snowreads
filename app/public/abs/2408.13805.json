{"id":"2408.13805","title":"Prior Learning in Introspective VAEs","authors":"Ioannis Athanasiadis, Shashi Nagarajan, Fredrik Lindsten and Michael\n  Felsberg","authorsParsed":[["Athanasiadis","Ioannis",""],["Nagarajan","Shashi",""],["Lindsten","Fredrik",""],["Felsberg","Michael",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 10:54:25 GMT"}],"updateDate":"2024-08-27","timestamp":1724583265000,"abstract":"  Variational Autoencoders (VAEs) are a popular framework for unsupervised\nlearning and data generation. A plethora of methods have been proposed focusing\non improving VAEs, with the incorporation of adversarial objectives and the\nintegration of prior learning mechanisms being prominent directions. When it\ncomes to the former, an indicative instance is the recently introduced family\nof Introspective VAEs aiming at ensuring that a low likelihood is assigned to\nunrealistic samples. In this study, we focus on the Soft-IntroVAE (S-IntroVAE)\nand investigate the implication of incorporating a multimodal and learnable\nprior into this framework. Namely, we formulate the prior as a third player and\nshow that when trained in cooperation with the decoder constitutes an effective\nway for prior learning, which shares the Nash Equilibrium with the vanilla\nS-IntroVAE. Furthermore, based on a modified formulation of the optimal ELBO in\nS-IntroVAE, we develop theoretically motivated regularizations, that is (i)\nadaptive variance clipping to stabilize training when learning the prior and\n(ii) responsibility regularization to discourage the formation of inactive\nprior mode. Finally, we perform a series of targeted experiments on a 2D\ndensity estimation benchmark and in an image generation setting comprised of\nthe (F)-MNIST and CIFAR-10 datasets demonstrating the benefit of prior learning\nin S-IntroVAE in generation and representation learning.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}