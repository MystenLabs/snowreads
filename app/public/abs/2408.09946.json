{"id":"2408.09946","title":"Microscopic Analysis on LLM players via Social Deduction Game","authors":"Byungjun Kim, Dayeon Seo, and Bugeun Kim","authorsParsed":[["Kim","Byungjun",""],["Seo","Dayeon",""],["Kim","Bugeun",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 12:35:23 GMT"}],"updateDate":"2024-08-20","timestamp":1724070923000,"abstract":"  Recent studies have begun developing autonomous game players for social\ndeduction games using large language models (LLMs). When building LLM players,\nfine-grained evaluations are crucial for addressing weaknesses in game-playing\nabilities. However, existing studies have often overlooked such assessments.\nSpecifically, we point out two issues with the evaluation methods employed.\nFirst, game-playing abilities have typically been assessed through game-level\noutcomes rather than specific event-level skills; Second, error analyses have\nlacked structured methodologies. To address these issues, we propose an\napproach utilizing a variant of the SpyFall game, named SpyGame. We conducted\nan experiment with four LLMs, analyzing their gameplay behavior in SpyGame both\nquantitatively and qualitatively. For the quantitative analysis, we introduced\neight metrics to resolve the first issue, revealing that these metrics are more\neffective than existing ones for evaluating the two critical skills: intent\nidentification and camouflage. In the qualitative analysis, we performed\nthematic analysis to resolve the second issue. This analysis identifies four\nmajor categories that affect gameplay of LLMs. Additionally, we demonstrate how\nthese categories complement and support the findings from the quantitative\nanalysis.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}