{"id":"2407.05591","title":"On the Power of Convolution Augmented Transformer","authors":"Mingchen Li, Xuechen Zhang, Yixiao Huang, Samet Oymak","authorsParsed":[["Li","Mingchen",""],["Zhang","Xuechen",""],["Huang","Yixiao",""],["Oymak","Samet",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 04:08:35 GMT"}],"updateDate":"2024-07-09","timestamp":1720411715000,"abstract":"  The transformer architecture has catalyzed revolutionary advances in language\nmodeling. However, recent architectural recipes, such as state-space models,\nhave bridged the performance gap. Motivated by this, we examine the benefits of\nConvolution-Augmented Transformer (CAT) for recall, copying, and length\ngeneralization tasks. CAT incorporates convolutional filters in the K/Q/V\nembeddings of an attention layer. Through CAT, we show that the locality of the\nconvolution synergizes with the global view of the attention. Unlike comparable\narchitectures, such as Mamba or transformer, CAT can provably solve the\nassociative recall (AR) and copying tasks using a single layer while also\nenjoying guaranteed length generalization. We also establish computational\ntradeoffs between convolution and attention by characterizing how convolution\ncan mitigate the need for full attention by summarizing the context window and\ncreating salient summary tokens to attend. Evaluations on real datasets\ncorroborate our findings and demonstrate that CAT and its variations indeed\nenhance the language modeling performance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}