{"id":"2408.01380","title":"Coalitions of Large Language Models Increase the Robustness of AI Agents","authors":"Prattyush Mangal, Carol Mak, Theo Kanakis, Timothy Donovan, Dave\n  Braines, Edward Pyzer-Knapp","authorsParsed":[["Mangal","Prattyush",""],["Mak","Carol",""],["Kanakis","Theo",""],["Donovan","Timothy",""],["Braines","Dave",""],["Pyzer-Knapp","Edward",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 16:37:44 GMT"}],"updateDate":"2024-08-05","timestamp":1722616664000,"abstract":"  The emergence of Large Language Models (LLMs) have fundamentally altered the\nway we interact with digital systems and have led to the pursuit of LLM powered\nAI agents to assist in daily workflows. LLMs, whilst powerful and capable of\ndemonstrating some emergent properties, are not logical reasoners and often\nstruggle to perform well at all sub-tasks carried out by an AI agent to plan\nand execute a workflow. While existing studies tackle this lack of proficiency\nby generalised pretraining at a huge scale or by specialised fine-tuning for\ntool use, we assess if a system comprising of a coalition of pretrained LLMs,\neach exhibiting specialised performance at individual sub-tasks, can match the\nperformance of single model agents. The coalition of models approach showcases\nits potential for building robustness and reducing the operational costs of\nthese AI agents by leveraging traits exhibited by specific models. Our findings\ndemonstrate that fine-tuning can be mitigated by considering a coalition of\npretrained models and believe that this approach can be applied to other\nnon-agentic systems which utilise LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"8S1pEAjjOyZvxqNGOBIa3g5OEK8stNYuhUZen9tLWZc","pdfSize":"792362"}
