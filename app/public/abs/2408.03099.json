{"id":"2408.03099","title":"Topic Modeling with Fine-tuning LLMs and Bag of Sentences","authors":"Johannes Schneider","authorsParsed":[["Schneider","Johannes",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 11:04:07 GMT"}],"updateDate":"2024-08-07","timestamp":1722942247000,"abstract":"  Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"e3Pj2cF2fSw8e-uq9CrhYM3QLGpop7ZW9GQDGK05UTI","pdfSize":"468969"}
