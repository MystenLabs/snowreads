{"id":"2407.11907","title":"GraphFM: A Scalable Framework for Multi-Graph Pretraining","authors":"Divyansha Lachi, Mehdi Azabou, Vinam Arora, Eva Dyer","authorsParsed":[["Lachi","Divyansha",""],["Azabou","Mehdi",""],["Arora","Vinam",""],["Dyer","Eva",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 16:51:43 GMT"}],"updateDate":"2024-07-17","timestamp":1721148703000,"abstract":"  Graph neural networks are typically trained on individual datasets, often\nrequiring highly specialized models and extensive hyperparameter tuning. This\ndataset-specific approach arises because each graph dataset often has unique\nnode features and diverse connectivity structures, making it difficult to build\na generalist model. To address these challenges, we introduce a scalable\nmulti-graph multi-task pretraining approach specifically tailored for node\nclassification tasks across diverse graph datasets from different domains. Our\nmethod, Graph Foundation Model (GraphFM), leverages a Perceiver-based encoder\nthat employs learned latent tokens to compress domain-specific features into a\ncommon latent space. This approach enhances the model's ability to generalize\nacross different graphs and allows for scaling across diverse data. We\ndemonstrate the efficacy of our approach by training a model on 152 different\ngraph datasets comprising over 7.4 million nodes and 189 million edges,\nestablishing the first set of scaling laws for multi-graph pretraining on\ndatasets spanning many domains (e.g., molecules, citation and product graphs).\nOur results show that pretraining on a diverse array of real and synthetic\ngraphs improves the model's adaptability and stability, while performing\ncompetitively with state-of-the-art specialist models. This work illustrates\nthat multi-graph pretraining can significantly reduce the burden imposed by the\ncurrent graph training paradigm, unlocking new capabilities for the field of\ngraph neural networks by creating a single generalist model that performs\ncompetitively across a wide range of datasets and tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Social and Information Networks"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}