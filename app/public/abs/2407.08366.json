{"id":"2407.08366","title":"An Economic Framework for 6-DoF Grasp Detection","authors":"Xiao-Ming Wu, Jia-Feng Cai, Jian-Jian Jiang, Dian Zheng, Yi-Lin Wei,\n  Wei-Shi Zheng","authorsParsed":[["Wu","Xiao-Ming",""],["Cai","Jia-Feng",""],["Jiang","Jian-Jian",""],["Zheng","Dian",""],["Wei","Yi-Lin",""],["Zheng","Wei-Shi",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 10:19:48 GMT"}],"updateDate":"2024-07-12","timestamp":1720693188000,"abstract":"  Robotic grasping in clutters is a fundamental task in robotic manipulation.\nIn this work, we propose an economic framework for 6-DoF grasp detection,\naiming to economize the resource cost in training and meanwhile maintain\neffective grasp performance. To begin with, we discover that the dense\nsupervision is the bottleneck of current SOTA methods that severely encumbers\nthe entire training overload, meanwhile making the training difficult to\nconverge. To solve the above problem, we first propose an economic supervision\nparadigm for efficient and effective grasping. This paradigm includes a\nwell-designed supervision selection strategy, selecting key labels basically\nwithout ambiguity, and an economic pipeline to enable the training after\nselection. Furthermore, benefit from the economic supervision, we can focus on\na specific grasp, and thus we devise a focal representation module, which\ncomprises an interactive grasp head and a composite score estimation to\ngenerate the specific grasp more accurately. Combining all together, the\nEconomicGrasp framework is proposed. Our extensive experiments show that\nEconomicGrasp surpasses the SOTA grasp method by about 3AP on average, and with\nextremely low resource cost, for about 1/4 training time cost, 1/8 memory cost\nand 1/30 storage cost. Our code is available at\nhttps://github.com/iSEE-Laboratory/EconomicGrasp.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}