{"id":"2408.05021","title":"Two-norm discrepancy and convergence of the stochastic gradient method\n  with application to shape optimization","authors":"Marc Dambrine and Caroline Geiersbach and Helmut Harbrecht","authorsParsed":[["Dambrine","Marc",""],["Geiersbach","Caroline",""],["Harbrecht","Helmut",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 12:20:30 GMT"}],"updateDate":"2024-08-12","timestamp":1723206030000,"abstract":"  The present article is dedicated to proving convergence of the stochastic\ngradient method in case of random shape optimization problems. To that end, we\nconsider Bernoulli's exterior free boundary problem with a random interior\nboundary. We recast this problem into a shape optimization problem by means of\nthe minimization of the expected Dirichlet energy. By restricting ourselves to\nthe class of convex, sufficiently smooth domains of bounded curvature, the\nshape optimization problem becomes strongly convex with respect to an\nappropriate norm. Since this norm is weaker than the differentiability norm, we\nare confronted with the so-called two-norm discrepancy, a well-known phenomenon\nfrom optimal control. We therefore need to adapt the convergence theory of the\nstochastic gradient method to this specific setting correspondingly. The\ntheoretical findings are supported and validated by numerical experiments.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}