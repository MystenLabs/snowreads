{"id":"2407.18968","title":"Intelligence Analysis of Language Models","authors":"Liane Galanti and Ethan Baron","authorsParsed":[["Galanti","Liane",""],["Baron","Ethan",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 13:48:16 GMT"}],"updateDate":"2024-07-30","timestamp":1721483296000,"abstract":"  In this project, we test the effectiveness of Large Language Models (LLMs) on\nthe Abstraction and Reasoning Corpus (ARC) dataset. This dataset serves as a\nrepresentative benchmark for testing abstract reasoning abilities, requiring a\nfundamental understanding of key concepts such as object identification, basic\ncounting, and elementary geometric principles. Tasks from this dataset are\nconverted into a prompt-based format for evaluation. Initially, we assess the\nmodels' potential through a Zero-shot approach. Subsequently, we investigate\nthe application of the Chain-of-Thought (CoT) technique, aiming to determine\nits role in improving model performance. Our results suggest that, despite the\nhigh expectations placed on contemporary LLMs, these models still struggle in\nnon-linguistic domains, even when dealing with simpler subsets of the ARC\ndataset. Our study is the first to concentrate on the capabilities of\nopen-source models in this context. The code, dataset, and prompts supporting\nthis project's findings can be found in our GitHub repository, accessible at:\nhttps://github.com/Lianga2000/LLMsOnARC.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"y-3mM_7ueuYHTVLGoGT8ybKHR4Y6rB4QiJxTJAyA5FI","pdfSize":"319527","objectId":"0x8c4a2a1c6b0f140ef057427bb80d72f8b77d9b66a4140968840516d6564c8f75","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
