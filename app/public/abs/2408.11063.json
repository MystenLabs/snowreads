{"id":"2408.11063","title":"Tabular Transfer Learning via Prompting LLMs","authors":"Jaehyun Nam, Woomin Song, Seong Hyeon Park, Jihoon Tack, Sukmin Yun,\n  Jaehyung Kim, Kyu Hwan Oh, Jinwoo Shin","authorsParsed":[["Nam","Jaehyun",""],["Song","Woomin",""],["Park","Seong Hyeon",""],["Tack","Jihoon",""],["Yun","Sukmin",""],["Kim","Jaehyung",""],["Oh","Kyu Hwan",""],["Shin","Jinwoo",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 11:30:52 GMT"}],"updateDate":"2024-08-22","timestamp":1723203052000,"abstract":"  Learning with a limited number of labeled data is a central problem in\nreal-world applications of machine learning, as it is often expensive to obtain\nannotations. To deal with the scarcity of labeled data, transfer learning is a\nconventional approach; it suggests to learn a transferable knowledge by\ntraining a neural network from multiple other sources. In this paper, we\ninvestigate transfer learning of tabular tasks, which has been less studied and\nsuccessful in the literature, compared to other domains, e.g., vision and\nlanguage. This is because tables are inherently heterogeneous, i.e., they\ncontain different columns and feature spaces, making transfer learning\ndifficult. On the other hand, recent advances in natural language processing\nsuggest that the label scarcity issue can be mitigated by utilizing in-context\nlearning capability of large language models (LLMs). Inspired by this and the\nfact that LLMs can also process tables within a unified language space, we ask\nwhether LLMs can be effective for tabular transfer learning, in particular,\nunder the scenarios where the source and target datasets are of different\nformat. As a positive answer, we propose a novel tabular transfer learning\nframework, coined Prompt to Transfer (P2T), that utilizes unlabeled (or\nheterogeneous) source data with LLMs. Specifically, P2T identifies a column\nfeature in a source dataset that is strongly correlated with a target task\nfeature to create examples relevant to the target task, thus creating\npseudo-demonstrations for prompts. Experimental results demonstrate that P2T\noutperforms previous methods on various tabular learning benchmarks, showing\ngood promise for the important, yet underexplored tabular transfer learning\nproblem. Code is available at https://github.com/jaehyun513/P2T.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}