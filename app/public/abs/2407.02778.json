{"id":"2407.02778","title":"Foster Adaptivity and Balance in Learning with Noisy Labels","authors":"Mengmeng Sheng, Zeren Sun, Tao Chen, Shuchao Pang, Yucheng Wang,\n  Yazhou Yao","authorsParsed":[["Sheng","Mengmeng",""],["Sun","Zeren",""],["Chen","Tao",""],["Pang","Shuchao",""],["Wang","Yucheng",""],["Yao","Yazhou",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 03:10:24 GMT"}],"updateDate":"2024-07-04","timestamp":1719976224000,"abstract":"  Label noise is ubiquitous in real-world scenarios, posing a practical\nchallenge to supervised models due to its effect in hurting the generalization\nperformance of deep neural networks. Existing methods primarily employ the\nsample selection paradigm and usually rely on dataset-dependent prior knowledge\n(\\eg, a pre-defined threshold) to cope with label noise, inevitably degrading\nthe adaptivity. Moreover, existing methods tend to neglect the class balance in\nselecting samples, leading to biased model performance. To this end, we propose\na simple yet effective approach named \\textbf{SED} to deal with label noise in\na \\textbf{S}elf-adaptiv\\textbf{E} and class-balance\\textbf{D} manner.\nSpecifically, we first design a novel sample selection strategy to empower\nself-adaptivity and class balance when identifying clean and noisy data. A\nmean-teacher model is then employed to correct labels of noisy samples.\nSubsequently, we propose a self-adaptive and class-balanced sample re-weighting\nmechanism to assign different weights to detected noisy samples. Finally, we\nadditionally employ consistency regularization on selected clean samples to\nimprove model generalization performance. Extensive experimental results on\nsynthetic and real-world datasets demonstrate the effectiveness and superiority\nof our proposed method. The source code has been made available at\nhttps://github.com/NUST-Machine-Intelligence-Laboratory/SED.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}