{"id":"2408.12505","title":"Stochastic Compositional Minimax Optimization with Provable Convergence\n  Guarantees","authors":"Yuyang Deng, Fuli Qiao, Mehrdad Mahdavi","authorsParsed":[["Deng","Yuyang",""],["Qiao","Fuli",""],["Mahdavi","Mehrdad",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 16:00:31 GMT"}],"updateDate":"2024-08-23","timestamp":1724342431000,"abstract":"  Stochastic compositional minimax problems are prevalent in machine learning,\nyet there are only limited established on the convergence of this class of\nproblems. In this paper, we propose a formal definition of the stochastic\ncompositional minimax problem, which involves optimizing a minimax loss with a\ncompositional structure either in primal , dual, or both primal and dual\nvariables. We introduce a simple yet effective algorithm, stochastically\nCorrected stOchastic gradient Descent Ascent (CODA), which is a descent ascent\ntype algorithm with compositional correction steps, and establish its\nconvergence rate in aforementioned three settings. In the presence of the\ncompositional structure in primal, the objective function typically becomes\nnonconvex in primal due to function composition. Thus, we consider the\nnonconvex-strongly-concave and nonconvex-concave settings and show that CODA\ncan efficiently converge to a stationary point. In the case of composition on\nthe dual, the objective function becomes nonconcave in the dual variable, and\nwe demonstrate convergence in the strongly-convex-nonconcave and\nconvex-nonconcave setting. In the case of composition on both variables, the\nprimal and dual variables may lose convexity and concavity, respectively.\nTherefore, we anaylze the convergence in weakly-convex-weakly-concave setting.\nWe also give a variance reduction version algorithm, CODA+, which achieves the\nbest known rate on nonconvex-strongly-concave and nonconvex-concave\ncompositional minimax problem. This work initiates the theoretical study of the\nstochastic compositional minimax problem on various settings and may inform\nmodern machine learning scenarios such as domain adaptation or robust\nmodel-agnostic meta-learning.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}