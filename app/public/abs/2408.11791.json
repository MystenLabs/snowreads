{"id":"2408.11791","title":"Critique-out-Loud Reward Models","authors":"Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang,\n  Prithviraj Ammanabrolu","authorsParsed":[["Ankner","Zachary",""],["Paul","Mansheej",""],["Cui","Brandon",""],["Chang","Jonathan D.",""],["Ammanabrolu","Prithviraj",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:24:15 GMT"}],"updateDate":"2024-08-22","timestamp":1724261055000,"abstract":"  Traditionally, reward models used for reinforcement learning from human\nfeedback (RLHF) are trained to directly predict preference scores without\nleveraging the generation capabilities of the underlying large language model\n(LLM). This limits the capabilities of reward models as they must reason\nimplicitly about the quality of a response, i.e., preference modeling must be\nperformed in a single forward pass through the model. To enable reward models\nto reason explicitly about the quality of a response, we introduce\nCritique-out-Loud (CLoud) reward models. CLoud reward models operate by first\ngenerating a natural language critique of the assistant's response that is then\nused to predict a scalar reward for the quality of the response. We demonstrate\nthe success of CLoud reward models for both Llama-3-8B and 70B base models:\ncompared to classic reward models CLoud reward models improve pairwise\npreference classification accuracy on RewardBench by 4.65 and 5.84 percentage\npoints for the 8B and 70B base models respectively. Furthermore, CLoud reward\nmodels lead to a Pareto improvement for win rate on ArenaHard when used as the\nscoring model for Best-of-N. Finally, we explore how to exploit the dynamic\ninference compute capabilities of CLoud reward models by performing\nself-consistency decoding for reward prediction.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}