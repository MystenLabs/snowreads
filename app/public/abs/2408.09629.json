{"id":"2408.09629","title":"A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic\n  Text Classification","authors":"Claudio M. V. de Andrade, Washington Cunha, Davi Reis, Adriana Silvina\n  Pagano, Leonardo Rocha, Marcos Andr\\'e Gon\\c{c}alves","authorsParsed":[["de Andrade","Claudio M. V.",""],["Cunha","Washington",""],["Reis","Davi",""],["Pagano","Adriana Silvina",""],["Rocha","Leonardo",""],["Gonçalves","Marcos André",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 01:22:21 GMT"}],"updateDate":"2024-08-20","timestamp":1724030541000,"abstract":"  Transformer models have achieved state-of-the-art results, with Large\nLanguage Models (LLMs), an evolution of first-generation transformers (1stTR),\nbeing considered the cutting edge in several NLP tasks. However, the literature\nhas yet to conclusively demonstrate that LLMs consistently outperform 1stTRs\nacross all NLP tasks. This study compares three 1stTRs (BERT, RoBERTa, and\nBART) with two open LLMs (Llama 2 and Bloom) across 11 sentiment analysis\ndatasets. The results indicate that open LLMs may moderately outperform or\nmatch 1stTRs in 8 out of 11 datasets but only when fine-tuned. Given this\nsubstantial cost for only moderate gains, the practical applicability of these\nmodels in cost-sensitive scenarios is questionable. In this context, a\nconfidence-based strategy that seamlessly integrates 1stTRs with open LLMs\nbased on prediction certainty is proposed. High-confidence documents are\nclassified by the more cost-effective 1stTRs, while uncertain cases are handled\nby LLMs in zero-shot or few-shot modes, at a much lower cost than fine-tuned\nversions. Experiments in sentiment analysis demonstrate that our solution not\nonly outperforms 1stTRs, zero-shot, and few-shot LLMs but also competes closely\nwith fine-tuned LLMs at a fraction of the cost.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}