{"id":"2408.12406","title":"Generalized SAM: Efficient Fine-Tuning of SAM for Variable Input Image\n  Sizes","authors":"Sota Kato, Hinako Mitsuoka and Kazuhiro Hotta","authorsParsed":[["Kato","Sota",""],["Mitsuoka","Hinako",""],["Hotta","Kazuhiro",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 13:58:08 GMT"}],"updateDate":"2024-08-23","timestamp":1724335088000,"abstract":"  There has been a lot of recent research on improving the efficiency of\nfine-tuning foundation models. In this paper, we propose a novel efficient\nfine-tuning method that allows the input image size of Segment Anything Model\n(SAM) to be variable. SAM is a powerful foundational model for image\nsegmentation trained on huge datasets, but it requires fine-tuning to recognize\narbitrary classes. The input image size of SAM is fixed at 1024 x 1024,\nresulting in substantial computational demands during training. Furthermore,\nthe fixed input image size may result in the loss of image information, e.g.\ndue to fixed aspect ratios. To address this problem, we propose Generalized SAM\n(GSAM). Different from the previous methods, GSAM is the first to apply random\ncropping during training with SAM, thereby significantly reducing the\ncomputational cost of training. Experiments on datasets of various types and\nvarious pixel counts have shown that GSAM can train more efficiently than SAM\nand other fine-tuning methods for SAM, achieving comparable or higher accuracy.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}