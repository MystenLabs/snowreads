{"id":"2407.15046","title":"Audio-visual training for improved grounding in video-text LLMs","authors":"Shivprasad Sagare, Hemachandran S, Kinshuk Sarabhai, Prashant\n  Ullegaddi, Rajeshkumar SA","authorsParsed":[["Sagare","Shivprasad",""],["S","Hemachandran",""],["Sarabhai","Kinshuk",""],["Ullegaddi","Prashant",""],["SA","Rajeshkumar",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 03:59:14 GMT"}],"updateDate":"2024-07-23","timestamp":1721534354000,"abstract":"  Recent advances in multimodal LLMs, have led to several video-text models\nbeing proposed for critical video-related tasks. However, most of the previous\nworks support visual input only, essentially muting the audio signal in the\nvideo. Few models that support both audio and visual input, are not explicitly\ntrained on audio data. Hence, the effect of audio towards video understanding\nis largely unexplored. To this end, we propose a model architecture that\nhandles audio-visual inputs explicitly. We train our model with both audio and\nvisual data from a video instruction-tuning dataset. Comparison with\nvision-only baselines, and other audio-visual models showcase that training on\naudio data indeed leads to improved grounding of responses. For better\nevaluation of audio-visual models, we also release a human-annotated benchmark\ndataset, with audio-aware question-answer pairs.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"nOEHo-sSGtzYAcATp4EdYzuW6efN14OP2UGPCXRD1P0","pdfSize":"446440","objectId":"0xf3fa9f2f593e4ebfcabfd234cdf456cb6fa595306ec437e110fb4f443c5cd816","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
