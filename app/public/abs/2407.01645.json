{"id":"2407.01645","title":"Sign Gradient Descent-based Neuronal Dynamics: ANN-to-SNN Conversion\n  Beyond ReLU Network","authors":"Hyunseok Oh, Youngki Lee","authorsParsed":[["Oh","Hyunseok",""],["Lee","Youngki",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 02:09:20 GMT"}],"updateDate":"2024-07-03","timestamp":1719799760000,"abstract":"  Spiking neural network (SNN) is studied in multidisciplinary domains to (i)\nenable order-of-magnitudes energy-efficient AI inference and (ii)\ncomputationally simulate neuro-scientific mechanisms. The lack of discrete\ntheory obstructs the practical application of SNN by limiting its performance\nand nonlinearity support. We present a new optimization-theoretic perspective\nof the discrete dynamics of spiking neurons. We prove that a discrete dynamical\nsystem of simple integrate-and-fire models approximates the sub-gradient method\nover unconstrained optimization problems. We practically extend our theory to\nintroduce a novel sign gradient descent (signGD)-based neuronal dynamics that\ncan (i) approximate diverse nonlinearities beyond ReLU and (ii) advance\nANN-to-SNN conversion performance in low time steps. Experiments on large-scale\ndatasets show that our technique achieves (i) state-of-the-art performance in\nANN-to-SNN conversion and (ii) is the first to convert new DNN architectures,\ne.g., ConvNext, MLP-Mixer, and ResMLP. We publicly share our source code at\nhttps://github.com/snuhcs/snn_signgd .\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}