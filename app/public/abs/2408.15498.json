{"id":"2408.15498","title":"Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single\n  Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep\n  Feed-forward Neural Network","authors":"Yijun Zhou, Om Arora-Jain, Xia Jiang","authorsParsed":[["Zhou","Yijun",""],["Arora-Jain","Om",""],["Jiang","Xia",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 03:00:43 GMT"}],"updateDate":"2024-08-29","timestamp":1724814043000,"abstract":"  While machine learning has advanced in medicine, its widespread use in\nclinical applications, especially in predicting breast cancer metastasis, is\nstill limited. We have been dedicated to constructing a DFNN model to predict\nbreast cancer metastasis n years in advance. However, the challenge lies in\nefficiently identifying optimal hyperparameter values through grid search,\ngiven the constraints of time and resources. Issues such as the infinite\npossibilities for continuous hyperparameters like l1 and l2, as well as the\ntime-consuming and costly process, further complicate the task. To address\nthese challenges, we developed Single Hyperparameter Grid Search (SHGS)\nstrategy, serving as a preselection method before grid search. Our experiments\nwith SHGS applied to DFNN models for breast cancer metastasis prediction focus\non analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,\nlearning rate, decay, and momentum. We created three figures, each depicting\nthe experiment results obtained from three LSM-I-10-Plus-year datasets. These\nfigures illustrate the relationship between model performance and the target\nhyperparameter values. For each hyperparameter, we analyzed whether changes in\nthis hyperparameter would affect model performance, examined if there were\nspecific patterns, and explored how to choose values for the particular\nhyperparameter. Our experimental findings reveal that the optimal value of a\nhyperparameter is not only dependent on the dataset but is also significantly\ninfluenced by the settings of other hyperparameters. Additionally, our\nexperiments suggested some reduced range of values for a target hyperparameter,\nwhich may be helpful for low-budget grid search. This approach serves as a\nprior experience and foundation for subsequent use of grid search to enhance\nmodel performance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing","Quantitative Biology/Quantitative Methods"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}