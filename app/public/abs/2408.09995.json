{"id":"2408.09995","title":"Uniting contrastive and generative learning for event sequences models","authors":"Aleksandr Yugay and Alexey Zaytsev","authorsParsed":[["Yugay","Aleksandr",""],["Zaytsev","Alexey",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 13:47:17 GMT"}],"updateDate":"2024-08-20","timestamp":1724075237000,"abstract":"  High-quality representation of transactional sequences is vital for modern\nbanking applications, including risk management, churn prediction, and\npersonalized customer offers. Different tasks require distinct representation\nproperties: local tasks benefit from capturing the client's current state,\nwhile global tasks rely on general behavioral patterns. Previous research has\ndemonstrated that various self-supervised approaches yield representations that\nbetter capture either global or local qualities.\n  This study investigates the integration of two self-supervised learning\ntechniques - instance-wise contrastive learning and a generative approach based\non restoring masked events in latent space. The combined approach creates\nrepresentations that balance local and global transactional data\ncharacteristics. Experiments conducted on several public datasets, focusing on\nsequence classification and next-event type prediction, show that the\nintegrated method achieves superior performance compared to individual\napproaches and demonstrates synergistic effects. These findings suggest that\nthe proposed approach offers a robust framework for advancing event sequences\nrepresentation learning in the financial sector.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}