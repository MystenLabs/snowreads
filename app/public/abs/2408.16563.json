{"id":"2408.16563","title":"MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair\n  Face Recognition","authors":"Eduarda Caldeira, Jaime S. Cardoso, Ana F. Sequeira, Pedro C. Neto","authorsParsed":[["Caldeira","Eduarda",""],["Cardoso","Jaime S.",""],["Sequeira","Ana F.",""],["Neto","Pedro C.",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 14:30:45 GMT"}],"updateDate":"2024-08-30","timestamp":1724941845000,"abstract":"  As in school, one teacher to cover all subjects is insufficient to distill\nequally robust information to a student. Hence, each subject is taught by a\nhighly specialised teacher. Following a similar philosophy, we propose a\nmultiple specialized teacher framework to distill knowledge to a student\nnetwork. In our approach, directed at face recognition use cases, we train four\nteachers on one specific ethnicity, leading to four highly specialized and\nbiased teachers. Our strategy learns a project of these four teachers into a\ncommon space and distill that information to a student network. Our results\nhighlighted increased performance and reduced bias for all our experiments. In\naddition, we further show that having biased/specialized teachers is crucial by\nshowing that our approach achieves better results than when knowledge is\ndistilled from four teachers trained on balanced datasets. Our approach\nrepresents a step forward to the understanding of the importance of\nethnicity-specific features.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}