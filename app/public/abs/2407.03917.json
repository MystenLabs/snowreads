{"id":"2407.03917","title":"Timestep-Aware Correction for Quantized Diffusion Models","authors":"Yuzhe Yao, Feng Tian, Jun Chen, Haonan Lin, Guang Dai, Yong Liu,\n  Jingdong Wang","authorsParsed":[["Yao","Yuzhe",""],["Tian","Feng",""],["Chen","Jun",""],["Lin","Haonan",""],["Dai","Guang",""],["Liu","Yong",""],["Wang","Jingdong",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 13:22:31 GMT"}],"updateDate":"2024-07-08","timestamp":1720099351000,"abstract":"  Diffusion models have marked a significant breakthrough in the synthesis of\nsemantically coherent images. However, their extensive noise estimation\nnetworks and the iterative generation process limit their wider application,\nparticularly on resource-constrained platforms like mobile devices. Existing\npost-training quantization (PTQ) methods have managed to compress diffusion\nmodels to low precision. Nevertheless, due to the iterative nature of diffusion\nmodels, quantization errors tend to accumulate throughout the generation\nprocess. This accumulation of error becomes particularly problematic in\nlow-precision scenarios, leading to significant distortions in the generated\nimages. We attribute this accumulation issue to two main causes: error\npropagation and exposure bias. To address these problems, we propose a\ntimestep-aware correction method for quantized diffusion model, which\ndynamically corrects the quantization error. By leveraging the proposed method\nin low-precision diffusion models, substantial enhancement of output quality\ncould be achieved with only negligible computation overhead. Extensive\nexperiments underscore our method's effectiveness and generalizability. By\nemploying the proposed correction strategy, we achieve state-of-the-art (SOTA)\nresults on low-precision models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}