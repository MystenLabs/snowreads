{"id":"2408.16766","title":"CSGO: Content-Style Composition in Text-to-Image Generation","authors":"Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai,\n  Renyuan Huang, Zechao Li","authorsParsed":[["Xing","Peng",""],["Wang","Haofan",""],["Sun","Yanpeng",""],["Wang","Qixun",""],["Bai","Xu",""],["Ai","Hao",""],["Huang","Renyuan",""],["Li","Zechao",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 17:59:30 GMT"},{"version":"v2","created":"Wed, 4 Sep 2024 10:42:41 GMT"}],"updateDate":"2024-09-05","timestamp":1724954370000,"abstract":"  The diffusion model has shown exceptional capabilities in controlled image\ngeneration, which has further fueled interest in image style transfer. Existing\nworks mainly focus on training free-based methods (e.g., image inversion) due\nto the scarcity of specific data. In this study, we present a data construction\npipeline for content-style-stylized image triplets that generates and\nautomatically cleanses stylized data triplets. Based on this pipeline, we\nconstruct a dataset IMAGStyle, the first large-scale style transfer dataset\ncontaining 210k image triplets, available for the community to explore and\nresearch. Equipped with IMAGStyle, we propose CSGO, a style transfer model\nbased on end-to-end training, which explicitly decouples content and style\nfeatures employing independent feature injection. The unified CSGO implements\nimage-driven style transfer, text-driven stylized synthesis, and text\nediting-driven stylized synthesis. Extensive experiments demonstrate the\neffectiveness of our approach in enhancing style control capabilities in image\ngeneration. Additional visualization and access to the source code can be\nlocated on the project page: \\url{https://csgo-gen.github.io/}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}