{"id":"2408.01777","title":"Infinite random forests for imbalanced classification tasks","authors":"Moria Mayala, Olivier Wintenberger, Charles Tillier and Cl\\'ement\n  Dombry","authorsParsed":[["Mayala","Moria",""],["Wintenberger","Olivier",""],["Tillier","Charles",""],["Dombry","Cl√©ment",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 13:14:34 GMT"}],"updateDate":"2024-08-06","timestamp":1722690874000,"abstract":"  This paper investigates predictive probability inference for classification\ntasks using random forests in the context of imbalanced data. In this setting,\nwe analyze the asymptotic properties of simplified versions of the original\nBreiman's algorithm, namely subsampling and under-sampling Infinite Random\nForests (IRFs), and establish the asymptotic normality of these two models. The\nunder-sampling IRFs, that tackle the challenge of the predicting the minority\nclass by a downsampling strategy to under-represent the majority class show\nasymptotic bias. To address this problem, we introduce a new estimator based on\nan Importance Sampling debiasing procedure built upon on odds ratios. We apply\nour results considering 1-Nearest Neighbors (1-NN) as individual trees of the\nIRFs. The resulting bagged 1-NN estimator achieves the same asymptotic rate in\nthe three configurations but with a different asymptotic variance. Finally, we\nconduct simulations to validate the empirical relevance of our theoretical\nfindings.\n","subjects":["Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}