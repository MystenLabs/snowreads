{"id":"2408.10388","title":"Narrowing the Gap between Vision and Action in Navigation","authors":"Yue Zhang and Parisa Kordjamshidi","authorsParsed":[["Zhang","Yue",""],["Kordjamshidi","Parisa",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 20:09:56 GMT"}],"updateDate":"2024-08-21","timestamp":1724098196000,"abstract":"  The existing methods for Vision and Language Navigation in the Continuous\nEnvironment (VLN-CE) commonly incorporate a waypoint predictor to discretize\nthe environment. This simplifies the navigation actions into a view selection\ntask and improves navigation performance significantly compared to direct\ntraining using low-level actions. However, the VLN-CE agents are still far from\nthe real robots since there are gaps between their visual perception and\nexecuted actions. First, VLN-CE agents that discretize the visual environment\nare primarily trained with high-level view selection, which causes them to\nignore crucial spatial reasoning within the low-level action movements. Second,\nin these models, the existing waypoint predictors neglect object semantics and\ntheir attributes related to passibility, which can be informative in indicating\nthe feasibility of actions. To address these two issues, we introduce a\nlow-level action decoder jointly trained with high-level action prediction,\nenabling the current VLN agent to learn and ground the selected visual view to\nthe low-level controls. Moreover, we enhance the current waypoint predictor by\nutilizing visual representations containing rich semantic information and\nexplicitly masking obstacles based on humans' prior knowledge about the\nfeasibility of actions. Empirically, our agent can improve navigation\nperformance metrics compared to the strong baselines on both high-level and\nlow-level actions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}