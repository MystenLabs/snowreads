{"id":"2407.02894","title":"Translatotron-V(ison): An End-to-End Model for In-Image Machine\n  Translation","authors":"Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Min Zhang, Jinsong Su","authorsParsed":[["Lan","Zhibin",""],["Niu","Liqiang",""],["Meng","Fandong",""],["Zhou","Jie",""],["Zhang","Min",""],["Su","Jinsong",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 08:15:39 GMT"}],"updateDate":"2024-07-04","timestamp":1719994539000,"abstract":"  In-image machine translation (IIMT) aims to translate an image containing\ntexts in source language into an image containing translations in target\nlanguage. In this regard, conventional cascaded methods suffer from issues such\nas error propagation, massive parameters, and difficulties in deployment and\nretaining visual characteristics of the input image. Thus, constructing\nend-to-end models has become an option, which, however, faces two main\nchallenges: 1) the huge modeling burden, as it is required to simultaneously\nlearn alignment across languages and preserve the visual characteristics of the\ninput image; 2) the difficulties of directly predicting excessively lengthy\npixel sequences. In this paper, we propose \\textit{Translatotron-V(ision)}, an\nend-to-end IIMT model consisting of four modules. In addition to an image\nencoder, and an image decoder, our model contains a target text decoder and an\nimage tokenizer. Among them, the target text decoder is used to alleviate the\nlanguage alignment burden, and the image tokenizer converts long sequences of\npixels into shorter sequences of visual tokens, preventing the model from\nfocusing on low-level visual features. Besides, we present a two-stage training\nframework for our model to assist the model in learning alignment across\nmodalities and languages. Finally, we propose a location-aware evaluation\nmetric called Structure-BLEU to assess the translation quality of the generated\nimages. Experimental results demonstrate that our model achieves competitive\nperformance compared to cascaded models with only 70.9\\% of parameters, and\nsignificantly outperforms the pixel-level end-to-end IIMT model.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}