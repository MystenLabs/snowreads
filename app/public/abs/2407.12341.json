{"id":"2407.12341","title":"LLM-based query paraphrasing for video search","authors":"Jiaxin Wu, Chong-Wah Ngo, Wing-Kwong Chan, Sheng-Hua Zhong","authorsParsed":[["Wu","Jiaxin",""],["Ngo","Chong-Wah",""],["Chan","Wing-Kwong",""],["Zhong","Sheng-Hua",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 06:33:51 GMT"}],"updateDate":"2024-07-18","timestamp":1721198031000,"abstract":"  Text-to-video retrieval answers user queries through search by concepts and\nembeddings. Limited by the size of the concept bank and the amount of training\ndata, answering queries in the wild is not always effective due to the\nout-of-vocabulary problem. Furthermore, neither concept-based nor\nembedding-based search can perform reasoning to consolidate the search results\nfor complex queries mixed with logical and spatial constraints. To address\nthese problems, we leverage large language models (LLM) to paraphrase the query\nby text-to-text (T2T), text-to-image (T2I), and image-to-text (I2T)\ntransformations. These transformations rephrase abstract concepts into simple\nwords to address the out-of-vocabulary problem. Furthermore, the complex\nrelationship in a query can be decoupled into simpler sub-queries, yielding\nbetter retrieval performance when fusing the search results of these\nsub-queries. To address the LLM hallucination problem, this paper also proposes\na novel consistency-based verification strategy to filter the paraphrased\nqueries that are factually incorrect. Extensive experiments are conducted for\nad-hoc video search and known-item search on the TRECVid datasets. We provide\nempirical insights into how traditionally difficult-to-answer queries can be\nresolved by query paraphrasing.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}