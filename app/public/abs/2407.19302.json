{"id":"2407.19302","title":"IBMEA: Exploring Variational Information Bottleneck for Multi-modal\n  Entity Alignment","authors":"Taoyu Su, Jiawei Sheng, Shicheng Wang, Xinghua Zhang, Hongbo Xu,\n  Tingwen Liu","authorsParsed":[["Su","Taoyu",""],["Sheng","Jiawei",""],["Wang","Shicheng",""],["Zhang","Xinghua",""],["Xu","Hongbo",""],["Liu","Tingwen",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 17:12:37 GMT"}],"updateDate":"2024-07-30","timestamp":1722100357000,"abstract":"  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}