{"id":"2408.05640","title":"Federated Smoothing Proximal Gradient for Quantile Regression with\n  Non-Convex Penalties","authors":"Reza Mirzaeifard, Diyako Ghaderyan, Stefan Werner","authorsParsed":[["Mirzaeifard","Reza",""],["Ghaderyan","Diyako",""],["Werner","Stefan",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 21:50:19 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 11:52:42 GMT"}],"updateDate":"2024-08-14","timestamp":1723326619000,"abstract":"  Distributed sensors in the internet-of-things (IoT) generate vast amounts of\nsparse data. Analyzing this high-dimensional data and identifying relevant\npredictors pose substantial challenges, especially when data is preferred to\nremain on the device where it was collected for reasons such as data integrity,\ncommunication bandwidth, and privacy. This paper introduces a federated\nquantile regression algorithm to address these challenges. Quantile regression\nprovides a more comprehensive view of the relationship between variables than\nmean regression models. However, traditional approaches face difficulties when\ndealing with nonconvex sparse penalties and the inherent non-smoothness of the\nloss function. For this purpose, we propose a federated smoothing proximal\ngradient (FSPG) algorithm that integrates a smoothing mechanism with the\nproximal gradient framework, thereby enhancing both precision and computational\nspeed. This integration adeptly handles optimization over a network of devices,\neach holding local data samples, making it particularly effective in federated\nlearning scenarios. The FSPG algorithm ensures steady progress and reliable\nconvergence in each iteration by maintaining or reducing the value of the\nobjective function. By leveraging nonconvex penalties, such as the minimax\nconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), the\nproposed method can identify and preserve key predictors within sparse models.\nComprehensive simulations validate the robust theoretical foundations of the\nproposed algorithm and demonstrate improved estimation precision and reliable\nconvergence.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}