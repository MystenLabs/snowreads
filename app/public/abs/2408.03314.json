{"id":"2408.03314","title":"Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling Model Parameters","authors":"Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar","authorsParsed":[["Snell","Charlie",""],["Lee","Jaehoon",""],["Xu","Kelvin",""],["Kumar","Aviral",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 17:35:05 GMT"}],"updateDate":"2024-08-07","timestamp":1722965705000,"abstract":"  Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TiMokwpi06yfZCMjh-ZjJ0dn9H_f13hBiBn4M_my5rY","pdfSize":"3870997"}
