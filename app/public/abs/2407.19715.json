{"id":"2407.19715","title":"Generalization bounds for regression and classification on adaptive\n  covering input domains","authors":"Wen-Liang Hwang","authorsParsed":[["Hwang","Wen-Liang",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 05:40:08 GMT"}],"updateDate":"2024-07-30","timestamp":1722231608000,"abstract":"  Our main focus is on the generalization bound, which serves as an upper limit\nfor the generalization error. Our analysis delves into regression and\nclassification tasks separately to ensure a thorough examination. We assume the\ntarget function is real-valued and Lipschitz continuous for regression tasks.\nWe use the 2-norm and a root-mean-square-error (RMSE) variant to measure the\ndisparities between predictions and actual values. In the case of\nclassification tasks, we treat the target function as a one-hot classifier,\nrepresenting a piece-wise constant function, and employ 0/1 loss for error\nmeasurement. Our analysis underscores the differing sample complexity required\nto achieve a concentration inequality of generalization bounds, highlighting\nthe variation in learning efficiency for regression and classification tasks.\nFurthermore, we demonstrate that the generalization bounds for regression and\nclassification functions are inversely proportional to a polynomial of the\nnumber of parameters in a network, with the degree depending on the hypothesis\nclass and the network architecture. These findings emphasize the advantages of\nover-parameterized networks and elucidate the conditions for benign overfitting\nin such systems.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}