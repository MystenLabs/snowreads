{"id":"2407.07089","title":"Fine-Tuning Linear Layers Only Is a Simple yet Effective Way for Task\n  Arithmetic","authors":"Ruochen Jin, Bojian Hou, Jiancong Xiao, Weijie Su and Li Shen","authorsParsed":[["Jin","Ruochen",""],["Hou","Bojian",""],["Xiao","Jiancong",""],["Su","Weijie",""],["Shen","Li",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 17:59:17 GMT"}],"updateDate":"2024-07-10","timestamp":1720547957000,"abstract":"  Task arithmetic has recently emerged as a cost-effective and scalable\napproach to edit pre-trained models directly in weight space, by adding the\nfine-tuned weights of different tasks. The performance has been further\nimproved by a linear property which is illustrated by weight disentanglement.\nYet, conventional linearization methods (e.g., NTK linearization) not only\ndouble the time and training cost but also have a disadvantage on single-task\nperformance. We propose a simple yet effective and efficient method that only\nfine-tunes linear layers, which improves weight disentanglement and efficiency\nsimultaneously. Specifically, our study reveals that only fine-tuning the\nlinear layers in the attention modules makes the whole model occur in a linear\nregime, significantly improving weight disentanglement. To further understand\nhow our method improves the disentanglement of task arithmetic, we present a\ncomprehensive study of task arithmetic by differentiating the role of\nrepresentation model and task-specific model. In particular, we find that the\nrepresentation model plays an important role in improving weight\ndisentanglement whereas the task-specific models such as the classification\nheads can degenerate the weight disentanglement performance. Overall, our work\nuncovers novel insights into the fundamental mechanisms of task arithmetic and\noffers a more reliable and effective approach to editing pre-trained models.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}