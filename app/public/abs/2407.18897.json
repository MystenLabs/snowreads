{"id":"2407.18897","title":"Small Molecule Optimization with Large Language Models","authors":"Philipp Guevorguian, Menua Bedrosian, Tigran Fahradyan, Gayane\n  Chilingaryan, Hrant Khachatrian, Armen Aghajanyan","authorsParsed":[["Guevorguian","Philipp",""],["Bedrosian","Menua",""],["Fahradyan","Tigran",""],["Chilingaryan","Gayane",""],["Khachatrian","Hrant",""],["Aghajanyan","Armen",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 17:51:33 GMT"}],"updateDate":"2024-07-29","timestamp":1722016293000,"abstract":"  Recent advancements in large language models have opened new possibilities\nfor generative molecular drug design. We present Chemlactica and Chemma, two\nlanguage models fine-tuned on a novel corpus of 110M molecules with computed\nproperties, totaling 40B tokens. These models demonstrate strong performance in\ngenerating molecules with specified properties and predicting new molecular\ncharacteristics from limited samples. We introduce a novel optimization\nalgorithm that leverages our language models to optimize molecules for\narbitrary properties given limited access to a black box oracle. Our approach\ncombines ideas from genetic algorithms, rejection sampling, and prompt\noptimization. It achieves state-of-the-art performance on multiple molecular\noptimization benchmarks, including an 8% improvement on Practical Molecular\nOptimization compared to previous methods. We publicly release the training\ncorpus, the language models and the optimization algorithm.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing","Quantitative Biology/Quantitative Methods"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}