{"id":"2408.15474","title":"Drop the beat! Freestyler for Accompaniment Conditioned Rapping Voice\n  Generation","authors":"Ziqian Ning, Shuai Wang, Yuepeng Jiang, Jixun Yao, Lei He, Shifeng\n  Pan, Jie Ding, Lei Xie","authorsParsed":[["Ning","Ziqian",""],["Wang","Shuai",""],["Jiang","Yuepeng",""],["Yao","Jixun",""],["He","Lei",""],["Pan","Shifeng",""],["Ding","Jie",""],["Xie","Lei",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 01:44:08 GMT"}],"updateDate":"2024-08-29","timestamp":1724809448000,"abstract":"  Rap, a prominent genre of vocal performance, remains underexplored in vocal\ngeneration. General vocal synthesis depends on precise note and duration\ninputs, requiring users to have related musical knowledge, which limits\nflexibility. In contrast, rap typically features simpler melodies, with a core\nfocus on a strong rhythmic sense that harmonizes with accompanying beats. In\nthis paper, we propose Freestyler, the first system that generates rapping\nvocals directly from lyrics and accompaniment inputs. Freestyler utilizes\nlanguage model-based token generation, followed by a conditional flow matching\nmodel to produce spectrograms and a neural vocoder to restore audio. It allows\na 3-second prompt to enable zero-shot timbre control. Due to the scarcity of\npublicly available rap datasets, we also present RapBank, a rap song dataset\ncollected from the internet, alongside a meticulously designed processing\npipeline. Experimental results show that Freestyler produces high-quality\nrapping voice generation with enhanced naturalness and strong alignment with\naccompanying beats, both stylistically and rhythmically.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}