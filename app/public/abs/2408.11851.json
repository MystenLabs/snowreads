{"id":"2408.11851","title":"SAGE-RT: Synthetic Alignment data Generation for Safety Evaluation and\n  Red Teaming","authors":"Anurakt Kumar, Divyanshu Kumar, Jatan Loya, Nitin Aravind Birur, Tanay\n  Baswa, Sahil Agarwal, Prashanth Harshangi","authorsParsed":[["Kumar","Anurakt",""],["Kumar","Divyanshu",""],["Loya","Jatan",""],["Birur","Nitin Aravind",""],["Baswa","Tanay",""],["Agarwal","Sahil",""],["Harshangi","Prashanth",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 08:38:31 GMT"}],"updateDate":"2024-08-23","timestamp":1723624711000,"abstract":"  We introduce Synthetic Alignment data Generation for Safety Evaluation and\nRed Teaming (SAGE-RT or SAGE) a novel pipeline for generating synthetic\nalignment and red-teaming data. Existing methods fall short in creating nuanced\nand diverse datasets, providing necessary control over the data generation and\nvalidation processes, or require large amount of manually generated seed data.\nSAGE addresses these limitations by using a detailed taxonomy to produce\nsafety-alignment and red-teaming data across a wide range of topics. We\ngenerated 51,000 diverse and in-depth prompt-response pairs, encompassing over\n1,500 topics of harmfulness and covering variations of the most frequent types\nof jailbreaking prompts faced by large language models (LLMs). We show that the\nred-teaming data generated through SAGE jailbreaks state-of-the-art LLMs in\nmore than 27 out of 32 sub-categories, and in more than 58 out of 279\nleaf-categories (sub-sub categories). The attack success rate for GPT-4o,\nGPT-3.5-turbo is 100% over the sub-categories of harmfulness. Our approach\navoids the pitfalls of synthetic safety-training data generation such as mode\ncollapse and lack of nuance in the generation pipeline by ensuring a detailed\ncoverage of harmful topics using iterative expansion of the topics and\nconditioning the outputs on the generated raw-text. This method can be used to\ngenerate red-teaming and alignment data for LLM Safety completely synthetically\nto make LLMs safer or for red-teaming the models over a diverse range of\ntopics.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}