{"id":"2407.10998","title":"Discrete Diffusion Language Model for Long Text Summarization","authors":"Do Huu Dat, Do Duc Anh, Anh Tuan Luu, Wray Buntine","authorsParsed":[["Dat","Do Huu",""],["Anh","Do Duc",""],["Luu","Anh Tuan",""],["Buntine","Wray",""]],"versions":[{"version":"v1","created":"Tue, 25 Jun 2024 09:55:22 GMT"}],"updateDate":"2024-07-17","timestamp":1719309322000,"abstract":"  While diffusion models excel at conditional generating high-quality images,\nprior works in discrete diffusion models were not evaluated on conditional\nlong-text generation. In this work, we address the limitations of prior\ndiscrete diffusion models for conditional long-text generation, particularly in\nlong sequence-to-sequence tasks such as abstractive summarization. Despite fast\ndecoding speeds compared to autoregressive methods, previous diffusion models\nfailed on the abstractive summarization task due to the incompatibility between\nthe backbone architectures and the random noising process. To overcome these\nchallenges, we introduce a novel semantic-aware noising process that enables\nTransformer backbones to handle long sequences effectively. Additionally, we\npropose CrossMamba, an adaptation of the Mamba model to the encoder-decoder\nparadigm, which integrates seamlessly with the random absorbing noising\nprocess. Our approaches achieve state-of-the-art performance on three benchmark\nsummarization datasets: Gigaword, CNN/DailyMail, and Arxiv, outperforming\nexisting discrete diffusion models on ROUGE metrics as well as possessing much\nfaster speed in inference compared to autoregressive models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8DHjxaCnf9IGQyM7XqTn8llnId6ScG7nImJOjwgBZzM","pdfSize":"469675","objectId":"0x7f45927ea2fae4fa7c582e6a5e941245e6a0ac722986067214bd5bf5b497bc6b","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
