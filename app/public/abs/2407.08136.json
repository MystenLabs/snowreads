{"id":"2407.08136","title":"EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable\n  Landmark Conditions","authors":"Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, Chenguang Ma","authorsParsed":[["Chen","Zhiyuan",""],["Cao","Jiajiong",""],["Chen","Zhiquan",""],["Li","Yuming",""],["Ma","Chenguang",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 02:26:51 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 02:41:51 GMT"}],"updateDate":"2024-07-15","timestamp":1720664811000,"abstract":"  The area of portrait image animation, propelled by audio input, has witnessed\nnotable progress in the generation of lifelike and dynamic portraits.\nConventional methods are limited to utilizing either audios or facial key\npoints to drive images into videos, while they can yield satisfactory results,\ncertain issues exist. For instance, methods driven solely by audios can be\nunstable at times due to the relatively weaker audio signal, while methods\ndriven exclusively by facial key points, although more stable in driving, can\nresult in unnatural outcomes due to the excessive control of key point\ninformation. In addressing the previously mentioned challenges, in this paper,\nwe introduce a novel approach which we named EchoMimic. EchoMimic is\nconcurrently trained using both audios and facial landmarks. Through the\nimplementation of a novel training strategy, EchoMimic is capable of generating\nportrait videos not only by audios and facial landmarks individually, but also\nby a combination of both audios and selected facial landmarks. EchoMimic has\nbeen comprehensively compared with alternative algorithms across various public\ndatasets and our collected dataset, showcasing superior performance in both\nquantitative and qualitative evaluations. Additional visualization and access\nto the source code can be located on the EchoMimic project page.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}