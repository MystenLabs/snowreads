{"id":"2408.02970","title":"EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and\n  Quantization","authors":"Zhaopeng Feng, Zijie Meng, Zuozhu Liu","authorsParsed":[["Feng","Zhaopeng",""],["Meng","Zijie",""],["Liu","Zuozhu",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 05:50:41 GMT"}],"updateDate":"2024-08-07","timestamp":1722923441000,"abstract":"  Large language models (LLMs) have attracted considerable attention in various\nfields for their cost-effective solutions to diverse challenges, especially\nwith advancements in instruction tuning and quantization. E-commerce, with its\ncomplex tasks and extensive product-user interactions, presents a promising\napplication area for LLMs. However, the domain-specific concepts and knowledge\ninherent in e-commerce pose significant challenges for adapting general LLMs.\nTo address this issue, we developed EC-Guide\n\\href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}, a comprehensive\ne-commerce guide for instruction tuning and quantization of LLMs. We also\nheuristically integrated Chain-of-Thought (CoT) during inference to enhance\narithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th\nplace in Track 5 at the Amazon KDD Cup'24\n\\href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}.\nAdditionally, our solution is model-agnostic, enabling effective scalability\nacross larger systems.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}