{"id":"2407.20456","title":"Learning to Provably Satisfy High Relative Degree Constraints for\n  Black-Box Systems","authors":"Jean-Baptiste Bouvier, Kartik Nagpal and Negar Mehr","authorsParsed":[["Bouvier","Jean-Baptiste",""],["Nagpal","Kartik",""],["Mehr","Negar",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 23:24:59 GMT"}],"updateDate":"2024-07-31","timestamp":1722295499000,"abstract":"  In this paper, we develop a method for learning a control policy guaranteed\nto satisfy an affine state constraint of high relative degree in closed loop\nwith a black-box system. Previous reinforcement learning (RL) approaches to\nsatisfy safety constraints either require access to the system model, or assume\ncontrol affine dynamics, or only discourage violations with reward shaping.\nOnly recently have these issues been addressed with POLICEd RL, which\nguarantees constraint satisfaction for black-box systems. However, this\nprevious work can only enforce constraints of relative degree 1. To address\nthis gap, we build a novel RL algorithm explicitly designed to enforce an\naffine state constraint of high relative degree in closed loop with a black-box\ncontrol system. Our key insight is to make the learned policy be affine around\nthe unsafe set and to use this affine region to dissipate the inertia of the\nhigh relative degree constraint. We prove that such policies guarantee\nconstraint satisfaction for deterministic systems while being agnostic to the\nchoice of the RL training algorithm. Our results demonstrate the capacity of\nour approach to enforce hard constraints in the Gym inverted pendulum and on a\nspace shuttle landing simulation.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Systems and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}