{"id":"2407.04938","title":"SAM-Med3D-MoE: Towards a Non-Forgetting Segment Anything Model via\n  Mixture of Experts for 3D Medical Image Segmentation","authors":"Guoan Wang and Jin Ye and Junlong Cheng and Tianbin Li and Zhaolin\n  Chen and Jianfei Cai and Junjun He and Bohan Zhuang","authorsParsed":[["Wang","Guoan",""],["Ye","Jin",""],["Cheng","Junlong",""],["Li","Tianbin",""],["Chen","Zhaolin",""],["Cai","Jianfei",""],["He","Junjun",""],["Zhuang","Bohan",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 03:03:45 GMT"}],"updateDate":"2024-07-15","timestamp":1720235025000,"abstract":"  Volumetric medical image segmentation is pivotal in enhancing disease\ndiagnosis, treatment planning, and advancing medical research. While existing\nvolumetric foundation models for medical image segmentation, such as SAM-Med3D\nand SegVol, have shown remarkable performance on general organs and tumors,\ntheir ability to segment certain categories in clinical downstream tasks\nremains limited. Supervised Finetuning (SFT) serves as an effective way to\nadapt such foundation models for task-specific downstream tasks but at the cost\nof degrading the general knowledge previously stored in the original foundation\nmodel.To address this, we propose SAM-Med3D-MoE, a novel framework that\nseamlessly integrates task-specific finetuned models with the foundational\nmodel, creating a unified model at minimal additional training expense for an\nextra gating network. This gating network, in conjunction with a selection\nstrategy, allows the unified model to achieve comparable performance of the\noriginal models in their respective tasks both general and specialized without\nupdating any parameters of them.Our comprehensive experiments demonstrate the\nefficacy of SAM-Med3D-MoE, with an average Dice performance increase from 53 to\n56.4 on 15 specific classes. It especially gets remarkable gains of 29.6, 8.5,\n11.2 on the spinal cord, esophagus, and right hip, respectively. Additionally,\nit achieves 48.9 Dice on the challenging SPPIN2023 Challenge, significantly\nsurpassing the general expert's performance of 32.3. We anticipate that\nSAM-Med3D-MoE can serve as a new framework for adapting the foundation model to\nspecific areas in medical image analysis. Codes and datasets will be publicly\navailable.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}