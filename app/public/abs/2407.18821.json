{"id":"2407.18821","title":"Deep Companion Learning: Enhancing Generalization Through Historical\n  Consistency","authors":"Ruizhao Zhu, Venkatesh Saligrama","authorsParsed":[["Zhu","Ruizhao",""],["Saligrama","Venkatesh",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 15:31:13 GMT"}],"updateDate":"2024-07-29","timestamp":1722007873000,"abstract":"  We propose Deep Companion Learning (DCL), a novel training method for Deep\nNeural Networks (DNNs) that enhances generalization by penalizing inconsistent\nmodel predictions compared to its historical performance. To achieve this, we\ntrain a deep-companion model (DCM), by using previous versions of the model to\nprovide forecasts on new inputs. This companion model deciphers a meaningful\nlatent semantic structure within the data, thereby providing targeted\nsupervision that encourages the primary model to address the scenarios it finds\nmost challenging. We validate our approach through both theoretical analysis\nand extensive experimentation, including ablation studies, on a variety of\nbenchmark datasets (CIFAR-100, Tiny-ImageNet, ImageNet-1K) using diverse\narchitectural models (ShuffleNetV2, ResNet, Vision Transformer, etc.),\ndemonstrating state-of-the-art performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}