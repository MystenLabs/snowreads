{"id":"2407.11449","title":"Controllable Contextualized Image Captioning: Directing the Visual\n  Narrative through User-Defined Highlights","authors":"Shunqi Mao, Chaoyi Zhang, Hang Su, Hwanjun Song, Igor Shalyminov,\n  Weidong Cai","authorsParsed":[["Mao","Shunqi",""],["Zhang","Chaoyi",""],["Su","Hang",""],["Song","Hwanjun",""],["Shalyminov","Igor",""],["Cai","Weidong",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 07:32:48 GMT"}],"updateDate":"2024-07-17","timestamp":1721115168000,"abstract":"  Contextualized Image Captioning (CIC) evolves traditional image captioning\ninto a more complex domain, necessitating the ability for multimodal reasoning.\nIt aims to generate image captions given specific contextual information. This\npaper further introduces a novel domain of Controllable Contextualized Image\nCaptioning (Ctrl-CIC). Unlike CIC, which solely relies on broad context,\nCtrl-CIC accentuates a user-defined highlight, compelling the model to tailor\ncaptions that resonate with the highlighted aspects of the context. We present\ntwo approaches, Prompting-based Controller (P-Ctrl) and Recalibration-based\nController (R-Ctrl), to generate focused captions. P-Ctrl conditions the model\ngeneration on highlight by prepending captions with highlight-driven prefixes,\nwhereas R-Ctrl tunes the model to selectively recalibrate the encoder\nembeddings for highlighted tokens. Additionally, we design a GPT-4V empowered\nevaluator to assess the quality of the controlled captions alongside standard\nassessment methods. Extensive experimental results demonstrate the efficient\nand effective controllability of our method, charting a new direction in\nachieving user-adaptive image captioning. Code is available at\nhttps://github.com/ShunqiM/Ctrl-CIC .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}