{"id":"2408.04217","title":"Simplifying Translations for Children: Iterative Simplification\n  Considering Age of Acquisition with LLMs","authors":"Masashi Oshika, Makoto Morishita, Tsutomu Hirao, Ryohei Sasano, Koichi\n  Takeda","authorsParsed":[["Oshika","Masashi",""],["Morishita","Makoto",""],["Hirao","Tsutomu",""],["Sasano","Ryohei",""],["Takeda","Koichi",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 04:57:36 GMT"}],"updateDate":"2024-08-09","timestamp":1723093056000,"abstract":"  In recent years, neural machine translation (NMT) has been widely used in\neveryday life. However, the current NMT lacks a mechanism to adjust the\ndifficulty level of translations to match the user's language level.\nAdditionally, due to the bias in the training data for NMT, translations of\nsimple source sentences are often produced with complex words. In particular,\nthis could pose a problem for children, who may not be able to understand the\nmeaning of the translations correctly. In this study, we propose a method that\nreplaces words with high Age of Acquisitions (AoA) in translations with simpler\nwords to match the translations to the user's level. We achieve this by using\nlarge language models (LLMs), providing a triple of a source sentence, a\ntranslation, and a target word to be replaced. We create a benchmark dataset\nusing back-translation on Simple English Wikipedia. The experimental results\nobtained from the dataset show that our method effectively replaces high-AoA\nwords with lower-AoA words and, moreover, can iteratively replace most of the\nhigh-AoA words while still maintaining high BLEU and COMET scores.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}