{"id":"2407.12828","title":"Why Does New Knowledge Create Messy Ripple Effects in LLMs?","authors":"Jiaxin Qin, Zixuan Zhang, Chi Han, Manling Li, Pengfei Yu, Heng Ji","authorsParsed":[["Qin","Jiaxin",""],["Zhang","Zixuan",""],["Han","Chi",""],["Li","Manling",""],["Yu","Pengfei",""],["Ji","Heng",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 14:33:44 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 01:33:56 GMT"}],"updateDate":"2024-07-22","timestamp":1719930824000,"abstract":"  Extensive previous research has focused on post-training knowledge editing\n(KE) for language models (LMs) to ensure that knowledge remains accurate and\nup-to-date. One desired property and open question in KE is to let edited LMs\ncorrectly handle ripple effects, where LM is expected to answer its logically\nrelated knowledge accurately. In this paper, we answer the question of why most\nKE methods still create messy ripple effects. We conduct extensive analysis and\nidentify a salient indicator, GradSim, that effectively reveals when and why\nupdated knowledge ripples in LMs. GradSim is computed by the cosine similarity\nbetween gradients of the original fact and its related knowledge. We observe a\nstrong positive correlation between ripple effect performance and GradSim\nacross different LMs, KE methods, and evaluation metrics. Further\ninvestigations into three counter-intuitive failure cases (Negation,\nOver-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures\nare often associated with very low GradSim. This finding validates that GradSim\nis an effective indicator of when knowledge ripples in LMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}