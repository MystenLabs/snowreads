{"id":"2408.14770","title":"Text-guided Foundation Model Adaptation for Long-Tailed Medical Image\n  Classification","authors":"Sirui Li, Li Lin, Yijin Huang, Pujin Cheng, Xiaoying Tang","authorsParsed":[["Li","Sirui",""],["Lin","Li",""],["Huang","Yijin",""],["Cheng","Pujin",""],["Tang","Xiaoying",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 04:18:18 GMT"}],"updateDate":"2024-08-28","timestamp":1724732298000,"abstract":"  In medical contexts, the imbalanced data distribution in long-tailed\ndatasets, due to scarce labels for rare diseases, greatly impairs the\ndiagnostic accuracy of deep learning models. Recent multimodal text-image\nsupervised foundation models offer new solutions to data scarcity through\neffective representation learning. However, their limited medical-specific\npretraining hinders their performance in medical image classification relative\nto natural images. To address this issue, we propose a novel Text-guided\nFoundation model Adaptation for Long-Tailed medical image classification\n(TFA-LT). We adopt a two-stage training strategy, integrating representations\nfrom the foundation model using just two linear adapters and a single ensembler\nfor balanced outcomes. Experimental results on two long-tailed medical image\ndatasets validate the simplicity, lightweight and efficiency of our approach:\nrequiring only 6.1% GPU memory usage of the current best-performing algorithm,\nour method achieves an accuracy improvement of up to 27.1%, highlighting the\nsubstantial potential of foundation model adaptation in this area.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}