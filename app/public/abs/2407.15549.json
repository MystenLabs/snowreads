{"id":"2407.15549","title":"Latent Adversarial Training Improves Robustness to Persistent Harmful\n  Behaviors in LLMs","authors":"Abhay Sheshadri, Aidan Ewart, Phillip Guo, Aengus Lynch, Cindy Wu,\n  Vivek Hebbar, Henry Sleight, Asa Cooper Stickland, Ethan Perez, Dylan\n  Hadfield-Menell, Stephen Casper","authorsParsed":[["Sheshadri","Abhay",""],["Ewart","Aidan",""],["Guo","Phillip",""],["Lynch","Aengus",""],["Wu","Cindy",""],["Hebbar","Vivek",""],["Sleight","Henry",""],["Stickland","Asa Cooper",""],["Perez","Ethan",""],["Hadfield-Menell","Dylan",""],["Casper","Stephen",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 11:19:14 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 23:22:40 GMT"}],"updateDate":"2024-08-23","timestamp":1721647154000,"abstract":"  Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of 'jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Xq6p_hUO_10oJO3FZzHhvBB9dGgrHdVnxcUaKrHRsUA","pdfSize":"803845","objectId":"0x711fee7903e8e4f229c6160a9a05e138ae716542028e2e7bcfeae1d0760fc7b3","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
