{"id":"2407.18841","title":"QT-TDM: Planning with Transformer Dynamics Model and Autoregressive\n  Q-Learning","authors":"Mostafa Kotb, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter","authorsParsed":[["Kotb","Mostafa",""],["Weber","Cornelius",""],["Hafez","Muhammad Burhan",""],["Wermter","Stefan",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 16:05:26 GMT"}],"updateDate":"2024-07-29","timestamp":1722009926000,"abstract":"  Inspired by the success of the Transformer architecture in natural language\nprocessing and computer vision, we investigate the use of Transformers in\nReinforcement Learning (RL), specifically in modeling the environment's\ndynamics using Transformer Dynamics Models (TDMs). We evaluate the capabilities\nof TDMs for continuous control in real-time planning scenarios with Model\nPredictive Control (MPC). While Transformers excel in long-horizon prediction,\ntheir tokenization mechanism and autoregressive nature lead to costly planning\nover long horizons, especially as the environment's dimensionality increases.\nTo alleviate this issue, we use a TDM for short-term planning, and learn an\nautoregressive discrete Q-function using a separate Q-Transformer (QT) model to\nestimate a long-term return beyond the short-horizon planning. Our proposed\nmethod, QT-TDM, integrates the robust predictive capabilities of Transformers\nas dynamics models with the efficacy of a model-free Q-Transformer to mitigate\nthe computational burden associated with real-time planning. Experiments in\ndiverse state-based continuous control tasks show that QT-TDM is superior in\nperformance and sample efficiency compared to existing Transformer-based RL\nmodels while achieving fast and computationally efficient inference.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}