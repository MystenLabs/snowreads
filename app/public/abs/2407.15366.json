{"id":"2407.15366","title":"Walking in Others' Shoes: How Perspective-Taking Guides Large Language\n  Models in Reducing Toxicity and Bias","authors":"Rongwu Xu, Zi'an Zhou, Tianwei Zhang, Zehan Qi, Su Yao, Ke Xu, Wei Xu,\n  Han Qiu","authorsParsed":[["Xu","Rongwu",""],["Zhou","Zi'an",""],["Zhang","Tianwei",""],["Qi","Zehan",""],["Yao","Su",""],["Xu","Ke",""],["Xu","Wei",""],["Qiu","Han",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 04:25:01 GMT"}],"updateDate":"2024-07-23","timestamp":1721622301000,"abstract":"  The common toxicity and societal bias in contents generated by large language\nmodels (LLMs) necessitate strategies to reduce harm. Present solutions often\ndemand white-box access to the model or substantial training, which is\nimpractical for cutting-edge commercial LLMs. Moreover, prevailing prompting\nmethods depend on external tool feedback and fail to simultaneously lessen\ntoxicity and bias. Motivated by social psychology principles, we propose a\nnovel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that\ninspires LLMs to integrate diverse human perspectives and self-regulate their\nresponses. This self-correction mechanism can significantly diminish toxicity\n(up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations\nand ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and\nthree open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less\nharmful responses, outperforming five strong baselines.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}