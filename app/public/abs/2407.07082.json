{"id":"2407.07082","title":"Can Learned Optimization Make Reinforcement Learning Less Difficult?","authors":"Alexander David Goldie, Chris Lu, Matthew Thomas Jackson, Shimon\n  Whiteson, Jakob Nicolaus Foerster","authorsParsed":[["Goldie","Alexander David",""],["Lu","Chris",""],["Jackson","Matthew Thomas",""],["Whiteson","Shimon",""],["Foerster","Jakob Nicolaus",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 17:55:23 GMT"}],"updateDate":"2024-07-10","timestamp":1720547723000,"abstract":"  While reinforcement learning (RL) holds great potential for decision making\nin the real world, it suffers from a number of unique difficulties which often\nneed specific consideration. In particular: it is highly non-stationary;\nsuffers from high degrees of plasticity loss; and requires exploration to\nprevent premature convergence to local optima and maximize return. In this\npaper, we consider whether learned optimization can help overcome these\nproblems. Our method, Learned Optimization for Plasticity, Exploration and\nNon-stationarity (OPEN), meta-learns an update rule whose input features and\noutput structure are informed by previously proposed solutions to these\ndifficulties. We show that our parameterization is flexible enough to enable\nmeta-learning in diverse learning contexts, including the ability to use\nstochasticity for exploration. Our experiments demonstrate that when\nmeta-trained on single and small sets of environments, OPEN outperforms or\nequals traditionally used optimizers. Furthermore, OPEN shows strong\ngeneralization across a distribution of environments and a range of agent\narchitectures.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}