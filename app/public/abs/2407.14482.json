{"id":"2407.14482","title":"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities","authors":"Peng Xu, Wei Ping, Xianchao Wu, Chejian Xu, Zihan Liu, Mohammad\n  Shoeybi, Bryan Catanzaro","authorsParsed":[["Xu","Peng",""],["Ping","Wei",""],["Wu","Xianchao",""],["Xu","Chejian",""],["Liu","Zihan",""],["Shoeybi","Mohammad",""],["Catanzaro","Bryan",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:35:47 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 06:19:07 GMT"}],"updateDate":"2024-09-10","timestamp":1721410547000,"abstract":"  In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo) in long-context understanding and\nretrieval-augmented generation (RAG) capabilities. These two capabilities are\nessential for LLMs to process large volumes of information that cannot fit into\na single prompt and are complementary to each other, depending on the\ndownstream tasks and computational budgets. We present a detailed continued\ntraining recipe to extend the context window of Llama3-70B-base from 8K to 128K\ntokens, along with a three-stage instruction tuning process to enhance the\nmodel's instruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\noutperforms most existing state-of-the-art models, including\nGPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on\nultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only\na 4K context window, showing the strong long context capability across varying\nsequence lengths. We further provide extensive comparisons between direct\nlong-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution\nusing the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B\nand Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To\nadvance research in this field, we open-sourced the model weights, training\ndata, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"tBjUXD7Lsj82mWOZuyglNRhWQImwFuGoNSk4ARwPxOA","pdfSize":"420147","objectId":"0x87705321775de4a83417240af8a6d310205ffb166f432d16e8f8593e75651b5c","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
