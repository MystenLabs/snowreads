{"id":"2407.13044","title":"DropKAN: Regularizing KANs by masking post-activations","authors":"Mohammed Ghaith Altarabichi","authorsParsed":[["Altarabichi","Mohammed Ghaith",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 22:48:47 GMT"},{"version":"v2","created":"Mon, 22 Jul 2024 17:12:39 GMT"},{"version":"v3","created":"Fri, 16 Aug 2024 09:06:08 GMT"},{"version":"v4","created":"Tue, 20 Aug 2024 12:16:13 GMT"}],"updateDate":"2024-08-21","timestamp":1721256527000,"abstract":"  We propose DropKAN (Dropout Kolmogorov-Arnold Networks) a regularization\nmethod that prevents co-adaptation of activation function weights in\nKolmogorov-Arnold Networks (KANs). DropKAN functions by embedding the drop mask\ndirectly within the KAN layer, randomly masking the outputs of some activations\nwithin the KANs' computation graph. We show that this simple procedure that\nrequire minimal coding effort has a regularizing effect and consistently lead\nto better generalization of KANs. We analyze the adaptation of the standard\nDropout with KANs and demonstrate that Dropout applied to KANs' neurons can\nlead to unpredictable behavior in the feedforward pass. We carry an empirical\nstudy with real world Machine Learning datasets to validate our findings. Our\nresults suggest that DropKAN is consistently a better alternative to using\nstandard Dropout with KANs, and improves the generalization performance of\nKANs. Our implementation of DropKAN is available at:\n\\url{https://github.com/Ghaith81/dropkan}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kFQ7mwyBjiB6B2t8nVpqsDenJc2CJKkq0NlmKeGzqLg","pdfSize":"365195"}
