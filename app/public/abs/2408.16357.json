{"id":"2408.16357","title":"Law of Vision Representation in MLLMs","authors":"Shijia Yang, Bohan Zhai, Quanzeng You, Jianbo Yuan, Hongxia Yang,\n  Chenfeng Xu","authorsParsed":[["Yang","Shijia",""],["Zhai","Bohan",""],["You","Quanzeng",""],["Yuan","Jianbo",""],["Yang","Hongxia",""],["Xu","Chenfeng",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 08:56:48 GMT"}],"updateDate":"2024-08-30","timestamp":1724921808000,"abstract":"  We present the \"Law of Vision Representation\" in multimodal large language\nmodels (MLLMs). It reveals a strong correlation between the combination of\ncross-modal alignment, correspondence in vision representation, and MLLM\nperformance. We quantify the two factors using the cross-modal Alignment and\nCorrespondence score (AC score). Through extensive experiments involving\nthirteen different vision representation settings and evaluations across eight\nbenchmarks, we find that the AC score is linearly correlated to model\nperformance. By leveraging this relationship, we are able to identify and train\nthe optimal vision representation only, which does not require finetuning the\nlanguage model every time, resulting in a 99.7% reduction in computational\ncost.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}