{"id":"2408.02165","title":"SelfBC: Self Behavior Cloning for Offline Reinforcement Learning","authors":"Shirong Liu, Chenjia Bai, Zixian Guo, Hao Zhang, Gaurav Sharma and\n  Yang Liu","authorsParsed":[["Liu","Shirong",""],["Bai","Chenjia",""],["Guo","Zixian",""],["Zhang","Hao",""],["Sharma","Gaurav",""],["Liu","Yang",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 23:23:48 GMT"}],"updateDate":"2024-08-06","timestamp":1722813828000,"abstract":"  Policy constraint methods in offline reinforcement learning employ additional\nregularization techniques to constrain the discrepancy between the learned\npolicy and the offline dataset. However, these methods tend to result in overly\nconservative policies that resemble the behavior policy, thus limiting their\nperformance. We investigate this limitation and attribute it to the static\nnature of traditional constraints. In this paper, we propose a novel dynamic\npolicy constraint that restricts the learned policy on the samples generated by\nthe exponential moving average of previously learned policies. By integrating\nthis self-constraint mechanism into off-policy methods, our method facilitates\nthe learning of non-conservative policies while avoiding policy collapse in the\noffline setting. Theoretical results show that our approach results in a nearly\nmonotonically improved reference policy. Extensive experiments on the D4RL\nMuJoCo domain demonstrate that our proposed method achieves state-of-the-art\nperformance among the policy constraint methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}