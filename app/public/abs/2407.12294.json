{"id":"2407.12294","title":"VEON: Vocabulary-Enhanced Occupancy Prediction","authors":"Jilai Zheng, Pin Tang, Zhongdao Wang, Guoqing Wang, Xiangxuan Ren,\n  Bailan Feng, Chao Ma","authorsParsed":[["Zheng","Jilai",""],["Tang","Pin",""],["Wang","Zhongdao",""],["Wang","Guoqing",""],["Ren","Xiangxuan",""],["Feng","Bailan",""],["Ma","Chao",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 03:26:50 GMT"}],"updateDate":"2024-07-18","timestamp":1721186810000,"abstract":"  Perceiving the world as 3D occupancy supports embodied agents to avoid\ncollision with any types of obstacle. While open-vocabulary image understanding\nhas prospered recently, how to bind the predicted 3D occupancy grids with\nopen-world semantics still remains under-explored due to limited open-world\nannotations. Hence, instead of building our model from scratch, we try to blend\n2D foundation models, specifically a depth model MiDaS and a semantic model\nCLIP, to lift the semantics to 3D space, thus fulfilling 3D occupancy. However,\nbuilding upon these foundation models is not trivial. First, the MiDaS faces\nthe depth ambiguity problem, i.e., it only produces relative depth but fails to\nestimate bin depth for feature lifting. Second, the CLIP image features lack\nhigh-resolution pixel-level information, which limits the 3D occupancy\naccuracy. Third, open vocabulary is often trapped by the long-tail problem. To\naddress these issues, we propose VEON for Vocabulary-Enhanced Occupancy\npredictioN by not only assembling but also adapting these foundation models. We\nfirst equip MiDaS with a Zoedepth head and low-rank adaptation (LoRA) for\nrelative-metric-bin depth transformation while reserving beneficial depth\nprior. Then, a lightweight side adaptor network is attached to the CLIP vision\nencoder to generate high-resolution features for fine-grained 3D occupancy\nprediction. Moreover, we design a class reweighting strategy to give priority\nto the tail classes. With only 46M trainable parameters and zero manual\nsemantic labels, VEON achieves 15.14 mIoU on Occ3D-nuScenes, and shows the\ncapability of recognizing objects with open-vocabulary categories, meaning that\nour VEON is label-efficient, parameter-efficient, and precise enough.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}