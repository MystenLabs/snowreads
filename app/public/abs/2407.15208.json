{"id":"2407.15208","title":"Flow as the Cross-Domain Manipulation Interface","authors":"Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein,\n  Manuela Veloso, Shuran Song","authorsParsed":[["Xu","Mengda",""],["Xu","Zhenjia",""],["Xu","Yinghao",""],["Chi","Cheng",""],["Wetzstein","Gordon",""],["Veloso","Manuela",""],["Song","Shuran",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 16:15:02 GMT"}],"updateDate":"2024-07-23","timestamp":1721578502000,"abstract":"  We present Im2Flow2Act, a scalable learning framework that enables robots to\nacquire manipulation skills from diverse data sources. The key idea behind\nIm2Flow2Act is to use object flow as the manipulation interface, bridging\ndomain gaps between different embodiments (i.e., human and robot) and training\nenvironments (i.e., real-world and simulated). Im2Flow2Act comprises two\ncomponents: a flow generation network and a flow-conditioned policy. The flow\ngeneration network, trained on human demonstration videos, generates object\nflow from the initial scene image, conditioned on the task description. The\nflow-conditioned policy, trained on simulated robot play data, maps the\ngenerated object flow to robot actions to realize the desired object movements.\nBy using flow as input, this policy can be directly deployed in the real world\nwith a minimal sim-to-real gap. By leveraging real-world human videos and\nsimulated robot play data, we bypass the challenges of teleoperating physical\nrobots in the real world, resulting in a scalable system for diverse tasks. We\ndemonstrate Im2Flow2Act's capabilities in a variety of real-world tasks,\nincluding the manipulation of rigid, articulated, and deformable objects.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}