{"id":"2408.10334","title":"A Disguised Wolf Is More Harmful Than a Toothless Tiger: Adaptive\n  Malicious Code Injection Backdoor Attack Leveraging User Behavior as Triggers","authors":"Shangxi Wu and Jitao Sang","authorsParsed":[["Wu","Shangxi",""],["Sang","Jitao",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 18:18:04 GMT"}],"updateDate":"2024-08-21","timestamp":1724091484000,"abstract":"  In recent years, large language models (LLMs) have made significant progress\nin the field of code generation. However, as more and more users rely on these\nmodels for software development, the security risks associated with code\ngeneration models have become increasingly significant. Studies have shown that\ntraditional deep learning robustness issues also negatively impact the field of\ncode generation. In this paper, we first present the game-theoretic model that\nfocuses on security issues in code generation scenarios. This framework\noutlines possible scenarios and patterns where attackers could spread malicious\ncode models to create security threats. We also pointed out for the first time\nthat the attackers can use backdoor attacks to dynamically adjust the timing of\nmalicious code injection, which will release varying degrees of malicious code\ndepending on the skill level of the user. Through extensive experiments on\nleading code generation models, we validate our proposed game-theoretic model\nand highlight the significant threats that these new attack scenarios pose to\nthe safe use of code models.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}