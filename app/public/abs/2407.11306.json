{"id":"2407.11306","title":"PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient\n  Vision Transformer","authors":"Pierre-David Letourneau, Manish Kumar Singh, Hsin-Pai Cheng, Shizhong\n  Han, Yunxiao Shi, Dalton Jones, Matthew Harper Langston, Hong Cai, Fatih\n  Porikli","authorsParsed":[["Letourneau","Pierre-David",""],["Singh","Manish Kumar",""],["Cheng","Hsin-Pai",""],["Han","Shizhong",""],["Shi","Yunxiao",""],["Jones","Dalton",""],["Langston","Matthew Harper",""],["Cai","Hong",""],["Porikli","Fatih",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 01:45:44 GMT"}],"updateDate":"2024-07-17","timestamp":1721094344000,"abstract":"  We present Polynomial Attention Drop-in Replacement (PADRe), a novel and\nunifying framework designed to replace the conventional self-attention\nmechanism in transformer models. Notably, several recent alternative attention\nmechanisms, including Hyena, Mamba, SimA, Conv2Former, and Castling-ViT, can be\nviewed as specific instances of our PADRe framework. PADRe leverages polynomial\nfunctions and draws upon established results from approximation theory,\nenhancing computational efficiency without compromising accuracy. PADRe's key\ncomponents include multiplicative nonlinearities, which we implement using\nstraightforward, hardware-friendly operations such as Hadamard products,\nincurring only linear computational and memory costs. PADRe further avoids the\nneed for using complex functions such as Softmax, yet it maintains comparable\nor superior accuracy compared to traditional self-attention. We assess the\neffectiveness of PADRe as a drop-in replacement for self-attention across\ndiverse computer vision tasks. These tasks include image classification,\nimage-based 2D object detection, and 3D point cloud object detection. Empirical\nresults demonstrate that PADRe runs significantly faster than the conventional\nself-attention (11x ~ 43x faster on server GPU and mobile NPU) while\nmaintaining similar accuracy when substituting self-attention in the\ntransformer models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}