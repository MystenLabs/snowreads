{"id":"2408.12071","title":"Multi-Task Curriculum Graph Contrastive Learning with Clustering Entropy\n  Guidance","authors":"Chusheng Zeng, Bocheng Wang, Jinghui Yuan, Rong Wang, Mulin Chen","authorsParsed":[["Zeng","Chusheng",""],["Wang","Bocheng",""],["Yuan","Jinghui",""],["Wang","Rong",""],["Chen","Mulin",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 02:18:47 GMT"}],"updateDate":"2024-08-23","timestamp":1724293127000,"abstract":"  Recent advances in unsupervised deep graph clustering have been significantly\npromoted by contrastive learning. Despite the strides, most graph contrastive\nlearning models face challenges: 1) graph augmentation is used to improve\nlearning diversity, but commonly used random augmentation methods may destroy\ninherent semantics and cause noise; 2) the fixed positive and negative sample\nselection strategy is limited to deal with complex real data, thereby impeding\nthe model's capability to capture fine-grained patterns and relationships. To\nreduce these problems, we propose the Clustering-guided Curriculum Graph\ncontrastive Learning (CCGL) framework. CCGL uses clustering entropy as the\nguidance of the following graph augmentation and contrastive learning.\nSpecifically, according to the clustering entropy, the intra-class edges and\nimportant features are emphasized in augmentation. Then, a multi-task\ncurriculum learning scheme is proposed, which employs the clustering guidance\nto shift the focus from the discrimination task to the clustering task. In this\nway, the sample selection strategy of contrastive learning can be adjusted\nadaptively from early to late stage, which enhances the model's flexibility for\ncomplex data structure. Experimental results demonstrate that CCGL has achieved\nexcellent performance compared to state-of-the-art competitors.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}