{"id":"2408.10680","title":"Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on\n  Whisper","authors":"Tianyi Xu, Kaixun Huang, Pengcheng Guo, Yu Zhou, Longtao Huang, Hui\n  Xue, Lei Xie","authorsParsed":[["Xu","Tianyi",""],["Huang","Kaixun",""],["Guo","Pengcheng",""],["Zhou","Yu",""],["Huang","Longtao",""],["Xue","Hui",""],["Xie","Lei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 09:31:59 GMT"}],"updateDate":"2024-08-21","timestamp":1724146319000,"abstract":"  Pre-trained multilingual speech foundation models, like Whisper, have shown\nimpressive performance across different languages. However, adapting these\nmodels to new or specific languages is computationally extensive and faces\ncatastrophic forgetting problems. Addressing these issues, our study\ninvestigates strategies to enhance the model on new languages in the absence of\noriginal training data, while also preserving the established performance on\nthe original languages. Specifically, we first compare various LoRA-based\nmethods to find out their vulnerability to forgetting. To mitigate this issue,\nwe propose to leverage the LoRA parameters from the original model for\napproximate orthogonal gradient descent on the new samples. Additionally, we\nalso introduce a learnable rank coefficient to allocate trainable parameters\nfor more efficient training. Our experiments with a Chinese Whisper model (for\nUyghur and Tibetan) yield better results with a more compact parameter set.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YkXRP45u1GsX8nrBULH-XFjBLUzn9r8xoAvX7WOrwhs","pdfSize":"410194"}
