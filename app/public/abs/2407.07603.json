{"id":"2407.07603","title":"iiANET: Inception Inspired Attention Hybrid Network for efficient\n  Long-Range Dependency","authors":"Haruna Yunusa, Qin Shiyin, Abdulrahman Hamman Adama Chukkol, Isah\n  Bello, Adamu Lawan","authorsParsed":[["Yunusa","Haruna",""],["Shiyin","Qin",""],["Chukkol","Abdulrahman Hamman Adama",""],["Bello","Isah",""],["Lawan","Adamu",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 12:39:02 GMT"}],"updateDate":"2024-07-11","timestamp":1720615142000,"abstract":"  The recent emergence of hybrid models has introduced another transformative\napproach to solving computer vision tasks, slowly shifting away from\nconventional CNN (Convolutional Neural Network) and ViT (Vision Transformer).\nHowever, not enough effort has been made to efficiently combine these two\napproaches to improve capturing long-range dependencies prevalent in complex\nimages. In this paper, we introduce iiANET (Inception Inspired Attention\nNetwork), an efficient hybrid model designed to capture long-range dependencies\nin complex images. The fundamental building block, iiABlock, integrates global\n2D-MHSA (Multi-Head Self-Attention) with Registers, MBConv2 (MobileNetV2-based\nconvolution), and dilated convolution in parallel, enabling the model to\nadeptly leverage self-attention for capturing long-range dependencies while\nutilizing MBConv2 for effective local-detail extraction and dilated convolution\nfor efficiently expanding the kernel receptive field to capture more contextual\ninformation. Lastly, we serially integrate an ECANET (Efficient Channel\nAttention Network) at the end of each iiABlock to calibrate channel-wise\nattention for enhanced model performance. Extensive qualitative and\nquantitative comparative evaluation on various benchmarks demonstrates improved\nperformance over some state-of-the-art models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}