{"id":"2408.09474","title":"Image-Based Geolocation Using Large Vision-Language Models","authors":"Yi Liu, Junchen Ding, Gelei Deng, Yuekang Li, Tianwei Zhang, Weisong\n  Sun, Yaowen Zheng, Jingquan Ge and Yang Liu","authorsParsed":[["Liu","Yi",""],["Ding","Junchen",""],["Deng","Gelei",""],["Li","Yuekang",""],["Zhang","Tianwei",""],["Sun","Weisong",""],["Zheng","Yaowen",""],["Ge","Jingquan",""],["Liu","Yang",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 13:39:43 GMT"}],"updateDate":"2024-08-20","timestamp":1723988383000,"abstract":"  Geolocation is now a vital aspect of modern life, offering numerous benefits\nbut also presenting serious privacy concerns. The advent of large\nvision-language models (LVLMs) with advanced image-processing capabilities\nintroduces new risks, as these models can inadvertently reveal sensitive\ngeolocation information. This paper presents the first in-depth study analyzing\nthe challenges posed by traditional deep learning and LVLM-based geolocation\nmethods. Our findings reveal that LVLMs can accurately determine geolocations\nfrom images, even without explicit geographic training.\n  To address these challenges, we introduce \\tool{}, an innovative framework\nthat significantly enhances image-based geolocation accuracy. \\tool{} employs a\nsystematic chain-of-thought (CoT) approach, mimicking human geoguessing\nstrategies by carefully analyzing visual and contextual cues such as vehicle\ntypes, architectural styles, natural landscapes, and cultural elements.\nExtensive testing on a dataset of 50,000 ground-truth data points shows that\n\\tool{} outperforms both traditional models and human benchmarks in accuracy.\nIt achieves an impressive average score of 4550.5 in the GeoGuessr game, with\nan 85.37\\% win rate, and delivers highly precise geolocation predictions, with\nthe closest distances as accurate as 0.3 km. Furthermore, our study highlights\nissues related to dataset integrity, leading to the creation of a more robust\ndataset and a refined framework that leverages LVLMs' cognitive capabilities to\nimprove geolocation precision. These findings underscore \\tool{}'s superior\nability to interpret complex visual data, the urgent need to address emerging\nsecurity vulnerabilities posed by LVLMs, and the importance of responsible AI\ndevelopment to ensure user privacy protection.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CpA5f9c2FIjAvBfFA-H9p5xVK7wdW0RCkyuXgHoo858","pdfSize":"8015198"}
