{"id":"2408.15011","title":"Pre-training Everywhere: Parameter-Efficient Fine-Tuning for Medical\n  Image Analysis via Target Parameter Pre-training","authors":"Xingliang Lei, Yiwen Ye, Ziyang Chen, Minglei Shu, Yong Xia","authorsParsed":[["Lei","Xingliang",""],["Ye","Yiwen",""],["Chen","Ziyang",""],["Shu","Minglei",""],["Xia","Yong",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 12:48:46 GMT"}],"updateDate":"2024-08-28","timestamp":1724762926000,"abstract":"  Parameter-efficient fine-tuning (PEFT) techniques have emerged to address\nissues of overfitting and high computational costs associated with fully\nfine-tuning in the paradigm of self-supervised learning. Mainstream methods\nbased on PEFT involve adding a few trainable parameters while keeping the\npre-trained parameters of the backbone fixed. These methods achieve\ncomparative, and often superior, performance to fully fine-tuning,\ndemonstrating the powerful representation ability of the pre-trained backbone.\nDespite its success, these methods typically ignore the initialization of the\nnew parameters, often relying solely on random initialization. We argue that if\npre-training is significantly beneficial, it should be applied to all\nparameters requiring representational capacity. Motivated by this insight, we\npropose a simple yet effective fine-tuning framework based on Target Parameter\nPre-training (TPP). The target parameters refer to the new parameters\nintroduced during fine-tuning. TPP includes an additional stage before PEFT to\npre-train these target parameters. During this stage, the pre-trained backbone\nparameters are frozen, and only the target parameters are trainable. A defined\npre-text task is used to encourage the target parameters to learn specific\nrepresentations of downstream data. When PEFT is subsequently employed, the\npre-trained target parameters are loaded to enhance fine-tuning efficiency. The\nproposed TPP framework is versatile, allowing for the integration of various\npretext tasks for pre-training and supporting different PEFT methods as\nbackbones. We evaluated the fine-tining performance of our method using five\npublic datasets, including three modalities and two task types. The results\ndemonstrate that the proposed TPP can be easily integrated into existing PEFT\nmethods, significantly improving performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}