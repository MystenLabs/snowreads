{"id":"2407.02182","title":"Occlusion-Aware Seamless Segmentation","authors":"Yihong Cao, Jiaming Zhang, Hao Shi, Kunyu Peng, Yuhongxuan Zhang, Hui\n  Zhang, Rainer Stiefelhagen, Kailun Yang","authorsParsed":[["Cao","Yihong",""],["Zhang","Jiaming",""],["Shi","Hao",""],["Peng","Kunyu",""],["Zhang","Yuhongxuan",""],["Zhang","Hui",""],["Stiefelhagen","Rainer",""],["Yang","Kailun",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 11:41:12 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 07:55:52 GMT"}],"updateDate":"2024-07-18","timestamp":1719920472000,"abstract":"  Panoramic images can broaden the Field of View (FoV), occlusion-aware\nprediction can deepen the understanding of the scene, and domain adaptation can\ntransfer across viewing domains. In this work, we introduce a novel task,\nOcclusion-Aware Seamless Segmentation (OASS), which simultaneously tackles all\nthese three challenges. For benchmarking OASS, we establish a new\nhuman-annotated dataset for Blending Panoramic Amodal Seamless Segmentation,\ni.e., BlendPASS. Besides, we propose the first solution UnmaskFormer, aiming at\nunmasking the narrow FoV, occlusions, and domain gaps all at once.\nSpecifically, UnmaskFormer includes the crucial designs of Unmasking Attention\n(UA) and Amodal-oriented Mix (AoMix). Our method achieves state-of-the-art\nperformance on the BlendPASS dataset, reaching a remarkable mAPQ of 26.58% and\nmIoU of 43.66%. On public panoramic semantic segmentation datasets, i.e.,\nSynPASS and DensePASS, our method outperforms previous methods and obtains\n45.34% and 48.08% in mIoU, respectively. The fresh BlendPASS dataset and our\nsource code are available at https://github.com/yihong-97/OASS.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}