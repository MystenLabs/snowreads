{"id":"2408.14785","title":"Unsupervised-to-Online Reinforcement Learning","authors":"Junsu Kim, Seohong Park, Sergey Levine","authorsParsed":[["Kim","Junsu",""],["Park","Seohong",""],["Levine","Sergey",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 05:23:45 GMT"}],"updateDate":"2024-08-28","timestamp":1724736225000,"abstract":"  Offline-to-online reinforcement learning (RL), a framework that trains a\npolicy with offline RL and then further fine-tunes it with online RL, has been\nconsidered a promising recipe for data-driven decision-making. While sensible,\nthis framework has drawbacks: it requires domain-specific offline RL\npre-training for each task, and is often brittle in practice. In this work, we\npropose unsupervised-to-online RL (U2O RL), which replaces domain-specific\nsupervised offline RL with unsupervised offline RL, as a better alternative to\noffline-to-online RL. U2O RL not only enables reusing a single pre-trained\nmodel for multiple downstream tasks, but also learns better representations,\nwhich often result in even better performance and stability than supervised\noffline-to-online RL. To instantiate U2O RL in practice, we propose a general\nrecipe for U2O RL to bridge task-agnostic unsupervised offline skill-based\npolicy pre-training and supervised online fine-tuning. Throughout our\nexperiments in nine state-based and pixel-based environments, we empirically\ndemonstrate that U2O RL achieves strong performance that matches or even\noutperforms previous offline-to-online RL approaches, while being able to reuse\na single pre-trained model for a number of different downstream tasks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}