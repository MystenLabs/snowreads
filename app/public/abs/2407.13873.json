{"id":"2407.13873","title":"Keypoint Aware Masked Image Modelling","authors":"Madhava Krishna, A V Subramanyam","authorsParsed":[["Krishna","Madhava",""],["Subramanyam","A V",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 19:41:46 GMT"}],"updateDate":"2024-07-22","timestamp":1721331706000,"abstract":"  SimMIM is a widely used method for pretraining vision transformers using\nmasked image modeling. However, despite its success in fine-tuning performance,\nit has been shown to perform sub-optimally when used for linear probing. We\npropose an efficient patch-wise weighting derived from keypoint features which\ncaptures the local information and provides better context during SimMIM's\nreconstruction phase. Our method, KAMIM, improves the top-1 linear probing\naccuracy from 16.12% to 33.97%, and finetuning accuracy from 76.78% to 77.3%\nwhen tested on the ImageNet-1K dataset with a ViT-B when trained for the same\nnumber of epochs. We conduct extensive testing on different datasets, keypoint\nextractors, and model architectures and observe that patch-wise weighting\naugments linear probing performance for larger pretraining datasets. We also\nanalyze the learned representations of a ViT-B trained using KAMIM and observe\nthat they behave similar to contrastive learning with regard to its behavior,\nwith longer attention distances and homogenous self-attention across layers.\nOur code is publicly available at https://github.com/madhava20217/KAMIM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}