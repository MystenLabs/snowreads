{"id":"2407.06120","title":"Sketchy Moment Matching: Toward Fast and Provable Data Selection for\n  Finetuning","authors":"Yijun Dong, Hoang Phan, Xiang Pan, Qi Lei","authorsParsed":[["Dong","Yijun",""],["Phan","Hoang",""],["Pan","Xiang",""],["Lei","Qi",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 16:57:26 GMT"}],"updateDate":"2024-07-09","timestamp":1720457846000,"abstract":"  We revisit data selection in a modern context of finetuning from a\nfundamental perspective. Extending the classical wisdom of variance\nminimization in low dimensions to high-dimensional finetuning, our\ngeneralization analysis unveils the importance of additionally reducing bias\ninduced by low-rank approximation. Inspired by the variance-bias tradeoff in\nhigh dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a\nscalable data selection scheme with two stages. (i) First, the bias is\ncontrolled using gradient sketching that explores the finetuning parameter\nspace for an informative low-dimensional subspace $\\mathcal{S}$; (ii) then the\nvariance is reduced over $\\mathcal{S}$ via moment matching between the original\nand selected datasets. Theoretically, we show that gradient sketching is fast\nand provably accurate: selecting $n$ samples by reducing variance over\n$\\mathcal{S}$ preserves the fast-rate generalization $O(\\dim(\\mathcal{S})/n)$,\nindependent of the parameter dimension. Empirically, we concretize the\nvariance-bias balance via synthetic experiments and demonstrate the\neffectiveness of SkMM for finetuning in real vision tasks.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}