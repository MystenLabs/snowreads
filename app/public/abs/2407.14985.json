{"id":"2407.14985","title":"Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data","authors":"Antonis Antoniades, Xinyi Wang, Yanai Elazar, Alfonso Amayuelas, Alon\n  Albalak, Kexun Zhang, William Yang Wang","authorsParsed":[["Antoniades","Antonis",""],["Wang","Xinyi",""],["Elazar","Yanai",""],["Amayuelas","Alfonso",""],["Albalak","Alon",""],["Zhang","Kexun",""],["Wang","William Yang",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 21:24:40 GMT"}],"updateDate":"2024-07-23","timestamp":1721510680000,"abstract":"  Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}