{"id":"2408.10276","title":"FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation\n  Models","authors":"Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, Fenglong Ma","authorsParsed":[["Wang","Xiaochen",""],["Wang","Jiaqi",""],["Xiao","Houping",""],["Chen","Jinghui",""],["Ma","Fenglong",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 15:42:29 GMT"}],"updateDate":"2024-08-21","timestamp":1723909349000,"abstract":"  Foundation models have demonstrated remarkable capabilities in handling\ndiverse modalities and tasks, outperforming conventional artificial\nintelligence (AI) approaches that are highly task-specific and\nmodality-reliant. In the medical domain, however, the development of\ncomprehensive foundation models is constrained by limited access to diverse\nmodalities and stringent privacy regulations. To address these constraints,\nthis study introduces a novel knowledge injection approach, FedKIM, designed to\nscale the medical foundation model within a federated learning framework.\nFedKIM leverages lightweight local models to extract healthcare knowledge from\nprivate data and integrates this knowledge into a centralized foundation model\nusing a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE)\nmodule. This method not only preserves privacy but also enhances the model's\nability to handle complex medical tasks involving multiple modalities. Our\nextensive experiments across twelve tasks in seven modalities demonstrate the\neffectiveness of FedKIM in various settings, highlighting its potential to\nscale medical foundation models without direct access to sensitive data.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pHWL_c98UjFqFOKPprXRK6TK-AMxdlgjiTP_MZ6S5tU","pdfSize":"1042155"}
