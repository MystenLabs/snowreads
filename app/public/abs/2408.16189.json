{"id":"2408.16189","title":"A More Unified Theory of Transfer Learning","authors":"Steve Hanneke and Samory Kpotufe","authorsParsed":[["Hanneke","Steve",""],["Kpotufe","Samory",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 01:02:40 GMT"}],"updateDate":"2024-08-30","timestamp":1724893360000,"abstract":"  We show that some basic moduli of continuity $\\delta$ -- which measure how\nfast target risk decreases as source risk decreases -- appear to be at the root\nof many of the classical relatedness measures in transfer learning and related\nliterature. Namely, bounds in terms of $\\delta$ recover many of the existing\nbounds in terms of other measures of relatedness -- both in regression and\nclassification -- and can at times be tighter.\n  We are particularly interested in general situations where the learner has\naccess to both source data and some or no target data. The unified perspective\nallowed by the moduli $\\delta$ allow us to extend many existing notions of\nrelatedness at once to these scenarios involving target data: interestingly,\nwhile $\\delta$ itself might not be efficiently estimated, adaptive procedures\nexist -- based on reductions to confidence sets -- which can get nearly tight\nrates in terms of $\\delta$ with no prior distributional knowledge. Such\nadaptivity to unknown $\\delta$ immediately implies adaptivity to many classical\nrelatedness notions, in terms of combined source and target samples' sizes.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}