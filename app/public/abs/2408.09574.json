{"id":"2408.09574","title":"PhysBERT: A Text Embedding Model for Physics Scientific Literature","authors":"Thorsten Hellert, Jo\\~ao Montenegro and Andrea Pollastro","authorsParsed":[["Hellert","Thorsten",""],["Montenegro","Jo√£o",""],["Pollastro","Andrea",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 19:18:12 GMT"}],"updateDate":"2024-08-20","timestamp":1724008692000,"abstract":"  The specialized language and complex concepts in physics pose significant\nchallenges for information extraction through Natural Language Processing\n(NLP). Central to effective NLP applications is the text embedding model, which\nconverts text into dense vector representations for efficient information\nretrieval and semantic analysis. In this work, we introduce PhysBERT, the first\nphysics-specific text embedding model. Pre-trained on a curated corpus of 1.2\nmillion arXiv physics papers and fine-tuned with supervised data, PhysBERT\noutperforms leading general-purpose models on physics-specific tasks including\nthe effectiveness in fine-tuning for specific physics subdomains.\n","subjects":["Physics/Computational Physics","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}