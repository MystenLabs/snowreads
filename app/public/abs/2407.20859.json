{"id":"2407.20859","title":"Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction\n  Amplification","authors":"Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes,\n  Savvas Zannettou, Yang Zhang","authorsParsed":[["Zhang","Boyang",""],["Tan","Yicong",""],["Shen","Yun",""],["Salem","Ahmed",""],["Backes","Michael",""],["Zannettou","Savvas",""],["Zhang","Yang",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 14:35:31 GMT"}],"updateDate":"2024-07-31","timestamp":1722350131000,"abstract":"  Recently, autonomous agents built on large language models (LLMs) have\nexperienced significant development and are being deployed in real-world\napplications. These agents can extend the base LLM's capabilities in multiple\nways. For example, a well-built agent using GPT-3.5-Turbo as its core can\noutperform the more advanced GPT-4 model by leveraging external components.\nMore importantly, the usage of tools enables these systems to perform actions\nin the real world, moving from merely generating text to actively interacting\nwith their environment. Given the agents' practical applications and their\nability to execute consequential actions, it is crucial to assess potential\nvulnerabilities. Such autonomous systems can cause more severe damage than a\nstandalone language model if compromised. While some existing research has\nexplored harmful actions by LLM agents, our study approaches the vulnerability\nfrom a different perspective. We introduce a new type of attack that causes\nmalfunctions by misleading the agent into executing repetitive or irrelevant\nactions. We conduct comprehensive evaluations using various attack methods,\nsurfaces, and properties to pinpoint areas of susceptibility. Our experiments\nreveal that these attacks can induce failure rates exceeding 80\\% in multiple\nscenarios. Through attacks on implemented and deployable agents in multi-agent\nscenarios, we accentuate the realistic risks associated with these\nvulnerabilities. To mitigate such attacks, we propose self-examination\ndetection methods. However, our findings indicate these attacks are difficult\nto detect effectively using LLMs alone, highlighting the substantial risks\nassociated with this vulnerability.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}