{"id":"2407.00747","title":"A Comparative Study of Quality Evaluation Methods for Text Summarization","authors":"Huyen Nguyen, Haihua Chen, Lavanya Pobbathi, Junhua Ding","authorsParsed":[["Nguyen","Huyen",""],["Chen","Haihua",""],["Pobbathi","Lavanya",""],["Ding","Junhua",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 16:12:37 GMT"}],"updateDate":"2024-07-02","timestamp":1719763957000,"abstract":"  Evaluating text summarization has been a challenging task in natural language\nprocessing (NLP). Automatic metrics which heavily rely on reference summaries\nare not suitable in many situations, while human evaluation is time-consuming\nand labor-intensive. To bridge this gap, this paper proposes a novel method\nbased on large language models (LLMs) for evaluating text summarization. We\nalso conducts a comparative study on eight automatic metrics, human evaluation,\nand our proposed LLM-based method. Seven different types of state-of-the-art\n(SOTA) summarization models were evaluated. We perform extensive experiments\nand analysis on datasets with patent documents. Our results show that LLMs\nevaluation aligns closely with human evaluation, while widely-used automatic\nmetrics such as ROUGE-2, BERTScore, and SummaC do not and also lack\nconsistency. Based on the empirical comparison, we propose a LLM-powered\nframework for automatically evaluating and improving text summarization, which\nis beneficial and could attract wide attention among the community.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ii143m1M7Z9lUU-wFxROHZKjKyNUa5L2AVlIv0M3mto","pdfSize":"649595"}
