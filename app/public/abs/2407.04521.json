{"id":"2407.04521","title":"Unified continuous-time q-learning for mean-field game and mean-field\n  control problems","authors":"Xiaoli Wei, Xiang Yu, Fengyi Yuan","authorsParsed":[["Wei","Xiaoli",""],["Yu","Xiang",""],["Yuan","Fengyi",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 14:06:59 GMT"}],"updateDate":"2024-07-08","timestamp":1720188419000,"abstract":"  This paper studies the continuous-time q-learning in the mean-field\njump-diffusion models from the representative agent's perspective. To overcome\nthe challenge when the population distribution may not be directly observable,\nwe introduce the integrated q-function in decoupled form (decoupled\nIq-function) and establish its martingale characterization together with the\nvalue function, which provides a unified policy evaluation rule for both\nmean-field game (MFG) and mean-field control (MFC) problems. Moreover,\ndepending on the task to solve the MFG or MFC problem, we can employ the\ndecoupled Iq-function by different means to learn the mean-field equilibrium\npolicy or the mean-field optimal policy respectively. As a result, we devise a\nunified q-learning algorithm for both MFG and MFC problems by utilizing all\ntest policies stemming from the mean-field interactions. For several examples\nin the jump-diffusion setting, within and beyond the LQ framework, we can\nobtain the exact parameterization of the decoupled Iq-functions and the value\nfunctions, and illustrate our algorithm from the representative agent's\nperspective with satisfactory performance.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning","Quantitative Finance/Computational Finance"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}