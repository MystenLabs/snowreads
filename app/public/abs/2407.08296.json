{"id":"2407.08296","title":"Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive\n  Low-Rank Gradients","authors":"Zhenyu Zhang, Ajay Jaiswal, Lu Yin, Shiwei Liu, Jiawei Zhao, Yuandong\n  Tian, Zhangyang Wang","authorsParsed":[["Zhang","Zhenyu",""],["Jaiswal","Ajay",""],["Yin","Lu",""],["Liu","Shiwei",""],["Zhao","Jiawei",""],["Tian","Yuandong",""],["Wang","Zhangyang",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 08:42:58 GMT"}],"updateDate":"2024-07-12","timestamp":1720687378000,"abstract":"  Training Large Language Models (LLMs) is memory-intensive due to the large\nnumber of parameters and associated optimization states. GaLore, a recent\nmethod, reduces memory usage by projecting weight gradients into a low-rank\nsubspace without compromising performance. However, GaLore relies on\ntime-consuming Singular Value Decomposition (SVD) operations to identify the\nsubspace, and the frequent subspace updates lead to significant training time\noverhead. Moreover, GaLore offers minimal improvements in accuracy and\nefficiency compared to LoRA in more accessible fine-tuning scenarios. To\naddress these limitations, we introduce Q-Galore, a novel approach that\nsubstantially reduces memory usage by combining quantization and low-rank\nprojection, surpassing the benefits of GaLore. Our method is based on two key\nobservations: (i) the gradient subspace exhibits diverse properties, with some\nlayers converging early in training while others are subject to frequent\nchanges; (ii) the projection matrices are highly resilient to low-bit\nquantization. Leveraging these insights, Q-GaLore adaptively updates the\ngradient subspace based on its convergence statistics, achieving comparable\nperformance while significantly reducing the number of SVD operations. We\nmaintain the projection matrices in INT4 format and weights in INT8 format,\nincorporating stochastic rounding to capture accumulated gradient information.\nThis approach enables a high-precision training trajectory using only\nlow-precision weights. We demonstrate that Q-GaLore achieves highly competitive\nperformance with exceptional memory efficiency. At pre-training, Q-GaLore\nfacilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060\nTi with only 16 GB memory. At fine-tuning, it reduces memory consumption by up\nto 50% compared to LoRA and GaLore, while consistently outperforming QLoRA at\nthe same memory cost.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}