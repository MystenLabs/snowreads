{"id":"2408.04569","title":"Activation thresholds and expressiveness of polynomial neural networks","authors":"Bella Finkel, Jose Israel Rodriguez, Chenxi Wu, Thomas Yahl","authorsParsed":[["Finkel","Bella",""],["Rodriguez","Jose Israel",""],["Wu","Chenxi",""],["Yahl","Thomas",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 16:28:56 GMT"}],"updateDate":"2024-08-09","timestamp":1723134536000,"abstract":"  Polynomial neural networks have been implemented in a range of applications\nand present an advantageous framework for theoretical machine learning. A\npolynomial neural network of fixed architecture and activation degree gives an\nalgebraic map from the network's weights to a set of polynomials. The image of\nthis map is the space of functions representable by the network. Its Zariski\nclosure is an affine variety known as a neurovariety. The dimension of a\npolynomial neural network's neurovariety provides a measure of its\nexpressivity. In this work, we introduce the notion of the activation threshold\nof a network architecture which expresses when the dimension of a neurovariety\nachieves its theoretical maximum. In addition, we prove expressiveness results\nfor polynomial neural networks with equi-width~architectures.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing","Mathematics/Algebraic Geometry","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}