{"id":"2407.01012","title":"Swish-T : Enhancing Swish Activation with Tanh Bias for Improved Neural\n  Network Performance","authors":"Youngmin Seo, Jinha Kim and Unsang Park","authorsParsed":[["Seo","Youngmin",""],["Kim","Jinha",""],["Park","Unsang",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 06:52:34 GMT"},{"version":"v2","created":"Tue, 2 Jul 2024 03:48:06 GMT"},{"version":"v3","created":"Wed, 3 Jul 2024 05:36:00 GMT"}],"updateDate":"2024-07-04","timestamp":1719816754000,"abstract":"  We propose the Swish-T family, an enhancement of the existing non-monotonic\nactivation function Swish. Swish-T is defined by adding a Tanh bias to the\noriginal Swish function. This modification creates a family of Swish-T\nvariants, each designed to excel in different tasks, showcasing specific\nadvantages depending on the application context. The Tanh bias allows for\nbroader acceptance of negative values during initial training stages, offering\na smoother non-monotonic curve than the original Swish. We ultimately propose\nthe Swish-T$_{\\textbf{C}}$ function, while Swish-T and Swish-T$_{\\textbf{B}}$,\nbyproducts of Swish-T$_{\\textbf{C}}$, also demonstrate satisfactory\nperformance. Furthermore, our ablation study shows that using\nSwish-T$_{\\textbf{C}}$ as a non-parametric function can still achieve high\nperformance. The superiority of the Swish-T family has been empirically\ndemonstrated across various models and benchmark datasets, including MNIST,\nFashion MNIST, SVHN, CIFAR-10, and CIFAR-100. The code is publicly available at\nhttps://github.com/ictseoyoungmin/Swish-T-pytorch.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}