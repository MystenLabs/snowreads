{"id":"2408.07806","title":"From Decision to Action in Surgical Autonomy: Multi-Modal Large Language\n  Models for Robot-Assisted Blood Suction","authors":"Sadra Zargarzadeh, Maryam Mirzaei, Yafei Ou, Mahdi Tavakoli","authorsParsed":[["Zargarzadeh","Sadra",""],["Mirzaei","Maryam",""],["Ou","Yafei",""],["Tavakoli","Mahdi",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 20:30:34 GMT"}],"updateDate":"2024-08-16","timestamp":1723667434000,"abstract":"  The rise of Large Language Models (LLMs) has impacted research in robotics\nand automation. While progress has been made in integrating LLMs into general\nrobotics tasks, a noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in robotic\nsurgery, which entails the ability to reason and adapt to changes in the\nenvironment, remains a significant challenge. In this work, we propose a\nmulti-modal LLM integration in robot-assisted surgery for autonomous blood\nsuction. The reasoning and prioritization are delegated to the higher-level\ntask-planning LLM, and the motion planning and execution are handled by the\nlower-level deep reinforcement learning model, creating a distributed agency\nbetween the two components. As surgical operations are highly dynamic and may\nencounter unforeseen circumstances, blood clots and active bleeding were\nintroduced to influence decision-making. Results showed that using a\nmulti-modal LLM as a higher-level reasoning unit can account for these surgical\ncomplexities to achieve a level of reasoning previously unattainable in\nrobot-assisted surgeries. These findings demonstrate the potential of\nmulti-modal LLMs to significantly enhance contextual understanding and\ndecision-making in robotic-assisted surgeries, marking a step toward autonomous\nsurgical systems.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}