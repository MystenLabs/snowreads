{"id":"2407.15018","title":"Answer, Assemble, Ace: Understanding How Transformers Answer Multiple\n  Choice Questions","authors":"Sarah Wiegreffe, Oyvind Tafjord, Yonatan Belinkov, Hannaneh\n  Hajishirzi, Ashish Sabharwal","authorsParsed":[["Wiegreffe","Sarah",""],["Tafjord","Oyvind",""],["Belinkov","Yonatan",""],["Hajishirzi","Hannaneh",""],["Sabharwal","Ashish",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 00:10:23 GMT"}],"updateDate":"2024-07-23","timestamp":1721520623000,"abstract":"  Multiple-choice question answering (MCQA) is a key competence of performant\ntransformer language models that is tested by mainstream benchmarks. However,\nrecent evidence shows that models can have quite a range of performance,\nparticularly when the task format is diversified slightly (such as by shuffling\nanswer choice order). In this work we ask: how do successful models perform\nformatted MCQA? We employ vocabulary projection and activation patching methods\nto localize key hidden states that encode relevant information for predicting\nthe correct answer. We find that prediction of a specific answer symbol is\ncausally attributed to a single middle layer, and specifically its multi-head\nself-attention mechanism. We show that subsequent layers increase the\nprobability of the predicted answer symbol in vocabulary space, and that this\nprobability increase is associated with a sparse set of attention heads with\nunique roles. We additionally uncover differences in how different models\nadjust to alternative symbols. Finally, we demonstrate that a synthetic task\ncan disentangle sources of model error to pinpoint when a model has learned\nformatted MCQA, and show that an inability to separate answer symbol tokens in\nvocabulary space is a property of models unable to perform formatted MCQA\ntasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}