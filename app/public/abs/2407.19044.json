{"id":"2407.19044","title":"Advancing Neural Network Performance through Emergence-Promoting\n  Initialization Scheme","authors":"Johnny Jingze Li, Vivek Kurien George, Gabriel A. Silva","authorsParsed":[["Li","Johnny Jingze",""],["George","Vivek Kurien",""],["Silva","Gabriel A.",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 18:56:47 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 21:46:07 GMT"}],"updateDate":"2024-08-12","timestamp":1722020207000,"abstract":"  We introduce a novel yet straightforward neural network initialization scheme\nthat modifies conventional methods like Xavier and Kaiming initialization.\nInspired by the concept of emergence and leveraging the emergence measures\nproposed by Li (2023), our method adjusts the layer-wise weight scaling factors\nto achieve higher emergence values. This enhancement is easy to implement,\nrequiring no additional optimization steps for initialization compared to\nGradInit. We evaluate our approach across various architectures, including MLP\nand convolutional architectures for image recognition, and transformers for\nmachine translation. We demonstrate substantial improvements in both model\naccuracy and training speed, with and without batch normalization. The\nsimplicity, theoretical innovation, and demonstrable empirical advantages of\nour method make it a potent enhancement to neural network initialization\npractices. These results suggest a promising direction for leveraging emergence\nto improve neural network training methodologies. Code is available at:\nhttps://github.com/johnnyjingzeli/EmergenceInit.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UI2ZewDjpouPjlWsM5dGtSgENXYOO4Bj4cbBsc15new","pdfSize":"1266331"}
