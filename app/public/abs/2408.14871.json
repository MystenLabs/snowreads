{"id":"2408.14871","title":"Learning Robust Reward Machines from Noisy Labels","authors":"Roko Parac, Lorenzo Nodari, Leo Ardon, Daniel Furelos-Blanco, Federico\n  Cerutti, Alessandra Russo","authorsParsed":[["Parac","Roko",""],["Nodari","Lorenzo",""],["Ardon","Leo",""],["Furelos-Blanco","Daniel",""],["Cerutti","Federico",""],["Russo","Alessandra",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 08:41:42 GMT"}],"updateDate":"2024-08-28","timestamp":1724748102000,"abstract":"  This paper presents PROB-IRM, an approach that learns robust reward machines\n(RMs) for reinforcement learning (RL) agents from noisy execution traces. The\nkey aspect of RM-driven RL is the exploitation of a finite-state machine that\ndecomposes the agent's task into different subtasks. PROB-IRM uses a\nstate-of-the-art inductive logic programming framework robust to noisy examples\nto learn RMs from noisy traces using the Bayesian posterior degree of beliefs,\nthus ensuring robustness against inconsistencies. Pivotal for the results is\nthe interleaving between RM learning and policy learning: a new RM is learned\nwhenever the RL agent generates a trace that is believed not to be accepted by\nthe current RM. To speed up the training of the RL agent, PROB-IRM employs a\nprobabilistic formulation of reward shaping that uses the posterior Bayesian\nbeliefs derived from the traces. Our experimental analysis shows that PROB-IRM\ncan learn (potentially imperfect) RMs from noisy traces and exploit them to\ntrain an RL agent to solve its tasks successfully. Despite the complexity of\nlearning the RM from noisy traces, agents trained with PROB-IRM perform\ncomparably to agents provided with handcrafted RMs.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}