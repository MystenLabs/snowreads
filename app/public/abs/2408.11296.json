{"id":"2408.11296","title":"RePair: Automated Program Repair with Process-based Feedback","authors":"Yuze Zhao, Zhenya Huang, Yixiao Ma, Rui Li, Kai Zhang, Hao Jiang, Qi\n  Liu, Linbo Zhu, Yu Su","authorsParsed":[["Zhao","Yuze",""],["Huang","Zhenya",""],["Ma","Yixiao",""],["Li","Rui",""],["Zhang","Kai",""],["Jiang","Hao",""],["Liu","Qi",""],["Zhu","Linbo",""],["Su","Yu",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 02:53:23 GMT"}],"updateDate":"2024-08-22","timestamp":1724208803000,"abstract":"  The gap between the trepidation of program reliability and the expense of\nrepairs underscores the indispensability of Automated Program Repair (APR). APR\nis instrumental in transforming vulnerable programs into more robust ones,\nbolstering program reliability while simultaneously diminishing the financial\nburden of manual repairs. Commercial-scale language models (LM) have taken APR\nto unprecedented levels. However, the emergence reveals that for models fewer\nthan 100B parameters, making single-step modifications may be difficult to\nachieve the desired effect. Moreover, humans interact with the LM through\nexplicit prompts, which hinders the LM from receiving feedback from compiler\nand test cases to automatically optimize its repair policies. In this\nliterature, we explore how small-scale LM (less than 20B) achieve excellent\nperformance through process supervision and feedback. We start by constructing\na dataset named CodeNet4Repair, replete with multiple repair records, which\nsupervises the fine-tuning of a foundational model. Building upon the\nencouraging outcomes of reinforcement learning, we develop a reward model that\nserves as a critic, providing feedback for the fine-tuned LM's action,\nprogressively optimizing its policy. During inference, we require the LM to\ngenerate solutions iteratively until the repair effect no longer improves or\nhits the maximum step limit. The results show that process-based not only\noutperforms larger outcome-based generation methods, but also nearly matches\nthe performance of closed-source commercial large-scale LMs.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}