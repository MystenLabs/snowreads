{"id":"2408.04162","title":"Semantics or spelling? Probing contextual word embeddings with\n  orthographic noise","authors":"Jacob A. Matthews, John R. Starr and Marten van Schijndel","authorsParsed":[["Matthews","Jacob A.",""],["Starr","John R.",""],["van Schijndel","Marten",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 02:07:25 GMT"}],"updateDate":"2024-08-09","timestamp":1723082845000,"abstract":"  Pretrained language model (PLM) hidden states are frequently employed as\ncontextual word embeddings (CWE): high-dimensional representations that encode\nsemantic information given linguistic context. Across many areas of\ncomputational linguistics research, similarity between CWEs is interpreted as\nsemantic similarity. However, it remains unclear exactly what information is\nencoded in PLM hidden states. We investigate this practice by probing PLM\nrepresentations using minimal orthographic noise. We expect that if CWEs\nprimarily encode semantic information, a single character swap in the input\nword will not drastically affect the resulting representation,given sufficient\nlinguistic context. Surprisingly, we find that CWEs generated by popular PLMs\nare highly sensitive to noise in input data, and that this sensitivity is\nrelated to subword tokenization: the fewer tokens used to represent a word at\ninput, the more sensitive its corresponding CWE. This suggests that CWEs\ncapture information unrelated to word-level meaning and can be manipulated\nthrough trivial modifications of input data. We conclude that these PLM-derived\nCWEs may not be reliable semantic proxies, and that caution is warranted when\ninterpreting representational similarity\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}