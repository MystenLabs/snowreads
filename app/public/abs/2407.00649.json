{"id":"2407.00649","title":"Particle Semi-Implicit Variational Inference","authors":"Jen Ning Lim, Adam M. Johansen","authorsParsed":[["Lim","Jen Ning",""],["Johansen","Adam M.",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 10:21:41 GMT"}],"updateDate":"2024-07-02","timestamp":1719742901000,"abstract":"  Semi-implicit variational inference (SIVI) enriches the expressiveness of\nvariational families by utilizing a kernel and a mixing distribution to\nhierarchically define the variational distribution. Existing SIVI methods\nparameterize the mixing distribution using implicit distributions, leading to\nintractable variational densities. As a result, directly maximizing the\nevidence lower bound (ELBO) is not possible and so, they resort to either:\noptimizing bounds on the ELBO, employing costly inner-loop Markov chain Monte\nCarlo runs, or solving minimax objectives. In this paper, we propose a novel\nmethod for SIVI called Particle Variational Inference (PVI) which employs\nempirical measures to approximate the optimal mixing distributions\ncharacterized as the minimizer of a natural free energy functional via a\nparticle approximation of an Euclidean--Wasserstein gradient flow. This\napproach means that, unlike prior works, PVI can directly optimize the ELBO;\nfurthermore, it makes no parametric assumption about the mixing distribution.\nOur empirical results demonstrate that PVI performs favourably against other\nSIVI methods across various tasks. Moreover, we provide a theoretical analysis\nof the behaviour of the gradient flow of a related free energy functional:\nestablishing the existence and uniqueness of solutions as well as propagation\nof chaos results.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"zs-bsA5UA-WHXmOLIRDvEsmBf6LR2-h7w8iOReUuV1U","pdfSize":"2496562"}
