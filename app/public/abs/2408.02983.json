{"id":"2408.02983","title":"Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond","authors":"Jichuan Zhang, Yali Li, Xin Liu, Shengjin Wang","authorsParsed":[["Zhang","Jichuan",""],["Li","Yali",""],["Liu","Xin",""],["Wang","Shengjin",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 06:33:24 GMT"}],"updateDate":"2024-08-07","timestamp":1722926004000,"abstract":"  Non-exemplar class-incremental learning (NECIL) is to resist catastrophic\nforgetting without saving old class samples. Prior methodologies generally\nemploy simple rules to generate features for replaying, suffering from large\ndistribution gap between replayed features and real ones. To address the\naforementioned issue, we propose a simple, yet effective\n\\textbf{Diff}usion-based \\textbf{F}eature \\textbf{R}eplay (\\textbf{DiffFR})\nmethod for NECIL. First, to alleviate the limited representational capacity\ncaused by fixing the feature extractor, we employ Siamese-based self-supervised\nlearning for initial generalizable features. Second, we devise diffusion models\nto generate class-representative features highly similar to real features,\nwhich provides an effective way for exemplar-free knowledge memorization.\nThird, we introduce prototype calibration to direct the diffusion model's focus\ntowards learning the distribution shapes of features, rather than the entire\ndistribution. Extensive experiments on public datasets demonstrate significant\nperformance gains of our DiffFR, outperforming the state-of-the-art NECIL\nmethods by 3.0\\% in average. The code will be made publicly available soon.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}