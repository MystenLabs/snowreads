{"id":"2407.04152","title":"VoxAct-B: Voxel-Based Acting and Stabilizing Policy for Bimanual\n  Manipulation","authors":"I-Chun Arthur Liu, Sicheng He, Daniel Seita, Gaurav Sukhatme","authorsParsed":[["Liu","I-Chun Arthur",""],["He","Sicheng",""],["Seita","Daniel",""],["Sukhatme","Gaurav",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 20:58:20 GMT"}],"updateDate":"2024-07-08","timestamp":1720126700000,"abstract":"  Bimanual manipulation is critical to many robotics applications. In contrast\nto single-arm manipulation, bimanual manipulation tasks are challenging due to\nhigher-dimensional action spaces. Prior works leverage large amounts of data\nand primitive actions to address this problem, but may suffer from sample\ninefficiency and limited generalization across various tasks. To this end, we\npropose VoxAct-B, a language-conditioned, voxel-based method that leverages\nVision Language Models (VLMs) to prioritize key regions within the scene and\nreconstruct a voxel grid. We provide this voxel grid to our bimanual\nmanipulation policy to learn acting and stabilizing actions. This approach\nenables more efficient policy learning from voxels and is generalizable to\ndifferent tasks. In simulation, we show that VoxAct-B outperforms strong\nbaselines on fine-grained bimanual manipulation tasks. Furthermore, we\ndemonstrate VoxAct-B on real-world $\\texttt{Open Drawer}$ and $\\texttt{Open\nJar}$ tasks using two UR5s. Code, data, and videos will be available at\nhttps://voxact-b.github.io.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}