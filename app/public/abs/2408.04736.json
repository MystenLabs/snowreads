{"id":"2408.04736","title":"Towards Using Multiple Iterated, Reproduced, and Replicated Experiments\n  with Robots (MIRRER) for Evaluation and Benchmarking","authors":"Adam Norton and Brian Flynn (New England Robotics Validation and\n  Experimentation (NERVE) Center, University of Massachusetts Lowell)","authorsParsed":[["Norton","Adam","","New England Robotics Validation and\n  Experimentation"],["Flynn","Brian","","New England Robotics Validation and\n  Experimentation"]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 19:33:23 GMT"}],"updateDate":"2024-08-12","timestamp":1723145603000,"abstract":"  The robotics research field lacks formalized definitions and frameworks for\nevaluating advanced capabilities including generalizability (the ability for\nrobots to perform tasks under varied contexts) and reproducibility (the\nperformance of a reproduced robot capability in different labs under the same\nexperimental conditions). This paper presents an initial conceptual framework,\nMIRRER, that unites the concepts of performance evaluation, benchmarking, and\nreproduced/replicated experimentation in order to facilitate comparable\nrobotics research. Several open issues with the application of the framework\nare also presented.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}