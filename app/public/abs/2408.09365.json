{"id":"2408.09365","title":"Concept Distillation from Strong to Weak Models via\n  Hypotheses-to-Theories Prompting","authors":"Emmanuel Aboah Boateng, Cassiano O. Becker, Nabiha Asghar, Kabir\n  Walia, Ashwin Srinivasan, Ehi Nosakhare, Victor Dibia, Soundar Srinivasan","authorsParsed":[["Boateng","Emmanuel Aboah",""],["Becker","Cassiano O.",""],["Asghar","Nabiha",""],["Walia","Kabir",""],["Srinivasan","Ashwin",""],["Nosakhare","Ehi",""],["Dibia","Victor",""],["Srinivasan","Soundar",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 05:37:48 GMT"}],"updateDate":"2024-08-20","timestamp":1723959468000,"abstract":"  Hand-crafting high quality prompts to optimize the performance of language\nmodels is a complicated and labor-intensive process. Furthermore, when\nmigrating to newer, smaller, or weaker models (possibly due to latency or cost\ngains), prompts need to be updated to re-optimize the task performance. We\npropose Concept Distillation (CD), an automatic prompt optimization technique\nfor enhancing weaker models on complex tasks. CD involves: (1) collecting\nmistakes made by weak models with a base prompt (initialization), (2) using a\nstrong model to generate reasons for these mistakes and create rules/concepts\nfor weak models (induction), and (3) filtering these rules based on validation\nset performance and integrating them into the base prompt\n(deduction/verification). We evaluated CD on NL2Code and mathematical reasoning\ntasks, observing significant performance boosts for small and weaker language\nmodels. Notably, Mistral-7B's accuracy on Multi-Arith increased by 20%, and\nPhi-3-mini-3.8B's accuracy on HumanEval rose by 34%. Compared to other\nautomated methods, CD offers an effective, cost-efficient strategy for\nimproving weak models' performance on complex tasks and enables seamless\nworkload migration across different language models without compromising\nperformance.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}