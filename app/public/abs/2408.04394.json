{"id":"2408.04394","title":"Automated Educational Question Generation at Different Bloom's Skill\n  Levels using Large Language Models: Strategies and Evaluation","authors":"Nicy Scaria, Suma Dharani Chenna, Deepak Subramani","authorsParsed":[["Scaria","Nicy",""],["Chenna","Suma Dharani",""],["Subramani","Deepak",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 11:56:57 GMT"}],"updateDate":"2024-08-23","timestamp":1723118217000,"abstract":"  Developing questions that are pedagogically sound, relevant, and promote\nlearning is a challenging and time-consuming task for educators. Modern-day\nlarge language models (LLMs) generate high-quality content across multiple\ndomains, potentially helping educators to develop high-quality questions.\nAutomated educational question generation (AEQG) is important in scaling online\neducation catering to a diverse student population. Past attempts at AEQG have\nshown limited abilities to generate questions at higher cognitive levels. In\nthis study, we examine the ability of five state-of-the-art LLMs of different\nsizes to generate diverse and high-quality questions of different cognitive\nlevels, as defined by Bloom's taxonomy. We use advanced prompting techniques\nwith varying complexity for AEQG. We conducted expert and LLM-based evaluations\nto assess the linguistic and pedagogical relevance and quality of the\nquestions. Our findings suggest that LLms can generate relevant and\nhigh-quality educational questions of different cognitive levels when prompted\nwith adequate information, although there is a significant variance in the\nperformance of the five LLms considered. We also show that automated evaluation\nis not on par with human evaluation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NQaYuMHJ5AcrFtKbi0oF98deuQYgGpWiLQb3ExRMs1Q","pdfSize":"1031517"}
