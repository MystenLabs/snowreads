{"id":"2408.01214","title":"High-Throughput Phenotyping of Clinical Text Using Large Language Models","authors":"Daniel B. Hier, S. Ilyas Munzir, Anne Stahlfeld, Tayo Obafemi-Ajayi,\n  and Michael D. Carrithers","authorsParsed":[["Hier","Daniel B.",""],["Munzir","S. Ilyas",""],["Stahlfeld","Anne",""],["Obafemi-Ajayi","Tayo",""],["Carrithers","Michael D.",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 12:00:00 GMT"}],"updateDate":"2024-08-05","timestamp":1722600000000,"abstract":"  High-throughput phenotyping automates the mapping of patient signs to\nstandardized ontology concepts and is essential for precision medicine. This\nstudy evaluates the automation of phenotyping of clinical summaries from the\nOnline Mendelian Inheritance in Man (OMIM) database using large language\nmodels. Due to their rich phenotype data, these summaries can be surrogates for\nphysician notes. We conduct a performance comparison of GPT-4 and\nGPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in\nidentifying, categorizing, and normalizing signs, achieving concordance with\nmanual annotators comparable to inter-rater agreement. Despite some limitations\nin sign normalization, the extensive pre-training of GPT-4 results in high\nperformance and generalizability across several phenotyping tasks while\nobviating the need for manually annotated training data. Large language models\nare expected to be the dominant method for automating high-throughput\nphenotyping of clinical text.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1e31JPpytB45nSmzJVwnAtO4aKnHUDkXNNsDpnipbPc","pdfSize":"8121423"}
