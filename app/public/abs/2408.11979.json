{"id":"2408.11979","title":"Only Strict Saddles in the Energy Landscape of Predictive Coding\n  Networks?","authors":"Francesco Innocenti, El Mehdi Achour, Ryan Singh, Christopher L.\n  Buckley","authorsParsed":[["Innocenti","Francesco",""],["Achour","El Mehdi",""],["Singh","Ryan",""],["Buckley","Christopher L.",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 20:23:44 GMT"}],"updateDate":"2024-08-23","timestamp":1724271824000,"abstract":"  Predictive coding (PC) is an energy-based learning algorithm that performs\niterative inference over network activities before weight updates. Recent work\nsuggests that PC can converge in fewer learning steps than backpropagation\nthanks to its inference procedure. However, these advantages are not always\nobserved, and the impact of PC inference on learning is theoretically not well\nunderstood. Here, we study the geometry of the PC energy landscape at the\n(inference) equilibrium of the network activities. For deep linear networks, we\nfirst show that the equilibrated energy is simply a rescaled mean squared error\nloss with a weight-dependent rescaling. We then prove that many highly\ndegenerate (non-strict) saddles of the loss including the origin become much\neasier to escape (strict) in the equilibrated energy. Our theory is validated\nby experiments on both linear and non-linear networks. Based on these results,\nwe conjecture that all the saddles of the equilibrated energy are strict.\nOverall, this work suggests that PC inference makes the loss landscape more\nbenign and robust to vanishing gradients, while also highlighting the challenge\nof speeding up PC inference on large-scale models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0oclnsnA0HS5hyMBWdv4zACaMXRpnqNss1vdMimduzk","pdfSize":"14774162"}
