{"id":"2407.06957","title":"Listen and Speak Fairly: A Study on Semantic Gender Bias in Speech\n  Integrated Large Language Models","authors":"Yi-Cheng Lin, Tzu-Quan Lin, Chih-Kai Yang, Ke-Han Lu, Wei-Chih Chen,\n  Chun-Yi Kuan, Hung-yi Lee","authorsParsed":[["Lin","Yi-Cheng",""],["Lin","Tzu-Quan",""],["Yang","Chih-Kai",""],["Lu","Ke-Han",""],["Chen","Wei-Chih",""],["Kuan","Chun-Yi",""],["Lee","Hung-yi",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 15:35:43 GMT"}],"updateDate":"2024-07-10","timestamp":1720539343000,"abstract":"  Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"5VWMhypSTjUT6paFSSdjQeT9qsnAnHfnw5XJ7MDTp4M","pdfSize":"509501"}
