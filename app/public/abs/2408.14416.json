{"id":"2408.14416","title":"Hyperdimensional Computing Empowered Federated Foundation Model over\n  Wireless Networks for Metaverse","authors":"Yahao Ding, Wen Shang, Minrui Xu, Zhaohui Yang, Ye Hu, Dusit Niyato,\n  Mohammad Shikh-Bahaei","authorsParsed":[["Ding","Yahao",""],["Shang","Wen",""],["Xu","Minrui",""],["Yang","Zhaohui",""],["Hu","Ye",""],["Niyato","Dusit",""],["Shikh-Bahaei","Mohammad",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 17:03:14 GMT"}],"updateDate":"2024-08-27","timestamp":1724691794000,"abstract":"  The Metaverse, a burgeoning collective virtual space merging augmented\nreality and persistent virtual worlds, necessitates advanced artificial\nintelligence (AI) and communication technologies to support immersive and\ninteractive experiences. Federated learning (FL) has emerged as a promising\ntechnique for collaboratively training AI models while preserving data privacy.\nHowever, FL faces challenges such as high communication overhead and\nsubstantial computational demands, particularly for neural network (NN) models.\nTo address these issues, we propose an integrated federated split learning and\nhyperdimensional computing (FSL-HDC) framework for emerging foundation models.\nThis novel approach reduces communication costs, computation load, and privacy\nrisks, making it particularly suitable for resource-constrained edge devices in\nthe Metaverse, ensuring real-time responsive interactions. Additionally, we\nintroduce an optimization algorithm that concurrently optimizes transmission\npower and bandwidth to minimize the maximum transmission time among all users\nto the server. The simulation results based on the MNIST dataset indicate that\nFSL-HDC achieves an accuracy rate of approximately 87.5%, which is slightly\nlower than that of FL-HDC. However, FSL-HDC exhibits a significantly faster\nconvergence speed, approximately 3.733x that of FSL-NN, and demonstrates\nrobustness to non-IID data distributions. Moreover, our proposed optimization\nalgorithm can reduce the maximum transmission time by up to 64% compared with\nthe baseline.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}