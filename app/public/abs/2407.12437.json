{"id":"2407.12437","title":"Variable-Agnostic Causal Exploration for Reinforcement Learning","authors":"Minh Hoang Nguyen, Hung Le, Svetha Venkatesh","authorsParsed":[["Nguyen","Minh Hoang",""],["Le","Hung",""],["Venkatesh","Svetha",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 09:45:27 GMT"}],"updateDate":"2024-07-18","timestamp":1721209527000,"abstract":"  Modern reinforcement learning (RL) struggles to capture real-world\ncause-and-effect dynamics, leading to inefficient exploration due to extensive\ntrial-and-error actions. While recent efforts to improve agent exploration have\nleveraged causal discovery, they often make unrealistic assumptions of causal\nvariables in the environments. In this paper, we introduce a novel framework,\nVariable-Agnostic Causal Exploration for Reinforcement Learning (VACERL),\nincorporating causal relationships to drive exploration in RL without\nspecifying environmental causal variables. Our approach automatically\nidentifies crucial observation-action steps associated with key variables using\nattention mechanisms. Subsequently, it constructs the causal graph connecting\nthese steps, which guides the agent towards observation-action pairs with\ngreater causal influence on task completion. This can be leveraged to generate\nintrinsic rewards or establish a hierarchy of subgoals to enhance exploration\nefficiency. Experimental results showcase a significant improvement in agent\nperformance in grid-world, 2d games and robotic domains, particularly in\nscenarios with sparse rewards and noisy actions, such as the notorious Noisy-TV\nenvironments.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}