{"id":"2407.06501","title":"STORYSUMM: Evaluating Faithfulness in Story Summarization","authors":"Melanie Subbiah, Faisal Ladhak, Akankshya Mishra, Griffin Adams, Lydia\n  B. Chilton, Kathleen McKeown","authorsParsed":[["Subbiah","Melanie",""],["Ladhak","Faisal",""],["Mishra","Akankshya",""],["Adams","Griffin",""],["Chilton","Lydia B.",""],["McKeown","Kathleen",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 02:06:30 GMT"}],"updateDate":"2024-07-10","timestamp":1720490790000,"abstract":"  Human evaluation has been the gold standard for checking faithfulness in\nabstractive summarization. However, with a challenging source domain like\nnarrative, multiple annotators can agree a summary is faithful, while missing\ndetails that are obvious errors only once pointed out. We therefore introduce a\nnew dataset, STORYSUMM, comprising LLM summaries of short stories with\nlocalized faithfulness labels and error explanations. This benchmark is for\nevaluation methods, testing whether a given method can detect challenging\ninconsistencies. Using this dataset, we first show that any one human\nannotation protocol is likely to miss inconsistencies, and we advocate for\npursuing a range of methods when establishing ground truth for a summarization\ndataset. We finally test recent automatic metrics and find that none of them\nachieve more than 70% balanced accuracy on this task, demonstrating that it is\na challenging benchmark for future work in faithfulness evaluation.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}