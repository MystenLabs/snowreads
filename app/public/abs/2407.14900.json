{"id":"2407.14900","title":"AGLLDiff: Guiding Diffusion Models Towards Unsupervised Training-free\n  Real-world Low-light Image Enhancement","authors":"Yunlong Lin, Tian Ye, Sixiang Chen, Zhenqi Fu, Yingying Wang, Wenhao\n  Chai, Zhaohu Xing, Lei Zhu and Xinghao Ding","authorsParsed":[["Lin","Yunlong",""],["Ye","Tian",""],["Chen","Sixiang",""],["Fu","Zhenqi",""],["Wang","Yingying",""],["Chai","Wenhao",""],["Xing","Zhaohu",""],["Zhu","Lei",""],["Ding","Xinghao",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 15:17:48 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 06:56:39 GMT"}],"updateDate":"2024-07-24","timestamp":1721488668000,"abstract":"  Existing low-light image enhancement (LIE) methods have achieved noteworthy\nsuccess in solving synthetic distortions, yet they often fall short in\npractical applications. The limitations arise from two inherent challenges in\nreal-world LIE: 1) the collection of distorted/clean image pairs is often\nimpractical and sometimes even unavailable, and 2) accurately modeling complex\ndegradations presents a non-trivial problem. To overcome them, we propose the\nAttribute Guidance Diffusion framework (AGLLDiff), a training-free method for\neffective real-world LIE. Instead of specifically defining the degradation\nprocess, AGLLDiff shifts the paradigm and models the desired attributes, such\nas image exposure, structure and color of normal-light images. These attributes\nare readily available and impose no assumptions about the degradation process,\nwhich guides the diffusion sampling process to a reliable high-quality solution\nspace. Extensive experiments demonstrate that our approach outperforms the\ncurrent leading unsupervised LIE methods across benchmarks in terms of\ndistortion-based and perceptual-based metrics, and it performs well even in\nsophisticated wild degradation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}