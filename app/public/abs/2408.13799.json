{"id":"2408.13799","title":"Non-asymptotic bounds for forward processes in denoising diffusions:\n  Ornstein-Uhlenbeck is hard to beat","authors":"Miha Bre\\v{s}ar and Aleksandar Mijatovi\\'c","authorsParsed":[["Brešar","Miha",""],["Mijatović","Aleksandar",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 10:28:31 GMT"}],"updateDate":"2024-08-27","timestamp":1724581711000,"abstract":"  Denoising diffusion probabilistic models (DDPMs) represent a recent advance\nin generative modelling that has delivered state-of-the-art results across many\ndomains of applications. Despite their success, a rigorous theoretical\nunderstanding of the error within DDPMs, particularly the non-asymptotic bounds\nrequired for the comparison of their efficiency, remain scarce. Making minimal\nassumptions on the initial data distribution, allowing for example the manifold\nhypothesis, this paper presents explicit non-asymptotic bounds on the forward\ndiffusion error in total variation (TV), expressed as a function of the\nterminal time $T$.\n  We parametrise multi-modal data distributions in terms of the distance $R$ to\ntheir furthest modes and consider forward diffusions with additive and\nmultiplicative noise. Our analysis rigorously proves that, under mild\nassumptions, the canonical choice of the Ornstein-Uhlenbeck (OU) process cannot\nbe significantly improved in terms of reducing the terminal time $T$ as a\nfunction of $R$ and error tolerance $\\varepsilon>0$. Motivated by data\ndistributions arising in generative modelling, we also establish a cut-off like\nphenomenon (as $R\\to\\infty$) for the convergence to its invariant measure in TV\nof an OU process, initialized at a multi-modal distribution with maximal mode\ndistance $R$.\n","subjects":["Mathematics/Statistics Theory","Mathematics/Probability","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}