{"id":"2407.19156","title":"Robust Multimodal 3D Object Detection via Modality-Agnostic Decoding and\n  Proximity-based Modality Ensemble","authors":"Juhan Cha and Minseok Joo and Jihwan Park and Sanghyeok Lee and Injae\n  Kim and Hyunwoo J. Kim","authorsParsed":[["Cha","Juhan",""],["Joo","Minseok",""],["Park","Jihwan",""],["Lee","Sanghyeok",""],["Kim","Injae",""],["Kim","Hyunwoo J.",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 03:21:44 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 10:06:31 GMT"}],"updateDate":"2024-08-20","timestamp":1722050504000,"abstract":"  Recent advancements in 3D object detection have benefited from multi-modal\ninformation from the multi-view cameras and LiDAR sensors. However, the\ninherent disparities between the modalities pose substantial challenges. We\nobserve that existing multi-modal 3D object detection methods heavily rely on\nthe LiDAR sensor, treating the camera as an auxiliary modality for augmenting\nsemantic details. This often leads to not only underutilization of camera data\nbut also significant performance degradation in scenarios where LiDAR data is\nunavailable. Additionally, existing fusion methods overlook the detrimental\nimpact of sensor noise induced by environmental changes, on detection\nperformance. In this paper, we propose MEFormer to address the LiDAR\nover-reliance problem by harnessing critical information for 3D object\ndetection from every available modality while concurrently safeguarding against\ncorrupted signals during the fusion process. Specifically, we introduce\nModality Agnostic Decoding (MOAD) that extracts geometric and semantic features\nwith a shared transformer decoder regardless of input modalities and provides\npromising improvement with a single modality as well as multi-modality.\nAdditionally, our Proximity-based Modality Ensemble (PME) module adaptively\nutilizes the strengths of each modality depending on the environment while\nmitigating the effects of a noisy sensor. Our MEFormer achieves\nstate-of-the-art performance of 73.9% NDS and 71.5% mAP in the nuScenes\nvalidation set. Extensive analyses validate that our MEFormer improves\nrobustness against challenging conditions such as sensor malfunctions or\nenvironmental changes. The source code is available at\nhttps://github.com/hanchaa/MEFormer\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}