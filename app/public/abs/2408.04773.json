{"id":"2408.04773","title":"Exploiting Consistency-Preserving Loss and Perceptual Contrast\n  Stretching to Boost SSL-based Speech Enhancement","authors":"Muhammad Salman Khan, Moreno La Quatra, Kuo-Hsuan Hung, Szu-Wei Fu,\n  Sabato Marco Siniscalchi, Yu Tsao","authorsParsed":[["Khan","Muhammad Salman",""],["La Quatra","Moreno",""],["Hung","Kuo-Hsuan",""],["Fu","Szu-Wei",""],["Siniscalchi","Sabato Marco",""],["Tsao","Yu",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 22:11:43 GMT"}],"updateDate":"2024-08-12","timestamp":1723155103000,"abstract":"  Self-supervised representation learning (SSL) has attained SOTA results on\nseveral downstream speech tasks, but SSL-based speech enhancement (SE)\nsolutions still lag behind. To address this issue, we exploit three main ideas:\n(i) Transformer-based masking generation, (ii) consistency-preserving loss, and\n(iii) perceptual contrast stretching (PCS). In detail, conformer layers,\nleveraging an attention mechanism, are introduced to effectively model\nframe-level representations and obtain the Ideal Ratio Mask (IRM) for SE.\nMoreover, we incorporate consistency in the loss function, which processes the\ninput to account for the inconsistency effects of signal reconstruction from\nthe spectrogram. Finally, PCS is employed to improve the contrast of input and\ntarget features according to perceptual importance. Evaluated on the\nVoiceBank-DEMAND task, the proposed solution outperforms previously SSL-based\nSE solutions when tested on several objective metrics, attaining a SOTA PESQ\nscore of 3.54.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}