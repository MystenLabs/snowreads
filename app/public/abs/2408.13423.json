{"id":"2408.13423","title":"Training-free Long Video Generation with Chain of Diffusion Model\n  Experts","authors":"Wenhao Li, Yichao Cao, Xiu Su, Xi Lin, Shan You, Mingkai Zheng, Yi\n  Chen, Chang Xu","authorsParsed":[["Li","Wenhao",""],["Cao","Yichao",""],["Su","Xiu",""],["Lin","Xi",""],["You","Shan",""],["Zheng","Mingkai",""],["Chen","Yi",""],["Xu","Chang",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 01:33:28 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 07:12:52 GMT"},{"version":"v3","created":"Mon, 2 Sep 2024 18:02:03 GMT"}],"updateDate":"2024-09-04","timestamp":1724463208000,"abstract":"  Video generation models hold substantial potential in areas such as\nfilmmaking. However, current video diffusion models need high computational\ncosts and produce suboptimal results due to high complexity of video generation\ntask. In this paper, we propose \\textbf{ConFiner}, an efficient high-quality\nvideo generation framework that decouples video generation into easier\nsubtasks: structure \\textbf{con}trol and spatial-temporal re\\textbf{fine}ment.\nIt can generate high-quality videos with chain of off-the-shelf diffusion model\nexperts, each expert responsible for a decoupled subtask. During the\nrefinement, we introduce coordinated denoising, which can merge multiple\ndiffusion experts' capabilities into a single sampling. Furthermore, we design\nConFiner-Long framework, which can generate long coherent video with three\nconstraint strategies on ConFiner. Experimental results indicate that with only\n10\\% of the inference cost, our ConFiner surpasses representative models like\nLavie and Modelscope across all objective and subjective metrics. And\nConFiner-Long can generate high-quality and coherent videos with up to 600\nframes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}