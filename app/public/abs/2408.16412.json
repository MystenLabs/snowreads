{"id":"2408.16412","title":"Text-Enhanced Zero-Shot Action Recognition: A training-free approach","authors":"Massimo Bosetti, Shibingfeng Zhang, Benedetta Liberatori, Giacomo\n  Zara, Elisa Ricci and Paolo Rota","authorsParsed":[["Bosetti","Massimo",""],["Zhang","Shibingfeng",""],["Liberatori","Benedetta",""],["Zara","Giacomo",""],["Ricci","Elisa",""],["Rota","Paolo",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 10:20:05 GMT"}],"updateDate":"2024-09-12","timestamp":1724926805000,"abstract":"  Vision-language models (VLMs) have demonstrated remarkable performance across\nvarious visual tasks, leveraging joint learning of visual and textual\nrepresentations. While these models excel in zero-shot image tasks, their\napplication to zero-shot video action recognition (ZSVAR) remains challenging\ndue to the dynamic and temporal nature of actions. Existing methods for ZS-VAR\ntypically require extensive training on specific datasets, which can be\nresource-intensive and may introduce domain biases. In this work, we propose\nText-Enhanced Action Recognition (TEAR), a simple approach to ZS-VAR that is\ntraining-free and does not require the availability of training data or\nextensive computational resources. Drawing inspiration from recent findings in\nvision and language literature, we utilize action descriptors for decomposition\nand contextual information to enhance zero-shot action recognition. Through\nexperiments on UCF101, HMDB51, and Kinetics-600 datasets, we showcase the\neffectiveness and applicability of our proposed approach in addressing the\nchallenges of ZS-VAR.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"vFb5QnVvEKvvZN43ongO0JUMHy7htTO4isNJVLJVPNY","pdfSize":"1099946"}
