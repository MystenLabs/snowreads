{"id":"2408.07471","title":"Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization","authors":"Yuxin Jiang, Bo Huang, Yufei Wang, Xingshan Zeng, Liangyou Li, Yasheng\n  Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang","authorsParsed":[["Jiang","Yuxin",""],["Huang","Bo",""],["Wang","Yufei",""],["Zeng","Xingshan",""],["Li","Liangyou",""],["Wang","Yasheng",""],["Jiang","Xin",""],["Shang","Lifeng",""],["Tang","Ruiming",""],["Wang","Wei",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 11:29:47 GMT"}],"updateDate":"2024-08-15","timestamp":1723634987000,"abstract":"  Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}