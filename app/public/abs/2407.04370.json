{"id":"2407.04370","title":"Regulating Model Reliance on Non-Robust Features by Smoothing Input\n  Marginal Density","authors":"Peiyu Yang, Naveed Akhtar, Mubarak Shah, Ajmal Mian","authorsParsed":[["Yang","Peiyu",""],["Akhtar","Naveed",""],["Shah","Mubarak",""],["Mian","Ajmal",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 09:16:56 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 03:09:41 GMT"}],"updateDate":"2024-07-10","timestamp":1720171016000,"abstract":"  Trustworthy machine learning necessitates meticulous regulation of model\nreliance on non-robust features. We propose a framework to delineate and\nregulate such features by attributing model predictions to the input. Within\nour approach, robust feature attributions exhibit a certain consistency, while\nnon-robust feature attributions are susceptible to fluctuations. This behavior\nallows identification of correlation between model reliance on non-robust\nfeatures and smoothness of marginal density of the input samples. Hence, we\nuniquely regularize the gradients of the marginal density w.r.t. the input\nfeatures for robustness. We also devise an efficient implementation of our\nregularization to address the potential numerical instability of the underlying\noptimization process. Moreover, we analytically reveal that, as opposed to our\nmarginal density smoothing, the prevalent input gradient regularization\nsmoothens conditional or joint density of the input, which can cause limited\nrobustness. Our experiments validate the effectiveness of the proposed method,\nproviding clear evidence of its capability to address the feature leakage\nproblem and mitigate spurious correlations. Extensive results further establish\nthat our technique enables the model to exhibit robustness against\nperturbations in pixel values, input gradients, and density.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}