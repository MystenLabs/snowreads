{"id":"2408.00278","title":"High Performance Im2win and Direct Convolutions using Three Tensor\n  Layouts on SIMD Architectures","authors":"Xiang Fu, Xinpeng Zhang, Jixiang Ma, Peng Zhao, Shuai Lu, Xu T. Liu","authorsParsed":[["Fu","Xiang",""],["Zhang","Xinpeng",""],["Ma","Jixiang",""],["Zhao","Peng",""],["Lu","Shuai",""],["Liu","Xu T.",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 04:37:03 GMT"}],"updateDate":"2024-08-02","timestamp":1722487023000,"abstract":"  Convolution is the core component within deep neural networks and it is\ncomputationally intensive and time consuming. Tensor data layouts significantly\nimpact convolution operations in terms of memory access and computational\nefficiency. Yet, there is still a lack of comprehensive performance\ncharacterization on data layouts on SIMD architectures concerning convolution\nmethods. This paper proposes three novel data layouts for im2win convolution:\nNHWC, CHWN, and CHWN8, and introduces a set of general optimization techniques\nfor both direct and im2win convolutions. We compare the optimized im2win\nconvolution with the direct convolution and PyTorch's im2col-based convolution\nacross the aforementioned layouts on SIMD machines. The experiments\ndemonstrated that the im2win convolution with the new NHWC layout achieved up\nto 355% performance speedup over NCHW layout. Our optimizations also\nsignificantly improve the performance of both im2win and direct convolutions.\nOur optimized im2win and direct convolutions achieved up to 95% and 94% of\nmachine's theoretical peak performance, respectively.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}