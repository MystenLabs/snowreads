{"id":"2407.06538","title":"Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge\n  Distillation: A Case Study","authors":"Aniruddha Roy, Pretam Ray, Ayush Maheshwari, Sudeshna Sarkar, Pawan\n  Goyal","authorsParsed":[["Roy","Aniruddha",""],["Ray","Pretam",""],["Maheshwari","Ayush",""],["Sarkar","Sudeshna",""],["Goyal","Pawan",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 04:19:52 GMT"}],"updateDate":"2024-07-10","timestamp":1720498792000,"abstract":"  Neural Machine Translation (NMT) remains a formidable challenge, especially\nwhen dealing with low-resource languages. Pre-trained sequence-to-sequence\n(seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive\nperformance in various low-resource NMT tasks. However, their pre-training has\nbeen confined to 50 languages, leaving out support for numerous low-resource\nlanguages, particularly those spoken in the Indian subcontinent. Expanding\nmBART-50's language support requires complex pre-training, risking performance\ndecline due to catastrophic forgetting. Considering these expanding challenges,\nthis paper explores a framework that leverages the benefits of a pre-trained\nlanguage model along with knowledge distillation in a seq2seq architecture to\nfacilitate translation for low-resource languages, including those not covered\nby mBART-50. The proposed framework employs a multilingual encoder-based\nseq2seq model as the foundational architecture and subsequently uses\ncomplementary knowledge distillation techniques to mitigate the impact of\nimbalanced training. Our framework is evaluated on three low-resource Indic\nlanguages in four Indic-to-Indic directions, yielding significant BLEU-4 and\nchrF improvements over baselines. Further, we conduct human evaluation to\nconfirm effectiveness of our approach. Our code is publicly available at\nhttps://github.com/raypretam/Two-step-low-res-NMT.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}