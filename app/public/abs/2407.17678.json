{"id":"2407.17678","title":"Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads","authors":"Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Hao\n  Peng, Xia Song","authorsParsed":[["Lin","Xihui",""],["Zhang","Yunan",""],["Ge","Suyu",""],["Patra","Barun",""],["Chaudhary","Vishrav",""],["Peng","Hao",""],["Song","Xia",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 00:27:07 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 22:06:20 GMT"}],"updateDate":"2024-08-29","timestamp":1721867227000,"abstract":"  Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"EGj5Exo9sbUXUDPqEzVcNpcEJA6p_YJTTlm4cg8I5lg","pdfSize":"881787","objectId":"0xa3a46f205b795bb4113a53954054540f357a7230ec54e52bccf655aeb543e1f7","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
