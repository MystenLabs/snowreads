{"id":"2407.18953","title":"Evaluating Front-end & Back-end of Human Automation Interaction\n  Applications A Hypothetical Benchmark","authors":"Gon\\c{c}alo Hora de Carvalho","authorsParsed":[["de Carvalho","Gon√ßalo Hora",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 13:55:13 GMT"}],"updateDate":"2024-07-30","timestamp":1720792513000,"abstract":"  Human Factors, Cognitive Engineering, and Human-Automation Interaction (HAI)\nform a trifecta, where users and technological systems of ever increasing\nautonomous control occupy a centre position. But with great autonomy comes\ngreat responsibility. It is in this context that we propose metrics and a\nbenchmark framework based on known regimes in Artificial Intelligence (AI). A\nbenchmark is a set of tests and metrics or measurements conducted on those\ntests or tasks. We hypothesise about possible tasks designed to assess\noperator-system interactions and both the front-end and back-end components of\nHAI applications. Here, front-end pertains to the user interface and direct\ninteractions the user has with a system, while the back-end is composed of the\nunderlying processes and mechanisms that support the front-end experience. By\nevaluating HAI systems through the proposed metrics, based on Cognitive\nEngineering studies of judgment and prediction, we attempt to unify many known\ntaxonomies and design guidelines for HAI systems in a benchmark. This is\nfacilitated by providing a structured approach to quantifying the efficacy and\nreliability of these systems in a formal way, thus, designing a testable\nbenchmark capable of reproducible results.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RHn5bKrrnGUUtLrNtuWRH4ZjvuOFKRz4QQZLVGb-EkY","pdfSize":"668137"}
