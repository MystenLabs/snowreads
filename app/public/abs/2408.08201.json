{"id":"2408.08201","title":"Heavy Labels Out! Dataset Distillation with Label Space Lightening","authors":"Ruonan Yu, Songhua Liu, Zigeng Chen, Jingwen Ye, Xinchao Wang","authorsParsed":[["Yu","Ruonan",""],["Liu","Songhua",""],["Chen","Zigeng",""],["Ye","Jingwen",""],["Wang","Xinchao",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 15:08:58 GMT"}],"updateDate":"2024-08-16","timestamp":1723734538000,"abstract":"  Dataset distillation or condensation aims to condense a large-scale training\ndataset into a much smaller synthetic one such that the training performance of\ndistilled and original sets on neural networks are similar. Although the number\nof training samples can be reduced substantially, current state-of-the-art\nmethods heavily rely on enormous soft labels to achieve satisfactory\nperformance. As a result, the required storage can be comparable even to\noriginal datasets, especially for large-scale ones. To solve this problem,\ninstead of storing these heavy labels, we propose a novel label-lightening\nframework termed HeLlO aiming at effective image-to-label projectors, with\nwhich synthetic labels can be directly generated online from synthetic images.\nSpecifically, to construct such projectors, we leverage prior knowledge in\nopen-source foundation models, e.g., CLIP, and introduce a LoRA-like\nfine-tuning strategy to mitigate the gap between pre-trained and target\ndistributions, so that original models for soft-label generation can be\ndistilled into a group of low-rank matrices. Moreover, an effective image\noptimization method is proposed to further mitigate the potential error between\nthe original and distilled label generators. Extensive experiments demonstrate\nthat with only about 0.003% of the original storage required for a complete set\nof soft labels, we achieve comparable performance to current state-of-the-art\ndataset distillation methods on large-scale datasets. Our code will be\navailable.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}