{"id":"2408.14862","title":"Leveraging Self-supervised Audio Representations for Data-Efficient\n  Acoustic Scene Classification","authors":"Yiqiang Cai, Shengchen Li, Xi Shao","authorsParsed":[["Cai","Yiqiang",""],["Li","Shengchen",""],["Shao","Xi",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 08:33:26 GMT"}],"updateDate":"2024-08-28","timestamp":1724747606000,"abstract":"  Acoustic scene classification (ASC) predominantly relies on supervised\napproaches. However, acquiring labeled data for training ASC models is often\ncostly and time-consuming. Recently, self-supervised learning (SSL) has emerged\nas a powerful method for extracting features from unlabeled audio data,\nbenefiting many downstream audio tasks. This paper proposes a data-efficient\nand low-complexity ASC system by leveraging self-supervised audio\nrepresentations extracted from general-purpose audio datasets. We introduce\nBEATs, an audio SSL pre-trained model, to extract the general representations\nfrom AudioSet. Through extensive experiments, it has been demonstrated that the\nself-supervised audio representations can help to achieve high ASC accuracy\nwith limited labeled fine-tuning data. Furthermore, we find that ensembling the\nSSL models fine-tuned with different strategies contributes to a further\nperformance improvement. To meet low-complexity requirements, we use knowledge\ndistillation to transfer the self-supervised knowledge from large teacher\nmodels to an efficient student model. The experimental results suggest that the\nself-supervised teachers effectively improve the classification accuracy of the\nstudent model. Our best-performing system obtains an average accuracy of 56.7%.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}