{"id":"2407.14679","title":"Compact Language Models via Pruning and Knowledge Distillation","authors":"Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi,\n  Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan\n  Kautz, Pavlo Molchanov","authorsParsed":[["Muralidharan","Saurav",""],["Sreenivas","Sharath Turuvekere",""],["Joshi","Raviraj",""],["Chochowski","Marcin",""],["Patwary","Mostofa",""],["Shoeybi","Mohammad",""],["Catanzaro","Bryan",""],["Kautz","Jan",""],["Molchanov","Pavlo",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 21:47:57 GMT"}],"updateDate":"2024-07-23","timestamp":1721425677000,"abstract":"  Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"QNDGX80gNruF-PfLl7e1qDSMGh-ziSH38Yj7l8EgCK4","pdfSize":"1550039"}
