{"id":"2408.03193","title":"Efficient NeRF Optimization -- Not All Samples Remain Equally Hard","authors":"Juuso Korhonen, Goutham Rangu, Hamed R. Tavakoli, Juho Kannala","authorsParsed":[["Korhonen","Juuso",""],["Rangu","Goutham",""],["Tavakoli","Hamed R.",""],["Kannala","Juho",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 13:49:01 GMT"}],"updateDate":"2024-08-07","timestamp":1722952141000,"abstract":"  We propose an application of online hard sample mining for efficient training\nof Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality\nfor many 3D reconstruction and rendering tasks but require substantial\ncomputational resources. The encoding of the scene information within the NeRF\nnetwork parameters necessitates stochastic sampling. We observe that during the\ntraining, a major part of the compute time and memory usage is spent on\nprocessing already learnt samples, which no longer affect the model update\nsignificantly. We identify the backward pass on the stochastic samples as the\ncomputational bottleneck during the optimization. We thus perform the first\nforward pass in inference mode as a relatively low-cost search for hard\nsamples. This is followed by building the computational graph and updating the\nNeRF network parameters using only the hard samples. To demonstrate the\neffectiveness of the proposed approach, we apply our method to Instant-NGP,\nresulting in significant improvements of the view-synthesis quality over the\nbaseline (1 dB improvement on average per training time, or 2x speedup to reach\nthe same PSNR level) along with approx. 40% memory savings coming from using\nonly the hard samples to build the computational graph. As our method only\ninterfaces with the network module, we expect it to be widely applicable.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}