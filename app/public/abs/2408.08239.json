{"id":"2408.08239","title":"Strong Data Processing Inequalities and their Applications to Reliable\n  Computation","authors":"Andrew K. Yang","authorsParsed":[["Yang","Andrew K.",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 16:06:44 GMT"}],"updateDate":"2024-08-16","timestamp":1723738004000,"abstract":"  In 1952, von Neumann gave a series of groundbreaking lectures that proved it\nwas possible for circuits consisting of 3-input majority gates that have a\nsufficiently small independent probability $\\delta > 0$ of malfunctioning to\nreliably compute Boolean functions. In 1999, Evans and Schulman used a strong\ndata-processing inequality (SDPI) to establish the tightest known necessary\ncondition $\\delta < \\frac{1}{2} - \\frac{1}{2\\sqrt{k}}$ for reliable computation\nwhen the circuit consists of components that have at most $k$ inputs. In 2017,\nPolyanskiy and Wu distilled Evans and Schulman's SDPI argument to establish a\ngeneral result on the contraction of mutual information in Bayesian networks.\n  In this essay, we will first introduce the problem of reliable computation\nfrom unreliable components and establish the existence of noise thresholds. We\nwill then provide an exposition of von Neumann's result with 3-input majority\ngates and extend it to minority gates. We will then provide an introduction to\nSDPIs, which have many applications, including in statistical mechanics,\nportfolio theory, and lower bounds on statistical estimation under privacy\nconstraints. We will then use the introduced material to provide an exposition\nof Polyanskiy and Wu's 2017 result on Bayesian networks, from which the 1999\nresult of Evans-Schulman follows.\n","subjects":["Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"r0qp5SP0D4d2PH5ZRxjo3xx1Uj5wbnQT3LI7KZ2EOjc","pdfSize":"1395851"}
