{"id":"2408.05711","title":"Contrastive masked auto-encoders based self-supervised hashing for 2D\n  image and 3D point cloud cross-modal retrieval","authors":"Rukai Wei, Heng Cui, Yu Liu, Yufeng Hou, Yanzhao Xie, Ke Zhou","authorsParsed":[["Wei","Rukai",""],["Cui","Heng",""],["Liu","Yu",""],["Hou","Yufeng",""],["Xie","Yanzhao",""],["Zhou","Ke",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 07:03:21 GMT"}],"updateDate":"2024-08-13","timestamp":1723359801000,"abstract":"  Implementing cross-modal hashing between 2D images and 3D point-cloud data is\na growing concern in real-world retrieval systems. Simply applying existing\ncross-modal approaches to this new task fails to adequately capture latent\nmulti-modal semantics and effectively bridge the modality gap between 2D and\n3D. To address these issues without relying on hand-crafted labels, we propose\ncontrastive masked autoencoders based self-supervised hashing (CMAH) for\nretrieval between images and point-cloud data. We start by contrasting 2D-3D\npairs and explicitly constraining them into a joint Hamming space. This\ncontrastive learning process ensures robust discriminability for the generated\nhash codes and effectively reduces the modality gap. Moreover, we utilize\nmulti-modal auto-encoders to enhance the model's understanding of multi-modal\nsemantics. By completing the masked image/point-cloud data modeling task, the\nmodel is encouraged to capture more localized clues. In addition, the proposed\nmulti-modal fusion block facilitates fine-grained interactions among different\nmodalities. Extensive experiments on three public datasets demonstrate that the\nproposed CMAH significantly outperforms all baseline methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}