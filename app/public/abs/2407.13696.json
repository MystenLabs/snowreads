{"id":"2407.13696","title":"Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with\n  BenchBench","authors":"Yotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, Elron Bandel,\n  Eyal Shnarch, Michal Shmueli-Scheuer, Leshem Choshen","authorsParsed":[["Perlitz","Yotam",""],["Gera","Ariel",""],["Arviv","Ofir",""],["Yehudai","Asaf",""],["Bandel","Elron",""],["Shnarch","Eyal",""],["Shmueli-Scheuer","Michal",""],["Choshen","Leshem",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:00:23 GMT"},{"version":"v2","created":"Thu, 12 Sep 2024 08:36:47 GMT"}],"updateDate":"2024-09-13","timestamp":1721322023000,"abstract":"  Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: github.com/IBM/BenchBench\n  Leaderboard: hf.co/spaces/IBM/BenchBench\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"IwkbWgpN-MPe9w4or95MGJrsFVJAAl_WQIBSYJW1eRg","pdfSize":"4441047"}
