{"id":"2407.03995","title":"ROER: Regularized Optimal Experience Replay","authors":"Changling Li, Zhang-Wei Hong, Pulkit Agrawal, Divyansh Garg, Joni\n  Pajarinen","authorsParsed":[["Li","Changling",""],["Hong","Zhang-Wei",""],["Agrawal","Pulkit",""],["Garg","Divyansh",""],["Pajarinen","Joni",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 15:14:57 GMT"}],"updateDate":"2024-09-20","timestamp":1720106097000,"abstract":"  Experience replay serves as a key component in the success of online\nreinforcement learning (RL). Prioritized experience replay (PER) reweights\nexperiences by the temporal difference (TD) error empirically enhancing the\nperformance. However, few works have explored the motivation of using TD error.\nIn this work, we provide an alternative perspective on TD-error-based\nreweighting. We show the connections between the experience prioritization and\noccupancy optimization. By using a regularized RL objective with $f-$divergence\nregularizer and employing its dual form, we show that an optimal solution to\nthe objective is obtained by shifting the distribution of off-policy data in\nthe replay buffer towards the on-policy optimal distribution using\nTD-error-based occupancy ratios. Our derivation results in a new pipeline of TD\nerror prioritization. We specifically explore the KL divergence as the\nregularizer and obtain a new form of prioritization scheme, the regularized\noptimal experience replay (ROER). We evaluate the proposed prioritization\nscheme with the Soft Actor-Critic (SAC) algorithm in continuous control MuJoCo\nand DM Control benchmark tasks where our proposed scheme outperforms baselines\nin 6 out of 11 tasks while the results of the rest match with or do not deviate\nfar from the baselines. Further, using pretraining, ROER achieves noticeable\nimprovement on difficult Antmaze environment where baselines fail, showing\napplicability to offline-to-online fine-tuning. Code is available at\n\\url{https://github.com/XavierChanglingLi/Regularized-Optimal-Experience-Replay}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}