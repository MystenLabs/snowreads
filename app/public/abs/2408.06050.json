{"id":"2408.06050","title":"What Ails Generative Structure-based Drug Design: Too Little or Too Much\n  Expressivity?","authors":"Rafa{\\l} Karczewski, Samuel Kaski, Markus Heinonen, Vikas Garg","authorsParsed":[["Karczewski","Rafa≈Ç",""],["Kaski","Samuel",""],["Heinonen","Markus",""],["Garg","Vikas",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 10:55:29 GMT"}],"updateDate":"2024-08-13","timestamp":1723460129000,"abstract":"  Several generative models with elaborate training and sampling procedures\nhave been proposed recently to accelerate structure-based drug design (SBDD);\nhowever, perplexingly, their empirical performance turns out to be suboptimal.\nWe seek to better understand this phenomenon from both theoretical and\nempirical perspectives. Since most of these models apply graph neural networks\n(GNNs), one may suspect that they inherit the representational limitations of\nGNNs. We analyze this aspect, establishing the first such results for\nprotein-ligand complexes. A plausible counterview may attribute the\nunderperformance of these models to their excessive parameterizations, inducing\nexpressivity at the expense of generalization. We also investigate this\npossibility with a simple metric-aware approach that learns an economical\nsurrogate for affinity to infer an unlabelled molecular graph and optimizes for\nlabels conditioned on this graph and molecular properties. The resulting model\nachieves state-of-the-art results using 100x fewer trainable parameters and\naffords up to 1000x speedup. Collectively, our findings underscore the need to\nreassess and redirect the existing paradigm and efforts for SBDD.\n","subjects":["Computing Research Repository/Machine Learning","Quantitative Biology/Biomolecules"],"license":"http://creativecommons.org/licenses/by/4.0/"}