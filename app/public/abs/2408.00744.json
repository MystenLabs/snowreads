{"id":"2408.00744","title":"Collaborative Vision-Text Representation Optimizing for Open-Vocabulary\n  Segmentation","authors":"Siyu Jiao, Hongguang Zhu, Jiannan Huang, Yao Zhao, Yunchao Wei,\n  Humphrey Shi","authorsParsed":[["Jiao","Siyu",""],["Zhu","Hongguang",""],["Huang","Jiannan",""],["Zhao","Yao",""],["Wei","Yunchao",""],["Shi","Humphrey",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:48:08 GMT"}],"updateDate":"2024-08-02","timestamp":1722534488000,"abstract":"  Pre-trained vision-language models, e.g. CLIP, have been increasingly used to\naddress the challenging Open-Vocabulary Segmentation (OVS) task, benefiting\nfrom their well-aligned vision-text embedding space. Typical solutions involve\neither freezing CLIP during training to unilaterally maintain its zero-shot\ncapability, or fine-tuning CLIP vision encoder to achieve perceptual\nsensitivity to local regions. However, few of them incorporate vision-text\ncollaborative optimization. Based on this, we propose the Content-Dependent\nTransfer to adaptively enhance each text embedding by interacting with the\ninput image, which presents a parameter-efficient way to optimize the text\nrepresentation. Besides, we additionally introduce a Representation\nCompensation strategy, reviewing the original CLIP-V representation as\ncompensation to maintain the zero-shot capability of CLIP. In this way, the\nvision and text representation of CLIP are optimized collaboratively, enhancing\nthe alignment of the vision-text feature space. To the best of our knowledge,\nwe are the first to establish the collaborative vision-text optimizing\nmechanism within the OVS field. Extensive experiments demonstrate our method\nachieves superior performance on popular OVS benchmarks. In open-vocabulary\nsemantic segmentation, our method outperforms the previous state-of-the-art\napproaches by +0.5, +2.3, +3.4, +0.4 and +1.1 mIoU, respectively on A-847,\nA-150, PC-459, PC-59 and PAS-20. Furthermore, in a panoptic setting on ADE20K,\nwe achieve the performance of 27.1 PQ, 73.5 SQ, and 32.9 RQ. Code will be\navailable at https://github.com/jiaosiyu1999/MAFT-Plus.git .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PugDSDIwcy6JfTjRP31oWVqT3eDbVLPuPJt6HWUHWcU","pdfSize":"10796764","txDigest":"82T2xHjxJrZcW7uN7mE7fyzk8MtGaYFPVeN7QyNsbcX","endEpoch":"1","status":"CERTIFIED"}
