{"id":"2407.12808","title":"Towards Optimal Trade-offs in Knowledge Distillation for CNNs and Vision\n  Transformers at the Edge","authors":"John Violos, Symeon Papadopoulos, Ioannis Kompatsiaris","authorsParsed":[["Violos","John",""],["Papadopoulos","Symeon",""],["Kompatsiaris","Ioannis",""]],"versions":[{"version":"v1","created":"Tue, 25 Jun 2024 16:15:02 GMT"}],"updateDate":"2024-07-19","timestamp":1719332102000,"abstract":"  This paper discusses four facets of the Knowledge Distillation (KD) process\nfor Convolutional Neural Networks (CNNs) and Vision Transformer (ViT)\narchitectures, particularly when executed on edge devices with constrained\nprocessing capabilities. First, we conduct a comparative analysis of the KD\nprocess between CNNs and ViT architectures, aiming to elucidate the feasibility\nand efficacy of employing different architectural configurations for the\nteacher and student, while assessing their performance and efficiency. Second,\nwe explore the impact of varying the size of the student model on accuracy and\ninference speed, while maintaining a constant KD duration. Third, we examine\nthe effects of employing higher resolution images on the accuracy, memory\nfootprint and computational workload. Last, we examine the performance\nimprovements obtained by fine-tuning the student model after KD to specific\ndownstream tasks. Through empirical evaluations and analyses, this research\nprovides AI practitioners with insights into optimal strategies for maximizing\nthe effectiveness of the KD process on edge devices.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rB3SRVDrPtxj7pvDg6wIdCfplXCpFFVh_j9o6qDUtNs","pdfSize":"100402","objectId":"0xec5324beec73354d5b691f131431eaa6ad837daedaaaf479ec6afd3dd25cee08","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
