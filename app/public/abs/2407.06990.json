{"id":"2407.06990","title":"Segment-Based Interactive Machine Translation for Pre-trained Models","authors":"Angel Navarro and Francisco Casacuberta","authorsParsed":[["Navarro","Angel",""],["Casacuberta","Francisco",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 16:04:21 GMT"}],"updateDate":"2024-07-10","timestamp":1720541061000,"abstract":"  Pre-trained large language models (LLM) are starting to be widely used in\nmany applications. In this work, we explore the use of these models in\ninteractive machine translation (IMT) environments. In particular, we have\nchosen mBART (multilingual Bidirectional and Auto-Regressive Transformer) and\nmT5 (multilingual Text-to-Text Transfer Transformer) as the LLMs to perform our\nexperiments. The system generates perfect translations interactively using the\nfeedback provided by the user at each iteration. The Neural Machine Translation\n(NMT) model generates a preliminary hypothesis with the feedback, and the user\nvalidates new correct segments and performs a word correction--repeating the\nprocess until the sentence is correctly translated. We compared the performance\nof mBART, mT5, and a state-of-the-art (SoTA) machine translation model on a\nbenchmark dataset regarding user effort, Word Stroke Ratio (WSR), Key Stroke\nRatio (KSR), and Mouse Action Ratio (MAR). The experimental results indicate\nthat mBART performed comparably with SoTA models, suggesting that it is a\nviable option for this field of IMT. The implications of this finding extend to\nthe development of new machine translation models for interactive environments,\nas it indicates that some novel pre-trained models exhibit SoTA performance in\nthis domain, highlighting the potential benefits of adapting these models to\nspecific needs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}