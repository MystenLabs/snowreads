{"id":"2408.12317","title":"Adapt CLIP as Aggregation Instructor for Image Dehazing","authors":"Xiaozhe Zhang, Fengying Xie, Haidong Ding, Linpeng Pan, Zhenwei Shi","authorsParsed":[["Zhang","Xiaozhe",""],["Xie","Fengying",""],["Ding","Haidong",""],["Pan","Linpeng",""],["Shi","Zhenwei",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 11:51:50 GMT"}],"updateDate":"2024-08-23","timestamp":1724327510000,"abstract":"  Most dehazing methods suffer from limited receptive field and do not explore\nthe rich semantic prior encapsulated in vision-language models, which have\nproven effective in downstream tasks. In this paper, we introduce CLIPHaze, a\npioneering hybrid framework that synergizes the efficient global modeling of\nMamba with the prior knowledge and zero-shot capabilities of CLIP to address\nboth issues simultaneously. Specifically, our method employs parallel state\nspace model and window-based self-attention to obtain global contextual\ndependency and local fine-grained perception, respectively. To seamlessly\naggregate information from both paths, we introduce CLIP-instructed Aggregation\nModule (CAM). For non-homogeneous and homogeneous haze, CAM leverages zero-shot\nestimated haze density map and high-quality image embedding without degradation\ninformation to explicitly and implicitly determine the optimal neural operation\nrange for each pixel, thereby adaptively fusing two paths with different\nreceptive fields. Extensive experiments on various benchmarks demonstrate that\nCLIPHaze achieves state-of-the-art (SOTA) performance, particularly in\nnon-homogeneous haze. Code will be publicly after acceptance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}