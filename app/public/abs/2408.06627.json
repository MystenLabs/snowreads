{"id":"2408.06627","title":"WorldScribe: Towards Context-Aware Live Visual Descriptions","authors":"Ruei-Che Chang, Yuxuan Liu, Anhong Guo","authorsParsed":[["Chang","Ruei-Che",""],["Liu","Yuxuan",""],["Guo","Anhong",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 04:32:45 GMT"}],"updateDate":"2024-08-14","timestamp":1723523565000,"abstract":"  Automated live visual descriptions can aid blind people in understanding\ntheir surroundings with autonomy and independence. However, providing\ndescriptions that are rich, contextual, and just-in-time has been a\nlong-standing challenge in accessibility. In this work, we develop WorldScribe,\na system that generates automated live real-world visual descriptions that are\ncustomizable and adaptive to users' contexts: (i) WorldScribe's descriptions\nare tailored to users' intents and prioritized based on semantic relevance.\n(ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively\nsuccinct descriptions for dynamic scenes, while presenting longer and detailed\nones for stable settings. (iii) WorldScribe is adaptive to sound contexts,\ne.g., increasing volume in noisy environments, or pausing when conversations\nstart. Powered by a suite of vision, language, and sound recognition models,\nWorldScribe introduces a description generation pipeline that balances the\ntradeoffs between their richness and latency to support real-time use. The\ndesign of WorldScribe is informed by prior work on providing visual\ndescriptions and a formative study with blind participants. Our user study and\nsubsequent pipeline evaluation show that WorldScribe can provide real-time and\nfairly accurate visual descriptions to facilitate environment understanding\nthat is adaptive and customized to users' contexts. Finally, we discuss the\nimplications and further steps toward making live visual descriptions more\ncontext-aware and humanized.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}