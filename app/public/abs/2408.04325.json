{"id":"2408.04325","title":"HydraFormer: One Encoder For All Subsampling Rates","authors":"Yaoxun Xu, Xingchen Song, Zhiyong Wu, Di Wu, Zhendong Peng, Binbin\n  Zhang","authorsParsed":[["Xu","Yaoxun",""],["Song","Xingchen",""],["Wu","Zhiyong",""],["Wu","Di",""],["Peng","Zhendong",""],["Zhang","Binbin",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 09:08:27 GMT"}],"updateDate":"2024-08-09","timestamp":1723108107000,"abstract":"  In automatic speech recognition, subsampling is essential for tackling\ndiverse scenarios. However, the inadequacy of a single subsampling rate to\naddress various real-world situations often necessitates training and deploying\nmultiple models, consequently increasing associated costs. To address this\nissue, we propose HydraFormer, comprising HydraSub, a Conformer-based encoder,\nand a BiTransformer-based decoder. HydraSub encompasses multiple branches, each\nrepresenting a distinct subsampling rate, allowing for the flexible selection\nof any branch during inference based on the specific use case. HydraFormer can\nefficiently manage different subsampling rates, significantly reducing training\nand deployment expenses. Experiments on AISHELL-1 and LibriSpeech datasets\nreveal that HydraFormer effectively adapts to various subsampling rates and\nlanguages while maintaining high recognition performance. Additionally,\nHydraFormer showcases exceptional stability, sustaining consistent performance\nunder various initialization conditions, and exhibits robust transferability by\nlearning from pretrained single subsampling rate automatic speech recognition\nmodels\\footnote{Model code and scripts:\nhttps://github.com/HydraFormer/hydraformer}.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}