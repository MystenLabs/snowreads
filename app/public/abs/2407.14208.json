{"id":"2407.14208","title":"Memory-Efficient Pseudo-Labeling for Online Source-Free Universal Domain\n  Adaptation using a Gaussian Mixture Model","authors":"Pascal Schlachter, Simon Wagner, Bin Yang","authorsParsed":[["Schlachter","Pascal",""],["Wagner","Simon",""],["Yang","Bin",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 11:13:31 GMT"}],"updateDate":"2024-07-22","timestamp":1721387611000,"abstract":"  In practice, domain shifts are likely to occur between training and test\ndata, necessitating domain adaptation (DA) to adjust the pre-trained source\nmodel to the target domain. Recently, universal domain adaptation (UniDA) has\ngained attention for addressing the possibility of an additional category\n(label) shift between the source and target domain. This means new classes can\nappear in the target data, some source classes may no longer be present, or\nboth at the same time. For practical applicability, UniDA methods must handle\nboth source-free and online scenarios, enabling adaptation without access to\nthe source data and performing batch-wise updates in parallel with prediction.\nIn an online setting, preserving knowledge across batches is crucial. However,\nexisting methods often require substantial memory, e.g. by using memory queues,\nwhich is impractical because memory is limited and valuable, in particular on\nembedded systems. Therefore, we consider memory-efficiency as an additional\nconstraint in this paper. To achieve memory-efficient online source-free\nuniversal domain adaptation (SF-UniDA), we propose a novel method that\ncontinuously captures the distribution of known classes in the feature space\nusing a Gaussian mixture model (GMM). This approach, combined with\nentropy-based out-of-distribution detection, allows for the generation of\nreliable pseudo-labels. Finally, we combine a contrastive loss with a KL\ndivergence loss to perform the adaptation. Our approach not only achieves\nstate-of-the-art results in all experiments on the DomainNet dataset but also\nsignificantly outperforms the existing methods on the challenging VisDA-C\ndataset, setting a new benchmark for online SF-UniDA. Our code is available at\nhttps://github.com/pascalschlachter/GMM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}