{"id":"2408.07414","title":"WavLM model ensemble for audio deepfake detection","authors":"David Combei, Adriana Stan, Dan Oneata, Horia Cucu","authorsParsed":[["Combei","David",""],["Stan","Adriana",""],["Oneata","Dan",""],["Cucu","Horia",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 09:43:35 GMT"}],"updateDate":"2024-08-15","timestamp":1723628615000,"abstract":"  Audio deepfake detection has become a pivotal task over the last couple of\nyears, as many recent speech synthesis and voice cloning systems generate\nhighly realistic speech samples, thus enabling their use in malicious\nactivities. In this paper we address the issue of audio deepfake detection as\nit was set in the ASVspoof5 challenge. First, we benchmark ten types of\npretrained representations and show that the self-supervised representations\nstemming from the wav2vec2 and wavLM families perform best. Of the two, wavLM\nis better when restricting the pretraining data to LibriSpeech, as required by\nthe challenge rules. To further improve performance, we finetune the wavLM\nmodel for the deepfake detection task. We extend the ASVspoof5 dataset with\nsamples from other deepfake detection datasets and apply data augmentation. Our\nfinal challenge submission consists of a late fusion combination of four models\nand achieves an equal error rate of 6.56% and 17.08% on the two evaluation\nsets.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}