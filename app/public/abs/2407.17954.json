{"id":"2407.17954","title":"Scaling Training Data with Lossy Image Compression","authors":"Katherine L. Mentzer and Andrea Montanari","authorsParsed":[["Mentzer","Katherine L.",""],["Montanari","Andrea",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 11:19:55 GMT"}],"updateDate":"2024-07-26","timestamp":1721906395000,"abstract":"  Empirically-determined scaling laws have been broadly successful in\npredicting the evolution of large machine learning models with training data\nand number of parameters. As a consequence, they have been useful for\noptimizing the allocation of limited resources, most notably compute time.\n  In certain applications, storage space is an important constraint, and data\nformat needs to be chosen carefully as a consequence. Computer vision is a\nprominent example: images are inherently analog, but are always stored in a\ndigital format using a finite number of bits. Given a dataset of digital\nimages, the number of bits $L$ to store each of them can be further reduced\nusing lossy data compression. This, however, can degrade the quality of the\nmodel trained on such images, since each example has lower resolution.\n  In order to capture this trade-off and optimize storage of training data, we\npropose a `storage scaling law' that describes the joint evolution of test\nerror with sample size and number of bits per image. We prove that this law\nholds within a stylized model for image compression, and verify it empirically\non two computer vision tasks, extracting the relevant parameters. We then show\nthat this law can be used to optimize the lossy compression level. At given\nstorage, models trained on optimally compressed images present a significantly\nsmaller test error with respect to models trained on the original data.\nFinally, we investigate the potential benefits of randomizing the compression\nlevel.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Information Theory","Computing Research Repository/Machine Learning","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}