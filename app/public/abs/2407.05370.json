{"id":"2407.05370","title":"Learning Label Refinement and Threshold Adjustment for Imbalanced\n  Semi-Supervised Learning","authors":"Zeju Li, Ying-Qiu Zheng, Chen Chen, Saad Jbabdi","authorsParsed":[["Li","Zeju",""],["Zheng","Ying-Qiu",""],["Chen","Chen",""],["Jbabdi","Saad",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 13:46:22 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 01:49:31 GMT"}],"updateDate":"2024-09-18","timestamp":1720359982000,"abstract":"  Semi-supervised learning (SSL) algorithms struggle to perform well when\nexposed to imbalanced training data. In this scenario, the generated\npseudo-labels can exhibit a bias towards the majority class, and models that\nemploy these pseudo-labels can further amplify this bias. Here we investigate\npseudo-labeling strategies for imbalanced SSL including pseudo-label refinement\nand threshold adjustment, through the lens of statistical analysis. We find\nthat existing SSL algorithms which generate pseudo-labels using heuristic\nstrategies or uncalibrated model confidence are unreliable when imbalanced\nclass distributions bias pseudo-labels. To address this, we introduce\nSEmi-supervised learning with pseudo-label optimization based on VALidation\ndata (SEVAL) to enhance the quality of pseudo-labelling for imbalanced SSL. We\npropose to learn refinement and thresholding parameters from a partition of the\ntraining dataset in a class-balanced way. SEVAL adapts to specific tasks with\nimproved pseudo-labels accuracy and ensures pseudo-labels correctness on a\nper-class basis. Our experiments show that SEVAL surpasses state-of-the-art SSL\nmethods, delivering more accurate and effective pseudo-labels in various\nimbalanced SSL situations. SEVAL, with its simplicity and flexibility, can\nenhance various SSL techniques effectively. The code is publicly available\n(https://github.com/ZerojumpLine/SEVAL).\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}