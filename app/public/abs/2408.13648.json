{"id":"2408.13648","title":"Explanatory Model Monitoring to Understand the Effects of Feature Shifts\n  on Performance","authors":"Thomas Decker, Alexander Koebler, Michael Lebacher, Ingo Thon, Volker\n  Tresp, Florian Buettner","authorsParsed":[["Decker","Thomas",""],["Koebler","Alexander",""],["Lebacher","Michael",""],["Thon","Ingo",""],["Tresp","Volker",""],["Buettner","Florian",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 18:28:19 GMT"}],"updateDate":"2024-08-27","timestamp":1724524099000,"abstract":"  Monitoring and maintaining machine learning models are among the most\ncritical challenges in translating recent advances in the field into real-world\napplications. However, current monitoring methods lack the capability of\nprovide actionable insights answering the question of why the performance of a\nparticular model really degraded. In this work, we propose a novel approach to\nexplain the behavior of a black-box model under feature shifts by attributing\nan estimated performance change to interpretable input characteristics. We\nrefer to our method that combines concepts from Optimal Transport and Shapley\nValues as Explanatory Performance Estimation (XPE). We analyze the underlying\nassumptions and demonstrate the superiority of our approach over several\nbaselines on different data sets across various data modalities such as images,\naudio, and tabular data. We also indicate how the generated results can lead to\nvaluable insights, enabling explanatory model monitoring by revealing potential\nroot causes for model deterioration and guiding toward actionable\ncountermeasures.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}