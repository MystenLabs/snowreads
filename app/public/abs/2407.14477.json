{"id":"2407.14477","title":"Data-Centric Human Preference Optimization with Rationales","authors":"Hoang Anh Just, Ming Jin, Anit Sahu, Huy Phan, Ruoxi Jia","authorsParsed":[["Just","Hoang Anh",""],["Jin","Ming",""],["Sahu","Anit",""],["Phan","Huy",""],["Jia","Ruoxi",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:27:52 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 02:10:12 GMT"},{"version":"v3","created":"Sat, 3 Aug 2024 17:32:08 GMT"}],"updateDate":"2024-08-06","timestamp":1721410072000,"abstract":"  Reinforcement learning from human feedback plays a crucial role in aligning\nlanguage models towards human preferences, traditionally represented through\ncomparisons between pairs or sets of responses within a given context. While\nmany studies have enhanced algorithmic techniques to optimize learning from\nsuch data, this work shifts focus to improving preference learning through a\ndata-centric approach. Specifically, we propose enriching existing preference\ndatasets with machine-generated rationales that explain the reasons behind\nchoices. We develop a simple and principled framework to augment current\npreference learning methods with rationale information. Our comprehensive\nanalysis highlights how rationales enhance learning efficiency. Extensive\nexperiments reveal that rationale-enriched preference learning offers multiple\nadvantages: it improves data efficiency, accelerates convergence to\nhigher-performing models, and reduces verbosity bias and hallucination.\nFurthermore, this framework is versatile enough to integrate with various\npreference optimization algorithms. Overall, our findings highlight the\npotential of re-imagining data design for preference learning, demonstrating\nthat even freely available machine-generated rationales can significantly boost\nperformance across multiple dimensions. The code repository is available at\nhttps: //github.com/reds-lab/preference-learning-with-rationales\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}