{"id":"2408.09101","title":"Heterogeneity-Aware Memory Efficient Federated Learning via Progressive\n  Layer Freezing","authors":"Wu Yebo, Li Li, Tian Chunlin, Chang Tao, Lin Chi, Wang Cong, Xu\n  Cheng-Zhong","authorsParsed":[["Yebo","Wu",""],["Li","Li",""],["Chunlin","Tian",""],["Tao","Chang",""],["Chi","Lin",""],["Cong","Wang",""],["Cheng-Zhong","Xu",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 05:12:09 GMT"}],"updateDate":"2024-08-20","timestamp":1723871529000,"abstract":"  In this paper, we propose SmartFreeze, a framework that effectively reduces\nthe memory footprint by conducting the training in a progressive manner.\nInstead of updating the full model in each training round, SmartFreeze divides\nthe shared model into blocks consisting of a specified number of layers. It\nfirst trains the front block with a well-designed output module, safely freezes\nit after convergence, and then triggers the training of the next one. This\nprocess iterates until the whole model has been successfully trained. In this\nway, the backward computation of the frozen blocks and the corresponding memory\nspace for storing the intermediate outputs and gradients are effectively saved.\nExcept for the progressive training framework, SmartFreeze consists of the\nfollowing two core components: a pace controller and a participant selector.\nThe pace controller is designed to effectively monitor the training progress of\neach block at runtime and safely freezes them after convergence while the\nparticipant selector selects the right devices to participate in the training\nfor each block by jointly considering the memory capacity, the statistical and\nsystem heterogeneity. Extensive experiments are conducted to evaluate the\neffectiveness of SmartFreeze on both simulation and hardware testbeds. The\nresults demonstrate that SmartFreeze effectively reduces average memory usage\nby up to 82%. Moreover, it simultaneously improves the model accuracy by up to\n83.1% and accelerates the training process up to 2.02X.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}