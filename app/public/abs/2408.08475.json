{"id":"2408.08475","title":"Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy","authors":"Mary Anne Smart, Priyanka Nanayakkara, Rachel Cummings, Gabriel\n  Kaptchuk, Elissa Redmiles","authorsParsed":[["Smart","Mary Anne",""],["Nanayakkara","Priyanka",""],["Cummings","Rachel",""],["Kaptchuk","Gabriel",""],["Redmiles","Elissa",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 01:21:57 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 01:04:07 GMT"}],"updateDate":"2024-08-20","timestamp":1723771317000,"abstract":"  Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/"}