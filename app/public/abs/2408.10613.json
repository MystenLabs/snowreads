{"id":"2408.10613","title":"Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval","authors":"Guangyuan Ma, Yongliang Ma, Xing Wu, Zhenpeng Su, Ming Zhou, Songlin\n  Hu","authorsParsed":[["Ma","Guangyuan",""],["Ma","Yongliang",""],["Wu","Xing",""],["Su","Zhenpeng",""],["Zhou","Ming",""],["Hu","Songlin",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 07:48:19 GMT"}],"updateDate":"2024-08-21","timestamp":1724140099000,"abstract":"  Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably leads to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}