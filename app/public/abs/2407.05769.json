{"id":"2407.05769","title":"Boosting 3D Object Detection with Semantic-Aware Multi-Branch Framework","authors":"Hao Jing, Anhong Wang, Lijun Zhao, Yakun Yang, Donghan Bu, Jing Zhang,\n  Yifan Zhang, Junhui Hou","authorsParsed":[["Jing","Hao",""],["Wang","Anhong",""],["Zhao","Lijun",""],["Yang","Yakun",""],["Bu","Donghan",""],["Zhang","Jing",""],["Zhang","Yifan",""],["Hou","Junhui",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 09:25:45 GMT"}],"updateDate":"2024-07-09","timestamp":1720430745000,"abstract":"  In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds,\nproviding reliable geometric information. However, traditional sampling methods\nof preprocessing often ignore semantic features, leading to detail loss and\nground point interference in 3D object detection. To address this, we propose a\nmulti-branch two-stage 3D object detection framework using a Semantic-aware\nMulti-branch Sampling (SMS) module and multi-view consistency constraints. The\nSMS module includes random sampling, Density Equalization Sampling (DES) for\nenhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on\nnon-ground points. The sampled multi-view points are processed through a\nConsistent KeyPoint Selection (CKPS) module to generate consistent keypoint\nmasks for efficient proposal sampling. The first-stage detector uses\nmulti-branch parallel learning with multi-view consistency loss for feature\naggregation, while the second-stage detector fuses multi-view data through a\nMulti-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The\nexperimental results on KITTI 3D object detection benchmark dataset show that\nour method achieves excellent detection performance improvement for a variety\nof backbones, especially for low-performance backbones with the simple network\nstructures.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}