{"id":"2408.00712","title":"MotionFix: Text-Driven 3D Human Motion Editing","authors":"Nikos Athanasiou, Alp\\'ar Ceske, Markos Diomataris, Michael J. Black,\n  G\\\"ul Varol","authorsParsed":[["Athanasiou","Nikos",""],["Ceske","Alpár",""],["Diomataris","Markos",""],["Black","Michael J.",""],["Varol","Gül",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 16:58:50 GMT"},{"version":"v2","created":"Thu, 19 Sep 2024 17:28:40 GMT"}],"updateDate":"2024-09-20","timestamp":1722531530000,"abstract":"  The focus of this paper is on 3D motion editing. Given a 3D human motion and\na textual description of the desired modification, our goal is to generate an\nedited motion as described by the text. The key challenges include the scarcity\nof training data and the need to design a model that accurately edits the\nsource motion. In this paper, we address both challenges. We propose a\nmethodology to semi-automatically collect a dataset of triplets comprising (i)\na source motion, (ii) a target motion, and (iii) an edit text, introducing the\nnew MotionFix dataset. Access to this data allows us to train a conditional\ndiffusion model, TMED, that takes both the source motion and the edit text as\ninput. We develop several baselines to evaluate our model, comparing it against\nmodels trained solely on text-motion pair datasets, and demonstrate the\nsuperior performance of our model trained on triplets. We also introduce new\nretrieval-based metrics for motion editing, establishing a benchmark on the\nevaluation set of MotionFix. Our results are promising, paving the way for\nfurther research in fine-grained motion generation. Code, models, and data are\navailable at https://motionfix.is.tue.mpg.de/ .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}