{"id":"2407.05206","title":"Helios: An extremely low power event-based gesture recognition for\n  always-on smart eyewear","authors":"Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Ben\n  Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, David Trickett, Chris\n  Mair, Taru Muhonen, Rory Clark, Louis Berridge, Richard Vigars and Iain\n  Wallace","authorsParsed":[["Bhattacharyya","Prarthana",""],["Mitton","Joshua",""],["Page","Ryan",""],["Morgan","Owen",""],["Menzies","Ben",""],["Homewood","Gabriel",""],["Jacobs","Kemi",""],["Baesso","Paolo",""],["Trickett","David",""],["Mair","Chris",""],["Muhonen","Taru",""],["Clark","Rory",""],["Berridge","Louis",""],["Vigars","Richard",""],["Wallace","Iain",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 23:16:41 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 12:33:53 GMT"},{"version":"v3","created":"Mon, 12 Aug 2024 10:23:27 GMT"},{"version":"v4","created":"Mon, 26 Aug 2024 09:15:11 GMT"}],"updateDate":"2024-08-27","timestamp":1720307801000,"abstract":"  This paper introduces Helios, the first extremely low-power, real-time,\nevent-based hand gesture recognition system designed for all-day on smart\neyewear. As augmented reality (AR) evolves, current smart glasses like the Meta\nRay-Bans prioritize visual and wearable comfort at the expense of\nfunctionality. Existing human-machine interfaces (HMIs) in these devices, such\nas capacitive touch and voice controls, present limitations in ergonomics,\nprivacy and power consumption. Helios addresses these challenges by leveraging\nnatural hand interactions for a more intuitive and comfortable user experience.\nOur system utilizes a extremely low-power and compact 3mmx4mm/20mW event camera\nto perform natural hand-based gesture recognition for always-on smart eyewear.\nThe camera's output is processed by a convolutional neural network (CNN)\nrunning on a NXP Nano UltraLite compute platform, consuming less than 350mW.\nHelios can recognize seven classes of gestures, including subtle microgestures\nlike swipes and pinches, with 91% accuracy. We also demonstrate real-time\nperformance across 20 users at a remarkably low latency of 60ms. Our user\ntesting results align with the positive feedback we received during our recent\nsuccessful demo at AWE-USA-2024.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}