{"id":"2408.11796","title":"LLM Pruning and Distillation in Practice: The Minitron Approach","authors":"Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi,\n  Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan\n  Kautz, Pavlo Molchanov","authorsParsed":[["Sreenivas","Sharath Turuvekere",""],["Muralidharan","Saurav",""],["Joshi","Raviraj",""],["Chochowski","Marcin",""],["Patwary","Mostofa",""],["Shoeybi","Mohammad",""],["Catanzaro","Bryan",""],["Kautz","Jan",""],["Molchanov","Pavlo",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:38:48 GMT"},{"version":"v2","created":"Mon, 26 Aug 2024 17:50:46 GMT"}],"updateDate":"2024-08-27","timestamp":1724261928000,"abstract":"  We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}