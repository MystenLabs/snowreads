{"id":"2408.10383","title":"BrewCLIP: A Bifurcated Representation Learning Framework for\n  Audio-Visual Retrieval","authors":"Zhenyu Lu, Lakshay Sethi","authorsParsed":[["Lu","Zhenyu",""],["Sethi","Lakshay",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 19:56:10 GMT"}],"updateDate":"2024-08-21","timestamp":1724097370000,"abstract":"  Previous methods for audio-image matching generally fall into one of two\ncategories: pipeline models or End-to-End models. Pipeline models first\ntranscribe speech and then encode the resulting text; End-to-End models encode\nspeech directly. Generally, pipeline models outperform end-to-end models, but\nthe intermediate transcription necessarily discards some potentially useful\nnon-textual information. In addition to textual information, speech can convey\ndetails such as accent, mood, and and emphasis, which should be effectively\ncaptured in the encoded representation. In this paper, we investigate whether\nnon-textual information, which is overlooked by pipeline-based models, can be\nleveraged to improve speech-image matching performance. We thoroughly analyze\nand compare End-to-End models, pipeline models, and our proposed dual-channel\nmodel for robust audio-image retrieval on a variety of datasets. Our approach\nachieves a substantial performance gain over the previous state-of-the-art by\nleveraging strong pretrained models, a prompting mechanism and a bifurcated\ndesign.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}