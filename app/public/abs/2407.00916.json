{"id":"2407.00916","title":"Learnability in Online Kernel Selection with Memory Constraint via\n  Data-dependent Regret Analysis","authors":"Junfan Li, Shizhong Liao","authorsParsed":[["Li","Junfan",""],["Liao","Shizhong",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 02:42:27 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 03:42:46 GMT"}],"updateDate":"2024-07-04","timestamp":1719801747000,"abstract":"  Online kernel selection is a fundamental problem of online kernel methods.In\nthis paper,we study online kernel selection with memory constraint in which the\nmemory of kernel selection and online prediction procedures is limited to a\nfixed budget. An essential question is what is the intrinsic relationship among\nonline learnability, memory constraint, and data complexity? To answer the\nquestion,it is necessary to show the trade-offs between regret and memory\nconstraint.Previous work gives a worst-case lower bound depending on the data\nsize,and shows learning is impossible within a small memory constraint.In\ncontrast, we present distinct results by offering data-dependent upper bounds\nthat rely on two data complexities:kernel alignment and the cumulative losses\nof competitive hypothesis.We propose an algorithmic framework giving\ndata-dependent upper bounds for two types of loss functions.For the hinge loss\nfunction,our algorithm achieves an expected upper bound depending on kernel\nalignment.For smooth loss functions,our algorithm achieves a high-probability\nupper bound depending on the cumulative losses of competitive hypothesis.We\nalso prove a matching lower bound for smooth loss functions.Our results show\nthat if the two data complexities are sub-linear,then learning is possible\nwithin a small memory constraint.Our algorithmic framework depends on a new\nbuffer maintaining framework and a reduction from online kernel selection to\nprediction with expert advice. Finally,we empirically verify the prediction\nperformance of our algorithms on benchmark datasets.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}