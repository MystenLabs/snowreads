{"id":"2408.17046","title":"Text-to-Image Generation Via Energy-Based CLIP","authors":"Roy Ganz and Michael Elad","authorsParsed":[["Ganz","Roy",""],["Elad","Michael",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 07:08:01 GMT"}],"updateDate":"2024-09-02","timestamp":1725001681000,"abstract":"  Joint Energy Models (JEMs), while drawing significant research attention,\nhave not been successfully scaled to real-world, high-resolution datasets. We\npresent EB-CLIP, a novel approach extending JEMs to the multimodal\nvision-language domain using CLIP, integrating both generative and\ndiscriminative objectives. For the generative objective, we introduce an\nimage-text joint-energy function based on Cosine similarity in the CLIP space,\ntraining CLIP to assign low energy to real image-caption pairs and high energy\notherwise. For the discriminative objective, we employ contrastive adversarial\nloss, extending the adversarial training objective to the multimodal domain.\nEB-CLIP not only generates realistic images from text but also achieves\ncompetitive results on the compositionality benchmark, outperforming leading\nmethods with fewer parameters. Additionally, we demonstrate the superior\nguidance capability of EB-CLIP by enhancing CLIP-based generative frameworks\nand converting unconditional diffusion models to text-based ones. Lastly, we\nshow that EB-CLIP can serve as a more robust evaluation metric for\ntext-to-image generative tasks than CLIP.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}