{"id":"2408.16981","title":"The Sample-Communication Complexity Trade-off in Federated Q-Learning","authors":"Sudeep Salgia and Yuejie Chi","authorsParsed":[["Salgia","Sudeep",""],["Chi","Yuejie",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 03:03:03 GMT"}],"updateDate":"2024-09-02","timestamp":1724986983000,"abstract":"  We consider the problem of federated Q-learning, where $M$ agents aim to\ncollaboratively learn the optimal Q-function of an unknown infinite-horizon\nMarkov decision process with finite state and action spaces. We investigate the\ntrade-off between sample and communication complexities for the widely used\nclass of intermittent communication algorithms. We first establish the converse\nresult, where it is shown that a federated Q-learning algorithm that offers any\nspeedup with respect to the number of agents in the per-agent sample complexity\nneeds to incur a communication cost of at least an order of\n$\\frac{1}{1-\\gamma}$ up to logarithmic factors, where $\\gamma$ is the discount\nfactor. We also propose a new algorithm, called Fed-DVR-Q, which is the first\nfederated Q-learning algorithm to simultaneously achieve order-optimal sample\nand communication complexities. Thus, together these results provide a complete\ncharacterization of the sample-communication complexity trade-off in federated\nQ-learning.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}