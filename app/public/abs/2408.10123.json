{"id":"2408.10123","title":"Learning Precise Affordances from Egocentric Videos for Robotic\n  Manipulation","authors":"Gen Li, Nikolaos Tsagkas, Jifei Song, Ruaridh Mon-Williams, Sethu\n  Vijayakumar, Kun Shao, Laura Sevilla-Lara","authorsParsed":[["Li","Gen",""],["Tsagkas","Nikolaos",""],["Song","Jifei",""],["Mon-Williams","Ruaridh",""],["Vijayakumar","Sethu",""],["Shao","Kun",""],["Sevilla-Lara","Laura",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 16:11:47 GMT"}],"updateDate":"2024-08-20","timestamp":1724083907000,"abstract":"  Affordance, defined as the potential actions that an object offers, is\ncrucial for robotic manipulation tasks. A deep understanding of affordance can\nlead to more intelligent AI systems. For example, such knowledge directs an\nagent to grasp a knife by the handle for cutting and by the blade when passing\nit to someone. In this paper, we present a streamlined affordance learning\nsystem that encompasses data collection, effective model training, and robot\ndeployment. First, we collect training data from egocentric videos in an\nautomatic manner. Different from previous methods that focus only on the object\ngraspable affordance and represent it as coarse heatmaps, we cover both\ngraspable (e.g., object handles) and functional affordances (e.g., knife\nblades, hammer heads) and extract data with precise segmentation masks. We then\npropose an effective model, termed Geometry-guided Affordance Transformer\n(GKT), to train on the collected data. GKT integrates an innovative Depth\nFeature Injector (DFI) to incorporate 3D shape and geometric priors, enhancing\nthe model's understanding of affordances. To enable affordance-oriented\nmanipulation, we further introduce Aff-Grasp, a framework that combines GKT\nwith a grasp generation model. For comprehensive evaluation, we create an\naffordance evaluation dataset with pixel-wise annotations, and design\nreal-world tasks for robot experiments. The results show that GKT surpasses the\nstate-of-the-art by 15.9% in mIoU, and Aff-Grasp achieves high success rates of\n95.5% in affordance prediction and 77.1% in successful grasping among 179\ntrials, including evaluations with seen, unseen objects, and cluttered scenes.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}