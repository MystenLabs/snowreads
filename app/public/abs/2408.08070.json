{"id":"2408.08070","title":"MambaMIM: Pre-training Mamba with State Space Token-interpolation","authors":"Fenghe Tang, Bingkun Nian, Yingtai Li, Jie Yang, Liu Wei, S. Kevin\n  Zhou","authorsParsed":[["Tang","Fenghe",""],["Nian","Bingkun",""],["Li","Yingtai",""],["Yang","Jie",""],["Wei","Liu",""],["Zhou","S. Kevin",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 10:35:26 GMT"}],"updateDate":"2024-08-16","timestamp":1723718126000,"abstract":"  Generative self-supervised learning demonstrates outstanding representation\nlearning capabilities in both Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs). However, there are currently no generative pre-training\nmethods related to selective state space models (Mamba) that can handle\nlong-range dependencies effectively. To address this challenge, we introduce a\ngenerative self-supervised learning method for Mamba (MambaMIM) based on\nSelective Structure State Space Sequence Token-interpolation (S6T), a\ngeneral-purpose pre-training method for arbitrary Mamba architectures. Our\nmethod, MambaMIM, incorporates a bottom-up 3D hybrid masking strategy in the\nencoder to maintain masking consistency across different architectures.\nAdditionally, S6T is employed to learn causal relationships between the masked\nsequence in the state space. MambaMIM can be used on any single or hybrid Mamba\narchitectures to enhance the Mamba long-range representation capability.\nExtensive downstream experiments reveal the feasibility and advancement of\nusing Mamba for pre-training medical image tasks. The code is available at:\nhttps://github.com/FengheTan9/MambaMIM\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}