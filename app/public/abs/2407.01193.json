{"id":"2407.01193","title":"Cross-Architecture Auxiliary Feature Space Translation for Efficient\n  Few-Shot Personalized Object Detection","authors":"Francesco Barbato, Umberto Michieli, Jijoong Moon, Pietro Zanuttigh,\n  Mete Ozay","authorsParsed":[["Barbato","Francesco",""],["Michieli","Umberto",""],["Moon","Jijoong",""],["Zanuttigh","Pietro",""],["Ozay","Mete",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 11:33:53 GMT"}],"updateDate":"2024-07-02","timestamp":1719833633000,"abstract":"  Recent years have seen object detection robotic systems deployed in several\npersonal devices (e.g., home robots and appliances). This has highlighted a\nchallenge in their design, i.e., they cannot efficiently update their knowledge\nto distinguish between general classes and user-specific instances (e.g., a dog\nvs. user's dog). We refer to this challenging task as Instance-level\nPersonalized Object Detection (IPOD). The personalization task requires many\nsamples for model tuning and optimization in a centralized server, raising\nprivacy concerns. An alternative is provided by approaches based on recent\nlarge-scale Foundation Models, but their compute costs preclude on-device\napplications.\n  In our work we tackle both problems at the same time, designing a Few-Shot\nIPOD strategy called AuXFT. We introduce a conditional coarse-to-fine few-shot\nlearner to refine the coarse predictions made by an efficient object detector,\nshowing that using an off-the-shelf model leads to poor personalization due to\nneural collapse. Therefore, we introduce a Translator block that generates an\nauxiliary feature space where features generated by a self-supervised model\n(e.g., DINOv2) are distilled without impacting the performance of the detector.\nWe validate AuXFT on three publicly available datasets and one in-house\nbenchmark designed for the IPOD task, achieving remarkable gains in all\nconsidered scenarios with excellent time-complexity trade-off: AuXFT reaches a\nperformance of 80% its upper bound at just 32% of the inference time, 13% of\nVRAM and 19% of the model size.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}