{"id":"2408.09698","title":"Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation","authors":"Yuyang Ye, Zhi Zheng, Yishan Shen, Tianshu Wang, Hengruo Zhang, Peijun\n  Zhu, Runlong Yu, Kai Zhang, Hui Xiong","authorsParsed":[["Ye","Yuyang",""],["Zheng","Zhi",""],["Shen","Yishan",""],["Wang","Tianshu",""],["Zhang","Hengruo",""],["Zhu","Peijun",""],["Yu","Runlong",""],["Zhang","Kai",""],["Xiong","Hui",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 04:44:32 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 16:09:33 GMT"}],"updateDate":"2024-08-21","timestamp":1724042672000,"abstract":"  Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}