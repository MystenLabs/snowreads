{"id":"2408.03554","title":"Empirical Analysis of Large Vision-Language Models against Goal\n  Hijacking via Visual Prompt Injection","authors":"Subaru Kimura, Ryota Tanaka, Shumpei Miyawaki, Jun Suzuki, Keisuke\n  Sakaguchi","authorsParsed":[["Kimura","Subaru",""],["Tanaka","Ryota",""],["Miyawaki","Shumpei",""],["Suzuki","Jun",""],["Sakaguchi","Keisuke",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 05:30:10 GMT"}],"updateDate":"2024-08-08","timestamp":1723008610000,"abstract":"  We explore visual prompt injection (VPI) that maliciously exploits the\nability of large vision-language models (LVLMs) to follow instructions drawn\nonto the input image. We propose a new VPI method, \"goal hijacking via visual\nprompt injection\" (GHVPI), that swaps the execution task of LVLMs from an\noriginal task to an alternative task designated by an attacker. The\nquantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and\ndemonstrates a notable attack success rate of 15.8%, which is an unignorable\nsecurity risk. Our analysis also shows that successful GHVPI requires high\ncharacter recognition capability and instruction-following ability in LVLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}