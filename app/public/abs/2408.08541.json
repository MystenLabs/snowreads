{"id":"2408.08541","title":"Where is the signal in tokenization space?","authors":"Renato Lui Geh and Honghua Zhang and Kareem Ahmed and Benjie Wang and\n  Guy Van den Broeck","authorsParsed":[["Geh","Renato Lui",""],["Zhang","Honghua",""],["Ahmed","Kareem",""],["Wang","Benjie",""],["Broeck","Guy Van den",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 05:56:10 GMT"}],"updateDate":"2024-08-19","timestamp":1723787770000,"abstract":"  Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}