{"id":"2407.04264","title":"Langevin Dynamics: A Unified Perspective on Optimization via Lyapunov\n  Potentials","authors":"August Y. Chen, Ayush Sekhari, Karthik Sridharan","authorsParsed":[["Chen","August Y.",""],["Sekhari","Ayush",""],["Sridharan","Karthik",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 05:34:10 GMT"}],"updateDate":"2024-07-08","timestamp":1720157650000,"abstract":"  We study the problem of non-convex optimization using Stochastic Gradient\nLangevin Dynamics (SGLD). SGLD is a natural and popular variation of stochastic\ngradient descent where at each step, appropriately scaled Gaussian noise is\nadded. To our knowledge, the only strategy for showing global convergence of\nSGLD on the loss function is to show that SGLD can sample from a stationary\ndistribution which assigns larger mass when the function is small (the Gibbs\nmeasure), and then to convert these guarantees to optimization results.\n  We employ a new strategy to analyze the convergence of SGLD to global minima,\nbased on Lyapunov potentials and optimization. We convert the same mild\nconditions from previous works on SGLD into geometric properties based on\nLyapunov potentials. This adapts well to the case with a stochastic gradient\noracle, which is natural for machine learning applications where one wants to\nminimize population loss but only has access to stochastic gradients via\nminibatch training samples. Here we provide 1) improved rates in the setting of\nprevious works studying SGLD for optimization, 2) the first finite gradient\ncomplexity guarantee for SGLD where the function is Lipschitz and the Gibbs\nmeasure defined by the function satisfies a Poincar\\'e Inequality, and 3) prove\nif continuous-time Langevin Dynamics succeeds for optimization, then\ndiscrete-time SGLD succeeds under mild regularity assumptions.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7jAEY1ei0D504Y_twYjNiAKO6wtJapRhMh5XKFbLqrg","pdfSize":"888268"}
