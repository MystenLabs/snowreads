{"id":"2407.18658","title":"Adversarial Robustification via Text-to-Image Diffusion Models","authors":"Daewon Choi, Jongheon Jeong, Huiwon Jang, Jinwoo Shin","authorsParsed":[["Choi","Daewon",""],["Jeong","Jongheon",""],["Jang","Huiwon",""],["Shin","Jinwoo",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 10:49:14 GMT"}],"updateDate":"2024-07-29","timestamp":1721990954000,"abstract":"  Adversarial robustness has been conventionally believed as a challenging\nproperty to encode for neural networks, requiring plenty of training data. In\nthe recent paradigm of adopting off-the-shelf models, however, access to their\ntraining data is often infeasible or not practical, while most of such models\nare not originally trained concerning adversarial robustness. In this paper, we\ndevelop a scalable and model-agnostic solution to achieve adversarial\nrobustness without using any data. Our intuition is to view recent\ntext-to-image diffusion models as \"adaptable\" denoisers that can be optimized\nto specify target tasks. Based on this, we propose: (a) to initiate a\ndenoise-and-classify pipeline that offers provable guarantees against\nadversarial attacks, and (b) to leverage a few synthetic reference images\ngenerated from the text-to-image model that enables novel adaptation schemes.\nOur experiments show that our data-free scheme applied to the pre-trained CLIP\ncould improve the (provable) adversarial robustness of its diverse zero-shot\nclassification derivatives (while maintaining their accuracy), significantly\nsurpassing prior approaches that utilize the full training data. Not only for\nCLIP, we also demonstrate that our framework is easily applicable for\nrobustifying other visual classifiers efficiently.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}