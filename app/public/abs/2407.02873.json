{"id":"2407.02873","title":"Robot Shape and Location Retention in Video Generation Using Diffusion\n  Models","authors":"Peng Wang, Zhihao Guo, Abdul Latheef Sait, Minh Huy Pham","authorsParsed":[["Wang","Peng",""],["Guo","Zhihao",""],["Sait","Abdul Latheef",""],["Pham","Minh Huy",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 07:38:26 GMT"}],"updateDate":"2024-07-04","timestamp":1719992306000,"abstract":"  Diffusion models have marked a significant milestone in the enhancement of\nimage and video generation technologies. However, generating videos that\nprecisely retain the shape and location of moving objects such as robots\nremains a challenge. This paper presents diffusion models specifically tailored\nto generate videos that accurately maintain the shape and location of mobile\nrobots. This development offers substantial benefits to those working on\ndetecting dangerous interactions between humans and robots by facilitating the\ncreation of training data for collision detection models, circumventing the\nneed for collecting data from the real world, which often involves legal and\nethical issues. Our models incorporate techniques such as embedding accessible\nrobot pose information and applying semantic mask regulation within the\nConvNext backbone network. These techniques are designed to refine intermediate\noutputs, therefore improving the retention performance of shape and location.\nThrough extensive experimentation, our models have demonstrated notable\nimprovements in maintaining the shape and location of different robots, as well\nas enhancing overall video generation quality, compared to the benchmark\ndiffusion model. Codes will be opensourced at\n\\href{https://github.com/PengPaulWang/diffusion-robots}{Github}.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}