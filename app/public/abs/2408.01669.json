{"id":"2408.01669","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","authors":"Chaolei Tan, Zihang Lin, Junfu Pu, Zhongang Qi, Wei-Yi Pei, Zhi Qu,\n  Yexin Wang, Ying Shan, Wei-Shi Zheng, Jian-Fang Hu","authorsParsed":[["Tan","Chaolei",""],["Lin","Zihang",""],["Pu","Junfu",""],["Qi","Zhongang",""],["Pei","Wei-Yi",""],["Qu","Zhi",""],["Wang","Yexin",""],["Shan","Ying",""],["Zheng","Wei-Shi",""],["Hu","Jian-Fang",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 05:35:13 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 12:06:18 GMT"},{"version":"v3","created":"Thu, 8 Aug 2024 11:19:37 GMT"},{"version":"v4","created":"Sun, 18 Aug 2024 18:06:06 GMT"}],"updateDate":"2024-08-20","timestamp":1722663313000,"abstract":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"m1zhYm3na7gkhF3sHiBtsnePmWWEw27F_tVf02RDn2k","pdfSize":"4289708"}
