{"id":"2407.03115","title":"$L_p$-norm Distortion-Efficient Adversarial Attack","authors":"Chao Zhou, Yuan-Gen Wang, Zi-jia Wang and Xiangui Kang","authorsParsed":[["Zhou","Chao",""],["Wang","Yuan-Gen",""],["Wang","Zi-jia",""],["Kang","Xiangui",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:00:33 GMT"}],"updateDate":"2024-07-04","timestamp":1720015233000,"abstract":"  Adversarial examples have shown a powerful ability to make a well-trained\nmodel misclassified. Current mainstream adversarial attack methods only\nconsider one of the distortions among $L_0$-norm, $L_2$-norm, and\n$L_\\infty$-norm. $L_0$-norm based methods cause large modification on a single\npixel, resulting in naked-eye visible detection, while $L_2$-norm and\n$L_\\infty$-norm based methods suffer from weak robustness against adversarial\ndefense since they always diffuse tiny perturbations to all pixels. A more\nrealistic adversarial perturbation should be sparse and imperceptible. In this\npaper, we propose a novel $L_p$-norm distortion-efficient adversarial attack,\nwhich not only owns the least $L_2$-norm loss but also significantly reduces\nthe $L_0$-norm distortion. To this aim, we design a new optimization scheme,\nwhich first optimizes an initial adversarial perturbation under $L_2$-norm\nconstraint, and then constructs a dimension unimportance matrix for the initial\nperturbation. Such a dimension unimportance matrix can indicate the adversarial\nunimportance of each dimension of the initial perturbation. Furthermore, we\nintroduce a new concept of adversarial threshold for the dimension unimportance\nmatrix. The dimensions of the initial perturbation whose unimportance is higher\nthan the threshold will be all set to zero, greatly decreasing the $L_0$-norm\ndistortion. Experimental results on three benchmark datasets show that under\nthe same query budget, the adversarial examples generated by our method have\nlower $L_0$-norm and $L_2$-norm distortion than the state-of-the-art.\nEspecially for the MNIST dataset, our attack reduces 8.1$\\%$ $L_2$-norm\ndistortion meanwhile remaining 47$\\%$ pixels unattacked. This demonstrates the\nsuperiority of the proposed method over its competitors in terms of adversarial\nrobustness and visual imperceptibility.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}