{"id":"2407.00468","title":"MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and\n  Efficient Evaluation","authors":"Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan\n  Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju,\n  Luchen Liu, Tianyu Liu, Baobao Chang, Ming Zhang","authorsParsed":[["Huang","Jinsheng",""],["Chen","Liang",""],["Guo","Taian",""],["Zeng","Fu",""],["Zhao","Yusheng",""],["Wu","Bohan",""],["Yuan","Ye",""],["Zhao","Haozhe",""],["Guo","Zhihui",""],["Zhang","Yichi",""],["Yuan","Jingyang",""],["Ju","Wei",""],["Liu","Luchen",""],["Liu","Tianyu",""],["Chang","Baobao",""],["Zhang","Ming",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 15:28:45 GMT"}],"updateDate":"2024-07-02","timestamp":1719674925000,"abstract":"  Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises $2,138$ question triplets, totaling\n$6,414$ distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by $31.73\\%$, compared\nto an average gap of $8.03\\%$ in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by $23.09\\%$, whereas the gap for previous\nbenchmarks is just $14.64\\%$). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d-l5YY-_WFGfG3MpLwverR7mmlOOpNaNuHGjx34aaB4","pdfSize":"4225681"}
