{"id":"2407.19239","title":"MaTrRec: Uniting Mamba and Transformer for Sequential Recommendation","authors":"Shun Zhang and Runsen Zhang and Zhirong Yang","authorsParsed":[["Zhang","Shun",""],["Zhang","Runsen",""],["Yang","Zhirong",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 12:07:46 GMT"}],"updateDate":"2024-07-30","timestamp":1722082066000,"abstract":"  Sequential recommendation systems aim to provide personalized recommendations\nby analyzing dynamic preferences and dependencies within user behavior\nsequences. Recently, Transformer models can effectively capture user\npreferences. However, their quadratic computational complexity limits\nrecommendation performance on long interaction sequence data. Inspired by the\nState Space Model (SSM)representative model, Mamba, which efficiently captures\nuser preferences in long interaction sequences with linear complexity, we find\nthat Mamba's recommendation effectiveness is limited in short interaction\nsequences, with failing to recall items of actual interest to users and\nexacerbating the data sparsity cold start problem. To address this issue, we\ninnovatively propose a new model, MaTrRec, which combines the strengths of\nMamba and Transformer. This model fully leverages Mamba's advantages in\nhandling long-term dependencies and Transformer's global attention advantages\nin short-term dependencies, thereby enhances predictive capabilities on both\nlong and short interaction sequence datasets while balancing model efficiency.\nNotably, our model significantly improves the data sparsity cold start problem,\nwith an improvement of up to 33% on the highly sparse Amazon Musical\nInstruments dataset. We conducted extensive experimental evaluations on five\nwidely used public datasets. The experimental results show that our model\noutperforms the current state-of-the-art sequential recommendation models on\nall five datasets. The code is available at\nhttps://github.com/Unintelligentmumu/MaTrRec.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}