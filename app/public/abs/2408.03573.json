{"id":"2408.03573","title":"Active Testing of Large Language Model via Multi-Stage Sampling","authors":"Yuheng Huang, Jiayang Song, Qiang Hu, Felix Juefei-Xu, Lei Ma","authorsParsed":[["Huang","Yuheng",""],["Song","Jiayang",""],["Hu","Qiang",""],["Juefei-Xu","Felix",""],["Ma","Lei",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 06:17:48 GMT"}],"updateDate":"2024-08-08","timestamp":1723011468000,"abstract":"  Performance evaluation plays a crucial role in the development life cycle of\nlarge language models (LLMs). It estimates the model's capability, elucidates\nbehavior characteristics, and facilitates the identification of potential\nissues and limitations, thereby guiding further improvement. Given that LLMs'\ndiverse task-handling abilities stem from large volumes of training data, a\ncomprehensive evaluation also necessitates abundant, well-annotated, and\nrepresentative test data to assess LLM performance across various downstream\ntasks. However, the demand for high-quality test data often entails substantial\ntime, computational resources, and manual efforts, sometimes causing the\nevaluation to be inefficient or impractical. To address these challenges,\nresearchers propose active testing, which estimates the overall performance by\nselecting a subset of test data. Nevertheless, the existing active testing\nmethods tend to be inefficient, even inapplicable, given the unique new\nchallenges of LLMs (e.g., diverse task types, increased model complexity, and\nunavailability of training data). To mitigate such limitations and expedite the\ndevelopment cycle of LLMs, in this work, we introduce AcTracer, an active\ntesting framework tailored for LLMs that strategically selects a small subset\nof test data to achieve a nearly optimal performance estimation for LLMs.\nAcTracer utilizes both internal and external information from LLMs to guide the\ntest sampling process, reducing variance through a multi-stage pool-based\nactive selection. Our experiment results demonstrate that AcTracer achieves\nstate-of-the-art performance compared to existing methods across various tasks,\nwith up to 38.83% improvement over previous SOTA.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}