{"id":"2407.10814","title":"Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot\n  Whole Slide Image Classification","authors":"Linhao Qu, Dingkang Yang, Dan Huang, Qinhao Guo, Rongkui Luo, Shaoting\n  Zhang, Xiaosong Wang","authorsParsed":[["Qu","Linhao",""],["Yang","Dingkang",""],["Huang","Dan",""],["Guo","Qinhao",""],["Luo","Rongkui",""],["Zhang","Shaoting",""],["Wang","Xiaosong",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:31:55 GMT"}],"updateDate":"2024-07-16","timestamp":1721057515000,"abstract":"  Current multi-instance learning algorithms for pathology image analysis often\nrequire a substantial number of Whole Slide Images for effective training but\nexhibit suboptimal performance in scenarios with limited learning data. In\nclinical settings, restricted access to pathology slides is inevitable due to\npatient privacy concerns and the prevalence of rare or emerging diseases. The\nemergence of the Few-shot Weakly Supervised WSI Classification accommodates the\nsignificant challenge of the limited slide data and sparse slide-level labels\nfor diagnosis. Prompt learning based on the pre-trained models (\\eg, CLIP)\nappears to be a promising scheme for this setting; however, current research in\nthis area is limited, and existing algorithms often focus solely on patch-level\nprompts or confine themselves to language prompts. This paper proposes a\nmulti-instance prompt learning framework enhanced with pathology knowledge,\n\\ie, integrating visual and textual prior knowledge into prompts at both patch\nand slide levels. The training process employs a combination of static and\nlearnable prompts, effectively guiding the activation of pre-trained models and\nfurther facilitating the diagnosis of key pathology patterns. Lightweight\nMessenger (self-attention) and Summary (attention-pooling) layers are\nintroduced to model relationships between patches and slides within the same\npatient data. Additionally, alignment-wise contrastive losses ensure the\nfeature-level alignment between visual and textual learnable prompts for both\npatches and slides. Our method demonstrates superior performance in three\nchallenging clinical tasks, significantly outperforming comparative few-shot\nmethods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}