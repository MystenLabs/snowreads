{"id":"2407.10855","title":"Weighted Grouped Query Attention in Transformers","authors":"Sai Sena Chinnakonduru, Astarag Mohapatra","authorsParsed":[["Chinnakonduru","Sai Sena",""],["Mohapatra","Astarag",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 16:07:13 GMT"}],"updateDate":"2024-07-16","timestamp":1721059633000,"abstract":"  The attention mechanism forms the foundational blocks for transformer\nlanguage models. Recent approaches show that scaling the model achieves\nhuman-level performance. However, with increasing demands for scaling and\nconstraints on hardware memory, the inference costs of these models remain\nhigh. To reduce the inference time, Multi-Query Attention (MQA) and\nGrouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet\nal., 2023) respectively. In this paper, we propose a variation of Grouped-Query\nAttention, termed Weighted Grouped-Query Attention (WGQA). We introduced new\nlearnable parameters for each key and value head in the T5 decoder attention\nblocks, enabling the model to take a weighted average during finetuning. Our\nmodel achieves an average of 0.53% improvement over GQA, and the performance\nconverges to traditional Multi-head attention (MHA) with no additional overhead\nduring inference. We evaluated the introduction of these parameters and\nsubsequent finetuning informs the model about the grouping mechanism during\ntraining, thereby enhancing performance. Additionally, we demonstrate the\nscaling laws in our analysis by comparing the results between T5-small and\nT5-base architecture.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7CadLNmJOwnVjxOLoOrpQ_vpvZM_ou2opzoP8OaodXs","pdfSize":"317592"}
