{"id":"2407.07124","title":"FedClust: Tackling Data Heterogeneity in Federated Learning through\n  Weight-Driven Client Clustering","authors":"Md Sirajul Islam, Simin Javaherian, Fei Xu, Xu Yuan, Li Chen,\n  Nian-Feng Tzeng","authorsParsed":[["Islam","Md Sirajul",""],["Javaherian","Simin",""],["Xu","Fei",""],["Yuan","Xu",""],["Chen","Li",""],["Tzeng","Nian-Feng",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 02:47:16 GMT"}],"updateDate":"2024-07-11","timestamp":1720493236000,"abstract":"  Federated learning (FL) is an emerging distributed machine learning paradigm\nthat enables collaborative training of machine learning models over\ndecentralized devices without exposing their local data. One of the major\nchallenges in FL is the presence of uneven data distributions across client\ndevices, violating the well-known assumption of\nindependent-and-identically-distributed (IID) training samples in conventional\nmachine learning. To address the performance degradation issue incurred by such\ndata heterogeneity, clustered federated learning (CFL) shows its promise by\ngrouping clients into separate learning clusters based on the similarity of\ntheir local data distributions. However, state-of-the-art CFL approaches\nrequire a large number of communication rounds to learn the distribution\nsimilarities during training until the formation of clusters is stabilized.\nMoreover, some of these algorithms heavily rely on a predefined number of\nclusters, thus limiting their flexibility and adaptability. In this paper, we\npropose {\\em FedClust}, a novel approach for CFL that leverages the correlation\nbetween local model weights and the data distribution of clients. {\\em\nFedClust} groups clients into clusters in a one-shot manner by measuring the\nsimilarity degrees among clients based on the strategically selected partial\nweights of locally trained models. We conduct extensive experiments on four\nbenchmark datasets with different non-IID data settings. Experimental results\ndemonstrate that {\\em FedClust} achieves higher model accuracy up to $\\sim$45\\%\nas well as faster convergence with a significantly reduced communication cost\nup to 2.7$\\times$ compared to its state-of-the-art counterparts.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}