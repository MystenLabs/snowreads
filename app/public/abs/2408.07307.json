{"id":"2408.07307","title":"Nonlocal Attention Operator: Materializing Hidden Knowledge Towards\n  Interpretable Physics Discovery","authors":"Yue Yu, Ning Liu, Fei Lu, Tian Gao, Siavash Jafarzadeh, Stewart\n  Silling","authorsParsed":[["Yu","Yue",""],["Liu","Ning",""],["Lu","Fei",""],["Gao","Tian",""],["Jafarzadeh","Siavash",""],["Silling","Stewart",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 05:57:56 GMT"}],"updateDate":"2024-08-15","timestamp":1723615076000,"abstract":"  Despite the recent popularity of attention-based neural architectures in core\nAI fields like natural language processing (NLP) and computer vision (CV),\ntheir potential in modeling complex physical systems remains under-explored.\nLearning problems in physical systems are often characterized as discovering\noperators that map between function spaces based on a few instances of function\npairs. This task frequently presents a severely ill-posed PDE inverse problem.\nIn this work, we propose a novel neural operator architecture based on the\nattention mechanism, which we coin Nonlocal Attention Operator (NAO), and\nexplore its capability towards developing a foundation physical model. In\nparticular, we show that the attention mechanism is equivalent to a double\nintegral operator that enables nonlocal interactions among spatial tokens, with\na data-dependent kernel characterizing the inverse mapping from data to the\nhidden parameter field of the underlying operator. As such, the attention\nmechanism extracts global prior information from training data generated by\nmultiple systems, and suggests the exploratory space in the form of a nonlinear\nkernel map. Consequently, NAO can address ill-posedness and rank deficiency in\ninverse PDE problems by encoding regularization and achieving generalizability.\nWe empirically demonstrate the advantages of NAO over baseline neural models in\nterms of generalizability to unseen data resolutions and system states. Our\nwork not only suggests a novel neural operator architecture for learning\ninterpretable foundation models of physical systems, but also offers a new\nperspective towards understanding the attention mechanism.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Analysis of PDEs"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"1Aga-xLHFTsWF0r_44aqQzFEJR8Z803ERI6vMQy8ju8","pdfSize":"1134511"}
