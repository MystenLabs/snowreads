{"id":"2407.14766","title":"Implementing Fairness: the view from a FairDream","authors":"Thomas Souverain, Johnathan Nguyen, Nicolas Meric, Paul \\'Egr\\'e","authorsParsed":[["Souverain","Thomas",""],["Nguyen","Johnathan",""],["Meric","Nicolas",""],["Égré","Paul",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 06:06:24 GMT"}],"updateDate":"2024-07-23","timestamp":1721455584000,"abstract":"  In this paper, we propose an experimental investigation of the problem of AI\nfairness in classification. We train an AI model and develop our own fairness\npackage FairDream to detect inequalities and then to correct for them, using\nincome prediction as a case study. Our experiments show that it is a property\nof FairDream to fulfill fairness objectives which are conditional on the ground\ntruth (Equalized Odds), even when the algorithm is set the task of equalizing\npositives across groups (Demographic Parity). While this may be seen as an\nanomaly, we explain this property by comparing our approach with a closely\nrelated fairness method (GridSearch), which can enforce Demographic Parity at\nthe expense of Equalized Odds. We grant that a fairness metric conditioned on\ntrue labels does not give a sufficient criterion to reach fairness, but we\nargue that it gives us at least a necessary condition to implement Demographic\nParity cautiously. We also explain why neither Equal Calibration nor Equal\nPrecision stand as relevant fairness criteria in classification. Addressing\ntheir limitations to warn the decision-maker for any disadvantaging rate,\nEqualized Odds avoids the peril of strict conservatism, while keeping away the\nutopia of a whole redistribution of resources through algorithms.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}