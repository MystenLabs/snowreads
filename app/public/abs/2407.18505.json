{"id":"2407.18505","title":"VoxSim: A perceptual voice similarity dataset","authors":"Junseok Ahn, Youkyum Kim, Yeunju Choi, Doyeop Kwak, Ji-Hoon Kim,\n  Seongkyu Mun, Joon Son Chung","authorsParsed":[["Ahn","Junseok",""],["Kim","Youkyum",""],["Choi","Yeunju",""],["Kwak","Doyeop",""],["Kim","Ji-Hoon",""],["Mun","Seongkyu",""],["Chung","Joon Son",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 04:27:13 GMT"}],"updateDate":"2024-07-29","timestamp":1721968033000,"abstract":"  This paper introduces VoxSim, a dataset of perceptual voice similarity\nratings. Recent efforts to automate the assessment of speech synthesis\ntechnologies have primarily focused on predicting mean opinion score of\nnaturalness, leaving speaker voice similarity relatively unexplored due to a\nlack of extensive training data. To address this, we generate about 41k\nutterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for\nspeaker recognition, and collect nearly 70k speaker similarity scores through a\nlistening test. VoxSim offers a valuable resource for the development and\nbenchmarking of speaker similarity prediction models. We provide baseline\nresults of speaker similarity prediction models on the VoxSim test set and\nfurther demonstrate that the model trained on our dataset generalises to the\nout-of-domain VCC2018 dataset.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}