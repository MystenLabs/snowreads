{"id":"2408.11444","title":"A Practical Trigger-Free Backdoor Attack on Neural Networks","authors":"Jiahao Wang, Xianglong Zhang, Xiuzhen Cheng, Pengfei Hu and Guoming\n  Zhang","authorsParsed":[["Wang","Jiahao",""],["Zhang","Xianglong",""],["Cheng","Xiuzhen",""],["Hu","Pengfei",""],["Zhang","Guoming",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 08:53:36 GMT"}],"updateDate":"2024-08-22","timestamp":1724230416000,"abstract":"  Backdoor attacks on deep neural networks have emerged as significant security\nthreats, especially as DNNs are increasingly deployed in security-critical\napplications. However, most existing works assume that the attacker has access\nto the original training data. This limitation restricts the practicality of\nlaunching such attacks in real-world scenarios. Additionally, using a specified\ntrigger to activate the injected backdoor compromises the stealthiness of the\nattacks. To address these concerns, we propose a trigger-free backdoor attack\nthat does not require access to any training data. Specifically, we design a\nnovel fine-tuning approach that incorporates the concept of malicious data into\nthe concept of the attacker-specified class, resulting the misclassification of\ntrigger-free malicious data into the attacker-specified class. Furthermore,\ninstead of relying on training data to preserve the model's knowledge, we\nemploy knowledge distillation methods to maintain the performance of the\ninfected model on benign samples, and introduce a parameter importance\nevaluation mechanism based on elastic weight constraints to facilitate the\nfine-tuning of the infected model. The effectiveness, practicality, and\nstealthiness of the proposed attack are comprehensively evaluated on three\nreal-world datasets. Furthermore, we explore the potential for enhancing the\nattack through the use of auxiliary datasets and model inversion.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}