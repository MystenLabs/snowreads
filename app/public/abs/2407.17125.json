{"id":"2407.17125","title":"Behavioral Testing: Can Large Language Models Implicitly Resolve\n  Ambiguous Entities?","authors":"Anastasiia Sedova, Robert Litschko, Diego Frassinelli, Benjamin Roth,\n  Barbara Plank","authorsParsed":[["Sedova","Anastasiia",""],["Litschko","Robert",""],["Frassinelli","Diego",""],["Roth","Benjamin",""],["Plank","Barbara",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 09:48:48 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 07:39:44 GMT"}],"updateDate":"2024-07-26","timestamp":1721814528000,"abstract":"  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. In this paper, we\nfocus on entity type ambiguity and analyze current state-of-the-art LLMs for\ntheir proficiency and consistency in applying their factual knowledge when\nprompted for entities under ambiguity. To do so, we propose an evaluation\nprotocol that disentangles knowing from applying knowledge, and test\nstate-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform\npoorly with ambiguous prompts, achieving only 80% accuracy. Our results further\ndemonstrate systematic discrepancies in LLM behavior and their failure to\nconsistently apply information, indicating that the models can exhibit\nknowledge without being able to utilize it, significant biases for preferred\nreadings, as well as self inconsistencies. Our study highlights the importance\nof handling entity ambiguity in future for more trustworthy LLMs\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2gg5c4IhCxcjm2J5NIV6hcJ3FbddLd15LTh8nm1EXLY","pdfSize":"1241055"}
