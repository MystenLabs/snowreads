{"id":"2407.16958","title":"Cheems: Wonderful Matrices More Efficient and More Effective\n  Architecture","authors":"Jingze Shi, Lu He, Yuhan Wang, Tianyu He, Bingheng Wu and Mingkun Hou","authorsParsed":[["Shi","Jingze",""],["He","Lu",""],["Wang","Yuhan",""],["He","Tianyu",""],["Wu","Bingheng",""],["Hou","Mingkun",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 02:52:02 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 01:34:13 GMT"}],"updateDate":"2024-07-26","timestamp":1721789522000,"abstract":"  Recent studies have shown that, relative position encoding performs well in\nselective state space model scanning algorithms, and the architecture that\nbalances SSM and Attention enhances the efficiency and effectiveness of the\nalgorithm, while the sparse activation of the mixture of experts reduces the\ntraining cost. I studied the effectiveness of using different position\nencodings in structured state space dual algorithms, and the more effective\nSSD-Attn internal and external function mixing method, and designed a more\nefficient cross domain mixture of experts. I found that the same matrix is very\nwonderful in different algorithms, which allows us to establish a new hybrid\nsparse architecture: Cheems. Compared with other hybrid architectures, it is\nmore efficient and more effective in language modeling tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"l2nD6oREj4tk_0sBkkk4PU-U9FlTHdBBFZVbJ7Ur43Y","pdfSize":"1665576"}
