{"id":"2407.00634","title":"Tarsier: Recipes for Training and Evaluating Large Video Description\n  Models","authors":"Jiawei Wang, Liping Yuan, Yuchen Zhang","authorsParsed":[["Wang","Jiawei",""],["Yuan","Liping",""],["Zhang","Yuchen",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 09:21:01 GMT"}],"updateDate":"2024-07-02","timestamp":1719739261000,"abstract":"  Generating fine-grained video descriptions is a fundamental challenge in\nvideo understanding. In this work, we introduce Tarsier, a family of\nlarge-scale video-language models designed to generate high-quality video\ndescriptions. Tarsier employs CLIP-ViT to encode frames separately and then\nuses an LLM to model temporal relationships. Despite its simple architecture,\nwe demonstrate that with a meticulously designed two-stage training procedure,\nthe Tarsier models exhibit substantially stronger video description\ncapabilities than any existing open-source model, showing a $+51.4\\%$ advantage\nin human side-by-side evaluation over the strongest model. Additionally, they\nare comparable to state-of-the-art proprietary models, with a $+12.3\\%$\nadvantage against GPT-4V and a $-6.7\\%$ disadvantage against Gemini 1.5 Pro.\nBesides video description, Tarsier proves to be a versatile generalist model,\nachieving new state-of-the-art results across nine public benchmarks, including\nmulti-choice VQA, open-ended VQA, and zero-shot video captioning. Our second\ncontribution is the introduction of a new benchmark for evaluating video\ndescription models, consisting of a new challenging dataset featuring videos\nfrom diverse sources and varying complexity, along with an automatic method\nspecifically designed to assess the quality of fine-grained video descriptions.\nWe make our models and evaluation benchmark publicly available at\n\\url{https://github.com/bytedance/tarsier}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}