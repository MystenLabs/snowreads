{"id":"2408.06854","title":"LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large\n  Language Models","authors":"Jia-Chen Zhang, Yu-Jie Xiong, He-Xi Qiu, Dong-Hai Zhu, Chun-Ming Xia","authorsParsed":[["Zhang","Jia-Chen",""],["Xiong","Yu-Jie",""],["Qiu","He-Xi",""],["Zhu","Dong-Hai",""],["Xia","Chun-Ming",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 12:31:30 GMT"}],"updateDate":"2024-08-14","timestamp":1723552290000,"abstract":"  Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}