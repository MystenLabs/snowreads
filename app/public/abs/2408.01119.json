{"id":"2408.01119","title":"Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer","authors":"Robert Belanec, Simon Ostermann, Ivan Srba, Maria Bielikova","authorsParsed":[["Belanec","Robert",""],["Ostermann","Simon",""],["Srba","Ivan",""],["Bielikova","Maria",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 09:00:03 GMT"}],"updateDate":"2024-08-05","timestamp":1722589203000,"abstract":"  Prompt tuning is a modular and efficient solution for training large language\nmodels (LLMs). One of its main advantages is task modularity, making it\nsuitable for multi-task problems. However, current soft-prompt-based methods\noften sacrifice multi-task modularity, requiring the training process to be\nfully or partially repeated for each newly added task. While recent work on\ntask vectors applied arithmetic operations on full model weights to achieve the\ndesired multi-task performance, a similar approach for soft-prompts is still\nmissing. To this end, we introduce Task Prompt Vectors, created by element-wise\ndifference between weights of tuned soft-prompts and their random\ninitialization. Experimental results on 12 NLU datasets show that task prompt\nvectors can be used in low-resource settings to effectively initialize prompt\ntuning on similar tasks. In addition, we show that task prompt vectors are\nindependent of the random initialization of prompt tuning. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, by\narithmetic addition of task prompt vectors from multiple tasks, we are able to\noutperform a state-of-the-art baseline in some cases.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DjhL2xeIlFlB1B9BI95d_7Tw-8lO5j6fz2AeidxCq6s","pdfSize":"1147747","txDigest":"CZpVhUNRfkSw3M37mcTNMERNHCyB2UEPHkTXXvNkbiYv","endEpoch":"1","status":"CERTIFIED"}
