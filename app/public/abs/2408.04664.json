{"id":"2408.04664","title":"Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via\n  Language-Contrastive Decoding (LCD)","authors":"Avshalom Manevich, Reut Tsarfaty","authorsParsed":[["Manevich","Avshalom",""],["Tsarfaty","Reut",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 08:10:34 GMT"}],"updateDate":"2024-08-12","timestamp":1722931834000,"abstract":"  Large Vision-Language Models (LVLMs) are an extension of Large Language\nModels (LLMs) that facilitate processing both image and text inputs, expanding\nAI capabilities. However, LVLMs struggle with object hallucinations due to\ntheir reliance on text cues and learned object co-occurrence biases. While most\nresearch quantifies these hallucinations, mitigation strategies are still\nlacking. Our study introduces a Language Contrastive Decoding (LCD) algorithm\nthat adjusts LVLM outputs based on LLM distribution confidence levels,\neffectively reducing object hallucinations. We demonstrate the advantages of\nLCD in leading LVLMs, showing up to %4 improvement in POPE F1 scores and up to\n%36 reduction in CHAIR scores on the COCO validation set, while also improving\ncaptioning quality scores. Our method effectively improves LVLMs without\nneeding complex post-processing or retraining, and is easily applicable to\ndifferent models. Our findings highlight the potential of further exploration\nof LVLM-specific decoding algorithms.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8Kp-fd1hHpOvqyre4quNAJ5xCM96ycdACHJn6BRtc68","pdfSize":"1441560"}
