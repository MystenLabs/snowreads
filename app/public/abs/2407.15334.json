{"id":"2407.15334","title":"Explore the LiDAR-Camera Dynamic Adjustment Fusion for 3D Object\n  Detection","authors":"Yiran Yang, Xu Gao, Tong Wang, Xin Hao, Yifeng Shi, Xiao Tan, Xiaoqing\n  Ye, Jingdong Wang","authorsParsed":[["Yang","Yiran",""],["Gao","Xu",""],["Wang","Tong",""],["Hao","Xin",""],["Shi","Yifeng",""],["Tan","Xiao",""],["Ye","Xiaoqing",""],["Wang","Jingdong",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 02:42:15 GMT"}],"updateDate":"2024-07-23","timestamp":1721616135000,"abstract":"  Camera and LiDAR serve as informative sensors for accurate and robust\nautonomous driving systems. However, these sensors often exhibit heterogeneous\nnatures, resulting in distributional modality gaps that present significant\nchallenges for fusion. To address this, a robust fusion technique is crucial,\nparticularly for enhancing 3D object detection. In this paper, we introduce a\ndynamic adjustment technology aimed at aligning modal distributions and\nlearning effective modality representations to enhance the fusion process.\nSpecifically, we propose a triphase domain aligning module. This module adjusts\nthe feature distributions from both the camera and LiDAR, bringing them closer\nto the ground truth domain and minimizing differences. Additionally, we explore\nimproved representation acquisition methods for dynamic fusion, which includes\nmodal interaction and specialty enhancement. Finally, an adaptive learning\ntechnique that merges the semantics and geometry information for dynamical\ninstance optimization. Extensive experiments in the nuScenes dataset present\ncompetitive performance with state-of-the-art approaches. Our code will be\nreleased in the future.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}