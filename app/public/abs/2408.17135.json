{"id":"2408.17135","title":"Temporal and Interactive Modeling for Efficient Human-Human Motion\n  Generation","authors":"Yabiao Wang, Shuo Wang, Jiangning Zhang, Ke Fan, Jiafu Wu, Zhengkai\n  Jiang, Yong Liu","authorsParsed":[["Wang","Yabiao",""],["Wang","Shuo",""],["Zhang","Jiangning",""],["Fan","Ke",""],["Wu","Jiafu",""],["Jiang","Zhengkai",""],["Liu","Yong",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 09:22:07 GMT"}],"updateDate":"2024-09-02","timestamp":1725009727000,"abstract":"  Human-human motion generation is essential for understanding humans as social\nbeings. Although several transformer-based methods have been proposed, they\ntypically model each individual separately and overlook the causal\nrelationships in temporal motion sequences. Furthermore, the attention\nmechanism in transformers exhibits quadratic computational complexity,\nsignificantly reducing their efficiency when processing long sequences. In this\npaper, we introduce TIM (Temporal and Interactive Modeling), an efficient and\neffective approach that presents the pioneering human-human motion generation\nmodel utilizing RWKV. Specifically, we first propose Causal Interactive\nInjection to leverage the temporal properties of motion sequences and avoid\nnon-causal and cumbersome modeling. Then we present Role-Evolving Mixing to\nadjust to the ever-evolving roles throughout the interaction. Finally, to\ngenerate smoother and more rational motion, we design Localized Pattern\nAmplification to capture short-term motion patterns. Extensive experiments on\nInterHuman demonstrate that our method achieves superior performance. Notably,\nTIM has achieved state-of-the-art results using only 32% of InterGen's\ntrainable parameters. Code will be available soon. Homepage:\nhttps://aigc-explorer.github.io/TIM-page/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}