{"id":"2407.10468","title":"LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis","authors":"Zhenxiong Tan, Xinyin Ma, Gongfan Fang, Xinchao Wang","authorsParsed":[["Tan","Zhenxiong",""],["Ma","Xinyin",""],["Fang","Gongfan",""],["Wang","Xinchao",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 06:49:05 GMT"}],"updateDate":"2024-07-16","timestamp":1721026145000,"abstract":"  Latent diffusion models have shown promising results in audio generation,\nmaking notable advancements over traditional methods. However, their\nperformance, while impressive with short audio clips, faces challenges when\nextended to longer audio sequences. These challenges are due to model's\nself-attention mechanism and training predominantly on 10-second clips, which\ncomplicates the extension to longer audio without adaptation. In response to\nthese issues, we introduce a novel approach, LiteFocus that enhances the\ninference of existing audio latent diffusion models in long audio synthesis.\nObserved the attention pattern in self-attention, we employ a dual sparse form\nfor attention calculation, designated as same-frequency focus and\ncross-frequency compensation, which curtails the attention computation under\nsame-frequency constraints, while enhancing audio quality through\ncross-frequency refillment. LiteFocus demonstrates substantial reduction on\ninference time with diffusion-based TTA model by 1.99x in synthesizing\n80-second audio clips while also obtaining improved audio quality.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}