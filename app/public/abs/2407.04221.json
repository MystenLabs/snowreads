{"id":"2407.04221","title":"Autoverse: An Evolvable Game Language for Learning Robust Embodied\n  Agents","authors":"Sam Earle, Julian Togelius","authorsParsed":[["Earle","Sam",""],["Togelius","Julian",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 02:18:02 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 09:39:14 GMT"}],"updateDate":"2024-08-07","timestamp":1720145882000,"abstract":"  We introduce Autoverse, an evolvable, domain-specific language for\nsingle-player 2D grid-based games, and demonstrate its use as a scalable\ntraining ground for Open-Ended Learning (OEL) algorithms. Autoverse uses\ncellular-automaton-like rewrite rules to describe game mechanics, allowing it\nto express various game environments (e.g. mazes, dungeons, sokoban puzzles)\nthat are popular testbeds for Reinforcement Learning (RL) agents. Each rewrite\nrule can be expressed as a series of simple convolutions, allowing for\nenvironments to be parallelized on the GPU, thereby drastically accelerating RL\ntraining. Using Autoverse, we propose jump-starting open-ended learning by\nimitation learning from search. In such an approach, we first evolve Autoverse\nenvironments (their rules and initial map topology) to maximize the number of\niterations required by greedy tree search to discover a new best solution,\nproducing a curriculum of increasingly complex environments and playtraces. We\nthen distill these expert playtraces into a neural-network-based policy using\nimitation learning. Finally, we use the learned policy as a starting point for\nopen-ended RL, where new training environments are continually evolved to\nmaximize the RL player agent's value function error (a proxy for its regret, or\nthe learnability of generated environments), finding that this approach\nimproves the performance and generality of resultant player agents.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}