{"id":"2407.15869","title":"Long Input Sequence Network for Long Time Series Forecasting","authors":"Chao Ma, Yikai Hou, Xiang Li, Yinggang Sun, Haining Yu","authorsParsed":[["Ma","Chao",""],["Hou","Yikai",""],["Li","Xiang",""],["Sun","Yinggang",""],["Yu","Haining",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 08:43:12 GMT"}],"updateDate":"2024-07-24","timestamp":1721292192000,"abstract":"  Short fixed-length inputs are the main bottleneck of deep learning methods in\nlong time-series forecasting tasks. Prolonging input length causes overfitting,\nrapidly deteriorating accuracy. Our research indicates that the overfitting is\na combination reaction of the multi-scale pattern coupling in time series and\nthe fixed focusing scale of current models. First, we find that the patterns\nexhibited by a time series across various scales are reflective of its\nmulti-periodic nature, where each scale corresponds to specific period length.\nSecond, We find that the token size predominantly dictates model behavior, as\nit determines the scale at which the model focuses and the context size it can\naccommodate. Our idea is to decouple the multi-scale temporal patterns of time\nseries and to model each pattern with its corresponding period length as token\nsize. We introduced a novel series-decomposition module(MPSD), and a\nMulti-Token Pattern Recognition neural network(MTPR), enabling the model to\nhandle \\textit{inputs up to $10\\times$ longer}. Sufficient context enhances\nperformance(\\textit{38% maximum precision improvement}), and the decoupling\napproach offers \\textit{Low complexity($0.22\\times$ cost)} and \\textit{high\ninterpretability}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}