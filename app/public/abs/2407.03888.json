{"id":"2407.03888","title":"Continuous-time q-Learning for Jump-Diffusion Models under Tsallis\n  Entropy","authors":"Lijun Bo, Yijie Huang, Xiang Yu, Tingting Zhang","authorsParsed":[["Bo","Lijun",""],["Huang","Yijie",""],["Yu","Xiang",""],["Zhang","Tingting",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 12:26:31 GMT"}],"updateDate":"2024-07-08","timestamp":1720095991000,"abstract":"  This paper studies continuous-time reinforcement learning for controlled\njump-diffusion models by featuring the q-function (the continuous-time\ncounterpart of Q-function) and the q-learning algorithms under the Tsallis\nentropy regularization. Contrary to the conventional Shannon entropy, the\ngeneral form of Tsallis entropy renders the optimal policy not necessary a\nGibbs measure, where some Lagrange multiplier and KKT multiplier naturally\narise from certain constraints to ensure the learnt policy to be a probability\ndistribution. As a consequence,the relationship between the optimal policy and\nthe q-function also involves the Lagrange multiplier. In response, we establish\nthe martingale characterization of the q-function under Tsallis entropy and\ndevise two q-learning algorithms depending on whether the Lagrange multiplier\ncan be derived explicitly or not. In the latter case, we need to consider\ndifferent parameterizations of the q-function and the policy and update them\nalternatively. Finally, we examine two financial applications, namely an\noptimal portfolio liquidation problem and a non-LQ control problem. It is\ninteresting to see therein that the optimal policies under the Tsallis entropy\nregularization can be characterized explicitly, which are distributions\nconcentrate on some compact support. The satisfactory performance of our\nq-learning algorithm is illustrated in both examples.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}