{"id":"2407.20272","title":"An Efficient Inference Framework for Early-exit Large Language Models","authors":"Ruijie Miao, Yihan Yan, Xinshuo Yao, Tong Yang","authorsParsed":[["Miao","Ruijie",""],["Yan","Yihan",""],["Yao","Xinshuo",""],["Yang","Tong",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 07:50:17 GMT"}],"updateDate":"2024-07-31","timestamp":1721893817000,"abstract":"  Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}