{"id":"2408.01963","title":"A Novel Metric for Measuring the Robustness of Large Language Models in\n  Non-adversarial Scenarios","authors":"Samuel Ackerman, Ella Rabinovich, Eitan Farchi, Ateret Anaby-Tavor","authorsParsed":[["Ackerman","Samuel",""],["Rabinovich","Ella",""],["Farchi","Eitan",""],["Anaby-Tavor","Ateret",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 08:43:09 GMT"}],"updateDate":"2024-08-06","timestamp":1722760989000,"abstract":"  We evaluate the robustness of several large language models on multiple\ndatasets. Robustness here refers to the relative insensitivity of the model's\nanswers to meaning-preserving variants of their input. Benchmark datasets are\nconstructed by introducing naturally-occurring, non-malicious perturbations, or\nby generating semantically equivalent paraphrases of input questions or\nstatements. We further propose a novel metric for assessing a model robustness,\nand demonstrate its benefits in the non-adversarial scenario by empirical\nevaluation of several models on the created datasets.\n","subjects":["Computing Research Repository/Computation and Language","Statistics/Applications"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LB54Mu3NBL1DtWTsFhObiiy7xEbzkVXa4q8y7NpQdSE","pdfSize":"742707","txDigest":"CmVjdMJ2FsSgjQk5wuZVy6FgygAZEbZwsGgVDZRCzGWG","endEpoch":"1","status":"CERTIFIED"}
