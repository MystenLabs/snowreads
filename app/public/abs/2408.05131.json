{"id":"2408.05131","title":"Range Membership Inference Attacks","authors":"Jiashu Tao, Reza Shokri","authorsParsed":[["Tao","Jiashu",""],["Shokri","Reza",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 15:39:06 GMT"}],"updateDate":"2024-08-12","timestamp":1723217946000,"abstract":"  Machine learning models can leak private information about their training\ndata, but the standard methods to measure this risk, based on membership\ninference attacks (MIAs), have a major limitation. They only check if a given\ndata point \\textit{exactly} matches a training point, neglecting the potential\nof similar or partially overlapping data revealing the same private\ninformation. To address this issue, we introduce the class of range membership\ninference attacks (RaMIAs), testing if the model was trained on any data in a\nspecified range (defined based on the semantics of privacy). We formulate the\nRaMIAs game and design a principled statistical test for its complex\nhypotheses. We show that RaMIAs can capture privacy loss more accurately and\ncomprehensively than MIAs on various types of data, such as tabular, image, and\nlanguage. RaMIA paves the way for a more comprehensive and meaningful privacy\nauditing of machine learning algorithms.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}