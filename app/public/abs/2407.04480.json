{"id":"2407.04480","title":"LoCo: Low-Bit Communication Adaptor for Large-scale Model Training","authors":"Xingyu Xie, Zhijie Lin, Kim-Chuan Toh, Pan Zhou","authorsParsed":[["Xie","Xingyu",""],["Lin","Zhijie",""],["Toh","Kim-Chuan",""],["Zhou","Pan",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 13:01:36 GMT"}],"updateDate":"2024-07-08","timestamp":1720184496000,"abstract":"  To efficiently train large-scale models, low-bit gradient communication\ncompresses full-precision gradients on local GPU nodes into low-precision ones\nfor higher gradient synchronization efficiency among GPU nodes. However, it\noften degrades training quality due to compression information loss. To address\nthis, we propose the Low-bit Communication Adaptor (LoCo), which compensates\ngradients on local GPU nodes before compression, ensuring efficient\nsynchronization without compromising training quality. Specifically, LoCo\ndesigns a moving average of historical compensation errors to stably estimate\nconcurrent compression error and then adopts it to compensate for the\nconcurrent gradient compression, yielding a less lossless compression. This\nmechanism allows it to be compatible with general optimizers like Adam and\nsharding strategies like FSDP. Theoretical analysis shows that integrating LoCo\ninto full-precision optimizers like Adam and SGD does not impair their\nconvergence speed on nonconvex problems. Experimental results show that across\nlarge-scale model training frameworks like Megatron-LM and PyTorch's FSDP, LoCo\nsignificantly improves communication efficiency, e.g., improving Adam's\ntraining speed by 14% to 40% without performance degradation on large language\nmodels like LLAMAs and MoE.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}