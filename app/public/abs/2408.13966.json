{"id":"2408.13966","title":"Reducing the Cost: Cross-Prompt Pre-Finetuning for Short Answer Scoring","authors":"Hiroaki Funayama, Yuya Asazuma, Yuichiroh Matsubayashi, Tomoya\n  Mizumoto and Kentaro Inui","authorsParsed":[["Funayama","Hiroaki",""],["Asazuma","Yuya",""],["Matsubayashi","Yuichiroh",""],["Mizumoto","Tomoya",""],["Inui","Kentaro",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 00:23:56 GMT"}],"updateDate":"2024-08-27","timestamp":1724631836000,"abstract":"  Automated Short Answer Scoring (SAS) is the task of automatically scoring a\ngiven input to a prompt based on rubrics and reference answers. Although SAS is\nuseful in real-world applications, both rubrics and reference answers differ\nbetween prompts, thus requiring a need to acquire new data and train a model\nfor each new prompt. Such requirements are costly, especially for schools and\nonline courses where resources are limited and only a few prompts are used. In\nthis work, we attempt to reduce this cost through a two-phase approach: train a\nmodel on existing rubrics and answers with gold score signals and finetune it\non a new prompt. Specifically, given that scoring rubrics and reference answers\ndiffer for each prompt, we utilize key phrases, or representative expressions\nthat the answer should contain to increase scores, and train a SAS model to\nlearn the relationship between key phrases and answers using already annotated\nprompts (i.e., cross-prompts). Our experimental results show that finetuning on\nexisting cross-prompt data with key phrases significantly improves scoring\naccuracy, especially when the training data is limited. Finally, our extensive\nanalysis shows that it is crucial to design the model so that it can learn the\ntask's general property.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}