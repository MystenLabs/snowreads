{"id":"2408.13980","title":"FusionSAM: Latent Space driven Segment Anything Model for Multimodal\n  Fusion and Segmentation","authors":"Daixun Li, Weiying Xie, Mingxiang Cao, Yunke Wang, Jiaqing Zhang,\n  Yunsong Li, Leyuan Fang, Chang Xu","authorsParsed":[["Li","Daixun",""],["Xie","Weiying",""],["Cao","Mingxiang",""],["Wang","Yunke",""],["Zhang","Jiaqing",""],["Li","Yunsong",""],["Fang","Leyuan",""],["Xu","Chang",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 02:20:55 GMT"}],"updateDate":"2024-08-27","timestamp":1724638855000,"abstract":"  Multimodal image fusion and segmentation enhance scene understanding in\nautonomous driving by integrating data from various sensors. However, current\nmodels struggle to efficiently segment densely packed elements in such scenes,\ndue to the absence of comprehensive fusion features that can guide mid-process\nfine-tuning and focus attention on relevant areas. The Segment Anything Model\n(SAM) has emerged as a transformative segmentation method. It provides more\neffective prompts through its flexible prompt encoder, compared to transformers\nlacking fine-tuned control. Nevertheless, SAM has not been extensively studied\nin the domain of multimodal fusion for natural images. In this paper, we\nintroduce SAM into multimodal image segmentation for the first time, proposing\na novel framework that combines Latent Space Token Generation (LSTG) and Fusion\nMask Prompting (FMP) modules to enhance SAM's multimodal fusion and\nsegmentation capabilities. Specifically, we first obtain latent space features\nof the two modalities through vector quantization and embed them into a\ncross-attention-based inter-domain fusion module to establish long-range\ndependencies between modalities. Then, we use these comprehensive fusion\nfeatures as prompts to guide precise pixel-level segmentation. Extensive\nexperiments on several public datasets demonstrate that the proposed method\nsignificantly outperforms SAM and SAM2 in multimodal autonomous driving\nscenarios, achieving at least 3.9$\\%$ higher segmentation mIoU than the\nstate-of-the-art approaches.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}