{"id":"2408.06610","title":"CROME: Cross-Modal Adapters for Efficient Multimodal LLM","authors":"Sayna Ebrahimi, Sercan O. Arik, Tejas Nama, Tomas Pfister","authorsParsed":[["Ebrahimi","Sayna",""],["Arik","Sercan O.",""],["Nama","Tejas",""],["Pfister","Tomas",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 03:45:11 GMT"}],"updateDate":"2024-08-14","timestamp":1723520711000,"abstract":"  Multimodal Large Language Models (MLLMs) demonstrate remarkable\nimage-language capabilities, but their widespread use faces challenges in\ncost-effective training and adaptation. Existing approaches often necessitate\nexpensive language model retraining and limited adaptability. Additionally, the\ncurrent focus on zero-shot performance improvements offers insufficient\nguidance for task-specific tuning. We propose CROME, an efficient\nvision-language instruction tuning framework. It features a novel gated\ncross-modal adapter that effectively combines visual and textual\nrepresentations prior to input into a frozen LLM. This lightweight adapter,\ntrained with minimal parameters, enables efficient cross-modal understanding.\nNotably, CROME demonstrates superior zero-shot performance on standard visual\nquestion answering and instruction-following benchmarks. Moreover, it yields\nfine-tuning with exceptional parameter efficiency, competing with task-specific\nspecialist state-of-the-art methods. CROME demonstrates the potential of pre-LM\nalignment for building scalable, adaptable, and parameter-efficient multimodal\nmodels.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZZaD5mSjsoqFdI7NACOQEhSjY0lg2QVZkzy9mhkjoeY","pdfSize":"1974261"}
