{"id":"2407.05324","title":"PICA: Physics-Integrated Clothed Avatar","authors":"Bo Peng, Yunfan Tao, Haoyu Zhan, Yudong Guo and Juyong Zhang","authorsParsed":[["Peng","Bo",""],["Tao","Yunfan",""],["Zhan","Haoyu",""],["Guo","Yudong",""],["Zhang","Juyong",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 10:23:21 GMT"}],"updateDate":"2024-07-09","timestamp":1720347801000,"abstract":"  We introduce PICA, a novel representation for high-fidelity animatable\nclothed human avatars with physics-accurate dynamics, even for loose clothing.\nPrevious neural rendering-based representations of animatable clothed humans\ntypically employ a single model to represent both the clothing and the\nunderlying body. While efficient, these approaches often fail to accurately\nrepresent complex garment dynamics, leading to incorrect deformations and\nnoticeable rendering artifacts, especially for sliding or loose garments.\nFurthermore, previous works represent garment dynamics as pose-dependent\ndeformations and facilitate novel pose animations in a data-driven manner. This\noften results in outcomes that do not faithfully represent the mechanics of\nmotion and are prone to generating artifacts in out-of-distribution poses. To\naddress these issues, we adopt two individual 3D Gaussian Splatting (3DGS)\nmodels with different deformation characteristics, modeling the human body and\nclothing separately. This distinction allows for better handling of their\nrespective motion characteristics. With this representation, we integrate a\ngraph neural network (GNN)-based clothed body physics simulation module to\nensure an accurate representation of clothing dynamics. Our method, through its\ncarefully designed features, achieves high-fidelity rendering of clothed human\nbodies in complex and novel driving poses, significantly outperforming previous\nmethods under the same settings.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}