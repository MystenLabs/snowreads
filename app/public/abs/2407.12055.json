{"id":"2407.12055","title":"Integrating Query-aware Segmentation and Cross-Attention for Robust VQA","authors":"Wonjun Choi, Sangbeom Lee, Seungyeon Lee, Heechul Jung and Dong-Gyu\n  Lee","authorsParsed":[["Choi","Wonjun",""],["Lee","Sangbeom",""],["Lee","Seungyeon",""],["Jung","Heechul",""],["Lee","Dong-Gyu",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 04:48:44 GMT"}],"updateDate":"2024-07-18","timestamp":1720500524000,"abstract":"  This paper introduces a method for VizWiz-VQA using LVLM with trainable\ncross-attention and LoRA finetuning. We train the model with the following\nconditions: 1) Training with original images. 2) Training with enhanced images\nusing CLIPSeg to highlight or contrast the original image. 3) Training with\nintegrating the output features of Vision Transformer (ViT) and CLIPSeg\nfeatures of the original images. Then, we ensemble the results based on\nLevenshtein distance to enhance the prediction of the final answer. In the\nexperiments, we demonstrate and analyze the proposed method's effectiveness.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}