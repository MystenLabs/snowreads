{"id":"2408.17068","title":"User-Driven Voice Generation and Editing through Latent Space Navigation","authors":"Yusheng Tian, Junbin Liu, Tan Lee","authorsParsed":[["Tian","Yusheng",""],["Liu","Junbin",""],["Lee","Tan",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 07:51:45 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 10:14:11 GMT"}],"updateDate":"2024-09-10","timestamp":1725004305000,"abstract":"  This paper presents a user-driven approach for synthesizing highly specific\ntarget voices based on user feedback, which is particularly beneficial for\nspeech-impaired individuals who wish to recreate their lost voices but lack\nprior recordings. Specifically, we leverage the neural analysis and synthesis\nframework to construct a low-dimensional, yet sufficiently expressive latent\nspeaker embedding space. Within this latent space, we implement a search\nalgorithm that guides users to their desired voice through completing a\nsequence of straightforward comparison tasks. Both synthetic simulations and\nreal-world user studies demonstrate that the proposed approach can effectively\napproximate target voices. Moreover, by analyzing the mel-spectrogram\ngenerator's Jacobians, we identify a set of meaningful voice editing directions\nwithin the latent space. These directions enable users to further fine-tune\nspecific attributes of the generated voice, including the pitch level, pitch\nrange, volume, vocal tension, nasality, and tone color. Audio samples are\navailable at https://myspeechprojects.github.io/voicedesign/.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}