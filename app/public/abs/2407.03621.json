{"id":"2407.03621","title":"The Mysterious Case of Neuron 1512: Injectable Realignment Architectures\n  Reveal Internal Characteristics of Meta's Llama 2 Model","authors":"Brenden Smith, Dallin Baker, Clayton Chase, Myles Barney, Kaden\n  Parker, Makenna Allred, Peter Hu, Alex Evans, Nancy Fulda","authorsParsed":[["Smith","Brenden",""],["Baker","Dallin",""],["Chase","Clayton",""],["Barney","Myles",""],["Parker","Kaden",""],["Allred","Makenna",""],["Hu","Peter",""],["Evans","Alex",""],["Fulda","Nancy",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 04:05:19 GMT"}],"updateDate":"2024-07-08","timestamp":1720065919000,"abstract":"  Large Language Models (LLMs) have an unrivaled and invaluable ability to\n\"align\" their output to a diverse range of human preferences, by mirroring them\nin the text they generate. The internal characteristics of such models,\nhowever, remain largely opaque. This work presents the Injectable Realignment\nModel (IRM) as a novel approach to language model interpretability and\nexplainability. Inspired by earlier work on Neural Programming Interfaces, we\nconstruct and train a small network -- the IRM -- to induce emotion-based\nalignments within a 7B parameter LLM architecture. The IRM outputs are injected\nvia layerwise addition at various points during the LLM's forward pass, thus\nmodulating its behavior without changing the weights of the original model.\nThis isolates the alignment behavior from the complex mechanisms of the\ntransformer model. Analysis of the trained IRM's outputs reveals a curious\npattern. Across more than 24 training runs and multiple alignment datasets,\npatterns of IRM activations align themselves in striations associated with a\nneuron's index within each transformer layer, rather than being associated with\nthe layers themselves. Further, a single neuron index (1512) is strongly\ncorrelated with all tested alignments. This result, although initially\ncounterintuitive, is directly attributable to design choices present within\nalmost all commercially available transformer architectures, and highlights a\npotential weak point in Meta's pretrained Llama 2 models. It also demonstrates\nthe value of the IRM architecture for language model analysis and\ninterpretability. Our code and datasets are available at\nhttps://github.com/DRAGNLabs/injectable-alignment-model\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}