{"id":"2407.18322","title":"The Need for Guardrails with Large Language Models in Medical\n  Safety-Critical Settings: An Artificial Intelligence Application in the\n  Pharmacovigilance Ecosystem","authors":"Joe B Hakim, Jeffery L Painter, Darmendra Ramcharran, Vijay Kara, Greg\n  Powell, Paulina Sobczak, Chiho Sato, Andrew Bate, Andrew Beam","authorsParsed":[["Hakim","Joe B",""],["Painter","Jeffery L",""],["Ramcharran","Darmendra",""],["Kara","Vijay",""],["Powell","Greg",""],["Sobczak","Paulina",""],["Sato","Chiho",""],["Bate","Andrew",""],["Beam","Andrew",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 19:52:41 GMT"},{"version":"v2","created":"Wed, 4 Sep 2024 17:16:05 GMT"}],"updateDate":"2024-09-05","timestamp":1719863561000,"abstract":"  Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rQ00PQEnvJ1cVE-SsP73UqwZZXgO8msAgQxpP1b3fag","pdfSize":"2263406","objectId":"0x925c34c0fde671e0cf8befa8c3970f4922949c534c84b307e5cb4970763ceba7","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
