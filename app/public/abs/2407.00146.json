{"id":"2407.00146","title":"The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language\n  Understanding in Arabic","authors":"Shahad Al-Khalifa and Hend Al-Khalifa","authorsParsed":[["Al-Khalifa","Shahad",""],["Al-Khalifa","Hend",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 16:34:31 GMT"}],"updateDate":"2024-07-02","timestamp":1719592471000,"abstract":"  Despite the growing importance of Arabic as a global language, there is a\nnotable lack of language models pre-trained exclusively on Arabic data. This\nshortage has led to limited benchmarks available for assessing language model\nperformance in Arabic. To address this gap, we introduce two novel benchmarks\ndesigned to evaluate models' mathematical reasoning and language understanding\nabilities in Arabic. These benchmarks are derived from a General Aptitude Test\n(GAT) called Qiyas exam, a standardized test widely used for university\nadmissions in Saudi Arabia. For validation purposes, we assess the performance\nof ChatGPT-3.5-trubo and ChatGPT-4 on our benchmarks. Our findings reveal that\nthese benchmarks pose a significant challenge, with ChatGPT-4 achieving an\noverall average accuracy of 64%, while ChatGPT-3.5-trubo achieved an overall\naccuracy of 49% across the various question types in the Qiyas benchmark. We\nbelieve the release of these benchmarks will pave the way for enhancing the\nmathematical reasoning and language understanding capabilities of future models\ntailored for the low-resource Arabic language.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}