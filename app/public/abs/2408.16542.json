{"id":"2408.16542","title":"SALSA: Speedy ASR-LLM Synchronous Aggregation","authors":"Ashish Mittal, Darshan Prabhu, Sunita Sarawagi, Preethi Jyothi","authorsParsed":[["Mittal","Ashish",""],["Prabhu","Darshan",""],["Sarawagi","Sunita",""],["Jyothi","Preethi",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 14:00:57 GMT"}],"updateDate":"2024-08-30","timestamp":1724940057000,"abstract":"  Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"V0ZHvD_xkY10iHvy2ikE-WHs-F31RxIjj5hKNFF-O7Q","pdfSize":"1719941"}
