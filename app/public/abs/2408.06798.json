{"id":"2408.06798","title":"Token Compensator: Altering Inference Cost of Vision Transformer without\n  Re-Tuning","authors":"Shibo Jie, Yehui Tang, Jianyuan Guo, Zhi-Hong Deng, Kai Han, Yunhe\n  Wang","authorsParsed":[["Jie","Shibo",""],["Tang","Yehui",""],["Guo","Jianyuan",""],["Deng","Zhi-Hong",""],["Han","Kai",""],["Wang","Yunhe",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 10:36:43 GMT"}],"updateDate":"2024-08-14","timestamp":1723545403000,"abstract":"  Token compression expedites the training and inference of Vision Transformers\n(ViTs) by reducing the number of the redundant tokens, e.g., pruning\ninattentive tokens or merging similar tokens. However, when applied to\ndownstream tasks, these approaches suffer from significant performance drop\nwhen the compression degrees are mismatched between training and inference\nstages, which limits the application of token compression on off-the-shelf\ntrained models. In this paper, we propose a model arithmetic framework to\ndecouple the compression degrees between the two stages. In advance, we\nadditionally perform a fast parameter-efficient self-distillation stage on the\npre-trained models to obtain a small plugin, called Token Compensator (ToCom),\nwhich describes the gap between models across different compression degrees.\nDuring inference, ToCom can be directly inserted into any downstream\noff-the-shelf models with any mismatched training and inference compression\ndegrees to acquire universal performance improvements without further training.\nExperiments on over 20 downstream tasks demonstrate the effectiveness of our\nframework. On CIFAR100, fine-grained visual classification, and VTAB-1k, ToCom\ncan yield up to a maximum improvement of 2.3%, 1.5%, and 2.0% in the average\nperformance of DeiT-B, respectively. Code: https://github.com/JieShibo/ToCom\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}