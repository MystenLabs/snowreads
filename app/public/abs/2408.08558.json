{"id":"2408.08558","title":"Linear combinations of latents in diffusion models: interpolation and\n  beyond","authors":"Erik Bodin, Henry Moss, Carl Henrik Ek","authorsParsed":[["Bodin","Erik",""],["Moss","Henry",""],["Ek","Carl Henrik",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 06:43:58 GMT"}],"updateDate":"2024-08-19","timestamp":1723790638000,"abstract":"  Generative models are crucial for applications like data synthesis and\naugmentation. Diffusion, Flow Matching and Continuous Normalizing Flows have\nshown effectiveness across various modalities, and rely on Gaussian latent\nvariables for generation. As any generated object is directly associated with a\nparticular latent variable, we can manipulate the variables to exert control\nover the generation process. However, standard approaches for combining latent\nvariables, such as spherical interpolation, only apply or work well in special\ncases. Moreover, current methods for obtaining low-dimensional representations\nof the data, important for e.g. surrogate models for search and creative\napplications, are network and data modality specific. In this work we show that\nthe standard methods to combine variables do not yield intermediates following\nthe distribution the models are trained to expect. We propose Combination of\nGaussian variables (COG), a novel interpolation method that addresses this, is\neasy to implement yet matches or improves upon current methods. COG addresses\nlinear combinations in general and, as we demonstrate, also supports other\noperations including e.g. defining subspaces of the latent space, simplifying\nthe creation of expressive low-dimensional spaces of high-dimensional objects\nusing generative models based on Gaussian latents.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jQXvDGmCDOT1PGnrNjaVbnLogP-rjvGlEMQw2FAWXys","pdfSize":"43361437"}
