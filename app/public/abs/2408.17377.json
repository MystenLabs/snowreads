{"id":"2408.17377","title":"NDP: Next Distribution Prediction as a More Broad Target","authors":"Junhao Ruan, Abudukeyumu Abudula, Xinyu Liu, Bei Li, Yinqiao Li,\n  Chenglong Wang, Yuchun Fan, Yuan Ge, Tong Xiao and Jingbo Zhu","authorsParsed":[["Ruan","Junhao",""],["Abudula","Abudukeyumu",""],["Liu","Xinyu",""],["Li","Bei",""],["Li","Yinqiao",""],["Wang","Chenglong",""],["Fan","Yuchun",""],["Ge","Yuan",""],["Xiao","Tong",""],["Zhu","Jingbo",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 16:13:49 GMT"}],"updateDate":"2024-09-02","timestamp":1725034429000,"abstract":"  Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}