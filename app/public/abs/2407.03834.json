{"id":"2407.03834","title":"10 Years of Fair Representations: Challenges and Opportunities","authors":"Mattia Cerrato and Marius K\\\"oppel and Philipp Wolf and Stefan Kramer","authorsParsed":[["Cerrato","Mattia",""],["KÃ¶ppel","Marius",""],["Wolf","Philipp",""],["Kramer","Stefan",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 11:04:26 GMT"}],"updateDate":"2024-07-08","timestamp":1720091066000,"abstract":"  Fair Representation Learning (FRL) is a broad set of techniques, mostly based\non neural networks, that seeks to learn new representations of data in which\nsensitive or undesired information has been removed. Methodologically, FRL was\npioneered by Richard Zemel et al. about ten years ago. The basic concepts,\nobjectives and evaluation strategies for FRL methodologies remain unchanged to\nthis day. In this paper, we look back at the first ten years of FRL by i)\nrevisiting its theoretical standing in light of recent work in deep learning\ntheory that shows the hardness of removing information in neural network\nrepresentations and ii) presenting the results of a massive experimentation\n(225.000 model fits and 110.000 AutoML fits) we conducted with the objective of\nimproving on the common evaluation scenario for FRL. More specifically, we use\nautomated machine learning (AutoML) to adversarially \"mine\" sensitive\ninformation from supposedly fair representations. Our theoretical and\nexperimental analysis suggests that deterministic, unquantized FRL\nmethodologies have serious issues in removing sensitive information, which is\nespecially troubling as they might seem \"fair\" at first glance.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}