{"id":"2408.11710","title":"Leveraging Large Language Models for Enhancing the Understandability of\n  Generated Unit Tests","authors":"Amirhossein Deljouyi, Roham Koohestani, Maliheh Izadi, Andy Zaidman","authorsParsed":[["Deljouyi","Amirhossein",""],["Koohestani","Roham",""],["Izadi","Maliheh",""],["Zaidman","Andy",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 15:35:34 GMT"}],"updateDate":"2024-08-22","timestamp":1724254534000,"abstract":"  Automated unit test generators, particularly search-based software testing\ntools like EvoSuite, are capable of generating tests with high coverage.\nAlthough these generators alleviate the burden of writing unit tests, they\noften pose challenges for software engineers in terms of understanding the\ngenerated tests. To address this, we introduce UTGen, which combines\nsearch-based software testing and large language models to enhance the\nunderstandability of automatically generated test cases. We achieve this\nenhancement through contextualizing test data, improving identifier naming, and\nadding descriptive comments. Through a controlled experiment with 32\nparticipants from both academia and industry, we investigate how the\nunderstandability of unit tests affects a software engineer's ability to\nperform bug-fixing tasks. We selected bug-fixing to simulate a real-world\nscenario that emphasizes the importance of understandable test cases. We\nobserve that participants working on assignments with UTGen test cases fix up\nto 33% more bugs and use up to 20% less time when compared to baseline test\ncases. From the post-test questionnaire, we gathered that participants found\nthat enhanced test names, test data, and variable names improved their\nbug-fixing process.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}