{"id":"2407.03311","title":"Efficient Imitation Without Demonstrations via Value-Penalized Auxiliary\n  Control from Examples","authors":"Trevor Ablett, Bryan Chan, Jayce Haoran Wang, Jonathan Kelly","authorsParsed":[["Ablett","Trevor",""],["Chan","Bryan",""],["Wang","Jayce Haoran",""],["Kelly","Jonathan",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 17:54:11 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 02:01:07 GMT"}],"updateDate":"2024-09-10","timestamp":1720029251000,"abstract":"  Learning from examples of success is an ap pealing approach to reinforcement\nlearning but it presents a challenging exploration problem, especially for\ncomplex or long-horizon tasks. This work introduces value-penalized auxiliary\ncontrol from examples (VPACE), an algorithm that significantly improves\nexploration in example-based control by adding examples of simple auxiliary\ntasks. For instance, a manipulation task may have auxiliary examples of an\nobject being reached for, grasped, or lifted. We show that the na\\\"{i}ve\napplication of scheduled auxiliary control to example-based learning can lead\nto value overestimation and poor performance. We resolve the problem with an\nabove-success-level value penalty. Across both simulated and real robotic\nenvironments, we show that our approach substantially improves learning\nefficiency for challenging tasks, while maintaining bounded value estimates. We\ncompare with existing approaches to example-based learning, inverse\nreinforcement learning, and an exploration bonus. Preliminary results also\nsuggest that VPACE may learn more efficiently than the more common approaches\nof using full trajectories or true sparse rewards. Videos, code, and datasets:\nhttps://papers.starslab.ca/vpace.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}