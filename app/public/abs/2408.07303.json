{"id":"2408.07303","title":"Enhancing Visual Question Answering through Ranking-Based Hybrid\n  Training and Multimodal Fusion","authors":"Peiyuan Chen and Zecheng Zhang and Yiping Dong and Li Zhou and Han\n  Wang","authorsParsed":[["Chen","Peiyuan",""],["Zhang","Zecheng",""],["Dong","Yiping",""],["Zhou","Li",""],["Wang","Han",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 05:18:43 GMT"}],"updateDate":"2024-08-15","timestamp":1723612723000,"abstract":"  Visual Question Answering (VQA) is a challenging task that requires systems\nto provide accurate answers to questions based on image content. Current VQA\nmodels struggle with complex questions due to limitations in capturing and\nintegrating multimodal information effectively. To address these challenges, we\npropose the Rank VQA model, which leverages a ranking-inspired hybrid training\nstrategy to enhance VQA performance. The Rank VQA model integrates high-quality\nvisual features extracted using the Faster R-CNN model and rich semantic text\nfeatures obtained from a pre-trained BERT model. These features are fused\nthrough a sophisticated multimodal fusion technique employing multi-head\nself-attention mechanisms. Additionally, a ranking learning module is\nincorporated to optimize the relative ranking of answers, thus improving answer\naccuracy. The hybrid training strategy combines classification and ranking\nlosses, enhancing the model's generalization ability and robustness across\ndiverse datasets. Experimental results demonstrate the effectiveness of the\nRank VQA model. Our model significantly outperforms existing state-of-the-art\nmodels on standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of\nboth accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank\nVQA is evident in its ability to handle complex questions that require\nunderstanding nuanced details and making sophisticated inferences from the\nimage and text. This work highlights the effectiveness of a ranking-based\nhybrid training strategy in improving VQA performance and lays the groundwork\nfor further research in multimodal learning methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}