{"id":"2408.10764","title":"Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion\n  for Efficient Inference Intervention in Large Language Model","authors":"Chenhan Yuan, Fei Huang, Ru Peng, Keming Lu, Bowen Yu, Chang Zhou,\n  Jingren Zhou","authorsParsed":[["Yuan","Chenhan",""],["Huang","Fei",""],["Peng","Ru",""],["Lu","Keming",""],["Yu","Bowen",""],["Zhou","Chang",""],["Zhou","Jingren",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 12:00:35 GMT"}],"updateDate":"2024-08-21","timestamp":1724155235000,"abstract":"  Transformer-based large language models (LLMs) exhibit limitations such as\ngenerating unsafe responses, unreliable reasoning, etc. Existing inference\nintervention approaches attempt to mitigate these issues by finetuning\nadditional models to produce calibration signals (such as rewards) that guide\nthe LLM's decoding process. However, this solution introduces substantial time\nand space overhead due to the separate models required. This work proposes\nNon-disruptive parameters insertion (Otter), inserting extra parameters into\nthe transformer architecture to predict calibration signals along with the\noriginal LLM output. Otter offers state-of-the-art performance on multiple\ndemanding tasks while saving up to 86.5\\% extra space and 98.5\\% extra time.\nFurthermore, Otter seamlessly integrates with existing inference engines,\nrequiring only a one-line code change, and the original model response remains\naccessible after the parameter insertion. Our code is publicly available at\n\\url{https://github.com/chenhan97/Otter}\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}