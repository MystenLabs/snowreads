{"id":"2407.09375","title":"HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems\n  in Context","authors":"Federico Arangath Joseph, Kilian Konstantin Haefeli, Noah Liniger and\n  Caglar Gulcehre","authorsParsed":[["Joseph","Federico Arangath",""],["Haefeli","Kilian Konstantin",""],["Liniger","Noah",""],["Gulcehre","Caglar",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 15:56:11 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 15:34:25 GMT"}],"updateDate":"2024-07-22","timestamp":1720799771000,"abstract":"  This work explores the in-context learning capabilities of State Space Models\n(SSMs) and presents, to the best of our knowledge, the first theoretical\nexplanation of a possible underlying mechanism. We introduce a novel weight\nconstruction for SSMs, enabling them to predict the next state of any dynamical\nsystem after observing previous states without parameter fine-tuning. This is\naccomplished by extending the HiPPO framework to demonstrate that continuous\nSSMs can approximate the derivative of any input signal. Specifically, we find\nan explicit weight construction for continuous SSMs and provide an asymptotic\nerror bound on the derivative approximation. The discretization of this\ncontinuous SSM subsequently yields a discrete SSM that predicts the next state.\nFinally, we demonstrate the effectiveness of our parameterization empirically.\nThis work should be an initial step toward understanding how sequence models\nbased on SSMs learn in context.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}