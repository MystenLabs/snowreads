{"id":"2408.05534","title":"Can LLMs Replace Manual Annotation of Software Engineering Artifacts?","authors":"Toufique Ahmed, Premkumar Devanbu, Christoph Treude, Michael Pradel","authorsParsed":[["Ahmed","Toufique",""],["Devanbu","Premkumar",""],["Treude","Christoph",""],["Pradel","Michael",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 12:30:01 GMT"}],"updateDate":"2024-08-13","timestamp":1723293001000,"abstract":"  Experimental evaluations of software engineering innovations, e.g., tools and\nprocesses, often include human-subject studies as a component of a\nmulti-pronged strategy to obtain greater generalizability of the findings.\nHowever, human-subject studies in our field are challenging, due to the cost\nand difficulty of finding and employing suitable subjects, ideally,\nprofessional programmers with varying degrees of experience. Meanwhile, large\nlanguage models (LLMs) have recently started to demonstrate human-level\nperformance in several areas. This paper explores the possibility of\nsubstituting costly human subjects with much cheaper LLM queries in evaluations\nof code and code-related artifacts. We study this idea by applying six\nstate-of-the-art LLMs to ten annotation tasks from five datasets created by\nprior work, such as judging the accuracy of a natural language summary of a\nmethod or deciding whether a code change fixes a static analysis warning. Our\nresults show that replacing some human annotation effort with LLMs can produce\ninter-rater agreements equal or close to human-rater agreement. To help decide\nwhen and how to use LLMs in human-subject studies, we propose model-model\nagreement as a predictor of whether a given task is suitable for LLMs at all,\nand model confidence as a means to select specific samples where LLMs can\nsafely replace human annotators. Overall, our work is the first step toward\nmixed human-LLM evaluations in software engineering.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}