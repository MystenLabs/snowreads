{"id":"2408.06747","title":"ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic\n  Segmentation","authors":"Jingyun Wang and Guoliang Kang","authorsParsed":[["Wang","Jingyun",""],["Kang","Guoliang",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 09:10:48 GMT"}],"updateDate":"2024-08-14","timestamp":1723540248000,"abstract":"  Recent works utilize CLIP to perform the challenging unsupervised semantic\nsegmentation task where only images without annotations are available. However,\nwe observe that when adopting CLIP to such a pixel-level understanding task,\nunexpected bias (including class-preference bias and space-preference bias)\noccurs. Previous works don't explicitly model the bias, which largely\nconstrains the segmentation performance. In this paper, we propose to\nexplicitly model and rectify the bias existing in CLIP to facilitate the\nunsupervised semantic segmentation task. Specifically, we design a learnable\n''Reference'' prompt to encode class-preference bias and a projection of the\npositional embedding in vision transformer to encode space-preference bias\nrespectively. To avoid interference, two kinds of biases are firstly\nindependently encoded into the Reference feature and the positional feature.\nVia a matrix multiplication between two features, a bias logit map is generated\nto explicitly represent two kinds of biases. Then we rectify the logits of CLIP\nvia a simple element-wise subtraction. To make the rectified results smoother\nand more contextual, we design a mask decoder which takes the feature of CLIP\nand rectified logits as input and outputs a rectified segmentation mask with\nthe help of Gumbel-Softmax operation. To make the bias modeling and\nrectification process meaningful and effective, a contrastive loss based on\nmasked visual features and the text features of different classes is imposed.\nTo further improve the segmentation, we distill the knowledge from the\nrectified CLIP to the advanced segmentation architecture via minimizing our\ndesigned mask-guided, feature-guided and text-guided loss terms. Extensive\nexperiments on various benchmarks demonstrate that ReCLIP++ performs favorably\nagainst previous SOTAs. The implementation is available at:\nhttps://github.com/dogehhh/ReCLIP.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}