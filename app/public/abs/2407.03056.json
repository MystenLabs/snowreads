{"id":"2407.03056","title":"Improving Zero-shot Generalization of Learned Prompts via Unsupervised\n  Knowledge Distillation","authors":"Marco Mistretta, Alberto Baldrati, Marco Bertini and Andrew D.\n  Bagdanov","authorsParsed":[["Mistretta","Marco",""],["Baldrati","Alberto",""],["Bertini","Marco",""],["Bagdanov","Andrew D.",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 12:24:40 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 11:56:43 GMT"}],"updateDate":"2024-07-31","timestamp":1720009480000,"abstract":"  Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization\nto unseen tasks, but fall short of the performance of supervised methods in\ngeneralizing to downstream tasks with limited data. Prompt learning is emerging\nas a parameter-efficient method for adapting VLMs, but state-of-the-art\napproaches require annotated samples. In this paper we propose a novel approach\nto prompt learning based on unsupervised knowledge distillation from more\npowerful models. Our approach, which we call Knowledge Distillation Prompt\nLearning (KDPL), can be integrated into existing prompt learning techniques and\neliminates the need for labeled examples during adaptation. Our experiments on\nmore than ten standard benchmark datasets demonstrate that KDPL is very\neffective at improving generalization of learned prompts for zero-shot domain\ngeneralization, zero-shot cross-dataset generalization, and zero-shot\nbase-to-novel class generalization problems. KDPL requires no ground-truth\nlabels for adaptation, and moreover we show that even in the absence of any\nknowledge of training class names it can be used to effectively transfer\nknowledge. The code is publicly available at https://github.com/miccunifi/KDPL.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}