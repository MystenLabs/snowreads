{"id":"2407.02586","title":"Improving Visual Storytelling with Multimodal Large Language Models","authors":"Xiaochuan Lin, Xiangyong Chen","authorsParsed":[["Lin","Xiaochuan",""],["Chen","Xiangyong",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 18:13:55 GMT"}],"updateDate":"2024-07-04","timestamp":1719944035000,"abstract":"  Visual storytelling is an emerging field that combines images and narratives\nto create engaging and contextually rich stories. Despite its potential,\ngenerating coherent and emotionally resonant visual stories remains challenging\ndue to the complexity of aligning visual and textual information. This paper\npresents a novel approach leveraging large language models (LLMs) and large\nvision-language models (LVLMs) combined with instruction tuning to address\nthese challenges. We introduce a new dataset comprising diverse visual stories,\nannotated with detailed captions and multimodal elements. Our method employs a\ncombination of supervised and reinforcement learning to fine-tune the model,\nenhancing its narrative generation capabilities. Quantitative evaluations using\nGPT-4 and qualitative human assessments demonstrate that our approach\nsignificantly outperforms existing models, achieving higher scores in narrative\ncoherence, relevance, emotional depth, and overall quality. The results\nunderscore the effectiveness of instruction tuning and the potential of\nLLMs/LVLMs in advancing visual storytelling.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}