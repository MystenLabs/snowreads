{"id":"2407.18743","title":"Towards Effective and Efficient Continual Pre-training of Large Language\n  Models","authors":"Jie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yutao Zhu, Jinhao\n  Jiang, Yingqian Min, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin,\n  Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang,\n  Ji-Rong Wen","authorsParsed":[["Chen","Jie",""],["Chen","Zhipeng",""],["Wang","Jiapeng",""],["Zhou","Kun",""],["Zhu","Yutao",""],["Jiang","Jinhao",""],["Min","Yingqian",""],["Zhao","Wayne Xin",""],["Dou","Zhicheng",""],["Mao","Jiaxin",""],["Lin","Yankai",""],["Song","Ruihua",""],["Xu","Jun",""],["Chen","Xu",""],["Yan","Rui",""],["Wei","Zhewei",""],["Hu","Di",""],["Huang","Wenbing",""],["Wen","Ji-Rong",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 13:55:21 GMT"}],"updateDate":"2024-07-29","timestamp":1722002121000,"abstract":"  Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}