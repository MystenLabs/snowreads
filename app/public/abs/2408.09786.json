{"id":"2408.09786","title":"Cross-composition Feature Disentanglement for Compositional Zero-shot\n  Learning","authors":"Yuxia Geng, Runkai Zhu, Jiaoyan Chen, Jintai Chen, Zhuo Chen, Xiang\n  Chen, Can Xu, Yuxiang Wang, Xiaoliang Xu","authorsParsed":[["Geng","Yuxia",""],["Zhu","Runkai",""],["Chen","Jiaoyan",""],["Chen","Jintai",""],["Chen","Zhuo",""],["Chen","Xiang",""],["Xu","Can",""],["Wang","Yuxiang",""],["Xu","Xiaoliang",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 08:23:09 GMT"}],"updateDate":"2024-08-20","timestamp":1724055789000,"abstract":"  Disentanglement of visual features of primitives (i.e., attributes and\nobjects) has shown exceptional results in Compositional Zero-shot Learning\n(CZSL). However, due to the feature divergence of an attribute (resp. object)\nwhen combined with different objects (resp. attributes), it is challenging to\nlearn disentangled primitive features that are general across different\ncompositions. To this end, we propose the solution of cross-composition feature\ndisentanglement, which takes multiple primitive-sharing compositions as inputs\nand constrains the disentangled primitive features to be general across these\ncompositions. More specifically, we leverage a compositional graph to define\nthe overall primitive-sharing relationships between compositions, and build a\ntask-specific architecture upon the recently successful large pre-trained\nvision-language model (VLM) CLIP, with dual cross-composition disentangling\nadapters (called L-Adapter and V-Adapter) inserted into CLIP's frozen text and\nimage encoders, respectively. Evaluation on three popular CZSL benchmarks shows\nthat our proposed solution significantly improves the performance of CZSL, and\nits components have been verified by solid ablation studies.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bfjZzWJgoKvOwK3Um0ZIMwTD75qYac2VZQmj-kqYOB8","pdfSize":"1631724"}
