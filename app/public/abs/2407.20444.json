{"id":"2407.20444","title":"Importance Corrected Neural JKO Sampling","authors":"Johannes Hertrich and Robert Gruhlke","authorsParsed":[["Hertrich","Johannes",""],["Gruhlke","Robert",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 22:49:59 GMT"}],"updateDate":"2024-07-31","timestamp":1722293399000,"abstract":"  In order to sample from an unnormalized probability density function, we\npropose to combine continuous normalizing flows (CNFs) with\nrejection-resampling steps based on importance weights. We relate the iterative\ntraining of CNFs with regularized velocity fields to a JKO scheme and prove\nconvergence of the involved velocity fields to the velocity field of the\nWasserstein gradient flow (WGF). The alternation of local flow steps and\nnon-local rejection-resampling steps allows to overcome local minima or slow\nconvergence of the WGF for multimodal distributions. Since the proposal of the\nrejection step is generated by the model itself, they do not suffer from common\ndrawbacks of classical rejection schemes. The arising model can be trained\niteratively, reduces the reverse Kulback-Leibler (KL) loss function in each\nstep, allows to generate iid samples and moreover allows for evaluations of the\ngenerated underlying density. Numerical examples show that our method yields\naccurate results on various test distributions including high-dimensional\nmultimodal targets and outperforms the state of the art in almost all cases\nsignificantly.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Probability"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}