{"id":"2407.06321","title":"Open Problem: Tight Bounds for Kernelized Multi-Armed Bandits with\n  Bernoulli Rewards","authors":"Marco Mussi and Simone Drago and Alberto Maria Metelli","authorsParsed":[["Mussi","Marco",""],["Drago","Simone",""],["Metelli","Alberto Maria",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 18:38:11 GMT"}],"updateDate":"2024-07-10","timestamp":1720463891000,"abstract":"  We consider Kernelized Bandits (KBs) to optimize a function $f : \\mathcal{X}\n\\rightarrow [0,1]$ belonging to the Reproducing Kernel Hilbert Space (RKHS)\n$\\mathcal{H}_k$. Mainstream works on kernelized bandits focus on a subgaussian\nnoise model in which observations of the form $f(\\mathbf{x}_t)+\\epsilon_t$,\nbeing $\\epsilon_t$ a subgaussian noise, are available (Chowdhury and Gopalan,\n2017). Differently, we focus on the case in which we observe realizations $y_t\n\\sim \\text{Ber}(f(\\mathbf{x}_t))$ sampled from a Bernoulli distribution with\nparameter $f(\\mathbf{x}_t)$. While the Bernoulli model has been investigated\nsuccessfully in multi-armed bandits (Garivier and Capp\\'e, 2011), logistic\nbandits (Faury et al., 2022), bandits in metric spaces (Magureanu et al.,\n2014), it remains an open question whether tight results can be obtained for\nKBs. This paper aims to draw the attention of the online learning community to\nthis open problem.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}