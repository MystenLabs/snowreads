{"id":"2408.12430","title":"Positional Description for Numerical Normalization","authors":"Deepanshu Gupta and Javier Latorre","authorsParsed":[["Gupta","Deepanshu",""],["Latorre","Javier",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 14:24:20 GMT"}],"updateDate":"2024-08-23","timestamp":1724336660000,"abstract":"  We present a Positional Description Scheme (PDS) tailored for digit\nsequences, integrating placeholder value information for each digit. Given the\nstructural limitations of subword tokenization algorithms, language models\nencounter critical Text Normalization (TN) challenges when handling numerical\ntasks. Our schema addresses this challenge through straightforward\npre-processing, preserving the model architecture while significantly\nsimplifying number normalization, rendering the problem tractable. This\nsimplifies the task and facilitates more compact production-ready models\ncapable of learning from smaller datasets. Furthermore, our investigations\nreveal that PDS enhances the arithmetic processing capabilities of language\nmodels, resulting in a relative accuracy improvement of 23% to 51% on complex\narithmetic tasks. We demonstrate that PDS effectively mitigates fatal numerical\nnormalization errors in neural models, requiring only a modest amount of\ntraining data without rule-based Finite State Transducers (FST). We demonstrate\nthat PDS is essential for both the Text-To-Speech and Speech Recognition text\nprocessing, enabling effective TN under production constraints.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wBjJkR6By8YIm2ic-j7Ljl7do0eeZDZ84PyW1RppZws","pdfSize":"542666"}
