{"id":"2408.08995","title":"On the Undecidability of Artificial Intelligence Alignment: Machines\n  that Halt","authors":"Gabriel Adriano de Melo, Marcos Ricardo Omena De Albuquerque Maximo,\n  Nei Yoshihiro Soma, Paulo Andre Lima de Castro","authorsParsed":[["de Melo","Gabriel Adriano",""],["Maximo","Marcos Ricardo Omena De Albuquerque",""],["Soma","Nei Yoshihiro",""],["de Castro","Paulo Andre Lima",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 19:55:26 GMT"}],"updateDate":"2024-08-20","timestamp":1723838126000,"abstract":"  The inner alignment problem, which asserts whether an arbitrary artificial\nintelligence (AI) model satisfices a non-trivial alignment function of its\noutputs given its inputs, is undecidable. This is rigorously proved by Rice's\ntheorem, which is also equivalent to a reduction to Turing's Halting Problem,\nwhose proof sketch is presented in this work. Nevertheless, there is an\nenumerable set of provenly aligned AIs that are constructed from a finite set\nof provenly aligned operations. Therefore, we argue that the alignment should\nbe a guaranteed property from the AI architecture rather than a characteristic\nimposed post-hoc on an arbitrary AI model. Furthermore, while the outer\nalignment problem is the definition of a judge function that captures human\nvalues and preferences, we propose that such a function must also impose a\nhalting constraint that guarantees that the AI model always reaches a terminal\nstate in finite execution steps. Our work presents examples and models that\nillustrate this constraint and the intricate challenges involved, advancing a\ncompelling case for adopting an intrinsically hard-aligned approach to AI\nsystems architectures that ensures halting.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}