{"id":"2408.15381","title":"On Stateful Value Factorization in Multi-Agent Reinforcement Learning","authors":"Enrico Marchesini, Andrea Baisero, Rupali Bhati, Christopher Amato","authorsParsed":[["Marchesini","Enrico",""],["Baisero","Andrea",""],["Bhati","Rupali",""],["Amato","Christopher",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 19:45:26 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 22:49:17 GMT"}],"updateDate":"2024-09-11","timestamp":1724787926000,"abstract":"  Value factorization is a popular paradigm for designing scalable multi-agent\nreinforcement learning algorithms. However, current factorization methods make\nchoices without full justification that may limit their performance. For\nexample, the theory in prior work uses stateless (i.e., history) functions,\nwhile the practical implementations use state information -- making the\nmotivating theory a mismatch for the implementation. Also, methods have built\noff of previous approaches, inheriting their architectures without exploring\nother, potentially better ones. To address these concerns, we formally analyze\nthe theory of using the state instead of the history in current methods --\nreconnecting theory and practice. We then introduce DuelMIX, a factorization\nalgorithm that learns distinct per-agent utility estimators to improve\nperformance and achieve full expressiveness. Experiments on StarCraft II\nmicromanagement and Box Pushing tasks demonstrate the benefits of our\nintuitions.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6RmoX3t37vhSonEq0OOPxpNb7iK_Fb7Id-VPyQ9EUS4","pdfSize":"6288539"}
