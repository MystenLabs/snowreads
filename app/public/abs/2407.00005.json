{"id":"2407.00005","title":"Dual-pronged deep learning preprocessing on heterogeneous platforms with\n  CPU, GPU and CSD","authors":"Jia Wei and Xingjun Zhang and Witold Pedrycz and Longxiang Wang and\n  Jie Zhao","authorsParsed":[["Wei","Jia",""],["Zhang","Xingjun",""],["Pedrycz","Witold",""],["Wang","Longxiang",""],["Zhao","Jie",""]],"versions":[{"version":"v1","created":"Wed, 17 Apr 2024 23:48:46 GMT"}],"updateDate":"2024-07-02","timestamp":1713397726000,"abstract":"  Most existing data preprocessing is done at the CPU. Although some studies\nuse techniques such as multi-processing and double buffering to accelerate CPU\npreprocessing, CPU computational speed and storage bandwidth still limit the\nprocessing speed. Other studies try to use intelligent data storage devices,\nsuch as computational storage devices, to complete data preprocessing instead\nof CPUs. The current studies use only one device to complete data preprocessing\noperations, which cannot fully overlap data preprocessing and accelerator\ncomputation time. To fully exploit the independence and high bandwidth of the\nnovel CSD, this paper proposes an advanced, highly parallel dual-pronged data\npreprocessing algorithm (DDLP) that significantly improves the execution\nefficiency and computational overlap between heterogeneous devices. DDLP\nenables the CPU and CSD to start data preprocessing operations from both ends\nof the dataset separately. Meanwhile, we propose two adaptive dynamic selection\nstrategies to make DDLP control the GPU to automatically read data from\ndifferent sources. We achieve sufficient computational overlap between CSD data\npreprocessing and CPU preprocessing, GPU computation, and GPU data reading. In\naddition, DDLP leverages GPU Direct Storage technology to enable efficient\nSSD-to-GPU data transfer. DDLP reduces the usage of expensive CPU and DRAM\nresources, reduces the number of SSD-to-GPU data transfers, and improves the\nenergy efficiency of preprocessing while reducing the overall preprocessing and\ntraining time. Extensive experimental results show that DDLP can improve\nlearning speed by up to 23.5% on ImageNet Dataset while reducing energy\nconsumption by 19.7% and CPU and DRAM usage by 37.6%. DDLP also improve\nlearning speed by up to 27.6% on Cifar-10 Dataset.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}