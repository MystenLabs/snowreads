{"id":"2407.14467","title":"Check-Eval: A Checklist-based Approach for Evaluating Text Quality","authors":"Jayr Pereira and Andre Assumpcao and Roberto Lotufo","authorsParsed":[["Pereira","Jayr",""],["Assumpcao","Andre",""],["Lotufo","Roberto",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:14:16 GMT"},{"version":"v2","created":"Tue, 10 Sep 2024 14:08:29 GMT"}],"updateDate":"2024-09-11","timestamp":1721409256000,"abstract":"  Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"C3NLH2e_SQX38aine5Lcw-V67Tm_7SHs0-fACVkBziE","pdfSize":"561023"}
