{"id":"2407.16716","title":"Exploring The Neural Burden In Pruned Models: An Insight Inspired By\n  Neuroscience","authors":"Zeyu Wang, Weichen Dai, Xiangyu Zhou, Ji Qi, Yi Zhou","authorsParsed":[["Wang","Zeyu",""],["Dai","Weichen",""],["Zhou","Xiangyu",""],["Qi","Ji",""],["Zhou","Yi",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 03:43:21 GMT"},{"version":"v2","created":"Sat, 27 Jul 2024 10:57:30 GMT"}],"updateDate":"2024-07-30","timestamp":1721706201000,"abstract":"  Vision Transformer and its variants have been adopted in many visual tasks\ndue to their powerful capabilities, which also bring significant challenges in\ncomputation and storage. Consequently, researchers have introduced various\ncompression methods in recent years, among which the pruning techniques are\nwidely used to remove a significant fraction of the network. Therefore, these\nmethods can reduce significant percent of the FLOPs, but often lead to a\ndecrease in model performance. To investigate the underlying causes, we focus\non the pruning methods specifically belonging to the pruning-during-training\ncategory, then drew inspiration from neuroscience and propose a new concept for\nartificial neural network models named Neural Burden. We investigate its impact\nin the model pruning process, and subsequently explore a simple yet effective\napproach to mitigate the decline in model performance, which can be applied to\nany pruning-during-training technique. Extensive experiments indicate that the\nneural burden phenomenon indeed exists, and show the potential of our method.\nWe hope that our findings can provide valuable insights for future research.\nCode will be made publicly available after this paper is published.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}