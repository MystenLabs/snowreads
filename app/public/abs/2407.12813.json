{"id":"2407.12813","title":"Data Generation Using Large Language Models for Text Classification: An\n  Empirical Case Study","authors":"Yinheng Li, Rogerio Bonatti, Sara Abdali, Justin Wagle, Kazuhito\n  Koishida","authorsParsed":[["Li","Yinheng",""],["Bonatti","Rogerio",""],["Abdali","Sara",""],["Wagle","Justin",""],["Koishida","Kazuhito",""]],"versions":[{"version":"v1","created":"Thu, 27 Jun 2024 21:41:43 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 20:37:17 GMT"}],"updateDate":"2024-07-23","timestamp":1719524503000,"abstract":"  Using Large Language Models (LLMs) to generate synthetic data for model\ntraining has become increasingly popular in recent years. While LLMs are\ncapable of producing realistic training data, the effectiveness of data\ngeneration is influenced by various factors, including the choice of prompt,\ntask complexity, and the quality, quantity, and diversity of the generated\ndata. In this work, we focus exclusively on using synthetic data for text\nclassification tasks. Specifically, we use natural language understanding (NLU)\nmodels trained on synthetic data to assess the quality of synthetic data from\ndifferent generation approaches. This work provides an empirical analysis of\nthe impact of these factors and offers recommendations for better data\ngeneration practices.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d37Eoilgg25OtUDy8nL153rh0aBlWvMNkcR27AYxNss","pdfSize":"507535"}
