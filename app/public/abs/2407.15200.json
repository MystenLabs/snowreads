{"id":"2407.15200","title":"HyperbolicLR: Epoch insensitive learning rate scheduler","authors":"Tae-Geun Kim","authorsParsed":[["Kim","Tae-Geun",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 15:43:52 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 01:26:46 GMT"}],"updateDate":"2024-07-31","timestamp":1721576632000,"abstract":"  This study proposes two novel learning rate schedulers: the Hyperbolic\nLearning Rate Scheduler (HyperbolicLR) and the Exponential Hyperbolic Learning\nRate Scheduler (ExpHyperbolicLR). These schedulers attempt to address the\ninconsistent learning curves often observed in conventional schedulers when\nadjusting the number of epochs. By leveraging the asymptotic behavior of\nhyperbolic curves, the proposed schedulers maintain more consistent learning\ncurves across varying epoch settings. The HyperbolicLR algorithm directly\napplies this property to the epoch-learning rate space, while the\nExpHyperbolicLR maps this concept onto the exponential space of epochs and\nlearning rates. To evaluate the performance of these schedulers, first we found\nthe optimal hyperparameters for each scheduler on a small number of epochs,\nfixed these values, and compared their performance as the number of epochs\nincreased. Our experimental results on various deep learning tasks and\narchitectures demonstrate that both HyperbolicLR and ExpHyperbolicLR maintain\nmore consistent performance improvements compared to conventional schedulers as\nthe number of epochs increases. These findings suggest that our\nhyperbolic-based learning rate schedulers offer a more robust and efficient\napproach to training deep neural networks, especially in scenarios where\ncomputational resources or time constraints limit extensive hyperparameter\nsearches.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CiWtUJJHEKpQtYOSteqLWmvZQmj-UYrINf3myE6M_tc","pdfSize":"6504831"}
