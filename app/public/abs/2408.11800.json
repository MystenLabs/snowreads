{"id":"2408.11800","title":"PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting\n  and Permitting domain","authors":"Rounak Meyur, Hung Phan, Sridevi Wagle, Jan Strube, Mahantesh\n  Halappanavar, Sameera Horawalavithana, Anurag Acharya, Sai Munikoti","authorsParsed":[["Meyur","Rounak",""],["Phan","Hung",""],["Wagle","Sridevi",""],["Strube","Jan",""],["Halappanavar","Mahantesh",""],["Horawalavithana","Sameera",""],["Acharya","Anurag",""],["Munikoti","Sai",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:43:11 GMT"}],"updateDate":"2024-08-22","timestamp":1724262191000,"abstract":"  In the rapidly evolving landscape of Natural Language Processing (NLP) and\ntext generation, the emergence of Retrieval Augmented Generation (RAG) presents\na promising avenue for improving the quality and reliability of generated text\nby leveraging information retrieved from user specified database. Benchmarking\nis essential to evaluate and compare the performance of the different RAG\nconfigurations in terms of retriever and generator, providing insights into\ntheir effectiveness, scalability, and suitability for the specific domain and\napplications. In this paper, we present a comprehensive framework to generate a\ndomain relevant RAG benchmark. Our framework is based on automatic\nquestion-answer generation with Human (domain experts)-AI Large Language Model\n(LLM) teaming. As a case study, we demonstrate the framework by introducing\nPermitQA, a first-of-its-kind benchmark on the wind siting and permitting\ndomain which comprises of multiple scientific documents/reports related to\nenvironmental impact of wind energy projects. Our framework systematically\nevaluates RAG performance using diverse metrics and multiple question types\nwith varying complexity level. We also demonstrate the performance of different\nmodels on our benchmark.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}