{"id":"2407.21264","title":"Model Attribution in LLM-Generated Disinformation: A Domain\n  Generalization Approach with Supervised Contrastive Learning","authors":"Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu and\n  Huan Liu","authorsParsed":[["Beigi","Alimohammad",""],["Tan","Zhen",""],["Mudiam","Nivedh",""],["Chen","Canyu",""],["Shu","Kai",""],["Liu","Huan",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 00:56:09 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 08:10:43 GMT"}],"updateDate":"2024-08-15","timestamp":1722387369000,"abstract":"  Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}