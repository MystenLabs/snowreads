{"id":"2408.04381","title":"Understanding and Modeling Job Marketplace with Pretrained Language\n  Models","authors":"Yaochen Zhu, Liang Wu, Binchi Zhang, Song Wang, Qi Guo, Liangjie Hong,\n  Luke Simon, Jundong Li","authorsParsed":[["Zhu","Yaochen",""],["Wu","Liang",""],["Zhang","Binchi",""],["Wang","Song",""],["Guo","Qi",""],["Hong","Liangjie",""],["Simon","Luke",""],["Li","Jundong",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 11:35:52 GMT"}],"updateDate":"2024-08-09","timestamp":1723116952000,"abstract":"  Job marketplace is a heterogeneous graph composed of interactions among\nmembers (job-seekers), companies, and jobs. Understanding and modeling job\nmarketplace can benefit both job seekers and employers, ultimately contributing\nto the greater good of the society. However, existing graph neural network\n(GNN)-based methods have shallow understandings of the associated textual\nfeatures and heterogeneous relations. To address the above challenges, we\npropose PLM4Job, a job marketplace foundation model that tightly couples\npretrained language models (PLM) with job market graph, aiming to fully utilize\nthe pretrained knowledge and reasoning ability to model member/job textual\nfeatures as well as various member-job relations simultaneously. In the\npretraining phase, we propose a heterogeneous ego-graph-based prompting\nstrategy to model and aggregate member/job textual features based on the\ntopological structure around the target member/job node, where entity type\nembeddings and graph positional embeddings are introduced accordingly to model\ndifferent entities and their heterogeneous relations. Meanwhile, a\nproximity-aware attention alignment strategy is designed to dynamically adjust\nthe attention of the PLM on ego-graph node tokens in the prompt, such that the\nattention can be better aligned with job marketplace semantics. Extensive\nexperiments at LinkedIn demonstrate the effectiveness of PLM4Job.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}