{"id":"2408.17274","title":"The Transferability of Downsamped Sparse Graph Convolutional Networks","authors":"Qinji Shu, Hang Sheng, Feng Ji, Hui Feng, Bo Hu","authorsParsed":[["Shu","Qinji",""],["Sheng","Hang",""],["Ji","Feng",""],["Feng","Hui",""],["Hu","Bo",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 13:19:20 GMT"},{"version":"v2","created":"Sun, 8 Sep 2024 09:31:16 GMT"}],"updateDate":"2024-09-10","timestamp":1725023960000,"abstract":"  To accelerate the training of graph convolutional networks (GCNs) on\nreal-world large-scale sparse graphs, downsampling methods are commonly\nemployed as a preprocessing step. However, the effects of graph sparsity and\ntopological structure on the transferability of downsampling methods have not\nbeen rigorously analyzed or theoretically guaranteed, particularly when the\ntopological structure is affected by graph sparsity. In this paper, we\nintroduce a novel downsampling method based on a sparse random graph model and\nderive an expected upper bound for the transfer error. Our findings show that\nsmaller original graph sizes, higher expected average degrees, and increased\nsampling rates contribute to reducing this upper bound. Experimental results\nvalidate the theoretical predictions. By incorporating both sparsity and\ntopological similarity into the model, this study establishes an upper bound on\nthe transfer error for downsampling in the training of large-scale sparse\ngraphs and provides insight into the influence of topological structure on\ntransfer performance.\n","subjects":["Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}