{"id":"2408.11936","title":"Estimating Contribution Quality in Online Deliberations Using a Large\n  Language Model","authors":"Lodewijk Gelauff, Mohak Goyal, Bhargav Dindukurthi, Ashish Goel, and\n  Alice Siu","authorsParsed":[["Gelauff","Lodewijk",""],["Goyal","Mohak",""],["Dindukurthi","Bhargav",""],["Goel","Ashish",""],["Siu","Alice",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 18:41:32 GMT"}],"updateDate":"2024-08-23","timestamp":1724265692000,"abstract":"  Deliberation involves participants exchanging knowledge, arguments, and\nperspectives and has been shown to be effective at addressing polarization. The\nStanford Online Deliberation Platform facilitates large-scale deliberations. It\nenables video-based online discussions on a structured agenda for small groups\nwithout requiring human moderators. This paper's data comes from various\ndeliberation events, including one conducted in collaboration with Meta in 32\ncountries, and another with 38 post-secondary institutions in the US.\n  Estimating the quality of contributions in a conversation is crucial for\nassessing feature and intervention impacts. Traditionally, this is done by\nhuman annotators, which is time-consuming and costly. We use a large language\nmodel (LLM) alongside eight human annotators to rate contributions based on\njustification, novelty, expansion of the conversation, and potential for\nfurther expansion, with scores ranging from 1 to 5. Annotators also provide\nbrief justifications for their ratings. Using the average rating from other\nhuman annotators as the ground truth, we find the model outperforms individual\nhuman annotators. While pairs of human annotators outperform the model in\nrating justification and groups of three outperform it on all four metrics, the\nmodel remains competitive.\n  We illustrate the usefulness of the automated quality rating by assessing the\neffect of nudges on the quality of deliberation. We first observe that\nindividual nudges after prolonged inactivity are highly effective, increasing\nthe likelihood of the individual requesting to speak in the next 30 seconds by\n65%. Using our automated quality estimation, we show that the quality ratings\nfor statements prompted by nudging are similar to those made without nudging,\nsignifying that nudging leads to more ideas being generated in the conversation\nwithout losing overall quality.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}