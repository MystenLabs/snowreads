{"id":"2408.08147","title":"P/D-Serve: Serving Disaggregated Large Language Model at Scale","authors":"Yibo Jin, Tao Wang, Huimin Lin, Mingyang Song, Peiyang Li, Yipeng Ma,\n  Yicheng Shan, Zhengfan Yuan, Cailong Li, Yajing Sun, Tiandeng Wu, Xing Chu,\n  Ruizhi Huan, Li Ma, Xiao You, Wenting Zhou, Yunpeng Ye, Wen Liu, Xiangkun Xu,\n  Yongsheng Zhang, Tiantian Dong, Jiawei Zhu, Zhe Wang, Xijian Ju, Jianxun\n  Song, Haoliang Cheng, Xiaojing Li, Jiandong Ding, Hefei Guo, Zhengyong Zhang","authorsParsed":[["Jin","Yibo",""],["Wang","Tao",""],["Lin","Huimin",""],["Song","Mingyang",""],["Li","Peiyang",""],["Ma","Yipeng",""],["Shan","Yicheng",""],["Yuan","Zhengfan",""],["Li","Cailong",""],["Sun","Yajing",""],["Wu","Tiandeng",""],["Chu","Xing",""],["Huan","Ruizhi",""],["Ma","Li",""],["You","Xiao",""],["Zhou","Wenting",""],["Ye","Yunpeng",""],["Liu","Wen",""],["Xu","Xiangkun",""],["Zhang","Yongsheng",""],["Dong","Tiantian",""],["Zhu","Jiawei",""],["Wang","Zhe",""],["Ju","Xijian",""],["Song","Jianxun",""],["Cheng","Haoliang",""],["Li","Xiaojing",""],["Ding","Jiandong",""],["Guo","Hefei",""],["Zhang","Zhengyong",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 13:32:25 GMT"}],"updateDate":"2024-08-16","timestamp":1723728745000,"abstract":"  Serving disaggregated large language models (LLMs) over tens of thousands of\nxPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.\n1) Ignoring the diversity (various prefixes and tidal requests), treating all\nthe prompts in a mixed pool is inadequate. To facilitate the similarity per\nscenario and minimize the inner mismatch on P/D (prefill and decoding)\nprocessing, fine-grained organization is required, dynamically adjusting P/D\nratios for better performance. 2) Due to inaccurate estimation on workload\n(queue status or maintained connections), the global scheduler easily incurs\nunnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache\ntransfer over cluster-level RDMA (remote direct memory access) fails to achieve\ndesired D2D utilization as expected. To overcome previous problems, this paper\nproposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps\n(machine learning operations), which models end-to-end (E2E) P/D performance\nand enables: 1) fine-grained P/D organization, mapping the service with RoCE\n(RDMA over converged ethernet) as needed, to facilitate similar processing and\ndynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for\nidle prefill, decoupling the scheduler from regular inaccurate reports and\nlocal queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer\nvia optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,\nhas been deployed over tens of thousands of NPUs for more than eight months in\ncommercial use, and further achieves 60\\%, 42\\% and 46\\% improvements on E2E\nthroughput, time-to-first-token (TTFT) SLO (service level objective) and D2D\ntransfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x\nincrease on throughput, compared with aggregated LLMs.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}