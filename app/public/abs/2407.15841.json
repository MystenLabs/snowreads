{"id":"2407.15841","title":"SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language\n  Models","authors":"Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming\n  Gang, Kai Kang, Afshin Dehghan","authorsParsed":[["Xu","Mingze",""],["Gao","Mingfei",""],["Gan","Zhe",""],["Chen","Hong-You",""],["Lai","Zhengfeng",""],["Gang","Haiming",""],["Kang","Kai",""],["Dehghan","Afshin",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 17:58:04 GMT"},{"version":"v2","created":"Sun, 15 Sep 2024 05:00:18 GMT"}],"updateDate":"2024-09-17","timestamp":1721671084000,"abstract":"  We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video\nlarge language model (LLM) that can jointly capture detailed spatial semantics\nand long-range temporal context without exceeding the token budget of commonly\nused LLMs. This is realized by using a two-stream SlowFast design of inputs for\nVideo LLMs to aggregate features from sampled frames in an effective way.\nSpecifically, the Slow pathway extracts features at a low frame rate while\nkeeping as much spatial detail as possible (e.g., with 12x24 tokens), and the\nFast pathway operates on a high frame rate but uses a larger spatial pooling\nstride (e.g., downsampling 6x) to focus on the motion cues. As a result, this\ndesign allows us to adequately capture both spatial and temporal features that\nare beneficial for detailed video understanding. Experimental results show that\nSF-LLaVA outperforms existing training-free methods on a wide range of video\ntasks. On some benchmarks, it achieves comparable or even better performance\ncompared to state-of-the-art Video LLMs that are fine-tuned on video datasets.\nCode has been made available at: https://github.com/apple/ml-slowfast-llava.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}