{"id":"2408.03541","title":"EXAONE 3.0 7.8B Instruction Tuned Language Model","authors":"LG AI Research: Soyoung An, Kyunghoon Bae, Eunbi Choi, Stanley Jungkyu\n  Choi, Yemuk Choi, Seokhee Hong, Yeonjung Hong, Junwon Hwang, Hyojin Jeon,\n  Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Euisoon Kim,\n  Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun\n  Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee,\n  Kyungmin Lee, Moontae Lee, Seungjun Lee, Woohyung Lim, Sangha Park, Sooyoun\n  Park, Yongmin Park, Boseong Seo, Sihoon Yang, Heuiyeen Yeen, Kyungjae Yoo,\n  Hyeongu Yun","authorsParsed":[["Research","LG AI",""],[":","",""],["An","Soyoung",""],["Bae","Kyunghoon",""],["Choi","Eunbi",""],["Choi","Stanley Jungkyu",""],["Choi","Yemuk",""],["Hong","Seokhee",""],["Hong","Yeonjung",""],["Hwang","Junwon",""],["Jeon","Hyojin",""],["Jo","Gerrard Jeongwon",""],["Jo","Hyunjik",""],["Jung","Jiyeon",""],["Jung","Yountae",""],["Kim","Euisoon",""],["Kim","Hyosang",""],["Kim","Joonkee",""],["Kim","Seonghwan",""],["Kim","Soyeon",""],["Kim","Sunkyoung",""],["Kim","Yireun",""],["Kim","Youchul",""],["Lee","Edward Hwayoung",""],["Lee","Haeju",""],["Lee","Honglak",""],["Lee","Jinsik",""],["Lee","Kyungmin",""],["Lee","Moontae",""],["Lee","Seungjun",""],["Lim","Woohyung",""],["Park","Sangha",""],["Park","Sooyoun",""],["Park","Yongmin",""],["Seo","Boseong",""],["Yang","Sihoon",""],["Yeen","Heuiyeen",""],["Yoo","Kyungjae",""],["Yun","Hyeongu",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 04:38:38 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 04:35:23 GMT"},{"version":"v3","created":"Tue, 13 Aug 2024 10:09:32 GMT"}],"updateDate":"2024-08-14","timestamp":1723005518000,"abstract":"  We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}