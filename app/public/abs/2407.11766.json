{"id":"2407.11766","title":"Vectoring Languages","authors":"Joseph Chen","authorsParsed":[["Chen","Joseph",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:25:55 GMT"}],"updateDate":"2024-07-17","timestamp":1721139955000,"abstract":"  Recent breakthroughs in large language models (LLM) have stirred up global\nattention, and the research has been accelerating non-stop since then.\nPhilosophers and psychologists have also been researching the structure of\nlanguage for decades, but they are having a hard time finding a theory that\ndirectly benefits from the breakthroughs of LLMs. In this article, we propose a\nnovel structure of language that reflects well on the mechanisms behind\nlanguage models and go on to show that this structure is also better at\ncapturing the diverse nature of language compared to previous methods. An\nanalogy of linear algebra is adapted to strengthen the basis of this\nperspective. We further argue about the difference between this perspective and\nthe design philosophy for current language models. Lastly, we discuss how this\nperspective can lead us to research directions that may accelerate the\nimprovements of science fastest.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}