{"id":"2407.05658","title":"Random Features Hopfield Networks generalize retrieval to previously\n  unseen examples","authors":"Silvio Kalaj, Clarissa Lauditi, Gabriele Perugini, Carlo Lucibello,\n  Enrico M. Malatesta, Matteo Negri","authorsParsed":[["Kalaj","Silvio",""],["Lauditi","Clarissa",""],["Perugini","Gabriele",""],["Lucibello","Carlo",""],["Malatesta","Enrico M.",""],["Negri","Matteo",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 06:35:13 GMT"}],"updateDate":"2024-07-09","timestamp":1720420513000,"abstract":"  It has been recently shown that a learning transition happens when a Hopfield\nNetwork stores examples generated as superpositions of random features, where\nnew attractors corresponding to such features appear in the model. In this work\nwe reveal that the network also develops attractors corresponding to previously\nunseen examples generated with the same set of features. We explain this\nsurprising behaviour in terms of spurious states of the learned features: we\nargue that, increasing the number of stored examples beyond the learning\ntransition, the model also learns to mix the features to represent both stored\nand previously unseen examples. We support this claim with the computation of\nthe phase diagram of the model.\n","subjects":["Condensed Matter/Disordered Systems and Neural Networks","Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}