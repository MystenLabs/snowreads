{"id":"2408.04780","title":"A Single-Loop Finite-Time Convergent Policy Optimization Algorithm for\n  Mean Field Games (and Average-Reward Markov Decision Processes)","authors":"Sihan Zeng, Sujay Bhatt, Alec Koppel, Sumitra Ganesh","authorsParsed":[["Zeng","Sihan",""],["Bhatt","Sujay",""],["Koppel","Alec",""],["Ganesh","Sumitra",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 22:36:23 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 17:37:11 GMT"},{"version":"v3","created":"Fri, 16 Aug 2024 00:23:41 GMT"}],"updateDate":"2024-08-19","timestamp":1723156583000,"abstract":"  We study the problem of finding an equilibrium of a mean field game (MFG) --\na policy performing optimally in a Markov decision process (MDP) determined by\nthe induced mean field, where the mean field is a distribution over a\npopulation of agents and a function of the policy itself. Prior solutions to\nMFGs are built upon either the contraction assumption on a mean field\noptimality-consistency operator or strict weak monotonicity. The class of\nproblems satisfying these assumptions represent only a small subset of MFGs, to\nwhich any MFG admitting more than one equilibrium does not belong. In this\nwork, we expand the class of solvable MFGs by introducing a \"herding condition\"\nand propose a direct gradient-based policy optimization algorithm that provably\nfinds an (not necessarily unique) equilibrium within the class. The algorithm,\nnamed Accelerated Single-loop Actor Critic Algorithm for Mean Field Games\n(ASAC-MFG), is data-driven, single-loop, and single-sample-path. We\ncharacterize the finite-time and finite-sample convergence of ASAC-MFG to a\nmean field equilibrium building on a novel multi-time-scale analysis. We\nsupport the theoretical results with illustrative numerical simulations.\n  As an additional contribution, we show how the developed novel analysis can\nbenefit the literature on average-reward MDPs. An MFG reduces to a standard MDP\nwhen the transition kernel and reward are independent of the mean field. As a\nbyproduct of our analysis for MFGs, we get an actor-critic algorithm for\nfinding the optimal policy in average-reward MDPs, with a convergence guarantee\nmatching the state-of-the-art. The prior bound is derived under the assumption\nthat the Bellman operator is contractive, which never holds in average-reward\nMDPs. Our analysis removes this assumption.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}