{"id":"2407.13887","title":"Learning Goal-Conditioned Representations for Language Reward Models","authors":"Vaskar Nath, Dylan Slack, Jeff Da, Yuntao Ma, Hugh Zhang, Spencer\n  Whitehead, Sean Hendryx","authorsParsed":[["Nath","Vaskar",""],["Slack","Dylan",""],["Da","Jeff",""],["Ma","Yuntao",""],["Zhang","Hugh",""],["Whitehead","Spencer",""],["Hendryx","Sean",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 20:23:11 GMT"}],"updateDate":"2024-07-22","timestamp":1721334191000,"abstract":"  Techniques that learn improved representations via offline data or\nself-supervised objectives have shown impressive results in traditional\nreinforcement learning (RL). Nevertheless, it is unclear how improved\nrepresentation learning can benefit reinforcement learning from human feedback\n(RLHF) on language models (LMs). In this work, we propose training reward\nmodels (RMs) in a contrastive, $\\textit{goal-conditioned}$ fashion by\nincreasing the representation similarity of future states along sampled\npreferred trajectories and decreasing the similarity along randomly sampled\ndispreferred trajectories. This objective significantly improves RM performance\nby up to 0.09 AUROC across challenging benchmarks, such as MATH and GSM8k.\nThese findings extend to general alignment as well -- on the Helpful-Harmless\ndataset, we observe $2.3\\%$ increase in accuracy. Beyond improving reward model\nperformance, we show this way of training RM representations enables improved\n$\\textit{steerability}$ because it allows us to evaluate the likelihood of an\naction achieving a particular goal-state (e.g., whether a solution is correct\nor helpful). Leveraging this insight, we find that we can filter up to $55\\%$\nof generated tokens during majority voting by discarding trajectories likely to\nend up in an \"incorrect\" state, which leads to significant cost savings. We\nadditionally find that these representations can perform fine-grained control\nby conditioning on desired future goal-states. For example, we show that\nsteering a Llama 3 model towards helpful generations with our approach improves\nhelpfulness by $9.6\\%$ over a supervised-fine-tuning trained baseline.\nSimilarly, steering the model towards complex generations improves complexity\nby $21.6\\%$ over the baseline. Overall, we find that training RMs in this\ncontrastive, goal-conditioned fashion significantly improves performance and\nenables model steerability.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_71FEW3KJgpFRhIl7DKnEt_dsgnyiFZMW6BOU5TAcz4","pdfSize":"1044166"}
