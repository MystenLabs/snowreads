{"id":"2408.15676","title":"VoxInstruct: Expressive Human Instruction-to-Speech Generation with\n  Unified Multilingual Codec Language Modelling","authors":"Yixuan Zhou, Xiaoyu Qin, Zeyu Jin, Shuoyi Zhou, Shun Lei, Songtao\n  Zhou, Zhiyong Wu, Jia Jia","authorsParsed":[["Zhou","Yixuan",""],["Qin","Xiaoyu",""],["Jin","Zeyu",""],["Zhou","Shuoyi",""],["Lei","Shun",""],["Zhou","Songtao",""],["Wu","Zhiyong",""],["Jia","Jia",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 09:57:17 GMT"}],"updateDate":"2024-08-29","timestamp":1724839037000,"abstract":"  Recent AIGC systems possess the capability to generate digital multimedia\ncontent based on human language instructions, such as text, image and video.\nHowever, when it comes to speech, existing methods related to human\ninstruction-to-speech generation exhibit two limitations. Firstly, they require\nthe division of inputs into content prompt (transcript) and description prompt\n(style and speaker), instead of directly supporting human instruction. This\ndivision is less natural in form and does not align with other AIGC models.\nSecondly, the practice of utilizing an independent description prompt to model\nspeech style, without considering the transcript content, restricts the ability\nto control speech at a fine-grained level. To address these limitations, we\npropose VoxInstruct, a novel unified multilingual codec language modeling\nframework that extends traditional text-to-speech tasks into a general human\ninstruction-to-speech task. Our approach enhances the expressiveness of human\ninstruction-guided speech generation and aligns the speech generation paradigm\nwith other modalities. To enable the model to automatically extract the content\nof synthesized speech from raw text instructions, we introduce speech semantic\ntokens as an intermediate representation for instruction-to-content guidance.\nWe also incorporate multiple Classifier-Free Guidance (CFG) strategies into our\ncodec language model, which strengthens the generated speech following human\ninstructions. Furthermore, our model architecture and training strategies allow\nfor the simultaneous support of combining speech prompt and descriptive human\ninstruction for expressive speech synthesis, which is a first-of-its-kind\nattempt. Codes, models and demos are at:\nhttps://github.com/thuhcsi/VoxInstruct.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"QJq_2SUMdqHJwBLMsOtbTPpQGNRoBDwRYAfWS6EHxV8","pdfSize":"4086340"}
