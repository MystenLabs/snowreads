{"id":"2407.10495","title":"Improving Hyperbolic Representations via Gromov-Wasserstein\n  Regularization","authors":"Yifei Yang, Wonjun Lee, Dongmian Zou, Gilad Lerman","authorsParsed":[["Yang","Yifei",""],["Lee","Wonjun",""],["Zou","Dongmian",""],["Lerman","Gilad",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 07:37:31 GMT"}],"updateDate":"2024-07-16","timestamp":1721029051000,"abstract":"  Hyperbolic representations have shown remarkable efficacy in modeling\ninherent hierarchies and complexities within data structures. Hyperbolic neural\nnetworks have been commonly applied for learning such representations from\ndata, but they often fall short in preserving the geometric structures of the\noriginal feature spaces. In response to this challenge, our work applies the\nGromov-Wasserstein (GW) distance as a novel regularization mechanism within\nhyperbolic neural networks. The GW distance quantifies how well the original\ndata structure is maintained after embedding the data in a hyperbolic space.\nSpecifically, we explicitly treat the layers of the hyperbolic neural networks\nas a transport map and calculate the GW distance accordingly. We validate that\nthe GW distance computed based on a training set well approximates the GW\ndistance of the underlying data distribution. Our approach demonstrates\nconsistent enhancements over current state-of-the-art methods across various\ntasks, including few-shot image classification, as well as semi-supervised\ngraph link prediction and node classification.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}