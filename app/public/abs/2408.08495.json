{"id":"2408.08495","title":"Achieving Complex Image Edits via Function Aggregation with Diffusion\n  Models","authors":"Mohammadreza Samadi, Fred X. Han, Mohammad Salameh, Hao Wu, Fengyu\n  Sun, Chunhua Zhou, Di Niu","authorsParsed":[["Samadi","Mohammadreza",""],["Han","Fred X.",""],["Salameh","Mohammad",""],["Wu","Hao",""],["Sun","Fengyu",""],["Zhou","Chunhua",""],["Niu","Di",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 02:33:55 GMT"}],"updateDate":"2024-08-19","timestamp":1723775635000,"abstract":"  Diffusion models have demonstrated strong performance in generative tasks,\nmaking them ideal candidates for image editing. Recent studies highlight their\nability to apply desired edits effectively by following textual instructions,\nyet two key challenges persist. First, these models struggle to apply multiple\nedits simultaneously, resulting in computational inefficiencies due to their\nreliance on sequential processing. Second, relying on textual prompts to\ndetermine the editing region can lead to unintended alterations in other parts\nof the image. In this work, we introduce FunEditor, an efficient diffusion\nmodel designed to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. FunEditor is 5 to 24 times faster inference\nthan existing methods on complex tasks like object movement. Our experiments\ndemonstrate that FunEditor significantly outperforms recent baselines,\nincluding both inference-time optimization methods and fine-tuned models,\nacross various metrics, such as image quality assessment (IQA) and\nobject-background consistency.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}