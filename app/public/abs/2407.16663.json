{"id":"2407.16663","title":"Computable learning of natural hypothesis classes","authors":"Matthew Harrison-Trainor, Syed Akbari","authorsParsed":[["Harrison-Trainor","Matthew",""],["Akbari","Syed",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 17:26:38 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 06:44:05 GMT"}],"updateDate":"2024-07-31","timestamp":1721755598000,"abstract":"  This paper is about the recent notion of computably probably approximately\ncorrect learning, which lies between the statistical learning theory where\nthere is no computational requirement on the learner and efficient PAC where\nthe learner must be polynomially bounded. Examples have recently been given of\nhypothesis classes which are PAC learnable but not computably PAC learnable,\nbut these hypothesis classes are unnatural or non-canonical in the sense that\nthey depend on a numbering of proofs, formulas, or programs. We use the\non-a-cone machinery from computability theory to prove that, under mild\nassumptions such as that the hypothesis class can be computably listable, any\nnatural hypothesis class which is learnable must be computably learnable. Thus\nthe counterexamples given previously are necessarily unnatural.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Logic"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HXjZYolMtyO3D9FoMY1HLzQDwgDVxAI1XneSs1uvLeM","pdfSize":"212518"}
