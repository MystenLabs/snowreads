{"id":"2407.04168","title":"Learning Interpretable Differentiable Logic Networks","authors":"Chang Yue and Niraj K. Jha","authorsParsed":[["Yue","Chang",""],["Jha","Niraj K.",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 21:58:26 GMT"}],"updateDate":"2024-07-08","timestamp":1720130306000,"abstract":"  The ubiquity of neural networks (NNs) in real-world applications, from\nhealthcare to natural language processing, underscores their immense utility in\ncapturing complex relationships within high-dimensional data. However, NNs come\nwith notable disadvantages, such as their \"black-box\" nature, which hampers\ninterpretability, as well as their tendency to overfit the training data. We\nintroduce a novel method for learning interpretable differentiable logic\nnetworks (DLNs) that are architectures that employ multiple layers of binary\nlogic operators. We train these networks by softening and differentiating their\ndiscrete components, e.g., through binarization of inputs, binary logic\noperations, and connections between neurons. This approach enables the use of\ngradient-based learning methods. Experimental results on twenty classification\ntasks indicate that differentiable logic networks can achieve accuracies\ncomparable to or exceeding that of traditional NNs. Equally importantly, these\nnetworks offer the advantage of interpretability. Moreover, their relatively\nsimple structure results in the number of logic gate-level operations during\ninference being up to a thousand times smaller than NNs, making them suitable\nfor deployment on edge devices.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}