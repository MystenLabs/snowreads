{"id":"2408.16031","title":"EMP: Enhance Memory in Data Pruning","authors":"Jinying Xiao and Ping Li and Jie Nie and Zhe Tang","authorsParsed":[["Xiao","Jinying",""],["Li","Ping",""],["Nie","Jie",""],["Tang","Zhe",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 10:29:52 GMT"}],"updateDate":"2024-08-30","timestamp":1724840992000,"abstract":"  Recently, large language and vision models have shown strong performance, but\ndue to high pre-training and fine-tuning costs, research has shifted towards\nfaster training via dataset pruning. Previous methods used sample loss as an\nevaluation criterion, aiming to select the most \"difficult\" samples for\ntraining. However, when the pruning rate increases, the number of times each\nsample is trained becomes more evenly distributed, which causes many critical\nor general samples to not be effectively fitted. We refer to this as\nLow-Frequency Learning (LFL). In other words, LFL prevents the model from\nremembering most samples. In our work, we decompose the scoring function of\nLFL, provide a theoretical explanation for the inefficiency of LFL, and propose\nadding a memory term to the scoring function to enhance the model's memory\ncapability, along with an approximation of this memory term. Similarly, we\nexplore memory in Self-Supervised Learning (SSL), marking the first discussion\non SSL memory. Using contrastive learning, we derive the memory term both\ntheoretically and experimentally. Finally, we propose Enhance Memory Pruning\n(EMP), which addresses the issue of insufficient memory under high pruning\nrates by enhancing the model's memory of data, thereby improving its\nperformance. We evaluated the performance of EMP in tasks such as image\nclassification, natural language understanding, and model pre-training. The\nresults show that EMP can improve model performance under extreme pruning\nrates. For example, in the CIFAR100-ResNet50 pre-training task, with 70\\%\npruning, EMP outperforms current methods by 2.2\\%.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}