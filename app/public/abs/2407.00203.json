{"id":"2407.00203","title":"PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through\n  Multi-agent Collaboration","authors":"Yuxuan Sun, Yunlong Zhang, Yixuan Si, Chenglu Zhu, Zhongyi Shui, Kai\n  Zhang, Jingxiong Li, Xingheng Lyu, Tao Lin, Lin Yang","authorsParsed":[["Sun","Yuxuan",""],["Zhang","Yunlong",""],["Si","Yixuan",""],["Zhu","Chenglu",""],["Shui","Zhongyi",""],["Zhang","Kai",""],["Li","Jingxiong",""],["Lyu","Xingheng",""],["Lin","Tao",""],["Yang","Lin",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 19:18:09 GMT"}],"updateDate":"2024-07-02","timestamp":1719602289000,"abstract":"  Vision Language Models (VLMs) like CLIP have attracted substantial attention\nin pathology, serving as backbones for applications such as zero-shot image\nclassification and Whole Slide Image (WSI) analysis. Additionally, they can\nfunction as vision encoders when combined with large language models (LLMs) to\nsupport broader capabilities. Current efforts to train pathology VLMs rely on\npathology image-text pairs from platforms like PubMed, YouTube, and Twitter,\nwhich provide limited, unscalable data with generally suboptimal image quality.\nIn this work, we leverage large-scale WSI datasets like TCGA to extract\nnumerous high-quality image patches. We then train a large multimodal model to\ngenerate captions for these images, creating PathGen-1.6M, a dataset containing\n1.6 million high-quality image-caption pairs. Our approach involves multiple\nagent models collaborating to extract representative WSI patches, generating\nand refining captions to obtain high-quality image-text pairs. Extensive\nexperiments show that integrating these generated pairs with existing datasets\nto train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances\nits ability to analyze pathological images, with substantial improvements\nacross nine pathology-related zero-shot image classification tasks and three\nwhole-slide image tasks. Furthermore, we construct 200K instruction-tuning data\nbased on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create\nmore powerful multimodal models through instruction tuning. Overall, we provide\na scalable pathway for high-quality data generation in pathology, paving the\nway for next-generation general pathology models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}