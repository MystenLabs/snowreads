{"id":"2408.09697","title":"Heta: Distributed Training of Heterogeneous Graph Neural Networks","authors":"Yuchen Zhong, Junwei Su, Chuan Wu, Minjie Wang","authorsParsed":[["Zhong","Yuchen",""],["Su","Junwei",""],["Wu","Chuan",""],["Wang","Minjie",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 04:43:56 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 04:46:18 GMT"}],"updateDate":"2024-08-21","timestamp":1724042636000,"abstract":"  Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_a6guKM2oVZK-LlMzfNf-LV4UotiGYQLzmvtnWLfYAk","pdfSize":"1367786"}
