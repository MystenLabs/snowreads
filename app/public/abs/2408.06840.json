{"id":"2408.06840","title":"Dynamic and Compressive Adaptation of Transformers From Images to Videos","authors":"Guozhen Zhang, Jingyu Liu, Shengming Cao, Xiaotong Zhao, Kevin Zhao,\n  Kai Ma, Limin Wang","authorsParsed":[["Zhang","Guozhen",""],["Liu","Jingyu",""],["Cao","Shengming",""],["Zhao","Xiaotong",""],["Zhao","Kevin",""],["Ma","Kai",""],["Wang","Limin",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 12:01:22 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 01:37:17 GMT"}],"updateDate":"2024-08-15","timestamp":1723550482000,"abstract":"  Recently, the remarkable success of pre-trained Vision Transformers (ViTs)\nfrom image-text matching has sparked an interest in image-to-video adaptation.\nHowever, most current approaches retain the full forward pass for each frame,\nleading to a high computation overhead for processing entire videos. In this\npaper, we present InTI, a novel approach for compressive image-to-video\nadaptation using dynamic Inter-frame Token Interpolation. InTI aims to softly\npreserve the informative tokens without disrupting their coherent\nspatiotemporal structure. Specifically, each token pair at identical positions\nwithin neighbor frames is linearly aggregated into a new token, where the\naggregation weights are generated by a multi-scale context-aware network. In\nthis way, the information of neighbor frames can be adaptively compressed in a\npoint-by-point manner, thereby effectively reducing the number of processed\nframes by half each time. Importantly, InTI can be seamlessly integrated with\nexisting adaptation methods, achieving strong performance without extra-complex\ndesign. On Kinetics-400, InTI reaches a top-1 accuracy of 87.1 with a\nremarkable 37.5% reduction in GFLOPs compared to naive adaptation. When\ncombined with additional temporal modules, InTI achieves a top-1 accuracy of\n87.6 with a 37% reduction in GFLOPs. Similar conclusions have been verified in\nother common datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}