{"id":"2407.03945","title":"A fast neural hybrid Newton solver adapted to implicit methods for\n  nonlinear dynamics","authors":"Tianyu Jin, Georg Maierhofer, Katharina Schratz, Yang Xiang","authorsParsed":[["Jin","Tianyu",""],["Maierhofer","Georg",""],["Schratz","Katharina",""],["Xiang","Yang",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 14:02:10 GMT"}],"updateDate":"2024-07-08","timestamp":1720101730000,"abstract":"  The use of implicit time-stepping schemes for the numerical approximation of\nsolutions to stiff nonlinear time-evolution equations brings well-known\nadvantages including, typically, better stability behaviour and corresponding\nsupport of larger time steps, and better structure preservation properties.\nHowever, this comes at the price of having to solve a nonlinear equation at\nevery time step of the numerical scheme. In this work, we propose a novel\noperator learning based hybrid Newton's method to accelerate this solution of\nthe nonlinear time step system for stiff time-evolution nonlinear equations. We\npropose a targeted learning strategy which facilitates robust unsupervised\nlearning in an offline phase and provides a highly efficient initialisation for\nthe Newton iteration leading to consistent acceleration of Newton's method. A\nquantifiable rate of improvement in Newton's method achieved by improved\ninitialisation is provided and we analyse the upper bound of the generalisation\nerror of our unsupervised learning strategy. These theoretical results are\nsupported by extensive numerical results, demonstrating the efficiency of our\nproposed neural hybrid solver both in one- and two-dimensional cases.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}