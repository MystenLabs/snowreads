{"id":"2407.01031","title":"PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs","authors":"Dan Peng, Zhihui Fu, Jun Wang","authorsParsed":[["Peng","Dan",""],["Fu","Zhihui",""],["Wang","Jun",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 07:26:56 GMT"}],"updateDate":"2024-07-02","timestamp":1719818816000,"abstract":"  Recent advancements in large language models (LLMs) have indeed showcased\ntheir impressive capabilities. On mobile devices, the wealth of valuable,\nnon-public data generated daily holds great promise for locally fine-tuning\npersonalized LLMs, while maintaining privacy through on-device processing.\nHowever, the constraints of mobile device resources pose challenges to direct\non-device LLM fine-tuning, mainly due to the memory-intensive nature of\nderivative-based optimization required for saving gradients and optimizer\nstates. To tackle this, we propose employing derivative-free optimization\ntechniques to enable on-device fine-tuning of LLM, even on memory-limited\nmobile devices. Empirical results demonstrate that the RoBERTa-large model and\nOPT-1.3B can be fine-tuned locally on the OPPO Reno 6 smartphone using around\n4GB and 6.5GB of memory respectively, using derivative-free optimization\ntechniques. This highlights the feasibility of on-device LLM fine-tuning on\nmobile devices, paving the way for personalized LLMs on resource-constrained\ndevices while safeguarding data privacy.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"ryeQSUSQV7ydb6KxZvGg-NEbj1QnOrjufC7_G4KvwhI","pdfSize":"201092"}
