{"id":"2407.02147","title":"GemmAr: Enhancing LLMs Through Arabic Instruction-Tuning","authors":"Hasna Chouikhi, Manel Aloui, Cyrine Ben Hammou, Ghaith Chaabane,\n  Haithem Kchaou, Chehir Dhaouadi","authorsParsed":[["Chouikhi","Hasna",""],["Aloui","Manel",""],["Hammou","Cyrine Ben",""],["Chaabane","Ghaith",""],["Kchaou","Haithem",""],["Dhaouadi","Chehir",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 10:43:49 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 15:36:11 GMT"}],"updateDate":"2024-07-10","timestamp":1719917029000,"abstract":"  Large language models (LLMs) have greatly impacted the natural language\nprocessing (NLP) field, particularly for the English language. These models\nhave demonstrated capabilities in understanding and generating human-like text.\nThe success of language models largely depends on the availability of\nhigh-quality instruction datasets, which consist of detailed task descriptions\nand corresponding responses that are essential for training the models to\naddress a variety of prompts accurately. However, the availability and quality\nof these resources vary by language. While models perform well in English, they\noften need help with languages like Arabic, due to the lack of datasets for\nfine-tuning Arabic-specific tasks. To address this issue, we introduce\nInstAr-500k, a new Arabic instruction dataset created by generating and\ncollecting content that covers several domains and instruction types. We assess\nthis dataset by fine-tuning an open-source Gemma-7B model on several downstream\ntasks to improve its functionality. Based on multiple evaluations, our\nfine-tuned model achieves excellent performance on several Arabic NLP\nbenchmarks. These outcomes emphasize the effectiveness of our dataset in\nelevating the capabilities of language models for Arabic. Our instruction\ndataset bridges the performance gap between English and Arabic language models\nby providing resources that amplify Arabic NLP development. Building on this\nfoundation, we developed a model, GemmAr-7B-V1, specifically tuned to excel at\na wide range of Arabic NLP tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"SCrsGle22bXeShmi7iHnF7cO4MXWeVJQVEiOqJ50bxA","pdfSize":"1981414"}
