{"id":"2407.13518","title":"Model-based Policy Optimization using Symbolic World Model","authors":"Andrey Gorodetskiy, Konstantin Mironov, Aleksandr Panov","authorsParsed":[["Gorodetskiy","Andrey",""],["Mironov","Konstantin",""],["Panov","Aleksandr",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 13:49:21 GMT"}],"updateDate":"2024-07-19","timestamp":1721310561000,"abstract":"  The application of learning-based control methods in robotics presents\nsignificant challenges. One is that model-free reinforcement learning\nalgorithms use observation data with low sample efficiency. To address this\nchallenge, a prevalent approach is model-based reinforcement learning, which\ninvolves employing an environment dynamics model. We suggest approximating\ntransition dynamics with symbolic expressions, which are generated via symbolic\nregression. Approximation of a mechanical system with a symbolic model has\nfewer parameters than approximation with neural networks, which can potentially\nlead to higher accuracy and quality of extrapolation. We use a symbolic\ndynamics model to generate trajectories in model-based policy optimization to\nimprove the sample efficiency of the learning algorithm. We evaluate our\napproach across various tasks within simulated environments. Our method\ndemonstrates superior sample efficiency in these tasks compared to model-free\nand model-based baseline methods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}