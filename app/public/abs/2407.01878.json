{"id":"2407.01878","title":"Compare without Despair: Reliable Preference Evaluation with Generation\n  Separability","authors":"Sayan Ghosh, Tejas Srinivasan, Swabha Swayamdipta","authorsParsed":[["Ghosh","Sayan",""],["Srinivasan","Tejas",""],["Swayamdipta","Swabha",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 01:37:56 GMT"},{"version":"v2","created":"Mon, 8 Jul 2024 20:50:55 GMT"}],"updateDate":"2024-07-10","timestamp":1719884276000,"abstract":"  Human evaluation of generated language through pairwise preference judgments\nis pervasive. However, under common scenarios, such as when generations from a\nmodel pair are very similar, or when stochastic decoding results in large\nvariations in generations, it results in inconsistent preference ratings. We\naddress these challenges by introducing a meta-evaluation measure,\nseparability, which estimates how suitable a test instance is for pairwise\npreference evaluation. For a candidate test instance, separability samples\nmultiple generations from a pair of models, and measures how distinguishable\nthe two sets of generations are. Our experiments show that instances with high\nseparability values yield more consistent preference ratings from both human-\nand auto-raters. Further, the distribution of separability allows insights into\nwhich test benchmarks are more valuable for comparing models. Finally, we\nincorporate separability into ELO ratings, accounting for how suitable each\ntest instance might be for reliably ranking LLMs. Overall, separability has\nimplications for consistent, efficient and robust preference evaluation of LLMs\nwith both human- and auto-raters.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}