{"id":"2408.08688","title":"The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation","authors":"Samee Arif, Sualeha Farid, Abdul Hameed Azeemi, Awais Athar, Agha Ali\n  Raza","authorsParsed":[["Arif","Samee",""],["Farid","Sualeha",""],["Azeemi","Abdul Hameed",""],["Athar","Awais",""],["Raza","Agha Ali",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 12:01:55 GMT"},{"version":"v2","created":"Sat, 24 Aug 2024 12:34:01 GMT"},{"version":"v3","created":"Sat, 7 Sep 2024 15:39:49 GMT"}],"updateDate":"2024-09-10","timestamp":1723809715000,"abstract":"  This paper presents synthetic Preference Optimization (PO) datasets generated\nusing multi-agent workflows and evaluates the effectiveness and potential of\nthese workflows in the dataset generation process. PO dataset generation\nrequires two modules: (1) response evaluation, and (2) response generation. In\nthe response evaluation module, the responses from Large Language Models (LLMs)\nare evaluated and ranked - a task typically carried out by human annotators\nthat we automate using LLMs. We assess the response evaluation module in a 2\nstep process. In step 1, we assess LLMs as evaluators using three distinct\nprompting strategies. In step 2, we apply the winning prompting strategy to\ncompare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In\neach step, we use inter-rater agreement using Cohen's Kappa between human\nannotators and LLMs. For the response generation module, we compare different\nconfigurations for the LLM Feedback Loop using the identified LLM evaluator\nconfiguration. We use the win rate (the fraction of times a generation\nframework is selected as the best by an LLM evaluator) to determine the best\nmulti-agent configuration for generation. After identifying the best\nconfigurations for both modules, we use models from the GPT, Gemma, and Llama\nfamilies to generate our PO datasets using the above pipeline. We generate two\ntypes of PO datasets, one to improve the generation capabilities of individual\nLLM and the other to improve the multi-agent workflow. Our evaluation shows\nthat GPT-4o-as-a-Judge is more consistent across datasets when the candidate\nresponses do not include responses from the GPT family. Additionally, we find\nthat the LLM Feedback Loop, with Llama as the generator and Gemma as the\nreviewer, achieves a notable 71.8% and 73.8% win rate over single-agent Llama\nand Gemma, respectively.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}