{"id":"2407.08843","title":"Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based\n  Models","authors":"Daniela de Albuquerque and John Pearson","authorsParsed":[["de Albuquerque","Daniela",""],["Pearson","John",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 19:58:19 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 00:57:16 GMT"}],"updateDate":"2024-08-22","timestamp":1720727899000,"abstract":"  Beyond estimating parameters of interest from data, one of the key goals of\nstatistical inference is to properly quantify uncertainty in these estimates.\nIn Bayesian inference, this uncertainty is provided by the posterior\ndistribution, the computation of which typically involves an intractable\nhigh-dimensional integral. Among available approximation methods,\nsampling-based approaches come with strong theoretical guarantees but scale\npoorly to large problems, while variational approaches scale well but offer few\ntheoretical guarantees. In particular, variational methods are known to produce\noverconfident estimates of posterior uncertainty and are typically\nnon-identifiable, with many latent variable configurations generating\nequivalent predictions. Here, we address these challenges by showing how\ndiffusion-based models (DBMs), which have recently produced state-of-the-art\nperformance in generative modeling tasks, can be repurposed for performing\ncalibrated, identifiable Bayesian inference. By exploiting a previously\nestablished connection between the stochastic and probability flow ordinary\ndifferential equations (pfODEs) underlying DBMs, we derive a class of models,\ninflationary flows, that uniquely and deterministically map high-dimensional\ndata to a lower-dimensional Gaussian distribution via ODE integration. This map\nis both invertible and neighborhood-preserving, with controllable numerical\nerror, with the result that uncertainties in the data are correctly propagated\nto the latent space. We demonstrate how such maps can be learned via standard\nDBM training using a novel noise schedule and are effective at both preserving\nand reducing intrinsic data dimensionality. The result is a class of highly\nexpressive generative models, uniquely defined on a low-dimensional latent\nspace, that afford principled Bayesian inference.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NQ-q5lFULfPZ2YNjIkvvNZTNbOviZb1Z_cJ5FwvsNG8","pdfSize":"13779461","objectId":"0x33281a52ea4ee52de23eac0de36bd8e4a78c580ec237aea81dc43413db7d1066","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
