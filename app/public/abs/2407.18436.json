{"id":"2407.18436","title":"A Model for Combinatorial Dictionary Learning and Inference","authors":"Avrim Blum, Kavya Ravichandran","authorsParsed":[["Blum","Avrim",""],["Ravichandran","Kavya",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 00:13:30 GMT"}],"updateDate":"2024-07-29","timestamp":1721952810000,"abstract":"  We are often interested in decomposing complex, structured data into simple\ncomponents that explain the data. The linear version of this problem is\nwell-studied as dictionary learning and factor analysis. In this work, we\npropose a combinatorial model in which to study this question, motivated by the\nway objects occlude each other in a scene to form an image. First, we identify\na property we call \"well-structuredness\" of a set of low-dimensional components\nwhich ensures that no two components in the set are too similar. We show how\nwell-structuredness is sufficient for learning the set of latent components\ncomprising a set of sample instances. We then consider the problem: given a set\nof components and an instance generated from some unknown subset of them,\nidentify which parts of the instance arise from which components. We consider\ntwo variants: (1) determine the minimal number of components required to\nexplain the instance; (2) determine the correct explanation for as many\nlocations as possible. For the latter goal, we also devise a version that is\nrobust to adversarial corruptions, with just a slightly stronger assumption on\nthe components. Finally, we show that the learning problem is computationally\ninfeasible in the absence of any assumptions.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Data Structures and Algorithms"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}