{"id":"2407.00637","title":"DP-MLM: Differentially Private Text Rewriting Using Masked Language\n  Models","authors":"Stephen Meisenbacher, Maulik Chevli, Juraj Vladika, and Florian\n  Matthes","authorsParsed":[["Meisenbacher","Stephen",""],["Chevli","Maulik",""],["Vladika","Juraj",""],["Matthes","Florian",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 09:31:01 GMT"}],"updateDate":"2024-07-02","timestamp":1719739861000,"abstract":"  The task of text privatization using Differential Privacy has recently taken\nthe form of $\\textit{text rewriting}$, in which an input text is obfuscated via\nthe use of generative (large) language models. While these methods have shown\npromising results in the ability to preserve privacy, these methods rely on\nautoregressive models which lack a mechanism to contextualize the private\nrewriting process. In response to this, we propose $\\textbf{DP-MLM}$, a new\nmethod for differentially private text rewriting based on leveraging masked\nlanguage models (MLMs) to rewrite text in a semantically similar $\\textit{and}$\nobfuscated manner. We accomplish this with a simple contextualization\ntechnique, whereby we rewrite a text one token at a time. We find that\nutilizing encoder-only MLMs provides better utility preservation at lower\n$\\varepsilon$ levels, as compared to previous methods relying on larger models\nwith a decoder. In addition, MLMs allow for greater customization of the\nrewriting mechanism, as opposed to generative approaches. We make the code for\n$\\textbf{DP-MLM}$ public and reusable, found at https://github.com/sjmeis/DPMLM .\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}