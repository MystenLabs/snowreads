{"id":"2408.12102","title":"Integrating Audio, Visual, and Semantic Information for Enhanced\n  Multimodal Speaker Diarization","authors":"Luyao Cheng and Hui Wang and Siqi Zheng and Yafeng Chen and Rongjie\n  Huang and Qinglin Zhang and Qian Chen and Xihao Li","authorsParsed":[["Cheng","Luyao",""],["Wang","Hui",""],["Zheng","Siqi",""],["Chen","Yafeng",""],["Huang","Rongjie",""],["Zhang","Qinglin",""],["Chen","Qian",""],["Li","Xihao",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 03:34:03 GMT"}],"updateDate":"2024-08-23","timestamp":1724297643000,"abstract":"  Speaker diarization, the process of segmenting an audio stream or transcribed\nspeech content into homogenous partitions based on speaker identity, plays a\ncrucial role in the interpretation and analysis of human speech. Most existing\nspeaker diarization systems rely exclusively on unimodal acoustic information,\nmaking the task particularly challenging due to the innate ambiguities of audio\nsignals. Recent studies have made tremendous efforts towards audio-visual or\naudio-semantic modeling to enhance performance. However, even the incorporation\nof up to two modalities often falls short in addressing the complexities of\nspontaneous and unstructured conversations. To exploit more meaningful dialogue\npatterns, we propose a novel multimodal approach that jointly utilizes audio,\nvisual, and semantic cues to enhance speaker diarization. Our method elegantly\nformulates the multimodal modeling as a constrained optimization problem.\nFirst, we build insights into the visual connections among active speakers and\nthe semantic interactions within spoken content, thereby establishing abundant\npairwise constraints. Then we introduce a joint pairwise constraint propagation\nalgorithm to cluster speakers based on these visual and semantic constraints.\nThis integration effectively leverages the complementary strengths of different\nmodalities, refining the affinity estimation between individual speaker\nembeddings. Extensive experiments conducted on multiple multimodal datasets\ndemonstrate that our approach consistently outperforms state-of-the-art speaker\ndiarization methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}