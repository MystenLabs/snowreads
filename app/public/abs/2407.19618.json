{"id":"2407.19618","title":"Experimenting on Markov Decision Processes with Local Treatments","authors":"Shuze Chen, David Simchi-Levi, Chonghuan Wang","authorsParsed":[["Chen","Shuze",""],["Simchi-Levi","David",""],["Wang","Chonghuan",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 00:41:11 GMT"}],"updateDate":"2024-07-30","timestamp":1722213671000,"abstract":"  As service systems grow increasingly complex and dynamic, many interventions\nbecome localized, available and taking effect only in specific states. This\npaper investigates experiments with local treatments on a widely-used class of\ndynamic models, Markov Decision Processes (MDPs). Particularly, we focus on\nutilizing the local structure to improve the inference efficiency of the\naverage treatment effect. We begin by demonstrating the efficiency of classical\ninference methods, including model-based estimation and temporal difference\nlearning under a fixed policy, as well as classical A/B testing with general\ntreatments. We then introduce a variance reduction technique that exploits the\nlocal treatment structure by sharing information for states unaffected by the\ntreatment policy. Our new estimator effectively overcomes the variance lower\nbound for general treatments while matching the more stringent lower bound\nincorporating the local treatment structure. Furthermore, our estimator can\noptimally achieve a linear reduction with the number of test arms for a major\npart of the variance. Finally, we explore scenarios with perfect knowledge of\nthe control arm and design estimators that further improve inference\nefficiency.\n","subjects":["Statistics/Methodology","Computing Research Repository/Machine Learning","Economics/Econometrics","Statistics/Applications","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}