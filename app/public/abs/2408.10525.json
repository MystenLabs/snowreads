{"id":"2408.10525","title":"MPGNet: Learning Move-Push-Grasping Synergy for Target-Oriented Grasping\n  in Occluded Scenes","authors":"Dayou Li, Chenkun Zhao, Shuo Yang, Ran Song, Xiaolei Li and Wei Zhang","authorsParsed":[["Li","Dayou",""],["Zhao","Chenkun",""],["Yang","Shuo",""],["Song","Ran",""],["Li","Xiaolei",""],["Zhang","Wei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 04:01:19 GMT"}],"updateDate":"2024-08-21","timestamp":1724126479000,"abstract":"  This paper focuses on target-oriented grasping in occluded scenes, where the\ntarget object is specified by a binary mask and the goal is to grasp the target\nobject with as few robotic manipulations as possible. Most existing methods\nrely on a push-grasping synergy to complete this task. To deliver a more\npowerful target-oriented grasping pipeline, we present MPGNet, a three-branch\nnetwork for learning a synergy between moving, pushing, and grasping actions.\nWe also propose a multi-stage training strategy to train the MPGNet which\ncontains three policy networks corresponding to the three actions. The\neffectiveness of our method is demonstrated via both simulated and real-world\nexperiments.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}