{"id":"2407.03841","title":"On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation","authors":"John Mendon\\c{c}a and Alon Lavie and Isabel Trancoso","authorsParsed":[["Mendon√ßa","John",""],["Lavie","Alon",""],["Trancoso","Isabel",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 11:14:47 GMT"}],"updateDate":"2024-07-08","timestamp":1720091687000,"abstract":"  Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}