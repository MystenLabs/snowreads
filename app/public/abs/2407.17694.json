{"id":"2407.17694","title":"Doubly Robust Conditional Independence Testing with Generative Neural\n  Networks","authors":"Yi Zhang, Linjun Huang, Yun Yang, Xiaofeng Shao","authorsParsed":[["Zhang","Yi",""],["Huang","Linjun",""],["Yang","Yun",""],["Shao","Xiaofeng",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 01:28:59 GMT"}],"updateDate":"2024-07-26","timestamp":1721870939000,"abstract":"  This article addresses the problem of testing the conditional independence of\ntwo generic random vectors $X$ and $Y$ given a third random vector $Z$, which\nplays an important role in statistical and machine learning applications. We\npropose a new non-parametric testing procedure that avoids explicitly\nestimating any conditional distributions but instead requires sampling from the\ntwo marginal conditional distributions of $X$ given $Z$ and $Y$ given $Z$. We\nfurther propose using a generative neural network (GNN) framework to sample\nfrom these approximated marginal conditional distributions, which tends to\nmitigate the curse of dimensionality due to its adaptivity to any\nlow-dimensional structures and smoothness underlying the data. Theoretically,\nour test statistic is shown to enjoy a doubly robust property against GNN\napproximation errors, meaning that the test statistic retains all desirable\nproperties of the oracle test statistic utilizing the true marginal conditional\ndistributions, as long as the product of the two approximation errors decays to\nzero faster than the parametric rate. Asymptotic properties of our statistic\nand the consistency of a bootstrap procedure are derived under both null and\nlocal alternatives. Extensive numerical experiments and real data analysis\nillustrate the effectiveness and broad applicability of our proposed test.\n","subjects":["Statistics/Methodology","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}