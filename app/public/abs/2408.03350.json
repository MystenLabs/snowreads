{"id":"2408.03350","title":"miniCTX: Neural Theorem Proving with (Long-)Contexts","authors":"Jiewen Hu, Thomas Zhu, Sean Welleck","authorsParsed":[["Hu","Jiewen",""],["Zhu","Thomas",""],["Welleck","Sean",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 20:19:18 GMT"}],"updateDate":"2024-08-08","timestamp":1722889158000,"abstract":"  We introduce miniCTX, which tests a model's ability to prove formal\nmathematical theorems that depend on new definitions, lemmas, or other\ncontextual information that was not observed during training. miniCTX contains\ntheorems sourced from real Lean projects and textbooks, each associated with a\ncontext that can span tens of thousands of tokens. Models are tasked with\nproving a theorem given access to code from the theorem's repository, which\ncontains context that is helpful or needed for the proof. As a baseline for\nminiCTX, we introduce file-tuning, a simple recipe that trains a model to\ngenerate a proof step conditioned on the preceding file contents. File-tuning\nsubstantially outperforms the traditional neural theorem proving approach that\nfine-tunes on states alone. Additionally, our file-tuned model improves\nperformance on the standard miniF2F benchmark, achieving a pass rate of 33.61%,\nwhich is a new state-of-the-art for 1.3B parameter models. Alongside miniCTX,\nwe offer ntp-toolkit for automatically extracting and annotating theorem\nproving data, making it easy to add new projects into miniCTX to ensure that\ncontexts are not seen during training. miniCTX offers a challenging and\nrealistic perspective on evaluating neural theorem provers.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}