{"id":"2408.00565","title":"MUFASA: Multi-View Fusion and Adaptation Network with Spatial Awareness\n  for Radar Object Detection","authors":"Xiangyuan Peng, Miao Tang, Huawei Sun, Kay Bierzynski, Lorenzo\n  Servadei, and Robert Wille","authorsParsed":[["Peng","Xiangyuan",""],["Tang","Miao",""],["Sun","Huawei",""],["Bierzynski","Kay",""],["Servadei","Lorenzo",""],["Wille","Robert",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 13:52:18 GMT"}],"updateDate":"2024-08-02","timestamp":1722520338000,"abstract":"  In recent years, approaches based on radar object detection have made\nsignificant progress in autonomous driving systems due to their robustness\nunder adverse weather compared to LiDAR. However, the sparsity of radar point\nclouds poses challenges in achieving precise object detection, highlighting the\nimportance of effective and comprehensive feature extraction technologies. To\naddress this challenge, this paper introduces a comprehensive feature\nextraction method for radar point clouds. This study first enhances the\ncapability of detection networks by using a plug-and-play module, GeoSPA. It\nleverages the Lalonde features to explore local geometric patterns.\nAdditionally, a distributed multi-view attention mechanism, DEMVA, is designed\nto integrate the shared information across the entire dataset with the global\ninformation of each individual frame. By employing the two modules, we present\nour method, MUFASA, which enhances object detection performance through\nimproved feature extraction. The approach is evaluated on the VoD and\nTJ4DRaDSet datasets to demonstrate its effectiveness. In particular, we achieve\nstate-of-the-art results among radar-based methods on the VoD dataset with the\nmAP of 50.24%.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}