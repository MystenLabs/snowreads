{"id":"2408.05008","title":"FlowDreamer: exploring high fidelity text-to-3D generation via rectified\n  flow","authors":"Hangyu Li and Xiangxiang Chu and Dingyuan Shi and Lin Wang","authorsParsed":[["Li","Hangyu",""],["Chu","Xiangxiang",""],["Shi","Dingyuan",""],["Wang","Lin",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 11:40:20 GMT"},{"version":"v2","created":"Fri, 13 Sep 2024 02:41:09 GMT"}],"updateDate":"2024-09-16","timestamp":1723203620000,"abstract":"  Recent advances in text-to-3D generation have made significant progress. In\nparticular, with the pretrained diffusion models, existing methods\npredominantly use Score Distillation Sampling (SDS) to train 3D models such as\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS). However, a\nhurdle is that they often encounter difficulties with over-smoothing textures\nand over-saturating colors. The rectified flow model - which utilizes a simple\nordinary differential equation (ODE) to represent a linear trajectory - shows\npromise as an alternative prior to text-to-3D generation. It learns a\ntime-independent vector field, thereby reducing the ambiguity in 3D model\nupdate gradients that are calculated using time-dependent scores in the SDS\nframework. In light of this, we first develop a mathematical analysis to\nseamlessly integrate SDS with rectified flow model, paving the way for our\ninitial framework known as Vector Field Distillation Sampling (VFDS). However,\nempirical findings indicate that VFDS still results in over-smoothing outcomes.\nTherefore, we analyze the grounding reasons for such a failure from the\nperspective of ODE trajectories. On top, we propose a novel framework, named\nFlowDreamer, which yields high-fidelity results with richer textual details and\nfaster convergence. The key insight is to leverage the coupling and reversible\nproperties of the rectified flow model to search for the corresponding noise,\nrather than using randomly sampled noise as in VFDS. Accordingly, we introduce\na novel Unique Couple Matching (UCM) loss, which guides the 3D model to\noptimize along the same trajectory. Our FlowDreamer is superior in its\nflexibility to be applied to both NeRF and 3D GS. Extensive experiments\ndemonstrate the high-fidelity outcomes and accelerated convergence of\nFlowDreamer.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}