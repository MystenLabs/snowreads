{"id":"2407.13399","title":"Correcting the Mythos of KL-Regularization: Direct Alignment without\n  Overoptimization via Chi-Squared Preference Optimization","authors":"Audrey Huang, Wenhao Zhan, Tengyang Xie, Jason D. Lee, Wen Sun, Akshay\n  Krishnamurthy, Dylan J. Foster","authorsParsed":[["Huang","Audrey",""],["Zhan","Wenhao",""],["Xie","Tengyang",""],["Lee","Jason D.",""],["Sun","Wen",""],["Krishnamurthy","Akshay",""],["Foster","Dylan J.",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 11:08:40 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 19:29:49 GMT"}],"updateDate":"2024-07-23","timestamp":1721300920000,"abstract":"  Language model alignment methods, such as reinforcement learning from human\nfeedback (RLHF), have led to impressive advances in language model\ncapabilities, but existing techniques are limited by a widely observed\nphenomenon known as overoptimization, where the quality of the language model\nplateaus or degrades over the course of the alignment process. Overoptimization\nis often attributed to overfitting to an inaccurate reward model, and while it\ncan be mitigated through online data collection, this is infeasible in many\nsettings. This raises a fundamental question: Do existing offline alignment\nalgorithms make the most of the data they have, or can their sample-efficiency\nbe improved further?\n  We address this question with a new algorithm for offline alignment,\n$\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to\nDirect Preference Optimization (DPO; Rafailov et al., 2023), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite\nthis minimal change, $\\chi$PO implicitly implements the principle of pessimism\nin the face of uncertainty via regularization with the $\\chi^2$-divergence --\nwhich quantifies uncertainty more effectively than KL-regularization -- and\nprovably alleviates overoptimization, achieving sample-complexity guarantees\nbased on single-policy concentrability -- the gold standard in offline\nreinforcement learning. $\\chi$PO's simplicity and strong guarantees make it the\nfirst practical and general-purpose offline alignment algorithm that is\nprovably robust to overoptimization.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1CMkEsCfzMuOjzchriYq0RVTwLkk9vYkUK2zBepMYYg","pdfSize":"1479341","objectId":"0xbb37ba1b5106f47216d7b20029af705f6ae71a8a2af4c1d4ade64c13246e33b7","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
