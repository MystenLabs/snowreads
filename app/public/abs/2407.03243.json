{"id":"2407.03243","title":"Visual Grounding with Attention-Driven Constraint Balancing","authors":"Weitai Kang, Luowei Zhou, Junyi Wu, Changchang Sun, Yan Yan","authorsParsed":[["Kang","Weitai",""],["Zhou","Luowei",""],["Wu","Junyi",""],["Sun","Changchang",""],["Yan","Yan",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 16:14:09 GMT"},{"version":"v2","created":"Sat, 6 Jul 2024 15:22:24 GMT"}],"updateDate":"2024-07-09","timestamp":1720023249000,"abstract":"  Unlike Object Detection, Visual Grounding task necessitates the detection of\nan object described by complex free-form language. To simultaneously model such\ncomplex semantic and visual representations, recent state-of-the-art studies\nadopt transformer-based models to fuse features from both modalities, further\nintroducing various modules that modulate visual features to align with the\nlanguage expressions and eliminate the irrelevant redundant information.\nHowever, their loss function, still adopting common Object Detection losses,\nsolely governs the bounding box regression output, failing to fully optimize\nfor the above objectives. To tackle this problem, in this paper, we first\nanalyze the attention mechanisms of transformer-based models. Building upon\nthis, we further propose a novel framework named Attention-Driven Constraint\nBalancing (AttBalance) to optimize the behavior of visual features within\nlanguage-relevant regions. Extensive experimental results show that our method\nbrings impressive improvements. Specifically, we achieve constant improvements\nover five different models evaluated on four different benchmarks. Moreover, we\nattain a new state-of-the-art performance by integrating our method into QRNet.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}