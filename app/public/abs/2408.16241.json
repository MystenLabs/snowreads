{"id":"2408.16241","title":"Making the Most of your Model: Methods for Finetuning and Applying\n  Pretrained Transformers","authors":"Davis Yoshida","authorsParsed":[["Yoshida","Davis",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 03:50:24 GMT"}],"updateDate":"2024-08-30","timestamp":1724903424000,"abstract":"  This thesis provides methods and analysis of models which make progress on\nthis goal. The techniques outlined are task agnostic, and should provide\nbenefit when used with nearly any transformer LM. We introduce two new\nfinetuning methods which add new capabilities to the models they are used on.\nThe first adds a recurrence mechanism, which removes the fixed-window sized\nconstraint and improves the efficiency of a transformer decoder. The second\nallows masked language models (MLMs) to be used for initialization of both the\nencoder and decoder of a non-autoregressive sequence-to-sequence transformer,\nopening up generative applications of models which were previously only used\nfor natural language understanding tasks.\n  We also introduce two new techniques for improving the quality of predictions\nof any transformer decoder without additional finetuning. One, hidden state\noptimization, can be applied to any transformer decoder to improve the quality\nof predictions at inference time, especially for few-shot classification. The\nother, conditional beam search, allows practitioners to search for natural\nlanguage generation (NLG) model outputs with high likelihood while conditioning\non the event that the output is not degenerate (e.g. empty, repetitive, etc.).\n  Finally, we provide theoretical and empirical insights on the divergence of\nmodel-likelihood and output quality which has widely been observed in prior\nwork. These insights apply to any model which represents a distribution over\ntext, and apply to language models which are not transformers or even\nautoregressive. We argue that the NLP community has, to some extent,\nmisunderstood the implications of these findings, and encourage a point of view\nwhich has more nuance.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WjATYbV7GvMkMVxiwpi1frek1StKV1_6lnFoiyX_IOs","pdfSize":"2466557"}
