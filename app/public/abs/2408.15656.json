{"id":"2408.15656","title":"Realigned Softmax Warping for Deep Metric Learning","authors":"Michael G. DeMoor, John J. Prevost","authorsParsed":[["DeMoor","Michael G.",""],["Prevost","John J.",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 09:17:25 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 09:45:59 GMT"}],"updateDate":"2024-09-04","timestamp":1724836645000,"abstract":"  Deep Metric Learning (DML) loss functions traditionally aim to control the\nforces of separability and compactness within an embedding space so that the\nsame class data points are pulled together and different class ones are pushed\napart. Within the context of DML, a softmax operation will typically normalize\ndistances into a probability for optimization, thus coupling all the push/pull\nforces together. This paper proposes a potential new class of loss functions\nthat operate within a euclidean domain and aim to take full advantage of the\ncoupled forces governing embedding space formation under a softmax. These\nforces of compactness and separability can be boosted or mitigated within\ncontrolled locations at will by using a warping function. In this work, we\nprovide a simple example of a warping function and use it to achieve\ncompetitive, state-of-the-art results on various metric learning benchmarks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}