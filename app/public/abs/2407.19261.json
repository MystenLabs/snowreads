{"id":"2407.19261","title":"Evaluating Large Language Models in Detecting Test Smells","authors":"Keila Lucas, Rohit Gheyi, Elvys Soares, M\\'arcio Ribeiro and Ivan\n  Machado","authorsParsed":[["Lucas","Keila",""],["Gheyi","Rohit",""],["Soares","Elvys",""],["Ribeiro","MÃ¡rcio",""],["Machado","Ivan",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 14:00:05 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 12:16:54 GMT"}],"updateDate":"2024-07-31","timestamp":1722088805000,"abstract":"  Test smells are coding issues that typically arise from inadequate practices,\na lack of knowledge about effective testing, or deadline pressures to complete\nprojects. The presence of test smells can negatively impact the maintainability\nand reliability of software. While there are tools that use advanced static\nanalysis or machine learning techniques to detect test smells, these tools\noften require effort to be used. This study aims to evaluate the capability of\nLarge Language Models (LLMs) in automatically detecting test smells. We\nevaluated ChatGPT-4, Mistral Large, and Gemini Advanced using 30 types of test\nsmells across codebases in seven different programming languages collected from\nthe literature. ChatGPT-4 identified 21 types of test smells. Gemini Advanced\nidentified 17 types, while Mistral Large detected 15 types of test smells.\nConclusion: The LLMs demonstrated potential as a valuable tool in identifying\ntest smells.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}