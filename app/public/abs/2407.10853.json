{"id":"2407.10853","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","authors":"Dylan Bouchard","authorsParsed":[["Bouchard","Dylan",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 16:04:44 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 15:12:39 GMT"}],"updateDate":"2024-08-08","timestamp":1721059484000,"abstract":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. This paper aims to provide a technical guide for\npractitioners to assess bias and fairness risks in LLM use cases. The main\ncontribution of this work is a decision framework that allows practitioners to\ndetermine which metrics to use for a specific LLM use case. To achieve this,\nthis study categorizes LLM bias and fairness risks, maps those risks to a\ntaxonomy of LLM use cases, and then formally defines various metrics to assess\neach type of risk. As part of this work, several new bias and fairness metrics\nare introduced, including innovative counterfactual metrics as well as metrics\nbased on stereotype classifiers. Instead of focusing solely on the model\nitself, the sensitivity of both prompt-risk and model-risk are taken into\naccount by defining evaluations at the level of an LLM use case, characterized\nby a model and a population of prompts. Furthermore, because all of the\nevaluation metrics are calculated solely using the LLM output, the proposed\nframework is highly practical and easily actionable for practitioners.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}