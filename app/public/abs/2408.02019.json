{"id":"2408.02019","title":"Personalized Federated Learning on Heterogeneous and Long-Tailed Data\n  via Expert Collaborative Learning","authors":"Fengling Lv and Xinyi Shang and Yang Zhou and Yiqun Zhang and Mengke\n  Li and Yang Lu","authorsParsed":[["Lv","Fengling",""],["Shang","Xinyi",""],["Zhou","Yang",""],["Zhang","Yiqun",""],["Li","Mengke",""],["Lu","Yang",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 13:11:49 GMT"}],"updateDate":"2024-08-06","timestamp":1722777109000,"abstract":"  Personalized Federated Learning (PFL) aims to acquire customized models for\neach client without disclosing raw data by leveraging the collective knowledge\nof distributed clients. However, the data collected in real-world scenarios is\nlikely to follow a long-tailed distribution. For example, in the medical\ndomain, it is more common for the number of general health notes to be much\nlarger than those specifically relatedto certain diseases. The presence of\nlong-tailed data can significantly degrade the performance of PFL models.\nAdditionally, due to the diverse environments in which each client operates,\ndata heterogeneity is also a classic challenge in federated learning. In this\npaper, we explore the joint problem of global long-tailed distribution and data\nheterogeneity in PFL and propose a method called Expert Collaborative Learning\n(ECL) to tackle this problem. Specifically, each client has multiple experts,\nand each expert has a different training subset, which ensures that each class,\nespecially the minority classes, receives sufficient training. Multiple experts\ncollaborate synergistically to produce the final prediction output. Without\nspecial bells and whistles, the vanilla ECL outperforms other state-of-the-art\nPFL methods on several benchmark datasets under different degrees of data\nheterogeneity and long-tailed distribution.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}