{"id":"2407.21640","title":"MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical\n  Image Segmentation","authors":"Sina Ghorbani Kolahi, Seyed Kamal Chaharsooghi, Toktam Khatibi, Afshin\n  Bozorgpour, Reza Azad, Moein Heidari, Ilker Hacihaliloglu, Dorit Merhof","authorsParsed":[["Kolahi","Sina Ghorbani",""],["Chaharsooghi","Seyed Kamal",""],["Khatibi","Toktam",""],["Bozorgpour","Afshin",""],["Azad","Reza",""],["Heidari","Moein",""],["Hacihaliloglu","Ilker",""],["Merhof","Dorit",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 14:41:10 GMT"},{"version":"v2","created":"Sat, 3 Aug 2024 22:22:30 GMT"}],"updateDate":"2024-08-06","timestamp":1722436870000,"abstract":"  Medical image segmentation involves identifying and separating object\ninstances in a medical image to delineate various tissues and structures, a\ntask complicated by the significant variations in size, shape, and density of\nthese features. Convolutional neural networks (CNNs) have traditionally been\nused for this task but have limitations in capturing long-range dependencies.\nTransformers, equipped with self-attention mechanisms, aim to address this\nproblem. However, in medical image segmentation it is beneficial to merge both\nlocal and global features to effectively integrate feature maps across various\nscales, capturing both detailed features and broader semantic elements for\ndealing with variations in structures. In this paper, we introduce MSA$^2$Net,\na new deep segmentation framework featuring an expedient design of\nskip-connections. These connections facilitate feature fusion by dynamically\nweighting and combining coarse-grained encoder features with fine-grained\ndecoder feature maps. Specifically, we propose a Multi-Scale Adaptive Spatial\nAttention Gate (MASAG), which dynamically adjusts the receptive field (Local\nand Global contextual information) to ensure that spatially relevant features\nare selectively highlighted while minimizing background distractions. Extensive\nevaluations involving dermatology, and radiological datasets demonstrate that\nour MSA$^2$Net outperforms state-of-the-art (SOTA) works or matches their\nperformance. The source code is publicly available at\nhttps://github.com/xmindflow/MSA-2Net.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}