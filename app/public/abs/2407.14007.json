{"id":"2407.14007","title":"Multi-modal Relation Distillation for Unified 3D Representation Learning","authors":"Huiqun Wang, Yiping Bao, Panwang Pan, Zeming Li, Xiao Liu, Ruijie\n  Yang, Di Huang","authorsParsed":[["Wang","Huiqun",""],["Bao","Yiping",""],["Pan","Panwang",""],["Li","Zeming",""],["Liu","Xiao",""],["Yang","Ruijie",""],["Huang","Di",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 03:43:48 GMT"},{"version":"v2","created":"Wed, 18 Sep 2024 06:39:50 GMT"}],"updateDate":"2024-09-19","timestamp":1721360628000,"abstract":"  Recent advancements in multi-modal pre-training for 3D point clouds have\ndemonstrated promising results by aligning heterogeneous features across 3D\nshapes and their corresponding 2D images and language descriptions. However,\ncurrent straightforward solutions often overlook intricate structural relations\namong samples, potentially limiting the full capabilities of multi-modal\nlearning. To address this issue, we introduce Multi-modal Relation Distillation\n(MRD), a tri-modal pre-training framework, which is designed to effectively\ndistill reputable large Vision-Language Models (VLM) into 3D backbones. MRD\naims to capture both intra-relations within each modality as well as\ncross-relations between different modalities and produce more discriminative 3D\nshape representations. Notably, MRD achieves significant improvements in\ndownstream zero-shot classification tasks and cross-modality retrieval tasks,\ndelivering new state-of-the-art performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZBBDaWuKVo7a9lJXPm0MLTFOvv7iJpgmDXFkvOSj174","pdfSize":"2453795"}
