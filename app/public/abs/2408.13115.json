{"id":"2408.13115","title":"Convergence of Unadjusted Langevin in High Dimensions: Delocalization of\n  Bias","authors":"Yifan Chen, Xiaoou Cheng, Jonathan Niles-Weed, Jonathan Weare","authorsParsed":[["Chen","Yifan",""],["Cheng","Xiaoou",""],["Niles-Weed","Jonathan",""],["Weare","Jonathan",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 01:24:54 GMT"}],"updateDate":"2024-08-26","timestamp":1724117094000,"abstract":"  The unadjusted Langevin algorithm is commonly used to sample probability\ndistributions in extremely high-dimensional settings. However, existing\nanalyses of the algorithm for strongly log-concave distributions suggest that,\nas the dimension $d$ of the problem increases, the number of iterations\nrequired to ensure convergence within a desired error in the $W_2$ metric\nscales in proportion to $d$ or $\\sqrt{d}$. In this paper, we argue that,\ndespite this poor scaling of the $W_2$ error for the full set of variables, the\nbehavior for a small number of variables can be significantly better: a number\nof iterations proportional to $K$, up to logarithmic terms in $d$, often\nsuffices for the algorithm to converge to within a desired $W_2$ error for all\n$K$-marginals. We refer to this effect as delocalization of bias. We show that\nthe delocalization effect does not hold universally and prove its validity for\nGaussian distributions and strongly log-concave distributions with certain\nsparse interactions. Our analysis relies on a novel $W_{2,\\ell^\\infty}$ metric\nto measure convergence. A key technical challenge we address is the lack of a\none-step contraction property in this metric. Finally, we use asymptotic\narguments to explore potential generalizations of the delocalization effect\nbeyond the Gaussian and sparse interactions setting.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Probability","Statistics/Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}