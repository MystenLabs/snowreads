{"id":"2407.11699","title":"Relation DETR: Exploring Explicit Position Relation Prior for Object\n  Detection","authors":"Xiuquan Hou, Meiqin Liu, Senlin Zhang, Ping Wei, Badong Chen, Xuguang\n  Lan","authorsParsed":[["Hou","Xiuquan",""],["Liu","Meiqin",""],["Zhang","Senlin",""],["Wei","Ping",""],["Chen","Badong",""],["Lan","Xuguang",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 13:17:07 GMT"}],"updateDate":"2024-07-17","timestamp":1721135827000,"abstract":"  This paper presents a general scheme for enhancing the convergence and\nperformance of DETR (DEtection TRansformer). We investigate the slow\nconvergence problem in transformers from a new perspective, suggesting that it\narises from the self-attention that introduces no structural bias over inputs.\nTo address this issue, we explore incorporating position relation prior as\nattention bias to augment object detection, following the verification of its\nstatistical significance using a proposed quantitative macroscopic correlation\n(MC) metric. Our approach, termed Relation-DETR, introduces an encoder to\nconstruct position relation embeddings for progressive attention refinement,\nwhich further extends the traditional streaming pipeline of DETR into a\ncontrastive relation pipeline to address the conflicts between non-duplicate\npredictions and positive supervision. Extensive experiments on both generic and\ntask-specific datasets demonstrate the effectiveness of our approach. Under the\nsame configurations, Relation-DETR achieves a significant improvement (+2.0% AP\ncompared to DINO), state-of-the-art performance (51.7% AP for 1x and 52.1% AP\nfor 2x settings), and a remarkably faster convergence speed (over 40% AP with\nonly 2 training epochs) than existing DETR detectors on COCO val2017. Moreover,\nthe proposed relation encoder serves as a universal plug-in-and-play component,\nbringing clear improvements for theoretically any DETR-like methods.\nFurthermore, we introduce a class-agnostic detection dataset, SA-Det-100k. The\nexperimental results on the dataset illustrate that the proposed explicit\nposition relation achieves a clear improvement of 1.3% AP, highlighting its\npotential towards universal object detection. The code and dataset are\navailable at https://github.com/xiuqhou/Relation-DETR.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}