{"id":"2408.13919","title":"Quantum Multimodal Contrastive Learning Framework","authors":"Chi-Sheng Chen, Aidan Hung-Wen Tsai, Sheng-Chieh Huang","authorsParsed":[["Chen","Chi-Sheng",""],["Tsai","Aidan Hung-Wen",""],["Huang","Sheng-Chieh",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 19:08:43 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 05:42:43 GMT"},{"version":"v3","created":"Fri, 6 Sep 2024 16:16:22 GMT"}],"updateDate":"2024-09-09","timestamp":1724612923000,"abstract":"  In this paper, we propose a novel framework for multimodal contrastive\nlearning utilizing a quantum encoder to integrate EEG (electroencephalogram)\nand image data. This groundbreaking attempt explores the integration of quantum\nencoders within the traditional multimodal learning framework. By leveraging\nthe unique properties of quantum computing, our method enhances the\nrepresentation learning capabilities, providing a robust framework for\nanalyzing time series and visual information concurrently. We demonstrate that\nthe quantum encoder effectively captures intricate patterns within EEG signals\nand image features, facilitating improved contrastive learning across\nmodalities. This work opens new avenues for integrating quantum computing with\nmultimodal data analysis, particularly in applications requiring simultaneous\ninterpretation of temporal and visual data.\n","subjects":["Physics/Quantum Physics","Quantitative Biology/Neurons and Cognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}