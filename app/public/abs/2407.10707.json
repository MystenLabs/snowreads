{"id":"2407.10707","title":"Interactive Rendering of Relightable and Animatable Gaussian Avatars","authors":"Youyi Zhan, Tianjia Shao, He Wang, Yin Yang, Kun Zhou","authorsParsed":[["Zhan","Youyi",""],["Shao","Tianjia",""],["Wang","He",""],["Yang","Yin",""],["Zhou","Kun",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 13:25:07 GMT"}],"updateDate":"2024-07-16","timestamp":1721049907000,"abstract":"  Creating relightable and animatable avatars from multi-view or monocular\nvideos is a challenging task for digital human creation and virtual reality\napplications. Previous methods rely on neural radiance fields or ray tracing,\nresulting in slow training and rendering processes. By utilizing Gaussian\nSplatting, we propose a simple and efficient method to decouple body materials\nand lighting from sparse-view or monocular avatar videos, so that the avatar\ncan be rendered simultaneously under novel viewpoints, poses, and lightings at\ninteractive frame rates (6.9 fps). Specifically, we first obtain the canonical\nbody mesh using a signed distance function and assign attributes to each mesh\nvertex. The Gaussians in the canonical space then interpolate from nearby body\nmesh vertices to obtain the attributes. We subsequently deform the Gaussians to\nthe posed space using forward skinning, and combine the learnable environment\nlight with the Gaussian attributes for shading computation. To achieve fast\nshadow modeling, we rasterize the posed body mesh from dense viewpoints to\nobtain the visibility. Our approach is not only simple but also fast enough to\nallow interactive rendering of avatar animation under environmental light\nchanges. Experiments demonstrate that, compared to previous works, our method\ncan render higher quality results at a faster speed on both synthetic and real\ndatasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}