{"id":"2407.06443","title":"Exposing Privacy Gaps: Membership Inference Attack on Preference Data\n  for LLM Alignment","authors":"Qizhang Feng, Siva Rajesh Kasa, Hyokun Yun, Choon Hui Teo, Sravan Babu\n  Bodapati","authorsParsed":[["Feng","Qizhang",""],["Kasa","Siva Rajesh",""],["Yun","Hyokun",""],["Teo","Choon Hui",""],["Bodapati","Sravan Babu",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 22:53:23 GMT"}],"updateDate":"2024-07-10","timestamp":1720479203000,"abstract":"  Large Language Models (LLMs) have seen widespread adoption due to their\nremarkable natural language capabilities. However, when deploying them in\nreal-world settings, it is important to align LLMs to generate texts according\nto acceptable human standards. Methods such as Proximal Policy Optimization\n(PPO) and Direct Preference Optimization (DPO) have made significant progress\nin refining LLMs using human preference data. However, the privacy concerns\ninherent in utilizing such preference data have yet to be adequately studied.\nIn this paper, we investigate the vulnerability of LLMs aligned using human\npreference datasets to membership inference attacks (MIAs), highlighting the\nshortcomings of previous MIA approaches with respect to preference data. Our\nstudy has two main contributions: first, we introduce a novel reference-based\nattack framework specifically for analyzing preference data called PREMIA\n(\\uline{Pre}ference data \\uline{MIA}); second, we provide empirical evidence\nthat DPO models are more vulnerable to MIA compared to PPO models. Our findings\nhighlight gaps in current privacy-preserving practices for LLM alignment.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}