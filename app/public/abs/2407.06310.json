{"id":"2407.06310","title":"Homogeneous Speaker Features for On-the-Fly Dysarthric and Elderly\n  Speaker Adaptation","authors":"Mengzhe Geng, Xurong Xie, Jiajun Deng, Zengrui Jin, Guinan Li, Tianzi\n  Wang, Shujie Hu, Zhaoqing Li, Helen Meng, Xunying Liu","authorsParsed":[["Geng","Mengzhe",""],["Xie","Xurong",""],["Deng","Jiajun",""],["Jin","Zengrui",""],["Li","Guinan",""],["Wang","Tianzi",""],["Hu","Shujie",""],["Li","Zhaoqing",""],["Meng","Helen",""],["Liu","Xunying",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 18:20:24 GMT"}],"updateDate":"2024-07-10","timestamp":1720462824000,"abstract":"  The application of data-intensive automatic speech recognition (ASR)\ntechnologies to dysarthric and elderly adult speech is confronted by their\nmismatch against healthy and nonaged voices, data scarcity and large\nspeaker-level variability. To this end, this paper proposes two novel\ndata-efficient methods to learn homogeneous dysarthric and elderly\nspeaker-level features for rapid, on-the-fly test-time adaptation of DNN/TDNN\nand Conformer ASR models. These include: 1) speaker-level variance-regularized\nspectral basis embedding (VR-SBE) features that exploit a special\nregularization term to enforce homogeneity of speaker features in adaptation;\nand 2) feature-based learning hidden unit contributions (f-LHUC) transforms\nthat are conditioned on VR-SBE features. Experiments are conducted on four\ntasks across two languages: the English UASpeech and TORGO dysarthric speech\ndatasets, the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly\nspeech corpora. The proposed on-the-fly speaker adaptation techniques\nconsistently outperform baseline iVector and xVector adaptation by\nstatistically significant word or character error rate reductions up to 5.32%\nabsolute (18.57% relative) and batch-mode LHUC speaker adaptation by 2.24%\nabsolute (9.20% relative), while operating with real-time factors speeding up\nto 33.6 times against xVectors during adaptation. The efficacy of the proposed\nadaptation techniques is demonstrated in a comparison against current ASR\ntechnologies including SSL pre-trained systems on UASpeech, where our best\nsystem produces a state-of-the-art WER of 23.33%. Analyses show VR-SBE features\nand f-LHUC transforms are insensitive to speaker-level data quantity in\ntesttime adaptation. T-SNE visualization reveals they have stronger\nspeaker-level homogeneity than baseline iVectors, xVectors and batch-mode LHUC\ntransforms.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}