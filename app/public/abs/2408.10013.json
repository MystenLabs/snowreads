{"id":"2408.10013","title":"TBA: Faster Large Language Model Training Using SSD-Based Activation\n  Offloading","authors":"Kun Wu, Jeongmin Brian Park, Xiaofan Zhang, Mert Hidayeto\\u{g}lu,\n  Vikram Sharma Mailthody, Sitao Huang, Steven Sam Lumetta, Wen-mei Hwu","authorsParsed":[["Wu","Kun",""],["Park","Jeongmin Brian",""],["Zhang","Xiaofan",""],["HidayetoÄŸlu","Mert",""],["Mailthody","Vikram Sharma",""],["Huang","Sitao",""],["Lumetta","Steven Sam",""],["Hwu","Wen-mei",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 14:09:48 GMT"}],"updateDate":"2024-08-20","timestamp":1724076588000,"abstract":"  The growth rate of the GPU memory capacity has not been able to keep up with\nthat of the size of large language models (LLMs), hindering the model training\nprocess. In particular, activations -- the intermediate tensors produced during\nforward propagation and reused in backward propagation -- dominate the GPU\nmemory use. To address this challenge, we propose TBA to efficiently offload\nactivations to high-capacity NVMe SSDs. This approach reduces GPU memory usage\nwithout impacting performance by adaptively overlapping data transfers with\ncomputation. TBA is compatible with popular deep learning frameworks like\nPyTorch, Megatron, and DeepSpeed, and it employs techniques such as tensor\ndeduplication, forwarding, and adaptive offloading to further enhance\nefficiency. We conduct extensive experiments on GPT, BERT, and T5. Results\ndemonstrate that TBA effectively reduces 47% of the activation peak memory\nusage. At the same time, TBA perfectly overlaps the I/O with the computation\nand incurs negligible performance overhead. We introduce the\nrecompute-offload-keep (ROK) curve to compare the TBA offloading with other two\ntensor placement strategies, keeping activations in memory and layerwise full\nrecomputation. We find that TBA achieves better memory savings than layerwise\nfull recomputation while retaining the performance of keeping the activations\nin memory.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}