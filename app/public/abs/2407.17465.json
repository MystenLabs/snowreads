{"id":"2407.17465","title":"u-$\\mu$P: The Unit-Scaled Maximal Update Parametrization","authors":"Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke\n  Y. Prince, Bj\\\"orn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi,\n  Samuel Weinbach, Douglas Orr","authorsParsed":[["Blake","Charlie",""],["Eichenberg","Constantin",""],["Dean","Josef",""],["Balles","Lukas",""],["Prince","Luke Y.",""],["Deiseroth","Bj√∂rn",""],["Cruz-Salinas","Andres Felipe",""],["Luschi","Carlo",""],["Weinbach","Samuel",""],["Orr","Douglas",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:58:42 GMT"}],"updateDate":"2024-07-25","timestamp":1721843922000,"abstract":"  The Maximal Update Parametrization ($\\mu$P) aims to make the optimal\nhyperparameters (HPs) of a model independent of its size, allowing them to be\nswept using a cheap proxy model rather than the full-size target model. We\npresent a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with\nUnit Scaling, a method for designing models that makes them easy to train in\nlow-precision. The two techniques have a natural affinity: $\\mu$P ensures that\nthe scale of activations is independent of model size, and Unit Scaling ensures\nthat activations, weights and gradients begin training with a scale of one.\nThis synthesis opens the door to a simpler scheme, whose default values are\nnear-optimal. This in turn facilitates a more efficient sweeping strategy, with\nu-$\\mu$P models reaching a lower loss than comparable $\\mu$P models and working\nout-of-the-box in FP8.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}