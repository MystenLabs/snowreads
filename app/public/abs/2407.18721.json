{"id":"2407.18721","title":"Ensemble Kalman inversion approximate Bayesian computation","authors":"Richard G Everitt","authorsParsed":[["Everitt","Richard G",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 13:14:28 GMT"}],"updateDate":"2024-07-29","timestamp":1721999668000,"abstract":"  Approximate Bayesian computation (ABC) is the most popular approach to\ninferring parameters in the case where the data model is specified in the form\nof a simulator. It is not possible to directly implement standard Monte Carlo\nmethods for inference in such a model, due to the likelihood not being\navailable to evaluate pointwise. The main idea of ABC is to perform inference\non an alternative model with an approximate likelihood (the ABC likelihood),\nestimated at each iteration from points simulated from the data model. The\ncentral challenge of ABC is then to trade-off bias (introduced by approximating\nthe model) with the variance introduced by estimating the ABC likelihood.\nStabilising the variance of the ABC likelihood requires a computational cost\nthat is exponential in the dimension of the data, thus the most common approach\nto reducing variance is to perform inference conditional on summary statistics.\nIn this paper we introduce a new approach to estimating the ABC likelihood:\nusing iterative ensemble Kalman inversion (IEnKI) (Iglesias, 2016; Iglesias et\nal., 2018). We first introduce new estimators of the marginal likelihood in the\ncase of a Gaussian data model using the IEnKI output, then show how this may be\nused in ABC. Performance is illustrated on the Lotka-Volterra model, where we\nobserve substantial improvements over standard ABC and other commonly-used\napproaches.\n","subjects":["Statistics/Methodology","Statistics/Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}