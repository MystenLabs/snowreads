{"id":"2407.09873","title":"Resource Management for Low-latency Cooperative Fine-tuning of\n  Foundation Models at the Network Edge","authors":"Hai Wu, Xu Chen, and Kaibin Huang","authorsParsed":[["Wu","Hai",""],["Chen","Xu",""],["Huang","Kaibin",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 12:47:14 GMT"}],"updateDate":"2024-07-16","timestamp":1720874834000,"abstract":"  The emergence of large-scale foundation models (FoMo's) that can perform\nhuman-like intelligence motivates their deployment at the network edge for\ndevices to access state-of-the-art artificial intelligence. For better user\nexperiences, the pre-trained FoMo's need to be adapted to specialized\ndownstream tasks through fine-tuning techniques. To transcend a single device's\nmemory and computation limitations, we advocate multi-device cooperation within\nthe device-edge cooperative fine-tuning (DEFT) paradigm, where edge devices\ncooperate to simultaneously optimize different parts of fine-tuning parameters\nwithin a FoMo. However, the parameter blocks reside at different depths within\na FoMo architecture, leading to varied computation latency-and-memory cost due\nto gradient backpropagation-based calculations. The heterogeneous on-device\ncomputation and memory capacities and channel conditions necessitate an\nintegrated communication-and-computation allocation of local computation loads\nand communication resources to achieve low-latency (LoLa) DEFT. To this end, we\nconsider the depth-ware DEFT block allocation problem. The involved optimal\nblock-device matching is tackled by the proposed low-complexity\nCutting-RecoUNting-CHecking (CRUNCH) algorithm, which is designed by exploiting\nthe monotone-increasing property between block depth and computation\nlatency-and-memory cost. Next, the joint bandwidth-and-block allocation makes\nthe problem more sophisticated. We observe a splittable Lagrangian expression\nthrough the transformation and analysis of the original problem, where the\nvariables indicating device involvement are introduced. Then, the dual ascent\nmethod is employed to tackle this problem iteratively. Through extensive\nexperiments conducted on the GLUE benchmark, our results demonstrate\nsignificant latency reduction achievable by LoLa DEFT for fine-tuning a RoBERTa\nmodel.\n","subjects":["Computing Research Repository/Information Theory","Computing Research Repository/Artificial Intelligence","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}