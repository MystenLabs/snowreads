{"id":"2407.04207","title":"Elevating All Zero-Shot Sketch-Based Image Retrieval Through Multimodal\n  Prompt Learning","authors":"Mainak Singha, Ankit Jha, Divyam Gupta, Pranav Singla, Biplab Banerjee","authorsParsed":[["Singha","Mainak",""],["Jha","Ankit",""],["Gupta","Divyam",""],["Singla","Pranav",""],["Banerjee","Biplab",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 01:30:42 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 01:51:35 GMT"}],"updateDate":"2024-07-24","timestamp":1720143042000,"abstract":"  We address the challenges inherent in sketch-based image retrieval (SBIR)\nacross various settings, including zero-shot SBIR, generalized zero-shot SBIR,\nand fine-grained zero-shot SBIR, by leveraging the vision-language foundation\nmodel CLIP. While recent endeavors have employed CLIP to enhance SBIR, these\napproaches predominantly follow uni-modal prompt processing and overlook to\nexploit CLIP's integrated visual and textual capabilities fully. To bridge this\ngap, we introduce SpLIP, a novel multi-modal prompt learning scheme designed to\noperate effectively with frozen CLIP backbones. We diverge from existing\nmulti-modal prompting methods that treat visual and textual prompts\nindependently or integrate them in a limited fashion, leading to suboptimal\ngeneralization. SpLIP implements a bi-directional prompt-sharing strategy that\nenables mutual knowledge exchange between CLIP's visual and textual encoders,\nfostering a more cohesive and synergistic prompt processing mechanism that\nsignificantly reduces the semantic gap between the sketch and photo embeddings.\nIn addition to pioneering multi-modal prompt learning, we propose two\ninnovative strategies for further refining the embedding space. The first is an\nadaptive margin generation for the sketch-photo triplet loss, regulated by\nCLIP's class textual embeddings. The second introduces a novel task, termed\nconditional cross-modal jigsaw, aimed at enhancing fine-grained sketch-photo\nalignment by implicitly modeling sketches' viable patch arrangement using\nknowledge of unshuffled photos. Our comprehensive experimental evaluations\nacross multiple benchmarks demonstrate the superior performance of SpLIP in all\nthree SBIR scenarios. Project page: https://mainaksingha01.github.io/SpLIP/ .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}