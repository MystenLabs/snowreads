{"id":"2407.19789","title":"Interpreting Low-level Vision Models with Causal Effect Maps","authors":"Jinfan Hu, Jinjin Gu, Shiyao Yu, Fanghua Yu, Zheyuan Li, Zhiyuan You,\n  Chaochao Lu, Chao Dong","authorsParsed":[["Hu","Jinfan",""],["Gu","Jinjin",""],["Yu","Shiyao",""],["Yu","Fanghua",""],["Li","Zheyuan",""],["You","Zhiyuan",""],["Lu","Chaochao",""],["Dong","Chao",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 08:33:32 GMT"}],"updateDate":"2024-07-30","timestamp":1722242012000,"abstract":"  Deep neural networks have significantly improved the performance of low-level\nvision tasks but also increased the difficulty of interpretability. A deep\nunderstanding of deep models is beneficial for both network design and\npractical reliability. To take up this challenge, we introduce causality theory\nto interpret low-level vision models and propose a model-/task-agnostic method\ncalled Causal Effect Map (CEM). With CEM, we can visualize and quantify the\ninput-output relationships on either positive or negative effects. After\nanalyzing various low-level vision tasks with CEM, we have reached several\ninteresting insights, such as: (1) Using more information of input images\n(e.g., larger receptive field) does NOT always yield positive outcomes. (2)\nAttempting to incorporate mechanisms with a global receptive field (e.g.,\nchannel attention) into image denoising may prove futile. (3) Integrating\nmultiple tasks to train a general model could encourage the network to\nprioritize local information over global context. Based on the causal effect\ntheory, the proposed diagnostic tool can refresh our common knowledge and bring\na deeper understanding of low-level vision models. Codes are available at\nhttps://github.com/J-FHu/CEM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}