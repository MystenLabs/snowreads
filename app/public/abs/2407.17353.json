{"id":"2407.17353","title":"Scalify: scale propagation for efficient low-precision LLM training","authors":"Paul Balan\\c{c}a, Sam Hosegood, Carlo Luschi, Andrew Fitzgibbon","authorsParsed":[["Balan√ßa","Paul",""],["Hosegood","Sam",""],["Luschi","Carlo",""],["Fitzgibbon","Andrew",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 15:26:01 GMT"}],"updateDate":"2024-07-25","timestamp":1721834761000,"abstract":"  Low-precision formats such as float8 have been introduced in machine learning\naccelerated hardware to improve computational efficiency for large language\nmodels training and inference. Nevertheless, adoption by the ML community has\nbeen slowed down by the complex, and sometimes brittle, techniques required to\nmatch higher precision training accuracy. In this work, we present Scalify, a\nend-to-end scale propagation paradigm for computational graphs, generalizing\nand formalizing existing tensor scaling methods. Experiment results show that\nScalify supports out-of-the-box float8 matrix multiplication and gradients\nrepresentation, as well as float16 optimizer state storage. Our JAX\nimplementation of Scalify is open-sourced at\nhttps://github.com/graphcore-research/jax-scalify\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"AxY1R0F18ZDccNmDxfWcsf-vYIEWWKS1mwyJelmLVQA","pdfSize":"445949"}
