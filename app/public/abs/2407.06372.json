{"id":"2407.06372","title":"Non-Robust Features are Not Always Useful in One-Class Classification","authors":"Matthew Lau, Haoran Wang, Alec Helbling, Matthew Hul, ShengYun Peng,\n  Martin Andreoni, Willian T. Lunardi, Wenke Lee","authorsParsed":[["Lau","Matthew",""],["Wang","Haoran",""],["Helbling","Alec",""],["Hul","Matthew",""],["Peng","ShengYun",""],["Andreoni","Martin",""],["Lunardi","Willian T.",""],["Lee","Wenke",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 20:32:19 GMT"}],"updateDate":"2024-07-10","timestamp":1720470739000,"abstract":"  The robustness of machine learning models has been questioned by the\nexistence of adversarial examples. We examine the threat of adversarial\nexamples in practical applications that require lightweight models for\none-class classification. Building on Ilyas et al. (2019), we investigate the\nvulnerability of lightweight one-class classifiers to adversarial attacks and\npossible reasons for it. Our results show that lightweight one-class\nclassifiers learn features that are not robust (e.g. texture) under stronger\nattacks. However, unlike in multi-class classification (Ilyas et al., 2019),\nthese non-robust features are not always useful for the one-class task,\nsuggesting that learning these unpredictive and non-robust features is an\nunwanted consequence of training.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}