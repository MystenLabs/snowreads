{"id":"2407.13337","title":"Long-Term 3D Point Tracking By Cost Volume Fusion","authors":"Hung Nguyen, Chanho Kim, Rigved Naukarkar, Li Fuxin","authorsParsed":[["Nguyen","Hung",""],["Kim","Chanho",""],["Naukarkar","Rigved",""],["Fuxin","Li",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 09:34:47 GMT"}],"updateDate":"2024-07-19","timestamp":1721295287000,"abstract":"  Long-term point tracking is essential to understand non-rigid motion in the\nphysical world better. Deep learning approaches have recently been incorporated\ninto long-term point tracking, but most prior work predominantly functions in\n2D. Although these methods benefit from the well-established backbones and\nmatching frameworks, the motions they produce do not always make sense in the\n3D physical world. In this paper, we propose the first deep learning framework\nfor long-term point tracking in 3D that generalizes to new points and videos\nwithout requiring test-time fine-tuning. Our model contains a cost volume\nfusion module that effectively integrates multiple past appearances and motion\ninformation via a transformer architecture, significantly enhancing overall\ntracking performance. In terms of 3D tracking performance, our model\nsignificantly outperforms simple scene flow chaining and previous 2D point\ntracking methods, even if one uses ground truth depth and camera pose to\nbackproject 2D point tracks in a synthetic scenario.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}