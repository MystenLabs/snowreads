{"id":"2407.21078","title":"Convergence rates for the Adam optimizer","authors":"Steffen Dereich and Arnulf Jentzen","authorsParsed":[["Dereich","Steffen",""],["Jentzen","Arnulf",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 22:49:04 GMT"}],"updateDate":"2024-08-01","timestamp":1722293344000,"abstract":"  Stochastic gradient descent (SGD) optimization methods are nowadays the\nmethod of choice for the training of deep neural networks (DNNs) in artificial\nintelligence systems. In practically relevant training problems, usually not\nthe plain vanilla standard SGD method is the employed optimization scheme but\ninstead suitably accelerated and adaptive SGD optimization methods are applied.\nAs of today, maybe the most popular variant of such accelerated and adaptive\nSGD optimization methods is the famous Adam optimizer proposed by Kingma & Ba\nin 2014. Despite the popularity of the Adam optimizer in implementations, it\nremained an open problem of research to provide a convergence analysis for the\nAdam optimizer even in the situation of simple quadratic stochastic\noptimization problems where the objective function (the function one intends to\nminimize) is strongly convex. In this work we solve this problem by\nestablishing optimal convergence rates for the Adam optimizer for a large class\nof stochastic optimization problems, in particular, covering simple quadratic\nstochastic optimization problems. The key ingredient of our convergence\nanalysis is a new vector field function which we propose to refer to as the\nAdam vector field. This Adam vector field accurately describes the macroscopic\nbehaviour of the Adam optimization process but differs from the negative\ngradient of the objective function (the function we intend to minimize) of the\nconsidered stochastic optimization problem. In particular, our convergence\nanalysis reveals that the Adam optimizer does typically not converge to\ncritical points of the objective function (zeros of the gradient of the\nobjective function) of the considered optimization problem but converges with\nrates to zeros of this Adam vector field.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning","Mathematics/Probability","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RXF32-5imImg2eZPFn5YEQFT3EBnGFnWyekbbvekIFg","pdfSize":"536356"}
