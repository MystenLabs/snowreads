{"id":"2407.17453","title":"$VILA^2$: VILA Augmented VILA","authors":"Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun\n  Cho, Marco Pavone, Song Han and Hongxu Yin","authorsParsed":[["Fang","Yunhao",""],["Zhu","Ligeng",""],["Lu","Yao",""],["Wang","Yan",""],["Molchanov","Pavlo",""],["Cho","Jang Hyun",""],["Pavone","Marco",""],["Han","Song",""],["Yin","Hongxu",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:37:05 GMT"}],"updateDate":"2024-07-25","timestamp":1721842625000,"abstract":"  Visual language models (VLMs) have rapidly progressed, driven by the success\nof large language models (LLMs). While model architectures and training\ninfrastructures advance rapidly, data curation remains under-explored. When\ndata quantity and quality become a bottleneck, existing work either directly\ncrawls more raw data from the Internet that does not have a guarantee of data\nquality or distills from black-box commercial models (e.g., GPT-4V / Gemini)\ncausing the performance upper bounded by that model. In this work, we introduce\na novel approach that includes a self-augment step and a specialist-augment\nstep to iteratively improve data quality and model performance. In the\nself-augment step, a VLM recaptions its own pretraining data to enhance data\nquality, and then retrains from scratch using this refined dataset to improve\nmodel performance. This process can iterate for several rounds. Once\nself-augmentation saturates, we employ several specialist VLMs finetuned from\nthe self-augmented VLM with domain-specific expertise, to further infuse\nspecialist knowledge into the generalist VLM through task-oriented recaptioning\nand retraining. With the combined self-augmented and specialist-augmented\ntraining, we introduce $VILA^2$ (VILA-augmented-VILA), a VLM family that\nconsistently improves the accuracy on a wide range of tasks over prior art, and\nachieves new state-of-the-art results on MMMU leaderboard among open-sourced\nmodels.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ni4gw3F11Yc33hU6chRJisHQ81NRPZ63NiK26gQ9WBk","pdfSize":"21226459"}
