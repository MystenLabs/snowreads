{"id":"2407.20042","title":"When to Stop? Towards Efficient Code Generation in LLMs with Excess\n  Token Prevention","authors":"Lianghong Guo, Yanlin Wang, Ensheng Shi, Wanjun Zhong, Hongyu Zhang,\n  Jiachi Chen, Ruikai Zhang, Yuchi Ma, Zibin Zheng","authorsParsed":[["Guo","Lianghong",""],["Wang","Yanlin",""],["Shi","Ensheng",""],["Zhong","Wanjun",""],["Zhang","Hongyu",""],["Chen","Jiachi",""],["Zhang","Ruikai",""],["Ma","Yuchi",""],["Zheng","Zibin",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 14:27:08 GMT"}],"updateDate":"2024-07-30","timestamp":1722263228000,"abstract":"  Code generation aims to automatically generate code snippets that meet given\nnatural language requirements and plays an important role in software\ndevelopment. Although Code LLMs have shown excellent performance in this\ndomain, their long generation time poses a signification limitation in practice\nuse. In this paper, we first conduct an in-depth preliminary study with\ndifferent Code LLMs on code generation tasks and identify a significant\nefficiency issue, i.e., continual generation of excess tokens. It harms the\ndeveloper productivity and leads to huge computational wastes. To address it,\nwe introduce CodeFast, an inference acceleration approach for Code LLMs on code\ngeneration. The key idea of CodeFast is to terminate the inference process in\ntime when unnecessary excess tokens are detected. First, we propose an\nautomatic data construction framework to obtain training data. Then, we train a\nunified lightweight model GenGuard applicable to multiple programming languages\nto predict whether to terminate inference at the current step. Finally, we\nenhance Code LLM with GenGuard to accelerate its inference in code generation\ntasks. We conduct extensive experiments with CodeFast on five representative\nCode LLMs across four widely used code generation datasets. Experimental\nresults show that (1) CodeFast can significantly improve the inference speed of\nvarious Code LLMs in code generation, ranging form 34% to 452%, without\ncompromising the quality of generated code. (2) CodeFast is stable across\ndifferent parameter settings and can generalize to untrained datasets. Our code\nand data are available at https://github.com/DeepSoftwareAnalytics/CodeFast\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}