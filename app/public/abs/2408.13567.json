{"id":"2408.13567","title":"Hybrid Training for Enhanced Multi-task Generalization in Multi-agent\n  Reinforcement Learning","authors":"Mingliang Zhang, Sichang Su, Chengyang He, and Guillaume Sartoretti","authorsParsed":[["Zhang","Mingliang",""],["Su","Sichang",""],["He","Chengyang",""],["Sartoretti","Guillaume",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 12:37:03 GMT"}],"updateDate":"2024-08-27","timestamp":1724503023000,"abstract":"  In multi-agent reinforcement learning (MARL), achieving multi-task\ngeneralization to diverse agents and objectives presents significant\nchallenges. Existing online MARL algorithms primarily focus on single-task\nperformance, but their lack of multi-task generalization capabilities typically\nresults in substantial computational waste and limited real-life applicability.\nMeanwhile, existing offline multi-task MARL approaches are heavily dependent on\ndata quality, often resulting in poor performance on unseen tasks. In this\npaper, we introduce HyGen, a novel hybrid MARL framework, Hybrid Training for\nEnhanced Multi-Task Generalization, which integrates online and offline\nlearning to ensure both multi-task generalization and training efficiency.\nSpecifically, our framework extracts potential general skills from offline\nmulti-task datasets. We then train policies to select the optimal skills under\nthe centralized training and decentralized execution paradigm (CTDE). During\nthis stage, we utilize a replay buffer that integrates both offline data and\nonline interactions. We empirically demonstrate that our framework effectively\nextracts and refines general skills, yielding impressive generalization to\nunseen tasks. Comparative analyses on the StarCraft multi-agent challenge show\nthat HyGen outperforms a wide range of existing solely online and offline\nmethods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Multiagent Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}