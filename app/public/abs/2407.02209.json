{"id":"2407.02209","title":"Generative Monoculture in Large Language Models","authors":"Fan Wu, Emily Black, Varun Chandrasekaran","authorsParsed":[["Wu","Fan",""],["Black","Emily",""],["Chandrasekaran","Varun",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 12:17:07 GMT"}],"updateDate":"2024-07-03","timestamp":1719922627000,"abstract":"  We introduce {\\em generative monoculture}, a behavior observed in large\nlanguage models (LLMs) characterized by a significant narrowing of model output\ndiversity relative to available training data for a given task: for example,\ngenerating only positive book reviews for books with a mixed reception. While\nin some cases, generative monoculture enhances performance (e.g., LLMs more\noften produce efficient code), the dangers are exacerbated in others (e.g.,\nLLMs refuse to share diverse opinions). As LLMs are increasingly used in\nhigh-impact settings such as education and web search, careful maintenance of\nLLM output diversity is essential to ensure a variety of facts and perspectives\nare preserved over time. We experimentally demonstrate the prevalence of\ngenerative monoculture through analysis of book review and code generation\ntasks, and find that simple countermeasures such as altering sampling or\nprompting strategies are insufficient to mitigate the behavior. Moreover, our\nresults suggest that the root causes of generative monoculture are likely\nembedded within the LLM's alignment processes, suggesting a need for developing\nfine-tuning paradigms that preserve or promote diversity.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}