{"id":"2407.03964","title":"Improving Sample Efficiency of Reinforcement Learning with Background\n  Knowledge from Large Language Models","authors":"Fuxiang Zhang, Junyou Li, Yi-Chen Li, Zongzhang Zhang, Yang Yu, Deheng\n  Ye","authorsParsed":[["Zhang","Fuxiang",""],["Li","Junyou",""],["Li","Yi-Chen",""],["Zhang","Zongzhang",""],["Yu","Yang",""],["Ye","Deheng",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 14:33:47 GMT"}],"updateDate":"2024-07-08","timestamp":1720103627000,"abstract":"  Low sample efficiency is an enduring challenge of reinforcement learning\n(RL). With the advent of versatile large language models (LLMs), recent works\nimpart common-sense knowledge to accelerate policy learning for RL processes.\nHowever, we note that such guidance is often tailored for one specific task but\nloses generalizability. In this paper, we introduce a framework that harnesses\nLLMs to extract background knowledge of an environment, which contains general\nunderstandings of the entire environment, making various downstream RL tasks\nbenefit from one-time knowledge representation. We ground LLMs by feeding a few\npre-collected experiences and requesting them to delineate background knowledge\nof the environment. Afterward, we represent the output knowledge as potential\nfunctions for potential-based reward shaping, which has a good property for\nmaintaining policy optimality from task rewards. We instantiate three variants\nto prompt LLMs for background knowledge, including writing code, annotating\npreferences, and assigning goals. Our experiments show that these methods\nachieve significant sample efficiency improvements in a spectrum of downstream\ntasks from Minigrid and Crafter domains.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}