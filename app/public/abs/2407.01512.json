{"id":"2407.01512","title":"Open-TeleVision: Teleoperation with Immersive Active Visual Feedback","authors":"Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, Xiaolong Wang","authorsParsed":[["Cheng","Xuxin",""],["Li","Jialong",""],["Yang","Shiqi",""],["Yang","Ge",""],["Wang","Xiaolong",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:55:35 GMT"},{"version":"v2","created":"Mon, 8 Jul 2024 16:59:38 GMT"}],"updateDate":"2024-07-09","timestamp":1719856535000,"abstract":"  Teleoperation serves as a powerful method for collecting on-robot data\nessential for robot learning from demonstrations. The intuitiveness and ease of\nuse of the teleoperation system are crucial for ensuring high-quality, diverse,\nand scalable data. To achieve this, we propose an immersive teleoperation\nsystem Open-TeleVision that allows operators to actively perceive the robot's\nsurroundings in a stereoscopic manner. Additionally, the system mirrors the\noperator's arm and hand movements on the robot, creating an immersive\nexperience as if the operator's mind is transmitted to a robot embodiment. We\nvalidate the effectiveness of our system by collecting data and training\nimitation learning policies on four long-horizon, precise tasks (Can Sorting,\nCan Insertion, Folding, and Unloading) for 2 different humanoid robots and\ndeploy them in the real world. The system is open-sourced at:\nhttps://robot-tv.github.io/\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}