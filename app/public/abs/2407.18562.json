{"id":"2407.18562","title":"Learning Robust Named Entity Recognizers From Noisy Data With Retrieval\n  Augmentation","authors":"Chaoyi Ai, Yong Jiang, Shen Huang, Pengjun Xie, Kewei Tu","authorsParsed":[["Ai","Chaoyi",""],["Jiang","Yong",""],["Huang","Shen",""],["Xie","Pengjun",""],["Tu","Kewei",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 07:30:41 GMT"}],"updateDate":"2024-07-29","timestamp":1721979041000,"abstract":"  Named entity recognition (NER) models often struggle with noisy inputs, such\nas those with spelling mistakes or errors generated by Optical Character\nRecognition processes, and learning a robust NER model is challenging. Existing\nrobust NER models utilize both noisy text and its corresponding gold text for\ntraining, which is infeasible in many real-world applications in which gold\ntext is not available. In this paper, we consider a more realistic setting in\nwhich only noisy text and its NER labels are available. We propose to retrieve\nrelevant text of the noisy text from a knowledge corpus and use it to enhance\nthe representation of the original noisy input. We design three retrieval\nmethods: sparse retrieval based on lexicon similarity, dense retrieval based on\nsemantic similarity, and self-retrieval based on task-specific text. After\nretrieving relevant text, we concatenate the retrieved text with the original\nnoisy text and encode them with a transformer network, utilizing self-attention\nto enhance the contextual token representations of the noisy text using the\nretrieved text. We further employ a multi-view training framework that improves\nrobust NER without retrieving text during inference. Experiments show that our\nretrieval-augmented model achieves significant improvements in various noisy\nNER settings.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}