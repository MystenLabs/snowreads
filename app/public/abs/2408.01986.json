{"id":"2408.01986","title":"DeMansia: Mamba Never Forgets Any Tokens","authors":"Ricky Fang","authorsParsed":[["Fang","Ricky",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 10:54:36 GMT"}],"updateDate":"2024-08-06","timestamp":1722768876000,"abstract":"  This paper examines the mathematical foundations of transformer\narchitectures, highlighting their limitations particularly in handling long\nsequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM),\nand LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia\nintegrates state space models with token labeling techniques to enhance\nperformance in image classification tasks, efficiently addressing the\ncomputational challenges posed by traditional transformers. The architecture,\nbenchmark, and comparisons with contemporary models demonstrate DeMansia's\neffectiveness. The implementation of this paper is available on GitHub at\nhttps://github.com/catalpaaa/DeMansia\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}