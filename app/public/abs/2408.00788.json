{"id":"2408.00788","title":"SpikeVoice: High-Quality Text-to-Speech Via Efficient Spiking Neural\n  Network","authors":"Kexin Wang, Jiahong Zhang, Yong Ren, Man Yao, Di Shang, Bo Xu, Guoqi\n  Li","authorsParsed":[["Wang","Kexin",""],["Zhang","Jiahong",""],["Ren","Yong",""],["Yao","Man",""],["Shang","Di",""],["Xu","Bo",""],["Li","Guoqi",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 15:22:52 GMT"}],"updateDate":"2024-08-05","timestamp":1721229772000,"abstract":"  Brain-inspired Spiking Neural Network (SNN) has demonstrated its\neffectiveness and efficiency in vision, natural language, and speech\nunderstanding tasks, indicating their capacity to \"see\", \"listen\", and \"read\".\nIn this paper, we design \\textbf{SpikeVoice}, which performs high-quality\nText-To-Speech (TTS) via SNN, to explore the potential of SNN to \"speak\". A\nmajor obstacle to using SNN for such generative tasks lies in the demand for\nmodels to grasp long-term dependencies. The serial nature of spiking neurons,\nhowever, leads to the invisibility of information at future spiking time steps,\nlimiting SNN models to capture sequence dependencies solely within the same\ntime step. We term this phenomenon \"partial-time dependency\". To address this\nissue, we introduce Spiking Temporal-Sequential Attention STSA in the\nSpikeVoice. To the best of our knowledge, SpikeVoice is the first TTS work in\nthe SNN field. We perform experiments using four well-established datasets that\ncover both Chinese and English languages, encompassing scenarios with both\nsingle-speaker and multi-speaker configurations. The results demonstrate that\nSpikeVoice can achieve results comparable to Artificial Neural Networks (ANN)\nwith only 10.5 energy consumption of ANN.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}