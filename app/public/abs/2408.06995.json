{"id":"2408.06995","title":"Low-Bitwidth Floating Point Quantization for Efficient High-Quality\n  Diffusion Models","authors":"Cheng Chen, Christina Giannoula, Andreas Moshovos","authorsParsed":[["Chen","Cheng",""],["Giannoula","Christina",""],["Moshovos","Andreas",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 15:56:20 GMT"}],"updateDate":"2024-08-14","timestamp":1723564580000,"abstract":"  Diffusion models are emerging models that generate images by iteratively\ndenoising random Gaussian noise using deep neural networks. These models\ntypically exhibit high computational and memory demands, necessitating\neffective post-training quantization for high-performance inference. Recent\nworks propose low-bitwidth (e.g., 8-bit or 4-bit) quantization for diffusion\nmodels, however 4-bit integer quantization typically results in low-quality\nimages. We observe that on several widely used hardware platforms, there is\nlittle or no difference in compute capability between floating-point and\ninteger arithmetic operations of the same bitwidth (e.g., 8-bit or 4-bit).\nTherefore, we propose an effective floating-point quantization method for\ndiffusion models that provides better image quality compared to integer\nquantization methods. We employ a floating-point quantization method that was\neffective for other processing tasks, specifically computer vision and natural\nlanguage tasks, and tailor it for diffusion models by integrating weight\nrounding learning during the mapping of the full-precision values to the\nquantized values in the quantization process. We comprehensively study integer\nand floating-point quantization methods in state-of-the-art diffusion models.\nOur floating-point quantization method not only generates higher-quality images\nthan that of integer quantization methods, but also shows no noticeable\ndegradation compared to full-precision models (32-bit floating-point), when\nboth weights and activations are quantized to 8-bit floating-point values,\nwhile has minimal degradation with 4-bit weights and 8-bit activations.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}