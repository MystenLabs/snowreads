{"id":"2408.01283","title":"A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity\n  Recognition","authors":"Hiroki Matsutani, Radu Marculescu","authorsParsed":[["Matsutani","Hiroki",""],["Marculescu","Radu",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 14:09:39 GMT"}],"updateDate":"2024-08-05","timestamp":1722607779000,"abstract":"  In this paper, we introduce a low-cost and low-power tiny supervised\non-device learning (ODL) core that can address the distributional shift of\ninput data for human activity recognition. Although ODL for resource-limited\nedge devices has been studied recently, how exactly to provide the training\nlabels to these devices at runtime remains an open-issue. To address this\nproblem, we propose to combine an automatic data pruning with supervised ODL to\nreduce the number queries needed to acquire predicted labels from a nearby\nteacher device and thus save power consumption during model retraining. The\ndata pruning threshold is automatically tuned, eliminating a manual threshold\ntuning. As a tinyML solution at a few mW for the human activity recognition, we\ndesign a supervised ODL core that supports our automatic data pruning using a\n45nm CMOS process technology. We show that the required memory size for the\ncore is smaller than the same-shaped multilayer perceptron (MLP) and the power\nconsumption is only 3.39mW. Experiments using a human activity recognition\ndataset show that the proposed automatic data pruning reduces the communication\nvolume by 55.7% and power consumption accordingly with only 0.9% accuracy loss.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Hardware Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}