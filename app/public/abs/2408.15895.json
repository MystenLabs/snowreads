{"id":"2408.15895","title":"Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models","authors":"Sebastian Vallejo Vera and Hunter Driggers","authorsParsed":[["Vera","Sebastian Vallejo",""],["Driggers","Hunter",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 16:05:20 GMT"}],"updateDate":"2024-08-29","timestamp":1724861120000,"abstract":"  Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}