{"id":"2408.10852","title":"EELE: Exploring Efficient and Extensible LoRA Integration in Emotional\n  Text-to-Speech","authors":"Xin Qi, Ruibo Fu, Zhengqi Wen, Jianhua Tao, Shuchen Shi, Yi Lu,\n  Zhiyong Wang, Xiaopeng Wang, Yuankun Xie, Yukun Liu, Guanjun Li, Xuefei Liu,\n  Yongwei Li","authorsParsed":[["Qi","Xin",""],["Fu","Ruibo",""],["Wen","Zhengqi",""],["Tao","Jianhua",""],["Shi","Shuchen",""],["Lu","Yi",""],["Wang","Zhiyong",""],["Wang","Xiaopeng",""],["Xie","Yuankun",""],["Liu","Yukun",""],["Li","Guanjun",""],["Liu","Xuefei",""],["Li","Yongwei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 13:45:28 GMT"}],"updateDate":"2024-08-21","timestamp":1724161528000,"abstract":"  In the current era of Artificial Intelligence Generated Content (AIGC), a\nLow-Rank Adaptation (LoRA) method has emerged. It uses a plugin-based approach\nto learn new knowledge with lower parameter quantities and computational costs,\nand it can be plugged in and out based on the specific sub-tasks, offering high\nflexibility. However, the current application schemes primarily incorporate\nLoRA into the pre-introduced conditional parts of the speech models. This fixes\nthe position of LoRA, limiting the flexibility and scalability of its\napplication. Therefore, we propose the Exploring Efficient and Extensible LoRA\nIntegration in Emotional Text-to-Speech (EELE) method. Starting from a general\nneutral speech model, we do not pre-introduce emotional information but instead\nuse the LoRA plugin to design a flexible adaptive scheme that endows the model\nwith emotional generation capabilities. Specifically, we initially train the\nmodel using only neutral speech data. After training is complete, we insert\nLoRA into different modules and fine-tune the model with emotional speech data\nto find the optimal insertion scheme. Through experiments, we compare and test\nthe effects of inserting LoRA at different positions within the model and\nassess LoRA's ability to learn various emotions, effectively proving the\nvalidity of our method. Additionally, we explore the impact of the rank size of\nLoRA and the difference compared to directly fine-tuning the entire model.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}