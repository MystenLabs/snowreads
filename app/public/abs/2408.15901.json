{"id":"2408.15901","title":"Nexus: Specialization meets Adaptability for Efficiently Training\n  Mixture of Experts","authors":"Nikolas Gritsch and Qizhen Zhang and Acyr Locatelli and Sara Hooker\n  and Ahmet \\\"Ust\\\"un","authorsParsed":[["Gritsch","Nikolas",""],["Zhang","Qizhen",""],["Locatelli","Acyr",""],["Hooker","Sara",""],["Üstün","Ahmet",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 16:12:55 GMT"}],"updateDate":"2024-08-29","timestamp":1724861575000,"abstract":"  Efficiency, specialization, and adaptability to new data distributions are\nqualities that are hard to combine in current Large Language Models. The\nMixture of Experts (MoE) architecture has been the focus of significant\nresearch because its inherent conditional computation enables such desirable\nproperties. In this work, we focus on \"upcycling\" dense expert models into an\nMoE, aiming to improve specialization while also adding the ability to adapt to\nnew tasks easily. We introduce Nexus, an enhanced MoE architecture with\nadaptive routing where the model learns to project expert embeddings from\ndomain representations. This approach allows Nexus to flexibly add new experts\nafter the initial upcycling through separately trained dense models, without\nrequiring large-scale MoE training for unseen data domains. Our experiments\nshow that Nexus achieves a relative gain of up to 2.1% over the baseline for\ninitial upcycling, and a 18.8% relative gain for extending the MoE with a new\nexpert by using limited finetuning data. This flexibility of Nexus is crucial\nto enable an open-source ecosystem where every user continuously assembles\ntheir own MoE-mix according to their needs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}