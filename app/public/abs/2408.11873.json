{"id":"2408.11873","title":"Parameter-Efficient Transfer Learning under Federated Learning for\n  Automatic Speech Recognition","authors":"Xuan Kan, Yonghui Xiao, Tien-Ju Yang, Nanxin Chen, Rajiv Mathews","authorsParsed":[["Kan","Xuan",""],["Xiao","Yonghui",""],["Yang","Tien-Ju",""],["Chen","Nanxin",""],["Mathews","Rajiv",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 22:28:20 GMT"}],"updateDate":"2024-08-23","timestamp":1724106500000,"abstract":"  This work explores the challenge of enhancing Automatic Speech Recognition\n(ASR) model performance across various user-specific domains while preserving\nuser data privacy. We employ federated learning and parameter-efficient domain\nadaptation methods to solve the (1) massive data requirement of ASR models from\nuser-specific scenarios and (2) the substantial communication cost between\nservers and clients during federated learning. We demonstrate that when\nequipped with proper adapters, ASR models under federated tuning can achieve\nsimilar performance compared with centralized tuning ones, thus providing a\npotential direction for future privacy-preserved ASR services. Besides, we\ninvestigate the efficiency of different adapters and adapter incorporation\nstrategies under the federated learning setting.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}