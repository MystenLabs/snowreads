{"id":"2407.04751","title":"A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off\n  in Trustworthy Federated Learning","authors":"Xiaojin Zhang, Mingcong Xu, Wei Chen","authorsParsed":[["Zhang","Xiaojin",""],["Xu","Mingcong",""],["Chen","Wei",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 08:15:09 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 16:11:04 GMT"}],"updateDate":"2024-07-10","timestamp":1720167309000,"abstract":"  In this paper, we first give an introduction to the theoretical basis of the\nprivacy-utility equilibrium in federated learning based on Bayesian privacy\ndefinitions and total variation distance privacy definitions. We then present\nthe \\textit{Learn-to-Distort-Data} framework, which provides a principled\napproach to navigate the privacy-utility equilibrium by explicitly modeling the\ndistortion introduced by the privacy-preserving mechanism as a learnable\nvariable and optimizing it jointly with the model parameters. We demonstrate\nthe applicability of our framework to a variety of privacy-preserving\nmechanisms on the basis of data distortion and highlight its connections to\nrelated areas such as adversarial training, input robustness, and unlearnable\nexamples. These connections enable leveraging techniques from these areas to\ndesign effective algorithms for privacy-utility equilibrium in federated\nlearning under the \\textit{Learn-to-Distort-Data} framework.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}