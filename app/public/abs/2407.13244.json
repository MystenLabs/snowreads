{"id":"2407.13244","title":"PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining\n  Tasks","authors":"Alessandro Berti, Humam Kourani, Wil M.P. van der Aalst","authorsParsed":[["Berti","Alessandro",""],["Kourani","Humam",""],["van der Aalst","Wil M. P.",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 07:57:31 GMT"}],"updateDate":"2024-07-19","timestamp":1721289451000,"abstract":"  Large Language Models (LLMs) have the potential to semi-automate some process\nmining (PM) analyses. While commercial models are already adequate for many\nanalytics tasks, the competitive level of open-source LLMs in PM tasks is\nunknown. In this paper, we propose PM-LLM-Benchmark, the first comprehensive\nbenchmark for PM focusing on domain knowledge (process-mining-specific and\nprocess-specific) and on different implementation strategies. We focus also on\nthe challenges in creating such a benchmark, related to the public availability\nof the data and on evaluation biases by the LLMs. Overall, we observe that most\nof the considered LLMs can perform some process mining tasks at a satisfactory\nlevel, but tiny models that would run on edge devices are still inadequate. We\nalso conclude that while the proposed benchmark is useful for identifying LLMs\nthat are adequate for process mining tasks, further research is needed to\novercome the evaluation biases and perform a more thorough ranking of the\ncompetitive LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Databases"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zAFhCCnYGyilnwHc_Kk49EpJ4gq41XOJ42jCna_1p2w","pdfSize":"179628"}
