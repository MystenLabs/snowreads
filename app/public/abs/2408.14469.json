{"id":"2408.14469","title":"Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos","authors":"Qirui Chen, Shangzhe Di, Weidi Xie","authorsParsed":[["Chen","Qirui",""],["Di","Shangzhe",""],["Xie","Weidi",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 17:58:47 GMT"}],"updateDate":"2024-08-27","timestamp":1724695127000,"abstract":"  This paper considers the problem of Multi-Hop Video Question Answering\n(MH-VidQA) in long-form egocentric videos. This task not only requires to\nanswer visual questions, but also to localize multiple relevant time intervals\nwithin the video as visual evidences. We develop an automated pipeline to\ncreate multi-hop question-answering pairs with associated temporal evidence,\nenabling to construct a large-scale dataset for instruction-tuning. To monitor\nthe progress of this new task, we further curate a high-quality benchmark,\nMultiHop-EgoQA, with careful manual verification and refinement. Experimental\nresults reveal that existing multi-modal systems exhibit inadequate multi-hop\ngrounding and reasoning abilities, resulting in unsatisfactory performance. We\nthen propose a novel architecture, termed as Grounding Scattered Evidence with\nLarge Language Model (GeLM), that enhances multi-modal large language models\n(MLLMs) by incorporating a grounding module to retrieve temporal evidence from\nvideos using flexible grounding tokens. Trained on our visual instruction data,\nGeLM demonstrates improved multi-hop grounding and reasoning capabilities,\nsetting a new baseline for this challenging task. Furthermore, when trained on\nthird-person view videos, the same architecture also achieves state-of-the-art\nperformance on the single-hop VidQA benchmark, ActivityNet-RTL, demonstrating\nits effectiveness.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}