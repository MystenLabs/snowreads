{"id":"2407.04066","title":"EMPL: A novel Efficient Meta Prompt Learning Framework for Few-shot\n  Unsupervised Domain Adaptation","authors":"Wanqi Yang, Haoran Wang, Lei Wang, Ge Song, Yang Gao","authorsParsed":[["Yang","Wanqi",""],["Wang","Haoran",""],["Wang","Lei",""],["Song","Ge",""],["Gao","Yang",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 17:13:06 GMT"}],"updateDate":"2024-07-08","timestamp":1720113186000,"abstract":"  Few-shot unsupervised domain adaptation (FS-UDA) utilizes few-shot labeled\nsource domain data to realize effective classification in unlabeled target\ndomain. However, current FS-UDA methods are still suffer from two issues: 1)\nthe data from different domains can not be effectively aligned by few-shot\nlabeled data due to the large domain gaps, 2) it is unstable and time-consuming\nto generalize to new FS-UDA tasks.To address this issue, we put forward a novel\nEfficient Meta Prompt Learning Framework for FS-UDA. Within this framework, we\nuse pre-trained CLIP model as the feature learning base model. First, we design\ndomain-shared prompt learning vectors composed of virtual tokens, which mainly\nlearns the meta knowledge from a large number of meta tasks to mitigate domain\ngaps. Secondly, we also design a task-shared prompt learning network to\nadaptively learn specific prompt vectors for each task, which aims to realize\nfast adaptation and task generalization. Thirdly, we learn a task-specific\ncross-domain alignment projection and a task-specific classifier with\nclosed-form solutions for each meta task, which can efficiently adapt the model\nto new tasks in one step. The whole learning process is formulated as a bilevel\noptimization problem, and a good initialization of model parameters is learned\nthrough meta-learning. Extensive experimental study demonstrates the promising\nperformance of our framework on benchmark datasets. Our method has the large\nimprovement of at least 15.4% on 5-way 1-shot and 8.7% on 5-way 5-shot,\ncompared with the state-of-the-art methods. Also, the performance of our method\non all the test tasks is more stable than the other methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}