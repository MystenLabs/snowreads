{"id":"2407.10188","title":"Unexpected Benefits of Self-Modeling in Neural Systems","authors":"Vickram N. Premakumar, Michael Vaiana, Florin Pop, Judd Rosenblatt,\n  Diogo Schwerz de Lucena, Kirsten Ziman, and Michael S. A. Graziano","authorsParsed":[["Premakumar","Vickram N.",""],["Vaiana","Michael",""],["Pop","Florin",""],["Rosenblatt","Judd",""],["de Lucena","Diogo Schwerz",""],["Ziman","Kirsten",""],["Graziano","Michael S. A.",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 13:16:23 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 21:54:12 GMT"}],"updateDate":"2024-07-25","timestamp":1720962983000,"abstract":"  Self-models have been a topic of great interest for decades in studies of\nhuman cognition and more recently in machine learning. Yet what benefits do\nself-models confer? Here we show that when artificial networks learn to predict\ntheir internal states as an auxiliary task, they change in a fundamental way.\nTo better perform the self-model task, the network learns to make itself\nsimpler, more regularized, more parameter-efficient, and therefore more\namenable to being predictively modeled. To test the hypothesis of\nself-regularizing through self-modeling, we used a range of network\narchitectures performing three classification tasks across two modalities. In\nall cases, adding self-modeling caused a significant reduction in network\ncomplexity. The reduction was observed in two ways. First, the distribution of\nweights was narrower when self-modeling was present. Second, a measure of\nnetwork complexity, the real log canonical threshold (RLCT), was smaller when\nself-modeling was present. Not only were measures of complexity reduced, but\nthe reduction became more pronounced as greater training weight was placed on\nthe auxiliary task of self-modeling. These results strongly support the\nhypothesis that self-modeling is more than simply a network learning to predict\nitself. The learning has a restructuring effect, reducing complexity and\nincreasing parameter efficiency. This self-regularization may help explain some\nof the benefits of self-models reported in recent machine learning literature,\nas well as the adaptive value of self-models to biological systems. In\nparticular, these findings may shed light on the possible interaction between\nthe ability to model oneself and the ability to be more easily modeled by\nothers in a social or cooperative context.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FiOgua6uQVK-bp0_-KLnIYUw2LFMfxoQKoD97N0dmU0","pdfSize":"1490155"}
