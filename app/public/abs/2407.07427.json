{"id":"2407.07427","title":"Unified Embedding Alignment for Open-Vocabulary Video Instance\n  Segmentation","authors":"Hao Fang, Peng Wu, Yawei Li, Xinxin Zhang, and Xiankai Lu","authorsParsed":[["Fang","Hao",""],["Wu","Peng",""],["Li","Yawei",""],["Zhang","Xinxin",""],["Lu","Xiankai",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 07:30:51 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 02:49:45 GMT"}],"updateDate":"2024-07-15","timestamp":1720596651000,"abstract":"  Open-Vocabulary Video Instance Segmentation (VIS) is attracting increasing\nattention due to its ability to segment and track arbitrary objects. However,\nthe recent Open-Vocabulary VIS attempts obtained unsatisfactory results,\nespecially in terms of generalization ability of novel categories. We discover\nthat the domain gap between the VLM features (e.g., CLIP) and the instance\nqueries and the underutilization of temporal consistency are two central\ncauses. To mitigate these issues, we design and train a novel Open-Vocabulary\nVIS baseline called OVFormer. OVFormer utilizes a lightweight module for\nunified embedding alignment between query embeddings and CLIP image embeddings\nto remedy the domain gap. Unlike previous image-based training methods, we\nconduct video-based model training and deploy a semi-online inference scheme to\nfully mine the temporal consistency in the video. Without bells and whistles,\nOVFormer achieves 21.9 mAP with a ResNet-50 backbone on LV-VIS, exceeding the\nprevious state-of-the-art performance by 7.7. Extensive experiments on some\nClose-Vocabulary VIS datasets also demonstrate the strong zero-shot\ngeneralization ability of OVFormer (+ 7.6 mAP on YouTube-VIS 2019, + 3.9 mAP on\nOVIS). Code is available at https://github.com/fanghaook/OVFormer.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}