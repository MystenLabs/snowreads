{"id":"2407.14117","title":"Rethinking Visual Content Refinement in Low-Shot CLIP Adaptation","authors":"Jinda Lu, Shuo Wang, Yanbin Hao, Haifeng Liu, Xiang Wang, Meng Wang","authorsParsed":[["Lu","Jinda",""],["Wang","Shuo",""],["Hao","Yanbin",""],["Liu","Haifeng",""],["Wang","Xiang",""],["Wang","Meng",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 08:34:23 GMT"}],"updateDate":"2024-07-22","timestamp":1721378063000,"abstract":"  Recent adaptations can boost the low-shot capability of Contrastive\nVision-Language Pre-training (CLIP) by effectively facilitating knowledge\ntransfer. However, these adaptation methods are usually operated on the global\nview of an input image, and thus biased perception of partial local details of\nthe image. To solve this problem, we propose a Visual Content Refinement (VCR)\nbefore the adaptation calculation during the test stage. Specifically, we first\ndecompose the test image into different scales to shift the feature extractor's\nattention to the details of the image. Then, we select the image view with the\nmax prediction margin in each scale to filter out the noisy image views, where\nthe prediction margins are calculated from the pre-trained CLIP model. Finally,\nwe merge the content of the aforementioned selected image views based on their\nscales to construct a new robust representation. Thus, the merged content can\nbe directly used to help the adapter focus on both global and local parts\nwithout any extra training parameters. We apply our method to 3 popular\nlow-shot benchmark tasks with 13 datasets and achieve a significant improvement\nover state-of-the-art methods. For example, compared to the baseline\n(Tip-Adapter) on the few-shot classification task, our method achieves about\n2\\% average improvement for both training-free and training-need settings.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}