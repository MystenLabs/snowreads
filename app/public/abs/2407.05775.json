{"id":"2407.05775","title":"Structural Generalization in Autonomous Cyber Incident Response with\n  Message-Passing Neural Networks and Reinforcement Learning","authors":"Jakob Nyberg, Pontus Johnson","authorsParsed":[["Nyberg","Jakob",""],["Johnson","Pontus",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 09:34:22 GMT"}],"updateDate":"2024-07-09","timestamp":1720431262000,"abstract":"  We believe that agents for automated incident response based on machine\nlearning need to handle changes in network structure. Computer networks are\ndynamic, and can naturally change in structure over time. Retraining agents for\nsmall network changes costs time and energy. We attempt to address this issue\nwith an existing method of relational agent learning, where the relations\nbetween objects are assumed to remain consistent across problem instances. The\nstate of the computer network is represented as a relational graph and encoded\nthrough a message passing neural network. The message passing neural network\nand an agent policy using the encoding are optimized end-to-end using\nreinforcement learning. We evaluate the approach on the second instance of the\nCyber Autonomy Gym for Experimentation (CAGE~2), a cyber incident simulator\nthat simulates attacks on an enterprise network. We create variants of the\noriginal network with different numbers of hosts and agents are tested without\nadditional training on them. Our results show that agents using relational\ninformation are able to find solutions despite changes to the network, and can\nperform optimally in some instances. Agents using the default vector state\nrepresentation perform better, but need to be specially trained on each network\nvariant, demonstrating a trade-off between specialization and generalization.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}