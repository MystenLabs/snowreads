{"id":"2408.04590","title":"Learn To Learn More Precisely","authors":"Runxi Cheng, Yongxian Wei, Xianglong He, Wanyun Zhu, Songsong Huang,\n  Fei Richard Yu, Fei Ma, Chun Yuan","authorsParsed":[["Cheng","Runxi",""],["Wei","Yongxian",""],["He","Xianglong",""],["Zhu","Wanyun",""],["Huang","Songsong",""],["Yu","Fei Richard",""],["Ma","Fei",""],["Yuan","Chun",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 17:01:26 GMT"}],"updateDate":"2024-08-09","timestamp":1723136486000,"abstract":"  Meta-learning has been extensively applied in the domains of few-shot\nlearning and fast adaptation, achieving remarkable performance. While\nMeta-learning methods like Model-Agnostic Meta-Learning (MAML) and its variants\nprovide a good set of initial parameters for the model, the model still tends\nto learn shortcut features, which leads to poor generalization. In this paper,\nwe propose the formal conception of \"learn to learn more precisely\", which aims\nto make the model learn precise target knowledge from data and reduce the\neffect of noisy knowledge, such as background and noise. To achieve this\ntarget, we proposed a simple and effective meta-learning framework named Meta\nSelf-Distillation(MSD) to maximize the consistency of learned knowledge,\nenhancing the models' ability to learn precise target knowledge. In the inner\nloop, MSD uses different augmented views of the same support data to update the\nmodel respectively. Then in the outer loop, MSD utilizes the same query data to\noptimize the consistency of learned knowledge, enhancing the model's ability to\nlearn more precisely. Our experiment demonstrates that MSD exhibits remarkable\nperformance in few-shot classification tasks in both standard and augmented\nscenarios, effectively boosting the accuracy and consistency of knowledge\nlearned by the model.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uCiE29MpBJa4fAEQWOTJcULYxyRTW-mcD4CtaFEbnAQ","pdfSize":"6210827"}
