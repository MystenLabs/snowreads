{"id":"2408.14732","title":"OctFusion: Octree-based Diffusion Models for 3D Shape Generation","authors":"Bojun Xiong, Si-Tong Wei, Xin-Yang Zheng, Yan-Pei Cao, Zhouhui Lian,\n  Peng-Shuai Wang","authorsParsed":[["Xiong","Bojun",""],["Wei","Si-Tong",""],["Zheng","Xin-Yang",""],["Cao","Yan-Pei",""],["Lian","Zhouhui",""],["Wang","Peng-Shuai",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 01:55:40 GMT"}],"updateDate":"2024-08-28","timestamp":1724723740000,"abstract":"  Diffusion models have emerged as a popular method for 3D generation. However,\nit is still challenging for diffusion models to efficiently generate diverse\nand high-quality 3D shapes. In this paper, we introduce OctFusion, which can\ngenerate 3D shapes with arbitrary resolutions in 2.5 seconds on a single Nvidia\n4090 GPU, and the extracted meshes are guaranteed to be continuous and\nmanifold. The key components of OctFusion are the octree-based latent\nrepresentation and the accompanying diffusion models. The representation\ncombines the benefits of both implicit neural representations and explicit\nspatial octrees and is learned with an octree-based variational autoencoder.\nThe proposed diffusion model is a unified multi-scale U-Net that enables\nweights and computation sharing across different octree levels and avoids the\ncomplexity of widely used cascaded diffusion schemes. We verify the\neffectiveness of OctFusion on the ShapeNet and Objaverse datasets and achieve\nstate-of-the-art performances on shape generation tasks. We demonstrate that\nOctFusion is extendable and flexible by generating high-quality color fields\nfor textured mesh generation and high-quality 3D shapes conditioned on text\nprompts, sketches, or category labels. Our code and pre-trained models are\navailable at \\url{https://github.com/octree-nn/octfusion}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}