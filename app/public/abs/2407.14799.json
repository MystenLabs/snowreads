{"id":"2407.14799","title":"FairViT: Fair Vision Transformer via Adaptive Masking","authors":"Bowei Tian, Ruijie Du and Yanning Shen","authorsParsed":[["Tian","Bowei",""],["Du","Ruijie",""],["Shen","Yanning",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 08:10:37 GMT"}],"updateDate":"2024-07-23","timestamp":1721463037000,"abstract":"  Vision Transformer (ViT) has achieved excellent performance and demonstrated\nits promising potential in various computer vision tasks. The wide deployment\nof ViT in real-world tasks requires a thorough understanding of the societal\nimpact of the model. However, most ViT-based works do not take fairness into\naccount and it is unclear whether directly applying CNN-oriented debiased\nalgorithm to ViT is feasible. Moreover, previous works typically sacrifice\naccuracy for fairness. Therefore, we aim to develop an algorithm that improves\naccuracy without sacrificing fairness. In this paper, we propose FairViT, a\nnovel accurate and fair ViT framework. To this end, we introduce a novel\ndistance loss and deploy adaptive fairness-aware masks on attention layers\nupdating with model parameters. Experimental results show \\sys can achieve\naccuracy better than other alternatives, even with competitive computational\nefficiency. Furthermore, \\sys achieves appreciable fairness results.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computers and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}