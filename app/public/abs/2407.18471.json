{"id":"2407.18471","title":"Constructing the CORD-19 Vaccine Dataset","authors":"Manisha Singh, Divy Sharma, Alonso Ma, Bridget Tyree, Margaret\n  Mitchell","authorsParsed":[["Singh","Manisha",""],["Sharma","Divy",""],["Ma","Alonso",""],["Tyree","Bridget",""],["Mitchell","Margaret",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 02:44:55 GMT"}],"updateDate":"2024-07-29","timestamp":1721961895000,"abstract":"  We introduce new dataset 'CORD-19-Vaccination' to cater to scientists\nspecifically looking into COVID-19 vaccine-related research. This dataset is\nextracted from CORD-19 dataset [Wang et al., 2020] and augmented with new\ncolumns for language detail, author demography, keywords, and topic per paper.\nFacebook's fastText model is used to identify languages [Joulin et al., 2016].\nTo establish author demography (author affiliation, lab/institution location,\nand lab/institution country columns) we processed the JSON file for each paper\nand then further enhanced using Google's search API to determine country\nvalues. 'Yake' was used to extract keywords from the title, abstract, and body\nof each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to\nadd topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset,\nwe demonstrate a question-answering task like the one used in the CORD-19\nKaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential\nsentence classification was performed on each paper's abstract using the model\nfrom Dernoncourt et al. [2016]. We partially hand annotated the training\ndataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination'\ncontains 30k research papers and can be immensely valuable for NLP research\nsuch as text mining, information extraction, and question answering, specific\nto the domain of COVID-19 vaccine research.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}