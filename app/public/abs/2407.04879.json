{"id":"2407.04879","title":"All Neural Low-latency Directional Speech Extraction","authors":"Ashutosh Pandey, Sanha Lee, Juan Azcarreta, Daniel Wong, Buye Xu","authorsParsed":[["Pandey","Ashutosh",""],["Lee","Sanha",""],["Azcarreta","Juan",""],["Wong","Daniel",""],["Xu","Buye",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 22:17:11 GMT"}],"updateDate":"2024-07-09","timestamp":1720217831000,"abstract":"  We introduce a novel all neural model for low-latency directional speech\nextraction. The model uses direction of arrival (DOA) embeddings from a\npredefined spatial grid, which are transformed and fused into a recurrent\nneural network based speech extraction model. This process enables the model to\neffectively extract speech from a specified DOA. Unlike previous methods that\nrelied on hand-crafted directional features, the proposed model trains DOA\nembeddings from scratch using speech enhancement loss, making it suitable for\nlow-latency scenarios. Additionally, it operates at a high frame rate, taking\nin DOA with each input frame, which brings in the capability of quickly\nadapting to changing scene in highly dynamic real-world scenarios. We provide\nextensive evaluation to demonstrate the model's efficacy in directional speech\nextraction, robustness to DOA mismatch, and its capability to quickly adapt to\nabrupt changes in DOA.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}