{"id":"2407.21248","title":"Adaptive Pre-training Data Detection for Large Language Models via\n  Surprising Tokens","authors":"Anqi Zhang, Chaofeng Wu","authorsParsed":[["Zhang","Anqi",""],["Wu","Chaofeng",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 23:43:59 GMT"}],"updateDate":"2024-08-01","timestamp":1722383039000,"abstract":"  While large language models (LLMs) are extensively used, there are raising\nconcerns regarding privacy, security, and copyright due to their opaque\ntraining data, which brings the problem of detecting pre-training data on the\ntable. Current solutions to this problem leverage techniques explored in\nmachine learning privacy such as Membership Inference Attacks (MIAs), which\nheavily depend on LLMs' capability of verbatim memorization. However, this\nreliance presents challenges, especially given the vast amount of training data\nand the restricted number of effective training epochs. In this paper, we\npropose an adaptive pre-training data detection method which alleviates this\nreliance and effectively amplify the identification. Our method adaptively\nlocates \\textit{surprising tokens} of the input. A token is surprising to a LLM\nif the prediction on the token is \"certain but wrong\", which refers to low\nShannon entropy of the probability distribution and low probability of the\nground truth token at the same time. By using the prediction probability of\nsurprising tokens to measure \\textit{surprising}, the detection method is\nachieved based on the simple hypothesis that seeing seen data is less\nsurprising for the model compared with seeing unseen data. The method can be\napplied without any access to the the pre-training data corpus or additional\ntraining like reference models. Our approach exhibits a consistent enhancement\ncompared to existing methods in diverse experiments conducted on various\nbenchmarks and models, achieving a maximum improvement of 29.5\\%. We also\nintroduce a new benchmark Dolma-Book developed upon a novel framework, which\nemploys book data collected both before and after model training to provide\nfurther evaluation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"T6pXTamZG_D7XClZFOD-eRkuZvzFIv0IIVNzlFPcC_E","pdfSize":"1777547"}
