{"id":"2407.05000","title":"LoRA-GA: Low-Rank Adaptation with Gradient Approximation","authors":"Shaowen Wang, Linxi Yu, Jian Li","authorsParsed":[["Wang","Shaowen",""],["Yu","Linxi",""],["Li","Jian",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 08:37:21 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 07:32:23 GMT"}],"updateDate":"2024-07-17","timestamp":1720255041000,"abstract":"  Fine-tuning large-scale pretrained models is prohibitively expensive in terms\nof computational and memory costs. LoRA, as one of the most popular\nParameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective\nalternative by fine-tuning an auxiliary low-rank model that has significantly\nfewer parameters. Although LoRA reduces the computational and memory\nrequirements significantly at each iteration, extensive empirical evidence\nindicates that it converges at a considerably slower rate compared to full\nfine-tuning, ultimately leading to increased overall compute and often worse\ntest performance. In our paper, we perform an in-depth investigation of the\ninitialization method of LoRA and show that careful initialization (without any\nchange of the architecture and the training algorithm) can significantly\nenhance both efficiency and performance. In particular, we introduce a novel\ninitialization method, LoRA-GA (Low Rank Adaptation with Gradient\nApproximation), which aligns the gradients of low-rank matrix product with\nthose of full fine-tuning at the first step. Our extensive experiments\ndemonstrate that LoRA-GA achieves a convergence rate comparable to that of full\nfine-tuning (hence being significantly faster than vanilla LoRA as well as\nvarious recent improvements) while simultaneously attaining comparable or even\nbetter performance. For example, on the subset of the GLUE dataset with\nT5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as\nLlama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05%\non MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up\nto 2-4 times convergence speed improvement compared to vanilla LoRA, validating\nits effectiveness in accelerating convergence and enhancing model performance.\nCode is available at https://github.com/Outsider565/LoRA-GA.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}