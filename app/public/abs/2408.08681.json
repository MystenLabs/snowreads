{"id":"2408.08681","title":"A Mean Field Ansatz for Zero-Shot Weight Transfer","authors":"Xingyuan Chen, Wenwei Kuang, Lei Deng, Wei Han, Bo Bai, Goncalo dos\n  Reis","authorsParsed":[["Chen","Xingyuan",""],["Kuang","Wenwei",""],["Deng","Lei",""],["Han","Wei",""],["Bai","Bo",""],["Reis","Goncalo dos",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 11:53:52 GMT"}],"updateDate":"2024-08-19","timestamp":1723809232000,"abstract":"  The pre-training cost of large language models (LLMs) is prohibitive. One\ncutting-edge approach to reduce the cost is zero-shot weight transfer, also\nknown as model growth for some cases, which magically transfers the weights\ntrained in a small model to a large model. However, there are still some\ntheoretical mysteries behind the weight transfer. In this paper, inspired by\nprior applications of mean field theory to neural network dynamics, we\nintroduce a mean field ansatz to provide a theoretical explanation for weight\ntransfer. Specifically, we propose the row-column (RC) ansatz under the mean\nfield point of view, which describes the measure structure of the weights in\nthe neural network (NN) and admits a close measure dynamic. Thus, the weights\nof different sizes NN admit a common distribution under proper assumptions, and\nweight transfer methods can be viewed as sampling methods. We empirically\nvalidate the RC ansatz by exploring simple MLP examples and LLMs such as GPT-3\nand Llama-3.1. We show the mean-field point of view is adequate under suitable\nassumptions which can provide theoretical support for zero-shot weight\ntransfer.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis","Mathematics/Probability"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"SXSks5PLKrl6NWK8yvdZcww971kZMDfavdqEYJKHYMc","pdfSize":"3637011"}
