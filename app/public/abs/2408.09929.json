{"id":"2408.09929","title":"Data Augmentation of Contrastive Learning is Estimating\n  Positive-incentive Noise","authors":"Hongyuan Zhang, Yanchen Xu, Sida Huang, Xuelong Li","authorsParsed":[["Zhang","Hongyuan",""],["Xu","Yanchen",""],["Huang","Sida",""],["Li","Xuelong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 12:07:42 GMT"}],"updateDate":"2024-08-20","timestamp":1724069262000,"abstract":"  Inspired by the idea of Positive-incentive Noise (Pi-Noise or $\\pi$-Noise)\nthat aims at learning the reliable noise beneficial to tasks, we scientifically\ninvestigate the connection between contrastive learning and $\\pi$-noise in this\npaper. By converting the contrastive loss to an auxiliary Gaussian distribution\nto quantitatively measure the difficulty of the specific contrastive model\nunder the information theory framework, we properly define the task entropy,\nthe core concept of $\\pi$-noise, of contrastive learning. It is further proved\nthat the predefined data augmentation in the standard contrastive learning\nparadigm can be regarded as a kind of point estimation of $\\pi$-noise. Inspired\nby the theoretical study, a framework that develops a $\\pi$-noise generator to\nlearn the beneficial noise (instead of estimation) as data augmentations for\ncontrast is proposed. The designed framework can be applied to diverse types of\ndata and is also completely compatible with the existing contrastive models.\nFrom the visualization, we surprisingly find that the proposed method\nsuccessfully learns effective augmentations.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}