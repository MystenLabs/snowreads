{"id":"2407.05112","title":"Releasing Malevolence from Benevolence: The Menace of Benign Data on\n  Machine Unlearning","authors":"Binhao Ma, Tianhang Zheng, Hongsheng Hu, Di Wang, Shuo Wang, Zhongjie\n  Ba, Zhan Qin, Kui Ren","authorsParsed":[["Ma","Binhao",""],["Zheng","Tianhang",""],["Hu","Hongsheng",""],["Wang","Di",""],["Wang","Shuo",""],["Ba","Zhongjie",""],["Qin","Zhan",""],["Ren","Kui",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 15:42:28 GMT"}],"updateDate":"2024-07-09","timestamp":1720280548000,"abstract":"  Machine learning models trained on vast amounts of real or synthetic data\noften achieve outstanding predictive performance across various domains.\nHowever, this utility comes with increasing concerns about privacy, as the\ntraining data may include sensitive information. To address these concerns,\nmachine unlearning has been proposed to erase specific data samples from\nmodels. While some unlearning techniques efficiently remove data at low costs,\nrecent research highlights vulnerabilities where malicious users could request\nunlearning on manipulated data to compromise the model. Despite these attacks'\neffectiveness, perturbed data differs from original training data, failing hash\nverification. Existing attacks on machine unlearning also suffer from practical\nlimitations and require substantial additional knowledge and resources. To fill\nthe gaps in current unlearning attacks, we introduce the Unlearning Usability\nAttack. This model-agnostic, unlearning-agnostic, and budget-friendly attack\ndistills data distribution information into a small set of benign data. These\ndata are identified as benign by automatic poisoning detection tools due to\ntheir positive impact on model training. While benign for machine learning,\nunlearning these data significantly degrades model information. Our evaluation\ndemonstrates that unlearning this benign data, comprising no more than 1% of\nthe total training data, can reduce model accuracy by up to 50%. Furthermore,\nour findings show that well-prepared benign data poses challenges for recent\nunlearning techniques, as erasing these synthetic instances demands higher\nresources than regular data. These insights underscore the need for future\nresearch to reconsider \"data poisoning\" in the context of machine unlearning.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}