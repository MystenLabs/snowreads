{"id":"2407.07321","title":"RAG vs. Long Context: Examining Frontier Large Language Models for\n  Environmental Review Document Comprehension","authors":"Hung Phan, Anurag Acharya, Sarthak Chaturvedi, Shivam Sharma, Mike\n  Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai\n  Munikoti, Sameera Horawalavithana","authorsParsed":[["Phan","Hung",""],["Acharya","Anurag",""],["Chaturvedi","Sarthak",""],["Sharma","Shivam",""],["Parker","Mike",""],["Nally","Dan",""],["Jannesari","Ali",""],["Pazdernik","Karl",""],["Halappanavar","Mahantesh",""],["Munikoti","Sai",""],["Horawalavithana","Sameera",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 02:33:09 GMT"}],"updateDate":"2024-07-11","timestamp":1720578789000,"abstract":"  Large Language Models (LLMs) have been applied to many research problems\nacross various domains. One of the applications of LLMs is providing\nquestion-answering systems that cater to users from different fields. The\neffectiveness of LLM-based question-answering systems has already been\nestablished at an acceptable level for users posing questions in popular and\npublic domains such as trivia and literature. However, it has not often been\nestablished in niche domains that traditionally require specialized expertise.\nTo this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performance\nof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answering\nquestions originating from Environmental Impact Statements prepared by U.S.\nfederal government agencies in accordance with the National Environmental\nEnvironmental Act (NEPA). We specifically measure the ability of LLMs to\nunderstand the nuances of legal, technical, and compliance-related information\npresent in NEPA documents in different contextual scenarios. For example, we\ntest the LLMs' internal prior NEPA knowledge by providing questions without any\ncontext, as well as assess how LLMs synthesize the contextual information\npresent in long NEPA documents to facilitate the question/answering task. We\ncompare the performance of the long context LLMs and RAG powered models in\nhandling different types of questions (e.g., problem-solving, divergent). Our\nresults suggest that RAG powered models significantly outperform the long\ncontext models in the answer accuracy regardless of the choice of the frontier\nLLM. Our further analysis reveals that many models perform better answering\nclosed questions than divergent and problem-solving questions.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}