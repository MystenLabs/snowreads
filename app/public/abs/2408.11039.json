{"id":"2408.11039","title":"Transfusion: Predict the Next Token and Diffuse Images with One\n  Multi-Modal Model","authors":"Chunting Zhou and Lili Yu and Arun Babu and Kushal Tirumala and\n  Michihiro Yasunaga and Leonid Shamis and Jacob Kahn and Xuezhe Ma and Luke\n  Zettlemoyer and Omer Levy","authorsParsed":[["Zhou","Chunting",""],["Yu","Lili",""],["Babu","Arun",""],["Tirumala","Kushal",""],["Yasunaga","Michihiro",""],["Shamis","Leonid",""],["Kahn","Jacob",""],["Ma","Xuezhe",""],["Zettlemoyer","Luke",""],["Levy","Omer",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 17:48:20 GMT"}],"updateDate":"2024-08-21","timestamp":1724176100000,"abstract":"  We introduce Transfusion, a recipe for training a multi-modal model over\ndiscrete and continuous data. Transfusion combines the language modeling loss\nfunction (next token prediction) with diffusion to train a single transformer\nover mixed-modality sequences. We pretrain multiple Transfusion models up to 7B\nparameters from scratch on a mixture of text and image data, establishing\nscaling laws with respect to a variety of uni- and cross-modal benchmarks. Our\nexperiments show that Transfusion scales significantly better than quantizing\nimages and training a language model over discrete image tokens. By introducing\nmodality-specific encoding and decoding layers, we can further improve the\nperformance of Transfusion models, and even compress each image to just 16\npatches. We further demonstrate that scaling our Transfusion recipe to 7B\nparameters and 2T multi-modal tokens produces a model that can generate images\nand text on a par with similar scale diffusion models and language models,\nreaping the benefits of both worlds.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}