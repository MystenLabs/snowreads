{"id":"2408.05676","title":"A Decoding Acceleration Framework for Industrial Deployable LLM-based\n  Recommender Systems","authors":"Yunjia Xi, Hangyu Wang, Bo Chen, Jianghao Lin, Menghui Zhu, Weiwen\n  Liu, Ruiming Tang, Weinan Zhang, Yong Yu","authorsParsed":[["Xi","Yunjia",""],["Wang","Hangyu",""],["Chen","Bo",""],["Lin","Jianghao",""],["Zhu","Menghui",""],["Liu","Weiwen",""],["Tang","Ruiming",""],["Zhang","Weinan",""],["Yu","Yong",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 02:31:13 GMT"}],"updateDate":"2024-08-13","timestamp":1723343473000,"abstract":"  Recently, increasing attention has been paid to LLM-based recommender\nsystems, but their deployment is still under exploration in the industry. Most\ndeployments utilize LLMs as feature enhancers, generating augmentation\nknowledge in the offline stage. However, in recommendation scenarios, involving\nnumerous users and items, even offline generation with LLMs consumes\nconsiderable time and resources. This generation inefficiency stems from the\nautoregressive nature of LLMs, and a promising direction for acceleration is\nspeculative decoding, a Draft-then-Verify paradigm that increases the number of\ngenerated tokens per decoding step. In this paper, we first identify that\nrecommendation knowledge generation is suitable for retrieval-based speculative\ndecoding. Then, we discern two characteristics: (1) extensive items and users\nin RSs bring retrieval inefficiency, and (2) RSs exhibit high diversity\ntolerance for text generated by LLMs. Based on the above insights, we propose a\nDecoding Acceleration Framework for LLM-based Recommendation (dubbed DARE),\nwith Customized Retrieval Pool to improve retrieval efficiency and Relaxed\nVerification to increase the acceptance rate of draft tokens, respectively.\nExtensive experiments demonstrate that DARE achieves a 3-5x speedup and is\ncompatible with various frameworks and backbone LLMs. DARE has also been\ndeployed to online advertising scenarios within a large-scale commercial\nenvironment, achieving a 3.45x speedup while maintaining the downstream\nperformance.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}