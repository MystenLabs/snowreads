{"id":"2408.04665","title":"LLM-based MOFs Synthesis Condition Extraction using Few-Shot\n  Demonstrations","authors":"Lei Shi, Zhimeng Liu, Yi Yang, Weize Wu, Yuyang Zhang, Hongbo Zhang,\n  Jing Lin, Siyu Wu, Zihan Chen, Ruiming Li, Nan Wang, Zipeng Liu, Huobin Tan,\n  Hongyi Gao, Yue Zhang, Ge Wang","authorsParsed":[["Shi","Lei",""],["Liu","Zhimeng",""],["Yang","Yi",""],["Wu","Weize",""],["Zhang","Yuyang",""],["Zhang","Hongbo",""],["Lin","Jing",""],["Wu","Siyu",""],["Chen","Zihan",""],["Li","Ruiming",""],["Wang","Nan",""],["Liu","Zipeng",""],["Tan","Huobin",""],["Gao","Hongyi",""],["Zhang","Yue",""],["Wang","Ge",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 14:53:25 GMT"}],"updateDate":"2024-08-12","timestamp":1722956005000,"abstract":"  The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from\nliterature text has been challenging but crucial for the logical design of new\nMOFs with desirable functionality. The recent advent of large language models\n(LLMs) provides disruptively new solution to this long-standing problem and\nlatest researches have reported over 90% F1 in extracting correct conditions\nfrom MOFs literature. We argue in this paper that most existing synthesis\nextraction practices with LLMs stay with the primitive zero-shot learning,\nwhich could lead to downgraded extraction and application performance due to\nthe lack of specialized knowledge. This work pioneers and optimizes the\nfew-shot in-context learning paradigm for LLM extraction of material synthesis\nconditions. First, we propose a human-AI joint data curation process to secure\nhigh-quality ground-truth demonstrations for few-shot learning. Second, we\napply a BM25 algorithm based on the retrieval-augmented generation (RAG)\ntechnique to adaptively select few-shot demonstrations for each MOF's\nextraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the\nproposed few-shot method achieves much higher average F1 performance (0.93 vs.\n0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under\nfully automatic evaluation that are more objective than the previous human\nevaluation. The proposed method is further validated through real-world\nmaterial experiments: compared with the baseline zero-shot LLM, the proposed\nfew-shot approach increases the MOFs structural inference performance (R^2) by\n29.4% in average.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}