{"id":"2407.13006","title":"Sparsity-based Safety Conservatism for Constrained Offline Reinforcement\n  Learning","authors":"Minjae Cho, Chuangchuang Sun","authorsParsed":[["Cho","Minjae",""],["Sun","Chuangchuang",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 20:57:05 GMT"}],"updateDate":"2024-07-19","timestamp":1721249825000,"abstract":"  Reinforcement Learning (RL) has made notable success in decision-making\nfields like autonomous driving and robotic manipulation. Yet, its reliance on\nreal-time feedback poses challenges in costly or hazardous settings.\nFurthermore, RL's training approach, centered on \"on-policy\" sampling, doesn't\nfully capitalize on data. Hence, Offline RL has emerged as a compelling\nalternative, particularly in conducting additional experiments is impractical,\nand abundant datasets are available. However, the challenge of distributional\nshift (extrapolation), indicating the disparity between data distributions and\nlearning policies, also poses a risk in offline RL, potentially leading to\nsignificant safety breaches due to estimation errors (interpolation). This\nconcern is particularly pronounced in safety-critical domains, where real-world\nproblems are prevalent. To address both extrapolation and interpolation errors,\nnumerous studies have introduced additional constraints to confine policy\nbehavior, steering it towards more cautious decision-making. While many studies\nhave addressed extrapolation errors, fewer have focused on providing effective\nsolutions for tackling interpolation errors. For example, some works tackle\nthis issue by incorporating potential cost-maximizing optimization by\nperturbing the original dataset. However, this, involving a bi-level\noptimization structure, may introduce significant instability or complicate\nproblem-solving in high-dimensional tasks. This motivates us to pinpoint areas\nwhere hazards may be more prevalent than initially estimated based on the\nsparsity of available data by providing significant insight into constrained\noffline RL. In this paper, we present conservative metrics based on data\nsparsity that demonstrate the high generalizability to any methods and efficacy\ncompared to using bi-level cost-ub-maximization.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}