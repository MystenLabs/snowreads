{"id":"2407.10462","title":"BandControlNet: Parallel Transformers-based Steerable Popular Music\n  Generation with Fine-Grained Spatiotemporal Features","authors":"Jing Luo, Xinyu Yang, Dorien Herremans","authorsParsed":[["Luo","Jing",""],["Yang","Xinyu",""],["Herremans","Dorien",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 06:33:25 GMT"}],"updateDate":"2024-07-16","timestamp":1721025205000,"abstract":"  Controllable music generation promotes the interaction between humans and\ncomposition systems by projecting the users' intent on their desired music. The\nchallenge of introducing controllability is an increasingly important issue in\nthe symbolic music generation field. When building controllable generative\npopular multi-instrument music systems, two main challenges typically present\nthemselves, namely weak controllability and poor music quality. To address\nthese issues, we first propose spatiotemporal features as powerful and\nfine-grained controls to enhance the controllability of the generative model.\nIn addition, an efficient music representation called REMI_Track is designed to\nconvert multitrack music into multiple parallel music sequences and shorten the\nsequence length of each track with Byte Pair Encoding (BPE) techniques.\nSubsequently, we release BandControlNet, a conditional model based on parallel\nTransformers, to tackle the multiple music sequences and generate high-quality\nmusic samples that are conditioned to the given spatiotemporal control\nfeatures. More concretely, the two specially designed modules of\nBandControlNet, namely structure-enhanced self-attention (SE-SA) and\nCross-Track Transformer (CTT), are utilized to strengthen the resulting musical\nstructure and inter-track harmony modeling respectively. Experimental results\ntested on two popular music datasets of different lengths demonstrate that the\nproposed BandControlNet outperforms other conditional music generation models\non most objective metrics in terms of fidelity and inference speed and shows\ngreat robustness in generating long music samples. The subjective evaluations\nshow BandControlNet trained on short datasets can generate music with\ncomparable quality to state-of-the-art models, while outperforming them\nsignificantly using longer datasets.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"BX09E3Rn740V_S-cfrC8J3p5_xZo3jdf3t0oBezhxsk","pdfSize":"2413375","objectId":"0xc26792207654eb6a024e9420f59cd09e5d281acad388041c092c259f78e925fc","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
