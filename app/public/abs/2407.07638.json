{"id":"2407.07638","title":"Tuning Vision-Language Models with Candidate Labels by Prompt Alignment","authors":"Zhifang Zhang, Beibei Li","authorsParsed":[["Zhang","Zhifang",""],["Li","Beibei",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:19:31 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 04:46:24 GMT"}],"updateDate":"2024-07-12","timestamp":1720617571000,"abstract":"  Vision-language models (VLMs) can learn high-quality representations from a\nlarge-scale training dataset of image-text pairs. Prompt learning is a popular\napproach to fine-tuning VLM to adapt them to downstream tasks. Despite the\nsatisfying performance, a major limitation of prompt learning is the demand for\nlabelled data. In real-world scenarios, we may only obtain candidate labels\n(where the true label is included) instead of the true labels due to data\nprivacy or sensitivity issues. In this paper, we provide the first study on\nprompt learning with candidate labels for VLMs. We empirically demonstrate that\nprompt learning is more advantageous than other fine-tuning methods, for\nhandling candidate labels. Nonetheless, its performance drops when the label\nambiguity increases. In order to improve its robustness, we propose a simple\nyet effective framework that better leverages the prior knowledge of VLMs to\nguide the learning process with candidate labels. Specifically, our framework\ndisambiguates candidate labels by aligning the model output with the mixed\nclass posterior jointly predicted by both the learnable and the handcrafted\nprompt. Besides, our framework can be equipped with various off-the-shelf\ntraining objectives for learning with candidate labels to further improve their\nperformance. Extensive experiments demonstrate the effectiveness of our\nproposed framework.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}