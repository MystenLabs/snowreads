{"id":"2407.01400","title":"GalLoP: Learning Global and Local Prompts for Vision-Language Models","authors":"Marc Lafon, Elias Ramzi, Cl\\'ement Rambour, Nicolas Audebert, Nicolas\n  Thome","authorsParsed":[["Lafon","Marc",""],["Ramzi","Elias",""],["Rambour","Cl√©ment",""],["Audebert","Nicolas",""],["Thome","Nicolas",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 15:50:15 GMT"}],"updateDate":"2024-07-02","timestamp":1719849015000,"abstract":"  Prompt learning has been widely adopted to efficiently adapt vision-language\nmodels (VLMs), e.g. CLIP, for few-shot image classification. Despite their\nsuccess, most prompt learning methods trade-off between classification accuracy\nand robustness, e.g. in domain generalization or out-of-distribution (OOD)\ndetection. In this work, we introduce Global-Local Prompts (GalLoP), a new\nprompt learning method that learns multiple diverse prompts leveraging both\nglobal and local visual features. The training of the local prompts relies on\nlocal features with an enhanced vision-text alignment. To focus only on\npertinent features, this local alignment is coupled with a sparsity strategy in\nthe selection of the local features. We enforce diversity on the set of prompts\nusing a new ``prompt dropout'' technique and a multiscale strategy on the local\nprompts. GalLoP outperforms previous prompt learning methods on accuracy on\neleven datasets in different few shots settings and with various backbones.\nFurthermore, GalLoP shows strong robustness performances in both domain\ngeneralization and OOD detection, even outperforming dedicated OOD detection\nmethods. Code and instructions to reproduce our results will be open-sourced.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}