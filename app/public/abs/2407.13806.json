{"id":"2407.13806","title":"Revisiting Attention for Multivariate Time Series Forecasting","authors":"Haixiang Wu","authorsParsed":[["Wu","Haixiang",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 06:28:20 GMT"}],"updateDate":"2024-07-22","timestamp":1721284100000,"abstract":"  Current Transformer methods for Multivariate Time-Series Forecasting (MTSF)\nare all based on the conventional attention mechanism. They involve sequence\nembedding and performing a linear projection of Q, K, and V, and then computing\nattention within this latent space. We have never delved into the attention\nmechanism to explore whether such a mapping space is optimal for MTSF. To\ninvestigate this issue, this study first proposes Frequency Spectrum attention\n(FSatten), a novel attention mechanism based on the frequency domain space. It\nemploys the Fourier transform for embedding and introduces Multi-head Spectrum\nScaling (MSS) to replace the conventional linear mapping of Q and K. FSatten\ncan accurately capture the periodic dependencies between sequences and\noutperform the conventional attention without changing mainstream\narchitectures. We further design a more general method dubbed Scaled Orthogonal\nattention (SOatten). We propose an orthogonal embedding and a Head-Coupling\nConvolution (HCC) based on the neighboring similarity bias to guide the model\nin learning comprehensive dependency patterns. Experiments show that FSatten\nand SOatten surpass the SOTA which uses conventional attention, making it a\ngood alternative as a basic attention mechanism for MTSF. The codes and log\nfiles will be released at: https://github.com/Joeland4/FSatten-SOatten.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}