{"id":"2407.09652","title":"How Chinese are Chinese Language Models? The Puzzling Lack of Language\n  Policy in China's LLMs","authors":"Andrea W Wen-Yi, Unso Eun Seo Jo, Lu Jia Lin, David Mimno","authorsParsed":[["Wen-Yi","Andrea W",""],["Jo","Unso Eun Seo",""],["Lin","Lu Jia",""],["Mimno","David",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 19:21:40 GMT"}],"updateDate":"2024-07-16","timestamp":1720812100000,"abstract":"  Contemporary language models are increasingly multilingual, but Chinese LLM\ndevelopers must navigate complex political and business considerations of\nlanguage diversity. Language policy in China aims at influencing the public\ndiscourse and governing a multi-ethnic society, and has gradually transitioned\nfrom a pluralist to a more assimilationist approach since 1949. We explore the\nimpact of these influences on current language technology. We evaluate six\nopen-source multilingual LLMs pre-trained by Chinese companies on 18 languages,\nspanning a wide range of Chinese, Asian, and Anglo-European languages. Our\nexperiments show Chinese LLMs performance on diverse languages is\nindistinguishable from international LLMs. Similarly, the models' technical\nreports also show lack of consideration for pretraining data language coverage\nexcept for English and Mandarin Chinese. Examining Chinese AI policy, model\nexperiments, and technical reports, we find no sign of any consistent policy,\neither for or against, language diversity in China's LLM development. This\nleaves a puzzling fact that while China regulates both the languages people use\ndaily as well as language model development, they do not seem to have any\npolicy on the languages in language models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}