{"id":"2407.08733","title":"Is Your Model Really A Good Math Reasoner? Evaluating Mathematical\n  Reasoning with Checklist","authors":"Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F.\n  Wong, Xiaowei Huang, Qiufeng Wang, Kaizhu Huang","authorsParsed":[["Zhou","Zihao",""],["Liu","Shudong",""],["Ning","Maizhen",""],["Liu","Wei",""],["Wang","Jindong",""],["Wong","Derek F.",""],["Huang","Xiaowei",""],["Wang","Qiufeng",""],["Huang","Kaizhu",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:58:58 GMT"}],"updateDate":"2024-07-12","timestamp":1720720738000,"abstract":"  Exceptional mathematical reasoning ability is one of the key features that\ndemonstrate the power of large language models (LLMs). How to comprehensively\ndefine and evaluate the mathematical abilities of LLMs, and even reflect the\nuser experience in real-world scenarios, has emerged as a critical issue.\nCurrent benchmarks predominantly concentrate on problem-solving capabilities,\nwhich presents a substantial risk of model overfitting and fails to accurately\nrepresent genuine mathematical reasoning abilities. In this paper, we argue\nthat if a model really understands a problem, it should be robustly and readily\napplied across a diverse array of tasks. Motivated by this, we introduce\nMATHCHECK, a well-designed checklist for testing task generalization and\nreasoning robustness, as well as an automatic tool to generate checklists\nefficiently. MATHCHECK includes multiple mathematical reasoning tasks and\nrobustness test types to facilitate a comprehensive evaluation of both\nmathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we\ndevelop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual\nreasoning and multi-modal reasoning capabilities, respectively, serving as\nupgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.\nWe adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,\nassessing their comprehensive mathematical reasoning abilities. Our results\ndemonstrate that while frontier LLMs like GPT-4o continue to excel in various\nabilities on the checklist, many other model families exhibit a significant\ndecline. Further experiments indicate that, compared to traditional math\nbenchmarks, MATHCHECK better reflects true mathematical abilities and\nrepresents mathematical intelligence more linearly, thereby supporting our\ndesign. On our MATHCHECK, we can easily conduct detailed behavior analysis to\ndeeply investigate models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}