{"id":"2407.17033","title":"Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling\n  with Denoising Diffusion Variational Inference","authors":"Jian Xu, Delu Zeng, John Paisley","authorsParsed":[["Xu","Jian",""],["Zeng","Delu",""],["Paisley","John",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 06:39:58 GMT"}],"updateDate":"2024-07-25","timestamp":1721803198000,"abstract":"  Deep Gaussian processes (DGPs) provide a robust paradigm for Bayesian deep\nlearning. In DGPs, a set of sparse integration locations called inducing points\nare selected to approximate the posterior distribution of the model. This is\ndone to reduce computational complexity and improve model efficiency. However,\ninferring the posterior distribution of inducing points is not straightforward.\nTraditional variational inference approaches to posterior approximation often\nlead to significant bias. To address this issue, we propose an alternative\nmethod called Denoising Diffusion Variational Inference (DDVI) that uses a\ndenoising diffusion stochastic differential equation (SDE) to generate\nposterior samples of inducing variables. We rely on score matching methods for\ndenoising diffusion model to approximate score functions with a neural network.\nFurthermore, by combining classical mathematical theory of SDEs with the\nminimization of KL divergence between the approximate and true processes, we\npropose a novel explicit variational lower bound for the marginal likelihood\nfunction of DGP. Through experiments on various datasets and comparisons with\nbaseline methods, we empirically demonstrate the effectiveness of DDVI for\nposterior inference of inducing points for DGP models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}