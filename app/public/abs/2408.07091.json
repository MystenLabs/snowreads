{"id":"2408.07091","title":"Node Level Graph Autoencoder: Unified Pretraining for Textual Graph\n  Learning","authors":"Wenbin Hu, Huihao Jing, Qi Hu, Haoran Li, Yangqiu Song","authorsParsed":[["Hu","Wenbin",""],["Jing","Huihao",""],["Hu","Qi",""],["Li","Haoran",""],["Song","Yangqiu",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 14:57:53 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 05:58:36 GMT"}],"updateDate":"2024-08-22","timestamp":1723215473000,"abstract":"  Textual graphs are ubiquitous in real-world applications, featuring rich text\ninformation with complex relationships, which enables advanced research across\nvarious fields. Textual graph representation learning aims to generate\nlow-dimensional feature embeddings from textual graphs that can improve the\nperformance of downstream tasks. A high-quality feature embedding should\neffectively capture both the structural and the textual information in a\ntextual graph. However, most textual graph dataset benchmarks rely on word2vec\ntechniques to generate feature embeddings, which inherently limits their\ncapabilities. Recent works on textual graph representation learning can be\ncategorized into two folds: supervised and unsupervised methods. Supervised\nmethods finetune a language model on labeled nodes, which have limited\ncapabilities when labeled data is scarce. Unsupervised methods, on the other\nhand, extract feature embeddings by developing complex training pipelines. To\naddress these limitations, we propose a novel unified unsupervised learning\nautoencoder framework, named Node Level Graph AutoEncoder (NodeGAE). We employ\nlanguage models as the backbone of the autoencoder, with pretraining on text\nreconstruction. Additionally, we add an auxiliary loss term to make the feature\nembeddings aware of the local graph structure. Our method maintains simplicity\nin the training process and demonstrates generalizability across diverse\ntextual graphs and downstream tasks. We evaluate our method on two core graph\nrepresentation learning downstream tasks: node classification and link\nprediction. Comprehensive experiments demonstrate that our approach\nsubstantially enhances the performance of diverse graph neural networks (GNNs)\nacross multiple textual graph datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}