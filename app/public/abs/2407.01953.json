{"id":"2407.01953","title":"CatMemo at the FinLLM Challenge Task: Fine-Tuning Large Language Models\n  using Data Fusion in Financial Applications","authors":"Yupeng Cao, Zhiyuan Yao, Zhi Chen, Zhiyang Deng","authorsParsed":[["Cao","Yupeng",""],["Yao","Zhiyuan",""],["Chen","Zhi",""],["Deng","Zhiyang",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 05:04:13 GMT"}],"updateDate":"2024-07-03","timestamp":1719896653000,"abstract":"  The integration of Large Language Models (LLMs) into financial analysis has\ngarnered significant attention in the NLP community. This paper presents our\nsolution to IJCAI-2024 FinLLM challenge, investigating the capabilities of LLMs\nwithin three critical areas of financial tasks: financial classification,\nfinancial text summarization, and single stock trading. We adopted Llama3-8B\nand Mistral-7B as base models, fine-tuning them through Parameter Efficient\nFine-Tuning (PEFT) and Low-Rank Adaptation (LoRA) approaches. To enhance model\nperformance, we combine datasets from task 1 and task 2 for data fusion. Our\napproach aims to tackle these diverse tasks in a comprehensive and integrated\nmanner, showcasing LLMs' capacity to address diverse and complex financial\ntasks with improved accuracy and decision-making capabilities.\n","subjects":["Computing Research Repository/Computational Engineering, Finance, and Science","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Quantitative Finance/Computational Finance"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}