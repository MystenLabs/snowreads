{"id":"2407.11335","title":"LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction","authors":"Penghui Du, Yu Wang, Yifan Sun, Luting Wang, Yue Liao, Gang Zhang,\n  Errui Ding, Yan Wang, Jingdong Wang, Si Liu","authorsParsed":[["Du","Penghui",""],["Wang","Yu",""],["Sun","Yifan",""],["Wang","Luting",""],["Liao","Yue",""],["Zhang","Gang",""],["Ding","Errui",""],["Wang","Yan",""],["Wang","Jingdong",""],["Liu","Si",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 02:58:33 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 07:52:52 GMT"}],"updateDate":"2024-07-19","timestamp":1721098713000,"abstract":"  Existing methods enhance open-vocabulary object detection by leveraging the\nrobust open-vocabulary recognition capabilities of Vision-Language Models\n(VLMs), such as CLIP.However, two main challenges emerge:(1) A deficiency in\nconcept representation, where the category names in CLIP's text space lack\ntextual and visual knowledge.(2) An overfitting tendency towards base\ncategories, with the open vocabulary knowledge biased towards base categories\nduring the transfer from VLMs to detectors.To address these challenges, we\npropose the Language Model Instruction (LaMI) strategy, which leverages the\nrelationships between visual concepts and applies them within a simple yet\neffective DETR-like detector, termed LaMI-DETR.LaMI utilizes GPT to construct\nvisual concepts and employs T5 to investigate visual similarities across\ncategories.These inter-category relationships refine concept representation and\navoid overfitting to base categories.Comprehensive experiments validate our\napproach's superior performance over existing methods in the same rigorous\nsetting without reliance on external training resources.LaMI-DETR achieves a\nrare box AP of 43.4 on OV-LVIS, surpassing the previous best by 7.8 rare box\nAP.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}