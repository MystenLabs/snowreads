{"id":"2408.16416","title":"Preconditioned Low-Rank Riemannian Optimization for Symmetric Positive\n  Definite Linear Matrix Equations","authors":"Ivan Bioli, Daniel Kressner, Leonardo Robol","authorsParsed":[["Bioli","Ivan",""],["Kressner","Daniel",""],["Robol","Leonardo",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 10:21:42 GMT"}],"updateDate":"2024-08-30","timestamp":1724926902000,"abstract":"  This work is concerned with the numerical solution of large-scale symmetric\npositive definite matrix equations of the form $A_1XB_1^\\top + A_2XB_2^\\top +\n\\dots + A_\\ell X B_\\ell^\\top = F$, as they arise from discretized partial\ndifferential equations and control problems. One often finds that $X$ admits\ngood low-rank approximations, in particular when the right-hand side matrix $F$\nhas low rank. For $\\ell \\le 2$ terms, the solution of such equations is well\nstudied and effective low-rank solvers have been proposed, including\nAlternating Direction Implicit (ADI) methods for Lyapunov and Sylvester\nequations. For $\\ell > 2$, several existing methods try to approach $X$ through\ncombining a classical iterative method, such as the conjugate gradient (CG)\nmethod, with low-rank truncation. In this work, we consider a more direct\napproach that approximates $X$ on manifolds of fixed-rank matrices through\nRiemannian CG. One particular challenge is the incorporation of effective\npreconditioners into such a first-order Riemannian optimization method. We\npropose several novel preconditioning strategies, including a change of metric\nin the ambient space, preconditioning the Riemannian gradient, and a variant of\nADI on the tangent space. Combined with a strategy for adapting the rank of the\napproximation, the resulting method is demonstrated to be competitive for a\nnumber of examples representative for typical applications.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}