{"id":"2407.06324","title":"B'MOJO: Hybrid State Space Realizations of Foundation Models with\n  Eidetic and Fading Memory","authors":"Luca Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao\n  Shen, Benjamin Bowman, Matthew Trager, Alessandro Achille, Stefano Soatto","authorsParsed":[["Zancato","Luca",""],["Seshadri","Arjun",""],["Dukler","Yonatan",""],["Golatkar","Aditya",""],["Shen","Yantao",""],["Bowman","Benjamin",""],["Trager","Matthew",""],["Achille","Alessandro",""],["Soatto","Stefano",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 18:41:01 GMT"}],"updateDate":"2024-07-10","timestamp":1720464061000,"abstract":"  We describe a family of architectures to support transductive inference by\nallowing memory to grow to a finite but a-priori unknown bound while making\nefficient use of finite resources for inference. Current architectures use such\nresources to represent data either eidetically over a finite span (\"context\" in\nTransformers), or fading over an infinite span (in State Space Models, or\nSSMs). Recent hybrid architectures have combined eidetic and fading memory, but\nwith limitations that do not allow the designer or the learning process to\nseamlessly modulate the two, nor to extend the eidetic memory span. We leverage\nideas from Stochastic Realization Theory to develop a class of models called\nB'MOJO to seamlessly combine eidetic and fading memory within an elementary\ncomposable module. The overall architecture can be used to implement models\nthat can access short-term eidetic memory \"in-context,\" permanent structural\nmemory \"in-weights,\" fading memory \"in-state,\" and long-term eidetic memory\n\"in-storage\" by natively incorporating retrieval from an asynchronously updated\nmemory. We show that Transformers, existing SSMs such as Mamba, and hybrid\narchitectures such as Jamba are special cases of B'MOJO and describe a basic\nimplementation, to be open sourced, that can be stacked and scaled efficiently\nin hardware. We test B'MOJO on transductive inference tasks, such as\nassociative recall, where it outperforms existing SSMs and Hybrid models; as a\nbaseline, we test ordinary language modeling where B'MOJO achieves perplexity\ncomparable to similarly-sized Transformers and SSMs up to 1.4B parameters,\nwhile being up to 10% faster to train. Finally, we show that B'MOJO's ability\nto modulate eidetic and fading memory results in better inference on longer\nsequences tested up to 32K tokens, four-fold the length of the longest\nsequences seen during training.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}