{"id":"2407.10099","title":"STGFormer: Spatio-Temporal GraphFormer for 3D Human Pose Estimation in\n  Video","authors":"Yang Liu and Zhiyong Zhang","authorsParsed":[["Liu","Yang",""],["Zhang","Zhiyong",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 06:45:27 GMT"}],"updateDate":"2024-07-16","timestamp":1720939527000,"abstract":"  The current methods of video-based 3D human pose estimation have achieved\nsignificant progress; however, they continue to confront the significant\nchallenge of depth ambiguity. To address this limitation, this paper presents\nthe spatio-temporal GraphFormer framework for 3D human pose estimation in\nvideo, which integrates body structure graph-based representations with\nspatio-temporal information. Specifically, we develop a spatio-temporal\ncriss-cross graph (STG) attention mechanism. This approach is designed to learn\nthe long-range dependencies in data across both time and space, integrating\ngraph information directly into the respective attention layers. Furthermore,\nwe introduce the dual-path modulated hop-wise regular GCN (MHR-GCN) module,\nwhich utilizes modulation to optimize parameter usage and employs\nspatio-temporal hop-wise skip connections to acquire higher-order information.\nAdditionally, this module processes temporal and spatial dimensions\nindependently to learn their respective features while avoiding mutual\ninfluence. Finally, we demonstrate that our method achieves state-of-the-art\nperformance in 3D human pose estimation on the Human3.6M and MPI-INF-3DHP\ndatasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}