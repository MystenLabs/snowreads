{"id":"2408.02674","title":"On Feasibility of Intent Obfuscating Attacks","authors":"Zhaobin Li and Patrick Shafto","authorsParsed":[["Li","Zhaobin",""],["Shafto","Patrick",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 06:13:22 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 13:29:36 GMT"}],"updateDate":"2024-08-30","timestamp":1721628802000,"abstract":"  Intent obfuscation is a common tactic in adversarial situations, enabling the\nattacker to both manipulate the target system and avoid culpability.\nSurprisingly, it has rarely been implemented in adversarial attacks on machine\nlearning systems. We are the first to propose using intent obfuscation to\ngenerate adversarial examples for object detectors: by perturbing another\nnon-overlapping object to disrupt the target object, the attacker hides their\nintended target. We conduct a randomized experiment on 5 prominent detectors --\nYOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN -- using both targeted\nand untargeted attacks and achieve success on all models and attacks. We\nanalyze the success factors characterizing intent obfuscating attacks,\nincluding target object confidence and perturb object sizes. We then\ndemonstrate that the attacker can exploit these success factors to increase\nsuccess rates for all models and attacks. Finally, we discuss main takeaways\nand legal repercussions.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}