{"id":"2408.02841","title":"Evaluating Posterior Probabilities: Decision Theory, Proper Scoring\n  Rules, and Calibration","authors":"Luciana Ferrer and Daniel Ramos","authorsParsed":[["Ferrer","Luciana",""],["Ramos","Daniel",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 21:35:51 GMT"}],"updateDate":"2024-08-07","timestamp":1722893751000,"abstract":"  Most machine learning classifiers are designed to output posterior\nprobabilities for the classes given the input sample. These probabilities may\nbe used to make the categorical decision on the class of the sample; provided\nas input to a downstream system; or provided to a human for interpretation.\nEvaluating the quality of the posteriors generated by these system is an\nessential problem which was addressed decades ago with the invention of proper\nscoring rules (PSRs). Unfortunately, much of the recent machine learning\nliterature uses calibration metrics -- most commonly, the expected calibration\nerror (ECE) -- as a proxy to assess posterior performance. The problem with\nthis approach is that calibration metrics reflect only one aspect of the\nquality of the posteriors, ignoring the discrimination performance. For this\nreason, we argue that calibration metrics should play no role in the assessment\nof posterior quality. Expected PSRs should instead be used for this job,\npreferably normalized for ease of interpretation. In this work, we first give a\nbrief review of PSRs from a practical perspective, motivating their definition\nusing Bayes decision theory. We discuss why expected PSRs provide a principled\nmeasure of the quality of a system's posteriors and why calibration metrics are\nnot the right tool for this job. We argue that calibration metrics, while not\nuseful for performance assessment, may be used as diagnostic tools during\nsystem development. With this purpose in mind, we discuss a simple and\npractical calibration metric, called calibration loss, derived from a\ndecomposition of expected PSRs. We compare this metric with the ECE and with\nthe expected score divergence calibration metric from the PSR literature and\nargue, using theoretical and empirical evidence, that calibration loss is\nsuperior to these two metrics.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"N7GTd4RiAEnAgDpIiH9_eqGQ5rz7Iyu874p0MmVkpAg","pdfSize":"922083"}
