{"id":"2407.20653","title":"FACL-Attack: Frequency-Aware Contrastive Learning for Transferable\n  Adversarial Attacks","authors":"Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon","authorsParsed":[["Yang","Hunmin",""],["Jeong","Jongoh",""],["Yoon","Kuk-Jin",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 08:50:06 GMT"}],"updateDate":"2024-07-31","timestamp":1722329406000,"abstract":"  Deep neural networks are known to be vulnerable to security risks due to the\ninherent transferable nature of adversarial examples. Despite the success of\nrecent generative model-based attacks demonstrating strong transferability, it\nstill remains a challenge to design an efficient attack strategy in a\nreal-world strict black-box setting, where both the target domain and model\narchitectures are unknown. In this paper, we seek to explore a feature\ncontrastive approach in the frequency domain to generate adversarial examples\nthat are robust in both cross-domain and cross-model settings. With that goal\nin mind, we propose two modules that are only employed during the training\nphase: a Frequency-Aware Domain Randomization (FADR) module to randomize\ndomain-variant low- and high-range frequency components and a\nFrequency-Augmented Contrastive Learning (FACL) module to effectively separate\ndomain-invariant mid-frequency features of clean and perturbed image. We\ndemonstrate strong transferability of our generated adversarial perturbations\nthrough extensive cross-domain and cross-model experiments, while keeping the\ninference time complexity.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}