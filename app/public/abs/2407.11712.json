{"id":"2407.11712","title":"Harnessing Large Language Models for Multimodal Product Bundling","authors":"Xiaohao Liu, Jie Wu, Zhulin Tao, Yunshan Ma, Yinwei Wei, Tat-seng Chua","authorsParsed":[["Liu","Xiaohao",""],["Wu","Jie",""],["Tao","Zhulin",""],["Ma","Yunshan",""],["Wei","Yinwei",""],["Chua","Tat-seng",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 13:30:14 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 13:57:55 GMT"}],"updateDate":"2024-07-18","timestamp":1721136614000,"abstract":"  Product bundling provides clients with a strategic combination of individual\nitems. And it has gained significant attention in recent years as a fundamental\nprerequisite for online services. Recent methods utilize multimodal information\nthrough sophisticated extractors for bundling, but remain limited by inferior\nsemantic understanding, the restricted scope of knowledge, and an inability to\nhandle cold-start issues. Despite the extensive knowledge and complex reasoning\ncapabilities of large language models (LLMs), their direct utilization fails to\nprocess multimodalities and exploit their knowledge for multimodal product\nbundling. Adapting LLMs for this purpose involves demonstrating the synergies\namong different modalities and designing an effective optimization strategy for\nbundling, which remains challenging. To this end, we introduce Bundle-LLM to\nbridge the gap between LLMs and product bundling tasks. Specifically, we\nutilize a hybrid item tokenization to integrate multimodal information, where a\nsimple yet powerful multimodal fusion module followed by a trainable projector\nembeds all non-textual features into a single token. This module not only\nexplicitly exhibits the interplays among modalities but also shortens the\nprompt length, thereby boosting efficiency. By designing a prompt template, we\nformulate product bundling as a multiple-choice question given candidate items.\nFurthermore, we adopt progressive optimization strategy to fine-tune the LLMs\nfor disentangled objectives, achieving effective product bundling capability\nwith comprehensive multimodal semantic understanding. Extensive experiments on\nfour datasets from two application domains show that our approach outperforms a\nrange of state-of-the-art (SOTA) methods.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}