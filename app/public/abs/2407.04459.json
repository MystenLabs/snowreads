{"id":"2407.04459","title":"Generalists vs. Specialists: Evaluating Large Language Models for Urdu","authors":"Samee Arif, Abdul Hameed Azeemi, Agha Ali Raza, Awais Athar","authorsParsed":[["Arif","Samee",""],["Azeemi","Abdul Hameed",""],["Raza","Agha Ali",""],["Athar","Awais",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 12:09:40 GMT"},{"version":"v2","created":"Sat, 7 Sep 2024 15:33:38 GMT"}],"updateDate":"2024-09-10","timestamp":1720181380000,"abstract":"  In this paper, we compare general-purpose pretrained models, GPT-4-Turbo and\nLlama-3-8b-Instruct with special-purpose models fine-tuned on specific tasks,\nXLM-Roberta-large, mT5-large, and Llama-3-8b-Instruct. We focus on seven\nclassification and six generation tasks to evaluate the performance of these\nmodels on Urdu language. Urdu has 70 million native speakers, yet it remains\nunderrepresented in Natural Language Processing (NLP). Despite the frequent\nadvancements in Large Language Models (LLMs), their performance in low-resource\nlanguages, including Urdu, still needs to be explored. We also conduct a human\nevaluation for the generation tasks and compare the results with the\nevaluations performed by GPT-4-Turbo and Llama-3-8b-Instruct. We find that\nspecial-purpose models consistently outperform general-purpose models across\nvarious tasks. We also find that the evaluation done by GPT-4-Turbo for\ngeneration tasks aligns more closely with human evaluation compared to the\nevaluation by Llama-3-8b-Instruct. This paper contributes to the NLP community\nby providing insights into the effectiveness of general and specific-purpose\nLLMs for low-resource languages.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}