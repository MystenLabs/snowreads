{"id":"2408.08803","title":"FourierKAN outperforms MLP on Text Classification Head Fine-tuning","authors":"Abdullah Al Imran and Md Farhan Ishmam","authorsParsed":[["Imran","Abdullah Al",""],["Ishmam","Md Farhan",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 15:28:02 GMT"},{"version":"v2","created":"Thu, 19 Sep 2024 14:18:59 GMT"}],"updateDate":"2024-09-20","timestamp":1723822082000,"abstract":"  In resource constraint settings, adaptation to downstream classification\ntasks involves fine-tuning the final layer of a classifier (i.e. classification\nhead) while keeping rest of the model weights frozen. Multi-Layer Perceptron\n(MLP) heads fine-tuned with pre-trained transformer backbones have long been\nthe de facto standard for text classification head fine-tuning. However, the\nfixed non-linearity of MLPs often struggles to fully capture the nuances of\ncontextual embeddings produced by pre-trained models, while also being\ncomputationally expensive. In our work, we investigate the efficacy of KAN and\nits variant, Fourier KAN (FR-KAN), as alternative text classification heads.\nOur experiments reveal that FR-KAN significantly outperforms MLPs with an\naverage improvement of 10% in accuracy and 11% in F1-score across seven\npre-trained transformer models and four text classification tasks. Beyond\nperformance gains, FR-KAN is more computationally efficient and trains faster\nwith fewer parameters. These results underscore the potential of FR-KAN to\nserve as a lightweight classification head, with broader implications for\nadvancing other Natural Language Processing (NLP) tasks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Cdw7r0FCV1xdT5x6xgBqhTxfdoHLD-cW9OIQSaHBX9I","pdfSize":"396292"}
