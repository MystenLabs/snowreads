{"id":"2408.14960","title":"Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual\n  Progress","authors":"Ayomide Odumakinde, Daniel D'souza, Pat Verga, Beyza Ermis, Sara\n  Hooker","authorsParsed":[["Odumakinde","Ayomide",""],["D'souza","Daniel",""],["Verga","Pat",""],["Ermis","Beyza",""],["Hooker","Sara",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 11:07:15 GMT"}],"updateDate":"2024-08-28","timestamp":1724756835000,"abstract":"  The use of synthetic data has played a critical role in recent state-of-art\nbreakthroughs. However, overly relying on a single oracle teacher model to\ngenerate data has been shown to lead to model collapse and invite propagation\nof biases. These limitations are particularly evident in multilingual settings,\nwhere the absence of a universally effective teacher model that excels across\nall languages presents significant challenges. In this work, we address these\nextreme difference by introducing \"multilingual arbitrage\", which capitalizes\non performance variations between multiple models for a given language. To do\nso, we strategically route samples through a diverse pool of models, each with\nunique strengths in different languages. Across exhaustive experiments on\nstate-of-art models, our work suggests that arbitrage techniques allow for\nspectacular gains in performance that far outperform relying on a single\nteacher. In particular, compared to the best single teacher, we observe gains\nof up to 56.5% improvement in win rates averaged across all languages when\nswitching to multilingual arbitrage. We observe the most significant gains for\nthe least resourced languages in our pool.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cDYGeVN8T1N4zqjblWd48xPQExIV1OkFwWvST_GVDKQ","pdfSize":"2658476"}
