{"id":"2407.21227","title":"Assessing Programming Task Difficulty for Efficient Evaluation of Large\n  Language Models","authors":"Florian Tambon, Amin Nikanjam, Foutse Khomh, Giuliano Antoniol","authorsParsed":[["Tambon","Florian",""],["Nikanjam","Amin",""],["Khomh","Foutse",""],["Antoniol","Giuliano",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 22:31:19 GMT"}],"updateDate":"2024-08-01","timestamp":1722378679000,"abstract":"  Large Language Models (LLMs) show promising potential in Software\nEngineering, especially for code-related tasks like code completion and code\ngeneration. LLMs' evaluation is generally centred around general metrics\ncomputed over benchmarks. While painting a macroscopic view of the benchmarks\nand of the LLMs' capacity, it is unclear how each programming task in these\nbenchmarks assesses the capabilities of the LLMs. In particular, the difficulty\nlevel of the tasks in the benchmarks is not reflected in the score used to\nreport the performance of the model. Yet, a model achieving a 90% score on a\nbenchmark of predominantly easy tasks is likely less capable than a model\nachieving a 90% score on a benchmark containing predominantly difficult tasks.\nThis paper devises a framework, HardEval, for assessing task difficulty for\nLLMs and crafting new tasks based on identified hard tasks. The framework uses\na diverse array of prompts for a single task across multiple LLMs to obtain a\ndifficulty score for each task of a benchmark. Using two code generation\nbenchmarks, HumanEval+ and ClassEval, we show that HardEval can reliably\nidentify the hard tasks within those benchmarks, highlighting that only 21% of\nHumanEval+ and 27% of ClassEval tasks are hard for LLMs. Through our analysis\nof task difficulty, we also characterize 6 practical hard task topics which we\nused to generate new hard tasks. Orthogonal to current benchmarking evaluation\nefforts, HardEval can assist researchers and practitioners in fostering better\nassessments of LLMs. The difficulty score can be used to identify hard tasks\nwithin existing benchmarks. This, in turn, can be leveraged to generate more\nhard tasks centred around specific topics either for evaluation or improvement\nof LLMs. HardEval generalistic approach can be applied to other domains such as\ncode completion or Q/A.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}