{"id":"2408.03865","title":"PackMamba: Efficient Processing of Variable-Length Sequences in Mamba\n  training","authors":"Haoran Xu, Ziqian Liu, Rong Fu, Zhongling Su, Zerui Wang, Zheng Cai,\n  Zhilin Pei, and Xingcheng Zhang","authorsParsed":[["Xu","Haoran",""],["Liu","Ziqian",""],["Fu","Rong",""],["Su","Zhongling",""],["Wang","Zerui",""],["Cai","Zheng",""],["Pei","Zhilin",""],["Zhang","Xingcheng",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 16:13:43 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 12:08:00 GMT"}],"updateDate":"2024-08-22","timestamp":1723047223000,"abstract":"  With the evolution of large language models, traditional Transformer models\nbecome computationally demanding for lengthy sequences due to the quadratic\ngrowth in computation with respect to the sequence length. Mamba, emerging as a\ngroundbreaking architecture in the field of generative AI, demonstrates\nremarkable proficiency in handling elongated sequences with reduced\ncomputational and memory complexity. Nevertheless, the existing training\nframework of Mamba presents inefficiency with variable-length sequence inputs.\nEither single-sequence training results in low GPU utilization, or batched\nprocessing of variable-length sequences to a maximum length incurs considerable\nmemory and computational overhead. To address this problem, we analyze the\nperformance of bottleneck operators in Mamba under diverse tensor shapes and\nproposed PackMamba, a high-throughput Mamba that efficiently handles\nvariable-length sequences. Diving deep into state-space models (SSMs), we\nmodify the parallel operators to avoid passing information between individual\nsequences while maintaining high performance. Experimental results on an NVIDIA\nA100 GPU demonstrate throughput exceeding the baseline single-sequence\nprocessing scheme: 3.06x speedup on the 1.4B model and 2.62x on the 2.8B model.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}