{"id":"2408.03397","title":"HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for\n  Transformer Acceleration","authors":"Pratyush Dhingra, Janardhan Rao Doppa, and Partha Pratim Pande","authorsParsed":[["Dhingra","Pratyush",""],["Doppa","Janardhan Rao",""],["Pande","Partha Pratim",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 18:48:01 GMT"}],"updateDate":"2024-08-08","timestamp":1722970081000,"abstract":"  Transformers have revolutionized deep learning and generative modeling to\nenable unprecedented advancements in natural language processing tasks and\nbeyond. However, designing hardware accelerators for executing transformer\nmodels is challenging due to the wide variety of computing kernels involved in\nthe transformer architecture. Existing accelerators are either inadequate to\naccelerate end-to-end transformer models or suffer notable thermal limitations.\nIn this paper, we propose the design of a three-dimensional heterogeneous\narchitecture referred to as HeTraX specifically optimized to accelerate\nend-to-end transformer models. HeTraX employs hardware resources aligned with\nthe computational kernels of transformers and optimizes both performance and\nenergy. Experimental results show that HeTraX outperforms existing\nstate-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while\nensuring thermally feasibility.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9os8wnTdjpRC3YXx3aDkFR2AjIToV5u9j_T7QRiOORU","pdfSize":"750819"}
