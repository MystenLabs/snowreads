{"id":"2407.04884","title":"Differentially Private Convex Approximation of Two-Layer ReLU Networks","authors":"Antti Koskela","authorsParsed":[["Koskela","Antti",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 22:43:32 GMT"}],"updateDate":"2024-07-09","timestamp":1720219412000,"abstract":"  We show that it is possible to privately train convex problems that give\nmodels with similar privacy-utility trade-off as one hidden-layer ReLU networks\ntrained with differentially private stochastic gradient descent (DP-SGD). As we\nshow, this is possible via a certain dual formulation of the ReLU minimization\nproblem. We derive a stochastic approximation of the dual problem that leads to\na strongly convex problem which allows applying, for example, the privacy\namplification by iteration type of analysis for gradient-based private\noptimizers, and in particular allows giving accurate privacy bounds for the\nnoisy cyclic mini-batch gradient descent with fixed disjoint mini-batches. We\nobtain on the MNIST and FashionMNIST problems for the noisy cyclic mini-batch\ngradient descent first empirical results that show similar\nprivacy-utility-trade-offs as DP-SGD applied to a ReLU network. We outline\ntheoretical utility bounds that illustrate the speed-ups of the private convex\napproximation of ReLU networks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}