{"id":"2408.16425","title":"A Comparative Study of Hyperparameter Tuning Methods","authors":"Subhasis Dasgupta, Jaydip Sen","authorsParsed":[["Dasgupta","Subhasis",""],["Sen","Jaydip",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 10:35:07 GMT"}],"updateDate":"2024-08-30","timestamp":1724927707000,"abstract":"  The study emphasizes the challenge of finding the optimal trade-off between\nbias and variance, especially as hyperparameter optimization increases in\ncomplexity. Through empirical analysis, three hyperparameter tuning algorithms\nTree-structured Parzen Estimator (TPE), Genetic Search, and Random Search are\nevaluated across regression and classification tasks. The results show that\nnonlinear models, with properly tuned hyperparameters, significantly outperform\nlinear models. Interestingly, Random Search excelled in regression tasks, while\nTPE was more effective for classification tasks. This suggests that there is no\none-size-fits-all solution, as different algorithms perform better depending on\nthe task and model type. The findings underscore the importance of selecting\nthe appropriate tuning method and highlight the computational challenges\ninvolved in optimizing machine learning models, particularly as search spaces\nexpand.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}