{"id":"2407.09040","title":"Error Bounds for a Kernel-Based Constrained Optimal Smoothing\n  Approximation","authors":"Laurence Grammont (UJM, CNRS, ECL, INSA Lyon, UCBL, ICJ), Fran\\c{c}ois\n  Bachoc (IMT, RT-UQ), Andr\\'es F. L\\'opez-Lopera (CERAMATHS)","authorsParsed":[["Grammont","Laurence","","UJM, CNRS, ECL, INSA Lyon, UCBL, ICJ"],["Bachoc","François","","IMT, RT-UQ"],["López-Lopera","Andrés F.","","CERAMATHS"]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 07:04:27 GMT"}],"updateDate":"2024-07-15","timestamp":1720767867000,"abstract":"  This paper establishes error bounds for the convergence of a piecewise linear\napproximation of the constrained optimal smoothing problem posed in a\nreproducing kernel Hilbert space (RKHS). This problem can be reformulated as a\nBayesian estimation problem involving a Gaussian process related to the kernel\nof the RKHS. Consequently, error bounds can be interpreted as a quantification\nof the maximum a posteriori (MAP) accuracy. To our knowledge, no error bounds\nhave been proposed for this type of problem so far. The convergence results are\nprovided as a function of the grid size, the regularity of the kernel, and the\ndistance from the kernel interpolant of the approximation to the set of\nconstraints. Inspired by the MaxMod algorithm from recent literature, which\nsequentially allocates knots for the piecewise linear approximation, we conduct\nour analysis for non-equispaced knots. These knots are even allowed to be\nnon-dense, which impacts the definition of the optimal smoothing solution and\nour error bound quantifiers. Finally, we illustrate our theorems through\nseveral numerical experiments involving constraints such as boundedness and\nmonotonicity.\n","subjects":["Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}