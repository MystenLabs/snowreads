{"id":"2408.09600","title":"Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning","authors":"Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Josh Kimball, Ling\n  Liu","authorsParsed":[["Huang","Tiansheng",""],["Bhattacharya","Gautam",""],["Joshi","Pratik",""],["Kimball","Josh",""],["Liu","Ling",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 21:45:03 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 03:45:21 GMT"}],"updateDate":"2024-09-04","timestamp":1724017503000,"abstract":"  Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the\nfine-tuning dataset can break the LLMs's safety alignment. Existing mitigation\nstrategies include alignment stage solutions \\cite{huang2024vaccine,\nrosati2024representation} and fine-tuning stage solutions\n\\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both\ncategories of defenses fail \\textit{when some specific training\nhyper-parameters are chosen} -- a large learning rate or a large number of\ntraining epochs in the fine-tuning stage can easily invalidate the defense,\nwhich however, is necessary to guarantee finetune performance. To this end, we\npropose Antidote, a post-fine-tuning stage solution, which remains\n\\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning\nstage}}. Antidote relies on the philosophy that by removing the harmful\nparameters, the harmful model can be recovered from the harmful behaviors,\nregardless of how those harmful parameters are formed in the fine-tuning stage.\nWith this philosophy, we introduce a one-shot pruning stage after harmful\nfine-tuning to remove the harmful weights that are responsible for the\ngeneration of harmful content. Despite its embarrassing simplicity, empirical\nresults show that Antidote can reduce harmful score while maintaining accuracy\non downstream tasks.Our project page is at\n\\url{https://huangtiansheng.github.io/Antidote_gh_page/}\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}