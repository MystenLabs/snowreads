{"id":"2407.13493","title":"Training Foundation Models as Data Compression: On Information, Model\n  Weights and Copyright Law","authors":"Giorgio Franceschelli, Claudia Cevenini, Mirco Musolesi","authorsParsed":[["Franceschelli","Giorgio",""],["Cevenini","Claudia",""],["Musolesi","Mirco",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 13:23:16 GMT"},{"version":"v2","created":"Wed, 18 Sep 2024 13:41:20 GMT"}],"updateDate":"2024-09-19","timestamp":1721308996000,"abstract":"  The training process of foundation models as for other classes of deep\nlearning systems is based on minimizing the reconstruction error over a\ntraining set. For this reason, they are susceptible to the memorization and\nsubsequent reproduction of training samples. In this paper, we introduce a\ntraining-as-compressing perspective, wherein the model's weights embody a\ncompressed representation of the training data. From a copyright standpoint,\nthis point of view implies that the weights could be considered a reproduction\nor a derivative work of a potentially protected set of works. We investigate\nthe technical and legal challenges that emerge from this framing of the\ncopyright of outputs generated by foundation models, including their\nimplications for practitioners and researchers. We demonstrate that adopting an\ninformation-centric approach to the problem presents a promising pathway for\ntackling these emerging complex legal issues.\n","subjects":["Computing Research Repository/Computers and Society","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}