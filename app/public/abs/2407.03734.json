{"id":"2407.03734","title":"Improving Self-supervised Pre-training using Accent-Specific Codebooks","authors":"Darshan Prabhu, Abhishek Gupta, Omkar Nitsure, Preethi Jyothi, Sriram\n  Ganapathy","authorsParsed":[["Prabhu","Darshan",""],["Gupta","Abhishek",""],["Nitsure","Omkar",""],["Jyothi","Preethi",""],["Ganapathy","Sriram",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 08:33:52 GMT"}],"updateDate":"2024-07-08","timestamp":1720082032000,"abstract":"  Speech accents present a serious challenge to the performance of\nstate-of-the-art end-to-end Automatic Speech Recognition (ASR) systems. Even\nwith self-supervised learning and pre-training of ASR models, accent invariance\nis seldom achieved. In this work, we propose an accent-aware adaptation\ntechnique for self-supervised learning that introduces a trainable set of\naccent-specific codebooks to the self-supervised architecture. These learnable\ncodebooks enable the model to capture accent specific information during\npre-training, that is further refined during ASR finetuning. On the Mozilla\nCommon Voice dataset, our proposed approach outperforms all other\naccent-adaptation approaches on both seen and unseen English accents, with up\nto 9% relative reduction in word error rate (WER).\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}