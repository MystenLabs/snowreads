{"id":"2408.04965","title":"Generalisation First, Memorisation Second? Memorisation Localisation for\n  Natural Language Classification Tasks","authors":"Verna Dankers, Ivan Titov","authorsParsed":[["Dankers","Verna",""],["Titov","Ivan",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 09:30:57 GMT"}],"updateDate":"2024-08-12","timestamp":1723195857000,"abstract":"  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Sx1C3t9Z1E2I_v2nfEredu80Fnm8buWd8MoQBFK2GL0","pdfSize":"2339013"}
