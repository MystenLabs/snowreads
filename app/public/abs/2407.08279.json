{"id":"2407.08279","title":"Continually Learn to Map Visual Concepts to Large Language Models in\n  Resource-constrained Environments","authors":"Clea Rebillard and Julio Hurtado and Andrii Krutsylo and Lucia Passaro\n  and Vincenzo Lomonaco","authorsParsed":[["Rebillard","Clea",""],["Hurtado","Julio",""],["Krutsylo","Andrii",""],["Passaro","Lucia",""],["Lomonaco","Vincenzo",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 08:28:40 GMT"}],"updateDate":"2024-07-12","timestamp":1720686520000,"abstract":"  Learning continually from a stream of non-i.i.d. data is an open challenge in\ndeep learning, even more so when working in resource-constrained environments\nsuch as embedded devices. Visual models that are continually updated through\nsupervised learning are often prone to overfitting, catastrophic forgetting,\nand biased representations. On the other hand, large language models contain\nknowledge about multiple concepts and their relations, which can foster a more\nrobust, informed and coherent learning process. This work proposes Continual\nVisual Mapping (CVM), an approach that continually ground vision\nrepresentations to a knowledge space extracted from a fixed Language model.\nSpecifically, CVM continually trains a small and efficient visual model to map\nits representations into a conceptual space established by a fixed Large\nLanguage Model. Due to their smaller nature, CVM can be used when directly\nadapting large visual pre-trained models is unfeasible due to computational or\ndata constraints. CVM overcome state-of-the-art continual learning methods on\nfive benchmarks and offers a promising avenue for addressing generalization\ncapabilities in continual learning, even in computationally constrained\ndevices.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}