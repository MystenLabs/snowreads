{"id":"2408.07914","title":"On the effect of noise on fitting linear regression models","authors":"Insha Ullah and A.H. Welsh","authorsParsed":[["Ullah","Insha",""],["Welsh","A. H.",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 03:41:02 GMT"}],"updateDate":"2024-08-16","timestamp":1723693262000,"abstract":"  In this study, we explore the effects of including noise predictors and noise\nobservations when fitting linear regression models. We present empirical and\ntheoretical results that show that double descent occurs in both cases, albeit\nwith contradictory implications: the implication for noise predictors is that\ncomplex models are often better than simple ones, while the implication for\nnoise observations is that simple models are often better than complex ones. We\nresolve this contradiction by showing that it is not the model complexity but\nrather the implicit shrinkage by the inclusion of noise in the model that\ndrives the double descent. Specifically, we show how noise predictors or\nobservations shrink the estimators of the regression coefficients and make the\ntest error asymptote, and then how the asymptotes of the test error and the\n``condition number anomaly'' ensure that double descent occurs. We also show\nthat including noise observations in the model makes the (usually unbiased)\nordinary least squares estimator biased and indicates that the ridge regression\nestimator may need a negative ridge parameter to avoid over-shrinkage.\n","subjects":["Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}