{"id":"2407.16968","title":"Stochastic Variance-Reduced Iterative Hard Thresholding in Graph\n  Sparsity Optimization","authors":"Derek Fox and Samuel Hernandez and Qianqian Tong","authorsParsed":[["Fox","Derek",""],["Hernandez","Samuel",""],["Tong","Qianqian",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 03:26:26 GMT"}],"updateDate":"2024-07-25","timestamp":1721791586000,"abstract":"  Stochastic optimization algorithms are widely used for large-scale data\nanalysis due to their low per-iteration costs, but they often suffer from slow\nasymptotic convergence caused by inherent variance. Variance-reduced techniques\nhave been therefore used to address this issue in structured sparse models\nutilizing sparsity-inducing norms or $\\ell_0$-norms. However, these techniques\nare not directly applicable to complex (non-convex) graph sparsity models,\nwhich are essential in applications like disease outbreak monitoring and social\nnetwork analysis. In this paper, we introduce two stochastic variance-reduced\ngradient-based methods to solve graph sparsity optimization: GraphSVRG-IHT and\nGraphSCSG-IHT. We provide a general framework for theoretical analysis,\ndemonstrating that our methods enjoy a linear convergence speed. Extensive\nexperiments validate\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}