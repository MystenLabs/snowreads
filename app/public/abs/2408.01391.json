{"id":"2408.01391","title":"FT K-means: A High-Performance K-means on GPU with Fault Tolerance","authors":"Shixun Wu, Yitong Ding, Yujia Zhai, Jinyang Liu, Jiajun Huang, Zizhe\n  Jian, Huangliang Dai, Sheng Di, Bryan M. Wong, Zizhong Chen, Franck Cappello","authorsParsed":[["Wu","Shixun",""],["Ding","Yitong",""],["Zhai","Yujia",""],["Liu","Jinyang",""],["Huang","Jiajun",""],["Jian","Zizhe",""],["Dai","Huangliang",""],["Di","Sheng",""],["Wong","Bryan M.",""],["Chen","Zizhong",""],["Cappello","Franck",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 17:01:36 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 21:55:08 GMT"}],"updateDate":"2024-08-09","timestamp":1722618096000,"abstract":"  K-means is a widely used algorithm in clustering, however, its efficiency is\nprimarily constrained by the computational cost of distance computing. Existing\nimplementations suffer from suboptimal utilization of computational units and\nlack resilience against soft errors. To address these challenges, we introduce\nFT K-means, a high-performance GPU-accelerated implementation of K-means with\nonline fault tolerance. We first present a stepwise optimization strategy that\nachieves competitive performance compared to NVIDIA's cuML library. We further\nimprove FT K-means with a template-based code generation framework that\nsupports different data types and adapts to different input shapes. A novel\nwarp-level tensor-core error correction scheme is proposed to address the\nfailure of existing fault tolerance methods due to memory asynchronization\nduring copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100\nGPU demonstrate that FT K-means without fault tolerance outperforms cuML's\nK-means implementation, showing a performance increase of 10\\%-300\\% in\nscenarios involving irregular data shapes. Moreover, the fault tolerance\nfeature of FT K-means introduces only an overhead of 11\\%, maintaining robust\nperformance even with tens of errors injected per second.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}