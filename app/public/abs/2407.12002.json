{"id":"2407.12002","title":"A Multimodal Transformer for Live Streaming Highlight Prediction","authors":"Jiaxin Deng, Shiyao Wang, Dong Shen, Liqin Zhao, Fan Yang, Guorui\n  Zhou, Gaofeng Meng","authorsParsed":[["Deng","Jiaxin",""],["Wang","Shiyao",""],["Shen","Dong",""],["Zhao","Liqin",""],["Yang","Fan",""],["Zhou","Guorui",""],["Meng","Gaofeng",""]],"versions":[{"version":"v1","created":"Sat, 15 Jun 2024 04:59:19 GMT"}],"updateDate":"2024-07-18","timestamp":1718427559000,"abstract":"  Recently, live streaming platforms have gained immense popularity.\nTraditional video highlight detection mainly focuses on visual features and\nutilizes both past and future content for prediction. However, live streaming\nrequires models to infer without future frames and process complex multimodal\ninteractions, including images, audio and text comments. To address these\nissues, we propose a multimodal transformer that incorporates historical\nlook-back windows. We introduce a novel Modality Temporal Alignment Module to\nhandle the temporal shift of cross-modal signals. Additionally, using existing\ndatasets with limited manual annotations is insufficient for live streaming\nwhose topics are constantly updated and changed. Therefore, we propose a novel\nBorder-aware Pairwise Loss to learn from a large-scale dataset and utilize user\nimplicit feedback as a weak supervision signal. Extensive experiments show our\nmodel outperforms various strong baselines on both real-world scenarios and\npublic datasets. And we will release our dataset and code to better assess this\ntopic.\n","subjects":["Computing Research Repository/Multimedia","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8M3iwNdo89wZzyUSUD894KCNDvlxVBJYdiXMxx4VT5M","pdfSize":"4256423"}
