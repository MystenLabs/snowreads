{"id":"2407.04822","title":"YourMT3+: Multi-instrument Music Transcription with Enhanced Transformer\n  Architectures and Cross-dataset Stem Augmentation","authors":"Sungkyun Chang, Emmanouil Benetos, Holger Kirchhoff, Simon Dixon","authorsParsed":[["Chang","Sungkyun",""],["Benetos","Emmanouil",""],["Kirchhoff","Holger",""],["Dixon","Simon",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 19:18:33 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 21:21:09 GMT"},{"version":"v3","created":"Thu, 1 Aug 2024 14:32:05 GMT"}],"updateDate":"2024-08-02","timestamp":1720207113000,"abstract":"  Multi-instrument music transcription aims to convert polyphonic music\nrecordings into musical scores assigned to each instrument. This task is\nchallenging for modeling as it requires simultaneously identifying multiple\ninstruments and transcribing their pitch and precise timing, and the lack of\nfully annotated data adds to the training difficulties. This paper introduces\nYourMT3+, a suite of models for enhanced multi-instrument music transcription\nbased on the recent language token decoding approach of MT3. We enhance its\nencoder by adopting a hierarchical attention transformer in the time-frequency\ndomain and integrating a mixture of experts. To address data limitations, we\nintroduce a new multi-channel decoding method for training with incomplete\nannotations and propose intra- and cross-stem augmentation for dataset mixing.\nOur experiments demonstrate direct vocal transcription capabilities,\neliminating the need for voice separation pre-processors. Benchmarks across ten\npublic datasets show our models' competitiveness with, or superiority to,\nexisting transcription models. Further testing on pop music recordings\nhighlights the limitations of current models. Fully reproducible code and\ndatasets are available with demos at \\url{https://github.com/mimbres/YourMT3}.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Machine Learning","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}