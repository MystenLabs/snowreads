{"id":"2408.16647","title":"DriveGenVLM: Real-world Video Generation for Vision Language Model based\n  Autonomous Driving","authors":"Yongjie Fu, Anmol Jain, Xuan Di, Xu Chen, and Zhaobin Mo","authorsParsed":[["Fu","Yongjie",""],["Jain","Anmol",""],["Di","Xuan",""],["Chen","Xu",""],["Mo","Zhaobin",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 15:52:56 GMT"}],"updateDate":"2024-08-30","timestamp":1724946776000,"abstract":"  The advancement of autonomous driving technologies necessitates increasingly\nsophisticated methods for understanding and predicting real-world scenarios.\nVision language models (VLMs) are emerging as revolutionary tools with\nsignificant potential to influence autonomous driving. In this paper, we\npropose the DriveGenVLM framework to generate driving videos and use VLMs to\nunderstand them. To achieve this, we employ a video generation framework\ngrounded in denoising diffusion probabilistic models (DDPM) aimed at predicting\nreal-world video sequences. We then explore the adequacy of our generated\nvideos for use in VLMs by employing a pre-trained model known as Efficient\nIn-context Learning on Egocentric Videos (EILEV). The diffusion model is\ntrained with the Waymo open dataset and evaluated using the Fr\\'echet Video\nDistance (FVD) score to ensure the quality and realism of the generated videos.\nCorresponding narrations are provided by EILEV for these generated videos,\nwhich may be beneficial in the autonomous driving domain. These narrations can\nenhance traffic scene understanding, aid in navigation, and improve planning\ncapabilities. The integration of video generation with VLMs in the DriveGenVLM\nframework represents a significant step forward in leveraging advanced AI\nmodels to address complex challenges in autonomous driving.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}