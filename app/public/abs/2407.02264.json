{"id":"2407.02264","title":"SOAF: Scene Occlusion-aware Neural Acoustic Field","authors":"Huiyu Gao, Jiahao Ma, David Ahmedt-Aristizabal, Chuong Nguyen,\n  Miaomiao Liu","authorsParsed":[["Gao","Huiyu",""],["Ma","Jiahao",""],["Ahmedt-Aristizabal","David",""],["Nguyen","Chuong",""],["Liu","Miaomiao",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 13:40:56 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 01:24:37 GMT"}],"updateDate":"2024-07-04","timestamp":1719927656000,"abstract":"  This paper tackles the problem of novel view audio-visual synthesis along an\narbitrary trajectory in an indoor scene, given the audio-video recordings from\nother known trajectories of the scene. Existing methods often overlook the\neffect of room geometry, particularly wall occlusion to sound propagation,\nmaking them less accurate in multi-room environments. In this work, we propose\na new approach called Scene Occlusion-aware Acoustic Field (SOAF) for accurate\nsound generation. Our approach derives a prior for sound energy field using\ndistance-aware parametric sound-propagation modelling and then transforms it\nbased on scene transmittance learned from the input video. We extract features\nfrom the local acoustic field centred around the receiver using a Fibonacci\nSphere to generate binaural audio for novel views with a direction-aware\nattention mechanism. Extensive experiments on the real dataset RWAVS and the\nsynthetic dataset SoundSpaces demonstrate that our method outperforms previous\nstate-of-the-art techniques in audio generation. Project page:\nhttps://github.com/huiyu-gao/SOAF/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}