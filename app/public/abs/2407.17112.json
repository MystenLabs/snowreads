{"id":"2407.17112","title":"Neural Dueling Bandits","authors":"Arun Verma, Zhongxiang Dai, Xiaoqiang Lin, Patrick Jaillet, Bryan Kian\n  Hsiang Low","authorsParsed":[["Verma","Arun",""],["Dai","Zhongxiang",""],["Lin","Xiaoqiang",""],["Jaillet","Patrick",""],["Low","Bryan Kian Hsiang",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 09:23:22 GMT"}],"updateDate":"2024-07-25","timestamp":1721813002000,"abstract":"  Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\npreference feedback over the selected arms for the past contexts. However,\nexisting algorithms assume the reward function is linear, which can be complex\nand non-linear in many real-life applications like online recommendations or\nranking web search results. To overcome this challenge, we use a neural network\nto estimate the reward function using preference feedback for the previously\nselected arms. We propose upper confidence bound- and Thompson sampling-based\nalgorithms with sub-linear regret guarantees that efficiently select arms in\neach round. We then extend our theoretical results to contextual bandit\nproblems with binary feedback, which is in itself a non-trivial contribution.\nExperimental results on the problem instances derived from synthetic datasets\ncorroborate our theoretical results.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}