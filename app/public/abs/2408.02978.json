{"id":"2408.02978","title":"ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval","authors":"Ruixiang Zhao, Jian Jia, Yan Li, Xuehan Bai, Quan Chen, Han Li, Peng\n  Jiang, Xirong Li","authorsParsed":[["Zhao","Ruixiang",""],["Jia","Jian",""],["Li","Yan",""],["Bai","Xuehan",""],["Chen","Quan",""],["Li","Han",""],["Jiang","Peng",""],["Li","Xirong",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 06:24:10 GMT"}],"updateDate":"2024-08-07","timestamp":1722925450000,"abstract":"  E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.\n","subjects":["Computing Research Repository/Multimedia","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"wLlSPA5M1gjAYxoREB_lQoQbrbwsbScFFaIcimhzvV8","pdfSize":"17410003"}
