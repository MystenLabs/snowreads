{"id":"2408.13253","title":"Domain-specific long text classification from sparse relevant\n  information","authors":"C\\'elia D'Cruz, Jean-Marc Bereder, Fr\\'ed\\'eric Precioso, Michel\n  Riveill","authorsParsed":[["D'Cruz","Célia",""],["Bereder","Jean-Marc",""],["Precioso","Frédéric",""],["Riveill","Michel",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 17:54:19 GMT"}],"updateDate":"2024-08-26","timestamp":1724435659000,"abstract":"  Large Language Models have undoubtedly revolutionized the Natural Language\nProcessing field, the current trend being to promote one-model-for-all tasks\n(sentiment analysis, translation, etc.). However, the statistical mechanisms at\nwork in the larger language models struggle to exploit the relevant information\nwhen it is very sparse, when it is a weak signal. This is the case, for\nexample, for the classification of long domain-specific documents, when the\nrelevance relies on a single relevant word or on very few relevant words from\ntechnical jargon. In the medical domain, it is essential to determine whether a\ngiven report contains critical information about a patient's condition. This\ncritical information is often based on one or few specific isolated terms. In\nthis paper, we propose a hierarchical model which exploits a short list of\npotential target terms to retrieve candidate sentences and represent them into\nthe contextualized embedding of the target term(s) they contain. A pooling of\nthe term(s) embedding(s) entails the document representation to be classified.\nWe evaluate our model on one public medical document benchmark in English and\non one private French medical dataset. We show that our narrower hierarchical\nmodel is better than larger language models for retrieving relevant long\ndocuments in a domain-specific context.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}