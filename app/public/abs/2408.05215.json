{"id":"2408.05215","title":"Physics-Informed Weakly Supervised Learning for Interatomic Potentials","authors":"Makoto Takamoto, Viktor Zaverkin, Mathias Niepert","authorsParsed":[["Takamoto","Makoto",""],["Zaverkin","Viktor",""],["Niepert","Mathias",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 12:49:04 GMT"}],"updateDate":"2024-08-13","timestamp":1721738944000,"abstract":"  Machine learning plays an increasingly important role in computational\nchemistry and materials science, complementing computationally intensive ab\ninitio and first-principles methods. Despite their utility, machine-learning\nmodels often lack generalization capability and robustness during atomistic\nsimulations, yielding unphysical energy and force predictions that hinder their\nreal-world applications. We address this challenge by introducing a\nphysics-informed, weakly supervised approach for training machine-learned\ninteratomic potentials (MLIPs). We introduce two novel loss functions,\nextrapolating the potential energy via a Taylor expansion and using the concept\nof conservative forces. Our approach improves the accuracy of MLIPs applied to\ntraining tasks with sparse training data sets and reduces the need for\npre-training computationally demanding models with large data sets.\nParticularly, we perform extensive experiments demonstrating reduced energy and\nforce errors -- often lower by a factor of two -- for various baseline models\nand benchmark data sets. Finally, we show that our approach facilitates MLIPs'\ntraining in a setting where the computation of forces is infeasible at the\nreference level, such as those employing complete-basis-set extrapolation.\n","subjects":["Physics/Chemical Physics","Computing Research Repository/Machine Learning","Physics/Biological Physics","Physics/Computational Physics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}