{"id":"2407.12866","title":"Beyond KV Caching: Shared Attention for Efficient LLMs","authors":"Bingli Liao and Danilo Vasconcellos Vargas","authorsParsed":[["Liao","Bingli",""],["Vargas","Danilo Vasconcellos",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 07:23:07 GMT"}],"updateDate":"2024-07-19","timestamp":1720855387000,"abstract":"  The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6H95dZuP7wU5E6r_-n4pN5dIqthxZ43ng7RcpMvQ_cI","pdfSize":"3070836","objectId":"0x9ef4ef0023a7254191dd77b71bcddb45a80279f5ae3dd9886286017bd9b69eef","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
