{"id":"2407.14996","title":"All Against Some: Efficient Integration of Large Language Models for\n  Message Passing in Graph Neural Networks","authors":"Ajay Jaiswal, Nurendra Choudhary, Ravinarayana Adkathimar, Muthu P.\n  Alagappan, Gaurush Hiranandani, Ying Ding, Zhangyang Wang, Edward W Huang,\n  Karthik Subbian","authorsParsed":[["Jaiswal","Ajay",""],["Choudhary","Nurendra",""],["Adkathimar","Ravinarayana",""],["Alagappan","Muthu P.",""],["Hiranandani","Gaurush",""],["Ding","Ying",""],["Wang","Zhangyang",""],["Huang","Edward W",""],["Subbian","Karthik",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 22:09:42 GMT"}],"updateDate":"2024-07-23","timestamp":1721513382000,"abstract":"  Graph Neural Networks (GNNs) have attracted immense attention in the past\ndecade due to their numerous real-world applications built around\ngraph-structured data. On the other hand, Large Language Models (LLMs) with\nextensive pretrained knowledge and powerful semantic comprehension abilities\nhave recently shown a remarkable ability to benefit applications using vision\nand text data. In this paper, we investigate how LLMs can be leveraged in a\ncomputationally efficient fashion to benefit rich graph-structured data, a\nmodality relatively unexplored in LLM literature. Prior works in this area\nexploit LLMs to augment every node features in an ad-hoc fashion (not scalable\nfor large graphs), use natural language to describe the complex structural\ninformation of graphs, or perform computationally expensive finetuning of LLMs\nin conjunction with GNNs. We propose E-LLaGNN (Efficient LLMs augmented GNNs),\na framework with an on-demand LLM service that enriches message passing\nprocedure of graph learning by enhancing a limited fraction of nodes from the\ngraph. More specifically, E-LLaGNN relies on sampling high-quality\nneighborhoods using LLMs, followed by on-demand neighborhood feature\nenhancement using diverse prompts from our prompt catalog, and finally\ninformation aggregation using message passing from conventional GNN\narchitectures. We explore several heuristics-based active node selection\nstrategies to limit the computational and memory footprint of LLMs when\nhandling millions of nodes. Through extensive experiments & ablation on popular\ngraph benchmarks of varying scales (Cora, PubMed, ArXiv, & Products), we\nillustrate the effectiveness of our E-LLaGNN framework and reveal many\ninteresting capabilities such as improved gradient flow in deep GNNs, LLM-free\ninference ability etc.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}