{"id":"2407.09905","title":"Global Reinforcement Learning: Beyond Linear and Convex Rewards via\n  Submodular Semi-gradient Methods","authors":"Riccardo De Santi, Manish Prajapat, Andreas Krause","authorsParsed":[["De Santi","Riccardo",""],["Prajapat","Manish",""],["Krause","Andreas",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 14:45:08 GMT"}],"updateDate":"2024-07-16","timestamp":1720881908000,"abstract":"  In classic Reinforcement Learning (RL), the agent maximizes an additive\nobjective of the visited states, e.g., a value function. Unfortunately,\nobjectives of this type cannot model many real-world applications such as\nexperiment design, exploration, imitation learning, and risk-averse RL to name\na few. This is due to the fact that additive objectives disregard interactions\nbetween states that are crucial for certain tasks. To tackle this problem, we\nintroduce Global RL (GRL), where rewards are globally defined over trajectories\ninstead of locally over states. Global rewards can capture negative\ninteractions among states, e.g., in exploration, via submodularity, positive\ninteractions, e.g., synergetic effects, via supermodularity, while mixed\ninteractions via combinations of them. By exploiting ideas from submodular\noptimization, we propose a novel algorithmic scheme that converts any GRL\nproblem to a sequence of classic RL problems and solves it efficiently with\ncurvature-dependent approximation guarantees. We also provide hardness of\napproximation results and empirically demonstrate the effectiveness of our\nmethod on several GRL instances.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}