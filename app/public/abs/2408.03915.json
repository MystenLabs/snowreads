{"id":"2408.03915","title":"Hard to Explain: On the Computational Hardness of In-Distribution Model\n  Interpretation","authors":"Guy Amir, Shahaf Bassan, Guy Katz","authorsParsed":[["Amir","Guy",""],["Bassan","Shahaf",""],["Katz","Guy",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 17:20:52 GMT"}],"updateDate":"2024-08-08","timestamp":1723051252000,"abstract":"  The ability to interpret Machine Learning (ML) models is becoming\nincreasingly essential. However, despite significant progress in the field,\nthere remains a lack of rigorous characterization regarding the innate\ninterpretability of different models. In an attempt to bridge this gap, recent\nwork has demonstrated that it is possible to formally assess interpretability\nby studying the computational complexity of explaining the decisions of various\nmodels. In this setting, if explanations for a particular model can be obtained\nefficiently, the model is considered interpretable (since it can be explained\n``easily''). However, if generating explanations over an ML model is\ncomputationally intractable, it is considered uninterpretable. Prior research\nidentified two key factors that influence the complexity of interpreting an ML\nmodel: (i) the type of the model (e.g., neural networks, decision trees, etc.);\nand (ii) the form of explanation (e.g., contrastive explanations, Shapley\nvalues, etc.). In this work, we claim that a third, important factor must also\nbe considered for this analysis -- the underlying distribution over which the\nexplanation is obtained. Considering the underlying distribution is key in\navoiding explanations that are socially misaligned, i.e., convey information\nthat is biased and unhelpful to users. We demonstrate the significant influence\nof the underlying distribution on the resulting overall interpretation\ncomplexity, in two settings: (i) prediction models paired with an external\nout-of-distribution (OOD) detector; and (ii) prediction models designed to\ninherently generate socially aligned explanations. Our findings prove that the\nexpressiveness of the distribution can significantly influence the overall\ncomplexity of interpretation, and identify essential prerequisites that a model\nmust possess to generate socially aligned explanations.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computational Complexity","Computing Research Repository/Logic in Computer Science"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}