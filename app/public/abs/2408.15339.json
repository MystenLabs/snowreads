{"id":"2408.15339","title":"UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function","authors":"Zhichao Wang, Bin Bi, Can Huang, Shiva Kumar Pentyala, Zixu James Zhu,\n  Sitaram Asur, Na Claire Cheng","authorsParsed":[["Wang","Zhichao",""],["Bi","Bin",""],["Huang","Can",""],["Pentyala","Shiva Kumar",""],["Zhu","Zixu James",""],["Asur","Sitaram",""],["Cheng","Na Claire",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 18:04:07 GMT"}],"updateDate":"2024-08-29","timestamp":1724781847000,"abstract":"  An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}