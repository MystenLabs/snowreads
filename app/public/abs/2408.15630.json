{"id":"2408.15630","title":"CodeSift: An LLM-Based Reference-Less Framework for Automatic Code\n  Validation","authors":"Pooja Aggarwal, Oishik Chatterjee, Ting Dai, Prateeti Mohapatra, Brent\n  Paulovicks, Brad Blancett, Arthur De Magalhaes","authorsParsed":[["Aggarwal","Pooja",""],["Chatterjee","Oishik",""],["Dai","Ting",""],["Mohapatra","Prateeti",""],["Paulovicks","Brent",""],["Blancett","Brad",""],["De Magalhaes","Arthur",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 08:32:21 GMT"}],"updateDate":"2024-08-29","timestamp":1724833941000,"abstract":"  The advent of large language models (LLMs) has greatly facilitated code\ngeneration, but ensuring the functional correctness of generated code remains a\nchallenge. Traditional validation methods are often time-consuming,\nerror-prone, and impractical for large volumes of code. We introduce CodeSift,\na novel framework that leverages LLMs as the first-line filter of code\nvalidation without the need for execution, reference code, or human feedback,\nthereby reducing the validation effort. We assess the effectiveness of our\nmethod across three diverse datasets encompassing two programming languages.\nOur results indicate that CodeSift outperforms state-of-the-art code evaluation\nmethods. Internal testing conducted with subject matter experts reveals that\nthe output generated by CodeSift is in line with human preference, reinforcing\nits effectiveness as a dependable automated code validation tool.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}