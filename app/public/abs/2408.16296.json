{"id":"2408.16296","title":"Rethinking Sparse Lexical Representations for Image Retrieval in the Age\n  of Rising Multi-Modal Large Language Models","authors":"Kengo Nakata, Daisuke Miyashita, Youyang Ng, Yasuto Hoshi, Jun Deguchi","authorsParsed":[["Nakata","Kengo",""],["Miyashita","Daisuke",""],["Ng","Youyang",""],["Hoshi","Yasuto",""],["Deguchi","Jun",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 06:54:03 GMT"}],"updateDate":"2024-08-30","timestamp":1724914443000,"abstract":"  In this paper, we rethink sparse lexical representations for image retrieval.\nBy utilizing multi-modal large language models (M-LLMs) that support visual\nprompting, we can extract image features and convert them into textual data,\nenabling us to utilize efficient sparse retrieval algorithms employed in\nnatural language processing for image retrieval tasks. To assist the LLM in\nextracting image features, we apply data augmentation techniques for key\nexpansion and analyze the impact with a metric for relevance between images and\ntextual data. We empirically show the superior precision and recall performance\nof our image retrieval method compared to conventional vision-language\nmodel-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a\nkeyword-based image retrieval scenario, where keywords serve as search queries.\nWe also demonstrate that the retrieval performance can be improved by\niteratively incorporating keywords into search queries.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}