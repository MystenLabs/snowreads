{"id":"2407.08959","title":"Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for\n  Few-shot Hierarchical Text Classification","authors":"Ke Ji and Peng Wang and Wenjun Ke and Guozheng Li and Jiajun Liu and\n  Jingsheng Gao and Ziyu Shang","authorsParsed":[["Ji","Ke",""],["Wang","Peng",""],["Ke","Wenjun",""],["Li","Guozheng",""],["Liu","Jiajun",""],["Gao","Jingsheng",""],["Shang","Ziyu",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 03:21:57 GMT"}],"updateDate":"2024-07-15","timestamp":1720754517000,"abstract":"  Recently, various pre-trained language models (PLMs) have been proposed to\nprove their impressive performances on a wide range of few-shot tasks. However,\nlimited by the unstructured prior knowledge in PLMs, it is difficult to\nmaintain consistent performance on complex structured scenarios, such as\nhierarchical text classification (HTC), especially when the downstream data is\nextremely scarce. The main challenge is how to transfer the unstructured\nsemantic space in PLMs to the downstream domain hierarchy. Unlike previous work\non HTC which directly performs multi-label classification or uses graph neural\nnetwork (GNN) to inject label hierarchy, in this work, we study the HTC problem\nunder a few-shot setting to adapt knowledge in PLMs from an unstructured manner\nto the downstream hierarchy. Technically, we design a simple yet effective\nmethod named Hierarchical Iterative Conditional Random Field (HierICRF) to\nsearch the most domain-challenging directions and exquisitely crafts\ndomain-hierarchy adaptation as a hierarchical iterative language modeling\nproblem, and then it encourages the model to make hierarchical consistency\nself-correction during the inference, thereby achieving knowledge transfer with\nhierarchical consistency preservation. We perform HierICRF on various\narchitectures, and extensive experiments on two popular HTC datasets\ndemonstrate that prompt with HierICRF significantly boosts the few-shot HTC\nperformance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%\nto 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot\nsettings, while remaining SOTA hierarchical consistency performance.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}