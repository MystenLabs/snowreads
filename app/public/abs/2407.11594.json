{"id":"2407.11594","title":"DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised\n  Pre-Training","authors":"Guillermo Jimenez-Perez, Pedro Osorio, Josef Cersovsky, Javier\n  Montalt-Tordera, Jens Hooge, Steffen Vogler and Sadegh Mohammadi","authorsParsed":[["Jimenez-Perez","Guillermo",""],["Osorio","Pedro",""],["Cersovsky","Josef",""],["Montalt-Tordera","Javier",""],["Hooge","Jens",""],["Vogler","Steffen",""],["Mohammadi","Sadegh",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 10:51:21 GMT"}],"updateDate":"2024-07-17","timestamp":1721127081000,"abstract":"  Diffusion models (DMs) have emerged as powerful foundation models for a\nvariety of tasks, with a large focus in synthetic image generation. However,\ntheir requirement of large annotated datasets for training limits their\napplicability in medical imaging, where datasets are typically smaller and\nsparsely annotated. We introduce DiNO-Diffusion, a self-supervised method for\ntraining latent diffusion models (LDMs) that conditions the generation process\non image embeddings extracted from DiNO. By eliminating the reliance on\nannotations, our training leverages over 868k unlabelled images from public\nchest X-Ray (CXR) datasets. Despite being self-supervised, DiNO-Diffusion shows\ncomprehensive manifold coverage, with FID scores as low as 4.7, and emerging\nproperties when evaluated in downstream tasks. It can be used to generate\nsemantically-diverse synthetic datasets even from small data pools,\ndemonstrating up to 20% AUC increase in classification performance when used\nfor data augmentation. Images were generated with different sampling strategies\nover the DiNO embedding manifold and using real images as a starting point.\nResults suggest, DiNO-Diffusion could facilitate the creation of large datasets\nfor flexible training of downstream AI models from limited amount of real data,\nwhile also holding potential for privacy preservation. Additionally,\nDiNO-Diffusion demonstrates zero-shot segmentation performance of up to 84.4%\nDice score when evaluating lung lobe segmentation. This evidences good CXR\nimage-anatomy alignment, akin to segmenting using textual descriptors on\nvanilla DMs. Finally, DiNO-Diffusion can be easily adapted to other medical\nimaging modalities or state-of-the-art diffusion models, opening the door for\nlarge-scale, multi-domain image generation pipelines for medical imaging.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"uVmymfMoZPklq5XdUVjI14V4WTKrLYMkHI9HBjB_W-o","pdfSize":"11875349"}
