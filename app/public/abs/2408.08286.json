{"id":"2408.08286","title":"Absence of Closed-Form Descriptions for Gradient Flow in Two-Layer\n  Narrow Networks","authors":"Yeachan Park","authorsParsed":[["Park","Yeachan",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:40:11 GMT"}],"updateDate":"2024-08-16","timestamp":1723743611000,"abstract":"  In the field of machine learning, comprehending the intricate training\ndynamics of neural networks poses a significant challenge. This paper explores\nthe training dynamics of neural networks, particularly whether these dynamics\ncan be expressed in a general closed-form solution. We demonstrate that the\ndynamics of the gradient flow in two-layer narrow networks is not an integrable\nsystem. Integrable systems are characterized by trajectories confined to\nsubmanifolds defined by level sets of first integrals (invariants),\nfacilitating predictable and reducible dynamics. In contrast, non-integrable\nsystems exhibit complex behaviors that are difficult to predict. To establish\nthe non-integrability, we employ differential Galois theory, which focuses on\nthe solvability of linear differential equations. We demonstrate that under\nmild conditions, the identity component of the differential Galois group of the\nvariational equations of the gradient flow is non-solvable. This result\nconfirms the system's non-integrability and implies that the training dynamics\ncannot be represented by Liouvillian functions, precluding a closed-form\nsolution for describing these dynamics. Our findings highlight the necessity of\nemploying numerical methods to tackle optimization problems within neural\nnetworks. The results contribute to a deeper understanding of neural network\ntraining dynamics and their implications for machine learning optimization\nstrategies.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Dynamical Systems"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}