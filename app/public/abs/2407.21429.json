{"id":"2407.21429","title":"Chat-like Asserts Prediction with the Support of Large Language Model","authors":"Han Wang, Han Hu, Chunyang Chen, Burak Turhan","authorsParsed":[["Wang","Han",""],["Hu","Han",""],["Chen","Chunyang",""],["Turhan","Burak",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 08:27:03 GMT"}],"updateDate":"2024-08-01","timestamp":1722414423000,"abstract":"  Unit testing is an essential component of software testing, with the assert\nstatements playing an important role in determining whether the tested function\noperates as expected. Although research has explored automated test case\ngeneration, generating meaningful assert statements remains an ongoing\nchallenge. While several studies have investigated assert statement generation\nin Java, limited work addresses this task in popular dynamically-typed\nprogramming languages like Python. In this paper, we introduce Chat-like\nexecution-based Asserts Prediction (\\tool), a novel Large Language Model-based\napproach for generating meaningful assert statements for Python projects. \\tool\nutilizes the persona, Chain-of-Thought, and one-shot learning techniques in the\nprompt design, and conducts rounds of communication with LLM and Python\ninterpreter to generate meaningful assert statements. We also present a Python\nassert statement dataset mined from GitHub. Our evaluation demonstrates that\n\\tool achieves 64.7\\% accuracy for single assert statement generation and 62\\%\nfor overall assert statement generation, outperforming the existing approaches.\nWe also analyze the mismatched assert statements, which may still share the\nsame functionality and discuss the potential help \\tool could offer to the\nautomated Python unit test generation. The findings indicate that \\tool has the\npotential to benefit the SE community through more practical usage scenarios.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}