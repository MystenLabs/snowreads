{"id":"2408.11424","title":"EMO-LLaMA: Enhancing Facial Emotion Understanding with Instruction\n  Tuning","authors":"Bohao Xing, Zitong Yu, Xin Liu, Kaishen Yuan, Qilang Ye, Weicheng Xie,\n  Huanjing Yue, Jingyu Yang, Heikki K\\\"alvi\\\"ainen","authorsParsed":[["Xing","Bohao",""],["Yu","Zitong",""],["Liu","Xin",""],["Yuan","Kaishen",""],["Ye","Qilang",""],["Xie","Weicheng",""],["Yue","Huanjing",""],["Yang","Jingyu",""],["Kälviäinen","Heikki",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 08:28:40 GMT"}],"updateDate":"2024-08-22","timestamp":1724228920000,"abstract":"  Facial expression recognition (FER) is an important research topic in\nemotional artificial intelligence. In recent decades, researchers have made\nremarkable progress. However, current FER paradigms face challenges in\ngeneralization, lack semantic information aligned with natural language, and\nstruggle to process both images and videos within a unified framework, making\ntheir application in multimodal emotion understanding and human-computer\ninteraction difficult. Multimodal Large Language Models (MLLMs) have recently\nachieved success, offering advantages in addressing these issues and\npotentially overcoming the limitations of current FER paradigms. However,\ndirectly applying pre-trained MLLMs to FER still faces several challenges. Our\nzero-shot evaluations of existing open-source MLLMs on FER indicate a\nsignificant performance gap compared to GPT-4V and current supervised\nstate-of-the-art (SOTA) methods. In this paper, we aim to enhance MLLMs'\ncapabilities in understanding facial expressions. We first generate instruction\ndata for five FER datasets with Gemini. We then propose a novel MLLM, named\nEMO-LLaMA, which incorporates facial priors from a pretrained facial analysis\nnetwork to enhance human facial information. Specifically, we design a Face\nInfo Mining module to extract both global and local facial information.\nAdditionally, we utilize a handcrafted prompt to introduce age-gender-race\nattributes, considering the emotional differences across different human\ngroups. Extensive experiments show that EMO-LLaMA achieves SOTA-comparable or\ncompetitive results across both static and dynamic FER datasets. The\ninstruction dataset and code are available at\nhttps://github.com/xxtars/EMO-LLaMA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}