{"id":"2407.13623","title":"Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies","authors":"Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan,\n  Ping Luo, Min Lin, Ngai Wong","authorsParsed":[["Tao","Chaofan",""],["Liu","Qian",""],["Dou","Longxu",""],["Muennighoff","Niklas",""],["Wan","Zhongwei",""],["Luo","Ping",""],["Lin","Min",""],["Wong","Ngai",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 15:58:54 GMT"},{"version":"v2","created":"Fri, 26 Jul 2024 12:59:47 GMT"}],"updateDate":"2024-07-29","timestamp":1721318334000,"abstract":"  Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe same result that the optimal vocabulary size depends on the available\ncompute budget and that larger models deserve larger vocabularies. However,\nmost LLMs use too small vocabulary sizes. For example, we predict that the\noptimal vocabulary size of Llama2-70B should have been at least 216K, 7 times\nlarger than its vocabulary of 32K. We validate our predictions empirically by\ntraining models with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly\nconsidering model parameters and vocabulary size for efficient scaling.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CLZBnq2B2-wh1HCpSwL-OICBjF_sH_dvQ8_e3DqAfsY","pdfSize":"3386610"}
