{"id":"2407.06168","title":"TARGO: Benchmarking Target-driven Object Grasping under Occlusions","authors":"Yan Xia, Ran Ding, Ziyuan Qin, Guanqi Zhan, Kaichen Zhou, Long Yang,\n  Hao Dong, Daniel Cremers","authorsParsed":[["Xia","Yan",""],["Ding","Ran",""],["Qin","Ziyuan",""],["Zhan","Guanqi",""],["Zhou","Kaichen",""],["Yang","Long",""],["Dong","Hao",""],["Cremers","Daniel",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:47:45 GMT"}],"updateDate":"2024-07-09","timestamp":1720460865000,"abstract":"  Recent advances in predicting 6D grasp poses from a single depth image have\nled to promising performance in robotic grasping. However, previous grasping\nmodels face challenges in cluttered environments where nearby objects impact\nthe target object's grasp. In this paper, we first establish a new benchmark\ndataset for TARget-driven Grasping under Occlusions, named TARGO. We make the\nfollowing contributions: 1) We are the first to study the occlusion level of\ngrasping. 2) We set up an evaluation benchmark consisting of large-scale\nsynthetic data and part of real-world data, and we evaluated five grasp models\nand found that even the current SOTA model suffers when the occlusion level\nincreases, leaving grasping under occlusion still a challenge. 3) We also\ngenerate a large-scale training dataset via a scalable pipeline, which can be\nused to boost the performance of grasping under occlusion and generalized to\nthe real world. 4) We further propose a transformer-based grasping model\ninvolving a shape completion module, termed TARGO-Net, which performs most\nrobustly as occlusion increases. Our benchmark dataset can be found at\nhttps://TARGO-benchmark.github.io/.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}