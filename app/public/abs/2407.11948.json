{"id":"2407.11948","title":"Rethinking Transformer-based Multi-document Summarization: An Empirical\n  Investigation","authors":"Congbo Ma, Wei Emma Zhang, Dileepa Pitawela, Haojie Zhuang, Yanfeng\n  Shu","authorsParsed":[["Ma","Congbo",""],["Zhang","Wei Emma",""],["Pitawela","Dileepa",""],["Zhuang","Haojie",""],["Shu","Yanfeng",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:42:37 GMT"}],"updateDate":"2024-07-17","timestamp":1721151757000,"abstract":"  The utilization of Transformer-based models prospers the growth of\nmulti-document summarization (MDS). Given the huge impact and widespread\nadoption of Transformer-based models in various natural language processing\ntasks, investigating their performance and behaviors in the context of MDS\nbecomes crucial for advancing the field and enhancing the quality of summary.\nTo thoroughly examine the behaviours of Transformer-based MDS models, this\npaper presents five empirical studies on (1) measuring the impact of document\nboundary separators quantitatively; (2) exploring the effectiveness of\ndifferent mainstream Transformer structures; (3) examining the sensitivity of\nthe encoder and decoder; (4) discussing different training strategies; and (5)\ndiscovering the repetition in a summary generation. The experimental results on\nprevalent MDS datasets and eleven evaluation metrics show the influence of\ndocument boundary separators, the granularity of different level features and\ndifferent model training strategies. The results also reveal that the decoder\nexhibits greater sensitivity to noises compared to the encoder. This\nunderscores the important role played by the decoder, suggesting a potential\ndirection for future research in MDS. Furthermore, the experimental results\nindicate that the repetition problem in the generated summaries has\ncorrelations with the high uncertainty scores.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}