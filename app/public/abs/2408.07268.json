{"id":"2408.07268","title":"Fast Unconstrained Optimization via Hessian Averaging and Adaptive\n  Gradient Sampling Methods","authors":"Thomas O'Leary-Roseberry and Raghu Bollapragada","authorsParsed":[["O'Leary-Roseberry","Thomas",""],["Bollapragada","Raghu",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 03:27:48 GMT"}],"updateDate":"2024-08-15","timestamp":1723606068000,"abstract":"  We consider minimizing finite-sum and expectation objective functions via\nHessian-averaging based subsampled Newton methods. These methods allow for\ngradient inexactness and have fixed per-iteration Hessian approximation costs.\nThe recent work (Na et al. 2023) demonstrated that Hessian averaging can be\nutilized to achieve fast $\\mathcal{O}\\left(\\sqrt{\\tfrac{\\log k}{k}}\\right)$\nlocal superlinear convergence for strongly convex functions in high\nprobability, while maintaining fixed per-iteration Hessian costs. These\nmethods, however, require gradient exactness and strong convexity, which poses\nchallenges for their practical implementation. To address this concern we\nconsider Hessian-averaged methods that allow gradient inexactness via norm\ncondition based adaptive-sampling strategies. For the finite-sum problem we\nutilize deterministic sampling techniques which lead to global linear and\nsublinear convergence rates for strongly convex and nonconvex functions\nrespectively. In this setting we are able to derive an improved deterministic\nlocal superlinear convergence rate of $\\mathcal{O}\\left(\\tfrac{1}{k}\\right)$.\nFor the %expected risk expectation problem we utilize stochastic sampling\ntechniques, and derive global linear and sublinear rates for strongly convex\nand nonconvex functions, as well as a\n$\\mathcal{O}\\left(\\tfrac{1}{\\sqrt{k}}\\right)$ local superlinear convergence\nrate, all in expectation. We present novel analysis techniques that differ from\nthe previous probabilistic results. Additionally, we propose scalable and\nefficient variations of these methods via diagonal approximations and derive\nthe novel diagonally-averaged Newton (Dan) method for large-scale problems. Our\nnumerical results demonstrate that the Hessian averaging not only helps with\nconvergence, but can lead to state-of-the-art performance on difficult problems\nsuch as CIFAR100 classification with ResNets.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}