{"id":"2407.03418","title":"HEMM: Holistic Evaluation of Multimodal Foundation Models","authors":"Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei\n  Yu, Ruslan Salakhutdinov, Louis-Philippe Morency","authorsParsed":[["Liang","Paul Pu",""],["Goindani","Akshay",""],["Chafekar","Talha",""],["Mathur","Leena",""],["Yu","Haofei",""],["Salakhutdinov","Ruslan",""],["Morency","Louis-Philippe",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 18:00:48 GMT"}],"updateDate":"2024-07-08","timestamp":1720029648000,"abstract":"  Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}