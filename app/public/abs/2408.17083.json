{"id":"2408.17083","title":"Focus-Consistent Multi-Level Aggregation for Compositional Zero-Shot\n  Learning","authors":"Fengyuan Dai, Siteng Huang, Min Zhang, Biao Gong, Donglin Wang","authorsParsed":[["Dai","Fengyuan",""],["Huang","Siteng",""],["Zhang","Min",""],["Gong","Biao",""],["Wang","Donglin",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 08:13:06 GMT"}],"updateDate":"2024-09-02","timestamp":1725005586000,"abstract":"  To transfer knowledge from seen attribute-object compositions to recognize\nunseen ones, recent compositional zero-shot learning (CZSL) methods mainly\ndiscuss the optimal classification branches to identify the elements, leading\nto the popularity of employing a three-branch architecture. However, these\nmethods mix up the underlying relationship among the branches, in the aspect of\nconsistency and diversity. Specifically, consistently providing the\nhighest-level features for all three branches increases the difficulty in\ndistinguishing classes that are superficially similar. Furthermore, a single\nbranch may focus on suboptimal regions when spatial messages are not shared\nbetween the personalized branches. Recognizing these issues and endeavoring to\naddress them, we propose a novel method called Focus-Consistent Multi-Level\nAggregation (FOMA). Our method incorporates a Multi-Level Feature Aggregation\n(MFA) module to generate personalized features for each branch based on the\nimage content. Additionally, a Focus-Consistent Constraint encourages a\nconsistent focus on the informative regions, thereby implicitly exchanging\nspatial information between all branches. Extensive experiments on three\nbenchmark datasets (UT-Zappos, C-GQA, and Clothing16K) demonstrate that our\nFOMA outperforms SOTA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}