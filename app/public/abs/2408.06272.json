{"id":"2408.06272","title":"A RAG-Based Question-Answering Solution for Cyber-Attack Investigation\n  and Attribution","authors":"Sampath Rajapaksha, Ruby Rani, and Erisa Karafili","authorsParsed":[["Rajapaksha","Sampath",""],["Rani","Ruby",""],["Karafili","Erisa",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 16:33:51 GMT"}],"updateDate":"2024-08-13","timestamp":1723480431000,"abstract":"  In the constantly evolving field of cybersecurity, it is imperative for\nanalysts to stay abreast of the latest attack trends and pertinent information\nthat aids in the investigation and attribution of cyber-attacks. In this work,\nwe introduce the first question-answering (QA) model and its application that\nprovides information to the cybersecurity experts about cyber-attacks\ninvestigations and attribution. Our QA model is based on Retrieval Augmented\nGeneration (RAG) techniques together with a Large Language Model (LLM) and\nprovides answers to the users' queries based on either our knowledge base (KB)\nthat contains curated information about cyber-attacks investigations and\nattribution or on outside resources provided by the users. We have tested and\nevaluated our QA model with various types of questions, including KB-based,\nmetadata-based, specific documents from the KB, and external sources-based\nquestions. We compared the answers for KB-based questions with those from\nOpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms\nOpenAI's GPT models by providing the source of the answers and overcoming the\nhallucination limitations of the GPT models, which is critical for cyber-attack\ninvestigation and attribution. Additionally, our analysis showed that when the\nRAG QA model is given few-shot examples rather than zero-shot instructions, it\ngenerates better answers compared to cases where no examples are supplied in\naddition to the query.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}