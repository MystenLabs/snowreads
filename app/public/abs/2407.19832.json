{"id":"2407.19832","title":"ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2","authors":"Wenjun Huang, Jiakai Pan, Jiahao Tang, Yanyu Ding, Yifei Xing, Yuhe\n  Wang, Zhengzhuo Wang, Jianguo Hu","authorsParsed":[["Huang","Wenjun",""],["Pan","Jiakai",""],["Tang","Jiahao",""],["Ding","Yanyu",""],["Xing","Yifei",""],["Wang","Yuhe",""],["Wang","Zhengzhuo",""],["Hu","Jianguo",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 09:38:15 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 11:42:02 GMT"},{"version":"v3","created":"Wed, 21 Aug 2024 09:52:52 GMT"}],"updateDate":"2024-08-22","timestamp":1722245895000,"abstract":"  Multimodal Large Language Models (MLLMs) have attracted much attention for\ntheir multifunctionality. However, traditional Transformer architectures incur\nsignificant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model, which\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear scalability and fast processing of long sequences. We replace\nthe Transformer-based backbone with a pre-trained Mamba-2 model and explore\nmethods for integrating 2D visual selective scanning mechanisms into multimodal\nlearning while also trying various visual encoders and Mamba-2 model variants.\nOur extensive experiments in various multimodal benchmark tests demonstrate the\ncompetitive performance of ML-Mamba and highlight the potential of state space\nmodels in multimodal tasks. The experimental results show that: (1) we\nempirically explore how to effectively apply the 2D vision selective scan\nmechanism for multimodal learning. We propose a novel multimodal connector\ncalled the Mamba-2 Scan Connector (MSC), which enhances representational\ncapabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art\nmethods such as TinyLaVA and MobileVLM v2 through its linear sequential\nmodeling while faster inference speed; (3) Compared to multimodal models\nutilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference\nperformance and effectiveness.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}