{"id":"2408.02148","title":"Environment Complexity and Nash Equilibria in a Sequential Social\n  Dilemma","authors":"Mustafa Yasir, Andrew Howes, Vasilios Mavroudis, Chris Hicks","authorsParsed":[["Yasir","Mustafa",""],["Howes","Andrew",""],["Mavroudis","Vasilios",""],["Hicks","Chris",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 21:27:36 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 16:16:06 GMT"}],"updateDate":"2024-08-09","timestamp":1722806856000,"abstract":"  Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.\n","subjects":["Computing Research Repository/Computer Science and Game Theory","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/"}