{"id":"2407.00869","title":"Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy\n  Failure for Jailbreak Attacks","authors":"Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, Yang Zhang","authorsParsed":[["Zhou","Yue",""],["Zou","Henry Peng",""],["Di Eugenio","Barbara",""],["Zhang","Yang",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 00:23:43 GMT"}],"updateDate":"2024-07-02","timestamp":1719793423000,"abstract":"  We find that language models have difficulties generating fallacious and\ndeceptive reasoning. When asked to generate deceptive outputs, language models\ntend to leak honest counterparts but believe them to be false. Exploiting this\ndeficiency, we propose a jailbreak attack method that elicits an aligned\nlanguage model for malicious output. Specifically, we query the model to\ngenerate a fallacious yet deceptively real procedure for the harmful behavior.\nSince a fallacious procedure is generally considered fake and thus harmless by\nLLMs, it helps bypass the safeguard mechanism. Yet the output is factually\nharmful since the LLM cannot fabricate fallacious solutions but proposes\ntruthful ones. We evaluate our approach over five safety-aligned large language\nmodels, comparing four previous jailbreak methods, and show that our approach\nachieves competitive performance with more harmful outputs. We believe the\nfindings could be extended beyond model safety, such as self-verification and\nhallucination.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"IQ5qhru4_2pQczLoG8rMW9euTUrReSv0CeLSavRV08M","pdfSize":"786012"}
