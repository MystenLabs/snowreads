{"id":"2408.04607","title":"Risk and cross validation in ridge regression with correlated samples","authors":"Alexander Atanasov and Jacob A. Zavatone-Veth and Cengiz Pehlevan","authorsParsed":[["Atanasov","Alexander",""],["Zavatone-Veth","Jacob A.",""],["Pehlevan","Cengiz",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 17:27:29 GMT"},{"version":"v2","created":"Sun, 11 Aug 2024 19:50:59 GMT"}],"updateDate":"2024-08-13","timestamp":1723138049000,"abstract":"  Recent years have seen substantial advances in our understanding of\nhigh-dimensional ridge regression, but existing theories assume that training\nexamples are independent. By leveraging recent techniques from random matrix\ntheory and free probability, we provide sharp asymptotics for the in- and\nout-of-sample risks of ridge regression when the data points have arbitrary\ncorrelations. We demonstrate that in this setting, the generalized cross\nvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.\nHowever, in the case where the noise residuals have the same correlations as\nthe data points, one can modify the GCV to yield an efficiently-computable\nunbiased estimator that concentrates in the high-dimensional limit, which we\ndub CorrGCV. We further extend our asymptotic analysis to the case where the\ntest point has nontrivial correlations with the training set, a setting often\nencountered in time series forecasting. Assuming knowledge of the correlation\nstructure of the time series, this again yields an extension of the GCV\nestimator, and sharply characterizes the degree to which such test points yield\nan overly optimistic prediction of long-time risk. We validate the predictions\nof our theory across a variety of high dimensional data.\n","subjects":["Statistics/Machine Learning","Condensed Matter/Disordered Systems and Neural Networks","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}