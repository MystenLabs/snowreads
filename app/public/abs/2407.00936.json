{"id":"2407.00936","title":"Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey","authors":"Xin Wang, Zirui Chen, Haofen Wang, Leong Hou U, Zhao Li, Wenbin Guo","authorsParsed":[["Wang","Xin",""],["Chen","Zirui",""],["Wang","Haofen",""],["U","Leong Hou",""],["Li","Zhao",""],["Guo","Wenbin",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 03:37:35 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 02:19:34 GMT"}],"updateDate":"2024-07-19","timestamp":1719805055000,"abstract":"  The integration of Large Language Models (LLM) with Knowledge Representation\nLearning (KRL) signifies a significant advancement in the field of artificial\nintelligence (AI), enhancing the ability to capture and utilize both structure\nand textual information. Despite the increasing research on enhancing KRL with\nLLMs, a thorough survey that analyse processes of these enhanced models is\nconspicuously absent. Our survey addresses this by categorizing these models\nbased on three distinct Transformer architectures, and by analyzing\nexperimental data from various KRL downstream tasks to evaluate the strengths\nand weaknesses of each approach. Finally, we identify and explore potential\nfuture research directions in this emerging yet underexplored domain.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"1G6yb_JdLWXjZhrX6fRXWor6l2Z98mnSFejOMcDtG7M","pdfSize":"1081940"}
