{"id":"2407.11298","title":"ThinkGrasp: A Vision-Language System for Strategic Part Grasping in\n  Clutter","authors":"Yaoyao Qian, Xupeng Zhu, Ondrej Biza, Shuo Jiang, Linfeng Zhao, Haojie\n  Huang, Yu Qi, Robert Platt","authorsParsed":[["Qian","Yaoyao",""],["Zhu","Xupeng",""],["Biza","Ondrej",""],["Jiang","Shuo",""],["Zhao","Linfeng",""],["Huang","Haojie",""],["Qi","Yu",""],["Platt","Robert",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 01:06:33 GMT"}],"updateDate":"2024-07-17","timestamp":1721091993000,"abstract":"  Robotic grasping in cluttered environments remains a significant challenge\ndue to occlusions and complex object arrangements. We have developed\nThinkGrasp, a plug-and-play vision-language grasping system that makes use of\nGPT-4o's advanced contextual reasoning for heavy clutter environment grasping\nstrategies. ThinkGrasp can effectively identify and generate grasp poses for\ntarget objects, even when they are heavily obstructed or nearly invisible, by\nusing goal-oriented language to guide the removal of obstructing objects. This\napproach progressively uncovers the target object and ultimately grasps it with\na few steps and a high success rate. In both simulated and real experiments,\nThinkGrasp achieved a high success rate and significantly outperformed\nstate-of-the-art methods in heavily cluttered environments or with diverse\nunseen objects, demonstrating strong generalization capabilities.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}