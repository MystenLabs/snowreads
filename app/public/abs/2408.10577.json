{"id":"2408.10577","title":"Optimizing Large Language Model Hyperparameters for Code Generation","authors":"Chetan Arora, Ahnaf Ibn Sayeed, Sherlock Licorish, Fanyu Wang,\n  Christoph Treude","authorsParsed":[["Arora","Chetan",""],["Sayeed","Ahnaf Ibn",""],["Licorish","Sherlock",""],["Wang","Fanyu",""],["Treude","Christoph",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 06:32:57 GMT"}],"updateDate":"2024-08-21","timestamp":1724135577000,"abstract":"  Large Language Models (LLMs), such as GPT models, are increasingly used in\nsoftware engineering for various tasks, such as code generation, requirements\nmanagement, and debugging. While automating these tasks has garnered\nsignificant attention, a systematic study on the impact of varying\nhyperparameters on code generation outcomes remains unexplored. This study aims\nto assess LLMs' code generation performance by exhaustively exploring the\nimpact of various hyperparameters. Hyperparameters for LLMs are adjustable\nsettings that affect the model's behaviour and performance. Specifically, we\ninvestigated how changes to the hyperparameters: temperature, top probability\n(top_p), frequency penalty, and presence penalty affect code generation\noutcomes. We systematically adjusted all hyperparameters together, exploring\nevery possible combination by making small increments to each hyperparameter at\na time. This exhaustive approach was applied to 13 Python code generation\ntasks, yielding one of four outcomes for each hyperparameter combination: no\noutput from the LLM, non executable code, code that fails unit tests, or\ncorrect and functional code. We analysed these outcomes for a total of 14,742\ngenerated Python code segments, focusing on correctness, to determine how the\nhyperparameters influence the LLM to arrive at each outcome. Using correlation\ncoefficient and regression tree analyses, we ascertained which hyperparameters\ninfluence which aspect of the LLM. Our results indicate that optimal\nperformance is achieved with a temperature below 0.5, top probability below\n0.75, frequency penalty above -1 and below 1.5, and presence penalty above -1.\nWe make our dataset and results available to facilitate replication.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"subnDLds3pWM5zLM5MysNW2Ff_ZyTJcCUAfLCNYkkwY","pdfSize":"913707"}
