{"id":"2407.19353","title":"A spring-block theory of feature learning in deep neural networks","authors":"Cheng Shi, Liming Pan and Ivan Dokmani\\'c","authorsParsed":[["Shi","Cheng",""],["Pan","Liming",""],["DokmaniÄ‡","Ivan",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 00:07:20 GMT"}],"updateDate":"2024-07-30","timestamp":1722125240000,"abstract":"  A central question in deep learning is how deep neural networks (DNNs) learn\nfeatures. DNN layers progressively collapse data into a regular low-dimensional\ngeometry. This collective effect of non-linearity, noise, learning rate, width,\ndepth, and numerous other parameters, has eluded first-principles theories\nwhich are built from microscopic neuronal dynamics. Here we present a\nnoise-non-linearity phase diagram that highlights where shallow or deep layers\nlearn features more effectively. We then propose a macroscopic mechanical\ntheory of feature learning that accurately reproduces this phase diagram,\noffering a clear intuition for why and how some DNNs are ``lazy'' and some are\n``active'', and relating the distribution of feature learning over layers with\ntest accuracy.\n","subjects":["Condensed Matter/Disordered Systems and Neural Networks","Condensed Matter/Statistical Mechanics","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}