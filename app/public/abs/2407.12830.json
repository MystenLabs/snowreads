{"id":"2407.12830","title":"Knowledge-based Consistency Testing of Large Language Models","authors":"Sai Sathiesh Rajan, Ezekiel Soremekun, Sudipta Chattopadhyay","authorsParsed":[["Rajan","Sai Sathiesh",""],["Soremekun","Ezekiel",""],["Chattopadhyay","Sudipta",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 11:16:54 GMT"}],"updateDate":"2024-07-19","timestamp":1720005414000,"abstract":"  In this work, we systematically expose and measure the inconsistency and\nknowledge gaps of Large Language Models (LLMs). Specifically, we propose an\nautomated testing framework (called KONTEST) which leverages a knowledge graph\nto construct test cases. KONTEST probes and measures the inconsistencies in the\nLLM's knowledge of the world via a combination of semantically-equivalent\nqueries and test oracles (metamorphic or ontological oracle). KONTEST further\nmitigates knowledge gaps via a weighted LLM model ensemble. Using four\nstate-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that\nKONTEST generates 19.2% error inducing inputs (1917 errors from 9983 test\ninputs). It also reveals a 16.5% knowledge gap across all tested LLMs.\nKONTEST's mitigation method reduces LLM knowledge gap by 32.48%. Our ablation\nstudy further shows that GPT3.5 is not suitable for knowledge-based consistency\ntesting because it is only 60%-68% effective in knowledge construction.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"-rs_gqMqrCqDWka0MnoyIt-h01gOwvEK30pI4rXRROI","pdfSize":"2148973","objectId":"0x4aea41582dc162d3022d488863d0ec903658b66af92a08dbf2c05f049eb7fb35","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
