{"id":"2407.15892","title":"MINI-SEQUENCE TRANSFORMER: Optimizing Intermediate Memory for Long\n  Sequences Training","authors":"Cheng Luo, Jiawei Zhao, Zhuoming Chen, Beidi Chen, Anima Anandkumar","authorsParsed":[["Luo","Cheng",""],["Zhao","Jiawei",""],["Chen","Zhuoming",""],["Chen","Beidi",""],["Anandkumar","Anima",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 01:52:30 GMT"}],"updateDate":"2024-07-24","timestamp":1721613150000,"abstract":"  We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations due to our careful memory optimizations. MsT is fully\ngeneral, implementation-agnostic, and requires minimal code changes to\nintegrate with existing LLM training frameworks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}