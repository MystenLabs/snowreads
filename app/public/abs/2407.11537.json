{"id":"2407.11537","title":"AEMIM: Adversarial Examples Meet Masked Image Modeling","authors":"Wenzhao Xiang, Chang Liu, Hang Su, Hongyang Yu","authorsParsed":[["Xiang","Wenzhao",""],["Liu","Chang",""],["Su","Hang",""],["Yu","Hongyang",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 09:39:13 GMT"}],"updateDate":"2024-07-17","timestamp":1721122753000,"abstract":"  Masked image modeling (MIM) has gained significant traction for its\nremarkable prowess in representation learning. As an alternative to the\ntraditional approach, the reconstruction from corrupted images has recently\nemerged as a promising pretext task. However, the regular corrupted images are\ngenerated using generic generators, often lacking relevance to the specific\nreconstruction task involved in pre-training. Hence, reconstruction from\nregular corrupted images cannot ensure the difficulty of the pretext task,\npotentially leading to a performance decline. Moreover, generating corrupted\nimages might introduce an extra generator, resulting in a notable computational\nburden. To address these issues, we propose to incorporate adversarial examples\ninto masked image modeling, as the new reconstruction targets. Adversarial\nexamples, generated online using only the trained models, can directly aim to\ndisrupt tasks associated with pre-training. Therefore, the incorporation not\nonly elevates the level of challenge in reconstruction but also enhances\nefficiency, contributing to the acquisition of superior representations by the\nmodel. In particular, we introduce a novel auxiliary pretext task that\nreconstructs the adversarial examples corresponding to the original images. We\nalso devise an innovative adversarial attack to craft more suitable adversarial\nexamples for MIM pre-training. It is noted that our method is not restricted to\nspecific model architectures and MIM strategies, rendering it an adaptable\nplug-in capable of enhancing all MIM methods. Experimental findings\nsubstantiate the remarkable capability of our approach in amplifying the\ngeneralization and robustness of existing MIM methods. Notably, our method\nsurpasses the performance of baselines on various tasks, including ImageNet,\nits variants, and other downstream tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}