{"id":"2408.13114","title":"Controlled Learning of Pointwise Nonlinearities in Neural-Network-Like\n  Architectures","authors":"Michael Unser, Alexis Goujon, Stanislas Ducotterd","authorsParsed":[["Unser","Michael",""],["Goujon","Alexis",""],["Ducotterd","Stanislas",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 14:39:27 GMT"}],"updateDate":"2024-08-26","timestamp":1724423967000,"abstract":"  We present a general variational framework for the training of freeform\nnonlinearities in layered computational architectures subject to some slope\nconstraints. The regularization that we add to the traditional training loss\npenalizes the second-order total variation of each trainable activation. The\nslope constraints allow us to impose properties such as 1-Lipschitz stability,\nfirm non-expansiveness, and monotonicity/invertibility. These properties are\ncrucial to ensure the proper functioning of certain classes of\nsignal-processing algorithms (e.g., plug-and-play schemes, unrolled proximal\ngradient, invertible flows). We prove that the global optimum of the stated\nconstrained-optimization problem is achieved with nonlinearities that are\nadaptive nonuniform linear splines. We then show how to solve the resulting\nfunction-optimization problem numerically by representing the nonlinearities in\na suitable (nonuniform) B-spline basis. Finally, we illustrate the use of our\nframework with the data-driven design of (weakly) convex regularizers for the\ndenoising of images and the resolution of inverse problems.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Functional Analysis"],"license":"http://creativecommons.org/licenses/by/4.0/"}