{"id":"2408.10862","title":"Feature Selection from Differentially Private Correlations","authors":"Ryan Swope, Amol Khanna, Philip Doldo, Saptarshi Roy, Edward Raff","authorsParsed":[["Swope","Ryan",""],["Khanna","Amol",""],["Doldo","Philip",""],["Roy","Saptarshi",""],["Raff","Edward",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 13:54:07 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 03:03:57 GMT"}],"updateDate":"2024-08-26","timestamp":1724162047000,"abstract":"  Data scientists often seek to identify the most important features in\nhigh-dimensional datasets. This can be done through $L_1$-regularized\nregression, but this can become inefficient for very high-dimensional datasets.\nAdditionally, high-dimensional regression can leak information about individual\ndatapoints in a dataset. In this paper, we empirically evaluate the established\nbaseline method for feature selection with differential privacy, the two-stage\nselection technique, and show that it is not stable under sparsity. This makes\nit perform poorly on real-world datasets, so we consider a different approach\nto private feature selection. We employ a correlations-based order statistic to\nchoose important features from a dataset and privatize them to ensure that the\nresults do not leak information about individual datapoints. We find that our\nmethod significantly outperforms the established baseline for private feature\nselection on many datasets.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}