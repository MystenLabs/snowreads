{"id":"2407.15300","title":"SELM: Enhancing Speech Emotion Recognition for Out-of-Domain Scenarios","authors":"Hazim Bukhari, Soham Deshmukh, Hira Dhamyal, Bhiksha Raj, Rita Singh","authorsParsed":[["Bukhari","Hazim",""],["Deshmukh","Soham",""],["Dhamyal","Hira",""],["Raj","Bhiksha",""],["Singh","Rita",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 00:01:20 GMT"}],"updateDate":"2024-07-23","timestamp":1721606480000,"abstract":"  Speech Emotion Recognition (SER) has been traditionally formulated as a\nclassification task. However, emotions are generally a spectrum whose\ndistribution varies from situation to situation leading to poor Out-of-Domain\n(OOD) performance. We take inspiration from statistical formulation of\nAutomatic Speech Recognition (ASR) and formulate the SER task as generating the\nmost likely sequence of text tokens to infer emotion. The formulation breaks\nSER into predicting acoustic model features weighted by language model\nprediction. As an instance of this approach, we present SELM, an\naudio-conditioned language model for SER that predicts different emotion views.\nWe train SELM on curated speech emotion corpus and test it on three OOD\ndatasets (RAVDESS, CREMAD, IEMOCAP) not used in training. SELM achieves\nsignificant improvements over the state-of-the-art baselines, with 17% and 7%\nrelative accuracy gains for RAVDESS and CREMA-D, respectively. Moreover, SELM\ncan further boost its performance by Few-Shot Learning using a few annotated\nexamples. The results highlight the effectiveness of our SER formulation,\nespecially to improve performance in OOD scenarios.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}