{"id":"2407.10047","title":"HSFusion: A high-level vision task-driven infrared and visible image\n  fusion network via semantic and geometric domain transformation","authors":"Chengjie Jiang and Xiaowen Liu and Bowen Zheng and Lu Bai and Jing Li","authorsParsed":[["Jiang","Chengjie",""],["Liu","Xiaowen",""],["Zheng","Bowen",""],["Bai","Lu",""],["Li","Jing",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 02:16:13 GMT"}],"updateDate":"2024-07-16","timestamp":1720923373000,"abstract":"  Infrared and visible image fusion has been developed from vision perception\noriented fusion methods to strategies which both consider the vision perception\nand high-level vision task. However, the existing task-driven methods fail to\naddress the domain gap between semantic and geometric representation. To\novercome these issues, we propose a high-level vision task-driven infrared and\nvisible image fusion network via semantic and geometric domain transformation,\nterms as HSFusion. Specifically, to minimize the gap between semantic and\ngeometric representation, we design two separate domain transformation branches\nby CycleGAN framework, and each includes two processes: the forward\nsegmentation process and the reverse reconstruction process. CycleGAN is\ncapable of learning domain transformation patterns, and the reconstruction\nprocess of CycleGAN is conducted under the constraint of these patterns. Thus,\nour method can significantly facilitate the integration of semantic and\ngeometric information and further reduces the domain gap. In fusion stage, we\nintegrate the infrared and visible features that extracted from the\nreconstruction process of two seperate CycleGANs to obtain the fused result.\nThese features, containing varying proportions of semantic and geometric\ninformation, can significantly enhance the high level vision tasks.\nAdditionally, we generate masks based on segmentation results to guide the\nfusion task. These masks can provide semantic priors, and we design adaptive\nweights for two distinct areas in the masks to facilitate image fusion.\nFinally, we conducted comparative experiments between our method and eleven\nother state-of-the-art methods, demonstrating that our approach surpasses\nothers in both visual appeal and semantic segmentation task.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}