{"id":"2407.05721","title":"PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation","authors":"Jinpeng Hu, Tengteng Dong, Luo Gang, Hui Ma, Peng Zou, Xiao Sun, Dan\n  Guo, Meng Wang","authorsParsed":[["Hu","Jinpeng",""],["Dong","Tengteng",""],["Gang","Luo",""],["Ma","Hui",""],["Zou","Peng",""],["Sun","Xiao",""],["Guo","Dan",""],["Wang","Meng",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 08:25:56 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 10:29:12 GMT"}],"updateDate":"2024-08-08","timestamp":1720427156000,"abstract":"  Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising generation, evidence judgment, and refinement. We augment this\nprocess with real-world psychological case backgrounds extracted from online\nplatforms, enhancing the relevance and applicability of the generated data.\nAdditionally, to compare the performance of PsycoLLM with other LLMs, we\ndevelop a comprehensive psychological benchmark based on authoritative\npsychological counseling examinations in China, which includes assessments of\nprofessional ethics, theoretical proficiency, and case analysis. The\nexperimental results on the benchmark illustrates the effectiveness of\nPsycoLLM, which demonstrates superior performance compared to other LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}