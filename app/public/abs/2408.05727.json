{"id":"2408.05727","title":"Hotfixing Large Language Models for Code: How Far Can\n  Parameter-Efficient Fine-Tuning Go?","authors":"Zhou Yang, David Lo","authorsParsed":[["Yang","Zhou",""],["Lo","David",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 08:34:43 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 22:00:44 GMT"}],"updateDate":"2024-08-21","timestamp":1723365283000,"abstract":"  Large Language Models for Code (LLM4Code) have become an integral part of\ndevelopers' workflows, assisting with tasks such as code completion and\ngeneration. However, these models are found to exhibit undesired behaviors\nafter their release, like generating buggy code, due to their extensive\ntraining on vast amounts of source code that contain such buggy code. The\ntraining data (usually coming from open-source software) keeps evolving, e.g.,\ndevelopers fix the buggy code. However, adapting such evolution to mitigate\nLLM4Code's undesired behaviors is non-trivial, as retraining models on the\nupdated dataset usually takes much time and resources. This motivates us to\npropose the concept of hotfixing LLM4Code, mitigating LLM4Code's undesired\nbehaviors effectively and efficiently with minimal negative effects.\n  This paper mainly focuses on hotfixing LLM4Code to make them generate less\nbuggy code and more fixed code. We begin by demonstrating that models from the\npopular CodeGen family frequently generate buggy code. Then, we define three\nlearning objectives in hotfixing and design multiple loss functions for each\nobjective: (1) learn the desired behaviors, (2) unlearn the undesired\nbehaviors, and (3) retain knowledge of other code. We evaluate four different\nfine-tuning techniques for hotfixing the models and gain the following\ninsights. Optimizing these three learning goals together, using LoRA (low-rank\nadaptation), effectively influences the model's behavior. Specifically, it\nincreases the generation of fixed code by up to 108.42% and decreases the\ngeneration of buggy code by up to 50.47%. Statistical tests confirm that\nhotfixing does not significantly affect the models' functional correctness on\nthe HumanEval benchmark. We also show that hotfixing demonstrates strong time\nefficiency.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"QhKaWTcLVzn_B4CU0OmkvfP8waZe5TdhNfSySuvNH58","pdfSize":"1056582"}
