{"id":"2408.00801","title":"Low Rank Field-Weighted Factorization Machines for Low Latency Item\n  Recommendation","authors":"Alex Shtoff, Michael Viderman, Naama Haramaty-Krasne, Oren Somekh,\n  Ariel Raviv, Tularam Ban","authorsParsed":[["Shtoff","Alex",""],["Viderman","Michael",""],["Haramaty-Krasne","Naama",""],["Somekh","Oren",""],["Raviv","Ariel",""],["Ban","Tularam",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 14:08:37 GMT"}],"updateDate":"2024-08-05","timestamp":1721657317000,"abstract":"  Factorization machine (FM) variants are widely used in recommendation systems\nthat operate under strict throughput and latency requirements, such as online\nadvertising systems. FMs are known both due to their ability to model pairwise\nfeature interactions while being resilient to data sparsity, and their\ncomputational graphs that facilitate fast inference and training. Moreover,\nwhen items are ranked as a part of a query for each incoming user, these graphs\nfacilitate computing the portion stemming from the user and context fields only\nonce per query. Consequently, in terms of inference cost, the number of user or\ncontext fields is practically unlimited. More advanced FM variants, such as\nFwFM, provide better accuracy by learning a representation of field-wise\ninteractions, but require computing all pairwise interaction terms explicitly.\nThe computational cost during inference is proportional to the square of the\nnumber of fields, including user, context, and item. When the number of fields\nis large, this is prohibitive in systems with strict latency constraints. To\nmitigate this caveat, heuristic pruning of low intensity field interactions is\ncommonly used to accelerate inference. In this work we propose an alternative\nto the pruning heuristic in FwFMs using a diagonal plus symmetric low-rank\ndecomposition. Our technique reduces the computational cost of inference, by\nallowing it to be proportional to the number of item fields only. Using a set\nof experiments on real-world datasets, we show that aggressive rank reduction\noutperforms similarly aggressive pruning, both in terms of accuracy and item\nrecommendation speed. We corroborate our claim of faster inference\nexperimentally, both via a synthetic test, and by having deployed our solution\nto a major online advertising system. The code to reproduce our experimental\nresults is at https://github.com/michaelviderman/pytorch-fm/tree/dev.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"c-6qk7jBC9y9rWlLOXPZG0r-TftjxPhPK3oHSJ8biEM","pdfSize":"1144957"}
