{"id":"2407.02769","title":"Fine-Grained Scene Image Classification with Modality-Agnostic Adapter","authors":"Yiqun Wang, Zhao Zhou, Xiangcheng Du, Xingjiao Wu, Yingbin Zheng,\n  Cheng Jin","authorsParsed":[["Wang","Yiqun",""],["Zhou","Zhao",""],["Du","Xiangcheng",""],["Wu","Xingjiao",""],["Zheng","Yingbin",""],["Jin","Cheng",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 02:57:14 GMT"}],"updateDate":"2024-07-04","timestamp":1719975434000,"abstract":"  When dealing with the task of fine-grained scene image classification, most\nprevious works lay much emphasis on global visual features when doing\nmulti-modal feature fusion. In other words, models are deliberately designed\nbased on prior intuitions about the importance of different modalities. In this\npaper, we present a new multi-modal feature fusion approach named MAA\n(Modality-Agnostic Adapter), trying to make the model learn the importance of\ndifferent modalities in different cases adaptively, without giving a prior\nsetting in the model architecture. More specifically, we eliminate the modal\ndifferences in distribution and then use a modality-agnostic Transformer\nencoder for a semantic-level feature fusion. Our experiments demonstrate that\nMAA achieves state-of-the-art results on benchmarks by applying the same\nmodalities with previous methods. Besides, it is worth mentioning that new\nmodalities can be easily added when using MAA and further boost the\nperformance. Code is available at https://github.com/quniLcs/MAA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}