{"id":"2408.14423","title":"DualSpeech: Enhancing Speaker-Fidelity and Text-Intelligibility Through\n  Dual Classifier-Free Guidance","authors":"Jinhyeok Yang, Junhyeok Lee, Hyeong-Seok Choi, Seunghun Ji, Hyeongju\n  Kim, Juheon Lee","authorsParsed":[["Yang","Jinhyeok",""],["Lee","Junhyeok",""],["Choi","Hyeong-Seok",""],["Ji","Seunghun",""],["Kim","Hyeongju",""],["Lee","Juheon",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 17:09:04 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 06:11:37 GMT"}],"updateDate":"2024-08-28","timestamp":1724692144000,"abstract":"  Text-to-Speech (TTS) models have advanced significantly, aiming to accurately\nreplicate human speech's diversity, including unique speaker identities and\nlinguistic nuances. Despite these advancements, achieving an optimal balance\nbetween speaker-fidelity and text-intelligibility remains a challenge,\nparticularly when diverse control demands are considered. Addressing this, we\nintroduce DualSpeech, a TTS model that integrates phoneme-level latent\ndiffusion with dual classifier-free guidance. This approach enables exceptional\ncontrol over speaker-fidelity and text-intelligibility. Experimental results\ndemonstrate that by utilizing the sophisticated control, DualSpeech surpasses\nexisting state-of-the-art TTS models in performance. Demos are available at\nhttps://bit.ly/48Ewoib.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KAhFeE4wOM8biqM5VGcFGWi6EUa-58QpjafeKxEJy4o","pdfSize":"291057"}
