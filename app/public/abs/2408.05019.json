{"id":"2408.05019","title":"Instruction Tuning-free Visual Token Complement for Multimodal LLMs","authors":"Dongsheng Wang, Jiequan Cui, Miaoge Li, Wang Lin, Bo Chen, and Hanwang\n  Zhang","authorsParsed":[["Wang","Dongsheng",""],["Cui","Jiequan",""],["Li","Miaoge",""],["Lin","Wang",""],["Chen","Bo",""],["Zhang","Hanwang",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 12:13:01 GMT"}],"updateDate":"2024-08-12","timestamp":1723205581000,"abstract":"  As the open community of large language models (LLMs) matures, multimodal\nLLMs (MLLMs) have promised an elegant bridge between vision and language.\nHowever, current research is inherently constrained by challenges such as the\nneed for high-quality instruction pairs and the loss of visual information in\nimage-to-text training objectives. To this end, we propose a Visual Token\nComplement framework (VTC) that helps MLLMs regain the missing visual features\nand thus improve response accuracy. Specifically, our VTC integrates\ntext-to-image generation as a guide to identifying the text-irrelevant\nfeatures, and a visual selector is then developed to generate complementary\nvisual tokens to enrich the original visual input. Moreover, an iterative\nstrategy is further designed to extract more visual information by iteratively\nusing the visual selector without any additional training. Notably, the\ntraining pipeline requires no additional image-text pairs, resulting in a\ndesired instruction tuning-free property. Both qualitative and quantitative\nexperiments demonstrate the superiority and efficiency of our VTC.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}