{"id":"2408.09085","title":"Segment Anything with Multiple Modalities","authors":"Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Naoto Yokoya, Shijian Lu","authorsParsed":[["Xiao","Aoran",""],["Xuan","Weihao",""],["Qi","Heli",""],["Xing","Yun",""],["Yokoya","Naoto",""],["Lu","Shijian",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 03:45:40 GMT"}],"updateDate":"2024-08-20","timestamp":1723866340000,"abstract":"  Robust and accurate segmentation of scenes has become one core functionality\nin various visual recognition and navigation tasks. This has inspired the\nrecent development of Segment Anything Model (SAM), a foundation model for\ngeneral mask segmentation. However, SAM is largely tailored for single-modal\nRGB images, limiting its applicability to multi-modal data captured with\nwidely-adopted sensor suites, such as LiDAR plus RGB, depth plus RGB, thermal\nplus RGB, etc. We develop MM-SAM, an extension and expansion of SAM that\nsupports cross-modal and multi-modal processing for robust and enhanced\nsegmentation with different sensor suites. MM-SAM features two key designs,\nnamely, unsupervised cross-modal transfer and weakly-supervised multi-modal\nfusion, enabling label-efficient and parameter-efficient adaptation toward\nvarious sensor modalities. It addresses three main challenges: 1) adaptation\ntoward diverse non-RGB sensors for single-modal processing, 2) synergistic\nprocessing of multi-modal data via sensor fusion, and 3) mask-free training for\ndifferent downstream tasks. Extensive experiments show that MM-SAM consistently\noutperforms SAM by large margins, demonstrating its effectiveness and\nrobustness across various sensors and data modalities.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}