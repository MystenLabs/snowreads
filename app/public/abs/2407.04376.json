{"id":"2407.04376","title":"DeepLNE++ leveraging knowledge distillation for accelerated multi-state\n  path-like collective variables","authors":"Thorben Fr\\\"ohlking, Valerio Rizzi, Simone Aureli, Francesco Luigi\n  Gervasio","authorsParsed":[["Fr√∂hlking","Thorben",""],["Rizzi","Valerio",""],["Aureli","Simone",""],["Gervasio","Francesco Luigi",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 09:26:42 GMT"}],"updateDate":"2024-07-08","timestamp":1720171602000,"abstract":"  Path-like collective variables can be very effective for accurately modeling\ncomplex biomolecular processes in molecular dynamics simulations. Recently, we\nintroduced DeepLNE, a machine learning-based path-like CV that provides a\nprogression variable s along the path as a non-linear combination of several\ndescriptors, effectively approximating the reaction coordinate. However,\nDeepLNE is computationally expensive for realistic systems needing many\ndescriptors and limited in its ability to handle multi-state reactions. Here we\npresent DeepLNE++, which uses a knowledge distillation approach to\nsignificantly accelerate the evaluation of DeepLNE, making it feasible to\ncompute free energy landscapes for large and complex biomolecular systems. In\naddition, DeepLNE++ encodes system-specific knowledge within a supervised\nmultitasking framework, enhancing its versatility and effectiveness.\n","subjects":["Physics/Chemical Physics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}