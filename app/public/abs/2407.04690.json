{"id":"2407.04690","title":"Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for\n  Interpreting Neural Networks","authors":"Aaron Mueller","authorsParsed":[["Mueller","Aaron",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 17:53:03 GMT"}],"updateDate":"2024-07-08","timestamp":1720201983000,"abstract":"  Interpretability research takes counterfactual theories of causality for\ngranted. Most causal methods rely on counterfactual interventions to inputs or\nthe activations of particular model components, followed by observations of the\nchange in models' output logits or behaviors. While this yields more faithful\nevidence than correlational methods, counterfactuals nonetheless have key\nproblems that bias our findings in specific and predictable ways. Specifically,\n(i) counterfactual theories do not effectively capture multiple independently\nsufficient causes of the same effect, which leads us to miss certain causes\nentirely; and (ii) counterfactual dependencies in neural networks are generally\nnot transitive, which complicates methods for extracting and interpreting\ncausal graphs from neural networks. We discuss the implications of these\nchallenges for interpretability researchers and propose concrete suggestions\nfor future work.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}