{"id":"2407.17777","title":"Advancing Multi-Modal Sensing Through Expandable Modality Alignment","authors":"Shenghong Dai, Shiqi Jiang, Yifan Yang, Ting Cao, Mo Li, Suman\n  Banerjee, Lili Qiu","authorsParsed":[["Dai","Shenghong",""],["Jiang","Shiqi",""],["Yang","Yifan",""],["Cao","Ting",""],["Li","Mo",""],["Banerjee","Suman",""],["Qiu","Lili",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 05:10:48 GMT"}],"updateDate":"2024-08-30","timestamp":1721884248000,"abstract":"  Sensing technology is widely used for comprehending the physical world, with\nnumerous modalities explored in past decades. While there has been considerable\nwork on multi-modality learning, they all require data of all modalities be\npaired. How to leverage multi-modality data with partially pairings remains an\nopen problem. To tackle this challenge, we introduce the Babel framework,\nencompassing the neural network architecture, data preparation and processing,\nas well as the training strategies. Babel serves as a scalable pre-trained\nmulti-modal sensing neural network, currently aligning six sensing modalities,\nnamely Wi-Fi, mmWave, IMU, LiDAR, video, and depth. To overcome the scarcity of\ncomplete paired data, the key idea of Babel involves transforming the\nN-modality alignment into a series of two-modality alignments by devising the\nexpandable network architecture. This concept is also realized via a series of\nnovel techniques, including the pre-trained modality tower that capitalizes on\navailable single-modal networks, and the adaptive training strategy balancing\nthe contribution of the newly incorporated modality with the previously\nestablished modality alignment.\n  Evaluation demonstrates Babel's outstanding performance on eight human\nactivity recognition datasets, compared to various baselines e.g., the top\nmulti-modal sensing framework, single-modal sensing networks, and multi-modal\nlarge language models. Babel not only effectively fuses multiple available\nmodalities (up to 22% accuracy increase), but also enhance the performance of\nindividual modality (12% averaged accuracy improvement). Case studies also\nhighlight exciting application scenarios empowered by Babel, including\ncross-modality retrieval (i.e., sensing imaging), and bridging LLM for sensing\ncomprehension.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}