{"id":"2408.14040","title":"Evaluating The Explainability of State-of-the-Art Machine Learning-based\n  IoT Network Intrusion Detection Systems","authors":"Ayush Kumar and Vrizlynn L.L. Thing","authorsParsed":[["Kumar","Ayush",""],["Thing","Vrizlynn L. L.",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 06:28:15 GMT"}],"updateDate":"2024-08-27","timestamp":1724653695000,"abstract":"  Internet-of-Things (IoT) Network Intrusion Detection Systems (NIDSs) which\nuse machine learning (ML) models achieve high detection performance and\naccuracy while avoiding dependence on fixed signatures extracted from attack\nartifacts. However, there is a noticeable hesitance among network security\nexperts and practitioners when it comes to deploying ML-based NIDSs in\nreal-world production environments due to their black-box nature, i.e., how and\nwhy the underlying models make their decisions. In this work, we analyze\nstate-of-the-art ML-based IoT NIDS models using explainable AI (xAI) techniques\n(e.g., TRUSTEE, SHAP). Using the explanations generated for the models'\ndecisions, the most prominent features used by each NIDS model considered are\npresented. We compare the explanations generated across xAI methods for a given\nNIDS model as well as the explanations generated across the NIDS models for a\ngiven xAI method. Finally, we evaluate the vulnerability of each NIDS model to\ninductive bias (artifacts learnt from training data). The results show that:\n(1) some ML-based IoT NIDS models can be better explained than other models,\n(2) xAI explanations are in conflict for most of the IoT NIDS models considered\nin this work and (3) some IoT NIDS models are more vulnerable to inductive bias\nthan other models.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}