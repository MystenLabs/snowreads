{"id":"2408.00244","title":"Enhanced Structured State Space Models via Grouped FIR Filtering and\n  Attention Sink Mechanisms","authors":"Tian Meng, Yang Tao, Wuliang Yin","authorsParsed":[["Meng","Tian",""],["Tao","Yang",""],["Yin","Wuliang",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 02:49:58 GMT"}],"updateDate":"2024-08-02","timestamp":1722480598000,"abstract":"  Structured State Space Models (SSMs) have emerged as compelling alternatives\nto Transformer architectures, offering linear-time complexity and superior\nperformance in various sequence modeling tasks. Despite their advantages, SSMs\nlike the original Mamba-2 face training difficulties due to the sensitivities\nintroduced by the extended series of recurrent matrix multiplications. In this\npaper, we propose an advanced architecture that mitigates these challenges by\ndecomposing A-multiplications into multiple groups and optimizing positional\nencoding through Grouped Finite Impulse Response (FIR) filtering. This new\nstructure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable\nmatrices for efficient computation. Furthermore, inspired by the \"attention\nsink\" phenomenon identified in streaming language models, we incorporate a\nsimilar mechanism to enhance the stability and performance of our model over\nextended sequences. Our approach further bridges the gap between SSMs and\nTransformer architectures, offering a viable path forward for scalable and\nhigh-performing sequence modeling.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KiGrN3t8nkCG9NUPUJ0R8oPFGRYXacZ4-Syam9WNaEQ","pdfSize":"352486","txDigest":"BRXF3bCDCbABi4Ko18A7s5EpyMj5LkPC7d3kSbxLhPc9","endEpoch":"1","status":"CERTIFIED"}
