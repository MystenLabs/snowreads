{"id":"2407.10805","title":"Think-on-Graph 2.0: Deep and Interpretable Large Language Model\n  Reasoning with Knowledge Graph-guided Retrieval","authors":"Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo","authorsParsed":[["Ma","Shengjie",""],["Xu","Chengjin",""],["Jiang","Xuhui",""],["Li","Muzhi",""],["Qu","Huaren",""],["Guo","Jian",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:20:40 GMT"},{"version":"v2","created":"Mon, 5 Aug 2024 09:51:15 GMT"},{"version":"v3","created":"Tue, 6 Aug 2024 06:47:06 GMT"}],"updateDate":"2024-08-07","timestamp":1721056840000,"abstract":"  Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}