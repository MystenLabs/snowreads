{"id":"2408.17383","title":"MoRe Fine-Tuning with 10x Fewer Parameters","authors":"Wenxuan Tan, Nicholas Roberts, Tzu-Heng Huang, Jitian Zhao, John\n  Cooper, Samuel Guo, Chengyu Duan, Frederic Sala","authorsParsed":[["Tan","Wenxuan",""],["Roberts","Nicholas",""],["Huang","Tzu-Heng",""],["Zhao","Jitian",""],["Cooper","John",""],["Guo","Samuel",""],["Duan","Chengyu",""],["Sala","Frederic",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 16:24:27 GMT"}],"updateDate":"2024-09-02","timestamp":1725035067000,"abstract":"  Parameter-efficient fine-tuning (PEFT) techniques have unlocked the potential\nto cheaply and easily specialize large pretrained models. However, the most\nprominent approaches, like low-rank adapters (LoRA), depend on heuristics or\nrules-of-thumb for their architectural choices -- potentially limiting their\nperformance for new models and architectures. This limitation suggests that\ntechniques from neural architecture search could be used to obtain optimal\nadapter architectures, but these are often expensive and difficult to\nimplement. We address this challenge with Monarch Rectangular Fine-tuning\n(MoRe), a simple framework to search over adapter architectures that relies on\nthe Monarch matrix class. Theoretically, we show that MoRe is more expressive\nthan LoRA. Empirically, our approach is more parameter-efficient and performant\nthan state-of-the-art PEFTs on a range of tasks and models, with as few as 5\\%\nof LoRA's parameters.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}