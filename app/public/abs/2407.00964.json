{"id":"2407.00964","title":"Multi-Modal Fusion-Based Multi-Task Semantic Communication System","authors":"Zengle Zhu, Rongqing Zhang, Xiang Cheng, Liuqing Yang","authorsParsed":[["Zhu","Zengle",""],["Zhang","Rongqing",""],["Cheng","Xiang",""],["Yang","Liuqing",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 04:49:03 GMT"}],"updateDate":"2024-07-02","timestamp":1719809343000,"abstract":"  In recent years, there has been significant progress in semantic\ncommunication systems empowered by deep learning techniques. It has greatly\nimproved the efficiency of information transmission. Nevertheless, traditional\nsemantic communication models still face challenges, particularly due to their\nsingle-task and single-modal orientation. Many of these models are designed for\nspecific tasks, which may result in limitations when applied to multi-task\ncommunication systems. Moreover, these models often overlook the correlations\namong different modal data in multi-modal tasks. It leads to an incomplete\nunderstanding of complex information, causing increased communication overhead\nand diminished performance. To address these problems, we propose a multi-modal\nfusion-based multi-task semantic communication (MFMSC) framework. In contrast\nto traditional semantic communication approaches, MFMSC can effectively handle\nvarious tasks across multiple modalities. Furthermore, we design a fusion\nmodule based on Bidirectional Encoder Representations from Transformers (BERT)\nfor multi-modal semantic information fusion. By leveraging the powerful\nsemantic understanding capabilities and self-attention mechanism of BERT, we\nachieve effective fusion of semantic information from different modalities. We\ncompare our model with multiple benchmarks. Simulation results show that MFMSC\noutperforms these models in terms of both performance and communication\noverhead.\n","subjects":["Electrical Engineering and Systems Science/Signal Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}