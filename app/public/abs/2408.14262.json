{"id":"2408.14262","title":"Self-supervised Speech Representations Still Struggle with African\n  American Vernacular English","authors":"Kalvin Chang, Yi-Hui Chou, Jiatong Shi, Hsuan-Ming Chen, Nicole\n  Holliday, Odette Scharenborg, David R. Mortensen","authorsParsed":[["Chang","Kalvin",""],["Chou","Yi-Hui",""],["Shi","Jiatong",""],["Chen","Hsuan-Ming",""],["Holliday","Nicole",""],["Scharenborg","Odette",""],["Mortensen","David R.",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 13:29:25 GMT"}],"updateDate":"2024-08-27","timestamp":1724678965000,"abstract":"  Underperformance of ASR systems for speakers of African American Vernacular\nEnglish (AAVE) and other marginalized language varieties is a well-documented\nphenomenon, and one that reinforces the stigmatization of these varieties. We\ninvestigate whether or not the recent wave of Self-Supervised Learning (SSL)\nspeech models can close the gap in ASR performance between AAVE and Mainstream\nAmerican English (MAE). We evaluate four SSL models (wav2vec 2.0, HuBERT,\nWavLM, and XLS-R) on zero-shot Automatic Speech Recognition (ASR) for these two\nvarieties and find that these models perpetuate the bias in performance against\nAAVE. Additionally, the models have higher word error rates on utterances with\nmore phonological and morphosyntactic features of AAVE. Despite the success of\nSSL speech models in improving ASR for low resource varieties, SSL pre-training\nalone may not bridge the gap between AAVE and MAE. Our code is publicly\navailable at https://github.com/cmu-llab/s3m-aave.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}