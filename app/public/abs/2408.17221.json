{"id":"2408.17221","title":"Geometry of Lightning Self-Attention: Identifiability and Dimension","authors":"Nathan W. Henry, Giovanni Luca Marchetti, Kathl\\'en Kohn","authorsParsed":[["Henry","Nathan W.",""],["Marchetti","Giovanni Luca",""],["Kohn","Kathl√©n",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 12:00:36 GMT"}],"updateDate":"2024-09-02","timestamp":1725019236000,"abstract":"  We consider function spaces defined by self-attention networks without\nnormalization, and theoretically analyze their geometry. Since these networks\nare polynomial, we rely on tools from algebraic geometry. In particular, we\nstudy the identifiability of deep attention by providing a description of the\ngeneric fibers of the parametrization for an arbitrary number of layers and, as\na consequence, compute the dimension of the function space. Additionally, for a\nsingle-layer model, we characterize the singular and boundary points. Finally,\nwe formulate a conjectural extension of our results to normalized\nself-attention networks, prove it for a single layer, and numerically verify it\nin the deep case.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Algebraic Geometry"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PvupoZVp8cV21JNM3QhrBMgC0ouTY3OVQ8THX8G8JTs","pdfSize":"1814222"}
