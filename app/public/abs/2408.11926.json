{"id":"2408.11926","title":"Defining Boundaries: The Impact of Domain Specification on\n  Cross-Language and Cross-Domain Transfer in Machine Translation","authors":"Lia Shahnazaryan, Meriem Beloucif","authorsParsed":[["Shahnazaryan","Lia",""],["Beloucif","Meriem",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 18:28:48 GMT"}],"updateDate":"2024-08-23","timestamp":1724264928000,"abstract":"  Recent advancements in neural machine translation (NMT) have revolutionized\nthe field, yet the dependency on extensive parallel corpora limits progress for\nlow-resource languages. Cross-lingual transfer learning offers a promising\nsolution by utilizing data from high-resource languages but often struggles\nwith in-domain NMT. In this paper, we investigate three pivotal aspects:\nenhancing the domain-specific quality of NMT by fine-tuning domain-relevant\ndata from different language pairs, identifying which domains are transferable\nin zero-shot scenarios, and assessing the impact of language-specific versus\ndomain-specific factors on adaptation effectiveness. Using English as the\nsource language and Spanish for fine-tuning, we evaluate multiple target\nlanguages including Portuguese, Italian, French, Czech, Polish, and Greek. Our\nfindings reveal significant improvements in domain-specific translation\nquality, especially in specialized fields such as medical, legal, and IT,\nunderscoring the importance of well-defined domain data and transparency of the\nexperiment setup in in-domain transfer learning.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2E1AlXwTWnrNRYuNy-XJZd60xLnUtzBZGz-JedvXmt4","pdfSize":"262752"}
