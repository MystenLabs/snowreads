{"id":"2407.15077","title":"B2MAPO: A Batch-by-Batch Multi-Agent Policy Optimization to Balance\n  Performance and Efficiency","authors":"Wenjing Zhang, Wei Zhang, Wenqing Hu, and Yifan Wang","authorsParsed":[["Zhang","Wenjing",""],["Zhang","Wei",""],["Hu","Wenqing",""],["Wang","Yifan",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 06:58:14 GMT"},{"version":"v2","created":"Mon, 29 Jul 2024 10:24:39 GMT"}],"updateDate":"2024-07-30","timestamp":1721545094000,"abstract":"  Most multi-agent reinforcement learning approaches adopt two types of policy\noptimization methods that either update policy simultaneously or sequentially.\nSimultaneously updating policies of all agents introduces non-stationarity\nproblem. Although sequentially updating policies agent-by-agent in an\nappropriate order improves policy performance, it is prone to low efficiency\ndue to sequential execution, resulting in longer model training and execution\ntime. Intuitively, partitioning policies of all agents according to their\ninterdependence and updating joint policy batch-by-batch can effectively\nbalance performance and efficiency. However, how to determine the optimal batch\npartition of policies and batch updating order are challenging problems.\nFirstly, a sequential batched policy updating scheme, B2MAPO (Batch by Batch\nMulti-Agent Policy Optimization), is proposed with a theoretical guarantee of\nthe monotonic incrementally tightened bound. Secondly, a universal modulized\nplug-and-play B2MAPO hierarchical framework, which satisfies CTDE principle, is\ndesigned to conveniently integrate any MARL models to fully exploit and merge\ntheir merits, including policy optimality and inference efficiency. Next, a\nDAG-based B2MAPO algorithm is devised, which is a carefully designed\nimplementation of B2MAPO framework. Comprehensive experimental results\nconducted on StarCraftII Multi-agent Challenge and Google Football Research\ndemonstrate the performance of DAG-based B2MAPO algorithm outperforms baseline\nmethods. Meanwhile, compared with A2PO, our algorithm reduces the model\ntraining and execution time by 60.4% and 78.7%, respectively.\n","subjects":["Computing Research Repository/Multiagent Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}