{"id":"2408.04961","title":"In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic\n  Segmentation","authors":"Dahyun Kang and Minsu Cho","authorsParsed":[["Kang","Dahyun",""],["Cho","Minsu",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 09:28:35 GMT"}],"updateDate":"2024-08-12","timestamp":1723195715000,"abstract":"  We present lazy visual grounding, a two-stage approach of unsupervised object\nmask discovery followed by object grounding, for open-vocabulary semantic\nsegmentation. Plenty of the previous art casts this task as pixel-to-text\nclassification without object-level comprehension, leveraging the image-to-text\nclassification capability of pretrained vision-and-language models. We argue\nthat visual objects are distinguishable without the prior text information as\nsegmentation is essentially a vision task. Lazy visual grounding first\ndiscovers object masks covering an image with iterative Normalized cuts and\nthen later assigns text on the discovered objects in a late interaction manner.\nOur model requires no additional training yet shows great performance on five\npublic datasets: Pascal VOC, Pascal Context, COCO-object, COCO-stuff, and ADE\n20K. Especially, the visually appealing segmentation results demonstrate the\nmodel capability to localize objects precisely. Paper homepage:\nhttps://cvlab.postech.ac.kr/research/lazygrounding\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"xi4SiVBqcgxkHpvDw-mNiBi61QXYqZKGGLo2WThJxNs","pdfSize":"7802544"}
