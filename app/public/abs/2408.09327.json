{"id":"2408.09327","title":"Threshold Filtering Packing for Supervised Fine-Tuning: Training Related\n  Samples within Packs","authors":"Jiancheng Dong, Lei Jiang, Wei Jin, Lu Cheng","authorsParsed":[["Dong","Jiancheng",""],["Jiang","Lei",""],["Jin","Wei",""],["Cheng","Lu",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 01:59:41 GMT"}],"updateDate":"2024-08-20","timestamp":1723946381000,"abstract":"  Packing for Supervised Fine-Tuning (SFT) in autoregressive models involves\nconcatenating data points of varying lengths until reaching the designed\nmaximum length to facilitate GPU processing. However, randomly concatenating\ndata points and feeding them into an autoregressive transformer can lead to\ncross-contamination of sequences due to the significant difference in their\nsubject matter. The mainstream approaches in SFT ensure that each token in the\nattention calculation phase only focuses on tokens within its own short\nsequence, without providing additional learning signals for the preceding\ncontext. To address these challenges, we introduce Threshold Filtering Packing\n(TFP), a method that selects samples with related context while maintaining\nsufficient diversity within the same pack. Our experiments show that TFP offers\na simple-to-implement and scalable approach that significantly enhances SFT\nperformance, with observed improvements of up to 7\\% on GSM8K, 4\\% on\nHumanEval, and 15\\% on the adult-census-income dataset.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}