{"id":"2407.19721","title":"Rina: Enhancing Ring-AllReduce with In-network Aggregation in\n  Distributed Model Training","authors":"Zixuan Chen, Xuandong Liu, Minglin Li, Yinfan Hu, Hao Mei, Huifeng\n  Xing, Hao Wang, Wanxin Shi, Sen Liu, and Yang Xu","authorsParsed":[["Chen","Zixuan",""],["Liu","Xuandong",""],["Li","Minglin",""],["Hu","Yinfan",""],["Mei","Hao",""],["Xing","Huifeng",""],["Wang","Hao",""],["Shi","Wanxin",""],["Liu","Sen",""],["Xu","Yang",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 06:06:10 GMT"}],"updateDate":"2024-07-30","timestamp":1722233170000,"abstract":"  Parameter Server (PS) and Ring-AllReduce (RAR) are two widely utilized\nsynchronization architectures in multi-worker Deep Learning (DL), also referred\nto as Distributed Deep Learning (DDL). However, PS encounters challenges with\nthe ``incast'' issue, while RAR struggles with problems caused by the long\ndependency chain. The emerging In-network Aggregation (INA) has been proposed\nto integrate with PS to mitigate its incast issue. However, such PS-based INA\nhas poor incremental deployment abilities as it requires replacing all the\nswitches to show significant performance improvement, which is not\ncost-effective. In this study, we present the incorporation of INA capabilities\ninto RAR, called RAR with In-Network Aggregation (Rina), to tackle both the\nproblems above. Rina features its agent-worker mechanism. When an INA-capable\nToR switch is deployed, all workers in this rack run as one abstracted worker\nwith the help of the agent, resulting in both excellent incremental deployment\ncapabilities and better throughput. We conducted extensive testbed and\nsimulation evaluations to substantiate the throughput advantages of Rina over\nexisting DDL training synchronization structures. Compared with the\nstate-of-the-art PS-based INA methods ATP, Rina can achieve more than 50\\%\nthroughput with the same hardware cost.\n","subjects":["Computing Research Repository/Networking and Internet Architecture","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}