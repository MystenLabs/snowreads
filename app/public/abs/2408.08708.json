{"id":"2408.08708","title":"Decoupling Feature Representations of Ego and Other Modalities for\n  Incomplete Multi-modal Brain Tumor Segmentation","authors":"Kaixiang Yang, Wenqi Shan, Xudong Li, Xuan Wang, Xikai Yang, Xi Wang,\n  Pheng-Ann Heng, Qiang Li, Zhiwei Wang","authorsParsed":[["Yang","Kaixiang",""],["Shan","Wenqi",""],["Li","Xudong",""],["Wang","Xuan",""],["Yang","Xikai",""],["Wang","Xi",""],["Heng","Pheng-Ann",""],["Li","Qiang",""],["Wang","Zhiwei",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 12:43:11 GMT"}],"updateDate":"2024-08-19","timestamp":1723812191000,"abstract":"  Multi-modal brain tumor segmentation typically involves four magnetic\nresonance imaging (MRI) modalities, while incomplete modalities significantly\ndegrade performance. Existing solutions employ explicit or implicit modality\nadaptation, aligning features across modalities or learning a fused feature\nrobust to modality incompleteness. They share a common goal of encouraging each\nmodality to express both itself and the others. However, the two expression\nabilities are entangled as a whole in a seamless feature space, resulting in\nprohibitive learning burdens. In this paper, we propose DeMoSeg to enhance the\nmodality adaptation by Decoupling the task of representing the ego and other\nModalities for robust incomplete multi-modal Segmentation. The decoupling is\nsuper lightweight by simply using two convolutions to map each modality onto\nfour feature sub-spaces. The first sub-space expresses itself (Self-feature),\nwhile the remaining sub-spaces substitute for other modalities\n(Mutual-features). The Self- and Mutual-features interactively guide each other\nthrough a carefully-designed Channel-wised Sparse Self-Attention (CSSA). After\nthat, a Radiologist-mimic Cross-modality expression Relationships (RCR) is\nintroduced to have available modalities provide Self-feature and also `lend'\ntheir Mutual-features to compensate for the absent ones by exploiting the\nclinical prior knowledge. The benchmark results on BraTS2020, BraTS2018 and\nBraTS2015 verify the DeMoSeg's superiority thanks to the alleviated modality\nadaptation difficulty. Concretely, for BraTS2020, DeMoSeg increases Dice by at\nleast 0.92%, 2.95% and 4.95% on whole tumor, tumor core and enhanced tumor\nregions, respectively, compared to other state-of-the-arts. Codes are at\nhttps://github.com/kk42yy/DeMoSeg\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}