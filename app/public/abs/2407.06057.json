{"id":"2407.06057","title":"Variational Best-of-N Alignment","authors":"Afra Amini, Tim Vieira, Ryan Cotterell","authorsParsed":[["Amini","Afra",""],["Vieira","Tim",""],["Cotterell","Ryan",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 15:59:44 GMT"}],"updateDate":"2024-07-09","timestamp":1720454384000,"abstract":"  Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on a controlled\ngeneration task suggest that while variational BoN is not as effective as BoN\nin aligning language models, it is close to BoN performance as vBoN appears\nmore often on the Pareto frontier of reward and KL divergence compared to\nmodels trained with KL-constrained RL objective.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}