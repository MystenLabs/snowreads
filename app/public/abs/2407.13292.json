{"id":"2407.13292","title":"Low-Resourced Speech Recognition for Iu Mien Language via\n  Weakly-Supervised Phoneme-based Multilingual Pre-training","authors":"Lukuan Dong, Donghong Qin, Fengbo Bai, Fanhua Song, Yan Liu, Chen Xu,\n  Zhijian Ou","authorsParsed":[["Dong","Lukuan",""],["Qin","Donghong",""],["Bai","Fengbo",""],["Song","Fanhua",""],["Liu","Yan",""],["Xu","Chen",""],["Ou","Zhijian",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 08:46:47 GMT"},{"version":"v2","created":"Mon, 16 Sep 2024 08:23:30 GMT"}],"updateDate":"2024-09-17","timestamp":1721292407000,"abstract":"  The mainstream automatic speech recognition (ASR) technology usually requires\nhundreds to thousands of hours of annotated speech data. Three approaches to\nlow-resourced ASR are phoneme or subword based supervised pre-training, and\nself-supervised pre-training over multilingual data. The Iu Mien language is\nthe main ethnic language of the Yao ethnic group in China and is low-resourced\nin the sense that the annotated speech is very limited. With less than 10 hours\nof transcribed Iu Mien language, this paper investigates and compares the three\napproaches for Iu Mien speech recognition. Our experiments are based on the\nrecently released, three backbone models pretrained over the 10 languages from\nthe CommonVoice dataset (CV-Lang10), which correspond to the three approaches\nfor low-resourced ASR. It is found that phoneme supervision can achieve better\nresults compared to subword supervision and self-supervision, thereby providing\nhigher data-efficiency. Particularly, the Whistle models, i.e., obtained by the\nweakly-supervised phoneme-based multilingual pre-training, obtain the most\ncompetitive results.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}