{"id":"2407.06542","title":"LIONs: An Empirically Optimized Approach to Align Language Models","authors":"Xiao Yu, Qingyang Wu, Yu Li, Zhou Yu","authorsParsed":[["Yu","Xiao",""],["Wu","Qingyang",""],["Li","Yu",""],["Yu","Zhou",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 04:34:39 GMT"}],"updateDate":"2024-07-10","timestamp":1720499679000,"abstract":"  Alignment is a crucial step to enhance the instruction-following and\nconversational abilities of language models. Despite many recent work proposing\nnew algorithms, datasets, and training pipelines, there is a lack of\ncomprehensive studies measuring the impact of various design choices throughout\nthe whole training process. We first conduct a rigorous analysis over a\nthree-stage training pipeline consisting of supervised fine-tuning, offline\npreference learning, and online preference learning. We have found that using\ntechniques like sequence packing, loss masking in SFT, increasing the\npreference dataset size in DPO, and online DPO training can significantly\nimprove the performance of language models. We then train from Gemma-2b-base\nand LLama-3-8b-base, and find that our best models exceed the performance of\nthe official instruct models tuned with closed-source data and algorithms. Our\ncode and models can be found at\nhttps://github.com/Columbia-NLP-Lab/LionAlignment.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}