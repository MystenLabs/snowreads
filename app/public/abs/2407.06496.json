{"id":"2407.06496","title":"It's Our Loss: No Privacy Amplification for Hidden State DP-SGD With\n  Non-Convex Loss","authors":"Meenatchi Sundaram Muthu Selva Annamalai","authorsParsed":[["Annamalai","Meenatchi Sundaram Muthu Selva",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 01:58:19 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 01:20:25 GMT"}],"updateDate":"2024-08-22","timestamp":1720490299000,"abstract":"  Differentially Private Stochastic Gradient Descent (DP-SGD) is a popular\niterative algorithm used to train machine learning models while formally\nguaranteeing the privacy of users. However, the privacy analysis of DP-SGD\nmakes the unrealistic assumption that all intermediate iterates (aka internal\nstate) of the algorithm are released since, in practice, only the final trained\nmodel, i.e., the final iterate of the algorithm is released. In this hidden\nstate setting, prior work has provided tighter analyses, albeit only when the\nloss function is constrained, e.g., strongly convex and smooth or linear. On\nthe other hand, the privacy leakage observed empirically from hidden state\nDP-SGD, even when using non-convex loss functions, suggests that there is in\nfact a gap between the theoretical privacy analysis and the privacy guarantees\nachieved in practice. Therefore, it remains an open question whether hidden\nstate privacy amplification for DP-SGD is possible for all (possibly\nnon-convex) loss functions in general.\n  In this work, we design a counter-example and show, both theoretically and\nempirically, that a hidden state privacy amplification result for DP-SGD for\nall loss functions in general is not possible. By carefully constructing a loss\nfunction for DP-SGD, we show that for specific loss functions, the final\niterate of DP-SGD alone leaks as much information as the sequence of all\niterates combined. Furthermore, we empirically verify this result by evaluating\nthe privacy leakage from the final iterate of DP-SGD with our loss function and\nshow that this exactly matches the theoretical upper bound guaranteed by DP.\nTherefore, we show that the current privacy analysis for DP-SGD is tight for\ngeneral loss functions and conclude that no privacy amplification is possible\nfor DP-SGD in general for all (possibly non-convex) loss functions.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}