{"id":"2407.12068","title":"Learning on Graphs with Large Language Models(LLMs): A Deep Dive into\n  Model Robustness","authors":"Kai Guo, Zewen Liu, Zhikai Chen, Hongzhi Wen, Wei Jin, Jiliang Tang,\n  Yi Chang","authorsParsed":[["Guo","Kai",""],["Liu","Zewen",""],["Chen","Zhikai",""],["Wen","Hongzhi",""],["Jin","Wei",""],["Tang","Jiliang",""],["Chang","Yi",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 09:05:31 GMT"},{"version":"v2","created":"Sun, 28 Jul 2024 16:44:21 GMT"}],"updateDate":"2024-07-30","timestamp":1721120731000,"abstract":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing tasks. Recently, several LLMs-based\npipelines have been developed to enhance learning on graphs with text\nattributes, showcasing promising performance. However, graphs are well-known to\nbe susceptible to adversarial attacks and it remains unclear whether LLMs\nexhibit robustness in learning on graphs. To address this gap, our work aims to\nexplore the potential of LLMs in the context of adversarial attacks on graphs.\nSpecifically, we investigate the robustness against graph structural and\ntextual perturbations in terms of two dimensions: LLMs-as-Enhancers and\nLLMs-as-Predictors. Through extensive experiments, we find that, compared to\nshallow models, both LLMs-as-Enhancers and LLMs-as-Predictors offer superior\nrobustness against structural and textual attacks.Based on these findings, we\ncarried out additional analyses to investigate the underlying causes.\nFurthermore, we have made our benchmark library openly available to facilitate\nquick and fair evaluations, and to encourage ongoing innovative research in\nthis field.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}