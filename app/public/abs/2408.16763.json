{"id":"2408.16763","title":"Finite Sample Valid Inference via Calibrated Bootstrap","authors":"Yiran Jiang, Chuanhai Liu, Heping Zhang","authorsParsed":[["Jiang","Yiran",""],["Liu","Chuanhai",""],["Zhang","Heping",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 17:57:22 GMT"}],"updateDate":"2024-08-30","timestamp":1724954242000,"abstract":"  While widely used as a general method for uncertainty quantification, the\nbootstrap method encounters difficulties that raise concerns about its validity\nin practical applications. This paper introduces a new resampling-based method,\ntermed $\\textit{calibrated bootstrap}$, designed to generate finite\nsample-valid parametric inference from a sample of size $n$. The central idea\nis to calibrate an $m$-out-of-$n$ resampling scheme, where the calibration\nparameter $m$ is determined against inferential pivotal quantities derived from\nthe cumulative distribution functions of loss functions in parameter\nestimation. The method comprises two algorithms. The first, named\n$\\textit{resampling approximation}$ (RA), employs a $\\textit{stochastic\napproximation}$ algorithm to find the value of the calibration parameter\n$m=m_\\alpha$ for a given $\\alpha$ in a manner that ensures the resulting\n$m$-out-of-$n$ bootstrapped $1-\\alpha$ confidence set is valid. The second\nalgorithm, termed $\\textit{distributional resampling}$ (DR), is developed to\nfurther select samples of bootstrapped estimates from the RA step when\nconstructing $1-\\alpha$ confidence sets for a range of $\\alpha$ values is of\ninterest. The proposed method is illustrated and compared to existing methods\nusing linear regression with and without $L_1$ penalty, within the context of a\nhigh-dimensional setting and a real-world data application. The paper concludes\nwith remarks on a few open problems worthy of consideration.\n","subjects":["Statistics/Methodology","Statistics/Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}