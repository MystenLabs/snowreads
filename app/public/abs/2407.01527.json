{"id":"2407.01527","title":"KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches","authors":"Jiayi Yuan, Hongyi Liu, Shaochen (Henry) Zhong, Yu-Neng Chuang,\n  Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu,\n  Zirui Liu, Xia Hu","authorsParsed":[["Yuan","Jiayi","","Henry"],["Liu","Hongyi","","Henry"],["Shaochen","","","Henry"],["Zhong","",""],["Chuang","Yu-Neng",""],["Li","Songchen",""],["Wang","Guanchu",""],["Le","Duy",""],["Jin","Hongye",""],["Chaudhary","Vipin",""],["Xu","Zhaozhuo",""],["Liu","Zirui",""],["Hu","Xia",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:59:47 GMT"}],"updateDate":"2024-07-02","timestamp":1719856787000,"abstract":"  Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches -- such as KV cache quantization, token dropping, prompt\ncompression, linear-time sequence models, and hybrid architectures -- have been\nproposed to produce efficient yet long context-capable models. Despite these\nadvancements, no existing work has comprehensively benchmarked these methods in\na reasonably aligned environment. In this work, we fill this gap by providing a\ntaxonomy of current methods and evaluating 10+ state-of-the-art approaches\nacross seven categories of long context tasks. Our work reveals numerous\npreviously unknown phenomena and offers insights -- as well as a friendly\nworkbench -- for the future development of long context-capable LLMs. The\nsource code will be available at https://github.com/henryzhongsc/longctx_bench\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LSwRuo_QM5EVpnqbduTQAAABOZKxHmTsv377WCuEFJw","pdfSize":"1869503"}
