{"id":"2407.08330","title":"HDT: Hierarchical Document Transformer","authors":"Haoyu He, Markus Flicke, Jan Buchmann, Iryna Gurevych, Andreas Geiger","authorsParsed":[["He","Haoyu",""],["Flicke","Markus",""],["Buchmann","Jan",""],["Gurevych","Iryna",""],["Geiger","Andreas",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 09:28:04 GMT"}],"updateDate":"2024-07-12","timestamp":1720690084000,"abstract":"  In this paper, we propose the Hierarchical Document Transformer (HDT), a\nnovel sparse Transformer architecture tailored for structured hierarchical\ndocuments. Such documents are extremely important in numerous domains,\nincluding science, law or medicine. However, most existing solutions are\ninefficient and fail to make use of the structure inherent to documents. HDT\nexploits document structure by introducing auxiliary anchor tokens and\nredesigning the attention mechanism into a sparse multi-level hierarchy. This\napproach facilitates information exchange between tokens at different levels\nwhile maintaining sparsity, thereby enhancing computational and memory\nefficiency while exploiting the document structure as an inductive bias. We\naddress the technical challenge of implementing HDT's sample-dependent\nhierarchical attention pattern by developing a novel sparse attention kernel\nthat considers the hierarchical structure of documents. As demonstrated by our\nexperiments, utilizing structural information present in documents leads to\nfaster convergence, higher sample efficiency and better performance on\ndownstream tasks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}