{"id":"2407.07531","title":"Beyond Benchmarking: A New Paradigm for Evaluation and Assessment of\n  Large Language Models","authors":"Jin Liu and Qingquan Li and Wenlong Du","authorsParsed":[["Liu","Jin",""],["Li","Qingquan",""],["Du","Wenlong",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 10:42:02 GMT"}],"updateDate":"2024-07-11","timestamp":1720608122000,"abstract":"  In current benchmarks for evaluating large language models (LLMs), there are\nissues such as evaluation content restriction, untimely updates, and lack of\noptimization guidance. In this paper, we propose a new paradigm for the\nmeasurement of LLMs: Benchmarking-Evaluation-Assessment. Our paradigm shifts\nthe \"location\" of LLM evaluation from the \"examination room\" to the \"hospital\".\nThrough conducting a \"physical examination\" on LLMs, it utilizes specific\ntask-solving as the evaluation content, performs deep attribution of existing\nproblems within LLMs, and provides recommendation for optimization.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}