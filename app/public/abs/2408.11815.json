{"id":"2408.11815","title":"Great Memory, Shallow Reasoning: Limits of $k$NN-LMs","authors":"Shangyi Geng, Wenting Zhao, Alexander M Rush","authorsParsed":[["Geng","Shangyi",""],["Zhao","Wenting",""],["Rush","Alexander M",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:59:05 GMT"}],"updateDate":"2024-08-22","timestamp":1724263145000,"abstract":"  $K$-nearest neighbor language models ($k$NN-LMs), which integrate retrieval\nwith next-word prediction, have demonstrated strong performance in language\nmodeling as well as downstream NLP benchmarks. These results have led\nresearchers to argue that models trained on poor quality or outdated data could\nperform well by employing a $k$NN extension that has access to a higher-quality\ndatastore. In this work, we ask whether this improved ability to recall\ninformation really translates into downstream abilities. We extensively\nevaluate $k$NN-LMs on a diverse set of tasks, ranging from sentiment\nclassification and commonsense reasoning to multi-hop reasoning. Results show\nthat $k$NN-LMs excel at memory-intensive tasks, where utilizing the patterns in\nthe input is sufficient for determining the output, but struggle with reasoning\ntasks that require integrating multiple pieces of information to derive new\nknowledge. We further demonstrate through oracle experiments and qualitative\nanalysis that even with perfect retrieval, $k$NN-LMs still fail to determine\nthe correct answers, placing an upper bound on their reasoning performance.\nCode and datastores are released at https://github.com/GSYfate/knnlm-limits/.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8eNoowfEy0ozjVeA1V3lEPmgbfa0Za8-K8FNSXXrk0E","pdfSize":"416806"}
