{"id":"2407.15429","title":"Learning at a Glance: Towards Interpretable Data-limited Continual\n  Semantic Segmentation via Semantic-Invariance Modelling","authors":"Bo Yuan, Danpei Zhao, Zhenwei Shi","authorsParsed":[["Yuan","Bo",""],["Zhao","Danpei",""],["Shi","Zhenwei",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 07:17:52 GMT"}],"updateDate":"2024-07-23","timestamp":1721632672000,"abstract":"  Continual semantic segmentation (CSS) based on incremental learning (IL) is a\ngreat endeavour in developing human-like segmentation models. However, current\nCSS approaches encounter challenges in the trade-off between preserving old\nknowledge and learning new ones, where they still need large-scale annotated\ndata for incremental training and lack interpretability. In this paper, we\npresent Learning at a Glance (LAG), an efficient, robust, human-like and\ninterpretable approach for CSS. Specifically, LAG is a simple and\nmodel-agnostic architecture, yet it achieves competitive CSS efficiency with\nlimited incremental data. Inspired by human-like recognition patterns, we\npropose a semantic-invariance modelling approach via semantic features\ndecoupling that simultaneously reconciles solid knowledge inheritance and\nnew-term learning. Concretely, the proposed decoupling manner includes two\nways, i.e., channel-wise decoupling and spatial-level neuron-relevant semantic\nconsistency. Our approach preserves semantic-invariant knowledge as solid\nprototypes to alleviate catastrophic forgetting, while also constraining\nsample-specific contents through an asymmetric contrastive learning method to\nenhance model robustness during IL steps. Experimental results in multiple\ndatasets validate the effectiveness of the proposed method. Furthermore, we\nintroduce a novel CSS protocol that better reflects realistic data-limited CSS\nsettings, and LAG achieves superior performance under multiple data-limited\nconditions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}