{"id":"2408.07505","title":"Large Language Models Know What Makes Exemplary Contexts","authors":"Quanyu Long, Jianda Chen, Wenya Wang and Sinno Jialin Pan","authorsParsed":[["Long","Quanyu",""],["Chen","Jianda",""],["Wang","Wenya",""],["Pan","Sinno Jialin",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 12:32:41 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 06:50:48 GMT"}],"updateDate":"2024-08-21","timestamp":1723638761000,"abstract":"  In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}