{"id":"2408.17207","title":"NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on\n  Prompt-Guided Camera and 4D mmWave Radar","authors":"Runwei Guan, Jianan Liu, Liye Jia, Haocheng Zhao, Shanliang Yao,\n  Xiaohui Zhu, Ka Lok Man, Eng Gee Lim, Jeremy Smith, Yutao Yue","authorsParsed":[["Guan","Runwei",""],["Liu","Jianan",""],["Jia","Liye",""],["Zhao","Haocheng",""],["Yao","Shanliang",""],["Zhu","Xiaohui",""],["Man","Ka Lok",""],["Lim","Eng Gee",""],["Smith","Jeremy",""],["Yue","Yutao",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 11:22:09 GMT"}],"updateDate":"2024-09-02","timestamp":1725016929000,"abstract":"  Recently, visual grounding and multi-sensors setting have been incorporated\ninto perception system for terrestrial autonomous driving systems and Unmanned\nSurface Vehicles (USVs), yet the high complexity of modern learning-based\nvisual grounding model using multi-sensors prevents such model to be deployed\non USVs in the real-life. To this end, we design a low-power multi-task model\nnamed NanoMVG for waterway embodied perception, guiding both camera and 4D\nmillimeter-wave radar to locate specific object(s) through natural language.\nNanoMVG can perform both box-level and mask-level visual grounding tasks\nsimultaneously. Compared to other visual grounding models, NanoMVG achieves\nhighly competitive performance on the WaterVG dataset, particularly in harsh\nenvironments and boasts ultra-low power consumption for long endurance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}