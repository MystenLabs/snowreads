{"id":"2407.08970","title":"Soft Prompts Go Hard: Steering Visual Language Models with Hidden\n  Meta-Instructions","authors":"Tingwei Zhang, Collin Zhang, John X. Morris, Eugene Bagdasarian,\n  Vitaly Shmatikov","authorsParsed":[["Zhang","Tingwei",""],["Zhang","Collin",""],["Morris","John X.",""],["Bagdasarian","Eugene",""],["Shmatikov","Vitaly",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 03:40:13 GMT"},{"version":"v2","created":"Fri, 6 Sep 2024 20:46:34 GMT"}],"updateDate":"2024-09-10","timestamp":1720755613000,"abstract":"  We introduce a new type of indirect injection attacks against language models\nthat operate on images: hidden ''meta-instructions'' that influence how the\nmodel interprets the image and steer the model's outputs to express an\nadversary-chosen style, sentiment, or point of view.\n  We explain how to create meta-instructions by generating images that act as\nsoft prompts. In contrast to jailbreaking attacks and adversarial examples,\noutputs produced in response to these images are plausible and based on the\nvisual content of the image, yet also satisfy the adversary's (meta-)objective.\n  We evaluate the efficacy of meta-instructions for multiple visual language\nmodels and adversarial meta-objectives, and demonstrate how they can ''unlock''\ncapabilities of the underlying language models that are unavailable via\nexplicit text instructions. We describe how meta-instruction attacks could\ncause harm by enabling creation of malicious, self-interpreting content that\ncarries spam, misinformation, and spin. Finally, we discuss defenses.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}