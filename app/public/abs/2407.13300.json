{"id":"2407.13300","title":"Robust ASR Error Correction with Conservative Data Filtering","authors":"Takuma Udagawa, Masayuki Suzuki, Masayasu Muraoka, Gakuto Kurata","authorsParsed":[["Udagawa","Takuma",""],["Suzuki","Masayuki",""],["Muraoka","Masayasu",""],["Kurata","Gakuto",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 09:05:49 GMT"}],"updateDate":"2024-07-19","timestamp":1721293549000,"abstract":"  Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings.\n","subjects":["Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}