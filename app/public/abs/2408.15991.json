{"id":"2408.15991","title":"Distribution Backtracking Builds A Faster Convergence Trajectory for\n  One-step Diffusion Distillation","authors":"Shengyuan Zhang, Ling Yang, Zejian Li, An Zhao, Chenye Meng, Changyuan\n  Yang, Guang Yang, Zhiyuan Yang, Lingyun Sun","authorsParsed":[["Zhang","Shengyuan",""],["Yang","Ling",""],["Li","Zejian",""],["Zhao","An",""],["Meng","Chenye",""],["Yang","Changyuan",""],["Yang","Guang",""],["Yang","Zhiyuan",""],["Sun","Lingyun",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 17:58:17 GMT"}],"updateDate":"2024-08-29","timestamp":1724867897000,"abstract":"  Accelerating the sampling speed of diffusion models remains a significant\nchallenge. Recent score distillation methods distill a heavy teacher model into\nan one-step student generator, which is optimized by calculating the difference\nbetween the two score functions on the samples generated by the student model.\nHowever, there is a score mismatch issue in the early stage of the distillation\nprocess, because existing methods mainly focus on using the endpoint of\npre-trained diffusion models as teacher models, overlooking the importance of\nthe convergence trajectory between the student generator and the teacher model.\nTo address this issue, we extend the score distillation process by introducing\nthe entire convergence trajectory of teacher models and propose Distribution\nBacktracking Distillation (DisBack) for distilling student generators. DisBask\nis composed of two stages: Degradation Recording and Distribution Backtracking.\nDegradation Recording is designed to obtain the convergence trajectory of\nteacher models, which records the degradation path from the trained teacher\nmodel to the untrained initial student generator. The degradation path\nimplicitly represents the intermediate distributions of teacher models. Then\nDistribution Backtracking trains a student generator to backtrack the\nintermediate distributions for approximating the convergence trajectory of\nteacher models. Extensive experiments show that DisBack achieves faster and\nbetter convergence than the existing distillation method and accomplishes\ncomparable generation performance. Notably, DisBack is easy to implement and\ncan be generalized to existing distillation methods to boost performance. Our\ncode is publicly available on https://github.com/SYZhang0805/DisBack.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}