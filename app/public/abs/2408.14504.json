{"id":"2408.14504","title":"Is Functional Correctness Enough to Evaluate Code Language Models?\n  Exploring Diversity of Generated Codes","authors":"Heejae Chon, Seonghyeon Lee, Jinyoung Yeo, Dongha Lee","authorsParsed":[["Chon","Heejae",""],["Lee","Seonghyeon",""],["Yeo","Jinyoung",""],["Lee","Dongha",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 07:40:22 GMT"}],"updateDate":"2024-08-28","timestamp":1724485222000,"abstract":"  Language models (LMs) have exhibited impressive abilities in generating codes\nfrom natural language requirements. In this work, we highlight the diversity of\ncode generated by LMs as a critical criterion for evaluating their code\ngeneration capabilities, in addition to functional correctness. Despite its\npractical implications, there is a lack of studies focused on assessing the\ndiversity of generated code, which overlooks its importance in the development\nof code LMs. We propose a systematic approach to evaluate the diversity of\ngenerated code, utilizing various metrics for inter-code similarity as well as\nfunctional correctness. Specifically, we introduce a pairwise code similarity\nmeasure that leverages large LMs' capabilities in code understanding and\nreasoning, demonstrating the highest correlation with human judgment. We\nextensively investigate the impact of various factors on the quality of\ngenerated code, including model sizes, temperatures, training approaches,\nprompting strategies, and the difficulty of input problems. Our consistent\nobservation of a positive correlation between the test pass score and the\ninter-code similarity score indicates that current LMs tend to produce\nfunctionally correct code with limited diversity.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Programming Languages"],"license":"http://creativecommons.org/licenses/by/4.0/"}