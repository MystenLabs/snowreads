{"id":"2408.05914","title":"Deep Multimodal Collaborative Learning for Polyp Re-Identification","authors":"Suncheng Xiang, Jincheng Li, Zhengjie Zhang, Shilun Cai, Jiale Guan,\n  Dahong Qian","authorsParsed":[["Xiang","Suncheng",""],["Li","Jincheng",""],["Zhang","Zhengjie",""],["Cai","Shilun",""],["Guan","Jiale",""],["Qian","Dahong",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 04:05:19 GMT"}],"updateDate":"2024-08-13","timestamp":1723435519000,"abstract":"  Colonoscopic Polyp Re-Identification aims to match the same polyp from a\nlarge gallery with images from different views taken using different cameras\nand plays an important role in the prevention and treatment of colorectal\ncancer in computer-aided diagnosis. However, traditional methods for object\nReID directly adopting CNN models trained on the ImageNet dataset usually\nproduce unsatisfactory retrieval performance on colonoscopic datasets due to\nthe large domain gap. Worsely, these solutions typically learn unimodal modal\nrepresentations on the basis of visual samples, which fails to explore\ncomplementary information from different modalities. To address this challenge,\nwe propose a novel Deep Multimodal Collaborative Learning framework named DMCL\nfor polyp re-identification, which can effectively encourage modality\ncollaboration and reinforce generalization capability in medical scenarios. On\nthe basis of it, a dynamic multimodal feature fusion strategy is introduced to\nleverage the optimized multimodal representations for multimodal fusion via\nend-to-end training. Experiments on the standard benchmarks show the benefits\nof the multimodal setting over state-of-the-art unimodal ReID models,\nespecially when combined with the specialized multimodal fusion strategy.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}