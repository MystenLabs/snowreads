{"id":"2407.14949","title":"CoCoG-2: Controllable generation of visual stimuli for understanding\n  human concept representation","authors":"Chen Wei, Jiachen Zou, Dietmar Heinke, Quanying Liu","authorsParsed":[["Wei","Chen",""],["Zou","Jiachen",""],["Heinke","Dietmar",""],["Liu","Quanying",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 17:52:32 GMT"}],"updateDate":"2024-07-23","timestamp":1721497952000,"abstract":"  Humans interpret complex visual stimuli using abstract concepts that\nfacilitate decision-making tasks such as food selection and risk avoidance.\nSimilarity judgment tasks are effective for exploring these concepts. However,\nmethods for controllable image generation in concept space are underdeveloped.\nIn this study, we present a novel framework called CoCoG-2, which integrates\ngenerated visual stimuli into similarity judgment tasks. CoCoG-2 utilizes a\ntraining-free guidance algorithm to enhance generation flexibility. CoCoG-2\nframework is versatile for creating experimental stimuli based on human\nconcepts, supporting various strategies for guiding visual stimuli generation,\nand demonstrating how these stimuli can validate various experimental\nhypotheses. CoCoG-2 will advance our understanding of the causal relationship\nbetween concept representations and behaviors by generating visual stimuli. The\ncode is available at \\url{https://github.com/ncclab-sustech/CoCoG-2}.\n","subjects":["Quantitative Biology/Neurons and Cognition","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/"}