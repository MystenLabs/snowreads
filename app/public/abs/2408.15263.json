{"id":"2408.15263","title":"S4DL: Shift-sensitive Spatial-Spectral Disentangling Learning for\n  Hyperspectral Image Unsupervised Domain Adaptation","authors":"Jie Feng, Tianshu Zhang, Junpeng Zhang, Ronghua Shang, Weisheng Dong,\n  Guangming Shi and Licheng Jiao","authorsParsed":[["Feng","Jie",""],["Zhang","Tianshu",""],["Zhang","Junpeng",""],["Shang","Ronghua",""],["Dong","Weisheng",""],["Shi","Guangming",""],["Jiao","Licheng",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 15:58:24 GMT"}],"updateDate":"2024-08-29","timestamp":1723391904000,"abstract":"  Unsupervised domain adaptation techniques, extensively studied in\nhyperspectral image (HSI) classification, aim to use labeled source domain data\nand unlabeled target domain data to learn domain invariant features for\ncross-scene classification. Compared to natural images, numerous spectral bands\nof HSIs provide abundant semantic information, but they also increase the\ndomain shift significantly. In most existing methods, both explicit alignment\nand implicit alignment simply align feature distribution, ignoring domain\ninformation in the spectrum. We noted that when the spectral channel between\nsource and target domains is distinguished obviously, the transfer performance\nof these methods tends to deteriorate. Additionally, their performance\nfluctuates greatly owing to the varying domain shifts across various datasets.\nTo address these problems, a novel shift-sensitive spatial-spectral\ndisentangling learning (S4DL) approach is proposed. In S4DL, gradient-guided\nspatial-spectral decomposition is designed to separate domain-specific and\ndomain-invariant representations by generating tailored masks under the\nguidance of the gradient from domain classification. A shift-sensitive adaptive\nmonitor is defined to adjust the intensity of disentangling according to the\nmagnitude of domain shift. Furthermore, a reversible neural network is\nconstructed to retain domain information that lies in not only in semantic but\nalso the shallow-level detailed information. Extensive experimental results on\nseveral cross-scene HSI datasets consistently verified that S4DL is better than\nthe state-of-the-art UDA methods. Our source code will be available at\nhttps://github.com/xdu-jjgs/S4DL.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}