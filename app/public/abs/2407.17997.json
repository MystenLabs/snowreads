{"id":"2407.17997","title":"On the Effect of Purely Synthetic Training Data for Different Automatic\n  Speech Recognition Architectures","authors":"Nick Rossenbach and Benedikt Hilmes and Ralf Schl\\\"uter","authorsParsed":[["Rossenbach","Nick",""],["Hilmes","Benedikt",""],["Schl√ºter","Ralf",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 12:44:45 GMT"}],"updateDate":"2024-07-26","timestamp":1721911485000,"abstract":"  In this work we evaluate the utility of synthetic data for training automatic\nspeech recognition (ASR). We use the ASR training data to train a\ntext-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce\nthe original training data, training ASR systems solely on synthetic data. For\nASR, we use three different architectures, attention-based encoder-decoder,\nhybrid deep neural network hidden Markov model and a Gaussian mixture hidden\nMarkov model, showing the different sensitivity of the models to synthetic data\ngeneration. In order to extend previous work, we present a number of ablation\nstudies on the effectiveness of synthetic vs. real training data for ASR. In\nparticular we focus on how the gap between training on synthetic and real data\nchanges by varying the speaker embedding or by scaling the model size. For the\nlatter we show that the TTS models generalize well, even when training scores\nindicate overfitting.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}