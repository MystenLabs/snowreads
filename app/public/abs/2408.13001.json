{"id":"2408.13001","title":"CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding\n  and Execution","authors":"Ruiyang Xu, Jialun Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Ben He,\n  Shing-Chi Cheung, Le Sun","authorsParsed":[["Xu","Ruiyang",""],["Cao","Jialun",""],["Lu","Yaojie",""],["Lin","Hongyu",""],["Han","Xianpei",""],["He","Ben",""],["Cheung","Shing-Chi",""],["Sun","Le",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 11:43:00 GMT"}],"updateDate":"2024-08-26","timestamp":1724413380000,"abstract":"  Code benchmarks such as HumanEval are widely adopted to evaluate Large\nLanguage Models' (LLMs) coding capabilities. However, there is an unignorable\nprogramming language bias in existing code benchmarks -- over 95% code\ngeneration benchmarks are dominated by Python, leaving the LLMs' capabilities\nin other programming languages such as Java and C/C++ unknown. Moreover, coding\ntask bias is also crucial. Most benchmarks focus on code generation capability,\nwhile benchmarks for code reasoning (given input, reasoning output; and given\noutput, reasoning input), an essential coding capability, are insufficient.\nYet, constructing multi-lingual benchmarks can be expensive and\nlabor-intensive, and codes in contest websites such as Leetcode suffer from\ndata contamination during training. To fill this gap, we propose CRUXEVAL-X, a\nmulti-lingual code reasoning benchmark that contains 19 programming languages.\nIt comprises at least 600 subjects for each language, along with 19K\ncontent-consistent tests in total. In particular, the construction pipeline of\nCRUXEVAL-X works in a fully automated and test-guided manner, which iteratively\ngenerates and repairs based on execution feedback. Also, to cross language\nbarriers (e.g., dynamic/static type systems in Python/C++), we formulated\nvarious transition rules between language pairs to facilitate translation. Our\nintensive evaluation of 24 representative LLMs reveals the correlation between\nlanguage pairs. For example, TypeScript and JavaScript show a significant\npositive correlation, while Racket has less correlation with other languages.\nMore interestingly, even a model trained solely on Python can achieve at most\n34.4% Pass@1 in other languages, revealing the cross-language generalization of\nLLMs.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}