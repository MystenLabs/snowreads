{"id":"2408.09462","title":"SpeechEE: A Novel Benchmark for Speech Event Extraction","authors":"Bin Wang, Meishan Zhang, Hao Fei, Yu Zhao, Bobo Li, Shengqiong Wu, Wei\n  Ji, Min Zhang","authorsParsed":[["Wang","Bin",""],["Zhang","Meishan",""],["Fei","Hao",""],["Zhao","Yu",""],["Li","Bobo",""],["Wu","Shengqiong",""],["Ji","Wei",""],["Zhang","Min",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 12:52:55 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 06:59:10 GMT"}],"updateDate":"2024-08-26","timestamp":1723985575000,"abstract":"  Event extraction (EE) is a critical direction in the field of information\nextraction, laying an important foundation for the construction of structured\nknowledge bases. EE from text has received ample research and attention for\nyears, yet there can be numerous real-world applications that require direct\ninformation acquisition from speech signals, online meeting minutes, interview\nsummaries, press releases, etc. While EE from speech has remained\nunder-explored, this paper fills the gap by pioneering a SpeechEE, defined as\ndetecting the event predicates and arguments from a given audio speech. To\nbenchmark the SpeechEE task, we first construct a large-scale high-quality\ndataset. Based on textual EE datasets under the sentence, document, and\ndialogue scenarios, we convert texts into speeches through both manual\nreal-person narration and automatic synthesis, empowering the data with diverse\nscenarios, languages, domains, ambiences, and speaker styles. Further, to\neffectively address the key challenges in the task, we tailor an E2E SpeechEE\nsystem based on the encoder-decoder architecture, where a novel Shrinking Unit\nmodule and a retrieval-aided decoding mechanism are devised. Extensive\nexperimental results on all SpeechEE subsets demonstrate the efficacy of the\nproposed model, offering a strong baseline for the task. At last, being the\nfirst work on this topic, we shed light on key directions for future research.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}