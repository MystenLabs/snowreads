{"id":"2407.13765","title":"Latent Causal Probing: A Formal Perspective on Probing with Causal\n  Models of Data","authors":"Charles Jin, Martin Rinard","authorsParsed":[["Jin","Charles",""],["Rinard","Martin",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:59:27 GMT"},{"version":"v2","created":"Wed, 31 Jul 2024 05:57:07 GMT"}],"updateDate":"2024-08-01","timestamp":1721325567000,"abstract":"  As language models (LMs) deliver increasing performance on a range of NLP\ntasks, probing classifiers have become an indispensable technique in the effort\nto better understand their inner workings. A typical setup involves (1)\ndefining an auxiliary task consisting of a dataset of text annotated with\nlabels, then (2) supervising small classifiers to predict the labels from the\nrepresentations of a pretrained LM as it processed the dataset. A high probing\naccuracy is interpreted as evidence that the LM has learned to perform the\nauxiliary task as an unsupervised byproduct of its original pretraining\nobjective. Despite the widespread usage of probes, however, the robust design\nand analysis of probing experiments remains a challenge. We develop a formal\nperspective on probing using structural causal models (SCM). Specifically,\ngiven an SCM which explains the distribution of tokens observed during\ntraining, we frame the central hypothesis as whether the LM has learned to\nrepresent the latent variables of the SCM. Empirically, we extend a recent\nstudy of LMs in the context of a synthetic grid-world navigation task, where\nhaving an exact model of the underlying causal structure allows us to draw\nstrong inferences from the result of probing experiments. Our techniques\nprovide robust empirical evidence for the ability of LMs to induce the latent\nconcepts underlying text.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}