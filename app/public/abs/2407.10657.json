{"id":"2407.10657","title":"An Empirical Study of Validating Synthetic Data for Formula Generation","authors":"Usneek Singh, Jos\\'e Cambronero, Sumit Gulwani, Aditya Kanade, Anirudh\n  Khatry, Vu Le, Mukul Singh, Gust Verbruggen","authorsParsed":[["Singh","Usneek",""],["Cambronero","Jos√©",""],["Gulwani","Sumit",""],["Kanade","Aditya",""],["Khatry","Anirudh",""],["Le","Vu",""],["Singh","Mukul",""],["Verbruggen","Gust",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 12:16:33 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 09:41:50 GMT"}],"updateDate":"2024-07-24","timestamp":1721045793000,"abstract":"  Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}