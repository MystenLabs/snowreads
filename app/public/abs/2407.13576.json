{"id":"2407.13576","title":"A Stochastic Record-Value Approach to Global Simulation Optimization","authors":"Rohan Rele, Zelda Zabinsky, Giulia Pedrielli, Aleksandr Aravkin","authorsParsed":[["Rele","Rohan",""],["Zabinsky","Zelda",""],["Pedrielli","Giulia",""],["Aravkin","Aleksandr",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 15:15:27 GMT"}],"updateDate":"2024-07-19","timestamp":1721315727000,"abstract":"  Black-box optimization is ubiquitous in machine learning, operations research\nand engineering simulation. Black-box optimization algorithms typically do not\nassume structural information about the objective function and thus must make\nuse of stochastic information to achieve statistical convergence to a globally\noptimal solution. One such class of methods is multi-start algorithms which use\na probabilistic criteria to: determine when to stop a single run of an\niterative optimization algorithm, also called an inner search, when to perform\na restart, or outer search, and when to terminate the entire algorithm.\nZabinsky, Bulger & Khompatraporn introduced a record-value theoretic\nmulti-start framework called Dynamic Multi-start Sequential Search (DMSS). We\nobserve that DMSS performs poorly when the inner search method is a\ndeterministic gradient-based search. In this thesis, we present an algorithmic\nmodification to DMSS and empirically show that the Revised DMSS (RDMSS)\nalgorithm can outperform DMSS in gradient-based settings for a broad class of\nobjective test functions. We give a theoretical analysis of a stochastic\nprocess that was constructed specifically as an inner search stopping criteria\nwithin RDMSS. We discuss computational considerations of the RDMSS algorithm.\nFinally, we present numerical results to determine its effectiveness.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LTRpmIio63xvUMZDiYuF7yLJqCt8Jf1mCXOx-IMt3eI","pdfSize":"5655721"}
