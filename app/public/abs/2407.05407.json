{"id":"2407.05407","title":"CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer\n  based on Supervised Semantic Tokens","authors":"Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang,\n  Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhifu Gao, Zhijie Yan","authorsParsed":[["Du","Zhihao",""],["Chen","Qian",""],["Zhang","Shiliang",""],["Hu","Kai",""],["Lu","Heng",""],["Yang","Yexin",""],["Hu","Hangrui",""],["Zheng","Siqi",""],["Gu","Yue",""],["Ma","Ziyang",""],["Gao","Zhifu",""],["Yan","Zhijie",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 15:16:19 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 07:42:51 GMT"}],"updateDate":"2024-07-10","timestamp":1720365379000,"abstract":"  Recent years have witnessed a trend that large language model (LLM) based\ntext-to-speech (TTS) emerges into the mainstream due to their high naturalness\nand zero-shot capacity. In this paradigm, speech signals are discretized into\ntoken sequences, which are modeled by an LLM with text as prompts and\nreconstructed by a token-based vocoder to waveforms. Obviously, speech tokens\nplay a critical role in LLM-based TTS models. Current speech tokens are learned\nin an unsupervised manner, which lacks explicit semantic information and\nalignment to the text. In this paper, we propose to represent speech with\nsupervised semantic tokens, which are derived from a multilingual speech\nrecognition model by inserting vector quantization into the encoder. Based on\nthe tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice,\nwhich consists of an LLM for text-to-token generation and a conditional flow\nmatching model for token-to-speech synthesis. Experimental results show that\nsupervised semantic tokens significantly outperform existing unsupervised\ntokens in terms of content consistency and speaker similarity for zero-shot\nvoice cloning. Moreover, we find that utilizing large-scale data further\nimproves the synthesis performance, indicating the scalable capacity of\nCosyVoice. To the best of our knowledge, this is the first attempt to involve\nsupervised speech tokens into TTS models.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}