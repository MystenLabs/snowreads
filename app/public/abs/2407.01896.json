{"id":"2407.01896","title":"LogEval: A Comprehensive Benchmark Suite for Large Language Models In\n  Log Analysis","authors":"Tianyu Cui, Shiyu Ma, Ziang Chen, Tong Xiao, Shimin Tao, Yilun Liu,\n  Shenglin Zhang, Duoming Lin, Changchang Liu, Yuzhe Cai, Weibin Meng, Yongqian\n  Sun, Dan Pei","authorsParsed":[["Cui","Tianyu",""],["Ma","Shiyu",""],["Chen","Ziang",""],["Xiao","Tong",""],["Tao","Shimin",""],["Liu","Yilun",""],["Zhang","Shenglin",""],["Lin","Duoming",""],["Liu","Changchang",""],["Cai","Yuzhe",""],["Meng","Weibin",""],["Sun","Yongqian",""],["Pei","Dan",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 02:39:33 GMT"}],"updateDate":"2024-07-03","timestamp":1719887973000,"abstract":"  Log analysis is crucial for ensuring the orderly and stable operation of\ninformation systems, particularly in the field of Artificial Intelligence for\nIT Operations (AIOps). Large Language Models (LLMs) have demonstrated\nsignificant potential in natural language processing tasks. In the AIOps\ndomain, they excel in tasks such as anomaly detection, root cause analysis of\nfaults, operations and maintenance script generation, and alert information\nsummarization. However, the performance of current LLMs in log analysis tasks\nremains inadequately validated. To address this gap, we introduce LogEval, a\ncomprehensive benchmark suite designed to evaluate the capabilities of LLMs in\nvarious log analysis tasks for the first time. This benchmark covers tasks such\nas log parsing, log anomaly detection, log fault diagnosis, and log\nsummarization. LogEval evaluates each task using 4,000 publicly available log\ndata entries and employs 15 different prompts for each task to ensure a\nthorough and fair assessment. By rigorously evaluating leading LLMs, we\ndemonstrate the impact of various LLM technologies on log analysis performance,\nfocusing on aspects such as self-consistency and few-shot contextual learning.\nWe also discuss findings related to model quantification, Chinese-English\nquestion-answering evaluation, and prompt engineering. These findings provide\ninsights into the strengths and weaknesses of LLMs in multilingual environments\nand the effectiveness of different prompt strategies. Various evaluation\nmethods are employed for different tasks to accurately measure the performance\nof LLMs in log analysis, ensuring a comprehensive assessment. The insights\ngained from LogEvals evaluation reveal the strengths and limitations of LLMs in\nlog analysis tasks, providing valuable guidance for researchers and\npractitioners.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}