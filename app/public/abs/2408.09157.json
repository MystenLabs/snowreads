{"id":"2408.09157","title":"On the KL-Divergence-based Robust Satisficing Model","authors":"Haojie Yan, Minglong Zhou, Jiayi Guo","authorsParsed":[["Yan","Haojie",""],["Zhou","Minglong",""],["Guo","Jiayi",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 10:05:05 GMT"}],"updateDate":"2024-08-20","timestamp":1723889105000,"abstract":"  Empirical risk minimization, a cornerstone in machine learning, is often\nhindered by the Optimizer's Curse stemming from discrepancies between the\nempirical and true data-generating distributions.To address this challenge, the\nrobust satisficing framework has emerged recently to mitigate ambiguity in the\ntrue distribution. Distinguished by its interpretable hyperparameter and\nenhanced performance guarantees, this approach has attracted increasing\nattention from academia. However, its applicability in tackling general machine\nlearning problems, notably deep neural networks, remains largely unexplored due\nto the computational challenges in solving this model efficiently across\ngeneral loss functions. In this study, we delve into the Kullback Leibler\ndivergence based robust satisficing model under a general loss function,\npresenting analytical interpretations, diverse performance guarantees,\nefficient and stable numerical methods, convergence analysis, and an extension\ntailored for hierarchical data structures. Through extensive numerical\nexperiments across three distinct machine learning tasks, we demonstrate the\nsuperior performance of our model compared to state-of-the-art benchmarks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}