{"id":"2407.12665","title":"Patch-Level Training for Large Language Models","authors":"Chenze Shao, Fandong Meng, Jie Zhou","authorsParsed":[["Shao","Chenze",""],["Meng","Fandong",""],["Zhou","Jie",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 15:48:39 GMT"},{"version":"v2","created":"Fri, 13 Sep 2024 03:07:37 GMT"}],"updateDate":"2024-09-16","timestamp":1721231319000,"abstract":"  As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}