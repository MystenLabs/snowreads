{"id":"2408.08071","title":"Universality of Real Minimal Complexity Reservoir","authors":"Robert Simon Fong, Boyu Li, Peter Ti\\v{n}o","authorsParsed":[["Fong","Robert Simon",""],["Li","Boyu",""],["Ti≈ào","Peter",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 10:44:33 GMT"}],"updateDate":"2024-08-16","timestamp":1723718673000,"abstract":"  Reservoir Computing (RC) models, a subclass of recurrent neural networks, are\ndistinguished by their fixed, non-trainable input layer and dynamically coupled\nreservoir, with only the static readout layer being trained. This design\ncircumvents the issues associated with backpropagating error signals through\ntime, thereby enhancing both stability and training efficiency. RC models have\nbeen successfully applied across a broad range of application domains.\nCrucially, they have been demonstrated to be universal approximators of\ntime-invariant dynamic filters with fading memory, under various settings of\napproximation norms and input driving sources.\n  Simple Cycle Reservoirs (SCR) represent a specialized class of RC models with\na highly constrained reservoir architecture, characterized by uniform ring\nconnectivity and binary input-to-reservoir weights with an aperiodic sign\npattern. For linear reservoirs, given the reservoir size, the reservoir\nconstruction has only one degree of freedom -- the reservoir cycle weight. Such\narchitectures are particularly amenable to hardware implementations without\nsignificant performance degradation in many practical tasks. In this study we\nendow these observations with solid theoretical foundations by proving that\nSCRs operating in real domain are universal approximators of time-invariant\ndynamic filters with fading memory. Our results supplement recent research\nshowing that SCRs in the complex domain can approximate, to arbitrary\nprecision, any unrestricted linear reservoir with a non-linear readout. We\nfurthermore introduce a novel method to drastically reduce the number of SCR\nunits, making such highly constrained architectures natural candidates for\nlow-complexity hardware implementations. Our findings are supported by\nempirical studies on real-world time series datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}