{"id":"2407.19651","title":"ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via\n  Universal Transform-Neck","authors":"Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro\n  Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi","authorsParsed":[["Kao","Chia-Hao",""],["Chien","Cheng",""],["Tseng","Yu-Jen",""],["Chen","Yi-Hsin",""],["Gnutti","Alessandro",""],["Lo","Shao-Yuan",""],["Peng","Wen-Hsiao",""],["Leonardi","Riccardo",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 02:32:44 GMT"}],"updateDate":"2024-07-30","timestamp":1722220364000,"abstract":"  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. The proposed framework is generic and\napplicable to multiple application scenarios, where the neural image codec can\nbe (1) pre-trained for human perception without updating, (2) fully updated for\njoint human and machine perception, or (3) fully updated for only machine\nperception. The transform-neck trained with the surrogate loss is universal,\nfor it can serve various downstream vision tasks enabled by a variety of MLLMs\nthat share the same visual encoder. Our framework has the striking feature of\nexcluding the downstream MLLMs from training the transform-neck, and\npotentially the neural image codec as well. This stands out from most existing\ncoding for machine approaches that involve downstream networks in training and\nthus could be impractical when the networks are MLLMs. Extensive experiments on\ndifferent neural image codecs and various MLLM-based vision tasks show that our\nmethod achieves great rate-accuracy performance with much less complexity,\ndemonstrating its effectiveness.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d8zBXIhrt3dyLkZo4hFHBCfg4TZjlCRyaZHUE8sOCOg","pdfSize":"19282934"}
