{"id":"2407.11890","title":"DepGAN: Leveraging Depth Maps for Handling Occlusions and Transparency\n  in Image Composition","authors":"Amr Ghoneim, Jiju Poovvancheri, Yasushi Akiyama, Dong Chen","authorsParsed":[["Ghoneim","Amr",""],["Poovvancheri","Jiju",""],["Akiyama","Yasushi",""],["Chen","Dong",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 16:18:40 GMT"}],"updateDate":"2024-07-17","timestamp":1721146720000,"abstract":"  Image composition is a complex task which requires a lot of information about\nthe scene for an accurate and realistic composition, such as perspective,\nlighting, shadows, occlusions, and object interactions. Previous methods have\npredominantly used 2D information for image composition, neglecting the\npotentials of 3D spatial information. In this work, we propose DepGAN, a\nGenerative Adversarial Network that utilizes depth maps and alpha channels to\nrectify inaccurate occlusions and enhance transparency effects in image\ncomposition. Central to our network is a novel loss function called Depth Aware\nLoss which quantifies the pixel wise depth difference to accurately delineate\nocclusion boundaries while compositing objects at different depth levels.\nFurthermore, we enhance our network's learning process by utilizing opacity\ndata, enabling it to effectively manage compositions involving transparent and\nsemi-transparent objects. We tested our model against state-of-the-art image\ncomposition GANs on benchmark (both real and synthetic) datasets. The results\nreveal that DepGAN significantly outperforms existing methods in terms of\naccuracy of object placement semantics, transparency and occlusion handling,\nboth visually and quantitatively. Our code is available at\nhttps://amrtsg.github.io/DepGAN/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}