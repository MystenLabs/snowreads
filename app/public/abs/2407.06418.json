{"id":"2407.06418","title":"System stabilization with policy optimization on unstable latent\n  manifolds","authors":"Steffen W. R. Werner, Benjamin Peherstorfer","authorsParsed":[["Werner","Steffen W. R.",""],["Peherstorfer","Benjamin",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 21:57:28 GMT"}],"updateDate":"2024-07-10","timestamp":1720475848000,"abstract":"  Stability is a basic requirement when studying the behavior of dynamical\nsystems. However, stabilizing dynamical systems via reinforcement learning is\nchallenging because only little data can be collected over short time horizons\nbefore instabilities are triggered and data become meaningless. This work\nintroduces a reinforcement learning approach that is formulated over latent\nmanifolds of unstable dynamics so that stabilizing policies can be trained from\nfew data samples. The unstable manifolds are minimal in the sense that they\ncontain the lowest dimensional dynamics that are necessary for learning\npolicies that guarantee stabilization. This is in stark contrast to generic\nlatent manifolds that aim to approximate all -- stable and unstable -- system\ndynamics and thus are higher dimensional and often require higher amounts of\ndata. Experiments demonstrate that the proposed approach stabilizes even\ncomplex physical systems from few data samples for which other methods that\noperate either directly in the system state space or on generic latent\nmanifolds fail.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Dynamical Systems","Mathematics/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}