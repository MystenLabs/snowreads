{"id":"2408.15240","title":"Generative Verifiers: Reward Modeling as Next-Token Prediction","authors":"Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral\n  Kumar, Rishabh Agarwal","authorsParsed":[["Zhang","Lunjun",""],["Hosseini","Arian",""],["Bansal","Hritik",""],["Kazemi","Mehran",""],["Kumar","Aviral",""],["Agarwal","Rishabh",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 17:57:45 GMT"}],"updateDate":"2024-08-28","timestamp":1724781465000,"abstract":"  Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}