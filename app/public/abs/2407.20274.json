{"id":"2407.20274","title":"Exploring the Plausibility of Hate and Counter Speech Detectors with\n  Explainable AI","authors":"Adrian Jaques B\\\"ock, Djordje Slijep\\v{c}evi\\'c, Matthias Zeppelzauer","authorsParsed":[["Böck","Adrian Jaques",""],["Slijepčević","Djordje",""],["Zeppelzauer","Matthias",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 10:17:04 GMT"}],"updateDate":"2024-07-31","timestamp":1721902624000,"abstract":"  In this paper we investigate the explainability of transformer models and\ntheir plausibility for hate speech and counter speech detection. We compare\nrepresentatives of four different explainability approaches, i.e.,\ngradient-based, perturbation-based, attention-based, and prototype-based\napproaches, and analyze them quantitatively with an ablation study and\nqualitatively in a user study. Results show that perturbation-based\nexplainability performs best, followed by gradient-based and attention-based\nexplainability. Prototypebased experiments did not yield useful results.\nOverall, we observe that explainability strongly supports the users in better\nunderstanding the model predictions.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"r1-VXyROEFuWWPaKuph5ACGfKibrIHNndEosOPwiErA","pdfSize":"596936"}
