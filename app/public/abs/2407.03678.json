{"id":"2407.03678","title":"Improving Self Consistency in LLMs through Probabilistic Tokenization","authors":"Ashutosh Sathe, Divyanshu Aggarwal, Sunayana Sitaram","authorsParsed":[["Sathe","Ashutosh",""],["Aggarwal","Divyanshu",""],["Sitaram","Sunayana",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 06:52:48 GMT"}],"updateDate":"2024-07-08","timestamp":1720075968000,"abstract":"  Prior research has demonstrated noticeable performance gains through the use\nof probabilistic tokenizations, an approach that involves employing multiple\ntokenizations of the same input string during the training phase of a language\nmodel. Despite these promising findings, modern large language models (LLMs)\nhave yet to be trained using probabilistic tokenizations. Interestingly, while\nthe tokenizers of these contemporary LLMs have the capability to generate\nmultiple tokenizations, this property remains underutilized.\n  In this work, we propose a novel method to leverage the multiple tokenization\ncapabilities of modern LLM tokenizers, aiming to enhance the self-consistency\nof LLMs in reasoning tasks. Our experiments indicate that when utilizing\nprobabilistic tokenizations, LLMs generate logically diverse reasoning paths,\nmoving beyond mere surface-level linguistic diversity.We carefully study\nprobabilistic tokenization and offer insights to explain the self consistency\nimprovements it brings through extensive experimentation on 5 LLM families and\n4 reasoning benchmarks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}