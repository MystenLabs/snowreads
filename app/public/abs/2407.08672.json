{"id":"2407.08672","title":"NODE-Adapter: Neural Ordinary Differential Equations for Better\n  Vision-Language Reasoning","authors":"Yi Zhang, Chun-Wun Cheng, Ke Yu, Zhihai He, Carola-Bibiane\n  Sch\\\"onlieb, and Angelica I.Aviles-Rivero","authorsParsed":[["Zhang","Yi",""],["Cheng","Chun-Wun",""],["Yu","Ke",""],["He","Zhihai",""],["Sch√∂nlieb","Carola-Bibiane",""],["Aviles-Rivero","Angelica I.",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:04:19 GMT"}],"updateDate":"2024-07-12","timestamp":1720717459000,"abstract":"  In this paper, we consider the problem of prototype-based vision-language\nreasoning problem. We observe that existing methods encounter three major\nchallenges: 1) escalating resource demands and prolonging training times, 2)\ncontending with excessive learnable parameters, and 3) fine-tuning based only\non a single modality. These challenges will hinder their capability to adapt\nVision-Language Models (VLMs) to downstream tasks. Motivated by this critical\nobservation, we propose a novel method called NODE-Adapter, which utilizes\nNeural Ordinary Differential Equations for better vision-language reasoning. To\nfully leverage both visual and textual modalities and estimate class prototypes\nmore effectively and accurately, we divide our method into two stages:\ncross-modal prototype construction and cross-modal prototype optimization using\nneural ordinary differential equations. Specifically, we exploit VLM to encode\nhand-crafted prompts into textual features and few-shot support images into\nvisual features. Then, we estimate the textual prototype and visual prototype\nby averaging the textual features and visual features, respectively, and\nadaptively combine the textual prototype and visual prototype to construct the\ncross-modal prototype. To alleviate the prototype bias, we then model the\nprototype optimization process as an initial value problem with Neural ODEs to\nestimate the continuous gradient flow. Our extensive experimental results,\nwhich cover few-shot classification, domain generalization, and visual\nreasoning on human-object interaction, demonstrate that the proposed method\nsignificantly outperforms existing state-of-the-art approaches.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}