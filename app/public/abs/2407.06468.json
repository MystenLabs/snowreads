{"id":"2407.06468","title":"AnatoMask: Enhancing Medical Image Segmentation with\n  Reconstruction-guided Self-masking","authors":"Yuheng Li, Tianyu Luan, Yizhou Wu, Shaoyan Pan, Yenho Chen, and\n  Xiaofeng Yang","authorsParsed":[["Li","Yuheng",""],["Luan","Tianyu",""],["Wu","Yizhou",""],["Pan","Shaoyan",""],["Chen","Yenho",""],["Yang","Xiaofeng",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 00:15:52 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 21:04:26 GMT"}],"updateDate":"2024-07-18","timestamp":1720484152000,"abstract":"  Due to the scarcity of labeled data, self-supervised learning (SSL) has\ngained much attention in 3D medical image segmentation, by extracting semantic\nrepresentations from unlabeled data. Among SSL strategies, Masked image\nmodeling (MIM) has shown effectiveness by reconstructing randomly masked images\nto learn detailed representations. However, conventional MIM methods require\nextensive training data to achieve good performance, which still poses a\nchallenge for medical imaging. Since random masking uniformly samples all\nregions within medical images, it may overlook crucial anatomical regions and\nthus degrade the pretraining efficiency. We propose AnatoMask, a novel MIM\nmethod that leverages reconstruction loss to dynamically identify and mask out\nanatomically significant regions to improve pretraining efficacy. AnatoMask\ntakes a self-distillation approach, where the model learns both how to find\nmore significant regions to mask and how to reconstruct these masked regions.\nTo avoid suboptimal learning, Anatomask adjusts the pretraining difficulty\nprogressively using a masking dynamics function. We have evaluated our method\non 4 public datasets with multiple imaging modalities (CT, MRI, and PET).\nAnatoMask demonstrates superior performance and scalability compared to\nexisting SSL methods. The code is available at\nhttps://github.com/ricklisz/AnatoMask.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}