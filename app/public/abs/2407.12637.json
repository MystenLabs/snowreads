{"id":"2407.12637","title":"Toward INT4 Fixed-Point Training via Exploring Quantization Error for\n  Gradients","authors":"Dohyung Kim, Junghyup Lee, Jeimin Jeon, Jaehyeon Moon, Bumsub Ham","authorsParsed":[["Kim","Dohyung",""],["Lee","Junghyup",""],["Jeon","Jeimin",""],["Moon","Jaehyeon",""],["Ham","Bumsub",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 15:06:12 GMT"}],"updateDate":"2024-07-18","timestamp":1721228772000,"abstract":"  Network quantization generally converts full-precision weights and/or\nactivations into low-bit fixed-point values in order to accelerate an inference\nprocess. Recent approaches to network quantization further discretize the\ngradients into low-bit fixed-point values, enabling an efficient training. They\ntypically set a quantization interval using a min-max range of the gradients or\nadjust the interval such that the quantization error for entire gradients is\nminimized. In this paper, we analyze the quantization error of gradients for\nthe low-bit fixed-point training, and show that lowering the error for\nlarge-magnitude gradients boosts the quantization performance significantly.\nBased on this, we derive an upper bound of quantization error for the large\ngradients in terms of the quantization interval, and obtain an optimal\ncondition for the interval minimizing the quantization error for large\ngradients. We also introduce an interval update algorithm that adjusts the\nquantization interval adaptively to maintain a small quantization error for\nlarge gradients. Experimental results demonstrate the effectiveness of our\nquantization method for various combinations of network architectures and\nbit-widths on various tasks, including image classification, object detection,\nand super-resolution.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}