{"id":"2408.02919","title":"Data Checklist: On Unit-Testing Datasets with Usable Information","authors":"Heidi C. Zhang, Shabnam Behzad, Kawin Ethayarajh, Dan Jurafsky","authorsParsed":[["Zhang","Heidi C.",""],["Behzad","Shabnam",""],["Ethayarajh","Kawin",""],["Jurafsky","Dan",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 03:08:36 GMT"}],"updateDate":"2024-08-07","timestamp":1722913716000,"abstract":"  Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for\nunderstanding the behavior of LLMs, analogous to unit-testing in software\nengineering. However, despite datasets being a key determinant of model\nbehavior, evaluating datasets, e.g., for the existence of annotation artifacts,\nis largely done ad hoc, once a problem in model behavior has already been found\ndownstream. In this work, we take a more principled approach to unit-testing\ndatasets by proposing a taxonomy based on the V-information literature. We call\na collection of such unit tests a data checklist. Using a checklist, not only\nare we able to recover known artifacts in well-known datasets such as SNLI, but\nwe also discover previously unknown artifacts in preference datasets for LLM\nalignment. Data checklists further enable a new kind of data filtering, which\nwe use to improve the efficacy and data efficiency of preference alignment.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pe0HrlXqVZBKC8p0rEuOUe4VjHqgbITrtro06jvDmM8","pdfSize":"874792"}
