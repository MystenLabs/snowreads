{"id":"2407.09431","title":"Rethinking temporal self-similarity for repetitive action counting","authors":"Yanan Luo, Jinhui Yi, Yazan Abu Farha, Moritz Wolter, Juergen Gall","authorsParsed":[["Luo","Yanan",""],["Yi","Jinhui",""],["Farha","Yazan Abu",""],["Wolter","Moritz",""],["Gall","Juergen",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 17:03:14 GMT"}],"updateDate":"2024-07-15","timestamp":1720803794000,"abstract":"  Counting repetitive actions in long untrimmed videos is a challenging task\nthat has many applications such as rehabilitation. State-of-the-art methods\npredict action counts by first generating a temporal self-similarity matrix\n(TSM) from the sampled frames and then feeding the matrix to a predictor\nnetwork. The self-similarity matrix, however, is not an optimal input to a\nnetwork since it discards too much information from the frame-wise embeddings.\nWe thus rethink how a TSM can be utilized for counting repetitive actions and\npropose a framework that learns embeddings and predicts action start\nprobabilities at full temporal resolution. The number of repeated actions is\nthen inferred from the action start probabilities. In contrast to current\napproaches that have the TSM as an intermediate representation, we propose a\nnovel loss based on a generated reference TSM, which enforces that the\nself-similarity of the learned frame-wise embeddings is consistent with the\nself-similarity of repeated actions. The proposed framework achieves\nstate-of-the-art results on three datasets, i.e., RepCount, UCFRep, and\nCountix.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}