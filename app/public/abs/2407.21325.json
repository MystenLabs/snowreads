{"id":"2407.21325","title":"EdgeLLM: A Highly Efficient CPU-FPGA Heterogeneous Edge Accelerator for\n  Large Language Models","authors":"Mingqiang Huang, Ao Shen, Kai Li, Haoxiang Peng, Boyu Li, Hao Yu","authorsParsed":[["Huang","Mingqiang",""],["Shen","Ao",""],["Li","Kai",""],["Peng","Haoxiang",""],["Li","Boyu",""],["Yu","Hao",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 04:16:37 GMT"}],"updateDate":"2024-08-01","timestamp":1722399397000,"abstract":"  The rapid advancements in artificial intelligence (AI), particularly the\nLarge Language Models (LLMs), have profoundly affected our daily work and\ncommunication forms. However, the colossal scale of LLM presents significant\noperational challenges, particularly when attempting to deploy them on\nresource-constrained edge devices such as smartphones, robots, and embedded\nsystems. In this work, we proposed EdgeLLM, an efficient CPU-FPGA heterogeneous\nacceleration framework, to markedly enhance the computational efficiency of\nLLMs on edge. We first analyzed the whole operators within AI models and\ndeveloped a universal data parallelism scheme, which is generic and can be\nadapted to any type of AI algorithm. Then, we developed fully-customized\nhardware operators according to the designated data formats. A multitude of\noptimization techniques have been integrated in the design, such as approximate\nFP16*INT4 and FP16*FP16 computation engines, group vector systolic arrays,\nlog-scale structured sparsity, asynchronous between data transfer and\nprocessing. Finally, we proposed an end-to-end compilation scheme that can\ndynamically compile all of the operators and map the whole model on CPU-FPGA\nheterogeneous system. The design has been deployed on AMD Xilinx VCU128 FPGA,\nour accelerator achieves 1.67x higher throughput and 7.4x higher energy\nefficiency than the commercial GPU (NVIDIA A100-SXM4-80G) on ChatGLM2-6B, and\nshows 10%~20% better performance than state-of-the-art FPGA accelerator of\nFlightLLM in terms of HBM bandwidth utilization and LLM throughput.\n","subjects":["Computing Research Repository/Hardware Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}