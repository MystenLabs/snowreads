{"id":"2407.17101","title":"PiPa++: Towards Unification of Domain Adaptive Semantic Segmentation via\n  Self-supervised Learning","authors":"Mu Chen and Zhedong Zheng and Yi Yang","authorsParsed":[["Chen","Mu",""],["Zheng","Zhedong",""],["Yang","Yi",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 08:53:29 GMT"}],"updateDate":"2024-07-25","timestamp":1721811209000,"abstract":"  Unsupervised domain adaptive segmentation aims to improve the segmentation\naccuracy of models on target domains without relying on labeled data from those\ndomains. This approach is crucial when labeled target domain data is scarce or\nunavailable. It seeks to align the feature representations of the source domain\n(where labeled data is available) and the target domain (where only unlabeled\ndata is present), thus enabling the model to generalize well to the target\ndomain. Current image- and video-level domain adaptation have been addressed\nusing different and specialized frameworks, training strategies and\noptimizations despite their underlying connections. In this paper, we propose a\nunified framework PiPa++, which leverages the core idea of ``comparing'' to (1)\nexplicitly encourage learning of discriminative pixel-wise features with\nintraclass compactness and inter-class separability, (2) promote the robust\nfeature learning of the identical patch against different contexts or\nfluctuations, and (3) enable the learning of temporal continuity under dynamic\nenvironments. With the designed task-smart contrastive sampling strategy,\nPiPa++ enables the mining of more informative training samples according to the\ntask demand. Extensive experiments demonstrate the effectiveness of our method\non both image-level and video-level domain adaption benchmarks. Moreover, the\nproposed method is compatible with other UDA approaches to further improve the\nperformance without introducing extra parameters.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}