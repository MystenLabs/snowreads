{"id":"2408.03282","title":"AMES: Asymmetric and Memory-Efficient Similarity Estimation for\n  Instance-level Retrieval","authors":"Pavel Suma, Giorgos Kordopatis-Zilos, Ahmet Iscen, Giorgos Tolias","authorsParsed":[["Suma","Pavel",""],["Kordopatis-Zilos","Giorgos",""],["Iscen","Ahmet",""],["Tolias","Giorgos",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 16:29:51 GMT"}],"updateDate":"2024-08-07","timestamp":1722961791000,"abstract":"  This work investigates the problem of instance-level image retrieval\nre-ranking with the constraint of memory efficiency, ultimately aiming to limit\nmemory usage to 1KB per image. Departing from the prevalent focus on\nperformance enhancements, this work prioritizes the crucial trade-off between\nperformance and memory requirements. The proposed model uses a\ntransformer-based architecture designed to estimate image-to-image similarity\nby capturing interactions within and across images based on their local\ndescriptors. A distinctive property of the model is the capability for\nasymmetric similarity estimation. Database images are represented with a\nsmaller number of descriptors compared to query images, enabling performance\nimprovements without increasing memory consumption. To ensure adaptability\nacross different applications, a universal model is introduced that adjusts to\na varying number of local descriptors during the testing phase. Results on\nstandard benchmarks demonstrate the superiority of our approach over both\nhand-crafted and learned models. In particular, compared with current\nstate-of-the-art methods that overlook their memory footprint, our approach not\nonly attains superior performance but does so with a significantly reduced\nmemory footprint. The code and pretrained models are publicly available at:\nhttps://github.com/pavelsuma/ames\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}