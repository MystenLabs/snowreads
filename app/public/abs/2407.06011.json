{"id":"2407.06011","title":"Igea: a Decoder-Only Language Model for Biomedical Text Generation in\n  Italian","authors":"Tommaso Mario Buonocore, Simone Rancati and Enea Parimbelli","authorsParsed":[["Buonocore","Tommaso Mario",""],["Rancati","Simone",""],["Parimbelli","Enea",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 15:04:21 GMT"}],"updateDate":"2024-07-09","timestamp":1720451061000,"abstract":"  The development of domain-specific language models has significantly advanced\nnatural language processing applications in various specialized fields,\nparticularly in biomedicine. However, the focus has largely been on\nEnglish-language models, leaving a gap for less-resourced languages such as\nItalian. This paper introduces Igea, the first decoder-only language model\ndesigned explicitly for biomedical text generation in Italian. Built on the\nMinerva model and continually pretrained on a diverse corpus of Italian medical\ntexts, Igea is available in three model sizes: 350 million, 1 billion, and 3\nbillion parameters. The models aim to balance computational efficiency and\nperformance, addressing the challenges of managing the peculiarities of medical\nterminology in Italian. We evaluate Igea using a mix of in-domain biomedical\ncorpora and general-purpose benchmarks, highlighting its efficacy and retention\nof general knowledge even after the domain-specific training. This paper\ndiscusses the model's development and evaluation, providing a foundation for\nfuture advancements in Italian biomedical NLP.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}