{"id":"2407.10058","title":"Learning to Refuse: Towards Mitigating Privacy Risks in LLMs","authors":"Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Wenliang Chen","authorsParsed":[["Liu","Zhenhua",""],["Zhu","Tong",""],["Tan","Chuanyuan",""],["Chen","Wenliang",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 03:05:53 GMT"},{"version":"v2","created":"Mon, 16 Sep 2024 07:20:13 GMT"}],"updateDate":"2024-09-17","timestamp":1720926353000,"abstract":"  Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"P7tsCCM8SuP7uni2rOsn8dQlAYe9GcdQ00l87Bw103M","pdfSize":"7530020"}
