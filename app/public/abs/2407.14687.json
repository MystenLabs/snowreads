{"id":"2407.14687","title":"Quantum Data Breach: Reusing Training Dataset by Untrusted Quantum\n  Clouds","authors":"Suryansh Upadhyay, Swaroop Ghosh","authorsParsed":[["Upadhyay","Suryansh",""],["Ghosh","Swaroop",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 22:06:34 GMT"}],"updateDate":"2024-07-23","timestamp":1721426794000,"abstract":"  Quantum computing (QC) has the potential to revolutionize fields like machine\nlearning, security, and healthcare. Quantum machine learning (QML) has emerged\nas a promising area, enhancing learning algorithms using quantum computers.\nHowever, QML models are lucrative targets due to their high training costs and\nextensive training times. The scarcity of quantum resources and long wait times\nfurther exacerbate the challenge. Additionally, QML providers may rely on a\nthird-party quantum cloud for hosting the model, exposing the models and\ntraining data. As QML-as-a-Service (QMLaaS) becomes more prevalent, reliance on\nthird party quantum clouds can pose a significant threat. This paper shows that\nadversaries in quantum clouds can use white-box access of the QML model during\ntraining to extract the state preparation circuit (containing training data)\nalong with the labels. The extracted training data can be reused for training a\nclone model or sold for profit. We propose a suite of techniques to prune and\nfix the incorrect labels. Results show that $\\approx$90\\% labels can be\nextracted correctly. The same model trained on the adversarially extracted data\nachieves approximately $\\approx$90\\% accuracy, closely matching the accuracy\nachieved when trained on the original data. To mitigate this threat, we propose\nmasking labels/classes and modifying the cost function for label obfuscation,\nreducing adversarial label prediction accuracy by $\\approx$70\\%.\n","subjects":["Physics/Quantum Physics"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}