{"id":"2407.08803","title":"PID Accelerated Temporal Difference Algorithms","authors":"Mark Bedaywi, Amin Rakhsha, Amir-massoud Farahmand","authorsParsed":[["Bedaywi","Mark",""],["Rakhsha","Amin",""],["Farahmand","Amir-massoud",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 18:23:46 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 16:59:07 GMT"}],"updateDate":"2024-09-04","timestamp":1720722226000,"abstract":"  Long-horizon tasks, which have a large discount factor, pose a challenge for\nmost conventional reinforcement learning (RL) algorithms. Algorithms such as\nValue Iteration and Temporal Difference (TD) learning have a slow convergence\nrate and become inefficient in these tasks. When the transition distributions\nare given, PID VI was recently introduced to accelerate the convergence of\nValue Iteration using ideas from control theory. Inspired by this, we introduce\nPID TD Learning and PID Q-Learning algorithms for the RL setting, in which only\nsamples from the environment are available. We give a theoretical analysis of\nthe convergence of PID TD Learning and its acceleration compared to the\nconventional TD Learning. We also introduce a method for adapting PID gains in\nthe presence of noise and empirically verify its effectiveness.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}