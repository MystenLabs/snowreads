{"id":"2407.17624","title":"Traditional Methods Outperform Generative LLMs at Forecasting Credit\n  Ratings","authors":"Felix Drinkall and Janet B. Pierrehumbert and Stefan Zohren","authorsParsed":[["Drinkall","Felix",""],["Pierrehumbert","Janet B.",""],["Zohren","Stefan",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 20:30:55 GMT"}],"updateDate":"2024-07-26","timestamp":1721853055000,"abstract":"  Large Language Models (LLMs) have been shown to perform well for many\ndownstream tasks. Transfer learning can enable LLMs to acquire skills that were\nnot targeted during pre-training. In financial contexts, LLMs can sometimes\nbeat well-established benchmarks. This paper investigates how well LLMs perform\nin the task of forecasting corporate credit ratings. We show that while LLMs\nare very good at encoding textual information, traditional methods are still\nvery competitive when it comes to encoding numeric and multimodal data. For our\ntask, current LLMs perform worse than a more traditional XGBoost architecture\nthat combines fundamental and macroeconomic data with high-density text-based\nembedding features.\n","subjects":["Quantitative Finance/Risk Management","Computing Research Repository/Computation and Language","Quantitative Finance/General Finance"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"5G0cK-Up7ZjyMsvBe0dX2yvpgf0Y7_eak6uOGaRX2W4","pdfSize":"935040","objectId":"0x32cea8c751d90a0ed10abadfa8492f54f5c54f4950ca811b55372c1762143e50","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
