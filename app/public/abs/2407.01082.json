{"id":"2407.01082","title":"Min P Sampling: Balancing Creativity and Coherence at High Temperature","authors":"Minh Nguyen, Andrew Baker, Andreas Kirsch, Clement Neo","authorsParsed":[["Nguyen","Minh",""],["Baker","Andrew",""],["Kirsch","Andreas",""],["Neo","Clement",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 08:37:25 GMT"}],"updateDate":"2024-07-02","timestamp":1719823045000,"abstract":"  Large Language Models (LLMs) generate longform text by successively sampling\nthe next token based on the probability distribution of the token vocabulary at\neach decoding step. Current popular truncation sampling methods such as top-$p$\nsampling, also known as nucleus sampling, often struggle to balance coherence\nand creativity in generating text, particularly when using higher temperatures.\nTo address this issue, we propose min-$p$, a dynamic truncation sampling\nmethod, that establishes a minimum base percentage threshold for tokens, which\nthe scales according to the probability of the top candidate token. Through\nexperiments on several benchmarks, such as GPQA, GSM8K and AlpacaEval Creative\nWriting, we demonstrate that min-$p$ improves the coherence and quality of\ngenerated text even at high temperatures, while also facilitating more creative\nand diverse outputs compared to top-$p$ and other sampling methods. As of\nwriting, min-$p$ has been adopted by multiple open-source LLM implementations,\nand have been independently assessed by members of the open-source LLM\ncommunity, further validating its practical utility and potential.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}