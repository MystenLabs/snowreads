{"id":"2408.03414","title":"Logistic Regression makes small LLMs strong and explainable\n  \"tens-of-shot\" classifiers","authors":"Marcus Buckmann and Edward Hill","authorsParsed":[["Buckmann","Marcus",""],["Hill","Edward",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 19:23:42 GMT"}],"updateDate":"2024-08-08","timestamp":1722972222000,"abstract":"  For simple classification tasks, we show that users can benefit from the\nadvantages of using small, local, generative language models instead of large\ncommercial models without a trade-off in performance or introducing extra\nlabelling costs. These advantages, including those around privacy,\navailability, cost, and explainability, are important both in commercial\napplications and in the broader democratisation of AI. Through experiments on\n17 sentence classification tasks (2-4 classes), we show that penalised logistic\nregression on the embeddings from a small LLM equals (and usually betters) the\nperformance of a large LLM in the \"tens-of-shot\" regime. This requires no more\nlabelled instances than are needed to validate the performance of the large\nLLM. Finally, we extract stable and sensible explanations for classification\ndecisions.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cgvVp0nL18XbWDtGylar1oihxOsmxBzppETYJcGgrYI","pdfSize":"1696479"}
