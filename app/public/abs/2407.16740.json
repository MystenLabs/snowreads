{"id":"2407.16740","title":"PLM-Net: Perception Latency Mitigation Network for Vision-Based Lateral\n  Control of Autonomous Vehicles","authors":"Aws Khalil, Jaerock Kwon","authorsParsed":[["Khalil","Aws",""],["Kwon","Jaerock",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 17:41:13 GMT"}],"updateDate":"2024-07-25","timestamp":1721756473000,"abstract":"  This study introduces the Perception Latency Mitigation Network (PLM-Net), a\nnovel deep learning approach for addressing perception latency in vision-based\nAutonomous Vehicle (AV) lateral control systems. Perception latency is the\ndelay between capturing the environment through vision sensors (e.g., cameras)\nand applying an action (e.g., steering). This issue is understudied in both\nclassical and neural-network-based control methods. Reducing this latency with\npowerful GPUs and FPGAs is possible but impractical for automotive platforms.\nPLM-Net comprises the Base Model (BM) and the Timed Action Prediction Model\n(TAPM). BM represents the original Lane Keeping Assist (LKA) system, while TAPM\npredicts future actions for different latency values. By integrating these\nmodels, PLM-Net mitigates perception latency. The final output is determined\nthrough linear interpolation of BM and TAPM outputs based on real-time latency.\nThis design addresses both constant and varying latency, improving driving\ntrajectories and steering control. Experimental results validate the efficacy\nof PLM-Net across various latency conditions. Source code:\nhttps://github.com/AwsKhalil/oscar/tree/devel-plm-net.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VcyOsGaGGr12Lb8PmM7zWahaRfo90VMPFgk-vONbHKc","pdfSize":"8433865"}
