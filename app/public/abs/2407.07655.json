{"id":"2407.07655","title":"The Selective G-Bispectrum and its Inversion: Applications to\n  G-Invariant Networks","authors":"Simon Mataigne, Johan Mathe, Sophia Sanborn, Christopher Hillar, Nina\n  Miolane","authorsParsed":[["Mataigne","Simon",""],["Mathe","Johan",""],["Sanborn","Sophia",""],["Hillar","Christopher",""],["Miolane","Nina",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:35:04 GMT"}],"updateDate":"2024-07-11","timestamp":1720618504000,"abstract":"  An important problem in signal processing and deep learning is to achieve\n\\textit{invariance} to nuisance factors not relevant for the task. Since many\nof these factors are describable as the action of a group $G$ (e.g. rotations,\ntranslations, scalings), we want methods to be $G$-invariant. The\n$G$-Bispectrum extracts every characteristic of a given signal up to group\naction: for example, the shape of an object in an image, but not its\norientation. Consequently, the $G$-Bispectrum has been incorporated into deep\nneural network architectures as a computational primitive for\n$G$-invariance\\textemdash akin to a pooling mechanism, but with greater\nselectivity and robustness. However, the computational cost of the\n$G$-Bispectrum ($\\mathcal{O}(|G|^2)$, with $|G|$ the size of the group) has\nlimited its widespread adoption. Here, we show that the $G$-Bispectrum\ncomputation contains redundancies that can be reduced into a \\textit{selective\n$G$-Bispectrum} with $\\mathcal{O}(|G|)$ complexity. We prove desirable\nmathematical properties of the selective $G$-Bispectrum and demonstrate how its\nintegration in neural networks enhances accuracy and robustness compared to\ntraditional approaches, while enjoying considerable speeds-up compared to the\nfull $G$-Bispectrum.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"b3hRttK1ydsKKtO2yo_4DFSim1vobYpGQDqoG_ozTJ0","pdfSize":"1798012"}
