{"id":"2408.11051","title":"FLAME: Learning to Navigate with Multimodal LLM in Urban Environments","authors":"Yunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang","authorsParsed":[["Xu","Yunzhe",""],["Pan","Yiyuan",""],["Liu","Zhe",""],["Wang","Hesheng",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 17:57:46 GMT"}],"updateDate":"2024-08-21","timestamp":1724176666000,"abstract":"  Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/"}