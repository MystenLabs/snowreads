{"id":"2408.05206","title":"Multi-Garment Customized Model Generation","authors":"Yichen Liu, Penghui Du, Yi Liu Quanwei Zhang","authorsParsed":[["Liu","Yichen",""],["Du","Penghui",""],["Zhang","Yi Liu Quanwei",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 17:57:33 GMT"}],"updateDate":"2024-08-12","timestamp":1723226253000,"abstract":"  This paper introduces Multi-Garment Customized Model Generation, a unified\nframework based on Latent Diffusion Models (LDMs) aimed at addressing the\nunexplored task of synthesizing images with free combinations of multiple\npieces of clothing. The method focuses on generating customized models wearing\nvarious targeted outfits according to different text prompts. The primary\nchallenge lies in maintaining the natural appearance of the dressed model while\npreserving the complex textures of each piece of clothing, ensuring that the\ninformation from different garments does not interfere with each other. To\ntackle these challenges, we first developed a garment encoder, which is a\ntrainable UNet copy with shared weights, capable of extracting detailed\nfeatures of garments in parallel. Secondly, our framework supports the\nconditional generation of multiple garments through decoupled multi-garment\nfeature fusion, allowing multiple clothing features to be injected into the\nbackbone network, significantly alleviating conflicts between garment\ninformation. Additionally, the proposed garment encoder is a plug-and-play\nmodule that can be combined with other extension modules such as IP-Adapter and\nControlNet, enhancing the diversity and controllability of the generated\nmodels. Extensive experiments demonstrate the superiority of our approach over\nexisting alternatives, opening up new avenues for the task of generating images\nwith multiple-piece clothing combinations\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}