{"id":"2407.11414","title":"SDPT: Synchronous Dual Prompt Tuning for Fusion-based Visual-Language\n  Pre-trained Models","authors":"Yang Zhou, Yongjian Wu, Jiya Saiyin, Bingzheng Wei, Maode Lai, Eric\n  Chang, Yan Xu","authorsParsed":[["Zhou","Yang",""],["Wu","Yongjian",""],["Saiyin","Jiya",""],["Wei","Bingzheng",""],["Lai","Maode",""],["Chang","Eric",""],["Xu","Yan",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 06:07:01 GMT"}],"updateDate":"2024-07-17","timestamp":1721110021000,"abstract":"  Prompt tuning methods have achieved remarkable success in parameter-efficient\nfine-tuning on large pre-trained models. However, their application to\ndual-modal fusion-based visual-language pre-trained models (VLPMs), such as\nGLIP, has encountered issues. Existing prompt tuning methods have not\neffectively addressed the modal mapping and aligning problem for tokens in\ndifferent modalities, leading to poor transfer generalization. To address this\nissue, we propose Synchronous Dual Prompt Tuning (SDPT). SDPT initializes a\nsingle set of learnable unified prototype tokens in the established modal\naligning space to represent the aligned semantics of text and image modalities\nfor downstream tasks. Furthermore, SDPT establishes inverse linear projections\nthat require no training to embed the information of unified prototype tokens\ninto the input space of different modalities. The inverse linear projections\nallow the unified prototype token to synchronously represent the two modalities\nand enable SDPT to share the unified semantics of text and image for downstream\ntasks across different modal prompts. Experimental results demonstrate that\nSDPT assists fusion-based VLPMs to achieve superior outcomes with only 0.04\\%\nof model parameters for training across various scenarios, outperforming other\nsingle- or dual-modal methods. The code will be released at\nhttps://github.com/wuyongjianCODE/SDPT.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}