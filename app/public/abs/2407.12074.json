{"id":"2407.12074","title":"Enhancing Parameter Efficiency and Generalization in Large-Scale Models:\n  A Regularized and Masked Low-Rank Adaptation Approach","authors":"Yuzhu Mao, Siqi Ping, Zihao Zhao, Yang Liu, Wenbo Ding","authorsParsed":[["Mao","Yuzhu",""],["Ping","Siqi",""],["Zhao","Zihao",""],["Liu","Yang",""],["Ding","Wenbo",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:26:31 GMT"}],"updateDate":"2024-07-18","timestamp":1721143591000,"abstract":"  Large pre-trained models, such as large language models (LLMs), present\nsignificant resource challenges for fine-tuning due to their extensive\nparameter sizes, especially for applications in mobile systems. To address\nthis, Low-Rank Adaptation (LoRA) has been developed to reduce resource\nconsumption while maintaining satisfactory fine-tuning results. Despite its\neffectiveness, the original LoRA method faces challenges of suboptimal\nperformance and overfitting. This paper investigates the intrinsic dimension of\nthe matrix updates approximated by the LoRA method and reveals the performance\nbenefits of increasing this intrinsic dimension. By employing regularization\nand a gradient masking method that encourages higher intrinsic dimension, the\nproposed method, termed Regularized and Masked LoRA (RM-LoRA), achieves\nsuperior generalization performance with the same or lower trainable parameter\nbudget compared to the original LoRA and its latest variants across various\nopen-source vision and language datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}