{"id":"2408.09273","title":"ConVerSum: A Contrastive Learning based Approach for Data-Scarce\n  Solution of Cross-Lingual Summarization Beyond Direct Equivalents","authors":"Sanzana Karim Lora and Rifat Shahriyar","authorsParsed":[["Lora","Sanzana Karim",""],["Shahriyar","Rifat",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 19:03:53 GMT"}],"updateDate":"2024-08-20","timestamp":1723921433000,"abstract":"  Cross-Lingual summarization (CLS) is a sophisticated branch in Natural\nLanguage Processing that demands models to accurately translate and summarize\narticles from different source languages. Despite the improvement of the\nsubsequent studies, This area still needs data-efficient solutions along with\neffective training methodologies. To the best of our knowledge, there is no\nfeasible solution for CLS when there is no available high-quality CLS data. In\nthis paper, we propose a novel data-efficient approach, ConVerSum, for CLS\nleveraging the power of contrastive learning, generating versatile candidate\nsummaries in different languages based on the given source document and\ncontrasting these summaries with reference summaries concerning the given\ndocuments. After that, we train the model with a contrastive ranking loss.\nThen, we rigorously evaluate the proposed approach against current\nmethodologies and compare it to powerful Large Language Models (LLMs)- Gemini,\nGPT 3.5, and GPT 4 proving our model performs better for low-resource\nlanguages' CLS. These findings represent a substantial improvement in the area,\nopening the door to more efficient and accurate cross-lingual summarizing\ntechniques.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}