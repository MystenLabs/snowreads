{"id":"2408.05023","title":"Investigating a Benchmark for Training-set free Evaluation of Linguistic\n  Capabilities in Machine Reading Comprehension","authors":"Viktor Schlegel, Goran Nenadic and Riza Batista-Navarro","authorsParsed":[["Schlegel","Viktor",""],["Nenadic","Goran",""],["Batista-Navarro","Riza",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 12:23:36 GMT"}],"updateDate":"2024-08-12","timestamp":1723206216000,"abstract":"  Performance of NLP systems is typically evaluated by collecting a large-scale\ndataset by means of crowd-sourcing to train a data-driven model and evaluate it\non a held-out portion of the data. This approach has been shown to suffer from\nspurious correlations and the lack of challenging examples that represent the\ndiversity of natural language. Instead, we examine a framework for evaluating\noptimised models in training-set free setting on synthetically generated\nchallenge sets. We find that despite the simplicity of the generation method,\nthe data can compete with crowd-sourced datasets with regard to naturalness and\nlexical diversity for the purpose of evaluating the linguistic capabilities of\nMRC models. We conduct further experiments and show that state-of-the-art\nlanguage model-based MRC systems can learn to succeed on the challenge set\ncorrectly, although, without capturing the general notion of the evaluated\nphenomenon.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"IKOO5OyTg0ZHUttMJ4DGbtq-ms78g8eeDJBS4MEVu6s","pdfSize":"123794"}
