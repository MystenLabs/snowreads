{"id":"2407.09276","title":"H2O-Danube3 Technical Report","authors":"Pascal Pfeiffer, Philipp Singer, Yauhen Babakhin, Gabor Fodor, Nischay\n  Dhankhar, Sri Satish Ambati","authorsParsed":[["Pfeiffer","Pascal",""],["Singer","Philipp",""],["Babakhin","Yauhen",""],["Fodor","Gabor",""],["Dhankhar","Nischay",""],["Ambati","Sri Satish",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 14:09:40 GMT"}],"updateDate":"2024-07-15","timestamp":1720793380000,"abstract":"  We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}