{"id":"2408.02239","title":"BOTS-LM: Training Large Language Models for Setswana","authors":"Nathan Brown and Vukosi Marivate","authorsParsed":[["Brown","Nathan",""],["Marivate","Vukosi",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 05:15:17 GMT"}],"updateDate":"2024-08-06","timestamp":1722834917000,"abstract":"  In this work we present BOTS-LM, a series of bilingual language models\nproficient in both Setswana and English. Leveraging recent advancements in data\navailability and efficient fine-tuning, BOTS-LM achieves performance similar to\nmodels significantly larger than itself while maintaining computational\nefficiency. Our initial release features an 8 billion parameter generative\nlarge language model, with upcoming 0.5 billion and 1 billion parameter large\nlanguage models and a 278 million parameter encoder-only model soon to be\nreleased. We find the 8 billion parameter model significantly outperforms\nLlama-3-70B and Aya 23 on English-Setswana translation tasks, approaching the\nperformance of dedicated machine translation models, while approaching 70B\nparameter performance on Setswana reasoning as measured by a machine translated\nsubset of the MMLU benchmark. To accompany the BOTS-LM series of language\nmodels, we release the largest Setswana web dataset, SetsText, totalling over\n267 million tokens. In addition, we release the largest machine translated\nSetswana dataset, the first and largest synthetic Setswana dataset, training\nand evaluation code, training logs, and MMLU-tsn, a machine translated subset\nof MMLU.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}