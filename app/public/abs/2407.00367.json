{"id":"2407.00367","title":"SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix","authors":"Peng Dai, Feitong Tan, Qiangeng Xu, David Futschik, Ruofei Du, Sean\n  Fanello, Xiaojuan Qi, Yinda Zhang","authorsParsed":[["Dai","Peng",""],["Tan","Feitong",""],["Xu","Qiangeng",""],["Futschik","David",""],["Du","Ruofei",""],["Fanello","Sean",""],["Qi","Xiaojuan",""],["Zhang","Yinda",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 08:33:55 GMT"}],"updateDate":"2024-07-02","timestamp":1719650035000,"abstract":"  Video generation models have demonstrated great capabilities of producing\nimpressive monocular videos, however, the generation of 3D stereoscopic video\nremains under-explored. We propose a pose-free and training-free approach for\ngenerating 3D stereoscopic videos using an off-the-shelf monocular video\ngeneration model. Our method warps a generated monocular video into camera\nviews on stereoscopic baseline using estimated video depth, and employs a novel\nframe matrix video inpainting framework. The framework leverages the video\ngeneration model to inpaint frames observed from different timestamps and\nviews. This effective approach generates consistent and semantically coherent\nstereoscopic videos without scene optimization or model fine-tuning. Moreover,\nwe develop a disocclusion boundary re-injection scheme that further improves\nthe quality of video inpainting by alleviating the negative effects propagated\nfrom disoccluded areas in the latent space. We validate the efficacy of our\nproposed method by conducting experiments on videos from various generative\nmodels, including Sora [4 ], Lumiere [2], WALT [8 ], and Zeroscope [ 42]. The\nexperiments demonstrate that our method has a significant improvement over\nprevious methods. The code will be released at\n\\url{https://daipengwa.github.io/SVG_ProjectPage}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}