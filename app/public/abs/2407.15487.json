{"id":"2407.15487","title":"In-Context Learning Improves Compositional Understanding of\n  Vision-Language Models","authors":"Matteo Nulli, Anesa Ibrahimi, Avik Pal, Hoshe Lee, Ivona Najdenkoska","authorsParsed":[["Nulli","Matteo",""],["Ibrahimi","Anesa",""],["Pal","Avik",""],["Lee","Hoshe",""],["Najdenkoska","Ivona",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 09:03:29 GMT"}],"updateDate":"2024-07-23","timestamp":1721639009000,"abstract":"  Vision-Language Models (VLMs) have shown remarkable capabilities in a large\nnumber of downstream tasks. Nonetheless, compositional image understanding\nremains a rather difficult task due to the object bias present in training\ndata. In this work, we investigate the reasons for such a lack of capability by\nperforming an extensive bench-marking of compositional understanding in VLMs.\nWe compare contrastive models with generative ones and analyze their\ndifferences in architecture, pre-training data, and training tasks and losses.\nFurthermore, we leverage In-Context Learning (ICL) as a way to improve the\nability of VLMs to perform more complex reasoning and understanding given an\nimage. Our extensive experiments demonstrate that our proposed approach\noutperforms baseline models across multiple compositional understanding\ndatasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}