{"id":"2408.07245","title":"q-exponential family for policy optimization","authors":"Lingwei Zhu, Haseeb Shah, Han Wang, Martha White","authorsParsed":[["Zhu","Lingwei",""],["Shah","Haseeb",""],["Wang","Han",""],["White","Martha",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 01:01:35 GMT"}],"updateDate":"2024-08-15","timestamp":1723597295000,"abstract":"  Policy optimization methods benefit from a simple and tractable policy\nfunctional, usually the Gaussian for continuous action spaces. In this paper,\nwe consider a broader policy family that remains tractable: the $q$-exponential\nfamily. This family of policies is flexible, allowing the specification of both\nheavy-tailed policies ($q>1$) and light-tailed policies ($q<1$). This paper\nexamines the interplay between $q$-exponential policies for several\nactor-critic algorithms conducted on both online and offline problems. We find\nthat heavy-tailed policies are more effective in general and can consistently\nimprove on Gaussian. In particular, we find the Student's t-distribution to be\nmore stable than the Gaussian across settings and that a heavy-tailed\n$q$-Gaussian for Tsallis Advantage Weighted Actor-Critic consistently performs\nwell in offline benchmark problems. Our code is available at\n\\url{https://github.com/lingweizhu/qexp}.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}