{"id":"2407.06723","title":"Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting\n  Region Captions","authors":"Yu-Guan Hsieh, Cheng-Yu Hsieh, Shih-Ying Yeh, Louis B\\'ethune, Hadi\n  Pour Ansari, Pavan Kumar Anasosalu Vasu, Chun-Liang Li, Ranjay Krishna, Oncel\n  Tuzel, Marco Cuturi","authorsParsed":[["Hsieh","Yu-Guan",""],["Hsieh","Cheng-Yu",""],["Yeh","Shih-Ying",""],["BÃ©thune","Louis",""],["Ansari","Hadi Pour",""],["Vasu","Pavan Kumar Anasosalu",""],["Li","Chun-Liang",""],["Krishna","Ranjay",""],["Tuzel","Oncel",""],["Cuturi","Marco",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 09:55:04 GMT"}],"updateDate":"2024-07-10","timestamp":1720518904000,"abstract":"  Humans describe complex scenes with compositionality, using simple text\ndescriptions enriched with links and relationships. While vision-language\nresearch has aimed to develop models with compositional understanding\ncapabilities, this is not reflected yet in existing datasets which, for the\nmost part, still use plain text to describe images. In this work, we propose a\nnew annotation strategy, graph-based captioning (GBC) that describes an image\nusing a labelled graph structure, with nodes of various types. The nodes in GBC\nare created using, in a first stage, object detection and dense captioning\ntools nested recursively to uncover and describe entity nodes, further linked\ntogether in a second stage by highlighting, using new types of nodes,\ncompositions and relations among entities. Since all GBC nodes hold plain text\ndescriptions, GBC retains the flexibility found in natural language, but can\nalso encode hierarchical information in its edges. We demonstrate that GBC can\nbe produced automatically, using off-the-shelf multimodal LLMs and\nopen-vocabulary detection models, by building a new dataset, GBC10M, gathering\nGBC annotations for about 10M images of the CC12M dataset. We use GBC10M to\nshowcase the wealth of node captions uncovered by GBC, as measured with CLIP\ntraining. We show that using GBC nodes' annotations -- notably those stored in\ncomposition and relation nodes -- results in significant performance boost on\ndownstream models when compared to other dataset formats. To further explore\nthe opportunities provided by GBC, we also propose a new attention mechanism\nthat can leverage the entire GBC graph, with encouraging experimental results\nthat show the extra benefits of incorporating the graph structure. Our datasets\nare released at \\url{https://huggingface.co/graph-based-captions}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}