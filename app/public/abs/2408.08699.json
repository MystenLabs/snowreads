{"id":"2408.08699","title":"RBLA: Rank-Based-LoRA-Aggregation for Fine-tuning Heterogeneous Models\n  in FLaaS","authors":"Shuaijun Chen, Omid Tavallaie, Niousha Nazemi, Albert Y. Zomaya","authorsParsed":[["Chen","Shuaijun",""],["Tavallaie","Omid",""],["Nazemi","Niousha",""],["Zomaya","Albert Y.",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 12:26:36 GMT"}],"updateDate":"2024-08-19","timestamp":1723811196000,"abstract":"  Federated Learning (FL) is a promising privacy-aware distributed learning\nframework that can be deployed on various devices, such as mobile phones,\ndesktops, and devices equipped with CPUs or GPUs. In the context of\nserver-based Federated Learning as a Service (FLaas), FL enables the central\nserver to coordinate the training process across multiple devices without\ndirect access to the local data, thereby enhancing privacy and data security.\nLow-Rank Adaptation (LoRA) is a method that fine-tunes models efficiently by\nfocusing on a low-dimensional subspace of the model's parameters. This approach\nsignificantly reduces computational and memory costs compared to fine-tuning\nall parameters from scratch. When integrated with FL, especially in a FLaas\nenvironment, LoRA allows for flexible and efficient deployment across diverse\nhardware with varying computational capabilities by adjusting the local model's\nrank. However, in LoRA-enabled FL, different clients may train models with\nvarying ranks, which poses a challenge for model aggregation on the server.\nCurrent methods of aggregating models of different ranks require padding\nweights to a uniform shape, which can degrade the global model's performance.\nTo address this issue, we propose Rank-Based LoRA Aggregation (RBLA), a novel\nmodel aggregation method designed for heterogeneous LoRA structures. RBLA\npreserves key features across models with different ranks. This paper analyzes\nthe issues with current padding methods that reshape models for aggregation in\na FLaas environment. Then, we introduce RBLA, a rank-based aggregation method\nthat maintains both low-rank and high-rank features. Finally, we demonstrate\nthe effectiveness of RBLA through comparative experiments with state-of-the-art\nmethods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}