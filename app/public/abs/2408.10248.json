{"id":"2408.10248","title":"Target-Dependent Multimodal Sentiment Analysis Via Employing Visual-to\n  Emotional-Caption Translation Network using Visual-Caption Pairs","authors":"Ananya Pandey, Dinesh Kumar Vishwakarma","authorsParsed":[["Pandey","Ananya",""],["Vishwakarma","Dinesh Kumar",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 15:56:55 GMT"}],"updateDate":"2024-08-21","timestamp":1722873415000,"abstract":"  The natural language processing and multimedia field has seen a notable surge\nin interest in multimodal sentiment recognition. Hence, this study aims to\nemploy Target-Dependent Multimodal Sentiment Analysis (TDMSA) to identify the\nlevel of sentiment associated with every target (aspect) stated within a\nmultimodal post consisting of a visual-caption pair. Despite the recent\nadvancements in multimodal sentiment recognition, there has been a lack of\nexplicit incorporation of emotional clues from the visual modality,\nspecifically those pertaining to facial expressions. The challenge at hand is\nto proficiently obtain visual and emotional clues and subsequently synchronise\nthem with the textual content. In light of this fact, this study presents a\nnovel approach called the Visual-to-Emotional-Caption Translation Network\n(VECTN) technique. The primary objective of this strategy is to effectively\nacquire visual sentiment clues by analysing facial expressions. Additionally,\nit effectively aligns and blends the obtained emotional clues with the target\nattribute of the caption mode. The experimental findings demonstrate that our\nmethodology is capable of producing ground-breaking outcomes when applied to\ntwo publicly accessible multimodal Twitter datasets, namely, Twitter-2015 and\nTwitter-2017. The experimental results show that the suggested model achieves\nan accuracy of 81.23% and a macro-F1 of 80.61% on the Twitter-15 dataset, while\n77.42% and 75.19% on the Twitter-17 dataset, respectively. The observed\nimprovement in performance reveals that our model is better than others when it\ncomes to collecting target-level sentiment in multimodal data using the\nexpressions of the face.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}