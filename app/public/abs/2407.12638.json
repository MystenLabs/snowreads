{"id":"2407.12638","title":"ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer\n  Neural Networks","authors":"Salma Afifi, Ishan Thakkar, Sudeep Pasricha","authorsParsed":[["Afifi","Salma",""],["Thakkar","Ishan",""],["Pasricha","Sudeep",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 15:08:14 GMT"}],"updateDate":"2024-07-18","timestamp":1721228894000,"abstract":"  Transformers have emerged as a powerful tool for natural language processing\n(NLP) and computer vision. Through the attention mechanism, these models have\nexhibited remarkable performance gains when compared to conventional approaches\nlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\nNevertheless, transformers typically demand substantial execution time due to\ntheir extensive computations and large memory footprint. Processing in-memory\n(PIM) and near-memory computing (NMC) are promising solutions to accelerating\ntransformers as they offer high compute parallelism and memory bandwidth.\nHowever, designing PIM/NMC architectures to support the complex operations and\nmassive amounts of data that need to be moved between layers in transformer\nneural networks remains a challenge. We propose ARTEMIS, a mixed\nanalog-stochastic in-DRAM accelerator for transformer models. Through employing\nminimal changes to the conventional DRAM arrays, ARTEMIS efficiently alleviates\nthe costs associated with transformer model execution by supporting stochastic\ncomputing for multiplications and temporal analog accumulations using a novel\nin-DRAM metal-on-metal capacitor. Our analysis indicates that ARTEMIS exhibits\nat least 3.0x speedup, 1.8x lower energy, and 1.9x better energy efficiency\ncompared to GPU, TPU, CPU, and state-of-the-art PIM transformer hardware\naccelerators.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}