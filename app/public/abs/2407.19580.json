{"id":"2407.19580","title":"Memory-efficient Training of LLMs with Larger Mini-batches","authors":"Dang Nguyen, Wenhan Yang, Rathul Anand, Yu Yang, Baharan Mirzasoleiman","authorsParsed":[["Nguyen","Dang",""],["Yang","Wenhan",""],["Anand","Rathul",""],["Yang","Yu",""],["Mirzasoleiman","Baharan",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 20:39:16 GMT"}],"updateDate":"2024-07-30","timestamp":1722199156000,"abstract":"  Training with larger mini-batches improves the performance and convergence\nrate of training machine learning models. However, training with large\nmini-batches becomes prohibitive for Large Language Models (LLMs) with billions\nof parameters, due to the large GPU memory requirement. To address this\nproblem, we propose finding small mini-batches that simulate the dynamics of\ntraining with larger mini-batches. Specifically, we formulate selecting smaller\nmini-batches of examples that closely capture gradients of large mini-batches\nas a submodular maximization problem. Nevertheless, the very large\ndimensionality of the gradients makes the problem very challenging to solve. To\naddress this, we leverage ideas from zeroth-order optimization and neural\nnetwork pruning to find lower-dimensional gradient estimates that allow finding\nhigh-quality subsets effectively with a limited amount of memory. We prove the\nsuperior convergence rate of training on the small mini-batches found by our\nmethod and empirically show its effectiveness. Our method can effectively\nreduce the memory requirement by 2x and speed up training by 1.3x, as we\nconfirm for fine-tuning Phi-2 on MathInstruct. Our method can be easily stacked\nwith LoRA and other memory-efficient methods to further reduce the memory\nrequirements of training LLMs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}