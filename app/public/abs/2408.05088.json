{"id":"2408.05088","title":"UNIC: Universal Classification Models via Multi-teacher Distillation","authors":"Mert Bulent Sariyildiz, Philippe Weinzaepfel, Thomas Lucas, Diane\n  Larlus, Yannis Kalantidis","authorsParsed":[["Sariyildiz","Mert Bulent",""],["Weinzaepfel","Philippe",""],["Lucas","Thomas",""],["Larlus","Diane",""],["Kalantidis","Yannis",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 14:18:57 GMT"}],"updateDate":"2024-08-12","timestamp":1723213137000,"abstract":"  Pretrained models have become a commodity and offer strong results on a broad\nrange of tasks. In this work, we focus on classification and seek to learn a\nunique encoder able to take from several complementary pretrained models. We\naim at even stronger generalization across a variety of classification tasks.\nWe propose to learn such an encoder via multi-teacher distillation. We first\nthoroughly analyse standard distillation when driven by multiple strong\nteachers with complementary strengths. Guided by this analysis, we gradually\npropose improvements to the basic distillation setup. Among those, we enrich\nthe architecture of the encoder with a ladder of expendable projectors, which\nincreases the impact of intermediate features during distillation, and we\nintroduce teacher dropping, a regularization mechanism that better balances the\nteachers' influence. Our final distillation strategy leads to student models of\nthe same capacity as any of the teachers, while retaining or improving upon the\nperformance of the best teacher for each task.\n  Project page and code: https://europe.naverlabs.com/unic\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}