{"id":"2408.15488","title":"Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services","authors":"Jialin Wu, Jiangyi Deng, Shengyuan Pang, Yanjiao Chen, Jiayang Xu,\n  Xinfeng Li, Wenyuan Xu","authorsParsed":[["Wu","Jialin",""],["Deng","Jiangyi",""],["Pang","Shengyuan",""],["Chen","Yanjiao",""],["Xu","Jiayang",""],["Li","Xinfeng",""],["Xu","Wenyuan",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 02:27:07 GMT"},{"version":"v2","created":"Thu, 5 Sep 2024 15:50:44 GMT"}],"updateDate":"2024-09-06","timestamp":1724812027000,"abstract":"  Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}