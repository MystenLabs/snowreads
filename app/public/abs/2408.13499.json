{"id":"2408.13499","title":"R2G: Reasoning to Ground in 3D Scenes","authors":"Yixuan Li, Zan Wang, Wei Liang","authorsParsed":[["Li","Yixuan",""],["Wang","Zan",""],["Liang","Wei",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 06:52:14 GMT"}],"updateDate":"2024-08-27","timestamp":1724482334000,"abstract":"  We propose Reasoning to Ground (R2G), a neural symbolic model that grounds\nthe target objects within 3D scenes in a reasoning manner. In contrast to prior\nworks, R2G explicitly models the 3D scene with a semantic concept-based scene\ngraph; recurrently simulates the attention transferring across object entities;\nthus makes the process of grounding the target objects with the highest\nprobability interpretable. Specifically, we respectively embed multiple object\nproperties within the graph nodes and spatial relations among entities within\nthe edges, utilizing a predefined semantic vocabulary. To guide attention\ntransferring, we employ learning or prompting-based methods to analyze the\nreferential utterance and convert it into reasoning instructions within the\nsame semantic space. In each reasoning round, R2G either (1) merges current\nattention distribution with the similarity between the instruction and embedded\nentity properties or (2) shifts the attention across the scene graph based on\nthe similarity between the instruction and embedded spatial relations. The\nexperiments on Sr3D/Nr3D benchmarks show that R2G achieves a comparable result\nwith the prior works while maintaining improved interpretability, breaking a\nnew path for 3D language grounding.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0aCizhtzVKTysK6c9sldsho_x7gqxVQdGkX9WCBYEMM","pdfSize":"9293538"}
