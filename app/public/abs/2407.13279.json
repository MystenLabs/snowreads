{"id":"2407.13279","title":"Analyzing and Bridging the Gap between Maximizing Total Reward and\n  Discounted Reward in Deep Reinforcement Learning","authors":"Shuyu Yin, Fei Wen, Peilin Liu, Tao Luo","authorsParsed":[["Yin","Shuyu",""],["Wen","Fei",""],["Liu","Peilin",""],["Luo","Tao",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 08:33:10 GMT"}],"updateDate":"2024-07-19","timestamp":1721291590000,"abstract":"  In deep reinforcement learning applications, maximizing discounted reward is\noften employed instead of maximizing total reward to ensure the convergence and\nstability of algorithms, even though the performance metric for evaluating the\npolicy remains the total reward. However, the optimal policies corresponding to\nthese two objectives may not always be consistent. To address this issue, we\nanalyzed the suboptimality of the policy obtained through maximizing discounted\nreward in relation to the policy that maximizes total reward and identified the\ninfluence of hyperparameters. Additionally, we proposed sufficient conditions\nfor aligning the optimal policies of these two objectives under various\nsettings. The primary contributions are as follows: We theoretically analyzed\nthe factors influencing performance when using discounted reward as a proxy for\ntotal reward, thereby enhancing the theoretical understanding of this scenario.\nFurthermore, we developed methods to align the optimal policies of the two\nobjectives in certain situations, which can improve the performance of\nreinforcement learning algorithms.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}