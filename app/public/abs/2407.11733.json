{"id":"2407.11733","title":"How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine\n  Studies","authors":"Alina Leidinger and Richard Rogers","authorsParsed":[["Leidinger","Alina",""],["Rogers","Richard",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:04:35 GMT"},{"version":"v2","created":"Thu, 1 Aug 2024 15:09:12 GMT"}],"updateDate":"2024-08-02","timestamp":1721138675000,"abstract":"  With the widespread availability of LLMs since the release of ChatGPT and\nincreased public scrutiny, commercial model development appears to have focused\ntheir efforts on 'safety' training concerning legal liabilities at the expense\nof social impact evaluation. This mimics a similar trend which we could observe\nfor search engine autocompletion some years prior. We draw on scholarship from\nNLP and search engine auditing and present a novel evaluation task in the style\nof autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by\nusing four metrics, namely refusal rates, toxicity, sentiment and regard, with\nand without safety system prompts. Our findings indicate an improvement to\nstereotyping outputs with the system prompt, but overall a lack of attention by\nLLMs under study to certain harms classified as toxic, particularly for prompts\nabout peoples/ethnicities and sexual orientation. Mentions of intersectional\nidentities trigger a disproportionate amount of stereotyping. Finally, we\ndiscuss the implications of these findings about stereotyping harms in light of\nthe coming intermingling of LLMs and search and the choice of stereotyping\nmitigation policy to adopt. We address model builders, academics, NLP\npractitioners and policy makers, calling for accountability and awareness\nconcerning stereotyping harms, be it for training data curation, leader board\ndesign and usage, or social impact measurement.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}