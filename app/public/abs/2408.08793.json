{"id":"2408.08793","title":"Backward-Compatible Aligned Representations via an Orthogonal\n  Transformation Layer","authors":"Simone Ricci, Niccol\\`o Biondi, Federico Pernici, Alberto Del Bimbo","authorsParsed":[["Ricci","Simone",""],["Biondi","Niccol√≤",""],["Pernici","Federico",""],["Del Bimbo","Alberto",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 15:05:28 GMT"}],"updateDate":"2024-08-19","timestamp":1723820728000,"abstract":"  Visual retrieval systems face significant challenges when updating models\nwith improved representations due to misalignment between the old and new\nrepresentations. The costly and resource-intensive backfilling process involves\nrecalculating feature vectors for images in the gallery set whenever a new\nmodel is introduced. To address this, prior research has explored\nbackward-compatible training methods that enable direct comparisons between new\nand old representations without backfilling. Despite these advancements,\nachieving a balance between backward compatibility and the performance of\nindependently trained models remains an open problem. In this paper, we address\nit by expanding the representation space with additional dimensions and\nlearning an orthogonal transformation to achieve compatibility with old models\nand, at the same time, integrate new information. This transformation preserves\nthe original feature space's geometry, ensuring that our model aligns with\nprevious versions while also learning new data. Our Orthogonal Compatible\nAligned (OCA) approach eliminates the need for re-indexing during model updates\nand ensures that features can be compared directly across different model\nupdates without additional mapping functions. Experimental results on CIFAR-100\nand ImageNet-1k demonstrate that our method not only maintains compatibility\nwith previous models but also achieves state-of-the-art accuracy, outperforming\nseveral existing methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pldVTRl04DHMTI8KJFK1T1KFxL-zeN6lHVR_G7FSTPI","pdfSize":"573524"}
