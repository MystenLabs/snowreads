{"id":"2407.01853","title":"Improving Multilingual Instruction Finetuning via Linguistically Natural\n  and Diverse Datasets","authors":"Sathish Reddy Indurthi, Wenxuan Zhou, Shamil Chollampatt, Ravi\n  Agrawal, Kaiqiang Song, Lingxiao Zhao, Chenguang Zhu","authorsParsed":[["Indurthi","Sathish Reddy",""],["Zhou","Wenxuan",""],["Chollampatt","Shamil",""],["Agrawal","Ravi",""],["Song","Kaiqiang",""],["Zhao","Lingxiao",""],["Zhu","Chenguang",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 23:47:09 GMT"}],"updateDate":"2024-07-03","timestamp":1719877629000,"abstract":"  Advancements in Large Language Models (LLMs) have significantly enhanced\ninstruction-following capabilities. However, most Instruction Fine-Tuning (IFT)\ndatasets are predominantly in English, limiting model performance in other\nlanguages. Traditional methods for creating multilingual IFT datasets such as\ntranslating existing English IFT datasets or converting existing NLP datasets\ninto IFT datasets by templating, struggle to capture linguistic nuances and\nensure prompt (instruction) diversity. To address this issue, we propose a\nnovel method for collecting multilingual IFT datasets that preserves linguistic\nnaturalness and ensures prompt diversity. This approach leverages\nEnglish-focused LLMs, monolingual corpora, and a scoring function to create\nhigh-quality, diversified IFT datasets in multiple languages. Experiments\ndemonstrate that LLMs finetuned using these IFT datasets show notable\nimprovements in both generative and discriminative tasks, indicating enhanced\nlanguage comprehension by LLMs in non-English contexts. Specifically, on the\nmultilingual summarization task, LLMs using our IFT dataset achieved 17.57% and\n15.23% improvements over LLMs fine-tuned with translation-based and\ntemplate-based datasets, respectively.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}