{"id":"2407.07786","title":"The Human Factor in AI Red Teaming: Perspectives from Social and\n  Collaborative Computing","authors":"Alice Qian Zhang, Ryland Shaw, Jacy Reese Anthis, Ashlee Milton, Emily\n  Tseng, Jina Suh, Lama Ahmad, Ram Shankar Siva Kumar, Julian Posada, Benjamin\n  Shestakofsky, Sarah T. Roberts, Mary L. Gray","authorsParsed":[["Zhang","Alice Qian",""],["Shaw","Ryland",""],["Anthis","Jacy Reese",""],["Milton","Ashlee",""],["Tseng","Emily",""],["Suh","Jina",""],["Ahmad","Lama",""],["Kumar","Ram Shankar Siva",""],["Posada","Julian",""],["Shestakofsky","Benjamin",""],["Roberts","Sarah T.",""],["Gray","Mary L.",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:02:13 GMT"},{"version":"v2","created":"Wed, 11 Sep 2024 16:02:31 GMT"}],"updateDate":"2024-09-12","timestamp":1720627333000,"abstract":"  Rapid progress in general-purpose AI has sparked significant interest in \"red\nteaming,\" a practice of adversarial testing originating in military and\ncybersecurity applications. AI red teaming raises many questions about the\nhuman factor, such as how red teamers are selected, biases and blindspots in\nhow tests are conducted, and harmful content's psychological effects on red\nteamers. A growing body of HCI and CSCW literature examines related\npractices-including data labeling, content moderation, and algorithmic\nauditing. However, few, if any have investigated red teaming itself. Future\nstudies may explore topics ranging from fairness to mental health and other\nareas of potential harm. We aim to facilitate a community of researchers and\npractitioners who can begin to meet these challenges with creativity,\ninnovation, and thoughtful reflection.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"MVNGonQ7_cQrQGaiHKyUyx64EEy_BWQGcfd-TNmy9n0","pdfSize":"131293"}
