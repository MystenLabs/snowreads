{"id":"2407.12860","title":"STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained\n  LLMs","authors":"Aaron Zolnai-Lucas, Jack Boylan, Chris Hokamp, Parsa Ghaffari","authorsParsed":[["Zolnai-Lucas","Aaron",""],["Boylan","Jack",""],["Hokamp","Chris",""],["Ghaffari","Parsa",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 08:50:25 GMT"}],"updateDate":"2024-07-19","timestamp":1720601425000,"abstract":"  We present Simplified Text-Attributed Graph Embeddings (STAGE), a\nstraightforward yet effective method for enhancing node features in Graph\nNeural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our\napproach leverages Large-Language Models (LLMs) to generate embeddings for\ntextual attributes. STAGE achieves competitive results on various node\nclassification benchmarks while also maintaining a simplicity in implementation\nrelative to current state-of-the-art (SoTA) techniques. We show that utilizing\npre-trained LLMs as embedding generators provides robust features for ensemble\nGNN training, enabling pipelines that are simpler than current SoTA approaches\nwhich require multiple expensive training and prompting stages. We also\nimplement diffusion-pattern GNNs in an effort to make this pipeline scalable to\ngraphs beyond academic benchmarks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}