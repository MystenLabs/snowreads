{"id":"2407.12860","title":"STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained\n  LLMs","authors":"Aaron Zolnai-Lucas, Jack Boylan, Chris Hokamp, Parsa Ghaffari","authorsParsed":[["Zolnai-Lucas","Aaron",""],["Boylan","Jack",""],["Hokamp","Chris",""],["Ghaffari","Parsa",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 08:50:25 GMT"}],"updateDate":"2024-07-19","timestamp":1720601425000,"abstract":"  We present Simplified Text-Attributed Graph Embeddings (STAGE), a\nstraightforward yet effective method for enhancing node features in Graph\nNeural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our\napproach leverages Large-Language Models (LLMs) to generate embeddings for\ntextual attributes. STAGE achieves competitive results on various node\nclassification benchmarks while also maintaining a simplicity in implementation\nrelative to current state-of-the-art (SoTA) techniques. We show that utilizing\npre-trained LLMs as embedding generators provides robust features for ensemble\nGNN training, enabling pipelines that are simpler than current SoTA approaches\nwhich require multiple expensive training and prompting stages. We also\nimplement diffusion-pattern GNNs in an effort to make this pipeline scalable to\ngraphs beyond academic benchmarks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3gU_iJqB83eOPS4tjFNyScVMzYdv3YbK3l3T-flbRn4","pdfSize":"603131","objectId":"0x0a27d0471fe8b814a577a682e3a01d82ec0ece7c3c83b39e8321a0f333688307","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
