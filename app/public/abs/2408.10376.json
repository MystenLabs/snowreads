{"id":"2408.10376","title":"Self-Play Ensemble Q-learning enabled Resource Allocation for Network\n  Slicing","authors":"Shavbo Salehi, Pedro Enrique Iturria-Rivera, Medhat Elsayed, Majid\n  Bavand, Raimundas Gaigalas, Yigit Ozcan, and Melike Erol-Kantarci","authorsParsed":[["Salehi","Shavbo",""],["Iturria-Rivera","Pedro Enrique",""],["Elsayed","Medhat",""],["Bavand","Majid",""],["Gaigalas","Raimundas",""],["Ozcan","Yigit",""],["Erol-Kantarci","Melike",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 19:47:38 GMT"}],"updateDate":"2024-08-21","timestamp":1724096858000,"abstract":"  In 5G networks, network slicing has emerged as a pivotal paradigm to address\ndiverse user demands and service requirements. To meet the requirements,\nreinforcement learning (RL) algorithms have been utilized widely, but this\nmethod has the problem of overestimation and exploration-exploitation\ntrade-offs. To tackle these problems, this paper explores the application of\nself-play ensemble Q-learning, an extended version of the RL-based technique.\nSelf-play ensemble Q-learning utilizes multiple Q-tables with various\nexploration-exploitation rates leading to different observations for choosing\nthe most suitable action for each state. Moreover, through self-play, each\nmodel endeavors to enhance its performance compared to its previous iterations,\nboosting system efficiency, and decreasing the effect of overestimation. For\nperformance evaluation, we consider three RL-based algorithms; self-play\nensemble Q-learning, double Q-learning, and Q-learning, and compare their\nperformance under different network traffic. Through simulations, we\ndemonstrate the effectiveness of self-play ensemble Q-learning in meeting the\ndiverse demands within 21.92% in latency, 24.22% in throughput, and 23.63\\% in\npacket drop rate in comparison with the baseline methods. Furthermore, we\nevaluate the robustness of self-play ensemble Q-learning and double Q-learning\nin situations where one of the Q-tables is affected by a malicious user. Our\nresults depicted that the self-play ensemble Q-learning method is more robust\nagainst adversarial users and prevents a noticeable drop in system performance,\nmitigating the impact of users manipulating policies.\n","subjects":["Computing Research Repository/Networking and Internet Architecture","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"CwxYjn8hwdFcL0iMY5o5GH6Ld_z2W0OSGE_J6d1vhSs","pdfSize":"539142"}
