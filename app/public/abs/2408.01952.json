{"id":"2408.01952","title":"CACE-Net: Co-guidance Attention and Contrastive Enhancement for\n  Effective Audio-Visual Event Localization","authors":"Xiang He, Xiangxi Liu, Yang Li, Dongcheng Zhao, Guobin Shen, Qingqun\n  Kong, Xin Yang, Yi Zeng","authorsParsed":[["He","Xiang",""],["Liu","Xiangxi",""],["Li","Yang",""],["Zhao","Dongcheng",""],["Shen","Guobin",""],["Kong","Qingqun",""],["Yang","Xin",""],["Zeng","Yi",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 07:48:12 GMT"}],"updateDate":"2024-08-06","timestamp":1722757692000,"abstract":"  The audio-visual event localization task requires identifying concurrent\nvisual and auditory events from unconstrained videos within a network model,\nlocating them, and classifying their category. The efficient extraction and\nintegration of audio and visual modal information have always been challenging\nin this field. In this paper, we introduce CACE-Net, which differs from most\nexisting methods that solely use audio signals to guide visual information. We\npropose an audio-visual co-guidance attention mechanism that allows for\nadaptive bi-directional cross-modal attentional guidance between audio and\nvisual information, thus reducing inconsistencies between modalities. Moreover,\nwe have observed that existing methods have difficulty distinguishing between\nsimilar background and event and lack the fine-grained features for event\nclassification. Consequently, we employ background-event contrast enhancement\nto increase the discrimination of fused feature and fine-tuned pre-trained\nmodel to extract more refined and discernible features from complex multimodal\ninputs. Specifically, we have enhanced the model's ability to discern subtle\ndifferences between event and background and improved the accuracy of event\nclassification in our model. Experiments on the AVE dataset demonstrate that\nCACE-Net sets a new benchmark in the audio-visual event localization task,\nproving the effectiveness of our proposed methods in handling complex\nmultimodal learning and event localization in unconstrained videos. Code is\navailable at https://github.com/Brain-Cog-Lab/CACE-Net.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}