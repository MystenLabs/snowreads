{"id":"2407.00806","title":"Benchmarks for Reinforcement Learning with Biased Offline Data and\n  Imperfect Simulators","authors":"Ori Linial, Guy Tennenholtz, Uri Shalit","authorsParsed":[["Linial","Ori",""],["Tennenholtz","Guy",""],["Shalit","Uri",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 19:22:59 GMT"}],"updateDate":"2024-07-02","timestamp":1719775379000,"abstract":"  In many reinforcement learning (RL) applications one cannot easily let the\nagent act in the world; this is true for autonomous vehicles, healthcare\napplications, and even some recommender systems, to name a few examples.\nOffline RL provides a way to train agents without real-world exploration, but\nis often faced with biases due to data distribution shifts, limited coverage,\nand incomplete representation of the environment. To address these issues,\npractical applications have tried to combine simulators with grounded offline\ndata, using so-called hybrid methods. However, constructing a reliable\nsimulator is in itself often challenging due to intricate system complexities\nas well as missing or incomplete information. In this work, we outline four\nprincipal challenges for combining offline data with imperfect simulators in\nRL: simulator modeling error, partial observability, state and action\ndiscrepancies, and hidden confounding. To help drive the RL community to pursue\nthese problems, we construct ``Benchmarks for Mechanistic Offline Reinforcement\nLearning'' (B4MRL), which provide dataset-simulator benchmarks for the\naforementioned challenges. Our results suggest the key necessity of such\nbenchmarks for future research.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}