{"id":"2407.15447","title":"SIGMA: Sinkhorn-Guided Masked Video Modeling","authors":"Mohammadreza Salehi, Michael Dorkenwald, Fida Mohammad Thoker,\n  Efstratios Gavves, Cees G. M. Snoek, Yuki M. Asano","authorsParsed":[["Salehi","Mohammadreza",""],["Dorkenwald","Michael",""],["Thoker","Fida Mohammad",""],["Gavves","Efstratios",""],["Snoek","Cees G. M.",""],["Asano","Yuki M.",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 08:04:09 GMT"}],"updateDate":"2024-07-23","timestamp":1721635449000,"abstract":"  Video-based pretraining offers immense potential for learning strong visual\nrepresentations on an unprecedented scale. Recently, masked video modeling\nmethods have shown promising scalability, yet fall short in capturing\nhigher-level semantics due to reconstructing predefined low-level targets such\nas pixels. To tackle this, we present Sinkhorn-guided Masked Video Modelling\n(SIGMA), a novel video pretraining method that jointly learns the video model\nin addition to a target feature space using a projection network. However, this\nsimple modification means that the regular L2 reconstruction loss will lead to\ntrivial solutions as both networks are jointly optimized. As a solution, we\ndistribute features of space-time tubes evenly across a limited number of\nlearnable clusters. By posing this as an optimal transport problem, we enforce\nhigh entropy in the generated features across the batch, infusing semantic and\ntemporal meaning into the feature space. The resulting cluster assignments are\nused as targets for a symmetric prediction task where the video model predicts\ncluster assignment of the projection network and vice versa. Experimental\nresults on ten datasets across three benchmarks validate the effectiveness of\nSIGMA in learning more performant, temporally-aware, and robust video\nrepresentations improving upon state-of-the-art methods. Our project website\nwith code is available at: https://quva-lab.github.io/SIGMA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}