{"id":"2408.10500","title":"SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for\n  Multimodal Emotion Recognition","authors":"Zebang Cheng, Shuyuan Tu, Dawei Huang, Minghan Li, Xiaojiang Peng,\n  Zhi-Qi Cheng, Alexander G. Hauptmann","authorsParsed":[["Cheng","Zebang",""],["Tu","Shuyuan",""],["Huang","Dawei",""],["Li","Minghan",""],["Peng","Xiaojiang",""],["Cheng","Zhi-Qi",""],["Hauptmann","Alexander G.",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 02:46:03 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 18:58:26 GMT"}],"updateDate":"2024-08-23","timestamp":1724121963000,"abstract":"  This paper presents our winning approach for the MER-NOISE and MER-OV tracks\nof the MER2024 Challenge on multimodal emotion recognition. Our system\nleverages the advanced emotional understanding capabilities of Emotion-LLaMA to\ngenerate high-quality annotations for unlabeled samples, addressing the\nchallenge of limited labeled data. To enhance multimodal fusion while\nmitigating modality-specific noise, we introduce Conv-Attention, a lightweight\nand efficient hybrid framework. Extensive experimentation vali-dates the\neffectiveness of our approach. In the MER-NOISE track, our system achieves a\nstate-of-the-art weighted average F-score of 85.30%, surpassing the second and\nthird-place teams by 1.47% and 1.65%, respectively. For the MER-OV track, our\nutilization of Emotion-LLaMA for open-vocabulary annotation yields an 8.52%\nimprovement in average accuracy and recall compared to GPT-4V, securing the\nhighest score among all participating large multimodal models. The code and\nmodel for Emotion-LLaMA are available at\nhttps://github.com/ZebangCheng/Emotion-LLaMA.\n","subjects":["Computing Research Repository/Multimedia","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}