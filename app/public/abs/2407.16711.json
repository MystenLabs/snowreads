{"id":"2407.16711","title":"Benchmarks as Microscopes: A Call for Model Metrology","authors":"Michael Saxon, Ari Holtzman, Peter West, William Yang Wang, Naomi\n  Saphra","authorsParsed":[["Saxon","Michael",""],["Holtzman","Ari",""],["West","Peter",""],["Wang","William Yang",""],["Saphra","Naomi",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 17:52:12 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 04:48:26 GMT"}],"updateDate":"2024-07-31","timestamp":1721670732000,"abstract":"  Modern language models (LMs) pose a new challenge in capability assessment.\nStatic benchmarks inevitably saturate without providing confidence in the\ndeployment tolerances of LM-based systems, but developers nonetheless claim\nthat their models have generalized traits such as reasoning or open-domain\nlanguage understanding based on these flawed metrics. The science and practice\nof LMs requires a new approach to benchmarking which measures specific\ncapabilities with dynamic assessments. To be confident in our metrics, we need\na new discipline of model metrology -- one which focuses on how to generate\nbenchmarks that predict performance under deployment. Motivated by our\nevaluation criteria, we outline how building a community of model metrology\npractitioners -- one focused on building tools and studying how to measure\nsystem capabilities -- is the best way to meet these needs to and add clarity\nto the AI discussion.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}