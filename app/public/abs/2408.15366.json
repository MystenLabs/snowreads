{"id":"2408.15366","title":"Pitfalls and Outlooks in Using COMET","authors":"Vil\\'em Zouhar, Pinzhen Chen, Tsz Kin Lam, Nikita Moghe, Barry Haddow","authorsParsed":[["Zouhar","Vil√©m",""],["Chen","Pinzhen",""],["Lam","Tsz Kin",""],["Moghe","Nikita",""],["Haddow","Barry",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 19:03:11 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 08:18:52 GMT"}],"updateDate":"2024-09-04","timestamp":1724785391000,"abstract":"  Since its introduction, the COMET metric has blazed a trail in the machine\ntranslation community, given its strong correlation with human judgements of\ntranslation quality. Its success stems from being a modified pre-trained\nmultilingual model finetuned for quality assessment. However, it being a\nmachine learning model also gives rise to a new set of pitfalls that may not be\nwidely known. We investigate these unexpected behaviours from three aspects: 1)\ntechnical: obsolete software versions and compute precision; 2) data: empty\ncontent, language mismatch, and translationese at test time as well as\ndistribution and domain biases in training; 3) usage and reporting:\nmulti-reference support and model referencing in the literature. All of these\nproblems imply that COMET scores is not comparable between papers or even\ntechnical setups and we put forward our perspective on fixing each issue.\nFurthermore, we release the SacreCOMET package that can generate a signature\nfor the software and model configuration as well as an appropriate citation.\nThe goal of this work is to help the community make more sound use of the COMET\nmetric.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}