{"id":"2408.15495","title":"Remove Symmetries to Control Model Expressivity","authors":"Liu Ziyin, Yizhou Xu, Isaac Chuang","authorsParsed":[["Ziyin","Liu",""],["Xu","Yizhou",""],["Chuang","Isaac",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 02:45:41 GMT"}],"updateDate":"2024-08-29","timestamp":1724813141000,"abstract":"  When symmetry is present in the loss function, the model is likely to be\ntrapped in a low-capacity state that is sometimes known as a \"collapse.\" Being\ntrapped in these low-capacity states can be a major obstacle to training across\nmany scenarios where deep learning technology is applied. We first prove two\nconcrete mechanisms through which symmetries lead to reduced capacities and\nignored features during training. We then propose a simple and theoretically\njustified algorithm, syre, to remove almost all symmetry-induced low-capacity\nstates in neural networks. The proposed method is shown to improve the training\nof neural networks in scenarios when this type of entrapment is especially a\nconcern. A remarkable merit of the proposed method is that it is model-agnostic\nand does not require any knowledge of the symmetry.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}