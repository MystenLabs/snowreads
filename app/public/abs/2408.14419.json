{"id":"2408.14419","title":"CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language\n  Models","authors":"Shubham Bharti, Shiyun Cheng, Jihyun Rho, Martina Rao, Xiaojin Zhu","authorsParsed":[["Bharti","Shubham",""],["Cheng","Shiyun",""],["Rho","Jihyun",""],["Rao","Martina",""],["Zhu","Xiaojin",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 17:04:23 GMT"}],"updateDate":"2024-08-27","timestamp":1724691863000,"abstract":"  We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large\nlanguage models. CHARTOM consists of specially designed data visualizing\ncharts. Given a chart, a language model needs to not only correctly comprehend\nthe chart (the FACT question) but also judge if the chart will be misleading to\na human reader (the MIND question). Both questions have significant societal\nbenefits. We detail the construction of the CHARTOM benchmark including its\ncalibration on human performance.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}