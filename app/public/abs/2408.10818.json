{"id":"2408.10818","title":"Learning Randomized Algorithms with Transformers","authors":"Johannes von Oswald and Seijin Kobayashi and Yassir Akram and Angelika\n  Steger","authorsParsed":[["von Oswald","Johannes",""],["Kobayashi","Seijin",""],["Akram","Yassir",""],["Steger","Angelika",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 13:13:36 GMT"}],"updateDate":"2024-08-21","timestamp":1724159616000,"abstract":"  Randomization is a powerful tool that endows algorithms with remarkable\nproperties. For instance, randomized algorithms excel in adversarial settings,\noften surpassing the worst-case performance of deterministic algorithms with\nlarge margins. Furthermore, their success probability can be amplified by\nsimple strategies such as repetition and majority voting. In this paper, we\nenhance deep neural networks, in particular transformer models, with\nrandomization. We demonstrate for the first time that randomized algorithms can\nbe instilled in transformers through learning, in a purely data- and\nobjective-driven manner. First, we analyze known adversarial objectives for\nwhich randomized algorithms offer a distinct advantage over deterministic ones.\nWe then show that common optimization techniques, such as gradient descent or\nevolutionary strategies, can effectively learn transformer parameters that make\nuse of the randomness provided to the model. To illustrate the broad\napplicability of randomization in empowering neural networks, we study three\nconceptual tasks: associative recall, graph coloring, and agents that explore\ngrid worlds. In addition to demonstrating increased robustness against\noblivious adversaries through learned randomization, our experiments reveal\nremarkable performance improvements due to the inherently random nature of the\nneural networks' computation and predictions.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}