{"id":"2408.11407","title":"Domain-invariant Progressive Knowledge Distillation for UAV-based Object\n  Detection","authors":"Liang Yao, Fan Liu, Chuanyi Zhang, Zhiquan Ou, Ting Wu","authorsParsed":[["Yao","Liang",""],["Liu","Fan",""],["Zhang","Chuanyi",""],["Ou","Zhiquan",""],["Wu","Ting",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 08:05:03 GMT"}],"updateDate":"2024-08-22","timestamp":1724227503000,"abstract":"  Knowledge distillation (KD) is an effective method for compressing models in\nobject detection tasks. Due to limited computational capability, UAV-based\nobject detection (UAV-OD) widely adopt the KD technique to obtain lightweight\ndetectors. Existing methods often overlook the significant differences in\nfeature space caused by the large gap in scale between the teacher and student\nmodels. This limitation hampers the efficiency of knowledge transfer during the\ndistillation process. Furthermore, the complex backgrounds in UAV images make\nit challenging for the student model to efficiently learn the object features.\nIn this paper, we propose a novel knowledge distillation framework for UAV-OD.\nSpecifically, a progressive distillation approach is designed to alleviate the\nfeature gap between teacher and student models. Then a new feature alignment\nmethod is provided to extract object-related features for enhancing student\nmodel's knowledge reception efficiency. Finally, extensive experiments are\nconducted to validate the effectiveness of our proposed approach. The results\ndemonstrate that our proposed method achieves state-of-the-art (SoTA)\nperformance in two UAV-OD datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}