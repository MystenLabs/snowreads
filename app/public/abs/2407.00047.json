{"id":"2407.00047","title":"One Queue Is All You Need: Resolving Head-of-Line Blocking in Large\n  Language Model Serving","authors":"Archit Patke, Dhemath Reddy, Saurabh Jha, Haoran Qiu, Christian Pinto,\n  Shengkun Cui, Chandra Narayanaswami, Zbigniew Kalbarczyk, Ravishankar Iyer","authorsParsed":[["Patke","Archit",""],["Reddy","Dhemath",""],["Jha","Saurabh",""],["Qiu","Haoran",""],["Pinto","Christian",""],["Cui","Shengkun",""],["Narayanaswami","Chandra",""],["Kalbarczyk","Zbigniew",""],["Iyer","Ravishankar",""]],"versions":[{"version":"v1","created":"Wed, 5 Jun 2024 21:17:34 GMT"}],"updateDate":"2024-07-02","timestamp":1717622254000,"abstract":"  $ $Large language models (LLMs) have become an increasingly important\nworkload for cloud providers catering to both enterprise and consumer\napplications. LLM inference requests from these applications have end-to-end\nlatency SLOs that must be adhered to in production settings. However, existing\nLLM serving systems focus on optimization objectives such as request serving\nthroughput or request execution latency rather than the end-to-end latency\nSLOs. Achieving end-to-end SLOs for latency-sensitive requests is challenging\ndue to head-of-line (HOL) blocking in the request queue, which results from\nbursty arrival rates and insufficient resources.\n  To address the above challenge, we propose QLM, a multi-model queue\nmanagement framework for LLM serving. QLM uses stochastic programming to\norchestrate the actions of multiple LLM Serving Operations (LSOs) to reduce HOL\nblocking and maximize SLO attainment. Specifically, QLM uses the following\nLSOs: model swapping, request eviction, GPU-CPU state swapping, load balancing,\nand warm model start. Evaluation on heterogeneous GPU devices and models with\nreal-world LLM serving dataset shows that QLM improves SLO attainment by 40-90%\nand throughput by 20-400% while maintaining or improving device utilization\ncompared to other state-of-the-art LLM serving systems.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}