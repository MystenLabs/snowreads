{"id":"2407.06504","title":"Reprogramming Distillation for Medical Foundation Models","authors":"Yuhang Zhou, Siyuan Du, Haolin Li, Jiangchao Yao, Ya Zhang, and\n  Yanfeng Wang","authorsParsed":[["Zhou","Yuhang",""],["Du","Siyuan",""],["Li","Haolin",""],["Yao","Jiangchao",""],["Zhang","Ya",""],["Wang","Yanfeng",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 02:17:51 GMT"}],"updateDate":"2024-07-10","timestamp":1720491471000,"abstract":"  Medical foundation models pre-trained on large-scale datasets have\ndemonstrated powerful versatile capabilities for various tasks. However, due to\nthe gap between pre-training tasks (or modalities) and downstream tasks (or\nmodalities), the real-world computation and speed constraints, it might not be\nstraightforward to apply medical foundation models in the downstream scenarios.\nPrevious methods, such as parameter efficient fine-tuning (PEFT) methods and\nknowledge distillation (KD) methods, are unable to simultaneously address the\ntask (or modality) inconsistency and achieve personalized lightweight\ndeployment under diverse real-world demands. To address the above issues, we\npropose a novel framework called Reprogramming Distillation (RD). On one hand,\nRD reprograms the original feature space of the foundation model so that it is\nmore relevant to downstream scenarios, aligning tasks and modalities. On the\nother hand, through a co-training mechanism and a shared classifier,\nconnections are established between the reprogrammed knowledge and the\nknowledge of student models, ensuring that the reprogrammed feature space can\nbe smoothly mimic by the student model of different structures. Further, to\nreduce the randomness under different training conditions, we design a Centered\nKernel Alignment (CKA) distillation to promote robust knowledge transfer.\nEmpirically, we show that on extensive datasets, RD consistently achieve\nsuperior performance compared with previous PEFT and KD methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}