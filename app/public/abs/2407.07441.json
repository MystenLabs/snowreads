{"id":"2407.07441","title":"HAFormer: Unleashing the Power of Hierarchy-Aware Features for\n  Lightweight Semantic Segmentation","authors":"Guoan Xu, Wenjing Jia, Tao Wu, Ligeng Chen, and Guangwei Gao","authorsParsed":[["Xu","Guoan",""],["Jia","Wenjing",""],["Wu","Tao",""],["Chen","Ligeng",""],["Gao","Guangwei",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 07:53:24 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 02:19:44 GMT"}],"updateDate":"2024-07-12","timestamp":1720598004000,"abstract":"  Both Convolutional Neural Networks (CNNs) and Transformers have shown great\nsuccess in semantic segmentation tasks. Efforts have been made to integrate\nCNNs with Transformer models to capture both local and global context\ninteractions. However, there is still room for enhancement, particularly when\nconsidering constraints on computational resources. In this paper, we introduce\nHAFormer, a model that combines the hierarchical features extraction ability of\nCNNs with the global dependency modeling capability of Transformers to tackle\nlightweight semantic segmentation challenges. Specifically, we design a\nHierarchy-Aware Pixel-Excitation (HAPE) module for adaptive multi-scale local\nfeature extraction. During the global perception modeling, we devise an\nEfficient Transformer (ET) module streamlining the quadratic calculations\nassociated with traditional Transformers. Moreover, a correlation-weighted\nFusion (cwF) module selectively merges diverse feature representations,\nsignificantly enhancing predictive accuracy. HAFormer achieves high performance\nwith minimal computational overhead and compact model size, achieving 74.2%\nmIoU on Cityscapes and 71.1% mIoU on CamVid test datasets, with frame rates of\n105FPS and 118FPS on a single 2080Ti GPU. The source codes are available at\nhttps://github.com/XU-GITHUB-curry/HAFormer.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}