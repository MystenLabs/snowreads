{"id":"2408.05283","title":"MUSE: Multi-Knowledge Passing on the Edges, Boosting Knowledge Graph\n  Completion","authors":"Pengjie Liu","authorsParsed":[["Liu","Pengjie",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 18:10:02 GMT"}],"updateDate":"2024-08-15","timestamp":1723227002000,"abstract":"  Knowledge Graph Completion (KGC) aims to predict the missing information in\nthe (head entity)-[relation]-(tail entity) triplet. Deep Neural Networks have\nachieved significant progress in the relation prediction task. However, most\nexisting KGC methods focus on single features (e.g., entity IDs) and sub-graph\naggregation, which cannot fully explore all the features in the Knowledge Graph\n(KG), and neglect the external semantic knowledge injection. To address these\nproblems, we propose MUSE, a knowledge-aware reasoning model to learn a\ntailored embedding space in three dimensions for missing relation prediction\nthrough a multi-knowledge representation learning mechanism. Our MUSE consists\nof three parallel components: 1) Prior Knowledge Learning for enhancing the\ntriplets' semantic representation by fine-tuning BERT; 2) Context Message\nPassing for enhancing the context messages of KG; 3) Relational Path\nAggregation for enhancing the path representation from the head entity to the\ntail entity. Our experimental results show that MUSE significantly outperforms\nother baselines on four public datasets, such as over 5.50% improvement in H@1\nand 4.20% improvement in MRR on the NELL995 dataset. The code and all datasets\nwill be released via https://github.com/NxxTGT/MUSE.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}