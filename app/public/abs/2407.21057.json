{"id":"2407.21057","title":"Multi-group Uncertainty Quantification for Long-form Text Generation","authors":"Terrance Liu, Zhiwei Steven Wu","authorsParsed":[["Liu","Terrance",""],["Wu","Zhiwei Steven",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 02:59:52 GMT"}],"updateDate":"2024-08-01","timestamp":1721876392000,"abstract":"  While large language models are rapidly moving towards consumer-facing\napplications, they are often still prone to factual errors and hallucinations.\nIn order to reduce the potential harms that may come from these errors, it is\nimportant for users to know to what extent they can trust an LLM when it makes\na factual claim. To this end, we study the problem of uncertainty\nquantification of factual correctness in long-form natural language generation.\nGiven some output from a large language model, we study both uncertainty at the\nlevel of individual claims contained within the output (via calibration) and\nuncertainty across the entire output itself (via conformal prediction).\nMoreover, we invoke multicalibration and multivalid conformal prediction to\nensure that such uncertainty guarantees are valid both marginally and across\ndistinct groups of prompts. Using the task of biography generation, we\ndemonstrate empirically that having access to and making use of additional\ngroup attributes for each prompt improves both overall and group-wise\nperformance. As the problems of calibration, conformal prediction, and their\nmulti-group counterparts have not been extensively explored previously in the\ncontext of long-form text generation, we consider these empirical results to\nform a benchmark for this setting.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}