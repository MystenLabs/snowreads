{"id":"2407.11216","title":"Finding Meaning in Points: Weakly Supervised Semantic Segmentation for\n  Event Cameras","authors":"Hoonhee Cho, Sung-Hoon Yoon, Hyeokjun Kweon, Kuk-Jin Yoon","authorsParsed":[["Cho","Hoonhee",""],["Yoon","Sung-Hoon",""],["Kweon","Hyeokjun",""],["Yoon","Kuk-Jin",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 20:00:50 GMT"}],"updateDate":"2024-07-17","timestamp":1721073650000,"abstract":"  Event cameras excel in capturing high-contrast scenes and dynamic objects,\noffering a significant advantage over traditional frame-based cameras. Despite\nactive research into leveraging event cameras for semantic segmentation,\ngenerating pixel-wise dense semantic maps for such challenging scenarios\nremains labor-intensive. As a remedy, we present EV-WSSS: a novel weakly\nsupervised approach for event-based semantic segmentation that utilizes sparse\npoint annotations. To fully leverage the temporal characteristics of event\ndata, the proposed framework performs asymmetric dual-student learning between\n1) the original forward event data and 2) the longer reversed event data, which\ncontain complementary information from the past and the future, respectively.\nBesides, to mitigate the challenges posed by sparse supervision, we propose\nfeature-level contrastive learning based on class-wise prototypes, carefully\naggregated at both spatial region and sample levels. Additionally, we further\nexcavate the potential of our dual-student learning model by exchanging\nprototypes between the two learning paths, thereby harnessing their\ncomplementary strengths. With extensive experiments on various datasets,\nincluding DSEC Night-Point with sparse point annotations newly provided by this\npaper, the proposed method achieves substantial segmentation results even\nwithout relying on pixel-level dense ground truths. The code and dataset are\navailable at https://github.com/Chohoonhee/EV-WSSS.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}