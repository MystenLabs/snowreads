{"id":"2408.09624","title":"Attention is a smoothed cubic spline","authors":"Zehua Lai, Lek-Heng Lim, and Yucong Liu","authorsParsed":[["Lai","Zehua",""],["Lim","Lek-Heng",""],["Liu","Yucong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 00:56:44 GMT"}],"updateDate":"2024-08-20","timestamp":1724029004000,"abstract":"  We highlight a perhaps important but hitherto unobserved insight: The\nattention module in a transformer is a smoothed cubic spline. Viewed in this\nmanner, this mysterious but critical component of a transformer becomes a\nnatural development of an old notion deeply entrenched in classical\napproximation theory. More precisely, we show that with ReLU-activation,\nattention, masked attention, encoder-decoder attention are all cubic splines.\nAs every component in a transformer is constructed out of compositions of\nvarious attention modules (= cubic splines) and feed forward neural networks (=\nlinear splines), all its components -- encoder, decoder, and encoder-decoder\nblocks; multilayered encoders and decoders; the transformer itself -- are cubic\nor higher-order splines. If we assume the Pierce-Birkhoff conjecture, then the\nconverse also holds, i.e., every spline is a ReLU-activated encoder. Since a\nspline is generally just $C^2$, one way to obtain a smoothed $C^\\infty$-version\nis by replacing ReLU with a smooth activation; and if this activation is chosen\nto be SoftMax, we recover the original transformer as proposed by Vaswani et\nal. This insight sheds light on the nature of the transformer by casting it\nentirely in terms of splines, one of the best known and thoroughly understood\nobjects in applied mathematics.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kmUqES3VoS_CDbdZDHBWx3Ip6lTqPW8A5zMlZv5dHsQ","pdfSize":"312946"}
