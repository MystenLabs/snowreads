{"id":"2407.14885","title":"Falcon2-11B Technical Report","authors":"Quentin Malartic, Nilabhra Roy Chowdhury, Ruxandra Cojocaru, Mugariya\n  Farooq, Giulia Campesan, Yasser Abdelaziz Dahou Djilali, Sanath Narayan,\n  Ankit Singh, Maksim Velikanov, Basma El Amel Boussaha, Mohammed Al-Yafeai,\n  Hamza Alobeidli, Leen Al Qadi, Mohamed El Amine Seddik, Kirill Fedyanin, Reda\n  Alami, Hakim Hacid","authorsParsed":[["Malartic","Quentin",""],["Chowdhury","Nilabhra Roy",""],["Cojocaru","Ruxandra",""],["Farooq","Mugariya",""],["Campesan","Giulia",""],["Djilali","Yasser Abdelaziz Dahou",""],["Narayan","Sanath",""],["Singh","Ankit",""],["Velikanov","Maksim",""],["Boussaha","Basma El Amel",""],["Al-Yafeai","Mohammed",""],["Alobeidli","Hamza",""],["Qadi","Leen Al",""],["Seddik","Mohamed El Amine",""],["Fedyanin","Kirill",""],["Alami","Reda",""],["Hacid","Hakim",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 14:23:15 GMT"}],"updateDate":"2024-07-23","timestamp":1721485395000,"abstract":"  We introduce Falcon2-11B, a foundation model trained on over five trillion\ntokens, and its multimodal counterpart, Falcon2-11B-vlm, which is a\nvision-to-text model. We report our findings during the training of the\nFalcon2-11B which follows a multi-stage approach where the early stages are\ndistinguished by their context length and a final stage where we use a curated,\nhigh-quality dataset. Additionally, we report the effect of doubling the batch\nsize mid-training and how training loss spikes are affected by the learning\nrate. The downstream performance of the foundation model is evaluated on\nestablished benchmarks, including multilingual and code datasets. The\nfoundation model shows strong generalization across all the tasks which makes\nit suitable for downstream finetuning use cases. For the vision language model,\nwe report the performance on several benchmarks and show that our model\nachieves a higher average score compared to open-source models of similar size.\nThe model weights and code of both Falcon2-11B and Falcon2-11B-vlm are made\navailable under a permissive license.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}