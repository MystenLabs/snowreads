{"id":"2407.00290","title":"Variable Time Step Reinforcement Learning for Robotic Applications","authors":"Dong Wang and Giovanni Beltrame","authorsParsed":[["Wang","Dong",""],["Beltrame","Giovanni",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 03:07:00 GMT"}],"updateDate":"2024-07-02","timestamp":1719630420000,"abstract":"  Traditional reinforcement learning (RL) generates discrete control policies,\n  assigning one action per cycle. These policies are usually implemented as in\na\n  fixed-frequency control loop. This rigidity presents challenges as optimal\n  control frequency is task-dependent; suboptimal frequencies increase\n  computational demands and reduce exploration efficiency. Variable Time Step\n  Reinforcement Learning (VTS-RL) addresses these issues with adaptive control\n  frequencies, executing actions only when necessary, thus reducing\n  computational load and extending the action space to include action\ndurations.\n  In this paper we introduce the Multi-Objective Soft Elastic Actor-Critic\n  (MOSEAC) method to perform VTS-RL, validating it through theoretical analysis\n  and experimentation in simulation and on real robots. Results show faster\n  convergence, better training results, and reduced energy consumption with\n  respect to other variable- or fixed-frequency approaches.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4F396ltkXgKvfOxLIpS78dWEZN2auYPNfDJSE0up3P0","pdfSize":"4356323"}
