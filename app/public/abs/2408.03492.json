{"id":"2408.03492","title":"Automated Theorem Provers Help Improve Large Language Model Reasoning","authors":"Lachlan McGinness, Peter Baumgartner","authorsParsed":[["McGinness","Lachlan",""],["Baumgartner","Peter",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 01:03:56 GMT"}],"updateDate":"2024-08-08","timestamp":1722992636000,"abstract":"  In this paper we demonstrate how logic programming systems and Automated\nfirst-order logic Theorem Provers (ATPs) can improve the accuracy of Large\nLanguage Models (LLMs) for logical reasoning tasks where the baseline\nperformance is given by direct LLM solutions. We first evaluate LLM reasoning\non steamroller problems using the PRONTOQA benchmark. We show how accuracy can\nbe improved with a neuro-symbolic architecture where the LLM acts solely as a\nfront-end for translating a given problem into a formal logic language and an\nautomated reasoning engine is called for solving it. However, this approach\ncritically hinges on the correctness of the LLM translation. To assess this\ntranslation correctness, we secondly define a framework of syntactic and\nsemantic error categories. We implemented the framework and used it to identify\nerrors that LLMs make in the benchmark domain. Based on these findings, we\nthirdly extended our method with capabilities for automatically correcting\nsyntactic and semantic errors. For semantic error correction we integrate\nfirst-order logic ATPs, which is our main and novel contribution. We\ndemonstrate that this approach reduces semantic errors significantly and\nfurther increases the accurracy of LLM logical reasoning.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NszYyHT5DoxIJQPMlXqvYTbu2EvOPL_92tUAZ8VhjHo","pdfSize":"828574"}
