{"id":"2408.03692","title":"Asynchronous Credit Assignment Framework for Multi-Agent Reinforcement\n  Learning","authors":"Yongheng Liang, Hejun Wu, Haitao Wang, Hao Cai","authorsParsed":[["Liang","Yongheng",""],["Wu","Hejun",""],["Wang","Haitao",""],["Cai","Hao",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 11:13:26 GMT"}],"updateDate":"2024-08-08","timestamp":1723029206000,"abstract":"  Credit assignment is a core problem that distinguishes agents' marginal\ncontributions for optimizing cooperative strategies in multi-agent\nreinforcement learning (MARL). Current credit assignment methods usually assume\nsynchronous decision-making among agents. However, a prerequisite for many\nrealistic cooperative tasks is asynchronous decision-making by agents, without\nwaiting for others to avoid disastrous consequences. To address this issue, we\npropose an asynchronous credit assignment framework with a problem model called\nADEX-POMDP and a multiplicative value decomposition (MVD) algorithm. ADEX-POMDP\nis an asynchronous problem model with extra virtual agents for a decentralized\npartially observable markov decision process. We prove that ADEX-POMDP\npreserves both the task equilibrium and the algorithm convergence. MVD utilizes\nmultiplicative interaction to efficiently capture the interactions of\nasynchronous decisions, and we theoretically demonstrate its advantages in\nhandling asynchronous tasks. Experimental results show that on two asynchronous\ndecision-making benchmarks, Overcooked and POAC, MVD not only consistently\noutperforms state-of-the-art MARL methods but also provides the\ninterpretability for asynchronous cooperation.\n","subjects":["Computing Research Repository/Multiagent Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}