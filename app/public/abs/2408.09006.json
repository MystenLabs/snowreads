{"id":"2408.09006","title":"Context-aware Code Summary Generation","authors":"Chia-Yi Su, Aakash Bansal, Yu Huang, Toby Jia-Jun Li, Collin McMillan","authorsParsed":[["Su","Chia-Yi",""],["Bansal","Aakash",""],["Huang","Yu",""],["Li","Toby Jia-Jun",""],["McMillan","Collin",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 20:15:34 GMT"}],"updateDate":"2024-08-20","timestamp":1723839334000,"abstract":"  Code summary generation is the task of writing natural language descriptions\nof a section of source code. Recent advances in Large Language Models (LLMs)\nand other AI-based technologies have helped make automatic code summarization a\nreality. However, the summaries these approaches write tend to focus on a\nnarrow area of code. The results are summaries that explain what that function\ndoes internally, but lack a description of why the function exists or its\npurpose in the broader context of the program. In this paper, we present an\napproach for including this context in recent LLM-based code summarization. The\ninput to our approach is a Java method and that project in which that method\nexists. The output is a succinct English description of why the method exists\nin the project. The core of our approach is a 350m parameter language model we\ntrain, which can be run locally to ensure privacy. We train the model in two\nsteps. First we distill knowledge about code summarization from a large model,\nthen we fine-tune the model using data from a study of human programmer who\nwere asked to write code summaries. We find that our approach outperforms GPT-4\non this task.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/"}