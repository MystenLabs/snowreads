{"id":"2408.10504","title":"QPO: Query-dependent Prompt Optimization via Multi-Loop Offline\n  Reinforcement Learning","authors":"Yilun Kong, Hangyu Mao, Qi Zhao, Bin Zhang, Jingqing Ruan, Li Shen,\n  Yongzhe Chang, Xueqian Wang, Rui Zhao, Dacheng Tao","authorsParsed":[["Kong","Yilun",""],["Mao","Hangyu",""],["Zhao","Qi",""],["Zhang","Bin",""],["Ruan","Jingqing",""],["Shen","Li",""],["Chang","Yongzhe",""],["Wang","Xueqian",""],["Zhao","Rui",""],["Tao","Dacheng",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 03:06:48 GMT"}],"updateDate":"2024-08-21","timestamp":1724123208000,"abstract":"  Prompt engineering has demonstrated remarkable success in enhancing the\nperformance of large language models (LLMs) across diverse tasks. However, most\nexisting prompt optimization methods only focus on the task-level performance,\noverlooking the importance of query-preferred prompts, which leads to\nsuboptimal performances. Additionally, these methods rely heavily on frequent\ninteractions with LLMs to obtain feedback for guiding the optimization process,\nincurring substantial redundant interaction costs. In this paper, we introduce\nQuery-dependent Prompt Optimization (QPO), which leverages multi-loop offline\nreinforcement learning to iteratively fine-tune a small pretrained language\nmodel to generate optimal prompts tailored to the input queries, thus\nsignificantly improving the prompting effect on the large target LLM. We derive\ninsights from offline prompting demonstration data, which already exists in\nlarge quantities as a by-product of benchmarking diverse prompts on\nopen-sourced tasks, thereby circumventing the expenses of online interactions.\nFurthermore, we continuously augment the offline dataset with the generated\nprompts in each loop, as the prompts from the fine-tuned model are supposed to\noutperform the source prompts in the original dataset. These iterative loops\nbootstrap the model towards generating optimal prompts. Experiments on various\nLLM scales and diverse NLP and math tasks demonstrate the efficacy and\ncost-efficiency of our method in both zero-shot and few-shot scenarios.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}