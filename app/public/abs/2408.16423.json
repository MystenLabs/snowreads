{"id":"2408.16423","title":"WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding","authors":"Mohan Li, Cong-Thanh Do, Simon Keizer, Youmna Farag, Svetlana\n  Stoyanchev, Rama Doddipatla","authorsParsed":[["Li","Mohan",""],["Do","Cong-Thanh",""],["Keizer","Simon",""],["Farag","Youmna",""],["Stoyanchev","Svetlana",""],["Doddipatla","Rama",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 10:31:52 GMT"}],"updateDate":"2024-08-30","timestamp":1724927512000,"abstract":"  Speech large language models (speech-LLMs) integrate speech and text-based\nfoundation models to provide a unified framework for handling a wide range of\ndownstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for\nspoken language understanding (SLU) that demonstrates robust performance in\nvarious zero-shot settings. WHISMA combines the speech encoder from Whisper\nwith the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a\ncomprehensive collection of SLU-related datasets. Our experiments show that\nWHISMA significantly improves the zero-shot slot filling performance on the\nSLURP benchmark, achieving a relative gain of 26.6% compared to the current\nstate-of-the-art model. Furthermore, to evaluate WHISMA's generalisation\ncapabilities to unseen domains, we develop a new task-agnostic benchmark named\nSLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing\nspeech-LLM (Qwen-Audio) with a relative gain of 33.0%.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}