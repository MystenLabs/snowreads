{"id":"2407.00128","title":"When Search Engine Services meet Large Language Models: Visions and\n  Challenges","authors":"Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Mengnan Du, Shuaiqiang\n  Wang, Dawei Yin, Sumi Helal","authorsParsed":[["Xiong","Haoyi",""],["Bian","Jiang",""],["Li","Yuchen",""],["Li","Xuhong",""],["Du","Mengnan",""],["Wang","Shuaiqiang",""],["Yin","Dawei",""],["Helal","Sumi",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 03:52:13 GMT"}],"updateDate":"2024-07-02","timestamp":1719546733000,"abstract":"  Combining Large Language Models (LLMs) with search engine services marks a\nsignificant shift in the field of services computing, opening up new\npossibilities to enhance how we search for and retrieve information, understand\ncontent, and interact with internet services. This paper conducts an in-depth\nexamination of how integrating LLMs with search engines can mutually benefit\nboth technologies. We focus on two main areas: using search engines to improve\nLLMs (Search4LLM) and enhancing search engine functions using LLMs\n(LLM4Search). For Search4LLM, we investigate how search engines can provide\ndiverse high-quality datasets for pre-training of LLMs, how they can use the\nmost relevant documents to help LLMs learn to answer queries more accurately,\nhow training LLMs with Learning-To-Rank (LTR) tasks can enhance their ability\nto respond with greater precision, and how incorporating recent search results\ncan make LLM-generated content more accurate and current. In terms of\nLLM4Search, we examine how LLMs can be used to summarize content for better\nindexing by search engines, improve query outcomes through optimization,\nenhance the ranking of search results by analyzing document relevance, and help\nin annotating data for learning-to-rank tasks in various learning contexts.\nHowever, this promising integration comes with its challenges, which include\naddressing potential biases and ethical issues in training models, managing the\ncomputational and other costs of incorporating LLMs into search services, and\ncontinuously updating LLM training with the ever-changing web content. We\ndiscuss these challenges and chart out required research directions to address\nthem. We also discuss broader implications for service computing, such as\nscalability, privacy concerns, and the need to adapt search engine\narchitectures for these advanced models.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}