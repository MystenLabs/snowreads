{"id":"2408.09156","title":"DSReLU: A Novel Dynamic Slope Function for Superior Model Training","authors":"Archisman Chakraborti and Bidyut B Chaudhuri","authorsParsed":[["Chakraborti","Archisman",""],["Chaudhuri","Bidyut B",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 10:01:30 GMT"}],"updateDate":"2024-08-20","timestamp":1723888890000,"abstract":"  This study introduces a novel activation function, characterized by a dynamic\nslope that adjusts throughout the training process, aimed at enhancing\nadaptability and performance in deep neural networks for computer vision tasks.\nThe rationale behind this approach is to overcome limitations associated with\ntraditional activation functions, such as ReLU, by providing a more flexible\nmechanism that can adapt to different stages of the learning process. Evaluated\non the Mini-ImageNet, CIFAR-100, and MIT-BIH datasets, our method demonstrated\nimprovements in classification metrics and generalization capabilities. These\nresults suggest that our dynamic slope activation function could offer a new\ntool for improving the performance of deep learning models in various image\nrecognition tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}