{"id":"2407.13526","title":"Discussion: Effective and Interpretable Outcome Prediction by Training\n  Sparse Mixtures of Linear Experts","authors":"Francesco Folino, Luigi Pontieri and Pietro Sabatino","authorsParsed":[["Folino","Francesco",""],["Pontieri","Luigi",""],["Sabatino","Pietro",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 13:59:10 GMT"}],"updateDate":"2024-07-19","timestamp":1721311150000,"abstract":"  Process Outcome Prediction entails predicting a discrete property of an\nunfinished process instance from its partial trace. High-capacity outcome\npredictors discovered with ensemble and deep learning methods have been shown\nto achieve top accuracy performances, but they suffer from a lack of\ntransparency. Aligning with recent efforts to learn inherently interpretable\noutcome predictors, we propose to train a sparse Mixture-of-Experts where both\nthe ``gate'' and ``expert'' sub-nets are Logistic Regressors. This\nensemble-like model is trained end-to-end while automatically selecting a\nsubset of input features in each sub-net, as an alternative to the common\napproach of performing a global feature selection step prior to model training.\nTest results on benchmark logs confirmed the validity and efficacy of this\napproach.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kfcH-aE2O97AU8M58oSeBvNzPGf-n4udLXABb4X6iv0","pdfSize":"550125"}
