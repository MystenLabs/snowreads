{"id":"2407.12022","title":"ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code\n  Generation","authors":"Peiyang Wu, Nan Guo, Xiao Xiao, Wenming Li, Xiaochun Ye and Dongrui\n  Fan","authorsParsed":[["Wu","Peiyang",""],["Guo","Nan",""],["Xiao","Xiao",""],["Li","Wenming",""],["Ye","Xiaochun",""],["Fan","Dongrui",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 01:44:57 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 08:08:45 GMT"}],"updateDate":"2024-07-24","timestamp":1719539097000,"abstract":"  Recently, large language models (LLMs) have demonstrated excellent\nperformance in understanding human instructions and generating code, which has\ninspired researchers to explore the feasibility of generating RTL code with\nLLMs. However, the existing approaches to fine-tune LLMs on RTL codes typically\nare conducted on fixed datasets, which do not fully stimulate the capability of\nLLMs and require large amounts of reference data. To mitigate these issues , we\nintroduce a simple yet effective iterative training paradigm named ITERTL.\nDuring each iteration, samples are drawn from the model trained in the previous\ncycle. Then these new samples are employed for training in this loop. Through\nthis iterative approach, the distribution mismatch between the model and the\ntraining samples is reduced. Additionally, the model is thus enabled to explore\na broader generative space and receive more comprehensive feedback. Theoretical\nanalyses are conducted to investigate the mechanism of the effectiveness.\nExperimental results show the model trained through our proposed approach can\ncompete with and even outperform the state-of-the-art (SOTA) open-source model\nwith nearly 37\\% reference samples, achieving remarkable 42.9\\% and 62.2\\%\npass@1 rate on two VerilogEval evaluation datasets respectively. While using\nthe same amount of reference samples, our method can achieved a relative\nimprovement of 16.9\\% and 12.5\\% in pass@1 compared to the non-iterative\nmethod. This study facilitates the application of LLMs for generating RTL code\nin practical scenarios with limited data.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}