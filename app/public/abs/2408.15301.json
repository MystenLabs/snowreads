{"id":"2408.15301","title":"The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical\n  Study","authors":"Minghai Qin","authorsParsed":[["Qin","Minghai",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 15:03:01 GMT"}],"updateDate":"2024-08-29","timestamp":1724770981000,"abstract":"  We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying\nlarge language models (LLMs) efficiently. Among various bit widths and\nrepresentations for weights and activations, the 8-bit integer weight and 8-bit\ninteger activation (W8A8) configuration is particularly popular due to its\nwidespread hardware support. However, the impact of W8A8 post-training\nquantization on model accuracy remains contentious. While several studies have\nsuggested calibrating either weights or activations to mitigate accuracy\ndegradation, a comprehensive solution has yet to be identified. In this paper,\nwe empirically investigate multiple LLMs featured on an open LLM leaderboard,\ndiscovering that the LLaMA3-70B model series have a unique accuracy degradation\nbehavior with W8A8 per-channel post-training quantization. In contrast, other\nmodel series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and\nFalcon demonstrate robust performance with W8A8, sometimes surpassing their\nFP16 counterparts. Contrary to previous assertions attributing degradation to\nthe large dynamic range of activations, our findings indicate that the weight\ndistribution of the LLaMA3-70B is the primary factor behind the vulnerability.\nBy meticulously analyzing the distinct characteristics of weight distributions\nacross Transformer blocks, we propose a mixed strategy with less than 3% of the\nlayers enabling finer W8A8 quantization granularity, while the remaining 97% of\nlayers retain the per-channel configuration. As a result, the average accuracy\nof LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of\nLLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires\nneither calibration nor fine-tuning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}