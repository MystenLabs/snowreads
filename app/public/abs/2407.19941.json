{"id":"2407.19941","title":"Boosting Graph Foundation Model from Structural Perspective","authors":"Yao Cheng and Yige Zhao and Jianxiang Yu and Xiang Li","authorsParsed":[["Cheng","Yao",""],["Zhao","Yige",""],["Yu","Jianxiang",""],["Li","Xiang",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 12:22:16 GMT"}],"updateDate":"2024-07-30","timestamp":1722255736000,"abstract":"  Graph foundation models have recently attracted significant attention due to\nits strong generalizability. Although existing methods resort to language\nmodels to learn unified semantic representations across domains, they disregard\nthe unique structural characteristics of graphs from different domains. To\naddress the problem, in this paper, we boost graph foundation model from\nstructural perspective and propose BooG. The model constructs virtual super\nnodes to unify structural characteristics of graph data from different domains.\nSpecifically, the super nodes fuse the information of anchor nodes and class\nlabels, where each anchor node captures the information of a node or a graph\ninstance to be classified. Instead of using the raw graph structure, we connect\nsuper nodes to all nodes within their neighborhood by virtual edges. This new\nstructure allows for effective information aggregation while unifying\ncross-domain structural characteristics. Additionally, we propose a novel\npre-training objective based on contrastive learning, which learns more\nexpressive representations for graph data and generalizes effectively to\ndifferent domains and downstream tasks. Experimental results on various\ndatasets and tasks demonstrate the superior performance of BooG. We provide our\ncode and data here: https://anonymous.4open.science/r/BooG-EE42/.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}