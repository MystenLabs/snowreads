{"id":"2407.07053","title":"Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning\n  Instruction Using Language Model","authors":"Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen,\n  Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, Yueting Zhuang","authorsParsed":[["Zhang","Wenqi",""],["Cheng","Zhenglin",""],["He","Yuanyu",""],["Wang","Mengna",""],["Shen","Yongliang",""],["Tan","Zeqi",""],["Hou","Guiyang",""],["He","Mingqian",""],["Ma","Yanna",""],["Lu","Weiming",""],["Zhuang","Yueting",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 17:18:27 GMT"},{"version":"v2","created":"Wed, 10 Jul 2024 17:17:15 GMT"},{"version":"v3","created":"Tue, 23 Jul 2024 17:12:12 GMT"},{"version":"v4","created":"Thu, 8 Aug 2024 19:12:44 GMT"}],"updateDate":"2024-08-12","timestamp":1720545507000,"abstract":"  Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. \\textbf{This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: \\url{https://github.com/zwq2018/Multi-modal-Self-instruct}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}