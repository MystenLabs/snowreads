{"id":"2407.09965","title":"Learning Online Scale Transformation for Talking Head Video Generation","authors":"Fa-Ting Hong, Dan Xu","authorsParsed":[["Hong","Fa-Ting",""],["Xu","Dan",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 18:08:46 GMT"}],"updateDate":"2024-07-16","timestamp":1720894126000,"abstract":"  One-shot talking head video generation uses a source image and driving video\nto create a synthetic video where the source person's facial movements imitate\nthose of the driving video. However, differences in scale between the source\nand driving images remain a challenge for face reenactment. Existing methods\nattempt to locate a frame in the driving video that aligns best with the source\nimage, but imprecise alignment can result in suboptimal outcomes.\n  To this end, we introduce a scale transformation module that can\nautomatically adjust the scale of the driving image to fit that of the source\nimage, by using the information of scale difference maintained in the detected\nkeypoints of the source image and the driving frame. Furthermore, to keep\nperceiving the scale information of faces during the generation process, we\nincorporate the scale information learned from the scale transformation module\ninto each layer of the generation process to produce a final result with an\naccurate scale. Our method can perform accurate motion transfer between the two\nimages without any anchor frame, achieved through the contributions of the\nproposed online scale transformation facial reenactment network. Extensive\nexperiments have demonstrated that our proposed method adjusts the scale of the\ndriving face automatically according to the source face, and generates\nhigh-quality faces with an accurate scale in the cross-identity facial\nreenactment.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}