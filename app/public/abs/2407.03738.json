{"id":"2407.03738","title":"BasisN: Reprogramming-Free RRAM-Based In-Memory-Computing by Basis\n  Combination for Deep Neural Networks","authors":"Amro Eldebiky, Grace Li Zhang, Xunzhao Yin, Cheng Zhuo, Ing-Chao Lin,\n  Ulf Schlichtmann, Bing Li","authorsParsed":[["Eldebiky","Amro",""],["Zhang","Grace Li",""],["Yin","Xunzhao",""],["Zhuo","Cheng",""],["Lin","Ing-Chao",""],["Schlichtmann","Ulf",""],["Li","Bing",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 08:47:05 GMT"}],"updateDate":"2024-07-08","timestamp":1720082825000,"abstract":"  Deep neural networks (DNNs) have made breakthroughs in various fields\nincluding image recognition and language processing. DNNs execute hundreds of\nmillions of multiply-and-accumulate (MAC) operations. To efficiently accelerate\nsuch computations, analog in-memory-computing platforms have emerged leveraging\nemerging devices such as resistive RAM (RRAM). However, such accelerators face\nthe hurdle of being required to have sufficient on-chip crossbars to hold all\nthe weights of a DNN. Otherwise, RRAM cells in the crossbars need to be\nreprogramed to process further layers, which causes huge time/energy overhead\ndue to the extremely slow writing and verification of the RRAM cells. As a\nresult, it is still not possible to deploy such accelerators to process\nlarge-scale DNNs in industry. To address this problem, we propose the BasisN\nframework to accelerate DNNs on any number of available crossbars without\nreprogramming. BasisN introduces a novel representation of the kernels in DNN\nlayers as combinations of global basis vectors shared between all layers with\nquantized coefficients. These basis vectors are written to crossbars only once\nand used for the computations of all layers with marginal hardware\nmodification. BasisN also provides a novel training approach to enhance\ncomputation parallelization with the global basis vectors and optimize the\ncoefficients to construct the kernels. Experimental results demonstrate that\ncycles per inference and energy-delay product were reduced to below 1% compared\nwith applying reprogramming on crossbars in processing large-scale DNNs such as\nDenseNet and ResNet on ImageNet and CIFAR100 datasets, while the training and\nhardware costs are negligible.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Machine Learning","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}