{"id":"2408.16592","title":"High-Dimensional Sparse Data Low-rank Representation via Accelerated\n  Asynchronous Parallel Stochastic Gradient Descent","authors":"Qicong Hu and Hao Wu","authorsParsed":[["Hu","Qicong",""],["Wu","Hao",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 14:55:33 GMT"}],"updateDate":"2024-08-30","timestamp":1724943333000,"abstract":"  Data characterized by high dimensionality and sparsity are commonly used to\ndescribe real-world node interactions. Low-rank representation (LR) can map\nhigh-dimensional sparse (HDS) data to low-dimensional feature spaces and infer\nnode interactions via modeling data latent associations. Unfortunately,\nexisting optimization algorithms for LR models are computationally inefficient\nand slowly convergent on large-scale datasets. To address this issue, this\npaper proposes an Accelerated Asynchronous Parallel Stochastic Gradient Descent\nA2PSGD for High-Dimensional Sparse Data Low-rank Representation with three\nfold-ideas: a) establishing a lock-free scheduler to simultaneously respond to\nscheduling requests from multiple threads; b) introducing a greedy\nalgorithm-based load balancing strategy for balancing the computational load\namong threads; c) incorporating Nesterov's accelerated gradient into the\nlearning scheme to accelerate model convergence. Empirical studies show that\nA2PSGD outperforms existing optimization algorithms for HDS data LR in both\naccuracy and training time.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}