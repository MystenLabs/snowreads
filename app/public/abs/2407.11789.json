{"id":"2407.11789","title":"Large Language Models as Misleading Assistants in Conversation","authors":"Betty Li Hou, Kejian Shi, Jason Phang, James Aung, Steven Adler, Rosie\n  Campbell","authorsParsed":[["Hou","Betty Li",""],["Shi","Kejian",""],["Phang","Jason",""],["Aung","James",""],["Adler","Steven",""],["Campbell","Rosie",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:45:22 GMT"}],"updateDate":"2024-07-17","timestamp":1721141122000,"abstract":"  Large Language Models (LLMs) are able to provide assistance on a wide range\nof information-seeking tasks. However, model outputs may be misleading, whether\nunintentionally or in cases of intentional deception. We investigate the\nability of LLMs to be deceptive in the context of providing assistance on a\nreading comprehension task, using LLMs as proxies for human users. We compare\noutcomes of (1) when the model is prompted to provide truthful assistance, (2)\nwhen it is prompted to be subtly misleading, and (3) when it is prompted to\nargue for an incorrect answer. Our experiments show that GPT-4 can effectively\nmislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up\nto a 23% drop in accuracy on the task compared to when a truthful assistant is\nused. We also find that providing the user model with additional context from\nthe passage partially mitigates the influence of the deceptive model. This work\nhighlights the ability of LLMs to produce misleading information and the\neffects this may have in real-world situations.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}