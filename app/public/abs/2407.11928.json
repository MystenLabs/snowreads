{"id":"2407.11928","title":"Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based\n  Approach","authors":"Tanvir Hossain, Khaled Mohammed Saifuddin, Muhammad Ifte Khairul\n  Islam, Farhan Tanvir, Esra Akbas","authorsParsed":[["Hossain","Tanvir",""],["Saifuddin","Khaled Mohammed",""],["Islam","Muhammad Ifte Khairul",""],["Tanvir","Farhan",""],["Akbas","Esra",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:21:36 GMT"}],"updateDate":"2024-07-17","timestamp":1721150496000,"abstract":"  Graph Neural Network (GNN) achieves great success for node-level and\ngraph-level tasks via encoding meaningful topological structures of networks in\nvarious domains, ranging from social to biological networks. However, repeated\naggregation operations lead to excessive mixing of node representations,\nparticularly in dense regions with multiple GNN layers, resulting in nearly\nindistinguishable embeddings. This phenomenon leads to the oversmoothing\nproblem that hampers downstream graph analytics tasks. To overcome this issue,\nwe propose a novel and flexible truss-based graph sparsification model that\nprunes edges from dense regions of the graph. Pruning redundant edges in dense\nregions helps to prevent the aggregation of excessive neighborhood information\nduring hierarchical message passing and pooling in GNN models. We then utilize\nour sparsification model in the state-of-the-art baseline GNNs and pooling\nmodels, such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL, DMonPool, and\nAdamGNN. Extensive experiments on different real-world datasets show that our\nmodel significantly improves the performance of the baseline GNN models in the\ngraph classification task.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}