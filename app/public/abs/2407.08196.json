{"id":"2407.08196","title":"SoupLM: Model Integration in Large Language and Multi-Modal Models","authors":"Yue Bai, Zichen Zhang, Jiasen Lu, Yun Fu","authorsParsed":[["Bai","Yue",""],["Zhang","Zichen",""],["Lu","Jiasen",""],["Fu","Yun",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 05:38:15 GMT"}],"updateDate":"2024-07-12","timestamp":1720676295000,"abstract":"  Training large language models (LLMs) and multimodal LLMs necessitates\nsignificant computing resources, and existing publicly available LLMs are\ntypically pre-trained on diverse, privately curated datasets spanning various\ntasks. For instance, LLaMA, Vicuna, and LLaVA are three LLM variants trained\nwith LLaMA base models using very different training recipes, tasks, and data\nmodalities. The training cost and complexity for such LLM variants grow\nrapidly. In this study, we propose to use a soup strategy to assemble these LLM\nvariants into a single well-generalized multimodal LLM (SoupLM) in a\ncost-efficient manner. Assembling these LLM variants efficiently brings\nknowledge and specialities trained from different domains and data modalities\ninto an integrated one (e.g., chatbot speciality from user-shared conversations\nfor Vicuna, and visual capacity from vision-language data for LLaVA),\ntherefore, to avoid computing costs of repetitive training on several different\ndomains. We propose series of soup strategies to systematically benchmark\nperformance gains across various configurations, and probe the soup behavior\nacross base models in the interpolation space.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}