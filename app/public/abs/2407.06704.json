{"id":"2407.06704","title":"Self-supervised visual learning from interactions with objects","authors":"Arthur Aubret, C\\'eline Teuli\\`ere and Jochen Triesch","authorsParsed":[["Aubret","Arthur",""],["Teulière","Céline",""],["Triesch","Jochen",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 09:31:15 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 09:41:40 GMT"}],"updateDate":"2024-08-09","timestamp":1720517475000,"abstract":"  Self-supervised learning (SSL) has revolutionized visual representation\nlearning, but has not achieved the robustness of human vision. A reason for\nthis could be that SSL does not leverage all the data available to humans\nduring learning. When learning about an object, humans often purposefully turn\nor move around objects and research suggests that these interactions can\nsubstantially enhance their learning. Here we explore whether such\nobject-related actions can boost SSL. For this, we extract the actions\nperformed to change from one ego-centric view of an object to another in four\nvideo datasets. We then introduce a new loss function to learn visual and\naction embeddings by aligning the performed action with the representations of\ntwo images extracted from the same clip. This permits the performed actions to\nstructure the latent visual representation. Our experiments show that our\nmethod consistently outperforms previous methods on downstream category\nrecognition. In our analysis, we find that the observed improvement is\nassociated with a better viewpoint-wise alignment of different objects from the\nsame category. Overall, our work demonstrates that embodied interactions with\nobjects can improve SSL of object categories.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}