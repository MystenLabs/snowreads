{"id":"2407.09141","title":"Accuracy is Not All You Need","authors":"Abhinav Dutta, Sanjeev Krishnan, Nipun Kwatra, Ramachandran Ramjee","authorsParsed":[["Dutta","Abhinav",""],["Krishnan","Sanjeev",""],["Kwatra","Nipun",""],["Ramjee","Ramachandran",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 10:19:02 GMT"}],"updateDate":"2024-07-15","timestamp":1720779542000,"abstract":"  When Large Language Models (LLMs) are compressed using techniques such as\nquantization, the predominant way to demonstrate the validity of such\ntechniques is by measuring the model's accuracy on various benchmarks.If the\naccuracies of the baseline model and the compressed model are close, it is\nassumed that there was negligible degradation in quality.However, even when the\naccuracy of baseline and compressed model are similar, we observe the\nphenomenon of flips, wherein answers change from correct to incorrect and vice\nversa in proportion.We conduct a detailed study of metrics across multiple\ncompression techniques, models and datasets, demonstrating that the behavior of\ncompressed models as visible to end-users is often significantly different from\nthe baseline model, even when accuracy is similar.We further evaluate\ncompressed models qualitatively and quantitatively using MT-Bench and show that\ncompressed models are significantly worse than baseline models in this\nfree-form generative task.Thus, we argue that compression techniques should\nalso be evaluated using distance metrics.We propose two such metrics,\nKL-Divergence and flips, and show that they are well correlated.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}