{"id":"2407.15309","title":"vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving","authors":"Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu\n  Feng, Shixuan Sun, Changxu Shao, Yuhong Guo, Junping Zhao, Ke Zhang, Minyi\n  Guo, Jingwen Leng","authorsParsed":[["Xu","Jiale",""],["Zhang","Rui",""],["Guo","Cong",""],["Hu","Weiming",""],["Liu","Zihan",""],["Wu","Feiyang",""],["Feng","Yu",""],["Sun","Shixuan",""],["Shao","Changxu",""],["Guo","Yuhong",""],["Zhao","Junping",""],["Zhang","Ke",""],["Guo","Minyi",""],["Leng","Jingwen",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 14:37:58 GMT"}],"updateDate":"2024-07-23","timestamp":1721659078000,"abstract":"  Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1-KbTxpZ4-jlEU38wb6BvJQjZtHsDLx7tXaWofDwMvI","pdfSize":"751763"}
