{"id":"2407.12939","title":"GenRC: Generative 3D Room Completion from Sparse Image Collections","authors":"Ming-Feng Li, Yueh-Feng Ku, Hong-Xuan Yen, Chi Liu, Yu-Lun Liu, Albert\n  Y. C. Chen, Cheng-Hao Kuo, Min Sun","authorsParsed":[["Li","Ming-Feng",""],["Ku","Yueh-Feng",""],["Yen","Hong-Xuan",""],["Liu","Chi",""],["Liu","Yu-Lun",""],["Chen","Albert Y. C.",""],["Kuo","Cheng-Hao",""],["Sun","Min",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 18:10:40 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 00:20:13 GMT"},{"version":"v3","created":"Fri, 2 Aug 2024 03:33:17 GMT"}],"updateDate":"2024-08-05","timestamp":1721239840000,"abstract":"  Sparse RGBD scene completion is a challenging task especially when\nconsidering consistent textures and geometries throughout the entire scene.\nDifferent from existing solutions that rely on human-designed text prompts or\npredefined camera trajectories, we propose GenRC, an automated training-free\npipeline to complete a room-scale 3D mesh with high-fidelity textures. To\nachieve this, we first project the sparse RGBD images to a highly incomplete 3D\nmesh. Instead of iteratively generating novel views to fill in the void, we\nutilized our proposed E-Diffusion to generate a view-consistent panoramic RGBD\nimage which ensures global geometry and appearance consistency. Furthermore, we\nmaintain the input-output scene stylistic consistency through textual inversion\nto replace human-designed text prompts. To bridge the domain gap among\ndatasets, E-Diffusion leverages models trained on large-scale datasets to\ngenerate diverse appearances. GenRC outperforms state-of-the-art methods under\nmost appearance and geometric metrics on ScanNet and ARKitScenes datasets, even\nthough GenRC is not trained on these datasets nor using predefined camera\ntrajectories. Project page: https://minfenli.github.io/GenRC\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}