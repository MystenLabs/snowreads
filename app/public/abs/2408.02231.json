{"id":"2408.02231","title":"REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language\n  Models","authors":"Agneet Chatterjee, Yiran Luo, Tejas Gokhale, Yezhou Yang, Chitta Baral","authorsParsed":[["Chatterjee","Agneet",""],["Luo","Yiran",""],["Gokhale","Tejas",""],["Yang","Yezhou",""],["Baral","Chitta",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 04:51:46 GMT"}],"updateDate":"2024-08-06","timestamp":1722833506000,"abstract":"  Text-to-Image (T2I) and multimodal large language models (MLLMs) have been\nadopted in solutions for several computer vision and multimodal learning tasks.\nHowever, it has been found that such vision-language models lack the ability to\ncorrectly reason over spatial relationships. To tackle this shortcoming, we\ndevelop the REVISION framework which improves spatial fidelity in\nvision-language models. REVISION is a 3D rendering based pipeline that\ngenerates spatially accurate synthetic images, given a textual prompt. REVISION\nis an extendable framework, which currently supports 100+ 3D assets, 11 spatial\nrelationships, all with diverse camera perspectives and backgrounds. Leveraging\nimages from REVISION as additional guidance in a training-free manner\nconsistently improves the spatial consistency of T2I models across all spatial\nrelationships, achieving competitive performance on the VISOR and T2I-CompBench\nbenchmarks. We also design RevQA, a question-answering benchmark to evaluate\nthe spatial reasoning abilities of MLLMs, and find that state-of-the-art models\nare not robust to complex spatial reasoning under adversarial settings. Our\nresults and findings indicate that utilizing rendering-based frameworks is an\neffective approach for developing spatially-aware generative models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}