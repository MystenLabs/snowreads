{"id":"2408.11512","title":"IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation","authors":"Baohao Liao, Christian Herold, Shahram Khadivi, Christof Monz","authorsParsed":[["Liao","Baohao",""],["Herold","Christian",""],["Khadivi","Shahram",""],["Monz","Christof",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 10:44:10 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 12:25:14 GMT"}],"updateDate":"2024-08-30","timestamp":1724237050000,"abstract":"  This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AtbHoBBBVwWDopUgIv6avEbphoGHFVCVflM_xUJZhPQ","pdfSize":"394001"}
