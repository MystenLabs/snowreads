{"id":"2407.04868","title":"Looking into Black Box Code Language Models","authors":"Muhammad Umair Haider, Umar Farooq, A.B. Siddique, Mark Marron","authorsParsed":[["Haider","Muhammad Umair",""],["Farooq","Umar",""],["Siddique","A. B.",""],["Marron","Mark",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 21:13:41 GMT"}],"updateDate":"2024-07-09","timestamp":1720214021000,"abstract":"  Language Models (LMs) have shown their application for tasks pertinent to\ncode and several code~LMs have been proposed recently. The majority of the\nstudies in this direction only focus on the improvements in performance of the\nLMs on different benchmarks, whereas LMs are considered black boxes. Besides\nthis, a handful of works attempt to understand the role of attention layers in\nthe code~LMs. Nonetheless, feed-forward layers remain under-explored which\nconsist of two-thirds of a typical transformer model's parameters.\n  In this work, we attempt to gain insights into the inner workings of code\nlanguage models by examining the feed-forward layers. To conduct our\ninvestigations, we use two state-of-the-art code~LMs, Codegen-Mono and\nPloycoder, and three widely used programming languages, Java, Go, and Python.\nWe focus on examining the organization of stored concepts, the editability of\nthese concepts, and the roles of different layers and input context size\nvariations for output generation. Our empirical findings demonstrate that lower\nlayers capture syntactic patterns while higher layers encode abstract concepts\nand semantics. We show concepts of interest can be edited within feed-forward\nlayers without compromising code~LM performance. Additionally, we observe\ninitial layers serve as ``thinking'' layers, while later layers are crucial for\npredicting subsequent code tokens. Furthermore, we discover earlier layers can\naccurately predict smaller contexts, but larger contexts need critical later\nlayers' contributions. We anticipate these findings will facilitate better\nunderstanding, debugging, and testing of code~LMs.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2ywlMY0c3sIL1Jq3-agSWMn-CjG_fNs3ysL9fUop4XU","pdfSize":"716521"}
