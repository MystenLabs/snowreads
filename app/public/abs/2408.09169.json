{"id":"2408.09169","title":"Automatic Metrics in Natural Language Generation: A Survey of Current\n  Evaluation Practices","authors":"Patr\\'icia Schmidtov\\'a and Saad Mahamood and Simone Balloccu and\n  Ond\\v{r}ej Du\\v{s}ek and Albert Gatt and Dimitra Gkatzia and David M.\n  Howcroft and Ond\\v{r}ej Pl\\'atek and Adarsa Sivaprasad","authorsParsed":[["Schmidtová","Patrícia",""],["Mahamood","Saad",""],["Balloccu","Simone",""],["Dušek","Ondřej",""],["Gatt","Albert",""],["Gkatzia","Dimitra",""],["Howcroft","David M.",""],["Plátek","Ondřej",""],["Sivaprasad","Adarsa",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 11:13:10 GMT"}],"updateDate":"2024-08-20","timestamp":1723893190000,"abstract":"  Automatic metrics are extensively used to evaluate natural language\nprocessing systems. However, there has been increasing focus on how they are\nused and reported by practitioners within the field. In this paper, we have\nconducted a survey on the use of automatic metrics, focusing particularly on\nnatural language generation (NLG) tasks. We inspect which metrics are used as\nwell as why they are chosen and how their use is reported. Our findings from\nthis survey reveal significant shortcomings, including inappropriate metric\nusage, lack of implementation details and missing correlations with human\njudgements. We conclude with recommendations that we believe authors should\nfollow to enable more rigour within the field.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}