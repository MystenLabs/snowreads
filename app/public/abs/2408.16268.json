{"id":"2408.16268","title":"UDD: Dataset Distillation via Mining Underutilized Regions","authors":"Shiguang Wang, Zhongyu Zhang, Jian Cheng","authorsParsed":[["Wang","Shiguang",""],["Zhang","Zhongyu",""],["Cheng","Jian",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 05:13:01 GMT"}],"updateDate":"2024-08-30","timestamp":1724908381000,"abstract":"  Dataset distillation synthesizes a small dataset such that a model trained on\nthis set approximates the performance of the original dataset. Recent studies\non dataset distillation focused primarily on the design of the optimization\nprocess, with methods such as gradient matching, feature alignment, and\ntraining trajectory matching. However, little attention has been given to the\nissue of underutilized regions in synthetic images. In this paper, we propose\nUDD, a novel approach to identify and exploit the underutilized regions to make\nthem informative and discriminate, and thus improve the utilization of the\nsynthetic dataset. Technically, UDD involves two underutilized regions\nsearching policies for different conditions, i.e., response-based policy and\ndata jittering-based policy. Compared with previous works, such two policies\nare utilization-sensitive, equipping with the ability to dynamically adjust the\nunderutilized regions during the training process. Additionally, we analyze the\ncurrent model optimization problem and design a category-wise feature\ncontrastive loss, which can enhance the distinguishability of different\ncategories and alleviate the shortcomings of the existing multi-formation\nmethods. Experimentally, our method improves the utilization of the synthetic\ndataset and outperforms the state-of-the-art methods on various datasets, such\nas MNIST, FashionMNIST, SVHN, CIFAR-10, and CIFAR-100. For example, the\nimprovements on CIFAR-10 and CIFAR-100 are 4.0\\% and 3.7\\% over the next best\nmethod with IPC=1, by mining the underutilized regions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}