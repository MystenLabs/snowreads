{"id":"2407.18134","title":"$\\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning\n  with Sample Similarity Graphs","authors":"Vlad Sobal, Mark Ibrahim, Randall Balestriero, Vivien Cabannes, Diane\n  Bouchacourt, Pietro Astolfi, Kyunghyun Cho, Yann LeCun","authorsParsed":[["Sobal","Vlad",""],["Ibrahim","Mark",""],["Balestriero","Randall",""],["Cabannes","Vivien",""],["Bouchacourt","Diane",""],["Astolfi","Pietro",""],["Cho","Kyunghyun",""],["LeCun","Yann",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 15:38:16 GMT"},{"version":"v2","created":"Wed, 11 Sep 2024 21:50:21 GMT"}],"updateDate":"2024-09-13","timestamp":1721921896000,"abstract":"  Learning good representations involves capturing the diverse ways in which\ndata samples relate. Contrastive loss - an objective matching related samples -\nunderlies methods from self-supervised to multimodal learning. Contrastive\nlosses, however, can be viewed more broadly as modifying a similarity graph to\nindicate how samples should relate in the embedding space. This view reveals a\nshortcoming in contrastive learning: the similarity graph is binary, as only\none sample is the related positive sample. Crucially, similarities\n\\textit{across} samples are ignored. Based on this observation, we revise the\nstandard contrastive loss to explicitly encode how a sample relates to others.\nWe experiment with this new objective, called $\\mathbb{X}$-Sample Contrastive,\nto train vision models based on similarities in class or text caption\ndescriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M\nwith 3 million, and CC12M with 12 million samples. The representations learned\nvia our objective outperform both contrastive self-supervised and\nvision-language models trained on the same data across a range of tasks. When\ntraining on CC12M, we outperform CLIP by $0.6\\%$ on both ImageNet and ImageNet\nReal. Our objective appears to work particularly well in lower-data regimes,\nwith gains over CLIP of $16.8\\%$ on ImageNet and $18.1\\%$ on ImageNet Real when\ntraining with CC3M. Finally, our objective seems to encourage the model to\nlearn representations that separate objects from their attributes and\nbackgrounds, with gains of $3.3$-$5.6$\\% over CLIP on ImageNet9. We hope the\nproposed solution takes a small step towards developing richer learning\nobjectives for understanding sample relations in foundation models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}