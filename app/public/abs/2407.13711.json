{"id":"2407.13711","title":"FSP-Laplace: Function-Space Priors for the Laplace Approximation in\n  Bayesian Deep Learning","authors":"Tristan Cinquin, Marvin Pf\\\"ortner, Vincent Fortuin, Philipp Hennig,\n  Robert Bamler","authorsParsed":[["Cinquin","Tristan",""],["Pf√∂rtner","Marvin",""],["Fortuin","Vincent",""],["Hennig","Philipp",""],["Bamler","Robert",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:08:58 GMT"}],"updateDate":"2024-07-19","timestamp":1721322538000,"abstract":"  Laplace approximations are popular techniques for endowing deep networks with\nepistemic uncertainty estimates as they can be applied without altering the\npredictions of the neural network, and they scale to large models and datasets.\nWhile the choice of prior strongly affects the resulting posterior\ndistribution, computational tractability and lack of interpretability of weight\nspace typically limit the Laplace approximation to isotropic Gaussian priors,\nwhich are known to cause pathological behavior as depth increases. As a remedy,\nwe directly place a prior on function space. More precisely, since Lebesgue\ndensities do not exist on infinite-dimensional function spaces, we have to\nrecast training as finding the so-called weak mode of the posterior measure\nunder a Gaussian process (GP) prior restricted to the space of functions\nrepresentable by the neural network. Through the GP prior, one can express\nstructured and interpretable inductive biases, such as regularity or\nperiodicity, directly in function space, while still exploiting the implicit\ninductive biases that allow deep networks to generalize. After model\nlinearization, the training objective induces a negative log-posterior density\nto which we apply a Laplace approximation, leveraging highly scalable methods\nfrom matrix-free linear algebra. Our method provides improved results where\nprior knowledge is abundant, e.g., in many scientific inference tasks. At the\nsame time, it stays competitive for black-box regression and classification\ntasks where neural networks typically excel.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}