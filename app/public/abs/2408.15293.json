{"id":"2408.15293","title":"Learning Granularity Representation for Temporal Knowledge Graph\n  Completion","authors":"Jinchuan Zhang, Tianqi Wan, Chong Mu, Guangxi Lu, Ling Tian","authorsParsed":[["Zhang","Jinchuan",""],["Wan","Tianqi",""],["Mu","Chong",""],["Lu","Guangxi",""],["Tian","Ling",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 08:19:34 GMT"}],"updateDate":"2024-08-29","timestamp":1724746774000,"abstract":"  Temporal Knowledge Graphs (TKGs) incorporate temporal information to reflect\nthe dynamic structural knowledge and evolutionary patterns of real-world facts.\nNevertheless, TKGs are still limited in downstream applications due to the\nproblem of incompleteness. Consequently, TKG completion (also known as link\nprediction) has been widely studied, with recent research focusing on\nincorporating independent embeddings of time or combining them with entities\nand relations to form temporal representations. However, most existing methods\noverlook the impact of history from a multi-granularity aspect. The inherent\nsemantics of human-defined temporal granularities, such as ordinal dates,\nreveal general patterns to which facts typically adhere. To counter this\nlimitation, this paper proposes \\textbf{L}earning \\textbf{G}ranularity\n\\textbf{Re}presentation (termed $\\mathsf{LGRe}$) for TKG completion. It\ncomprises two main components: Granularity Representation Learning (GRL) and\nAdaptive Granularity Balancing (AGB). Specifically, GRL employs time-specific\nmulti-layer convolutional neural networks to capture interactions between\nentities and relations at different granularities. After that, AGB generates\nadaptive weights for these embeddings according to temporal semantics,\nresulting in expressive representations of predictions. Moreover, to reflect\nsimilar semantics of adjacent timestamps, a temporal loss function is\nintroduced. Extensive experimental results on four event benchmarks demonstrate\nthe effectiveness of $\\mathsf{LGRe}$ in learning time-related representations.\nTo ensure reproducibility, our code is available at\nhttps://github.com/KcAcoZhang/LGRe.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}