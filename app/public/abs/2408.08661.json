{"id":"2408.08661","title":"MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector","authors":"Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang","authorsParsed":[["Fu","Wenjie",""],["Wang","Huandong",""],["Gao","Chen",""],["Liu","Guanghua",""],["Li","Yong",""],["Jiang","Tao",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 11:09:56 GMT"}],"updateDate":"2024-08-19","timestamp":1723806596000,"abstract":"  The increasing parameters and expansive dataset of large language models\n(LLMs) highlight the urgent demand for a technical solution to audit the\nunderlying privacy risks and copyright issues associated with LLMs. Existing\nstudies have partially addressed this need through an exploration of the\npre-training data detection problem, which is an instance of a membership\ninference attack (MIA). This problem involves determining whether a given piece\nof text has been used during the pre-training phase of the target LLM. Although\nexisting methods have designed various sophisticated MIA score functions to\nachieve considerable detection performance in pre-trained LLMs, how to achieve\nhigh-confidence detection and how to perform MIA on aligned LLMs remain\nchallenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA\nmethod, which instructs LLMs themselves to serve as a more precise pre-training\ndata detector internally, rather than design an external MIA score function.\nFurthermore, we design two instruction-based safeguards to respectively\nmitigate the privacy risks brought by the existing methods and MIA-Tuner. To\ncomprehensively evaluate the most recent state-of-the-art LLMs, we collect a\nmore up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely\nadopted benchmark WIKIMIA. We conduct extensive experiments across various\naligned and unaligned LLMs over the two benchmark datasets. The results\ndemonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nK60uJwWx7TKgwJEJtUjEpHbMuCBMRDKU3JfPk3C0GM","pdfSize":"2266526"}
