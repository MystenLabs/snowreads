{"id":"2407.18534","title":"Boosting Cross-Domain Point Classification via Distilling Relational\n  Priors from 2D Transformers","authors":"Longkun Zou, Wanru Zhu, Ke Chen, Lihua Guo, Kailing Guo, Kui Jia, and\n  Yaowei Wang","authorsParsed":[["Zou","Longkun",""],["Zhu","Wanru",""],["Chen","Ke",""],["Guo","Lihua",""],["Guo","Kailing",""],["Jia","Kui",""],["Wang","Yaowei",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 06:29:09 GMT"},{"version":"v2","created":"Mon, 5 Aug 2024 07:37:06 GMT"}],"updateDate":"2024-08-06","timestamp":1721975349000,"abstract":"  Semantic pattern of an object point cloud is determined by its topological\nconfiguration of local geometries. Learning discriminative representations can\nbe challenging due to large shape variations of point sets in local regions and\nincomplete surface in a global perspective, which can be made even more severe\nin the context of unsupervised domain adaptation (UDA). In specific,\ntraditional 3D networks mainly focus on local geometric details and ignore the\ntopological structure between local geometries, which greatly limits their\ncross-domain generalization. Recently, the transformer-based models have\nachieved impressive performance gain in a range of image-based tasks,\nbenefiting from its strong generalization capability and scalability stemming\nfrom capturing long range correlation across local patches. Inspired by such\nsuccesses of visual transformers, we propose a novel Relational Priors\nDistillation (RPD) method to extract relational priors from the well-trained\ntransformers on massive images, which can significantly empower cross-domain\nrepresentations with consistent topological priors of objects. To this end, we\nestablish a parameter-frozen pre-trained transformer module shared between 2D\nteacher and 3D student models, complemented by an online knowledge distillation\nstrategy for semantically regularizing the 3D student model. Furthermore, we\nintroduce a novel self-supervised task centered on reconstructing masked point\ncloud patches using corresponding masked multi-view image features, thereby\nempowering the model with incorporating 3D geometric information. Experiments\non the PointDA-10 and the Sim-to-Real datasets verify that the proposed method\nconsistently achieves the state-of-the-art performance of UDA for point cloud\nclassification. The source code of this work is available at\nhttps://github.com/zou-longkun/RPD.git.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}