{"id":"2408.09122","title":"MaskBEV: Towards A Unified Framework for BEV Detection and Map\n  Segmentation","authors":"Xiao Zhao, Xukun Zhang, Dingkang Yang, Mingyang Sun, Mingcheng Li,\n  Shunli Wang, and Lihua Zhang","authorsParsed":[["Zhao","Xiao",""],["Zhang","Xukun",""],["Yang","Dingkang",""],["Sun","Mingyang",""],["Li","Mingcheng",""],["Wang","Shunli",""],["Zhang","Lihua",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 07:11:38 GMT"}],"updateDate":"2024-08-20","timestamp":1723878698000,"abstract":"  Accurate and robust multimodal multi-task perception is crucial for modern\nautonomous driving systems. However, current multimodal perception research\nfollows independent paradigms designed for specific perception tasks, leading\nto a lack of complementary learning among tasks and decreased performance in\nmulti-task learning (MTL) due to joint training. In this paper, we propose\nMaskBEV, a masked attention-based MTL paradigm that unifies 3D object detection\nand bird's eye view (BEV) map segmentation. MaskBEV introduces a task-agnostic\nTransformer decoder to process these diverse tasks, enabling MTL to be\ncompleted in a unified decoder without requiring additional design of specific\ntask heads. To fully exploit the complementary information between BEV map\nsegmentation and 3D object detection tasks in BEV space, we propose spatial\nmodulation and scene-level context aggregation strategies. These strategies\nconsider the inherent dependencies between BEV segmentation and 3D detection,\nnaturally boosting MTL performance. Extensive experiments on nuScenes dataset\nshow that compared with previous state-of-the-art MTL methods, MaskBEV achieves\n1.3 NDS improvement in 3D object detection and 2.7 mIoU improvement in BEV map\nsegmentation, while also demonstrating slightly leading inference speed.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}