{"id":"2407.06531","title":"Decomposition Betters Tracking Everything Everywhere","authors":"Rui Li, Dong Liu","authorsParsed":[["Li","Rui",""],["Liu","Dong",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 04:01:23 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 14:11:47 GMT"}],"updateDate":"2024-07-17","timestamp":1720497683000,"abstract":"  Recent studies on motion estimation have advocated an optimized motion\nrepresentation that is globally consistent across the entire video, preferably\nfor every pixel. This is challenging as a uniform representation may not\naccount for the complex and diverse motion and appearance of natural videos. We\naddress this problem and propose a new test-time optimization method, named\nDecoMotion, for estimating per-pixel and long-range motion. DecoMotion\nexplicitly decomposes video content into static scenes and dynamic objects,\neither of which uses a quasi-3D canonical volume to represent. DecoMotion\nseparately coordinates the transformations between local and canonical spaces,\nfacilitating an affine transformation for the static scene that corresponds to\ncamera motion. For the dynamic volume, DecoMotion leverages discriminative and\ntemporally consistent features to rectify the non-rigid transformation. The two\nvolumes are finally fused to fully represent motion and appearance. This\ndivide-and-conquer strategy leads to more robust tracking through occlusions\nand deformations and meanwhile obtains decomposed appearances. We conduct\nevaluations on the TAP-Vid benchmark. The results demonstrate our method boosts\nthe point-tracking accuracy by a large margin and performs on par with some\nstate-of-the-art dedicated point-tracking solutions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}