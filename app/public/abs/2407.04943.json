{"id":"2407.04943","title":"Quantizing YOLOv7: A Comprehensive Study","authors":"Mohammadamin Baghbanbashi, Mohsen Raji, Behnam Ghavami","authorsParsed":[["Baghbanbashi","Mohammadamin",""],["Raji","Mohsen",""],["Ghavami","Behnam",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 03:23:04 GMT"}],"updateDate":"2024-07-09","timestamp":1720236184000,"abstract":"  YOLO is a deep neural network (DNN) model presented for robust real-time\nobject detection following the one-stage inference approach. It outperforms\nother real-time object detectors in terms of speed and accuracy by a wide\nmargin. Nevertheless, since YOLO is developed upon a DNN backbone with numerous\nparameters, it will cause excessive memory load, thereby deploying it on\nmemory-constrained devices is a severe challenge in practice. To overcome this\nlimitation, model compression techniques, such as quantizing parameters to\nlower-precision values, can be adopted. As the most recent version of YOLO,\nYOLOv7 achieves such state-of-the-art performance in speed and accuracy in the\nrange of 5 FPS to 160 FPS that it surpasses all former versions of YOLO and\nother existing models in this regard. So far, the robustness of several\nquantization schemes has been evaluated on older versions of YOLO. These\nmethods may not necessarily yield similar results for YOLOv7 as it utilizes a\ndifferent architecture. In this paper, we conduct in-depth research on the\neffectiveness of a variety of quantization schemes on the pre-trained weights\nof the state-of-the-art YOLOv7 model. Experimental results demonstrate that\nusing 4-bit quantization coupled with the combination of different\ngranularities results in ~3.92x and ~3.86x memory-saving for uniform and\nnon-uniform quantization, respectively, with only 2.5% and 1% accuracy loss\ncompared to the full-precision baseline model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Hardware Architecture","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}