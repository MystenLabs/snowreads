{"id":"2408.13414","title":"Epistemically robust selection of fitted models","authors":"Alexandre Ren\\'e, Andr\\'e Longtin","authorsParsed":[["René","Alexandre",""],["Longtin","André",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 00:38:08 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 08:12:39 GMT"}],"updateDate":"2024-08-30","timestamp":1724459888000,"abstract":"  Fitting models to data is an important part of the practice of science, made\nalmost ubiquitous by advances in machine learning. Very often however, fitted\nsolutions are not unique, but form an ensemble of candidate models --\nqualitatively different, yet with comparable quantitative performance. One then\nneeds a criterion which can select the best candidate models, or at least\nfalsify (reject) the worst ones. Because standard statistical approaches to\nmodel selection rely on assumptions which are usually invalid in scientific\ncontexts, they tend to be overconfident, rejecting models based on little more\nthan statistical noise. The ideal objective for fitting models is generally\nconsidered to be the risk: this is the theoretical average loss of a model\n(assuming unlimited data). In this work we develop a nonparametric method for\nestimating, for each candidate model, the epistemic uncertainty on its risk: in\nother words we associate to each model a distribution of scores which accounts\nfor expected modelling errors. We then propose that a model falsification\ncriterion should mirror established experimental practice: a falsification\nresult should be accepted only if it is reproducible across experimental\nvariations. The strength of this approach is illustrated using examples from\nphysics and neuroscience.\n","subjects":["Statistics/Methodology"],"license":"http://creativecommons.org/licenses/by/4.0/"}