{"id":"2407.12552","title":"Policies Grow on Trees: Model Checking Families of MDPs","authors":"Roman Andriushchenko, Milan \\v{C}e\\v{s}ka, Sebastian Junges, Filip\n  Mac\\'ak","authorsParsed":[["Andriushchenko","Roman",""],["Češka","Milan",""],["Junges","Sebastian",""],["Macák","Filip",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 13:32:04 GMT"}],"updateDate":"2024-07-18","timestamp":1721223124000,"abstract":"  Markov decision processes (MDPs) provide a fundamental model for sequential\ndecision making under process uncertainty. A classical synthesis task is to\ncompute for a given MDP a winning policy that achieves a desired specification.\nHowever, at design time, one typically needs to consider a family of MDPs\nmodelling various system variations. For a given family, we study synthesising\n(1) the subset of MDPs where a winning policy exists and (2) a preferably small\nnumber of winning policies that together cover this subset. We introduce policy\ntrees that concisely capture the synthesis result. The key ingredient for\nsynthesising policy trees is a recursive application of a game-based\nabstraction. We combine this abstraction with an efficient refinement procedure\nand a post-processing step. An extensive empirical evaluation demonstrates\nsuperior scalability of our approach compared to naive baselines. For one of\nthe benchmarks, we find 246 winning policies covering 94 million MDPs. Our\nalgorithm requires less than 30 minutes, whereas the naive baseline only covers\n3.7% of MDPs in 24 hours.\n","subjects":["Computing Research Repository/Logic in Computer Science"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LJ-6uF9V-6cp7NMh_x3wkdf10d8_w2v5De5TuYUWnzg","pdfSize":"941277"}
