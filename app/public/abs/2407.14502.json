{"id":"2407.14502","title":"M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models","authors":"Seunggeun Chi, Hyung-gun Chi, Hengbo Ma, Nakul Agarwal, Faizan\n  Siddiqui, Karthik Ramani, and Kwonjoon Lee","authorsParsed":[["Chi","Seunggeun",""],["Chi","Hyung-gun",""],["Ma","Hengbo",""],["Agarwal","Nakul",""],["Siddiqui","Faizan",""],["Ramani","Karthik",""],["Lee","Kwonjoon",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:57:33 GMT"}],"updateDate":"2024-07-22","timestamp":1721411853000,"abstract":"  We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novel\napproach for human motion generation from textual descriptions of multiple\nactions, utilizing the strengths of discrete diffusion models. This approach\nadeptly addresses the challenge of generating multi-motion sequences, ensuring\nseamless transitions of motions and coherence across a series of actions. The\nstrength of M2D2M lies in its dynamic transition probability within the\ndiscrete diffusion model, which adapts transition probabilities based on the\nproximity between motion tokens, encouraging mixing between different modes.\nComplemented by a two-phase sampling strategy that includes independent and\njoint denoising steps, M2D2M effectively generates long-term, smooth, and\ncontextually coherent human motion sequences, utilizing a model trained for\nsingle-motion generation. Extensive experiments demonstrate that M2D2M\nsurpasses current state-of-the-art benchmarks for motion generation from text\ndescriptions, showcasing its efficacy in interpreting language semantics and\ngenerating dynamic, realistic motions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"0ULTmWL9vdtwrCgjGAd4RIrZtl5bFXfz4AcYu4EcFDo","pdfSize":"4730625"}
