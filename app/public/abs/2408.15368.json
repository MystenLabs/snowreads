{"id":"2408.15368","title":"Optimization Solution Functions as Deterministic Policies for Offline\n  Reinforcement Learning","authors":"Vanshaj Khattar and Ming Jin","authorsParsed":[["Khattar","Vanshaj",""],["Jin","Ming",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 19:04:32 GMT"}],"updateDate":"2024-08-29","timestamp":1724785472000,"abstract":"  Offline reinforcement learning (RL) is a promising approach for many control\napplications but faces challenges such as limited data coverage and value\nfunction overestimation. In this paper, we propose an implicit actor-critic\n(iAC) framework that employs optimization solution functions as a deterministic\npolicy (actor) and a monotone function over the optimal value of optimization\nas a critic. By encoding optimality in the actor policy, we show that the\nlearned policies are robust to the suboptimality of the learned actor\nparameters via the exponentially decaying sensitivity (EDS) property. We obtain\nperformance guarantees for the proposed iAC framework and show its benefits\nover general function approximation schemes. Finally, we validate the proposed\nframework on two real-world applications and show a significant improvement\nover state-of-the-art (SOTA) offline RL methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3mbeopmTc0jd4ANLpQt7mKijY17vma8o0b4eC0-y-sc","pdfSize":"484807"}
