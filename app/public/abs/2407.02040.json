{"id":"2407.02040","title":"ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score\n  Distillation","authors":"Zhiyuan Ma, Yuxiang Wei, Yabin Zhang, Xiangyu Zhu, Zhen Lei, Lei Zhang","authorsParsed":[["Ma","Zhiyuan",""],["Wei","Yuxiang",""],["Zhang","Yabin",""],["Zhu","Xiangyu",""],["Lei","Zhen",""],["Zhang","Lei",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 08:12:14 GMT"}],"updateDate":"2024-07-03","timestamp":1719907934000,"abstract":"  By leveraging the text-to-image diffusion priors, score distillation can\nsynthesize 3D contents without paired text-3D training data. Instead of\nspending hours of online optimization per text prompt, recent studies have been\nfocused on learning a text-to-3D generative network for amortizing multiple\ntext-3D relations, which can synthesize 3D contents in seconds. However,\nexisting score distillation methods are hard to scale up to a large amount of\ntext prompts due to the difficulties in aligning pretrained diffusion prior\nwith the distribution of rendered images from various text prompts. Current\nstate-of-the-arts such as Variational Score Distillation finetune the\npretrained diffusion model to minimize the noise prediction error so as to\nalign the distributions, which are however unstable to train and will impair\nthe model's comprehension capability to numerous text prompts. Based on the\nobservation that the diffusion models tend to have lower noise prediction\nerrors at earlier timesteps, we propose Asynchronous Score Distillation (ASD),\nwhich minimizes the noise prediction error by shifting the diffusion timestep\nto earlier ones. ASD is stable to train and can scale up to 100k prompts. It\nreduces the noise prediction error without changing the weights of pre-trained\ndiffusion model, thus keeping its strong comprehension capability to prompts.\nWe conduct extensive experiments across different 2D diffusion models,\nincluding Stable Diffusion and MVDream, and text-to-3D generators, including\nHyper-iNGP, 3DConv-Net and Triplane-Transformer. The results demonstrate ASD's\neffectiveness in stable 3D generator training, high-quality 3D content\nsynthesis, and its superior prompt-consistency, especially under large prompt\ncorpus.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}