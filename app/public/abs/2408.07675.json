{"id":"2408.07675","title":"G$^2$V$^2$former: Graph Guided Video Vision Transformer for Face\n  Anti-Spoofing","authors":"Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li","authorsParsed":[["Yang","Jingyi",""],["Yu","Zitong",""],["Ni","Xiuming",""],["He","Jia",""],["Li","Hui",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 17:22:41 GMT"}],"updateDate":"2024-08-15","timestamp":1723656161000,"abstract":"  In videos containing spoofed faces, we may uncover the spoofing evidence\nbased on either photometric or dynamic abnormality, even a combination of both.\nPrevailing face anti-spoofing (FAS) approaches generally concentrate on the\nsingle-frame scenario, however, purely photometric-driven methods overlook the\ndynamic spoofing clues that may be exposed over time. This may lead FAS systems\nto conclude incorrect judgments, especially in cases where it is easily\ndistinguishable in terms of dynamics but challenging to discern in terms of\nphotometrics. To this end, we propose the Graph Guided Video Vision Transformer\n(G$^2$V$^2$former), which combines faces with facial landmarks for photometric\nand dynamic feature fusion. We factorize the attention into space and time, and\nfuse them via a spatiotemporal block. Specifically, we design a novel temporal\nattention called Kronecker temporal attention, which has a wider receptive\nfield, and is beneficial for capturing dynamic information. Moreover, we\nleverage the low-semantic motion of facial landmarks to guide the high-semantic\nchange of facial expressions based on the motivation that regions containing\nlandmarks may reveal more dynamic clues. Extensive experiments on nine\nbenchmark datasets demonstrate that our method achieves superior performance\nunder various scenarios. The codes will be released soon.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}