{"id":"2407.00389","title":"Query-Efficient Hard-Label Black-Box Attack against Vision Transformers","authors":"Chao Zhou, Xiaowen Shi and Yuan-Gen Wang","authorsParsed":[["Zhou","Chao",""],["Shi","Xiaowen",""],["Wang","Yuan-Gen",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 10:09:12 GMT"}],"updateDate":"2024-07-02","timestamp":1719655752000,"abstract":"  Recent studies have revealed that vision transformers (ViTs) face similar\nsecurity risks from adversarial attacks as deep convolutional neural networks\n(CNNs). However, directly applying attack methodology on CNNs to ViTs has been\ndemonstrated to be ineffective since the ViTs typically work on patch-wise\nencoding. This article explores the vulnerability of ViTs against adversarial\nattacks under a black-box scenario, and proposes a novel query-efficient\nhard-label adversarial attack method called AdvViT. Specifically, considering\nthat ViTs are highly sensitive to patch modification, we propose to optimize\nthe adversarial perturbation on the individual patches. To reduce the dimension\nof perturbation search space, we modify only a handful of low-frequency\ncomponents of each patch. Moreover, we design a weight mask matrix for all\npatches to further optimize the perturbation on different regions of a whole\nimage. We test six mainstream ViT backbones on the ImageNet-1k dataset.\nExperimental results show that compared with the state-of-the-art attacks on\nCNNs, our AdvViT achieves much lower $L_2$-norm distortion under the same query\nbudget, sufficiently validating the vulnerability of ViTs against adversarial\nattacks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}