{"id":"2408.05478","title":"Multi-agent Planning using Visual Language Models","authors":"Michele Brienza, Francesco Argenziano, Vincenzo Suriani, Domenico D.\n  Bloisi and Daniele Nardi","authorsParsed":[["Brienza","Michele",""],["Argenziano","Francesco",""],["Suriani","Vincenzo",""],["Bloisi","Domenico D.",""],["Nardi","Daniele",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 08:10:17 GMT"}],"updateDate":"2024-08-13","timestamp":1723277417000,"abstract":"  Large Language Models (LLMs) and Visual Language Models (VLMs) are attracting\nincreasing interest due to their improving performance and applications across\nvarious domains and tasks. However, LLMs and VLMs can produce erroneous\nresults, especially when a deep understanding of the problem domain is\nrequired. For instance, when planning and perception are needed simultaneously,\nthese models often struggle because of difficulties in merging multi-modal\ninformation. To address this issue, fine-tuned models are typically employed\nand trained on specialized data structures representing the environment. This\napproach has limited effectiveness, as it can overly complicate the context for\nprocessing. In this paper, we propose a multi-agent architecture for embodied\ntask planning that operates without the need for specific data structures as\ninput. Instead, it uses a single image of the environment, handling free-form\ndomains by leveraging commonsense knowledge. We also introduce a novel, fully\nautomatic evaluation procedure, PG2S, designed to better assess the quality of\na plan. We validated our approach using the widely recognized ALFRED dataset,\ncomparing PG2S to the existing KAS metric to further evaluate the quality of\nthe generated plans.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7gd9ADtifkaZTXPbS9HPQPAgrvZeY8ZzevGiGsc8a-Y","pdfSize":"2383043"}
