{"id":"2408.01137","title":"PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting\n  Network","authors":"Changqun Xia, Chenxi Xie, Zhentao He, Tianshu Yu, and Jia Li","authorsParsed":[["Xia","Changqun",""],["Xie","Chenxi",""],["He","Zhentao",""],["Yu","Tianshu",""],["Li","Jia",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 09:31:21 GMT"}],"updateDate":"2024-08-05","timestamp":1722591081000,"abstract":"  We present an advanced study on more challenging high-resolution salient\nobject detection (HRSOD) from both dataset and network framework perspectives.\nTo compensate for the lack of HRSOD dataset, we thoughtfully collect a\nlarge-scale high resolution salient object detection dataset, called UHRSD,\ncontaining 5,920 images from real-world complex scenarios at 4K-8K resolutions.\nAll the images are finely annotated in pixel-level, far exceeding previous\nlow-resolution SOD datasets. Aiming at overcoming the contradiction between the\nsampling depth and the receptive field size in the past methods, we propose a\nnovel one-stage framework for HR-SOD task using pyramid grafting mechanism. In\ngeneral, transformer-based and CNN-based backbones are adopted to extract\nfeatures from different resolution images independently and then these features\nare grafted from transformer branch to CNN branch. An attention-based\nCross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine\nbroken detailed information more holistically, guided by different source\nfeature during decoding process. Moreover, we design an Attention Guided Loss\n(AGL) to explicitly supervise the attention matrix generated by CMGM to help\nthe network better interact with the attention from different branches.\nComprehensive experiments on UHRSD and widely-used SOD datasets demonstrate\nthat our method can simultaneously locate salient object and preserve rich\ndetails, outperforming state-of-the-art methods. To verify the generalization\nability of the proposed framework, we apply it to the camouflaged object\ndetection (COD) task. Notably, our method performs superior to most\nstate-of-the-art COD methods without bells and whistles.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}