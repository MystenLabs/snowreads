{"id":"2408.03033","title":"L3iTC at the FinLLM Challenge Task: Quantization for Financial Text\n  Classification & Summarization","authors":"Elvys Linhares Pontes and Carlos-Emiliano Gonz\\'alez-Gallardo and\n  Mohamed Benjannet and Caryn Qu and Antoine Doucet","authorsParsed":[["Pontes","Elvys Linhares",""],["Gonz√°lez-Gallardo","Carlos-Emiliano",""],["Benjannet","Mohamed",""],["Qu","Caryn",""],["Doucet","Antoine",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 08:25:49 GMT"}],"updateDate":"2024-08-07","timestamp":1722932749000,"abstract":"  This article details our participation (L3iTC) in the FinLLM Challenge Task\n2024, focusing on two key areas: Task 1, financial text classification, and\nTask 2, financial text summarization. To address these challenges, we\nfine-tuned several large language models (LLMs) to optimize performance for\neach task. Specifically, we used 4-bit quantization and LoRA to determine which\nlayers of the LLMs should be trained at a lower precision. This approach not\nonly accelerated the fine-tuning process on the training data provided by the\norganizers but also enabled us to run the models on low GPU memory. Our\nfine-tuned models achieved third place for the financial classification task\nwith an F1-score of 0.7543 and secured sixth place in the financial\nsummarization task on the official test datasets.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computational Engineering, Finance, and Science"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}