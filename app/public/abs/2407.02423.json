{"id":"2407.02423","title":"On the Anatomy of Attention","authors":"Nikhil Khatri and Tuomas Laakkonen and Jonathon Liu and Vincent\n  Wang-Ma\\'scianica","authorsParsed":[["Khatri","Nikhil",""],["Laakkonen","Tuomas",""],["Liu","Jonathon",""],["Wang-Ma≈õcianica","Vincent",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 16:50:26 GMT"},{"version":"v2","created":"Sun, 7 Jul 2024 17:03:05 GMT"}],"updateDate":"2024-07-09","timestamp":1719939026000,"abstract":"  We introduce a category-theoretic diagrammatic formalism in order to\nsystematically relate and reason about machine learning models. Our diagrams\npresent architectures intuitively but without loss of essential detail, where\nnatural relationships between models are captured by graphical transformations,\nand important differences and similarities can be identified at a glance. In\nthis paper, we focus on attention mechanisms: translating folklore into\nmathematical derivations, and constructing a taxonomy of attention variants in\nthe literature. As a first example of an empirical investigation underpinned by\nour formalism, we identify recurring anatomical components of attention, which\nwe exhaustively recombine to explore a space of variations on the attention\nmechanism.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Category Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}