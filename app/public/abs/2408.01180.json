{"id":"2408.01180","title":"Nested Music Transformer: Sequentially Decoding Compound Tokens in\n  Symbolic Music and Audio Generation","authors":"Jiwoo Ryu, Hao-Wen Dong, Jongmin Jung, Dasaem Jeong","authorsParsed":[["Ryu","Jiwoo",""],["Dong","Hao-Wen",""],["Jung","Jongmin",""],["Jeong","Dasaem",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 11:02:38 GMT"}],"updateDate":"2024-08-05","timestamp":1722596558000,"abstract":"  Representing symbolic music with compound tokens, where each token consists\nof several different sub-tokens representing a distinct musical feature or\nattribute, offers the advantage of reducing sequence length. While previous\nresearch has validated the efficacy of compound tokens in music sequence\nmodeling, predicting all sub-tokens simultaneously can lead to suboptimal\nresults as it may not fully capture the interdependencies between them. We\nintroduce the Nested Music Transformer (NMT), an architecture tailored for\ndecoding compound tokens autoregressively, similar to processing flattened\ntokens, but with low memory usage. The NMT consists of two transformers: the\nmain decoder that models a sequence of compound tokens and the sub-decoder for\nmodeling sub-tokens of each compound token. The experiment results showed that\napplying the NMT to compound tokens can enhance the performance in terms of\nbetter perplexity in processing various symbolic music datasets and discrete\naudio tokens from the MAESTRO dataset.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Oeyogv3ZmsWmBNZ1vcInUH_C-9b89SHi5yIfqWn55nk","pdfSize":"692360","txDigest":"7pZ7BHcDyz8E7noGawNgx4ogPXX81edtBsYySpZEqNNM","endEpoch":"1","status":"CERTIFIED"}
