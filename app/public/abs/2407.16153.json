{"id":"2407.16153","title":"On the Benefits of Rank in Attention Layers","authors":"Noah Amsel, Gilad Yehudai, and Joan Bruna","authorsParsed":[["Amsel","Noah",""],["Yehudai","Gilad",""],["Bruna","Joan",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 03:40:24 GMT"}],"updateDate":"2024-07-24","timestamp":1721706024000,"abstract":"  Attention-based mechanisms are widely used in machine learning, most\nprominently in transformers. However, hyperparameters such as the rank of the\nattention matrices and the number of heads are scaled nearly the same way in\nall realizations of this architecture, without theoretical justification. In\nthis work we show that there are dramatic trade-offs between the rank and\nnumber of heads of the attention mechanism. Specifically, we present a simple\nand natural target function that can be represented using a single full-rank\nattention head for any context length, but that cannot be approximated by\nlow-rank attention unless the number of heads is exponential in the embedding\ndimension, even for short context lengths. Moreover, we prove that, for short\ncontext lengths, adding depth allows the target to be approximated by low-rank\nattention. For long contexts, we conjecture that full-rank attention is\nnecessary. Finally, we present experiments with off-the-shelf transformers that\nvalidate our theoretical findings.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}