{"id":"2407.19215","title":"Learning Sparse Parity with Noise in Linear Samples","authors":"Xue Chen, Wenxuan Shu, Zhaienhe Zhou","authorsParsed":[["Chen","Xue",""],["Shu","Wenxuan",""],["Zhou","Zhaienhe",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 08:57:04 GMT"}],"updateDate":"2024-07-30","timestamp":1722070624000,"abstract":"  We revisit the learning parity with noise problem with a sparse secret that\ninvolves at most $k$ out of $n$ variables. Let $\\eta$ denote the noise rate\nsuch that each label gets flipped with probability $\\eta$. In this work, we\nshow algorithms in the low-noise setting and high-noise setting separately. We\npresent an algorithm of running time $O(\\eta \\cdot n/k)^k$ for any $\\eta$ and\n$k$ satisfying $n>k/\\eta$. This improves the state-of-the-art for learning\nsparse parity in a wide range of parameters like $k\\le n^{0.99}$ and $\\eta <\n\\sqrt{k/n}$, where the best known algorithm had running time at least\n${\\binom{n}{k/2}} \\ge (n/k)^{k/2}$ . Different from previous approaches based\non generating biased samples , our new idea is to combine subset sampling and\nGaussian elimination. The resulting algorithm just needs $O(k/\\eta + k \\log\n\\frac{n}{k})$ samples and is structurally simpler than previous algorithms. In\nthe high-noise setting, we present an improvement on Valiant's classical\nalgorithm using $n^{\\frac{\\omega+o(1)}{3}\\cdot k}$ time (with the matrix\nmultiplication constant $\\omega$) and $\\tilde{O}(k^2)$ samples. For any\n$\\eta<1/2$, our algorithm has time complexity\n$(n/k)^{\\frac{\\omega+o(1)}{3}\\cdot k}$ and sample complexity $\\tilde{O}(k)$.\nHence it improves Valiant's algorithm in terms of both time complexity and\nsample complexity and generalizes Valiant's framework to give the\nstate-of-the-art bound for any $k \\le n^{0.99}$ and $\\eta \\in (0.4,0.5)$.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}