{"id":"2407.19625","title":"LoginMEA: Local-to-Global Interaction Network for Multi-modal Entity\n  Alignment","authors":"Taoyu Su, Xinghua Zhang, Jiawei Sheng, Zhenyu Zhang and Tingwen Liu","authorsParsed":[["Su","Taoyu",""],["Zhang","Xinghua",""],["Sheng","Jiawei",""],["Zhang","Zhenyu",""],["Liu","Tingwen",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 01:06:45 GMT"}],"updateDate":"2024-07-30","timestamp":1722215205000,"abstract":"  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs (MMKGs), whose entities can be\nassociated with relational triples and related images. Most previous studies\ntreat the graph structure as a special modality, and fuse different modality\ninformation with separate uni-modal encoders, neglecting valuable relational\nassociations in modalities. Other studies refine each uni-modal information\nwith graph structures, but may introduce unnecessary relations in specific\nmodalities. To this end, we propose a novel local-to-global interaction network\nfor MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal\ninteractions to generate holistic entity semantics and then refine them with\nglobal relational interactions of entity neighbors. In this design, the\nuni-modal information is fused adaptively, and can be refined with relations\naccordingly. To enrich local interactions of multi-modal entity information, we\ndevice modality weights and low-rank interactive fusion, allowing diverse\nimpacts and element-level interactions among modalities. To capture global\ninteractions of graph structures, we adopt relation reflection graph attention\nnetworks, which fully capture relational associations between entities.\nExtensive experiments demonstrate superior results of our method over 5\ncross-KG or bilingual benchmark datasets, indicating the effectiveness of\ncapturing local and global interactions.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}