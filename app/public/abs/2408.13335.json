{"id":"2408.13335","title":"Latent Space Disentanglement in Diffusion Transformers Enables Zero-shot\n  Fine-grained Semantic Editing","authors":"Zitao Shuai, Chenwei Wu, Zhengxu Tang, Bowen Song, Liyue Shen","authorsParsed":[["Shuai","Zitao",""],["Wu","Chenwei",""],["Tang","Zhengxu",""],["Song","Bowen",""],["Shen","Liyue",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 19:00:52 GMT"}],"updateDate":"2024-08-27","timestamp":1724439652000,"abstract":"  Diffusion Transformers (DiTs) have achieved remarkable success in diverse and\nhigh-quality text-to-image(T2I) generation. However, how text and image latents\nindividually and jointly contribute to the semantics of generated images,\nremain largely unexplored. Through our investigation of DiT's latent space, we\nhave uncovered key findings that unlock the potential for zero-shot\nfine-grained semantic editing: (1) Both the text and image spaces in DiTs are\ninherently decomposable. (2) These spaces collectively form a disentangled\nsemantic representation space, enabling precise and fine-grained semantic\ncontrol. (3) Effective image editing requires the combined use of both text and\nimage latent spaces. Leveraging these insights, we propose a simple and\neffective Extract-Manipulate-Sample (EMS) framework for zero-shot fine-grained\nimage editing. Our approach first utilizes a multi-modal Large Language Model\nto convert input images and editing targets into text descriptions. We then\nlinearly manipulate text embeddings based on the desired editing degree and\nemploy constrained score distillation sampling to manipulate image embeddings.\nWe quantify the disentanglement degree of the latent space of diffusion models\nby proposing a new metric. To evaluate fine-grained editing performance, we\nintroduce a comprehensive benchmark incorporating both human annotations,\nmanual evaluation, and automatic metrics. We have conducted extensive\nexperimental results and in-depth analysis to thoroughly uncover the semantic\ndisentanglement properties of the diffusion transformer, as well as the\neffectiveness of our proposed method. Our annotated benchmark dataset is\npublicly available at https://anonymous.com/anonymous/EMS-Benchmark,\nfacilitating reproducible research in this domain.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"yBcaIrkMAbWrp4a-1EGamCJm2_g1O1ViWMe_I3gxfmk","pdfSize":"49427578"}
