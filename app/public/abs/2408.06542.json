{"id":"2408.06542","title":"Value of Information and Reward Specification in Active Inference and\n  POMDPs","authors":"Ran Wei","authorsParsed":[["Wei","Ran",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 00:32:05 GMT"}],"updateDate":"2024-08-14","timestamp":1723509125000,"abstract":"  Expected free energy (EFE) is a central quantity in active inference which\nhas recently gained popularity due to its intuitive decomposition of the\nexpected value of control into a pragmatic and an epistemic component. While\nnumerous conjectures have been made to justify EFE as a decision making\nobjective function, the most widely accepted is still its intuitiveness and\nresemblance to variational free energy in approximate Bayesian inference. In\nthis work, we take a bottom up approach and ask: taking EFE as given, what's\nthe resulting agent's optimality gap compared with a reward-driven\nreinforcement learning (RL) agent, which is well understood? By casting EFE\nunder a particular class of belief MDP and using analysis tools from RL theory,\nwe show that EFE approximates the Bayes optimal RL policy via information\nvalue. We discuss the implications for objective specification of active\ninference agents.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}