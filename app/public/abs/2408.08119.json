{"id":"2408.08119","title":"The Unreasonable Effectiveness of Solving Inverse Problems with Neural\n  Networks","authors":"Philipp Holl, Nils Thuerey","authorsParsed":[["Holl","Philipp",""],["Thuerey","Nils",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 12:38:10 GMT"}],"updateDate":"2024-08-16","timestamp":1723725490000,"abstract":"  Finding model parameters from data is an essential task in science and\nengineering, from weather and climate forecasts to plasma control. Previous\nworks have employed neural networks to greatly accelerate finding solutions to\ninverse problems. Of particular interest are end-to-end models which utilize\ndifferentiable simulations in order to backpropagate feedback from the\nsimulated process to the network weights and enable roll-out of multiple time\nsteps. So far, it has been assumed that, while model inference is faster than\nclassical optimization, this comes at the cost of a decrease in solution\naccuracy. We show that this is generally not true. In fact, neural networks\ntrained to learn solutions to inverse problems can find better solutions than\nclassical optimizers even on their training set. To demonstrate this, we\nperform both a theoretical analysis as well an extensive empirical evaluation\non challenging problems involving local minima, chaos, and zero-gradient\nregions. Our findings suggest an alternative use for neural networks: rather\nthan generalizing to new data for fast inference, they can also be used to find\nbetter solutions on known data.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}