{"id":"2408.03669","title":"Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep\n  Graph Neural Networks","authors":"Jie Peng, Runlin Lei, Zhewei Wei","authorsParsed":[["Peng","Jie",""],["Lei","Runlin",""],["Wei","Zhewei",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 10:24:59 GMT"}],"updateDate":"2024-08-08","timestamp":1723026299000,"abstract":"  The drastic performance degradation of Graph Neural Networks (GNNs) as the\ndepth of the graph propagation layers exceeds 8-10 is widely attributed to a\nphenomenon of Over-smoothing. Although recent research suggests that\nOver-smoothing may not be the dominant reason for such a performance\ndegradation, they have not provided rigorous analysis from a theoretical view,\nwhich warrants further investigation. In this paper, we systematically analyze\nthe real dominant problem in deep GNNs and identify the issues that these GNNs\ntowards addressing Over-smoothing essentially work on via empirical experiments\nand theoretical gradient analysis. We theoretically prove that the difficult\ntraining problem of deep MLPs is actually the main challenge, and various\nexisting methods that supposedly tackle Over-smoothing actually improve the\ntrainability of MLPs, which is the main reason for their performance gains. Our\nfurther investigation into trainability issues reveals that properly\nconstrained smaller upper bounds of gradient flow notably enhance the\ntrainability of GNNs. Experimental results on diverse datasets demonstrate\nconsistency between our theoretical findings and empirical evidence. Our\nanalysis provides new insights in constructing deep graph models.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}