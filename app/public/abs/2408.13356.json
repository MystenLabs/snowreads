{"id":"2408.13356","title":"Network-Offloaded Bandwidth-Optimal Broadcast and Allgather for\n  Distributed AI","authors":"Mikhail Khalilov, Salvatore Di Girolamo, Marcin Chrapek, Rami\n  Nudelman, Gil Bloch, Torsten Hoefler","authorsParsed":[["Khalilov","Mikhail",""],["Di Girolamo","Salvatore",""],["Chrapek","Marcin",""],["Nudelman","Rami",""],["Bloch","Gil",""],["Hoefler","Torsten",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 20:04:13 GMT"}],"updateDate":"2024-08-27","timestamp":1724443453000,"abstract":"  In the Fully Sharded Data Parallel (FSDP) training pipeline, collective\noperations can be interleaved to maximize the communication/computation\noverlap. In this scenario, outstanding operations such as Allgather and\nReduce-Scatter can compete for the injection bandwidth and create pipeline\nbubbles. To address this problem, we propose a novel bandwidth-optimal\nAllgather collective algorithm that leverages hardware multicast. We use\nmulticast to build a constant-time reliable Broadcast protocol, a building\nblock for constructing an optimal Allgather schedule. Our Allgather algorithm\nachieves 2x traffic reduction on a 188-node testbed. To free the host side from\nrunning the protocol, we employ SmartNIC offloading. We extract the parallelism\nin our Allgather algorithm and map it to a SmartNIC specialized for hiding the\ncost of data movement. We show that our SmartNIC-offloaded collective progress\nengine can scale to the next generation of 1.6 Tbit/s links.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}