{"id":"2407.01019","title":"An Abstract Lyapunov Control Optimizer: Local Stabilization and Global\n  Convergence","authors":"Bilel Bensaid (IMB, CEA CESTA), Ga\\\"el Po\\\"ette (CEA CESTA), Rodolphe\n  Turpault (IMB)","authorsParsed":[["Bensaid","Bilel","","IMB, CEA CESTA"],["Poëtte","Gaël","","CEA CESTA"],["Turpault","Rodolphe","","IMB"]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 07:11:09 GMT"}],"updateDate":"2024-07-02","timestamp":1719817869000,"abstract":"  Recently, many machine learning optimizers have been analysed considering\nthem as the asymptotic limit of some differential equations when the step size\ngoes to zero. In other words, the optimizers can be seen as a finite difference\nscheme applied to a continuous dynamical system. But the major part of the\nresults in the literature concerns constant step size algorithms. The main aim\nof this paper is to investigate the guarantees of the adaptive step size\ncounterpart. In fact, this dynamical point of view can be used to design step\nsize update rules, by choosing a discretization of the continuous equation that\npreserves its most relevant features. In this work, we analyse this kind of\nadaptive optimizers and prove their Lyapunov stability and convergence\nproperties for any choice of hyperparameters. At the best of our knowledge,\nthis paper introduces for the first time the use of continuous selection theory\nfrom general topology to overcome some of the intrinsic difficulties due to the\nnon constant and non regular step size policies. The general framework\ndeveloped gives many new results on adaptive and constant step size\nMomentum/Heavy-Ball and p-GD algorithms.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}