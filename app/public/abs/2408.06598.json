{"id":"2408.06598","title":"A Perspective on Large Language Models, Intelligent Machines, and\n  Knowledge Acquisition","authors":"Vladimir Cherkassky and Eng Hock Lee","authorsParsed":[["Cherkassky","Vladimir",""],["Lee","Eng Hock",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 03:25:49 GMT"}],"updateDate":"2024-08-14","timestamp":1723519549000,"abstract":"  Large Language Models (LLMs) are known for their remarkable ability to\ngenerate synthesized 'knowledge', such as text documents, music, images, etc.\nHowever, there is a huge gap between LLM's and human capabilities for\nunderstanding abstract concepts and reasoning. We discuss these issues in a\nlarger philosophical context of human knowledge acquisition and the Turing\ntest. In addition, we illustrate the limitations of LLMs by analyzing GPT-4\nresponses to questions ranging from science and math to common sense reasoning.\nThese examples show that GPT-4 can often imitate human reasoning, even though\nit lacks understanding. However, LLM responses are synthesized from a large LLM\nmodel trained on all available data. In contrast, human understanding is based\non a small number of abstract concepts. Based on this distinction, we discuss\nthe impact of LLMs on acquisition of human knowledge and education.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"M1I6v8MCd8w4dO9LOsUBKOTmbEbdVHtX3TeWXVfuLBs","pdfSize":"466242"}
