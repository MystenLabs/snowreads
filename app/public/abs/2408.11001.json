{"id":"2408.11001","title":"MegaFusion: Extend Diffusion Models towards Higher-resolution Image\n  Generation without Further Tuning","authors":"Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, Yanfeng\n  Wang","authorsParsed":[["Wu","Haoning",""],["Shen","Shaocheng",""],["Hu","Qiang",""],["Zhang","Xiaoyun",""],["Zhang","Ya",""],["Wang","Yanfeng",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 16:53:34 GMT"},{"version":"v2","created":"Sat, 7 Sep 2024 13:08:04 GMT"}],"updateDate":"2024-09-10","timestamp":1724172814000,"abstract":"  Diffusion models have emerged as frontrunners in text-to-image generation,\nhowever, their fixed image resolution during training often leads to challenges\nin high-resolution image generation, such as semantic deviations and object\nreplication. This paper introduces MegaFusion, a novel approach that extends\nexisting diffusion-based text-to-image generation models towards efficient\nhigher-resolution generation without additional fine-tuning or extra\nadaptation. Specifically, we employ an innovative truncate and relay strategy\nto bridge the denoising processes across different resolutions, allowing for\nhigh-resolution image generation in a coarse-to-fine manner. Moreover, by\nintegrating dilated convolutions and noise re-scheduling, we further adapt the\nmodel's priors for higher resolution. The versatility and efficacy of\nMegaFusion make it universally applicable to both latent-space and pixel-space\ndiffusion models, along with other derivative models. Extensive experiments\nconfirm that MegaFusion significantly boosts the capability of existing models\nto produce images of megapixels and various aspect ratios, while only requiring\nabout 40% of the original computational cost.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}