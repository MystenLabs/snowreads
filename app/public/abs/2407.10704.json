{"id":"2407.10704","title":"Quantized Prompt for Efficient Generalization of Vision-Language Models","authors":"Tianxiang Hao, Xiaohan Ding, Juexiao Feng, Yuhong Yang, Hui Chen and\n  Guiguang Ding","authorsParsed":[["Hao","Tianxiang",""],["Ding","Xiaohan",""],["Feng","Juexiao",""],["Yang","Yuhong",""],["Chen","Hui",""],["Ding","Guiguang",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 13:19:56 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 22:52:27 GMT"}],"updateDate":"2024-07-23","timestamp":1721049596000,"abstract":"  In the past few years, large-scale pre-trained vision-language models like\nCLIP have achieved tremendous success in various fields. Naturally, how to\ntransfer the rich knowledge in such huge pre-trained models to downstream tasks\nand datasets becomes a hot topic. During downstream adaptation, the most\nchallenging problems are overfitting and catastrophic forgetting, which can\ncause the model to overly focus on the current data and lose more crucial\ndomain-general knowledge. Existing works use classic regularization techniques\nto solve the problems. As solutions become increasingly complex, the\never-growing storage and inference costs are also a significant problem that\nurgently needs to be addressed. While in this paper, we start from an\nobservation that proper random noise can suppress overfitting and catastrophic\nforgetting. Then we regard quantization error as a kind of noise, and explore\nquantization for regularizing vision-language model, which is quite efficiency\nand effective. Furthermore, to improve the model's generalization capability\nwhile maintaining its specialization capacity at minimal cost, we deeply\nanalyze the characteristics of the weight distribution in prompts, conclude\nseveral principles for quantization module design and follow such principles to\ncreate several competitive baselines. The proposed method is significantly\nefficient due to its inherent lightweight nature, making it possible to adapt\non extremely resource-limited devices. Our method can be fruitfully integrated\ninto many existing approaches like MaPLe, enhancing accuracy while reducing\nstorage overhead, making it more powerful yet versatile. Extensive experiments\non 11 datasets shows great superiority of our method sufficiently. Code is\navailable at https://github.com/beyondhtx/QPrompt.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}