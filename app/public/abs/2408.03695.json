{"id":"2408.03695","title":"Openstory++: A Large-scale Dataset and Benchmark for Instance-aware\n  Open-domain Visual Storytelling","authors":"Zilyu Ye, Jinxiu Liu, Ruotian Peng, Jinjin Cao, Zhiyang Chen, Yiyang\n  Zhang, Ziwei Xuan, Mingyuan Zhou, Xiaoqian Shen, Mohamed Elhoseiny, Qi Liu,\n  Guo-Jun Qi","authorsParsed":[["Ye","Zilyu",""],["Liu","Jinxiu",""],["Peng","Ruotian",""],["Cao","Jinjin",""],["Chen","Zhiyang",""],["Zhang","Yiyang",""],["Xuan","Ziwei",""],["Zhou","Mingyuan",""],["Shen","Xiaoqian",""],["Elhoseiny","Mohamed",""],["Liu","Qi",""],["Qi","Guo-Jun",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 11:20:37 GMT"}],"updateDate":"2024-08-08","timestamp":1723029637000,"abstract":"  Recent image generation models excel at creating high-quality images from\nbrief captions. However, they fail to maintain consistency of multiple\ninstances across images when encountering lengthy contexts. This inconsistency\nis largely due to in existing training datasets the absence of granular\ninstance feature labeling in existing training datasets. To tackle these\nissues, we introduce Openstory++, a large-scale dataset combining additional\ninstance-level annotations with both images and text. Furthermore, we develop a\ntraining methodology that emphasizes entity-centric image-text generation,\nensuring that the models learn to effectively interweave visual and textual\ninformation. Specifically, Openstory++ streamlines the process of keyframe\nextraction from open-domain videos, employing vision-language models to\ngenerate captions that are then polished by a large language model for\nnarrative continuity. It surpasses previous datasets by offering a more\nexpansive open-domain resource, which incorporates automated captioning,\nhigh-resolution imagery tailored for instance count, and extensive frame\nsequences for temporal consistency. Additionally, we present Cohere-Bench, a\npioneering benchmark framework for evaluating the image generation tasks when\nlong multimodal context is provided, including the ability to keep the\nbackground, style, instances in the given context coherent. Compared to\nexisting benchmarks, our work fills critical gaps in multi-modal generation,\npropelling the development of models that can adeptly generate and interpret\ncomplex narratives in open-domain environments. Experiments conducted within\nCohere-Bench confirm the superiority of Openstory++ in nurturing high-quality\nvisual storytelling models, enhancing their ability to address open-domain\ngeneration tasks. More details can be found at https://openstorypp.github.io/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}