{"id":"2408.02032","title":"Self-Introspective Decoding: Alleviating Hallucinations for Large\n  Vision-Language Models","authors":"Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen,\n  Peilin Zhao","authorsParsed":[["Huo","Fushuo",""],["Xu","Wenchao",""],["Zhang","Zhong",""],["Wang","Haozhao",""],["Chen","Zhicheng",""],["Zhao","Peilin",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 13:50:17 GMT"}],"updateDate":"2024-08-06","timestamp":1722779417000,"abstract":"  While Large Vision-Language Models (LVLMs) have rapidly advanced in recent\nyears, the prevalent issue known as the `hallucination' problem has emerged as\na significant bottleneck, hindering their real-world deployments. Existing\nmethods mitigate this issue mainly from two perspectives: One approach\nleverages extra knowledge like robust instruction tuning LVLMs with curated\ndatasets or employing auxiliary analysis networks, which inevitable incur\nadditional costs. Another approach, known as contrastive decoding, induces\nhallucinations by manually disturbing the vision or instruction raw inputs and\nmitigates them by contrasting the outputs of the disturbed and original LVLMs.\nHowever, these approaches rely on empirical holistic input disturbances and\ndouble the inference cost. To avoid these issues, we propose a simple yet\neffective method named Self-Introspective Decoding (SID). Our empirical\ninvestigation reveals that pretrained LVLMs can introspectively assess the\nimportance of vision tokens based on preceding vision and text (both\ninstruction and generated) tokens. We develop the Context and Text-aware Token\nSelection (CT2S) strategy, which preserves only unimportant vision tokens after\nearly layers of LVLMs to adaptively amplify text-informed hallucination during\nthe auto-regressive decoding. This approach ensures that multimodal knowledge\nabsorbed in the early layers induces multimodal contextual rather than aimless\nhallucinations. Subsequently, the original token logits subtract the amplified\nvision-and-text association hallucinations, guiding LVLMs decoding faithfully.\nExtensive experiments illustrate SID generates less-hallucination and\nhigher-quality texts across various metrics, without extra knowledge and much\nadditional computation burdens.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7Al4J4wtt7eifDOK5JkR0KF-6-zG8vZRx17zPMMBL7U","pdfSize":"11272361"}
