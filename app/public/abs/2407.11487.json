{"id":"2407.11487","title":"PRET: Planning with Directed Fidelity Trajectory for Vision and Language\n  Navigation","authors":"Renjie Lu, Jingke Meng, Wei-Shi Zheng","authorsParsed":[["Lu","Renjie",""],["Meng","Jingke",""],["Zheng","Wei-Shi",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 08:22:18 GMT"}],"updateDate":"2024-07-17","timestamp":1721118138000,"abstract":"  Vision and language navigation is a task that requires an agent to navigate\naccording to a natural language instruction. Recent methods predict sub-goals\non constructed topology map at each step to enable long-term action planning.\nHowever, they suffer from high computational cost when attempting to support\nsuch high-level predictions with GCN-like models. In this work, we propose an\nalternative method that facilitates navigation planning by considering the\nalignment between instructions and directed fidelity trajectories, which refers\nto a path from the initial node to the candidate locations on a directed graph\nwithout detours. This planning strategy leads to an efficient model while\nachieving strong performance. Specifically, we introduce a directed graph to\nillustrate the explored area of the environment, emphasizing directionality.\nThen, we firstly define the trajectory representation as a sequence of directed\nedge features, which are extracted from the panorama based on the corresponding\norientation. Ultimately, we assess and compare the alignment between\ninstruction and different trajectories during navigation to determine the next\nnavigation target. Our method outperforms previous SOTA method BEVBert on RxR\ndataset and is comparable on R2R dataset while largely reducing the\ncomputational cost. Code is available:\nhttps://github.com/iSEE-Laboratory/VLN-PRET.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}