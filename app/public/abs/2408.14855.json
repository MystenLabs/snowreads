{"id":"2408.14855","title":"Enhancing Analogical Reasoning in the Abstraction and Reasoning Corpus\n  via Model-Based RL","authors":"Jihwan Lee, Woochang Sim, Sejin Kim, Sundong Kim","authorsParsed":[["Lee","Jihwan",""],["Sim","Woochang",""],["Kim","Sejin",""],["Kim","Sundong",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 08:15:20 GMT"}],"updateDate":"2024-08-28","timestamp":1724746520000,"abstract":"  This paper demonstrates that model-based reinforcement learning (model-based\nRL) is a suitable approach for the task of analogical reasoning. We hypothesize\nthat model-based RL can solve analogical reasoning tasks more efficiently\nthrough the creation of internal models. To test this, we compared DreamerV3, a\nmodel-based RL method, with Proximal Policy Optimization, a model-free RL\nmethod, on the Abstraction and Reasoning Corpus (ARC) tasks. Our results\nindicate that model-based RL not only outperforms model-free RL in learning and\ngeneralizing from single tasks but also shows significant advantages in\nreasoning across similar tasks.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Logic in Computer Science"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zHcDk0dS5t8J6nKfy-x55gdI8Dse6ir98or4ThHfeFQ","pdfSize":"3435624"}
