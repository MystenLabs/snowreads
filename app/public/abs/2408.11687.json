{"id":"2408.11687","title":"Interpretable Long-term Action Quality Assessment","authors":"Xu Dong, Xinran Liu, Wanqing Li, Anthony Adeyemi-Ejeye, Andrew Gilbert","authorsParsed":[["Dong","Xu",""],["Liu","Xinran",""],["Li","Wanqing",""],["Adeyemi-Ejeye","Anthony",""],["Gilbert","Andrew",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 15:09:09 GMT"}],"updateDate":"2024-08-22","timestamp":1724252949000,"abstract":"  Long-term Action Quality Assessment (AQA) evaluates the execution of\nactivities in videos. However, the length presents challenges in fine-grained\ninterpretability, with current AQA methods typically producing a single score\nby averaging clip features, lacking detailed semantic meanings of individual\nclips. Long-term videos pose additional difficulty due to the complexity and\ndiversity of actions, exacerbating interpretability challenges. While\nquery-based transformer networks offer promising long-term modeling\ncapabilities, their interpretability in AQA remains unsatisfactory due to a\nphenomenon we term Temporal Skipping, where the model skips self-attention\nlayers to prevent output degradation. To address this, we propose an attention\nloss function and a query initialization method to enhance performance and\ninterpretability. Additionally, we introduce a weight-score regression module\ndesigned to approximate the scoring patterns observed in human judgments and\nreplace conventional single-score regression, improving the rationality of\ninterpretability. Our approach achieves state-of-the-art results on three\nreal-world, long-term AQA benchmarks. Our code is available at:\nhttps://github.com/dx199771/Interpretability-AQA\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"mA_qwpaYQnXqLtVoo8S-aVsYUMIKLqPMojrLjNT19sg","pdfSize":"11442292"}
