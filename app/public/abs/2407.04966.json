{"id":"2407.04966","title":"A Layer-Anchoring Strategy for Enhancing Cross-Lingual Speech Emotion\n  Recognition","authors":"Shreya G. Upadhyay, Carlos Busso, and Chi-Chun Lee","authorsParsed":[["Upadhyay","Shreya G.",""],["Busso","Carlos",""],["Lee","Chi-Chun",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 05:56:55 GMT"}],"updateDate":"2024-07-09","timestamp":1720245415000,"abstract":"  Cross-lingual speech emotion recognition (SER) is important for a wide range\nof everyday applications. While recent SER research relies heavily on large\npretrained models for emotion training, existing studies often concentrate\nsolely on the final transformer layer of these models. However, given the\ntask-specific nature and hierarchical architecture of these models, each\ntransformer layer encapsulates different levels of information. Leveraging this\nhierarchical structure, our study focuses on the information embedded across\ndifferent layers. Through an examination of layer feature similarity across\ndifferent languages, we propose a novel strategy called a layer-anchoring\nmechanism to facilitate emotion transfer in cross-lingual SER tasks. Our\napproach is evaluated using two distinct language affective corpora\n(MSP-Podcast and BIIC-Podcast), achieving a best UAR performance of 60.21% on\nthe BIIC-podcast corpus. The analysis uncovers interesting insights into the\nbehavior of popular pretrained models.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}