{"id":"2408.02600","title":"BioMamba: A Pre-trained Biomedical Language Representation Model\n  Leveraging Mamba","authors":"Ling Yue, Sixue Xing, Yingzhou Lu, Tianfan Fu","authorsParsed":[["Yue","Ling",""],["Xing","Sixue",""],["Lu","Yingzhou",""],["Fu","Tianfan",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 16:21:36 GMT"}],"updateDate":"2024-08-06","timestamp":1722874896000,"abstract":"  The advancement of natural language processing (NLP) in biology hinges on\nmodels' ability to interpret intricate biomedical literature. Traditional\nmodels often struggle with the complex and domain-specific language in this\nfield. In this paper, we present BioMamba, a pre-trained model specifically\ndesigned for biomedical text mining. BioMamba builds upon the Mamba\narchitecture and is pre-trained on an extensive corpus of biomedical\nliterature. Our empirical studies demonstrate that BioMamba significantly\noutperforms models like BioBERT and general-domain Mamba across various\nbiomedical tasks. For instance, BioMamba achieves a 100 times reduction in\nperplexity and a 4 times reduction in cross-entropy loss on the BioASQ test\nset. We provide an overview of the model architecture, pre-training process,\nand fine-tuning techniques. Additionally, we release the code and trained model\nto facilitate further research.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}