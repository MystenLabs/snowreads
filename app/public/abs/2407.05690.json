{"id":"2407.05690","title":"Pruning Large Language Models to Intra-module Low-rank Architecture with\n  Transitional Activations","authors":"Bowen Shen, Zheng Lin, Daren Zha, Wei Liu, Jian Luan, Bin Wang,\n  Weiping Wang","authorsParsed":[["Shen","Bowen",""],["Lin","Zheng",""],["Zha","Daren",""],["Liu","Wei",""],["Luan","Jian",""],["Wang","Bin",""],["Wang","Weiping",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 07:45:38 GMT"}],"updateDate":"2024-07-09","timestamp":1720424738000,"abstract":"  Structured pruning fundamentally reduces computational and memory overheads\nof large language models (LLMs) and offers a feasible solution for end-side LLM\ndeployment. Structurally pruned models remain dense and high-precision, highly\ncompatible with further tuning and compression. However, as the coarse-grained\nstructured pruning poses large damage to the highly interconnected model,\nachieving a high compression ratio for scaled-up LLMs remains a challenge. In\nthis paper, we introduce a task-agnostic structured pruning approach coupled\nwith a compact Transformer architecture design. The proposed approach, named\nTransAct, reduces transitional activations inside multi-head attention (MHA)\nand multi-layer perceptron (MLP) modules, while preserving the inter-module\nactivations that are sensitive to perturbations. Hence, the LLM is pruned into\nan intra-module low-rank architecture, significantly reducing weights, KV Cache\nand attention computation. TransAct is implemented on the LLaMA model and\nevaluated on downstream benchmarks. Results verify the optimality of our\napproach at high compression with respect to both efficiency and performance.\nFurther, ablation studies reveal the strength of activation-guided iterative\npruning and provide experimental analysis on the redundancy of MHA and MLP\nmodules.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}