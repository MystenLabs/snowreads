{"id":"2408.02290","title":"Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen\n  Languages","authors":"Carlos Mullov, Ngoc-Quan Pham, Alexander Waibel","authorsParsed":[["Mullov","Carlos",""],["Pham","Ngoc-Quan",""],["Waibel","Alexander",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 07:58:58 GMT"}],"updateDate":"2024-08-06","timestamp":1722844738000,"abstract":"  Multilingual neural machine translation systems learn to map sentences of\ndifferent languages into a common representation space. Intuitively, with a\ngrowing number of seen languages the encoder sentence representation grows more\nflexible and easily adaptable to new languages. In this work, we test this\nhypothesis by zero-shot translating from unseen languages. To deal with unknown\nvocabularies from unknown languages we propose a setup where we decouple\nlearning of vocabulary and syntax, i.e. for each language we learn word\nrepresentations in a separate step (using cross-lingual word embeddings), and\nthen train to translate while keeping those word representations frozen. We\ndemonstrate that this setup enables zero-shot translation from entirely unseen\nlanguages. Zero-shot translating with a model trained on Germanic and Romance\nlanguages we achieve scores of 42.6 BLEU for Portuguese-English and 20.7 BLEU\nfor Russian-English on TED domain. We explore how this zero-shot translation\ncapability develops with varying number of languages seen by the encoder.\nLastly, we explore the effectiveness of our decoupled learning strategy for\nunsupervised machine translation. By exploiting our model's zero-shot\ntranslation capability for iterative back-translation we attain near parity\nwith a supervised setting.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"4VeATcMtC8Yivz4gj0QWX4gj0yOoviAR46-QrqTPfIs","pdfSize":"584586","txDigest":"s4WuzciH1cFmLgLf8shHQa8e4sRqL5znUaoZUYxW6Fc","endEpoch":"1","status":"CERTIFIED"}
