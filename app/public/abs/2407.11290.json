{"id":"2407.11290","title":"Distributed memory parallel adaptive tensor-train cross approximation","authors":"Tianyi Shi, Daniel Hayes, Jing-Mei Qiu","authorsParsed":[["Shi","Tianyi",""],["Hayes","Daniel",""],["Qiu","Jing-Mei",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 00:36:42 GMT"}],"updateDate":"2024-07-17","timestamp":1721090202000,"abstract":"  The tensor-train (TT) format is a data-sparse tensor representation commonly\nused in high dimensional function approximations arising from computational and\ndata sciences. Various sequential and parallel TT decomposition algorithms have\nbeen proposed for different tensor inputs and assumptions. In this paper, we\npropose subtensor parallel adaptive TT cross, which partitions a tensor onto\ndistributed memory machines with multidimensional process grids, and constructs\nan TT approximation iteratively with tensor elements. We derive two iterative\nformulations for pivot selection and TT core construction under the distributed\nmemory setting, conduct communication and scaling analysis of the algorithm,\nand illustrate its performance with multiple test experiments. These include up\nto 6D Hilbert tensors and tensors constructed from Maxwellian distribution\nfunctions that arise in kinetic theory. Our results demonstrate significant\naccuracy with greatly reduced storage requirements via the TT cross\napproximation. Furthermore, we demonstrate good to optimal strong and weak\nscaling performance for the proposed parallel algorithm.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}