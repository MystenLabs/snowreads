{"id":"2407.06807","title":"A Hybrid Training-time and Run-time Defense Against Adversarial Attacks\n  in Modulation Classification","authors":"Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Guisheng Liao, Ambra\n  Demontis, and Fabio Roli","authorsParsed":[["Zhang","Lu",""],["Lambotharan","Sangarapillai",""],["Zheng","Gan",""],["Liao","Guisheng",""],["Demontis","Ambra",""],["Roli","Fabio",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 12:28:38 GMT"}],"updateDate":"2024-07-10","timestamp":1720528118000,"abstract":"  Motivated by the superior performance of deep learning in many applications\nincluding computer vision and natural language processing, several recent\nstudies have focused on applying deep neural network for devising future\ngenerations of wireless networks. However, several recent works have pointed\nout that imperceptible and carefully designed adversarial examples (attacks)\ncan significantly deteriorate the classification accuracy. In this paper, we\ninvestigate a defense mechanism based on both training-time and run-time\ndefense techniques for protecting machine learning-based radio signal\n(modulation) classification against adversarial attacks. The training-time\ndefense consists of adversarial training and label smoothing, while the\nrun-time defense employs a support vector machine-based neural rejection (NR).\nConsidering a white-box scenario and real datasets, we demonstrate that our\nproposed techniques outperform existing state-of-the-art technologies.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}