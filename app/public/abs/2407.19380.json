{"id":"2407.19380","title":"Empowering Clinicians with Medical Decision Transformers: A Framework\n  for Sepsis Treatment","authors":"Aamer Abdul Rahman, Pranav Agarwal, Rita Noumeir, Philippe Jouvet,\n  Vincent Michalski, Samira Ebrahimi Kahou","authorsParsed":[["Rahman","Aamer Abdul",""],["Agarwal","Pranav",""],["Noumeir","Rita",""],["Jouvet","Philippe",""],["Michalski","Vincent",""],["Kahou","Samira Ebrahimi",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 03:40:00 GMT"}],"updateDate":"2024-07-30","timestamp":1722138000000,"abstract":"  Offline reinforcement learning has shown promise for solving tasks in\nsafety-critical settings, such as clinical decision support. Its application,\nhowever, has been limited by the lack of interpretability and interactivity for\nclinicians. To address these challenges, we propose the medical decision\ntransformer (MeDT), a novel and versatile framework based on the\ngoal-conditioned reinforcement learning paradigm for sepsis treatment\nrecommendation. MeDT uses the decision transformer architecture to learn a\npolicy for drug dosage recommendation. During offline training, MeDT utilizes\ncollected treatment trajectories to predict administered treatments for each\ntime step, incorporating known treatment outcomes, target acuity scores, past\ntreatment decisions, and current and past medical states. This analysis enables\nMeDT to capture complex dependencies among a patient's medical history,\ntreatment decisions, outcomes, and short-term effects on stability. Our\nproposed conditioning uses acuity scores to address sparse reward issues and to\nfacilitate clinician-model interactions, enhancing decision-making. Following\ntraining, MeDT can generate tailored treatment recommendations by conditioning\non the desired positive outcome (survival) and user-specified short-term\nstability improvements. We carry out rigorous experiments on data from the\nMIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT\nrecommends interventions that outperform or are competitive with existing\noffline reinforcement learning methods while enabling a more interpretable,\npersonalized and clinician-directed approach.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Bk2EZLvbp0U1DK9hiXl3euLmuVj8v_dJFNkpe7_j7Jo","pdfSize":"3620058","objectId":"0x9be4f59d32fc3c2306ec03dd4b2a5a977708d266cfb7bc1d5aa97eef53616d75","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
