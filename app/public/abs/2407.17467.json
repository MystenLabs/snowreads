{"id":"2407.17467","title":"CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models","authors":"Jiawei Gu, Zacc Yang, Chuanghao Ding, Rui Zhao, Fei Tan","authorsParsed":[["Gu","Jiawei",""],["Yang","Zacc",""],["Ding","Chuanghao",""],["Zhao","Rui",""],["Tan","Fei",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:59:02 GMT"}],"updateDate":"2024-07-25","timestamp":1721843942000,"abstract":"  Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Therefore, if we value the balance between efficiency and\neffectiveness, CMR can be consider as the optimal mixture ratio.Through\nextensive experiments, we ascertain the predictability of CMR, and propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}