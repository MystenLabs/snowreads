{"id":"2408.16345","title":"The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization","authors":"Luka Borec, Philipp Sadler, David Schlangen","authorsParsed":[["Borec","Luka",""],["Sadler","Philipp",""],["Schlangen","David",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 08:30:33 GMT"}],"updateDate":"2024-08-30","timestamp":1724920233000,"abstract":"  This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0owSwPvVaD92ZqifwTCAVvqICZ11hYZNvvoO6Ar17Jw","pdfSize":"1827725"}
