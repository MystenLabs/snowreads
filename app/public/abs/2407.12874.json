{"id":"2407.12874","title":"SELF-GUIDE: Better Task-Specific Instruction Following via\n  Self-Synthetic Finetuning","authors":"Chenyang Zhao, Xueying Jia, Vijay Viswanathan, Tongshuang Wu, Graham\n  Neubig","authorsParsed":[["Zhao","Chenyang",""],["Jia","Xueying",""],["Viswanathan","Vijay",""],["Wu","Tongshuang",""],["Neubig","Graham",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 04:41:58 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 00:38:22 GMT"}],"updateDate":"2024-08-13","timestamp":1721104918000,"abstract":"  Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NXvN10dq9bYuErkvxNyW88B00Ic52tFnVJsZrNnY41M","pdfSize":"790293","objectId":"0xf5bc23ab1c48bac8e2783117c8f0ba8234ef6e60a202cb30f21b5d3a7f254f46","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
