{"id":"2407.14206","title":"Watermark Smoothing Attacks against Language Models","authors":"Hongyan Chang, Hamed Hassani, Reza Shokri","authorsParsed":[["Chang","Hongyan",""],["Hassani","Hamed",""],["Shokri","Reza",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 11:04:54 GMT"}],"updateDate":"2024-07-22","timestamp":1721387094000,"abstract":"  Watermarking is a technique used to embed a hidden signal in the probability\ndistribution of text generated by large language models (LLMs), enabling\nattribution of the text to the originating model. We introduce smoothing\nattacks and show that existing watermarking methods are not robust against\nminor modifications of text. An adversary can use weaker language models to\nsmooth out the distribution perturbations caused by watermarks without\nsignificantly compromising the quality of the generated text. The modified text\nresulting from the smoothing attack remains close to the distribution of text\nthat the original model (without watermark) would have produced. Our attack\nreveals a fundamental limitation of a wide range of watermarking techniques.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}