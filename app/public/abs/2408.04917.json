{"id":"2408.04917","title":"Avoid Wasted Annotation Costs in Open-set Active Learning with\n  Pre-trained Vision-Language Model","authors":"Jaehyuk Heo, Pilsung Kang","authorsParsed":[["Heo","Jaehyuk",""],["Kang","Pilsung",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 07:54:57 GMT"}],"updateDate":"2024-08-12","timestamp":1723190097000,"abstract":"  Active learning (AL) aims to enhance model performance by selectively\ncollecting highly informative data, thereby minimizing annotation costs.\nHowever, in practical scenarios, unlabeled data may contain out-of-distribution\n(OOD) samples, leading to wasted annotation costs if data is incorrectly\nselected. Recent research has explored methods to apply AL to open-set data,\nbut these methods often require or incur unavoidable cost losses to minimize\nthem. To address these challenges, we propose a novel selection strategy, CLIPN\nfor AL (CLIPNAL), which minimizes cost losses without requiring OOD samples.\nCLIPNAL sequentially evaluates the purity and informativeness of data. First,\nit utilizes a pre-trained vision-language model to detect and exclude OOD data\nby leveraging linguistic and visual information of in-distribution (ID) data\nwithout additional training. Second, it selects highly informative data from\nthe remaining ID data, and then the selected samples are annotated by human\nexperts. Experimental results on datasets with various open-set conditions\ndemonstrate that CLIPNAL achieves the lowest cost loss and highest performance\nacross all scenarios. Code is available at https://github.com/DSBA-Lab/OpenAL.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"VemjAx4P8bdROfQSlse-UunpUF4ISyl2Il_UAjr8w3o","pdfSize":"18896211"}
