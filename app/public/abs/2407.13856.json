{"id":"2407.13856","title":"Simultaneous Localization and Affordance Prediction for Tasks in\n  Egocentric Video","authors":"Zachary Chavis, Hyun Soo Park, Stephen J. Guy","authorsParsed":[["Chavis","Zachary",""],["Park","Hyun Soo",""],["Guy","Stephen J.",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 18:55:56 GMT"}],"updateDate":"2024-07-22","timestamp":1721328956000,"abstract":"  Vision-Language Models (VLMs) have shown great success as foundational models\nfor downstream vision and natural language applications in a variety of\ndomains. However, these models lack the spatial understanding necessary for\nrobotics applications where the agent must reason about the affordances\nprovided by the 3D world around them. We present a system which trains on\nspatially-localized egocentric videos in order to connect visual input and task\ndescriptions to predict a task's spatial affordance, that is the location where\na person would go to accomplish the task. We show our approach outperforms the\nbaseline of using a VLM to map similarity of a task's description over a set of\nlocation-tagged images. Our learning-based approach has less error both on\npredicting where a task may take place and on predicting what tasks are likely\nto happen at the current location. The resulting system enables robots to use\negocentric sensing to navigate to physical locations of novel tasks specified\nin natural language.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}