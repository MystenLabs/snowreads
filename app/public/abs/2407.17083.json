{"id":"2407.17083","title":"When Text and Images Don't Mix: Bias-Correcting Language-Image\n  Similarity Scores for Anomaly Detection","authors":"Adam Goodge, Bryan Hooi, Wee Siong Ng","authorsParsed":[["Goodge","Adam",""],["Hooi","Bryan",""],["Ng","Wee Siong",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 08:20:02 GMT"}],"updateDate":"2024-07-25","timestamp":1721809202000,"abstract":"  Contrastive Language-Image Pre-training (CLIP) achieves remarkable\nperformance in various downstream tasks through the alignment of image and text\ninput embeddings and holds great promise for anomaly detection. However, our\nempirical experiments show that the embeddings of text inputs unexpectedly\ntightly cluster together, far away from image embeddings, contrary to the\nmodel's contrastive training objective to align image-text input pairs. We show\nthat this phenomenon induces a `similarity bias' - in which false negative and\nfalse positive errors occur due to bias in the similarities between images and\nthe normal label text embeddings. To address this bias, we propose a novel\nmethodology called BLISS which directly accounts for this similarity bias\nthrough the use of an auxiliary, external set of text inputs. BLISS is simple,\nit does not require strong inductive biases about anomalous behaviour nor an\nexpensive training process, and it significantly outperforms baseline methods\non benchmark image datasets, even when access to normal data is extremely\nlimited.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}