{"id":"2408.07088","title":"Learning Rule-Induced Subgraph Representations for Inductive Relation\n  Prediction","authors":"Tianyu Liu, Qitan Lv, Jie Wang, Shuling Yang, Hanzhu Chen","authorsParsed":[["Liu","Tianyu",""],["Lv","Qitan",""],["Wang","Jie",""],["Yang","Shuling",""],["Chen","Hanzhu",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 02:27:46 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 06:33:40 GMT"}],"updateDate":"2024-08-21","timestamp":1723170466000,"abstract":"  Inductive relation prediction (IRP) -- where entities can be different during\ntraining and inference -- has shown great power for completing evolving\nknowledge graphs. Existing works mainly focus on using graph neural networks\n(GNNs) to learn the representation of the subgraph induced from the target\nlink, which can be seen as an implicit rule-mining process to measure the\nplausibility of the target link. However, these methods cannot differentiate\nthe target link and other links during message passing, hence the final\nsubgraph representation will contain irrelevant rule information to the target\nlink, which reduces the reasoning performance and severely hinders the\napplications for real-world scenarios. To tackle this problem, we propose a\nnovel \\textit{single-source edge-wise} GNN model to learn the\n\\textbf{R}ule-induc\\textbf{E}d \\textbf{S}ubgraph represen\\textbf{T}ations\n(\\textbf{REST}), which encodes relevant rules and eliminates irrelevant rules\nwithin the subgraph. Specifically, we propose a \\textit{single-source}\ninitialization approach to initialize edge features only for the target link,\nwhich guarantees the relevance of mined rules and target link. Then we propose\nseveral RNN-based functions for \\textit{edge-wise} message passing to model the\nsequential property of mined rules. REST is a simple and effective approach\nwith theoretical support to learn the \\textit{rule-induced subgraph\nrepresentation}. Moreover, REST does not need node labeling, which\nsignificantly accelerates the subgraph preprocessing time by up to\n\\textbf{11.66$\\times$}. Experiments on inductive relation prediction benchmarks\ndemonstrate the effectiveness of our REST. Our code is available at\nhttps://github.com/smart-lty/REST.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}