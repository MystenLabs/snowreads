{"id":"2408.17054","title":"BTMuda: A Bi-level Multi-source unsupervised domain adaptation framework\n  for breast cancer diagnosis","authors":"Yuxiang Yang, Xinyi Zeng, Pinxian Zeng, Binyu Yan, Xi Wu, Jiliu Zhou,\n  Yan Wang","authorsParsed":[["Yang","Yuxiang",""],["Zeng","Xinyi",""],["Zeng","Pinxian",""],["Yan","Binyu",""],["Wu","Xi",""],["Zhou","Jiliu",""],["Wang","Yan",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 07:25:53 GMT"}],"updateDate":"2024-09-02","timestamp":1725002753000,"abstract":"  Deep learning has revolutionized the early detection of breast cancer,\nresulting in a significant decrease in mortality rates. However, difficulties\nin obtaining annotations and huge variations in distribution between training\nsets and real scenes have limited their clinical applications. To address these\nlimitations, unsupervised domain adaptation (UDA) methods have been used to\ntransfer knowledge from one labeled source domain to the unlabeled target\ndomain, yet these approaches suffer from severe domain shift issues and often\nignore the potential benefits of leveraging multiple relevant sources in\npractical applications. To address these limitations, in this work, we\nconstruct a Three-Branch Mixed extractor and propose a Bi-level Multi-source\nunsupervised domain adaptation method called BTMuda for breast cancer\ndiagnosis. Our method addresses the problems of domain shift by dividing domain\nshift issues into two levels: intra-domain and inter-domain. To reduce the\nintra-domain shift, we jointly train a CNN and a Transformer as two paths of a\ndomain mixed feature extractor to obtain robust representations rich in both\nlow-level local and high-level global information. As for the inter-domain\nshift, we redesign the Transformer delicately to a three-branch architecture\nwith cross-attention and distillation, which learns domain-invariant\nrepresentations from multiple domains. Besides, we introduce two alignment\nmodules - one for feature alignment and one for classifier alignment - to\nimprove the alignment process. Extensive experiments conducted on three public\nmammographic datasets demonstrate that our BTMuda outperforms state-of-the-art\nmethods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}