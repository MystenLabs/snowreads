{"id":"2407.07874","title":"Toto: Time Series Optimized Transformer for Observability","authors":"Ben Cohen, Emaad Khwaja, Kan Wang, Charles Masson, Elise Ram\\'e,\n  Youssef Doubli, Othmane Abou-Amal","authorsParsed":[["Cohen","Ben",""],["Khwaja","Emaad",""],["Wang","Kan",""],["Masson","Charles",""],["Ram√©","Elise",""],["Doubli","Youssef",""],["Abou-Amal","Othmane",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 17:40:30 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 16:18:40 GMT"}],"updateDate":"2024-07-12","timestamp":1720633230000,"abstract":"  This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"bI63lcVXevETJqH5PruFrAri5YA2A7IvnDvwGbjPaa0","pdfSize":"465302"}
