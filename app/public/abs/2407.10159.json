{"id":"2407.10159","title":"RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D\n  LiDAR Segmentation","authors":"Li Li, Hubert P. H. Shum, Toby P. Breckon","authorsParsed":[["Li","Li",""],["Shum","Hubert P. H.",""],["Breckon","Toby P.",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 10:59:34 GMT"},{"version":"v2","created":"Sun, 25 Aug 2024 17:59:22 GMT"},{"version":"v3","created":"Fri, 13 Sep 2024 19:24:17 GMT"}],"updateDate":"2024-09-17","timestamp":1720954774000,"abstract":"  3D point clouds play a pivotal role in outdoor scene perception, especially\nin the context of autonomous driving. Recent advancements in 3D LiDAR\nsegmentation often focus intensely on the spatial positioning and distribution\nof points for accurate segmentation. However, these methods, while robust in\nvariable conditions, encounter challenges due to sole reliance on coordinates\nand point intensity, leading to poor isometric invariance and suboptimal\nsegmentation. To tackle this challenge, our work introduces Range-Aware\nPointwise Distance Distribution (RAPiD) features and the associated RAPiD-Seg\narchitecture. Our RAPiD features exhibit rigid transformation invariance and\neffectively adapt to variations in point density, with a design focus on\ncapturing the localized geometry of neighboring structures. They utilize\ninherent LiDAR isotropic radiation and semantic categorization for enhanced\nlocal representation and computational efficiency, while incorporating a 4D\ndistance metric that integrates geometric and surface material reflectivity for\nimproved semantic segmentation. To effectively embed high-dimensional RAPiD\nfeatures, we propose a double-nested autoencoder structure with a novel\nclass-aware embedding objective to encode high-dimensional features into\nmanageable voxel-wise embeddings. Additionally, we propose RAPiD-Seg which\nincorporates a channel-wise attention fusion and two effective RAPiD-Seg\nvariants, further optimizing the embedding for enhanced performance and\ngeneralization. Our method outperforms contemporary LiDAR segmentation work in\nterms of mIoU on SemanticKITTI (76.1) and nuScenes (83.6) datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"6queutei-oS9rUIpTs809RD4HIZOwFdaGtodWDoraFQ","pdfSize":"3827593"}
