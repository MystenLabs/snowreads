{"id":"2408.02301","title":"Network Fission Ensembles for Low-Cost Self-Ensembles","authors":"Hojung Lee, Jong-Seok Lee","authorsParsed":[["Lee","Hojung",""],["Lee","Jong-Seok",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 08:23:59 GMT"}],"updateDate":"2024-08-06","timestamp":1722846239000,"abstract":"  Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}