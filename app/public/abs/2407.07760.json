{"id":"2407.07760","title":"Learning Spatial-Semantic Features for Robust Video Object Segmentation","authors":"Xin Li, Deshui Miao, Zhenyu He, Yaowei Wang, Huchuan Lu, Ming-Hsuan\n  Yang","authorsParsed":[["Li","Xin",""],["Miao","Deshui",""],["He","Zhenyu",""],["Wang","Yaowei",""],["Lu","Huchuan",""],["Yang","Ming-Hsuan",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 15:36:00 GMT"}],"updateDate":"2024-07-11","timestamp":1720625760000,"abstract":"  Tracking and segmenting multiple similar objects with complex or separate\nparts in long-term videos is inherently challenging due to the ambiguity of\ntarget parts and identity confusion caused by occlusion, background clutter,\nand long-term variations. In this paper, we propose a robust video object\nsegmentation framework equipped with spatial-semantic features and\ndiscriminative object queries to address the above issues. Specifically, we\nconstruct a spatial-semantic network comprising a semantic embedding block and\nspatial dependencies modeling block to associate the pretrained ViT features\nwith global semantic features and local spatial features, providing a\ncomprehensive target representation. In addition, we develop a masked\ncross-attention module to generate object queries that focus on the most\ndiscriminative parts of target objects during query propagation, alleviating\nnoise accumulation and ensuring effective long-term query propagation. The\nexperimental results show that the proposed method set a new state-of-the-art\nperformance on multiple datasets, including the DAVIS2017 test (89.1%),\nYoutubeVOS 2019 (88.5%), MOSE (75.1%), LVOS test (73.0%), and LVOS val (75.1%),\nwhich demonstrate the effectiveness and generalization capacity of the proposed\nmethod. We will make all source code and trained models publicly available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}