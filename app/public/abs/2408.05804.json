{"id":"2408.05804","title":"A Single Goal is All You Need: Skills and Exploration Emerge from\n  Contrastive RL without Rewards, Demonstrations, or Subgoals","authors":"Grace Liu, Michael Tang, Benjamin Eysenbach","authorsParsed":[["Liu","Grace",""],["Tang","Michael",""],["Eysenbach","Benjamin",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 15:49:00 GMT"}],"updateDate":"2024-08-13","timestamp":1723391340000,"abstract":"  In this paper, we present empirical evidence of skills and directed\nexploration emerging from a simple RL algorithm long before any successful\ntrials are observed. For example, in a manipulation task, the agent is given a\nsingle observation of the goal state and learns skills, first for moving its\nend-effector, then for pushing the block, and finally for picking up and\nplacing the block. These skills emerge before the agent has ever successfully\nplaced the block at the goal location and without the aid of any reward\nfunctions, demonstrations, or manually-specified distance metrics. Once the\nagent has learned to reach the goal state reliably, exploration is reduced.\nImplementing our method involves a simple modification of prior work and does\nnot require density estimates, ensembles, or any additional hyperparameters.\nIntuitively, the proposed method seems like it should be terrible at\nexploration, and we lack a clear theoretical understanding of why it works so\neffectively, though our experiments provide some hints.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FwWZzei75E_nwKUu8iCplJ-pgV0tCOFJXHS94cPiXDc","pdfSize":"7709361"}
