{"id":"2408.13291","title":"Growing Deep Neural Network Considering with Similarity between Neurons","authors":"Taigo Sakai, Kazuhiro Hotta","authorsParsed":[["Sakai","Taigo",""],["Hotta","Kazuhiro",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 11:16:37 GMT"}],"updateDate":"2024-08-27","timestamp":1724411797000,"abstract":"  Deep learning has excelled in image recognition tasks through neural networks\ninspired by the human brain. However, the necessity for large models to improve\nprediction accuracy introduces significant computational demands and extended\ntraining times.Conventional methods such as fine-tuning, knowledge\ndistillation, and pruning have the limitations like potential accuracy drops.\nDrawing inspiration from human neurogenesis, where neuron formation continues\ninto adulthood, we explore a novel approach of progressively increasing neuron\nnumbers in compact models during training phases, thereby managing\ncomputational costs effectively. We propose a method that reduces feature\nextraction biases and neuronal redundancy by introducing constraints based on\nneuron similarity distributions. This approach not only fosters efficient\nlearning in new neurons but also enhances feature extraction relevancy for\ngiven tasks. Results on CIFAR-10 and CIFAR-100 datasets demonstrated accuracy\nimprovement, and our method pays more attention to whole object to be\nclassified in comparison with conventional method through Grad-CAM\nvisualizations. These results suggest that our method's potential to\ndecision-making processes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"v26uxvXZpVHoTcrOoZ0aMu1PMUaTk_SeMExWzk1aBXk","pdfSize":"2739397"}
