{"id":"2407.17226","title":"Sublinear Regret for An Actor-Critic Algorithm in Continuous-Time\n  Linear-Quadratic Reinforcement Learning","authors":"Yilie Huang, Yanwei Jia, Xun Yu Zhou","authorsParsed":[["Huang","Yilie",""],["Jia","Yanwei",""],["Zhou","Xun Yu",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 12:26:21 GMT"}],"updateDate":"2024-07-25","timestamp":1721823981000,"abstract":"  We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions where volatility of the\nstate processes depends on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an actor-critic algorithm to learn the optimal\npolicy parameter directly. Our main contributions include the introduction of a\nnovel exploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor. We conduct a simulation study to validate the theoretical\nresults and demonstrate the effectiveness and reliability of the proposed\nalgorithm. We also perform numerical comparisons between our method and those\nof the recent model-based stochastic LQ RL studies adapted to the state- and\ncontrol-dependent volatility setting, demonstrating a better performance of the\nformer in terms of regret bounds.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}