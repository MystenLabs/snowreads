{"id":"2408.14086","title":"ReLExS: Reinforcement Learning Explanations for Stackelberg No-Regret\n  Learners","authors":"Xiangge Huang, Jingyuan Li, Jiaqing Xie","authorsParsed":[["Huang","Xiangge",""],["Li","Jingyuan",""],["Xie","Jiaqing",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 08:12:26 GMT"}],"updateDate":"2024-08-27","timestamp":1724659946000,"abstract":"  With the constraint of a no regret follower, will the players in a two-player\nStackelberg game still reach Stackelberg equilibrium? We first show when the\nfollower strategy is either reward-average or transform-reward-average, the two\nplayers can always get the Stackelberg Equilibrium. Then, we extend that the\nplayers can achieve the Stackelberg equilibrium in the two-player game under\nthe no regret constraint. Also, we show a strict upper bound of the follower's\nutility difference between with and without no regret constraint. Moreover, in\nconstant-sum two-player Stackelberg games with non-regret action sequences, we\nensure the total optimal utility of the game remains also bounded.\n","subjects":["Computing Research Repository/Computer Science and Game Theory","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}