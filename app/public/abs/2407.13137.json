{"id":"2407.13137","title":"OE-BevSeg: An Object Informed and Environment Aware Multimodal Framework\n  for Bird's-eye-view Vehicle Semantic Segmentation","authors":"Jian Sun, Yuqi Dai, Chi-Man Vong, Qing Xu, Shengbo Eben Li, Jianqiang\n  Wang, Lei He, Keqiang Li","authorsParsed":[["Sun","Jian",""],["Dai","Yuqi",""],["Vong","Chi-Man",""],["Xu","Qing",""],["Li","Shengbo Eben",""],["Wang","Jianqiang",""],["He","Lei",""],["Li","Keqiang",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 03:48:22 GMT"}],"updateDate":"2024-07-19","timestamp":1721274502000,"abstract":"  Bird's-eye-view (BEV) semantic segmentation is becoming crucial in autonomous\ndriving systems. It realizes ego-vehicle surrounding environment perception by\nprojecting 2D multi-view images into 3D world space. Recently, BEV segmentation\nhas made notable progress, attributed to better view transformation modules,\nlarger image encoders, or more temporal information. However, there are still\ntwo issues: 1) a lack of effective understanding and enhancement of BEV space\nfeatures, particularly in accurately capturing long-distance environmental\nfeatures and 2) recognizing fine details of target objects. To address these\nissues, we propose OE-BevSeg, an end-to-end multimodal framework that enhances\nBEV segmentation performance through global environment-aware perception and\nlocal target object enhancement. OE-BevSeg employs an environment-aware BEV\ncompressor. Based on prior knowledge about the main composition of the BEV\nsurrounding environment varying with the increase of distance intervals,\nlong-sequence global modeling is utilized to improve the model's understanding\nand perception of the environment. From the perspective of enriching target\nobject information in segmentation results, we introduce the center-informed\nobject enhancement module, using centerness information to supervise and guide\nthe segmentation head, thereby enhancing segmentation performance from a local\nenhancement perspective. Additionally, we designed a multimodal fusion branch\nthat integrates multi-view RGB image features with radar/LiDAR features,\nachieving significant performance improvements. Extensive experiments show\nthat, whether in camera-only or multimodal fusion BEV segmentation tasks, our\napproach achieves state-of-the-art results by a large margin on the nuScenes\ndataset for vehicle segmentation, demonstrating superior applicability in the\nfield of autonomous driving.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}