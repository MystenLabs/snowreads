{"id":"2407.06204","title":"A Survey on Mixture of Experts","authors":"Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, Jiayi\n  Huang","authorsParsed":[["Cai","Weilin",""],["Jiang","Juyong",""],["Wang","Fan",""],["Tang","Jing",""],["Kim","Sunghun",""],["Huang","Jiayi",""]],"versions":[{"version":"v1","created":"Wed, 26 Jun 2024 16:34:33 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 07:13:37 GMT"}],"updateDate":"2024-08-09","timestamp":1719419673000,"abstract":"  Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge developments in MoE\nresearch, we have established a resource repository accessible at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}