{"id":"2408.06303","title":"Long-Form Answers to Visual Questions from Blind and Low Vision People","authors":"Mina Huh, Fangyuan Xu, Yi-Hao Peng, Chongyan Chen, Hansika Murugu,\n  Danna Gurari, Eunsol Choi, Amy Pavel","authorsParsed":[["Huh","Mina",""],["Xu","Fangyuan",""],["Peng","Yi-Hao",""],["Chen","Chongyan",""],["Murugu","Hansika",""],["Gurari","Danna",""],["Choi","Eunsol",""],["Pavel","Amy",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 17:15:02 GMT"}],"updateDate":"2024-08-13","timestamp":1723482902000,"abstract":"  Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"B1iJ_lRpRQCIconGPyDi1_Ow_L58KG-uPcCYZs2aR38","pdfSize":"34769792"}
