{"id":"2408.03238","title":"LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for\n  Accurate Robotic Grasping Under the Occlusion","authors":"Jinyu Zhang, Yongchong Gu, Jianxiong Gao, Haitao Lin, Qiang Sun,\n  Xinwei Sun, Xiangyang Xue, Yanwei Fu","authorsParsed":[["Zhang","Jinyu",""],["Gu","Yongchong",""],["Gao","Jianxiong",""],["Lin","Haitao",""],["Sun","Qiang",""],["Sun","Xinwei",""],["Xue","Xiangyang",""],["Fu","Yanwei",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 14:50:48 GMT"}],"updateDate":"2024-08-07","timestamp":1722955848000,"abstract":"  This paper addresses the challenge of perceiving complete object shapes\nthrough visual perception. While prior studies have demonstrated encouraging\noutcomes in segmenting the visible parts of objects within a scene, amodal\nsegmentation, in particular, has the potential to allow robots to infer the\noccluded parts of objects. To this end, this paper introduces a new framework\nthat explores amodal segmentation for robotic grasping in cluttered scenes,\nthus greatly enhancing robotic grasping abilities. Initially, we use a\nconventional segmentation algorithm to detect the visible segments of the\ntarget object, which provides shape priors for completing the full object mask.\nParticularly, to explore how to utilize semantic features from RGB images and\ngeometric information from depth images, we propose a Linear-fusion\nAttention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the\nlinear-fusion strategy to effectively fuse this cross-modal data, and then uses\nthe prior visible mask as attention map to guide the network to focus on target\nfeature locations for further complete mask recovery. Using the amodal mask of\nthe target object provides advantages in selecting more accurate and robust\ngrasp points compared to relying solely on the visible segments. The results on\ndifferent datasets show that our method achieves state-of-the-art performance.\nFurthermore, the robot experiments validate the feasibility and robustness of\nthis method in the real world. Our code and demonstrations are available on the\nproject page: https://jrryzh.github.io/LAC-Net.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}