{"id":"2408.17181","title":"Improving Extraction of Clinical Event Contextual Properties from\n  Electronic Health Records: A Comparative Study","authors":"Shubham Agarwal, Thomas Searle, Mart Ratas, Anthony Shek, James Teo,\n  Richard Dobson","authorsParsed":[["Agarwal","Shubham",""],["Searle","Thomas",""],["Ratas","Mart",""],["Shek","Anthony",""],["Teo","James",""],["Dobson","Richard",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 10:28:49 GMT"}],"updateDate":"2024-09-02","timestamp":1725013729000,"abstract":"  Electronic Health Records are large repositories of valuable clinical data,\nwith a significant portion stored in unstructured text format. This textual\ndata includes clinical events (e.g., disorders, symptoms, findings, medications\nand procedures) in context that if extracted accurately at scale can unlock\nvaluable downstream applications such as disease prediction. Using an existing\nNamed Entity Recognition and Linking methodology, MedCAT, these identified\nconcepts need to be further classified (contextualised) for their relevance to\nthe patient, and their temporal and negated status for example, to be useful\ndownstream. This study performs a comparative analysis of various natural\nlanguage models for medical text classification. Extensive experimentation\nreveals the effectiveness of transformer-based language models, particularly\nBERT. When combined with class imbalance mitigation techniques, BERT\noutperforms Bi-LSTM models by up to 28% and the baseline BERT model by up to\n16% for recall of the minority classes. The method has been implemented as part\nof CogStack/MedCAT framework and made available to the community for further\nresearch.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}