{"id":"2407.00744","title":"Disentangled Representations for Causal Cognition","authors":"Filippo Torresan, Manuel Baltieri","authorsParsed":[["Torresan","Filippo",""],["Baltieri","Manuel",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 16:10:17 GMT"}],"updateDate":"2024-07-02","timestamp":1719763817000,"abstract":"  Complex adaptive agents consistently achieve their goals by solving problems\nthat seem to require an understanding of causal information, information\npertaining to the causal relationships that exist among elements of combined\nagent-environment systems. Causal cognition studies and describes the main\ncharacteristics of causal learning and reasoning in human and non-human\nanimals, offering a conceptual framework to discuss cognitive performances\nbased on the level of apparent causal understanding of a task. Despite the use\nof formal intervention-based models of causality, including causal Bayesian\nnetworks, psychological and behavioural research on causal cognition does not\nyet offer a computational account that operationalises how agents acquire a\ncausal understanding of the world. Machine and reinforcement learning research\non causality, especially involving disentanglement as a candidate process to\nbuild causal representations, represent on the one hand a concrete attempt at\ndesigning causal artificial agents that can shed light on the inner workings of\nnatural causal cognition. In this work, we connect these two areas of research\nto build a unifying framework for causal cognition that will offer a\ncomputational perspective on studies of animal cognition, and provide insights\nin the development of new algorithms for causal reinforcement learning in AI.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Quantitative Biology/Neurons and Cognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}