{"id":"2407.11654","title":"R-SFLLM: Jamming Resilient Framework for Split Federated Learning with\n  Large Language Models","authors":"Aladin Djuhera, Vlad C. Andrei, Xinyang Li, Ullrich J. M\\\"onich,\n  Holger Boche, Walid Saad","authorsParsed":[["Djuhera","Aladin",""],["Andrei","Vlad C.",""],["Li","Xinyang",""],["MÃ¶nich","Ullrich J.",""],["Boche","Holger",""],["Saad","Walid",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 12:21:29 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 12:36:29 GMT"}],"updateDate":"2024-09-10","timestamp":1721132489000,"abstract":"  Split federated learning (SFL) is a compute-efficient paradigm in distributed\nmachine learning (ML), where components of large ML models are outsourced to\nremote servers. A significant challenge in SFL, particularly when deployed over\nwireless channels, is the susceptibility of transmitted model parameters to\nadversarial jamming that could jeopardize the learning process. This is\nparticularly pronounced for word embedding parameters in large language models\n(LLMs), which are crucial for language understanding. In this paper, rigorous\ninsights are provided into the influence of jamming LLM word embeddings in SFL\nby deriving an expression for the ML training loss divergence and showing that\nit is upper-bounded by the mean squared error (MSE). Based on this analysis, a\nphysical layer framework is developed for resilient SFL with LLMs (R-SFLLM)\nover wireless networks. R-SFLLM leverages wireless sensing data to gather\ninformation on the jamming directions-of-arrival (DoAs) for the purpose of\ndevising a novel, sensing-assisted anti-jamming strategy while jointly\noptimizing beamforming, user scheduling, and resource allocation. Extensive\nexperiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,\nachieving close-to-baseline performance across various natural language\nprocessing (NLP) tasks and datasets. The proposed methodology further\nintroduces an adversarial training component, where controlled noise exposure\nsignificantly enhances the LLM's resilience to perturbed parameters during\ntraining. The results show that more noise-sensitive models, such as RoBERTa,\nbenefit from this feature, especially when resource allocation is unfair. It is\nalso shown that worst-case jamming in particular translates into worst-case\nmodel outcomes, thereby necessitating the need for jamming-resilient SFL\nprotocols.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}