{"id":"2408.15966","title":"More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding","authors":"Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long\n  Hu, Min Chen","authorsParsed":[["Tang","Yuan",""],["Han","Xu",""],["Li","Xianzhi",""],["Yu","Qiao",""],["Xu","Jinfeng",""],["Hao","Yixue",""],["Hu","Long",""],["Chen","Min",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 17:38:44 GMT"},{"version":"v2","created":"Thu, 5 Sep 2024 06:33:31 GMT"}],"updateDate":"2024-09-06","timestamp":1724866724000,"abstract":"  Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}