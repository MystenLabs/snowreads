{"id":"2408.14009","title":"Optimizing TD3 for 7-DOF Robotic Arm Grasping: Overcoming Suboptimality\n  with Exploration-Enhanced Contrastive Learning","authors":"Wen-Han Hsieh and Jen-Yuan Chang","authorsParsed":[["Hsieh","Wen-Han",""],["Chang","Jen-Yuan",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 04:30:59 GMT"}],"updateDate":"2024-08-27","timestamp":1724646659000,"abstract":"  In actor-critic-based reinforcement learning algorithms such as Twin Delayed\nDeep Deterministic policy gradient (TD3), insufficient exploration of the\nspatial space can result in suboptimal policies when controlling 7-DOF robotic\narms. To address this issue, we propose a novel Exploration-Enhanced\nContrastive Learning (EECL) module that improves exploration by providing\nadditional rewards for encountering novel states. Our module stores previously\nexplored states in a buffer and identifies new states by comparing them with\nhistorical data using Euclidean distance within a K-dimensional tree (KDTree)\nframework. When the agent explores new states, exploration rewards are\nassigned. These rewards are then integrated into the TD3 algorithm, ensuring\nthat the Q-learning process incorporates these signals, promoting more\neffective strategy optimization. We evaluate our method on the robosuite panda\nlift task, demonstrating that it significantly outperforms the baseline TD3 in\nterms of both efficiency and convergence speed in the tested environment.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}