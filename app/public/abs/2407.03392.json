{"id":"2407.03392","title":"M5: A Whole Genome Bacterial Encoder at Single Nucleotide Resolution","authors":"Agust Egilsson","authorsParsed":[["Egilsson","Agust",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 15:30:44 GMT"}],"updateDate":"2024-07-08","timestamp":1720020644000,"abstract":"  A linear attention mechanism is described to extend the context length of an\nencoder only transformer, called M5 in this report, to a multi-million single\nnucleotide resolution foundation model pretrained on bacterial whole genomes.\nThe linear attention mechanism used approximates a full quadratic attention\nmechanism tightly and has a simple and lightweight implementation for the use\ncase when the key-query embedding dimensionality is low. The M5-small model is\nentirely trained and tested on one A100 GPU with 40gb of memory up to 196K\nnucleotides during training and 2M nucleotides during testing. We test the\nperformance of the M5-small model and record notable improvements in\nperformance as whole genome bacterial sequence lengths are increased as well as\ndemonstrating the stability of the full multi-head attention approximation used\nas sequence length is increased.\n","subjects":["Quantitative Biology/Quantitative Methods","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}