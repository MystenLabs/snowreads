{"id":"2408.14238","title":"Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy\n  Unleashes the Potential of Traditional Sequential Recommenders","authors":"Cong Xu, Zhangchi Zhu, Mo Yu, Jun Wang, Jianyong Wang, Wei Zhang","authorsParsed":[["Xu","Cong",""],["Zhu","Zhangchi",""],["Yu","Mo",""],["Wang","Jun",""],["Wang","Jianyong",""],["Zhang","Wei",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 12:52:02 GMT"}],"updateDate":"2024-08-27","timestamp":1724676722000,"abstract":"  Large language models (LLMs) have been garnering increasing attention in the\nrecommendation community. Some studies have observed that LLMs, when fine-tuned\nby the cross-entropy (CE) loss with a full softmax, could achieve\n`state-of-the-art' performance in sequential recommendation. However, most of\nthe baselines used for comparison are trained using a pointwise/pairwise loss\nfunction. This inconsistent experimental setting leads to the underestimation\nof traditional methods and further fosters over-confidence in the ranking\ncapability of LLMs.\n  In this study, we provide theoretical justification for the superiority of\nthe cross-entropy loss by demonstrating its two desirable properties: tightness\nand coverage. Furthermore, this study sheds light on additional novel insights:\n1) Taking into account only the recommendation performance, CE is not yet\noptimal as it is not a quite tight bound in terms of some ranking metrics. 2)\nIn scenarios that full softmax cannot be performed, an effective alternative is\nto scale up the sampled normalizing term. These findings then help unleash the\npotential of traditional recommendation models, allowing them to surpass\nLLM-based counterparts. Given the substantial computational burden, existing\nLLM-based methods are not as effective as claimed for sequential\nrecommendation. We hope that these theoretical understandings in conjunction\nwith the empirical results will facilitate an objective evaluation of LLM-based\nrecommendation in the future.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}