{"id":"2407.10930","title":"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together","authors":"Dilara Soylu, Christopher Potts, Omar Khattab","authorsParsed":[["Soylu","Dilara",""],["Potts","Christopher",""],["Khattab","Omar",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 17:30:31 GMT"}],"updateDate":"2024-07-16","timestamp":1721064631000,"abstract":"  Natural Language Processing (NLP) systems are increasingly taking the form of\nmulti-stage pipelines involving multiple distinct language models (LMs) and\nprompting strategies. Here we address the question of how to fine-tune such\nsystems to improve their performance. We cast this as a problem of optimizing\nthe underlying LM weights and the prompting strategies together, and consider a\nchallenging but highly realistic scenario in which we have no gold labels for\nany intermediate stages in the pipeline. To address this challenge, we evaluate\napproximate optimization strategies in which we bootstrap training labels for\nall pipeline stages and use these to optimize the pipeline's prompts and\nfine-tune its weights alternatingly. In experiments with multi-hop QA,\nmathematical reasoning, and feature-based classification, we find that simple\napproaches for optimizing the prompts and weights together outperform directly\noptimizing weights alone and prompts alone by up to 65% and 5%, respectively,\non average across LMs and tasks. We will release our new optimizers in DSPy at\nhttp://dspy.ai\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}