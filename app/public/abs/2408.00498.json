{"id":"2408.00498","title":"How Effective are Self-Supervised Models for Contact Identification in\n  Videos","authors":"Malitha Gunawardhana, Limalka Sadith, Liel David, Daniel Harari,\n  Muhammad Haris Khan","authorsParsed":[["Gunawardhana","Malitha",""],["Sadith","Limalka",""],["David","Liel",""],["Harari","Daniel",""],["Khan","Muhammad Haris",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 12:08:20 GMT"}],"updateDate":"2024-08-02","timestamp":1722514100000,"abstract":"  The exploration of video content via Self-Supervised Learning (SSL) models\nhas unveiled a dynamic field of study, emphasizing both the complex challenges\nand unique opportunities inherent in this area. Despite the growing body of\nresearch, the ability of SSL models to detect physical contacts in videos\nremains largely unexplored, particularly the effectiveness of methods such as\ndownstream supervision with linear probing or full fine-tuning. This work aims\nto bridge this gap by employing eight different convolutional neural networks\n(CNNs) based video SSL models to identify instances of physical contact within\nvideo sequences specifically. The Something-Something v2 (SSv2) and\nEpic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due\nto the promising results on UCF101 and HMDB51, coupled with their limited prior\nassessment on SSv2 and EK-100. Additionally, these datasets feature diverse\nenvironments and scenarios, essential for testing the robustness and accuracy\nof video-based models. This approach not only examines the effectiveness of\neach model in recognizing physical contacts but also explores the performance\nin the action recognition downstream task. By doing so, valuable insights into\nthe adaptability of SSL models in interpreting complex, dynamic visual\ninformation are contributed.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}