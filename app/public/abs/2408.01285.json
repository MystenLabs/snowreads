{"id":"2408.01285","title":"The Mismeasure of Man and Models: Evaluating Allocational Harms in Large\n  Language Models","authors":"Hannah Chen, Yangfeng Ji, David Evans","authorsParsed":[["Chen","Hannah",""],["Ji","Yangfeng",""],["Evans","David",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 14:13:06 GMT"}],"updateDate":"2024-08-05","timestamp":1722607986000,"abstract":"  Large language models (LLMs) are now being considered and even deployed for\napplications that support high-stakes decision-making, such as recruitment and\nclinical decisions. While several methods have been proposed for measuring\nbias, there remains a gap between predictions, which are what the proposed\nmethods consider, and how they are used to make decisions. In this work, we\nintroduce Rank-Allocational-Based Bias Index (RABBI), a model-agnostic bias\nmeasure that assesses potential allocational harms arising from biases in LLM\npredictions. We compare RABBI and current bias metrics on two allocation\ndecision tasks. We evaluate their predictive validity across ten LLMs and\nutility for model selection. Our results reveal that commonly-used bias metrics\nbased on average performance gap and distribution distance fail to reliably\ncapture group disparities in allocation outcomes, whereas RABBI exhibits a\nstrong correlation with allocation disparities. Our work highlights the need to\naccount for how models are used in contexts with limited resource constraints.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ih2Wn8osH2r9F6fz1-GGryC06OLQq79gcSB3TnYGE-A","pdfSize":"1242484","txDigest":"2dix9B2gjEP4GLDR3LkyfxCYfkB6dwuiyGznQPRJAW9p","endEpoch":"1","status":"CERTIFIED"}
