{"id":"2408.16380","title":"Exploiting temporal information to detect conversational groups in\n  videos and predict the next speaker","authors":"Lucrezia Tosato, Victor Fortier, Isabelle Bloch, Catherine Pelachaud","authorsParsed":[["Tosato","Lucrezia",""],["Fortier","Victor",""],["Bloch","Isabelle",""],["Pelachaud","Catherine",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 09:41:36 GMT"}],"updateDate":"2024-08-30","timestamp":1724924496000,"abstract":"  Studies in human human interaction have introduced the concept of F formation\nto describe the spatial arrangement of participants during social interactions.\nThis paper has two objectives. It aims at detecting F formations in video\nsequences and predicting the next speaker in a group conversation. The proposed\napproach exploits time information and human multimodal signals in video\nsequences. In particular, we rely on measuring the engagement level of people\nas a feature of group belonging. Our approach makes use of a recursive neural\nnetwork, the Long Short Term Memory (LSTM), to predict who will take the\nspeaker's turn in a conversation group. Experiments on the MatchNMingle dataset\nled to 85% true positives in group detection and 98% accuracy in predicting the\nnext speaker.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}