{"id":"2407.09024","title":"Aligning Diffusion Behaviors with Q-functions for Efficient Continuous\n  Control","authors":"Huayu Chen, Kaiwen Zheng, Hang Su, Jun Zhu","authorsParsed":[["Chen","Huayu",""],["Zheng","Kaiwen",""],["Su","Hang",""],["Zhu","Jun",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 06:32:36 GMT"}],"updateDate":"2024-07-15","timestamp":1720765956000,"abstract":"  Drawing upon recent advances in language model alignment, we formulate\noffline Reinforcement Learning as a two-stage optimization problem: First\npretraining expressive generative policies on reward-free behavior datasets,\nthen fine-tuning these policies to align with task-specific annotations like\nQ-values. This strategy allows us to leverage abundant and diverse behavior\ndata to enhance generalization and enable rapid adaptation to downstream tasks\nusing minimal annotations. In particular, we introduce Efficient Diffusion\nAlignment (EDA) for solving continuous control problems. EDA utilizes diffusion\nmodels for behavior modeling. However, unlike previous approaches, we represent\ndiffusion policies as the derivative of a scalar neural network with respect to\naction inputs. This representation is critical because it enables direct\ndensity calculation for diffusion models, making them compatible with existing\nLLM alignment theories. During policy fine-tuning, we extend preference-based\nalignment methods like Direct Preference Optimization (DPO) to align diffusion\nbehaviors with continuous Q-functions. Our evaluation on the D4RL benchmark\nshows that EDA exceeds all baseline methods in overall performance. Notably,\nEDA maintains about 95\\% of performance and still outperforms several baselines\ngiven only 1\\% of Q-labelled data during fine-tuning.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}