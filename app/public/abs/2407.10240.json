{"id":"2407.10240","title":"xLSTMTime : Long-term Time Series Forecasting With xLSTM","authors":"Musleh Alharthi and Ausif Mahmood","authorsParsed":[["Alharthi","Musleh",""],["Mahmood","Ausif",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 15:15:00 GMT"},{"version":"v2","created":"Sun, 21 Jul 2024 12:08:13 GMT"},{"version":"v3","created":"Mon, 12 Aug 2024 02:10:34 GMT"}],"updateDate":"2024-08-13","timestamp":1720970100000,"abstract":"  In recent years, transformer-based models have gained prominence in\nmultivariate long-term time series forecasting (LTSF), demonstrating\nsignificant advancements despite facing challenges such as high computational\ndemands, difficulty in capturing temporal dynamics, and managing long-term\ndependencies. The emergence of LTSF-Linear, with its straightforward linear\narchitecture, has notably outperformed transformer-based counterparts,\nprompting a reevaluation of the transformer's utility in time series\nforecasting. In response, this paper presents an adaptation of a recent\narchitecture termed extended LSTM (xLSTM) for LTSF. xLSTM incorporates\nexponential gating and a revised memory structure with higher capacity that has\ngood potential for LTSF. Our adopted architecture for LTSF termed as xLSTMTime\nsurpasses current approaches. We compare xLSTMTime's performance against\nvarious state-of-the-art models across multiple real-world da-tasets,\ndemonstrating superior forecasting capabilities. Our findings suggest that\nrefined recurrent architectures can offer competitive alternatives to\ntransformer-based models in LTSF tasks, po-tentially redefining the landscape\nof time series forecasting.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}