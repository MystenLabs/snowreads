{"id":"2408.03034","title":"A Course in Dynamic Optimization","authors":"Bar Light","authorsParsed":[["Light","Bar",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 08:29:31 GMT"}],"updateDate":"2024-08-07","timestamp":1722932971000,"abstract":"  These lecture notes are derived from a graduate-level course in dynamic\noptimization, offering an introduction to techniques and models extensively\nused in management science, economics, operations research, engineering, and\ncomputer science. The course emphasizes the theoretical underpinnings of\ndiscrete-time dynamic programming models and advanced algorithmic strategies\nfor solving these models. Unlike typical treatments, it provides a proof for\nthe principle of optimality for upper semi-continuous dynamic programming, a\nmiddle ground between the simpler countable state space case\n\\cite{bertsekas2012dynamic}, and the involved universally measurable case\n\\cite{bertsekas1996stochastic}. This approach is sufficiently rigorous to\ninclude important examples such as dynamic pricing, consumption-savings, and\ninventory management models. The course also delves into the properties of\nvalue and policy functions, leveraging classical results\n\\cite{topkis1998supermodularity} and recent developments. Additionally, it\noffers an introduction to reinforcement learning, including a formal proof of\nthe convergence of Q-learning algorithms. Furthermore, the notes delve into\npolicy gradient methods for the average reward case, presenting a convergence\nresult for the tabular case in this context. This result is simple and similar\nto the discounted case but appears to be new.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Systems and Control","Economics/Theoretical Economics","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}