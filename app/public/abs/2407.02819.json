{"id":"2407.02819","title":"Efficient Training of Language Models with Compact and Consistent Next\n  Token Distributions","authors":"Ashutosh Sathe, Sunita Sarawagi","authorsParsed":[["Sathe","Ashutosh",""],["Sarawagi","Sunita",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 05:40:41 GMT"}],"updateDate":"2024-07-04","timestamp":1719985241000,"abstract":"  Maximizing the likelihood of the next token is an established, statistically\nsound objective for pre-training language models. In this paper we show that we\ncan train better models faster by pre-aggregating the corpus with a collapsed\n$n$-gram distribution. Previous studies have proposed corpus-level $n$-gram\nstatistics as a regularizer; however, the construction and querying of such\n$n$-grams, if done naively, prove to be costly and significantly impede\ntraining speed, thereby limiting their application in modern large language\nmodel pre-training.\n  We introduce an alternative compact representation of the next token\ndistribution that, in expectation, aligns with the complete $n$-gram\ndistribution while markedly reducing variance across mini-batches compared to\nthe standard next-token loss. Empirically, we demonstrate that both the\n$n$-gram regularized model and our approximation yield substantial improvements\nin model quality and convergence rate compared to existing methods.\nFurthermore, our approximation facilitates scalability of gains to larger\ndatasets and models compared to the straightforward $n$-gram regularization\nmethod.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}