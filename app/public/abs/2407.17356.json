{"id":"2407.17356","title":"Gradient-based inference of abstract task representations for\n  generalization in neural networks","authors":"Ali Hummos, Felipe del R\\'io, Brabeeba Mien Wang, Julio Hurtado,\n  Cristian B. Calderon, Guangyu Robert Yang","authorsParsed":[["Hummos","Ali",""],["del RÃ­o","Felipe",""],["Wang","Brabeeba Mien",""],["Hurtado","Julio",""],["Calderon","Cristian B.",""],["Yang","Guangyu Robert",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 15:28:08 GMT"}],"updateDate":"2024-07-25","timestamp":1721834888000,"abstract":"  Humans and many animals show remarkably adaptive behavior and can respond\ndifferently to the same input depending on their internal goals. The brain not\nonly represents the intermediate abstractions needed to perform a computation\nbut also actively maintains a representation of the computation itself (task\nabstraction). Such separation of the computation and its abstraction is\nassociated with faster learning, flexible decision-making, and broad\ngeneralization capacity. We investigate if such benefits might extend to neural\nnetworks trained with task abstractions. For such benefits to emerge, one needs\na task inference mechanism that possesses two crucial abilities: First, the\nability to infer abstract task representations when no longer explicitly\nprovided (task inference), and second, manipulate task representations to adapt\nto novel problems (task recomposition). To tackle this, we cast task inference\nas an optimization problem from a variational inference perspective and ground\nour approach in an expectation-maximization framework. We show that gradients\nbackpropagated through a neural network to a task representation layer are an\nefficient heuristic to infer current task demands, a process we refer to as\ngradient-based inference (GBI). Further iterative optimization of the task\nrepresentation layer allows for recomposing abstractions to adapt to novel\nsituations. Using a toy example, a novel image classifier, and a language\nmodel, we demonstrate that GBI provides higher learning efficiency and\ngeneralization to novel tasks and limits forgetting. Moreover, we show that GBI\nhas unique advantages such as preserving information for uncertainty estimation\nand detecting out-of-distribution samples.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"BpF_Tf5kHXG4POP3lWGMJ5DHDxWiIYcsMLgdAR0afWs","pdfSize":"2451208","objectId":"0xd247a44b09b259ed6bc20439d9a23432f5167ec5062ea70b3dfb9a66c7313675","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
