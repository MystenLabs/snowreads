{"id":"2407.14974","title":"Out of spuriousity: Improving robustness to spurious correlations\n  without group annotations","authors":"Phuong Quynh Le, J\\\"org Schl\\\"otterer and Christin Seifert","authorsParsed":[["Le","Phuong Quynh",""],["Schlötterer","Jörg",""],["Seifert","Christin",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 20:24:14 GMT"}],"updateDate":"2024-07-23","timestamp":1721507054000,"abstract":"  Machine learning models are known to learn spurious correlations, i.e.,\nfeatures having strong relations with class labels but no causal relation.\nRelying on those correlations leads to poor performance in the data groups\nwithout these correlations and poor generalization ability. To improve the\nrobustness of machine learning models to spurious correlations, we propose an\napproach to extract a subnetwork from a fully trained network that does not\nrely on spurious correlations. The subnetwork is found by the assumption that\ndata points with the same spurious attribute will be close to each other in the\nrepresentation space when training with ERM, then we employ supervised\ncontrastive loss in a novel way to force models to unlearn the spurious\nconnections. The increase in the worst-group performance of our approach\ncontributes to strengthening the hypothesis that there exists a subnetwork in a\nfully trained dense network that is responsible for using only invariant\nfeatures in classification tasks, therefore erasing the influence of spurious\nfeatures even in the setup of multi spurious attributes and no prior knowledge\nof attributes labels.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}