{"id":"2408.09773","title":"Are Large Language Models More Honest in Their Probabilistic or\n  Verbalized Confidence?","authors":"Shiyu Ni, Keping Bi, Lulu Yu, Jiafeng Guo","authorsParsed":[["Ni","Shiyu",""],["Bi","Keping",""],["Yu","Lulu",""],["Guo","Jiafeng",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 08:01:11 GMT"}],"updateDate":"2024-08-20","timestamp":1724054471000,"abstract":"  Large language models (LLMs) have been found to produce hallucinations when\nthe question exceeds their internal knowledge boundaries. A reliable model\nshould have a clear perception of its knowledge boundaries, providing correct\nanswers within its scope and refusing to answer when it lacks knowledge.\nExisting research on LLMs' perception of their knowledge boundaries typically\nuses either the probability of the generated tokens or the verbalized\nconfidence as the model's confidence in its response. However, these studies\noverlook the differences and connections between the two. In this paper, we\nconduct a comprehensive analysis and comparison of LLMs' probabilistic\nperception and verbalized perception of their factual knowledge boundaries.\nFirst, we investigate the pros and cons of these two perceptions. Then, we\nstudy how they change under questions of varying frequencies. Finally, we\nmeasure the correlation between LLMs' probabilistic confidence and verbalized\nconfidence. Experimental results show that 1) LLMs' probabilistic perception is\ngenerally more accurate than verbalized perception but requires an in-domain\nvalidation set to adjust the confidence threshold. 2) Both perceptions perform\nbetter on less frequent questions. 3) It is challenging for LLMs to accurately\nexpress their internal confidence in natural language.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Pzd3GlbTn12ToYi0HrqN0-Fyk-qeC84Ixy0jAZfPYq4","pdfSize":"544433"}
