{"id":"2407.08189","title":"fairBERTs: Erasing Sensitive Information Through Semantic and\n  Fairness-aware Perturbations","authors":"Jinfeng Li, Yuefeng Chen, Xiangyu Liu, Longtao Huang, Rong Zhang, Hui\n  Xue","authorsParsed":[["Li","Jinfeng",""],["Chen","Yuefeng",""],["Liu","Xiangyu",""],["Huang","Longtao",""],["Zhang","Rong",""],["Xue","Hui",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 05:13:38 GMT"}],"updateDate":"2024-07-12","timestamp":1720674818000,"abstract":"  Pre-trained language models (PLMs) have revolutionized both the natural\nlanguage processing research and applications. However, stereotypical biases\n(e.g., gender and racial discrimination) encoded in PLMs have raised negative\nethical implications for PLMs, which critically limits their broader\napplications. To address the aforementioned unfairness issues, we present\nfairBERTs, a general framework for learning fair fine-tuned BERT series models\nby erasing the protected sensitive information via semantic and fairness-aware\nperturbations generated by a generative adversarial network. Through extensive\nqualitative and quantitative experiments on two real-world tasks, we\ndemonstrate the great superiority of fairBERTs in mitigating unfairness while\nmaintaining the model utility. We also verify the feasibility of transferring\nadversarial components in fairBERTs to other conventionally trained BERT-like\nmodels for yielding fairness improvements. Our findings may shed light on\nfurther research on building fairer fine-tuned PLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}