{"id":"2408.07665","title":"Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech\n  Large Language Models","authors":"Yi-Cheng Lin, Wei-Chih Chen, Hung-yi Lee","authorsParsed":[["Lin","Yi-Cheng",""],["Chen","Wei-Chih",""],["Lee","Hung-yi",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 16:55:06 GMT"}],"updateDate":"2024-08-15","timestamp":1723654506000,"abstract":"  Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies.\n","subjects":["Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}