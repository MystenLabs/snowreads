{"id":"2407.07364","title":"Real-time system optimal traffic routing under uncertainties -- Can\n  physics models boost reinforcement learning?","authors":"Zemian Ke, Qiling Zou, Jiachao Liu, Sean Qian","authorsParsed":[["Ke","Zemian",""],["Zou","Qiling",""],["Liu","Jiachao",""],["Qian","Sean",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 04:53:26 GMT"}],"updateDate":"2024-07-11","timestamp":1720587206000,"abstract":"  System optimal traffic routing can mitigate congestion by assigning routes\nfor a portion of vehicles so that the total travel time of all vehicles in the\ntransportation system can be reduced. However, achieving real-time optimal\nrouting poses challenges due to uncertain demands and unknown system dynamics,\nparticularly in expansive transportation networks. While physics model-based\nmethods are sensitive to uncertainties and model mismatches, model-free\nreinforcement learning struggles with learning inefficiencies and\ninterpretability issues. Our paper presents TransRL, a novel algorithm that\nintegrates reinforcement learning with physics models for enhanced performance,\nreliability, and interpretability. TransRL begins by establishing a\ndeterministic policy grounded in physics models, from which it learns from and\nis guided by a differentiable and stochastic teacher policy. During training,\nTransRL aims to maximize cumulative rewards while minimizing the Kullback\nLeibler (KL) divergence between the current policy and the teacher policy. This\napproach enables TransRL to simultaneously leverage interactions with the\nenvironment and insights from physics models. We conduct experiments on three\ntransportation networks with up to hundreds of links. The results demonstrate\nTransRL's superiority over traffic model-based methods for being adaptive and\nlearning from the actual network data. By leveraging the information from\nphysics models, TransRL consistently outperforms state-of-the-art reinforcement\nlearning algorithms such as proximal policy optimization (PPO) and soft actor\ncritic (SAC). Moreover, TransRL's actions exhibit higher reliability and\ninterpretability compared to baseline reinforcement learning approaches like\nPPO and SAC.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zopPSa7eRHOyZM6FHMg7rZe9SnA-Shr63uWN37WTRlU","pdfSize":"8887671"}
