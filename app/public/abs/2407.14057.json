{"id":"2407.14057","title":"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference","authors":"Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari,\n  Mahyar Najibi","authorsParsed":[["Fu","Qichen",""],["Cho","Minsik",""],["Merth","Thomas",""],["Mehta","Sachin",""],["Rastegari","Mohammad",""],["Najibi","Mahyar",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 06:34:45 GMT"}],"updateDate":"2024-07-22","timestamp":1721370885000,"abstract":"  The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}