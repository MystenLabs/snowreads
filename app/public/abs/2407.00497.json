{"id":"2407.00497","title":"LLMs-as-Instructors: Learning from Errors Toward Automating Model\n  Improvement","authors":"Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun,\n  Xuanjing Huang, Shuicheng Yan","authorsParsed":[["Ying","Jiahao",""],["Lin","Mingbao",""],["Cao","Yixin",""],["Tang","Wei",""],["Wang","Bo",""],["Sun","Qianru",""],["Huang","Xuanjing",""],["Yan","Shuicheng",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 17:16:04 GMT"}],"updateDate":"2024-07-02","timestamp":1719681364000,"abstract":"  This paper introduces the innovative \"LLMs-as-Instructors\" framework, which\nleverages the advanced Large Language Models (LLMs) to autonomously enhance the\ntraining of smaller target models. Inspired by the theory of \"Learning from\nErrors\", this framework employs an instructor LLM to meticulously analyze the\nspecific errors within a target model, facilitating targeted and efficient\ntraining cycles. Within this framework, we implement two strategies: \"Learning\nfrom Error,\" which focuses solely on incorrect responses to tailor training\ndata, and \"Learning from Error by Contrast\", which uses contrastive learning to\nanalyze both correct and incorrect responses for a deeper understanding of\nerrors.\n  Our empirical studies, conducted with several open-source models, demonstrate\nsignificant improvements across multiple benchmarks, including mathematical\nreasoning, coding abilities, and factual knowledge. Notably, the refined\nLlama-3-8b-Instruction has outperformed ChatGPT, illustrating the effectiveness\nof our approach. By leveraging the strengths of both strategies, we have\nattained a more balanced performance improvement on both in-domain and\nout-of-domain benchmarks. Our code can be found at\nhttps://yingjiahao14.github.io/LLMs-as-Instructors-pages/.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}