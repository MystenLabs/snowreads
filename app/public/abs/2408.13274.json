{"id":"2408.13274","title":"Robust Image Classification: Defensive Strategies against FGSM and PGD\n  Adversarial Attacks","authors":"Hetvi Waghela, Jaydip Sen, Sneha Rakshit","authorsParsed":[["Waghela","Hetvi",""],["Sen","Jaydip",""],["Rakshit","Sneha",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 02:00:02 GMT"}],"updateDate":"2024-08-27","timestamp":1724119202000,"abstract":"  Adversarial attacks, particularly the Fast Gradient Sign Method (FGSM) and\nProjected Gradient Descent (PGD) pose significant threats to the robustness of\ndeep learning models in image classification. This paper explores and refines\ndefense mechanisms against these attacks to enhance the resilience of neural\nnetworks. We employ a combination of adversarial training and innovative\npreprocessing techniques, aiming to mitigate the impact of adversarial\nperturbations. Our methodology involves modifying input data before\nclassification and investigating different model architectures and training\nstrategies. Through rigorous evaluation of benchmark datasets, we demonstrate\nthe effectiveness of our approach in defending against FGSM and PGD attacks.\nOur results show substantial improvements in model robustness compared to\nbaseline methods, highlighting the potential of our defense strategies in\nreal-world applications. This study contributes to the ongoing efforts to\ndevelop secure and reliable machine learning systems, offering practical\ninsights and paving the way for future research in adversarial defense. By\nbridging theoretical advancements and practical implementation, we aim to\nenhance the trustworthiness of AI applications in safety-critical domains.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}