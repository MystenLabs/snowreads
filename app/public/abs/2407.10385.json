{"id":"2407.10385","title":"By My Eyes: Grounding Multimodal Large Language Models with Sensor Data\n  via Visual Prompting","authors":"Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee,\n  Sung-Ju Lee","authorsParsed":[["Yoon","Hyungjun",""],["Tolera","Biniyam Aschalew",""],["Gong","Taesik",""],["Lee","Kimin",""],["Lee","Sung-Ju",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 01:33:54 GMT"}],"updateDate":"2024-07-16","timestamp":1721007234000,"abstract":"  Large language models (LLMs) have demonstrated exceptional abilities across\nvarious domains. However, utilizing LLMs for ubiquitous sensing applications\nremains challenging as existing text-prompt methods show significant\nperformance degradation when handling long sensor data sequences. We propose a\nvisual prompting approach for sensor data using multimodal LLMs (MLLMs). We\ndesign a visual prompt that directs MLLMs to utilize visualized sensor data\nalongside the target sensory task descriptions. Additionally, we introduce a\nvisualization generator that automates the creation of optimal visualizations\ntailored to a given sensory task, eliminating the need for prior task-specific\nknowledge. We evaluated our approach on nine sensory tasks involving four\nsensing modalities, achieving an average of 10% higher accuracy than text-based\nprompts and reducing token costs by 15.8x. Our findings highlight the\neffectiveness and cost-efficiency of visual prompts with MLLMs for various\nsensory tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}