{"id":"2408.10566","title":"SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic\n  Continual Learning","authors":"Yuqing Zhao, Divya Saxena, Jiannong Cao, Xiaoyun Liu and Changlin Song","authorsParsed":[["Zhao","Yuqing",""],["Saxena","Divya",""],["Cao","Jiannong",""],["Liu","Xiaoyun",""],["Song","Changlin",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 06:05:52 GMT"},{"version":"v2","created":"Mon, 26 Aug 2024 05:08:29 GMT"},{"version":"v3","created":"Thu, 12 Sep 2024 12:57:25 GMT"}],"updateDate":"2024-09-13","timestamp":1724133952000,"abstract":"  In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}