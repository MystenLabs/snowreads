{"id":"2408.15065","title":"The Benefits of Balance: From Information Projections to Variance\n  Reduction","authors":"Lang Liu, Ronak Mehta, Soumik Pal, Zaid Harchaoui","authorsParsed":[["Liu","Lang",""],["Mehta","Ronak",""],["Pal","Soumik",""],["Harchaoui","Zaid",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 13:48:15 GMT"}],"updateDate":"2024-08-28","timestamp":1724766495000,"abstract":"  Data balancing across multiple modalities/sources appears in various forms in\nseveral foundation models (e.g., CLIP and DINO) achieving universal\nrepresentation learning. We show that this iterative algorithm, usually used to\navoid representation collapse, enjoys an unsuspected benefit: reducing the\nvariance of estimators that are functionals of the empirical distribution over\nthese sources. We provide non-asymptotic bounds quantifying this variance\nreduction effect and relate them to the eigendecays of appropriately defined\nMarkov operators. We explain how various forms of data balancing in contrastive\nmultimodal learning and self-supervised clustering can be interpreted as\ninstances of this variance reduction scheme.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}