{"id":"2408.13654","title":"Symbolic Working Memory Enhances Language Models for Complex Rule\n  Application","authors":"Siyuan Wang, Zhongyu Wei, Yejin Choi, Xiang Ren","authorsParsed":[["Wang","Siyuan",""],["Wei","Zhongyu",""],["Choi","Yejin",""],["Ren","Xiang",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 19:11:54 GMT"}],"updateDate":"2024-08-27","timestamp":1724526714000,"abstract":"  Large Language Models (LLMs) have shown remarkable reasoning performance but\nstruggle with multi-step deductive reasoning involving a series of rule\napplication steps, especially when rules are presented non-sequentially. Our\npreliminary analysis shows that while LLMs excel in single-step rule\napplication, their performance drops significantly in multi-step scenarios due\nto the challenge in rule grounding. It requires anchoring the applicable rule\nand supporting facts at each step, amidst multiple input rules, facts, and\ninferred facts. To address this, we propose augmenting LLMs with external\nworking memory and introduce a neurosymbolic framework for rule application.\nThe memory stores facts and rules in both natural language and symbolic forms,\nenabling precise tracking. Utilizing this memory, our framework iteratively\nperforms symbolic rule grounding and LLM-based rule implementation. The former\nmatches predicates and variables of symbolic rules and facts to ground\napplicable rules at each step. Experiments indicate our framework's\neffectiveness in rule application and its robustness across various steps and\nsettings~\\footnote{Code and data are available at\n\\url{https://github.com/SiyuanWangw/RuleApplication}.}.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}