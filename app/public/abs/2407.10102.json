{"id":"2407.10102","title":"3DEgo: 3D Editing on the Go!","authors":"Umar Khalid, Hasan Iqbal, Azib Farooq, Jing Hua and Chen Chen","authorsParsed":[["Khalid","Umar",""],["Iqbal","Hasan",""],["Farooq","Azib",""],["Hua","Jing",""],["Chen","Chen",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 07:03:50 GMT"}],"updateDate":"2024-07-16","timestamp":1720940630000,"abstract":"  We introduce 3DEgo to address a novel problem of directly synthesizing\nphotorealistic 3D scenes from monocular videos guided by textual prompts.\nConventional methods construct a text-conditioned 3D scene through a\nthree-stage process, involving pose estimation using Structure-from-Motion\n(SfM) libraries like COLMAP, initializing the 3D model with unedited images,\nand iteratively updating the dataset with edited images to achieve a 3D scene\nwith text fidelity. Our framework streamlines the conventional multi-stage 3D\nediting process into a single-stage workflow by overcoming the reliance on\nCOLMAP and eliminating the cost of model initialization. We apply a diffusion\nmodel to edit video frames prior to 3D scene creation by incorporating our\ndesigned noise blender module for enhancing multi-view editing consistency, a\nstep that does not require additional training or fine-tuning of T2I diffusion\nmodels. 3DEgo utilizes 3D Gaussian Splatting to create 3D scenes from the\nmulti-view consistent edited frames, capitalizing on the inherent temporal\ncontinuity and explicit point cloud data. 3DEgo demonstrates remarkable editing\nprecision, speed, and adaptability across a variety of video sources, as\nvalidated by extensive evaluations on six datasets, including our own prepared\nGS25 dataset. Project Page: https://3dego.github.io/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}