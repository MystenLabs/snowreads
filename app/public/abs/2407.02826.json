{"id":"2407.02826","title":"SA-WavLM: Speaker-Aware Self-Supervised Pre-training for Mixture Speech","authors":"Jingru Lin, Meng Ge, Junyi Ao, Liqun Deng, Haizhou Li","authorsParsed":[["Lin","Jingru",""],["Ge","Meng",""],["Ao","Junyi",""],["Deng","Liqun",""],["Li","Haizhou",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 06:07:42 GMT"}],"updateDate":"2024-07-04","timestamp":1719986862000,"abstract":"  It was shown that pre-trained models with self-supervised learning (SSL)\ntechniques are effective in various downstream speech tasks. However, most such\nmodels are trained on single-speaker speech data, limiting their effectiveness\nin mixture speech. This motivates us to explore pre-training on mixture speech.\nThis work presents SA-WavLM, a novel pre-trained model for mixture speech.\nSpecifically, SA-WavLM follows an \"extract-merge-predict\" pipeline in which the\nrepresentations of each speaker in the input mixture are first extracted\nindividually and then merged before the final prediction. In this pipeline,\nSA-WavLM performs speaker-informed extractions with the consideration of the\ninteractions between different speakers. Furthermore, a speaker shuffling\nstrategy is proposed to enhance the robustness towards the speaker absence.\nExperiments show that SA-WavLM either matches or improves upon the\nstate-of-the-art pre-trained models.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}