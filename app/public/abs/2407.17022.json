{"id":"2407.17022","title":"Can Language Models Evaluate Human Written Text? Case Study on Korean\n  Student Writing for Education","authors":"Seungyoon Kim, Seungone Kim","authorsParsed":[["Kim","Seungyoon",""],["Kim","Seungone",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 06:02:57 GMT"}],"updateDate":"2024-07-25","timestamp":1721800977000,"abstract":"  Large language model (LLM)-based evaluation pipelines have demonstrated their\ncapability to robustly evaluate machine-generated text. Extending this\nmethodology to assess human-written text could significantly benefit\neducational settings by providing direct feedback to enhance writing skills,\nalthough this application is not straightforward. In this paper, we investigate\nwhether LLMs can effectively assess human-written text for educational\npurposes. We collected 100 texts from 32 Korean students across 15 types of\nwriting and employed GPT-4-Turbo to evaluate them using grammaticality,\nfluency, coherence, consistency, and relevance as criteria. Our analyses\nindicate that LLM evaluators can reliably assess grammaticality and fluency, as\nwell as more objective types of writing, though they struggle with other\ncriteria and types of writing. We publicly release our dataset and feedback.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WHLGY2uRBqPz1QFRgOqWwS_T2W6vWLEUbjNe8SYn2ig","pdfSize":"93131"}
