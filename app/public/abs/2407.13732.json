{"id":"2407.13732","title":"Realizable $H$-Consistent and Bayes-Consistent Loss Functions for\n  Learning to Defer","authors":"Anqi Mao, Mehryar Mohri, Yutao Zhong","authorsParsed":[["Mao","Anqi",""],["Mohri","Mehryar",""],["Zhong","Yutao",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:35:03 GMT"}],"updateDate":"2024-07-19","timestamp":1721324103000,"abstract":"  We present a comprehensive study of surrogate loss functions for learning to\ndefer. We introduce a broad family of surrogate losses, parameterized by a\nnon-increasing function $\\Psi$, and establish their realizable $H$-consistency\nunder mild conditions. For cost functions based on classification error, we\nfurther show that these losses admit $H$-consistency bounds when the hypothesis\nset is symmetric and complete, a property satisfied by common neural network\nand linear function hypothesis sets. Our results also resolve an open question\nraised in previous work (Mozannar et al., 2023) by proving the realizable\n$H$-consistency and Bayes-consistency of a specific surrogate loss.\nFurthermore, we identify choices of $\\Psi$ that lead to $H$-consistent\nsurrogate losses for any general cost function, thus achieving\nBayes-consistency, realizable $H$-consistency, and $H$-consistency bounds\nsimultaneously. We also investigate the relationship between $H$-consistency\nbounds and realizable $H$-consistency in learning to defer, highlighting key\ndifferences from standard classification. Finally, we empirically evaluate our\nproposed surrogate losses and compare them with existing baselines.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}