{"id":"2407.05603","title":"WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question\n  Answering","authors":"Pingyi Chen, Chenglu Zhu, Sunyi Zheng, Honglin Li, Lin Yang","authorsParsed":[["Chen","Pingyi",""],["Zhu","Chenglu",""],["Zheng","Sunyi",""],["Li","Honglin",""],["Yang","Lin",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 04:37:32 GMT"}],"updateDate":"2024-07-09","timestamp":1720413452000,"abstract":"  Whole slide imaging is routinely adopted for carcinoma diagnosis and\nprognosis. Abundant experience is required for pathologists to achieve accurate\nand reliable diagnostic results of whole slide images (WSI). The huge size and\nheterogeneous features of WSIs make the workflow of pathological reading\nextremely time-consuming. In this paper, we propose a novel framework (WSI-VQA)\nto interpret WSIs by generative visual question answering. WSI-VQA shows\nuniversality by reframing various kinds of slide-level tasks in a\nquestion-answering pattern, in which pathologists can achieve\nimmunohistochemical grading, survival prediction, and tumor subtyping following\nhuman-machine interaction. Furthermore, we establish a WSI-VQA dataset which\ncontains 8672 slide-level question-answering pairs with 977 WSIs. Besides the\nability to deal with different slide-level tasks, our generative model which is\nnamed Wsi2Text Transformer (W2T) outperforms existing discriminative models in\nmedical correctness, which reveals the potential of our model to be applied in\nthe clinical scenario. Additionally, we also visualize the co-attention mapping\nbetween word embeddings and WSIs as an intuitive explanation for diagnostic\nresults. The dataset and related code are available at\nhttps://github.com/cpystan/WSI-VQA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}