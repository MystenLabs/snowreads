{"id":"2407.15295","title":"VideoGameBunny: Towards vision assistants for video games","authors":"Mohammad Reza Taesiri, Cor-Paul Bezemer","authorsParsed":[["Taesiri","Mohammad Reza",""],["Bezemer","Cor-Paul",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 23:31:57 GMT"}],"updateDate":"2024-07-23","timestamp":1721604717000,"abstract":"  Large multimodal models (LMMs) hold substantial promise across various\ndomains, from personal assistance in daily tasks to sophisticated applications\nlike medical diagnostics. However, their capabilities have limitations in the\nvideo game domain, such as challenges with scene understanding, hallucinations,\nand inaccurate descriptions of video game content, especially in open-source\nmodels. This paper describes the development of VideoGameBunny, a LLaVA-style\nmodel based on Bunny, specifically tailored for understanding images from video\ngames. We release intermediate checkpoints, training logs, and an extensive\ndataset comprising 185,259 video game images from 413 titles, along with\n389,565 image-instruction pairs that include image captions, question-answer\npairs, and a JSON representation of 16 elements of 136,974 images. Our\nexperiments show that our high quality game-related data has the potential to\nmake a relatively small model outperform the much larger state-of-the-art model\nLLaVa-1.6-34b (which has more than 4x the number of parameters). Our study\npaves the way for future research in video game understanding on tasks such as\nplaying, commentary, and debugging. Code and data are available at\nhttps://videogamebunny.github.io/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"s06GwgFGqht84QLmjMXxScpnmpcrW8-LS8YFhRYdjI4","pdfSize":"10930344"}
