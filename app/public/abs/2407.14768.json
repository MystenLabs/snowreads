{"id":"2407.14768","title":"Teach Harder, Learn Poorer: Rethinking Hard Sample Distillation for\n  GNN-to-MLP Knowledge Distillation","authors":"Lirong Wu, Yunfan Liu, Haitao Lin, Yufei Huang, Stan Z. Li","authorsParsed":[["Wu","Lirong",""],["Liu","Yunfan",""],["Lin","Haitao",""],["Huang","Yufei",""],["Li","Stan Z.",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 06:13:00 GMT"}],"updateDate":"2024-07-23","timestamp":1721455980000,"abstract":"  To bridge the gaps between powerful Graph Neural Networks (GNNs) and\nlightweight Multi-Layer Perceptron (MLPs), GNN-to-MLP Knowledge Distillation\n(KD) proposes to distill knowledge from a well-trained teacher GNN into a\nstudent MLP. In this paper, we revisit the knowledge samples (nodes) in teacher\nGNNs from the perspective of hardness, and identify that hard sample\ndistillation may be a major performance bottleneck of existing graph KD\nalgorithms. The GNN-to-MLP KD involves two different types of hardness, one\nstudent-free knowledge hardness describing the inherent complexity of GNN\nknowledge, and the other student-dependent distillation hardness describing the\ndifficulty of teacher-to-student distillation. However, most of the existing\nwork focuses on only one of these aspects or regards them as one thing. This\npaper proposes a simple yet effective Hardness-aware GNN-to-MLP Distillation\n(HGMD) framework, which decouples the two hardnesses and estimates them using a\nnon-parametric approach. Finally, two hardness-aware distillation schemes\n(i.e., HGMD-weight and HGMD-mixup) are further proposed to distill\nhardness-aware knowledge from teacher GNNs into the corresponding nodes of\nstudent MLPs. As non-parametric distillation, HGMD does not involve any\nadditional learnable parameters beyond the student MLPs, but it still\noutperforms most of the state-of-the-art competitors. HGMD-mixup improves over\nthe vanilla MLPs by 12.95% and outperforms its teacher GNNs by 2.48% averaged\nover seven real-world datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}