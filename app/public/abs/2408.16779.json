{"id":"2408.16779","title":"Inductive Learning of Logical Theories with LLMs: A Complexity-graded\n  Analysis","authors":"Jo\\~ao Pedro Gandarela, Danilo S. Carvalho, Andr\\'e Freitas","authorsParsed":[["Gandarela","João Pedro",""],["Carvalho","Danilo S.",""],["Freitas","André",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 16:41:00 GMT"}],"updateDate":"2024-09-02","timestamp":1723740060000,"abstract":"  This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for the LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Logic in Computer Science"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"z3aWxsUX6SZPARZKLB4pggEU2ldXW0xKavP-A0ZVyjQ","pdfSize":"476111"}
