{"id":"2408.11276","title":"Chernoff Bounds for Tensor Expanders on Riemannian Manifolds Using Graph\n  Laplacian Approximation","authors":"Shih-Yu Chang","authorsParsed":[["Chang","Shih-Yu",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 01:59:27 GMT"}],"updateDate":"2024-08-22","timestamp":1724205567000,"abstract":"  This paper addresses the advancement of probability tail bound analysis, a\ncrucial statistical tool for assessing the probability of large deviations of\nrandom variables from their expected values. Traditional tail bounds, such as\nMarkov's, Chebyshev's, and Chernoff bounds, have proven valuable across\nnumerous scientific and engineering fields. However, as data complexity grows,\nthere is a pressing need to extend tail bound estimation from scalar variables\nto high-dimensional random objects. Existing studies often rely on the\nassumption of independence among high-dimensional random objects, an assumption\nthat may not always be valid. Building on the work of researchers like Garg et\nal. and Chang, who employed random walks to model high-dimensional ensembles,\nthis study introduces a more generalized approach by exploring random walks\nover manifolds. To address the challenges of constructing an appropriate\nunderlying graph for a manifold, we propose a novel method that enhances random\nwalks on graphs approximating the manifold. This approach ensures spectral\nsimilarity between the original manifold and the approximated graph, including\nmatching eigenvalues, eigenvectors, and eigenfunctions. Leveraging graph\napproximation technique proposed by Burago et al. for manifolds, we derive the\ntensor Chernoff bound and establish its range for random walks on a Riemannian\nmanifold according to the underlying manifold's spectral characteristics.\n","subjects":["Mathematics/Probability","Computing Research Repository/Machine Learning","Mathematics/Differential Geometry","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}