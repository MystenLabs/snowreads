{"id":"2408.13833","title":"Biomedical Large Languages Models Seem not to be Superior to Generalist\n  Models on Unseen Medical Data","authors":"Felix J. Dorfner, Amin Dada, Felix Busch, Marcus R. Makowski, Tianyu\n  Han, Daniel Truhn, Jens Kleesiek, Madhumita Sushil, Jacqueline Lammert, Lisa\n  C. Adams, Keno K. Bressem","authorsParsed":[["Dorfner","Felix J.",""],["Dada","Amin",""],["Busch","Felix",""],["Makowski","Marcus R.",""],["Han","Tianyu",""],["Truhn","Daniel",""],["Kleesiek","Jens",""],["Sushil","Madhumita",""],["Lammert","Jacqueline",""],["Adams","Lisa C.",""],["Bressem","Keno K.",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 13:36:22 GMT"}],"updateDate":"2024-08-27","timestamp":1724592982000,"abstract":"  Large language models (LLMs) have shown potential in biomedical applications,\nleading to efforts to fine-tune them on domain-specific data. However, the\neffectiveness of this approach remains unclear. This study evaluates the\nperformance of biomedically fine-tuned LLMs against their general-purpose\ncounterparts on a variety of clinical tasks. We evaluated their performance on\nclinical case challenges from the New England Journal of Medicine (NEJM) and\nthe Journal of the American Medical Association (JAMA) and on several clinical\ntasks (e.g., information extraction, document summarization, and clinical\ncoding). Using benchmarks specifically chosen to be likely outside the\nfine-tuning datasets of biomedical models, we found that biomedical LLMs mostly\nperform inferior to their general-purpose counterparts, especially on tasks not\nfocused on medical knowledge. While larger models showed similar performance on\ncase tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA\ncases), smaller biomedical models showed more pronounced underperformance\n(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).\nSimilar trends were observed across the CLUE (Clinical Language Understanding\nEvaluation) benchmark tasks, with general-purpose models often performing\nbetter on text generation, question answering, and coding tasks. Our results\nsuggest that fine-tuning LLMs to biomedical data may not provide the expected\nbenefits and may potentially lead to reduced performance, challenging\nprevailing assumptions about domain-specific adaptation of LLMs and\nhighlighting the need for more rigorous evaluation frameworks in healthcare AI.\nAlternative approaches, such as retrieval-augmented generation, may be more\neffective in enhancing the biomedical capabilities of LLMs without compromising\ntheir general knowledge.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cPWep9djs5ptLlPVJafreoaGDFZlEvtOJe3wr1OznvU","pdfSize":"207229"}
