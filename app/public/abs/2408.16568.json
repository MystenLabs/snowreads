{"id":"2408.16568","title":"Audio xLSTMs: Learning Self-Supervised Audio Representations with xLSTMs","authors":"Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan","authorsParsed":[["Yadav","Sarthak",""],["Theodoridis","Sergios",""],["Tan","Zheng-Hua",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 14:35:56 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 07:00:41 GMT"}],"updateDate":"2024-09-04","timestamp":1724942156000,"abstract":"  While the transformer has emerged as the eminent neural architecture, several\nindependent lines of research have emerged to address its limitations.\nRecurrent neural approaches have also observed a lot of renewed interest,\nincluding the extended long short-term memory (xLSTM) architecture, which\nreinvigorates the original LSTM architecture. However, while xLSTMs have shown\ncompetitive performance compared to the transformer, their viability for\nlearning self-supervised general-purpose audio representations has not yet been\nevaluated. This work proposes Audio xLSTM (AxLSTM), an approach to learn audio\nrepresentations from masked spectrogram patches in a self-supervised setting.\nPretrained on the AudioSet dataset, the proposed AxLSTM models outperform\ncomparable self-supervised audio spectrogram transformer (SSAST) baselines by\nup to 20% in relative performance across a set of ten diverse downstream tasks\nwhile having up to 45% fewer parameters.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}