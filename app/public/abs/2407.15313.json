{"id":"2407.15313","title":"Should we use model-free or model-based control? A case study of battery\n  management systems","authors":"Mohamad Fares El Hajj Chehade, Young-ho Cho, Sandeep Chinchali, Hao\n  Zhu","authorsParsed":[["Chehade","Mohamad Fares El Hajj",""],["Cho","Young-ho",""],["Chinchali","Sandeep",""],["Zhu","Hao",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 00:48:12 GMT"}],"updateDate":"2024-07-23","timestamp":1721609292000,"abstract":"  Reinforcement learning (RL) and model predictive control (MPC) each offer\ndistinct advantages and limitations when applied to control problems in power\nand energy systems. Despite various studies on these methods, benchmarks remain\nlacking and the preference for RL over traditional controls is not well\nunderstood. In this work, we put forth a comparative analysis using RL- and\nMPC-based controllers for optimizing a battery management system (BMS). The BMS\nproblem aims to minimize costs while adhering to operational limits. by\nadjusting the battery (dis)charging in response to fluctuating electricity\nprices over a time horizon. The MPC controller uses a learningbased forecast of\nfuture demand and price changes to formulate a multi-period linear program,\nthat can be solved using off-the-shelf solvers. Meanwhile, the RL controller\nrequires no timeseries modeling but instead is trained from the sample\ntrajectories using the proximal policy optimization (PPO) algorithm. Numerical\ntests compare these controllers across optimality, training time, testing time,\nand robustness, providing a comprehensive evaluation of their efficacy. RL not\nonly yields optimal solutions quickly but also ensures robustness to shifts in\ncustomer behavior, such as changes in demand distribution. However, as\nexpected, training the RL agent is more time-consuming than MPC.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/"}