{"id":"2407.06121","title":"Periodic agent-state based Q-learning for POMDPs","authors":"Amit Sinha, Mathieu Geist, Aditya Mahajan","authorsParsed":[["Sinha","Amit",""],["Geist","Mathieu",""],["Mahajan","Aditya",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 16:58:57 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 05:16:36 GMT"}],"updateDate":"2024-08-21","timestamp":1720457937000,"abstract":"  The standard approach for Partially Observable Markov Decision Processes\n(POMDPs) is to convert them to a fully observed belief-state MDP. However, the\nbelief state depends on the system model and is therefore not viable in\nreinforcement learning (RL) settings. A widely used alternative is to use an\nagent state, which is a model-free, recursively updateable function of the\nobservation history. Examples include frame stacking and recurrent neural\nnetworks. Since the agent state is model-free, it is used to adapt standard RL\nalgorithms to POMDPs. However, standard RL algorithms like Q-learning learn a\nstationary policy. Our main thesis that we illustrate via examples is that\nbecause the agent state does not satisfy the Markov property, non-stationary\nagent-state based policies can outperform stationary ones. To leverage this\nfeature, we propose PASQL (periodic agent-state based Q-learning), which is a\nvariant of agent-state-based Q-learning that learns periodic policies. By\ncombining ideas from periodic Markov chains and stochastic approximation, we\nrigorously establish that PASQL converges to a cyclic limit and characterize\nthe approximation error of the converged periodic policy. Finally, we present a\nnumerical experiment to highlight the salient features of PASQL and demonstrate\nthe benefit of learning periodic policies over stationary policies.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}