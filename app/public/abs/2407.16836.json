{"id":"2407.16836","title":"Inference Load-Aware Orchestration for Hierarchical Federated Learning","authors":"Anna Lackinger, Pantelis A. Frangoudis, Ivan \\v{C}ili\\'c, Alireza\n  Furutanpey, Ilir Murturi, Ivana Podnar \\v{Z}arko, Schahram Dustdar","authorsParsed":[["Lackinger","Anna",""],["Frangoudis","Pantelis A.",""],["Čilić","Ivan",""],["Furutanpey","Alireza",""],["Murturi","Ilir",""],["Žarko","Ivana Podnar",""],["Dustdar","Schahram",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 21:01:12 GMT"}],"updateDate":"2024-07-25","timestamp":1721768472000,"abstract":"  Hierarchical federated learning (HFL) designs introduce intermediate\naggregator nodes between clients and the global federated learning server in\norder to reduce communication costs and distribute server load. One side effect\nis that machine learning model replication at scale comes \"for free\" as part of\nthe HFL process: model replicas are hosted at the client end, intermediate\nnodes, and the global server level and are readily available for serving\ninference requests. This creates opportunities for efficient model serving but\nsimultaneously couples the training and serving processes and calls for their\njoint orchestration. This is particularly important for continual learning,\nwhere serving a model while (re)training it periodically, upon specific\ntriggers, or continuously, takes place over shared infrastructure spanning the\ncomputing continuum. Consequently, training and inference workloads can\ninterfere with detrimental effects on performance. To address this issue, we\npropose an inference load-aware HFL orchestration scheme, which makes informed\ndecisions on HFL configuration, considering knowledge about inference workloads\nand the respective processing capacity. Applying our scheme to a continual\nlearning use case in the transportation domain, we demonstrate that by\noptimizing aggregator node placement and device-aggregator association,\nsignificant inference latency savings can be achieved while communication costs\nare drastically reduced compared to flat centralized federated learning.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wC9SYACw8G86c5Upf-GiYLGz3RW0nXqGMXSVm2k39fw","pdfSize":"4268070"}
