{"id":"2407.15451","title":"Domain-Adaptive 2D Human Pose Estimation via Dual Teachers in Extremely\n  Low-Light Conditions","authors":"Yihao Ai, Yifei Qi, Bo Wang, Yu Cheng, Xinchao Wang, Robby T. Tan","authorsParsed":[["Ai","Yihao",""],["Qi","Yifei",""],["Wang","Bo",""],["Cheng","Yu",""],["Wang","Xinchao",""],["Tan","Robby T.",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 08:09:14 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 07:22:53 GMT"}],"updateDate":"2024-07-24","timestamp":1721635754000,"abstract":"  Existing 2D human pose estimation research predominantly concentrates on\nwell-lit scenarios, with limited exploration of poor lighting conditions, which\nare a prevalent aspect of daily life. Recent studies on low-light pose\nestimation require the use of paired well-lit and low-light images with ground\ntruths for training, which are impractical due to the inherent challenges\nassociated with annotation on low-light images. To this end, we introduce a\nnovel approach that eliminates the need for low-light ground truths. Our\nprimary novelty lies in leveraging two complementary-teacher networks to\ngenerate more reliable pseudo labels, enabling our model achieves competitive\nperformance on extremely low-light images without the need for training with\nlow-light ground truths. Our framework consists of two stages. In the first\nstage, our model is trained on well-lit data with low-light augmentations. In\nthe second stage, we propose a dual-teacher framework to utilize the unlabeled\nlow-light data, where a center-based main teacher produces the pseudo labels\nfor relatively visible cases, while a keypoints-based complementary teacher\nfocuses on producing the pseudo labels for the missed persons of the main\nteacher. With the pseudo labels from both teachers, we propose a\nperson-specific low-light augmentation to challenge a student model in training\nto outperform the teachers. Experimental results on real low-light dataset\n(ExLPose-OCN) show, our method achieves 6.8% (2.4 AP) improvement over the\nstate-of-the-art (SOTA) method, despite no low-light ground-truth data is used\nin our approach, in contrast to the SOTA method. Our code will be available\nat:https://github.com/ayh015-dev/DA-LLPose.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kQ9kjpg6bX8MJyBc7Ac6W9Z_kPPZpRc9icR_yLmsTWA","pdfSize":"19052035"}
