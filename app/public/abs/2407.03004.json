{"id":"2407.03004","title":"SemioLLM: Assessing Large Language Models for Semiological Analysis in\n  Epilepsy Research","authors":"Meghal Dani, Muthu Jeyanthi Prakash, Zeynep Akata and Stefanie Liebe","authorsParsed":[["Dani","Meghal",""],["Prakash","Muthu Jeyanthi",""],["Akata","Zeynep",""],["Liebe","Stefanie",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 11:02:12 GMT"}],"updateDate":"2024-07-04","timestamp":1720004532000,"abstract":"  Large Language Models have shown promising results in their ability to encode\ngeneral medical knowledge in standard medical question-answering datasets.\nHowever, their potential application in clinical practice requires evaluation\nin domain-specific tasks, where benchmarks are largely missing. In this study\nsemioLLM, we test the ability of state-of-the-art LLMs (GPT-3.5, GPT-4, Mixtral\n8x7B, and Qwen-72chat) to leverage their internal knowledge and reasoning for\nepilepsy diagnosis. Specifically, we obtain likelihood estimates linking\nunstructured text descriptions of seizures to seizure-generating brain regions,\nusing an annotated clinical database containing 1269 entries. We evaluate the\nLLM's performance, confidence, reasoning, and citation abilities in comparison\nto clinical evaluation. Models achieve above-chance classification performance\nwith prompt engineering significantly improving their outcome, with some models\nachieving close-to-clinical performance and reasoning. However, our analyses\nalso reveal significant pitfalls with several models being overly confident\nwhile showing poor performance, as well as exhibiting citation errors and\nhallucinations. In summary, our work provides the first extensive benchmark\ncomparing current SOTA LLMs in the medical domain of epilepsy and highlights\ntheir ability to leverage unstructured texts from patients' medical history to\naid diagnostic processes in health care.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}