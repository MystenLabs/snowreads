{"id":"2408.08065","title":"SPEED: Scalable Preprocessing of EEG Data for Self-Supervised Learning","authors":"Anders Gj{\\o}lbye, Lina Skerath, William Lehn-Schi{\\o}ler, Nicolas\n  Langer, Lars Kai Hansen","authorsParsed":[["Gjølbye","Anders",""],["Skerath","Lina",""],["Lehn-Schiøler","William",""],["Langer","Nicolas",""],["Hansen","Lars Kai",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 10:15:01 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 19:53:15 GMT"}],"updateDate":"2024-08-21","timestamp":1723716901000,"abstract":"  Electroencephalography (EEG) research typically focuses on tasks with\nnarrowly defined objectives, but recent studies are expanding into the use of\nunlabeled data within larger models, aiming for a broader range of\napplications. This addresses a critical challenge in EEG research. For example,\nKostas et al. (2021) show that self-supervised learning (SSL) outperforms\ntraditional supervised methods. Given the high noise levels in EEG data, we\nargue that further improvements are possible with additional preprocessing.\nCurrent preprocessing methods often fail to efficiently manage the large data\nvolumes required for SSL, due to their lack of optimization, reliance on\nsubjective manual corrections, and validation processes or inflexible protocols\nthat limit SSL. We propose a Python-based EEG preprocessing pipeline optimized\nfor self-supervised learning, designed to efficiently process large-scale data.\nThis optimization not only stabilizes self-supervised training but also\nenhances performance on downstream tasks compared to training with raw data.\n","subjects":["Electrical Engineering and Systems Science/Signal Processing","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}