{"id":"2408.05496","title":"Variational Inference Failures Under Model Symmetries: Permutation\n  Invariant Posteriors for Bayesian Neural Networks","authors":"Yoav Gelberg, Tycho F.A. van der Ouderaa, Mark van der Wilk, Yarin Gal","authorsParsed":[["Gelberg","Yoav",""],["van der Ouderaa","Tycho F. A.",""],["van der Wilk","Mark",""],["Gal","Yarin",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 09:06:34 GMT"}],"updateDate":"2024-08-13","timestamp":1723280794000,"abstract":"  Weight space symmetries in neural network architectures, such as permutation\nsymmetries in MLPs, give rise to Bayesian neural network (BNN) posteriors with\nmany equivalent modes. This multimodality poses a challenge for variational\ninference (VI) techniques, which typically rely on approximating the posterior\nwith a unimodal distribution. In this work, we investigate the impact of weight\nspace permutation symmetries on VI. We demonstrate, both theoretically and\nempirically, that these symmetries lead to biases in the approximate posterior,\nwhich degrade predictive performance and posterior fit if not explicitly\naccounted for. To mitigate this behavior, we leverage the symmetric structure\nof the posterior and devise a symmetrization mechanism for constructing\npermutation invariant variational posteriors. We show that the symmetrized\ndistribution has a strictly better fit to the true posterior, and that it can\nbe trained using the original ELBO objective with a modified KL regularization\nterm. We demonstrate experimentally that our approach mitigates the\naforementioned biases and results in improved predictions and a higher ELBO.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}