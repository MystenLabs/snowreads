{"id":"2407.03836","title":"ADAPT: Multimodal Learning for Detecting Physiological Changes under\n  Missing Modalities","authors":"Julie Mordacq, Leo Milecki, Maria Vakalopoulou, Steve Oudot, Vicky\n  Kalogeiton","authorsParsed":[["Mordacq","Julie",""],["Milecki","Leo",""],["Vakalopoulou","Maria",""],["Oudot","Steve",""],["Kalogeiton","Vicky",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 11:05:14 GMT"}],"updateDate":"2024-07-08","timestamp":1720091114000,"abstract":"  Multimodality has recently gained attention in the medical domain, where\nimaging or video modalities may be integrated with biomedical signals or health\nrecords. Yet, two challenges remain: balancing the contributions of modalities,\nespecially in cases with a limited amount of data available, and tackling\nmissing modalities. To address both issues, in this paper, we introduce the\nAnchoreD multimodAl Physiological Transformer (ADAPT), a multimodal, scalable\nframework with two key components: (i) aligning all modalities in the space of\nthe strongest, richest modality (called anchor) to learn a joint embedding\nspace, and (ii) a Masked Multimodal Transformer, leveraging both inter- and\nintra-modality correlations while handling missing modalities. We focus on\ndetecting physiological changes in two real-life scenarios: stress in\nindividuals induced by specific triggers and fighter pilots' loss of\nconsciousness induced by $g$-forces. We validate the generalizability of ADAPT\nthrough extensive experiments on two datasets for these tasks, where we set the\nnew state of the art while demonstrating its robustness across various modality\nscenarios and its high potential for real-life applications.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}