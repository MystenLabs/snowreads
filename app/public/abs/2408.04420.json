{"id":"2408.04420","title":"Recognizing Emotion Regulation Strategies from Human Behavior with Large\n  Language Models","authors":"Philipp M\\\"uller, Alexander Heimerl, Sayed Muddashir Hossain, Lea\n  Siegel, Jan Alexandersson, Patrick Gebhard, Elisabeth Andr\\'e, Tanja\n  Schneeberger","authorsParsed":[["Müller","Philipp",""],["Heimerl","Alexander",""],["Hossain","Sayed Muddashir",""],["Siegel","Lea",""],["Alexandersson","Jan",""],["Gebhard","Patrick",""],["André","Elisabeth",""],["Schneeberger","Tanja",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 12:47:10 GMT"}],"updateDate":"2024-08-09","timestamp":1723121230000,"abstract":"  Human emotions are often not expressed directly, but regulated according to\ninternal processes and social display rules. For affective computing systems,\nan understanding of how users regulate their emotions can be highly useful, for\nexample to provide feedback in job interview training, or in psychotherapeutic\nscenarios. However, at present no method to automatically classify different\nemotion regulation strategies in a cross-user scenario exists. At the same\ntime, recent studies showed that instruction-tuned Large Language Models (LLMs)\ncan reach impressive performance across a variety of affect recognition tasks\nsuch as categorical emotion recognition or sentiment analysis. While these\nresults are promising, it remains unclear to what extent the representational\npower of LLMs can be utilized in the more subtle task of classifying users'\ninternal emotion regulation strategy. To close this gap, we make use of the\nrecently introduced \\textsc{Deep} corpus for modeling the social display of the\nemotion shame, where each point in time is annotated with one of seven\ndifferent emotion regulation classes. We fine-tune Llama2-7B as well as the\nrecently introduced Gemma model using Low-rank Optimization on prompts\ngenerated from different sources of information on the \\textsc{Deep} corpus.\nThese include verbal and nonverbal behavior, person factors, as well as the\nresults of an in-depth interview after the interaction. Our results show, that\na fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation\nstrategy with high accuracy (0.84) without needing access to data from\npost-interaction interviews. This represents a significant improvement over\nprevious approaches based on Bayesian Networks and highlights the importance of\nmodeling verbal behavior in emotion regulation.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}