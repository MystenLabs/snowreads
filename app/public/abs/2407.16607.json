{"id":"2407.16607","title":"Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?","authors":"Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith","authorsParsed":[["Hayase","Jonathan",""],["Liu","Alisa",""],["Choi","Yejin",""],["Oh","Sewoong",""],["Smith","Noah A.",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 16:13:22 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 23:34:21 GMT"},{"version":"v3","created":"Thu, 5 Sep 2024 16:39:44 GMT"}],"updateDate":"2024-09-06","timestamp":1721751202000,"abstract":"  The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation: byte-pair encoding (BPE) tokenizers, used by the vast majority of\nmodern language models. Our key insight is that the ordered list of merge rules\nlearned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data. Given a tokenizer's merge list along with\nexample data for each category of interest, we formulate a linear program that\nsolves for the proportion of each category in the tokenizer's training set. In\ncontrolled experiments, we show that our attack recovers mixture ratios with\nhigh precision for tokenizers trained on known mixtures of natural languages,\nprogramming languages, and data sources. We then apply our approach to\noff-the-shelf tokenizers released with recent LMs. We confirm much publicly\ndisclosed information about these models, and also make several new inferences:\nGPT-4o and Mistral NeMo's tokenizers are much more multilingual than their\npredecessors, training on 39% and 47% non-English language data, respectively;\nLlama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use;\nGPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We\nhope our work sheds light on current design practices for pretraining data, and\ninspires continued research into data mixture inference for LMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}