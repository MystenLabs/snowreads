{"id":"2408.07196","title":"SeLoRA: Self-Expanding Low-Rank Adaptation of Latent Diffusion Model for\n  Medical Image Synthesis","authors":"Yuchen Mao, Hongwei Li, Wei Pang, Giorgos Papanastasiou, Guang Yang,\n  Chengjia Wang","authorsParsed":[["Mao","Yuchen",""],["Li","Hongwei",""],["Pang","Wei",""],["Papanastasiou","Giorgos",""],["Yang","Guang",""],["Wang","Chengjia",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 20:32:02 GMT"}],"updateDate":"2024-08-15","timestamp":1723581122000,"abstract":"  The persistent challenge of medical image synthesis posed by the scarcity of\nannotated data and the need to synthesize `missing modalities' for multi-modal\nanalysis, underscored the imperative development of effective synthesis\nmethods. Recently, the combination of Low-Rank Adaptation (LoRA) with latent\ndiffusion models (LDMs) has emerged as a viable approach for efficiently\nadapting pre-trained large language models, in the medical field. However, the\ndirect application of LoRA assumes uniform ranking across all linear layers,\noverlooking the significance of different weight matrices, and leading to\nsub-optimal outcomes. Prior works on LoRA prioritize the reduction of trainable\nparameters, and there exists an opportunity to further tailor this adaptation\nprocess to the intricate demands of medical image synthesis. In response, we\npresent SeLoRA, a Self-Expanding Low-Rank Adaptation Module, that dynamically\nexpands its ranking across layers during training, strategically placing\nadditional ranks on crucial layers, to allow the model to elevate synthesis\nquality where it matters most. The proposed method not only enables LDMs to\nfine-tune on medical data efficiently but also empowers the model to achieve\nimproved image quality with minimal ranking. The code of our SeLoRA method is\npublicly available on https://anonymous.4open.science/r/SeLoRA-980D .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}