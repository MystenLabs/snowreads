{"id":"2408.10795","title":"Adversarial Attack for Explanation Robustness of Rationalization Models","authors":"Yuankai Zhang, Lingxiao Kong, Haozhao Wang, Ruixuan Li, Jun Wang,\n  Yuhua Li, Wei Liu","authorsParsed":[["Zhang","Yuankai",""],["Kong","Lingxiao",""],["Wang","Haozhao",""],["Li","Ruixuan",""],["Wang","Jun",""],["Li","Yuhua",""],["Liu","Wei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 12:43:58 GMT"},{"version":"v2","created":"Wed, 18 Sep 2024 12:44:28 GMT"},{"version":"v3","created":"Thu, 19 Sep 2024 07:24:02 GMT"}],"updateDate":"2024-09-20","timestamp":1724157838000,"abstract":"  Rationalization models, which select a subset of input text as\nrationale-crucial for humans to understand and trust predictions-have recently\nemerged as a prominent research area in eXplainable Artificial Intelligence.\nHowever, most of previous studies mainly focus on improving the quality of the\nrationale, ignoring its robustness to malicious attack. Specifically, whether\nthe rationalization models can still generate high-quality rationale under the\nadversarial attack remains unknown. To explore this, this paper proposes UAT2E,\nwhich aims to undermine the explainability of rationalization models without\naltering their predictions, thereby eliciting distrust in these models from\nhuman users. UAT2E employs the gradient-based search on triggers and then\ninserts them into the original input to conduct both the non-target and target\nattack. Experimental results on five datasets reveal the vulnerability of\nrationalization models in terms of explanation, where they tend to select more\nmeaningless tokens under attacks. Based on this, we make a series of\nrecommendations for improving rationalization models in terms of explanation.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}