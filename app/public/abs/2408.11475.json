{"id":"2408.11475","title":"TrackGo: A Flexible and Efficient Method for Controllable Video\n  Generation","authors":"Haitao Zhou, Chuang Wang, Rui Nie, Jinxiao Lin, Dongdong Yu, Qian Yu,\n  Changhu Wang","authorsParsed":[["Zhou","Haitao",""],["Wang","Chuang",""],["Nie","Rui",""],["Lin","Jinxiao",""],["Yu","Dongdong",""],["Yu","Qian",""],["Wang","Changhu",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 09:42:04 GMT"}],"updateDate":"2024-08-22","timestamp":1724233324000,"abstract":"  Recent years have seen substantial progress in diffusion-based controllable\nvideo generation. However, achieving precise control in complex scenarios,\nincluding fine-grained object parts, sophisticated motion trajectories, and\ncoherent background movement, remains a challenge. In this paper, we introduce\nTrackGo, a novel approach that leverages free-form masks and arrows for\nconditional video generation. This method offers users with a flexible and\nprecise mechanism for manipulating video content. We also propose the\nTrackAdapter for control implementation, an efficient and lightweight adapter\ndesigned to be seamlessly integrated into the temporal self-attention layers of\na pretrained video generation model. This design leverages our observation that\nthe attention map of these layers can accurately activate regions corresponding\nto motion in videos. Our experimental results demonstrate that our new\napproach, enhanced by the TrackAdapter, achieves state-of-the-art performance\non key metrics such as FVD, FID, and ObjMC scores. The project page of TrackGo\ncan be found at: https://zhtjtcz.github.io/TrackGo-Page/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jcxP-YxSey9WUDpr-UHIy2SE1fKxdlLptOeImOi3eOU","pdfSize":"3201235"}
