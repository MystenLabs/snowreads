{"id":"2407.13163","title":"ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for\n  Recommender Systems","authors":"Yi Zhang, Ruihong Qiu, Jiajun Liu and Sen Wang","authorsParsed":[["Zhang","Yi",""],["Qiu","Ruihong",""],["Liu","Jiajun",""],["Wang","Sen",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 05:07:11 GMT"}],"updateDate":"2024-07-19","timestamp":1721279231000,"abstract":"  Offline reinforcement learning (RL) is an effective tool for real-world\nrecommender systems with its capacity to model the dynamic interest of users\nand its interactive nature. Most existing offline RL recommender systems focus\non model-based RL through learning a world model from offline data and building\nthe recommendation policy by interacting with this model. Although these\nmethods have made progress in the recommendation performance, the effectiveness\nof model-based offline RL methods is often constrained by the accuracy of the\nestimation of the reward model and the model uncertainties, primarily due to\nthe extreme discrepancy between offline logged data and real-world data in user\ninteractions with online platforms. To fill this gap, a more accurate reward\nmodel and uncertainty estimation are needed for the model-based RL methods. In\nthis paper, a novel model-based Reward Shaping in Offline Reinforcement\nLearning for Recommender Systems, ROLeR, is proposed for reward and uncertainty\nestimation in recommendation systems. Specifically, a non-parametric reward\nshaping method is designed to refine the reward model. In addition, a flexible\nand more representative uncertainty penalty is designed to fit the needs of\nrecommendation systems. Extensive experiments conducted on four benchmark\ndatasets showcase that ROLeR achieves state-of-the-art performance compared\nwith existing baselines. The source code can be downloaded at\nhttps://github.com/ArronDZhang/ROLeR.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}