{"id":"2408.13679","title":"Segment Any Mesh: Zero-shot Mesh Part Segmentation via Lifting Segment\n  Anything 2 to 3D","authors":"George Tang, William Zhao, Logan Ford, David Benhaim, Paul Zhang","authorsParsed":[["Tang","George",""],["Zhao","William",""],["Ford","Logan",""],["Benhaim","David",""],["Zhang","Paul",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 22:05:04 GMT"}],"updateDate":"2024-08-27","timestamp":1724537104000,"abstract":"  We propose Segment Any Mesh (SAMesh), a novel zero-shot method for mesh part\nsegmentation that overcomes the limitations of shape analysis-based,\nlearning-based, and current zero-shot approaches. SAMesh operates in two\nphases: multimodal rendering and 2D-to-3D lifting. In the first phase,\nmultiview renders of the mesh are individually processed through Segment\nAnything 2 (SAM2) to generate 2D masks. These masks are then lifted into a mesh\npart segmentation by associating masks that refer to the same mesh part across\nthe multiview renders. We find that applying SAM2 to multimodal feature renders\nof normals and shape diameter scalars achieves better results than using only\nuntextured renders of meshes. By building our method on top of SAM2, we\nseamlessly inherit any future improvements made to 2D segmentation. We compare\nour method with a robust, well-evaluated shape analysis method, Shape Diameter\nFunction (ShapeDiam), and show our method is comparable to or exceeds its\nperformance. Since current benchmarks contain limited object diversity, we also\ncurate and release a dataset of generated meshes and use it to demonstrate our\nmethod's improved generalization over ShapeDiam via human evaluation. We\nrelease the code and dataset at https://github.com/gtangg12/samesh\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}