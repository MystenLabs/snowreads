{"id":"2408.08776","title":"NEAR: A Training-Free Pre-Estimator of Machine Learning Model\n  Performance","authors":"Raphael T. Husistein, Markus Reiher, and Marco Eckhoff","authorsParsed":[["Husistein","Raphael T.",""],["Reiher","Markus",""],["Eckhoff","Marco",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 14:38:14 GMT"}],"updateDate":"2024-08-19","timestamp":1723819094000,"abstract":"  Artificial neural networks have been shown to be state-of-the-art machine\nlearning models in a wide variety of applications, including natural language\nprocessing and image recognition. However, building a performant neural network\nis a laborious task and requires substantial computing power. Neural\nArchitecture Search (NAS) addresses this issue by an automatic selection of the\noptimal network from a set of potential candidates. While many NAS methods\nstill require training of (some) neural networks, zero-cost proxies promise to\nidentify the optimal network without training. In this work, we propose the\nzero-cost proxy Network Expressivity by Activation Rank (NEAR). It is based on\nthe effective rank of the pre- and post-activation matrix, i.e., the values of\na neural network layer before and after applying its activation function. We\ndemonstrate the cutting-edge correlation between this network score and the\nmodel accuracy on NAS-Bench-101 and NATS-Bench-SSS/TSS. In addition, we present\na simple approach to estimate the optimal layer sizes in multi-layer\nperceptrons. Furthermore, we show that this score can be utilized to select\nhyperparameters such as the activation function and the neural network weight\ninitialization scheme.\n","subjects":["Computing Research Repository/Machine Learning","Condensed Matter/Disordered Systems and Neural Networks","Physics/Chemical Physics","Physics/Data Analysis, Statistics and Probability"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}