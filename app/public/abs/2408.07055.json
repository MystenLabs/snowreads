{"id":"2408.07055","title":"LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs","authors":"Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou,\n  Yuxiao Dong, Jie Tang, Juanzi Li","authorsParsed":[["Bai","Yushi",""],["Zhang","Jiajie",""],["Lv","Xin",""],["Zheng","Linzhi",""],["Zhu","Siqi",""],["Hou","Lei",""],["Dong","Yuxiao",""],["Tang","Jie",""],["Li","Juanzi",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 17:46:12 GMT"}],"updateDate":"2024-08-14","timestamp":1723571172000,"abstract":"  Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}