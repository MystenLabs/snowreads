{"id":"2407.21757","title":"Learning Video Context as Interleaved Multimodal Sequences","authors":"Kevin Qinghong Lin, Pengchuan Zhang, Difei Gao, Xide Xia, Joya Chen,\n  Ziteng Gao, Jinheng Xie, Xuhong Xiao, Mike Zheng Shou","authorsParsed":[["Lin","Kevin Qinghong",""],["Zhang","Pengchuan",""],["Gao","Difei",""],["Xia","Xide",""],["Chen","Joya",""],["Gao","Ziteng",""],["Xie","Jinheng",""],["Xiao","Xuhong",""],["Shou","Mike Zheng",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 17:23:57 GMT"},{"version":"v2","created":"Thu, 12 Sep 2024 14:01:56 GMT"}],"updateDate":"2024-09-13","timestamp":1722446637000,"abstract":"  Narrative videos, such as movies, pose significant challenges in video\nunderstanding due to their rich contexts (characters, dialogues, storylines)\nand diverse demands (identify who, relationship, and reason). In this paper, we\nintroduce MovieSeq, a multimodal language model developed to address the wide\nrange of challenges in understanding video contexts. Our core idea is to\nrepresent videos as interleaved multimodal sequences (including images, plots,\nvideos, and subtitles), either by linking external knowledge databases or using\noffline models (such as whisper for subtitles). Through instruction-tuning,\nthis approach empowers the language model to interact with videos using\ninterleaved multimodal instructions. For example, instead of solely relying on\nvideo as input, we jointly provide character photos alongside their names and\ndialogues, allowing the model to associate these elements and generate more\ncomprehensive responses. To demonstrate its effectiveness, we validate\nMovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)\nacross five settings (video classification, audio description, video-text\nretrieval, video captioning, and video question-answering). The code will be\npublic at https://github.com/showlab/MovieSeq.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}