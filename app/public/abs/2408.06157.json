{"id":"2408.06157","title":"Novel View Synthesis from a Single Image with Pretrained Diffusion\n  Guidance","authors":"Taewon Kang, Divya Kothandaraman, Dinesh Manocha, Ming C. Lin","authorsParsed":[["Kang","Taewon",""],["Kothandaraman","Divya",""],["Manocha","Dinesh",""],["Lin","Ming C.",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 13:53:40 GMT"}],"updateDate":"2024-08-13","timestamp":1723470820000,"abstract":"  Recent 3D novel view synthesis (NVS) methods are limited to\nsingle-object-centric scenes generated from new viewpoints and struggle with\ncomplex environments. They often require extensive 3D data for training,\nlacking generalization beyond training distribution. Conversely, 3D-free\nmethods can generate text-controlled views of complex, in-the-wild scenes using\na pretrained stable diffusion model without tedious fine-tuning, but lack\ncamera control. In this paper, we introduce HawkI++, a method capable of\ngenerating camera-controlled viewpoints from a single input image. HawkI++\nexcels in handling complex and diverse scenes without additional 3D data or\nextensive training. It leverages widely available pretrained NVS models for\nweak guidance, integrating this knowledge into a 3D-free view synthesis\napproach to achieve the desired results efficiently. Our experimental results\ndemonstrate that HawkI++ outperforms existing models in both qualitative and\nquantitative evaluations, providing high-fidelity and consistent novel view\nsynthesis at desired camera angles across a wide variety of scenes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}