{"id":"2408.12844","title":"Predicting Affective States from Screen Text Sentiment","authors":"Songyan Teng, Tianyi Zhang, Simon D'Alfonso, Vassilis Kostakos","authorsParsed":[["Teng","Songyan",""],["Zhang","Tianyi",""],["D'Alfonso","Simon",""],["Kostakos","Vassilis",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 05:25:11 GMT"}],"updateDate":"2024-08-26","timestamp":1724390711000,"abstract":"  The proliferation of mobile sensing technologies has enabled the study of\nvarious physiological and behavioural phenomena through unobtrusive data\ncollection from smartphone sensors. This approach offers real-time insights\ninto individuals' physical and mental states, creating opportunities for\npersonalised treatment and interventions. However, the potential of analysing\nthe textual content viewed on smartphones to predict affective states remains\nunderexplored. To better understand how the screen text that users are exposed\nto and interact with can influence their affects, we investigated a subset of\ndata obtained from a digital phenotyping study of Australian university\nstudents conducted in 2023. We employed linear regression, zero-shot, and\nmulti-shot prompting using a large language model (LLM) to analyse\nrelationships between screen text and affective states. Our findings indicate\nthat multi-shot prompting substantially outperforms both linear regression and\nzero-shot prompting, highlighting the importance of context in affect\nprediction. We discuss the value of incorporating textual and sentiment data\nfor improving affect prediction, providing a basis for future advancements in\nunderstanding smartphone use and wellbeing.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yycVCQbOtbYxuYoJ2VQjUq73RbQjPrkDx_k9RhUEyWc","pdfSize":"653078"}
