{"id":"2408.11393","title":"First Activations Matter: Training-Free Methods for Dynamic Activation\n  in Large Language Models","authors":"Chi Ma, Mincong Huang, Ying Zhang, Chao Wang, Yujie Wang, Lei Yu,\n  Chuan Liu, Wei Lin","authorsParsed":[["Ma","Chi",""],["Huang","Mincong",""],["Zhang","Ying",""],["Wang","Chao",""],["Wang","Yujie",""],["Yu","Lei",""],["Liu","Chuan",""],["Lin","Wei",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 07:38:51 GMT"}],"updateDate":"2024-08-22","timestamp":1724225931000,"abstract":"  Dynamic activation (DA) techniques, such as DejaVu and MoEfication, have\ndemonstrated their potential to significantly enhance the inference efficiency\nof large language models (LLMs). However, these techniques often rely on ReLU\nactivation functions or require additional parameters and training to maintain\nperformance. This paper introduces a training-free Threshold-based Dynamic\nActivation(TDA) method that leverage sequence information to exploit the\ninherent sparsity of models across various architectures. This method is\ndesigned to accelerate generation speed by 18-25\\% without significantly\ncompromising task performance, thereby addressing the limitations of existing\nDA techniques. Moreover, we delve into the root causes of LLM sparsity and\ntheoretically analyze two of its critical features: history-related activation\nuncertainty and semantic-irrelevant activation inertia. Our comprehensive\nanalyses not only provide a robust theoretical foundation for DA methods but\nalso offer valuable insights to guide future research in optimizing LLMs for\ngreater efficiency and effectiveness.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}