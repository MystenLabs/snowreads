{"id":"2407.10114","title":"TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley\n  Value Estimation","authors":"Roni Goldshmidt, Miriam Horovicz","authorsParsed":[["Goldshmidt","Roni",""],["Horovicz","Miriam",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 08:07:50 GMT"},{"version":"v2","created":"Mon, 22 Jul 2024 08:59:07 GMT"}],"updateDate":"2024-07-23","timestamp":1720944470000,"abstract":"  As large language models (LLMs) become increasingly prevalent in critical\napplications, the need for interpretable AI has grown. We introduce TokenSHAP,\na novel method for interpreting LLMs by attributing importance to individual\ntokens or substrings within input prompts. This approach adapts Shapley values\nfrom cooperative game theory to natural language processing, offering a\nrigorous framework for understanding how different parts of an input contribute\nto a model's response. TokenSHAP leverages Monte Carlo sampling for\ncomputational efficiency, providing interpretable, quantitative measures of\ntoken importance. We demonstrate its efficacy across diverse prompts and LLM\narchitectures, showing consistent improvements over existing baselines in\nalignment with human judgments, faithfulness to model behavior, and\nconsistency.\n  Our method's ability to capture nuanced interactions between tokens provides\nvaluable insights into LLM behavior, enhancing model transparency, improving\nprompt engineering, and aiding in the development of more reliable AI systems.\nTokenSHAP represents a significant step towards the necessary interpretability\nfor responsible AI deployment, contributing to the broader goal of creating\nmore transparent, accountable, and trustworthy AI systems.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"obWQd5_411xK7hTlf0aEW828OswVQ-tqH4ItX_hPDUA","pdfSize":"373944","objectId":"0xf9ed7071f11b779683d38ce4395b17e9787aee46d9905168886979d4d64f5569","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
