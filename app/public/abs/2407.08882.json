{"id":"2407.08882","title":"Emerging Practices for Large Multimodal Model (LMM) Assistance for\n  People with Visual Impairments: Implications for Design","authors":"Jingyi Xie, Rui Yu, He Zhang, Sooyeon Lee, Syed Masum Billah, John M.\n  Carroll","authorsParsed":[["Xie","Jingyi",""],["Yu","Rui",""],["Zhang","He",""],["Lee","Sooyeon",""],["Billah","Syed Masum",""],["Carroll","John M.",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 22:11:52 GMT"}],"updateDate":"2024-07-15","timestamp":1720735912000,"abstract":"  People with visual impairments perceive their environment non-visually and\noften use AI-powered assistive tools to obtain textual descriptions of visual\ninformation. Recent large vision-language model-based AI-powered tools like Be\nMy AI are more capable of understanding users' inquiries in natural language\nand describing the scene in audible text; however, the extent to which these\ntools are useful to visually impaired users is currently understudied. This\npaper aims to fill this gap. Our study with 14 visually impaired users reveals\nthat they are adapting these tools organically -- not only can these tools\nfacilitate complex interactions in household, spatial, and social contexts, but\nthey also act as an extension of users' cognition, as if the cognition were\ndistributed in the visual information. We also found that although the tools\nare currently not goal-oriented, users accommodate this limitation and embrace\nthe tools' capabilities for broader use. These findings enable us to envision\ndesign implications for creating more goal-oriented, real-time processing, and\nreliable AI-powered assistive technology.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}