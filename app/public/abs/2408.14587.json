{"id":"2408.14587","title":"Efficient fine-tuning of 37-level GraphCast with the Canadian global\n  deterministic analysis","authors":"Christopher Subich","authorsParsed":[["Subich","Christopher",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 19:16:08 GMT"}],"updateDate":"2024-08-28","timestamp":1724699768000,"abstract":"  This work describes a process for efficiently fine-tuning the GraphCast\ndata-driven forecast model to simulate another analysis system, here the Global\nDeterministic Prediction System (GDPS) of Environment and Climate Change Canada\n(ECCC). Using two years of training data (July 2019 -- December 2021) and 37\nGPU-days of computation to tune the 37-level, quarter-degree version of\nGraphCast, the resulting model significantly outperforms both the unmodified\nGraphCast and operational forecast, showing significant forecast skill in the\ntroposphere over lead times from 1 to 10 days. This fine-tuning is accomplished\nthrough abbreviating DeepMind's original training curriculum for GraphCast,\nrelying on a shorter single-step forecast stage to accomplish the bulk of the\nadaptation work and consolidating the autoregressive stages into separate 12hr,\n1d, 2d, and 3d stages with larger learning rates. Additionally, training over\n3d forecasts is split into two sub-steps to conserve host memory while\nmaintaining a strong correlation with training over the full period.\n","subjects":["Computing Research Repository/Machine Learning","Physics/Atmospheric and Oceanic Physics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}