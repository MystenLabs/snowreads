{"id":"2407.01445","title":"FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training\n  with Limited Resources","authors":"Xiyuan Wei, Fanjiang Ye, Ori Yonay, Xingyu Chen, Baixi Sun, Dingwen\n  Tao, Tianbao Yang","authorsParsed":[["Wei","Xiyuan",""],["Ye","Fanjiang",""],["Yonay","Ori",""],["Chen","Xingyu",""],["Sun","Baixi",""],["Tao","Dingwen",""],["Yang","Tianbao",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 16:37:18 GMT"},{"version":"v2","created":"Mon, 29 Jul 2024 15:04:15 GMT"}],"updateDate":"2024-07-30","timestamp":1719851838000,"abstract":"  Existing studies of training state-of-the-art Contrastive Language-Image\nPretraining (CLIP) models on large-scale data involve hundreds of or even\nthousands of GPUs due to the requirement of a large batch size. However, such a\nlarge amount of resources is not accessible to most people. While advanced\ncompositional optimization techniques for optimizing global contrastive losses\nhave been demonstrated effective for removing the requirement of large batch\nsize, their performance on large-scale data remains underexplored and not\noptimized. To bridge the gap, this paper explores several aspects of CLIP\ntraining with limited resources (e.g., up to tens of GPUs). First, we introduce\nFastCLIP, a general CLIP training framework built on advanced compositional\noptimization techniques while designed and optimized for the distributed\nsetting. Our framework is equipped with an efficient gradient reduction\nstrategy to reduce communication overhead. Second, to further boost training\nefficiency, we investigate three components of the framework from an\noptimization perspective: the schedule of the inner learning rate, the update\nrules of the temperature parameter and the model parameters, respectively.\nExperiments on different strategies for each component shed light on how to\nconduct CLIP training more efficiently. Finally, we benchmark the performance\nof FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different\ncompute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7\nmillion, 9.1 million to 315 million image-text pairs to demonstrate the\nsignificant improvement of FastCLIP in the resource-limited setting. We release\nthe code of FastCLIP at https://github.com/Optimization-AI/fast_clip .\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8nrAV9BmNYkK2Q8-FtWjT0Lfe2ygbZCMhCdf1ht7LXc","pdfSize":"1069402","objectId":"0x2229cd060218d907f6a609cb86598bc19568a9de33cc4c214f013ffc48b3b64d","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
