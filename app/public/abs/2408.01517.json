{"id":"2408.01517","title":"Gradient flow in parameter space is equivalent to linear interpolation\n  in output space","authors":"Thomas Chen, Patr\\'icia Mu\\~noz Ewald","authorsParsed":[["Chen","Thomas",""],["Ewald","Patrícia Muñoz",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 18:23:17 GMT"}],"updateDate":"2024-08-06","timestamp":1722622997000,"abstract":"  We prove that the usual gradient flow in parameter space that underlies many\ntraining algorithms for neural networks in deep learning can be continuously\ndeformed into an adapted gradient flow which yields (constrained) Euclidean\ngradient flow in output space. Moreover, if the Jacobian of the outputs with\nrespect to the parameters is full rank (for fixed training data), then the time\nvariable can be reparametrized so that the resulting flow is simply linear\ninterpolation, and a global minimum can be achieved.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Physics/Mathematical Physics","Mathematics/Mathematical Physics","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}