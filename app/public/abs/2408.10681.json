{"id":"2408.10681","title":"HMoE: Heterogeneous Mixture of Experts for Language Modeling","authors":"An Wang, Xingwu Sun, Ruobing Xie, Shuaipeng Li, Jiaqi Zhu, Zhen Yang,\n  Pinxue Zhao, J.N.Han, Zhanhui Kang, Di Wang, Naoaki Okazaki, Cheng-zhong Xu","authorsParsed":[["Wang","An",""],["Sun","Xingwu",""],["Xie","Ruobing",""],["Li","Shuaipeng",""],["Zhu","Jiaqi",""],["Yang","Zhen",""],["Zhao","Pinxue",""],["Han","J. N.",""],["Kang","Zhanhui",""],["Wang","Di",""],["Okazaki","Naoaki",""],["Xu","Cheng-zhong",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 09:35:24 GMT"}],"updateDate":"2024-08-21","timestamp":1724146524000,"abstract":"  Mixture of Experts (MoE) offers remarkable performance and computational\nefficiency by selectively activating subsets of model parameters.\nTraditionally, MoE models use homogeneous experts, each with identical\ncapacity. However, varying complexity in input data necessitates experts with\ndiverse capabilities, while homogeneous MoE hinders effective expert\nspecialization and efficient parameter utilization. In this study, we propose a\nnovel Heterogeneous Mixture of Experts (HMoE), where experts differ in size and\nthus possess diverse capacities. This heterogeneity allows for more specialized\nexperts to handle varying token complexities more effectively. To address the\nimbalance in expert activation, we propose a novel training objective that\nencourages the frequent activation of smaller experts, enhancing computational\nefficiency and parameter utilization. Extensive experiments demonstrate that\nHMoE achieves lower loss with fewer activated parameters and outperforms\nconventional homogeneous MoE models on various pre-training evaluation\nbenchmarks. Codes will be released upon acceptance.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Up54Cnihjdnn5jbobpA47T6JV9BWMHko3BYi8Q1dABE","pdfSize":"882056"}
