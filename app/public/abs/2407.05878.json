{"id":"2407.05878","title":"HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution","authors":"Xiang Zhang and Yulun Zhang and Fisher Yu","authorsParsed":[["Zhang","Xiang",""],["Zhang","Yulun",""],["Yu","Fisher",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 12:42:10 GMT"}],"updateDate":"2024-07-09","timestamp":1720442530000,"abstract":"  Transformers have exhibited promising performance in computer vision tasks\nincluding image super-resolution (SR). However, popular transformer-based SR\nmethods often employ window self-attention with quadratic computational\ncomplexity to window sizes, resulting in fixed small windows with limited\nreceptive fields. In this paper, we present a general strategy to convert\ntransformer-based SR networks to hierarchical transformers (HiT-SR), boosting\nSR performance with multi-scale features while maintaining an efficient design.\nSpecifically, we first replace the commonly used fixed small windows with\nexpanding hierarchical windows to aggregate features at different scales and\nestablish long-range dependencies. Considering the intensive computation\nrequired for large windows, we further design a spatial-channel correlation\nmethod with linear complexity to window sizes, efficiently gathering spatial\nand channel information from hierarchical windows. Extensive experiments verify\nthe effectiveness and efficiency of our HiT-SR, and our improved versions of\nSwinIR-Light, SwinIR-NG, and SRFormer-Light yield state-of-the-art SR results\nwith fewer parameters, FLOPs, and faster speeds ($\\sim7\\times$).\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}