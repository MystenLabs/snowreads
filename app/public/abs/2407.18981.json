{"id":"2407.18981","title":"Prompt Injection Attacks on Large Language Models in Oncology","authors":"Jan Clusmann, Dyke Ferber, Isabella C. Wiest, Carolin V. Schneider,\n  Titus J. Brinker, Sebastian Foersch, Daniel Truhn, Jakob N. Kather","authorsParsed":[["Clusmann","Jan",""],["Ferber","Dyke",""],["Wiest","Isabella C.",""],["Schneider","Carolin V.",""],["Brinker","Titus J.",""],["Foersch","Sebastian",""],["Truhn","Daniel",""],["Kather","Jakob N.",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 15:29:57 GMT"}],"updateDate":"2024-07-30","timestamp":1721748597000,"abstract":"  Vision-language artificial intelligence models (VLMs) possess medical\nknowledge and can be employed in healthcare in numerous ways, including as\nimage interpreters, virtual scribes, and general decision support systems.\nHowever, here, we demonstrate that current VLMs applied to medical tasks\nexhibit a fundamental security flaw: they can be attacked by prompt injection\nattacks, which can be used to output harmful information just by interacting\nwith the VLM, without any access to its parameters. We performed a quantitative\nstudy to evaluate the vulnerabilities to these attacks in four state of the art\nVLMs which have been proposed to be of utility in healthcare: Claude 3 Opus,\nClaude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show\nthat all of these models are susceptible. Specifically, we show that embedding\nsub-visual prompts in medical imaging data can cause the model to provide\nharmful output, and that these prompts are non-obvious to human observers.\nThus, our study demonstrates a key vulnerability in medical VLMs which should\nbe mitigated before widespread clinical adoption.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UXvFwkeERlECr9xCzis0f9AEH3NSFLlgqaXJiaegL2o","pdfSize":"10320066"}
