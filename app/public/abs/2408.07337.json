{"id":"2408.07337","title":"KIND: Knowledge Integration and Diversion in Diffusion Models","authors":"Yucheng Xie, Fu Feng, Jing Wang, Xin Geng, Yong Rui","authorsParsed":[["Xie","Yucheng",""],["Feng","Fu",""],["Wang","Jing",""],["Geng","Xin",""],["Rui","Yong",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 07:22:28 GMT"}],"updateDate":"2024-08-15","timestamp":1723620148000,"abstract":"  Pre-trained models have become the preferred backbone due to the expansion of\nmodel parameters, with techniques like Parameter-Efficient Fine-Tuning (PEFTs)\ntypically fixing the parameters of these models. However, pre-trained models\nmay not always be optimal, especially when there are discrepancies between\ntraining tasks and target tasks, potentially resulting in negative transfer. To\naddress this, we introduce \\textbf{KIND}, which performs \\textbf{K}nowledge\n\\textbf{IN}tegration and \\textbf{D}iversion in diffusion models. KIND first\nintegrates knowledge by decomposing parameter matrices of models using $U$,\n$\\Sigma$, and $V$ matrices, formally inspired by singular value decomposition\n(SVD). Then it explicitly partitions the components of these matrices into\n\\textbf{learngenes} and \\textbf{tailors} to condense common and class-specific\nknowledge, respectively, through a class gate. In this way, KIND redefines\ntraditional pre-training methods by adjusting training objectives from\nmaximizing model performance on current tasks to condensing transferable common\nknowledge, leveraging the \\textit{Learngene} framework. We conduct experiments\non ImageNet-1K and compare KIND with PEFT and other learngene methods. Results\nindicate that KIND achieves state-of-the-art performance compared to other PEFT\nand learngene methods. Specifically, the images generated by KIND achieves more\nthan 6.54 and 1.07 decrease in FID and sFID on DiT-L/2, utilizing only 45.4M\ntrainable parameters and saving at least 35.4G FLOPs in computational cost.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}