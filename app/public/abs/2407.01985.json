{"id":"2407.01985","title":"The Epistemic Uncertainty Hole: an issue of Bayesian Neural Networks","authors":"Mohammed Fellaji and Fr\\'ed\\'eric Pennerath","authorsParsed":[["Fellaji","Mohammed",""],["Pennerath","Frédéric",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 06:54:46 GMT"}],"updateDate":"2024-07-03","timestamp":1719903286000,"abstract":"  Bayesian Deep Learning (BDL) gives access not only to aleatoric uncertainty,\nas standard neural networks already do, but also to epistemic uncertainty, a\nmeasure of confidence a model has in its own predictions. In this article, we\nshow through experiments that the evolution of epistemic uncertainty metrics\nregarding the model size and the size of the training set, goes against\ntheoretical expectations. More precisely, we observe that the epistemic\nuncertainty collapses literally in the presence of large models and sometimes\nalso of little training data, while we expect the exact opposite behaviour.\nThis phenomenon, which we call \"epistemic uncertainty hole\", is all the more\nproblematic as it undermines the entire applicative potential of BDL, which is\nbased precisely on the use of epistemic uncertainty. As an example, we evaluate\nthe practical consequences of this uncertainty hole on one of the main\napplications of BDL, namely the detection of out-of-distribution samples\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0fq3M8y8oYyEtZeUzgwfzzVLZkX_I9vfZF0t7umlvmU","pdfSize":"719749"}
