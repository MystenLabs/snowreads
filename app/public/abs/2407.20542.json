{"id":"2407.20542","title":"HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose\n  Estimation","authors":"Wencan Cheng and Eunji Kim and Jong Hwan Ko","authorsParsed":[["Cheng","Wencan",""],["Kim","Eunji",""],["Ko","Jong Hwan",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 04:53:35 GMT"}],"updateDate":"2024-07-31","timestamp":1722315215000,"abstract":"  The extraction of keypoint positions from input hand frames, known as 3D hand\npose estimation, is crucial for various human-computer interaction\napplications. However, current approaches often struggle with the dynamic\nnature of self-occlusion of hands and intra-occlusion with interacting objects.\nTo address this challenge, this paper proposes the Denoising Adaptive Graph\nTransformer, HandDAGT, for hand pose estimation. The proposed HandDAGT\nleverages a transformer structure to thoroughly explore effective geometric\nfeatures from input patches. Additionally, it incorporates a novel attention\nmechanism to adaptively weigh the contribution of kinematic correspondence and\nlocal geometric features for the estimation of specific keypoints. This\nattribute enables the model to adaptively employ kinematic and local\ninformation based on the occlusion situation, enhancing its robustness and\naccuracy. Furthermore, we introduce a novel denoising training strategy aimed\nat improving the model's robust performance in the face of occlusion\nchallenges. Experimental results show that the proposed model significantly\noutperforms the existing methods on four challenging hand pose benchmark\ndatasets. Codes and pre-trained models are publicly available at\nhttps://github.com/cwc1260/HandDAGT.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}