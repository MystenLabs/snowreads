{"id":"2408.11300","title":"Offline Policy Learning via Skill-step Abstraction for Long-horizon\n  Goal-Conditioned Tasks","authors":"Donghoon Kim, Minjong Yoo, Honguk Woo","authorsParsed":[["Kim","Donghoon",""],["Yoo","Minjong",""],["Woo","Honguk",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 03:05:06 GMT"}],"updateDate":"2024-08-22","timestamp":1724209506000,"abstract":"  Goal-conditioned (GC) policy learning often faces a challenge arising from\nthe sparsity of rewards, when confronting long-horizon goals. To address the\nchallenge, we explore skill-based GC policy learning in offline settings, where\nskills are acquired from existing data and long-horizon goals are decomposed\ninto sequences of near-term goals that align with these skills. Specifically,\nwe present an `offline GC policy learning via skill-step abstraction' framework\n(GLvSA) tailored for tackling long-horizon GC tasks affected by goal\ndistribution shifts. In the framework, a GC policy is progressively learned\noffline in conjunction with the incremental modeling of skill-step abstractions\non the data. We also devise a GC policy hierarchy that not only accelerates GC\npolicy learning within the framework but also allows for parameter-efficient\nfine-tuning of the policy. Through experiments with the maze and Franka kitchen\nenvironments, we demonstrate the superiority and efficiency of our GLvSA\nframework in adapting GC policies to a wide range of long-horizon goals. The\nframework achieves competitive zero-shot and few-shot adaptation performance,\noutperforming existing GC policy learning and skill-based methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"egHSFcxrTB4I4lgmBf1Zi_pBuwdBUVXicO_krGJrkoI","pdfSize":"913372"}
