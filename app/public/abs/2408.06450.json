{"id":"2408.06450","title":"Evaluating Language Models for Efficient Code Generation","authors":"Jiawei Liu and Songrun Xie and Junhao Wang and Yuxiang Wei and Yifeng\n  Ding and Lingming Zhang","authorsParsed":[["Liu","Jiawei",""],["Xie","Songrun",""],["Wang","Junhao",""],["Wei","Yuxiang",""],["Ding","Yifeng",""],["Zhang","Lingming",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 18:59:13 GMT"}],"updateDate":"2024-08-14","timestamp":1723489153000,"abstract":"  We introduce Differential Performance Evaluation (DPE), a framework designed\nto reliably evaluate Large Language Models (LLMs) for efficient code\ngeneration. Traditional coding benchmarks often fail to provide reliable\ninsights into code efficiency, due to their reliance on simplistic test inputs\nand the absence of effective compound metrics. DPE addresses these issues by\nfocusing on efficiency-demanding programming tasks and establishing an\ninsightful compound metric for performance evaluation. DPE operates in two\nphases: To curate efficiency datasets, it selects efficiency-demanding tasks\nfrom existing coding benchmarks and generates computationally expensive inputs\nto stress the efficiency of LLM solutions. To assess the code efficiency, DPE\nprofiles the new solution and compares it globally against a set of reference\nsolutions that exhibit distinct efficiency levels, where the matched level\ndefines its efficiency score. As a proof of concept, we use DPE to create\nEvalPerf, a benchmark with 121 performance-challenging coding tasks. Our\ncomprehensive evaluation draws interesting findings on the efficiency impact of\nmodel sizes, instruction tuning, and prompting. For example, while the scaling\nlaw fails to account for code efficiency, general instruction tuning benefits\nboth code correctness and efficiency. We also evaluate the evaluation by\nexamining the effectiveness of DPE, showing that EvalPerf is reliable and\nconvenient to use even across platforms.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}