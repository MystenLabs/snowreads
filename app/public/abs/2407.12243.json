{"id":"2407.12243","title":"Explaining Deep Neural Networks by Leveraging Intrinsic Methods","authors":"Biagio La Rosa","authorsParsed":[["La Rosa","Biagio",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 01:20:17 GMT"}],"updateDate":"2024-07-18","timestamp":1721179217000,"abstract":"  Despite their impact on the society, deep neural networks are often regarded\nas black-box models due to their intricate structures and the absence of\nexplanations for their decisions. This opacity poses a significant challenge to\nAI systems wider adoption and trustworthiness. This thesis addresses this issue\nby contributing to the field of eXplainable AI, focusing on enhancing the\ninterpretability of deep neural networks. The core contributions lie in\nintroducing novel techniques aimed at making these networks more interpretable\nby leveraging an analysis of their inner workings. Specifically, the\ncontributions are threefold. Firstly, the thesis introduces designs for\nself-explanatory deep neural networks, such as the integration of external\nmemory for interpretability purposes and the usage of prototype and\nconstraint-based layers across several domains. Secondly, this research delves\ninto novel investigations on neurons within trained deep neural networks,\nshedding light on overlooked phenomena related to their activation values.\nLastly, the thesis conducts an analysis of the application of explanatory\ntechniques in the field of visual analytics, exploring the maturity of their\nadoption and the potential of these systems to convey explanations to users\neffectively.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-sVMiOm-61eD6jQ7OdyjxbY6wb92-ldLRQeXWmElvWs","pdfSize":"18639133"}
