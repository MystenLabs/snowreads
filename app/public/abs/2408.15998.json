{"id":"2408.15998","title":"Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders","authors":"Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree\n  Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey\n  Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu","authorsParsed":[["Shi","Min",""],["Liu","Fuxiao",""],["Wang","Shihao",""],["Liao","Shijia",""],["Radhakrishnan","Subhashree",""],["Huang","De-An",""],["Yin","Hongxu",""],["Sapra","Karan",""],["Yacoob","Yaser",""],["Shi","Humphrey",""],["Catanzaro","Bryan",""],["Tao","Andrew",""],["Kautz","Jan",""],["Yu","Zhiding",""],["Liu","Guilin",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 17:59:31 GMT"}],"updateDate":"2024-08-29","timestamp":1724867971000,"abstract":"  The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}