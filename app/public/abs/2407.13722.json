{"id":"2407.13722","title":"Enhanced $H$-Consistency Bounds","authors":"Anqi Mao, Mehryar Mohri, Yutao Zhong","authorsParsed":[["Mao","Anqi",""],["Mohri","Mehryar",""],["Zhong","Yutao",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:22:40 GMT"}],"updateDate":"2024-07-19","timestamp":1721323360000,"abstract":"  Recent research has introduced a key notion of $H$-consistency bounds for\nsurrogate losses. These bounds offer finite-sample guarantees, quantifying the\nrelationship between the zero-one estimation error (or other target loss) and\nthe surrogate loss estimation error for a specific hypothesis set. However,\nprevious bounds were derived under the condition that a lower bound of the\nsurrogate loss conditional regret is given as a convex function of the target\nconditional regret, without non-constant factors depending on the predictor or\ninput instance. Can we derive finer and more favorable $H$-consistency bounds?\nIn this work, we relax this condition and present a general framework for\nestablishing enhanced $H$-consistency bounds based on more general inequalities\nrelating conditional regrets. Our theorems not only subsume existing results as\nspecial cases but also enable the derivation of more favorable bounds in\nvarious scenarios. These include standard multi-class classification, binary\nand multi-class classification under Tsybakov noise conditions, and bipartite\nranking.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}