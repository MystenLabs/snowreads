{"id":"2408.02131","title":"Model Hijacking Attack in Federated Learning","authors":"Zheng Li, Siyuan Wu, Ruichuan Chen, Paarijaat Aditya, Istemi Ekin\n  Akkus, Manohar Vanga, Min Zhang, Hao Li, Yang Zhang","authorsParsed":[["Li","Zheng",""],["Wu","Siyuan",""],["Chen","Ruichuan",""],["Aditya","Paarijaat",""],["Akkus","Istemi Ekin",""],["Vanga","Manohar",""],["Zhang","Min",""],["Li","Hao",""],["Zhang","Yang",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 20:02:07 GMT"}],"updateDate":"2024-08-06","timestamp":1722801727000,"abstract":"  Machine learning (ML), driven by prominent paradigms such as centralized and\nfederated learning, has made significant progress in various critical\napplications ranging from autonomous driving to face recognition. However, its\nremarkable success has been accompanied by various attacks. Recently, the model\nhijacking attack has shown that ML models can be hijacked to execute tasks\ndifferent from their original tasks, which increases both accountability and\nparasitic computational risks. Nevertheless, thus far, this attack has only\nfocused on centralized learning. In this work, we broaden the scope of this\nattack to the federated learning domain, where multiple clients collaboratively\ntrain a global model without sharing their data. Specifically, we present\nHijackFL, the first-of-its-kind hijacking attack against the global model in\nfederated learning. The adversary aims to force the global model to perform a\ndifferent task (called hijacking task) from its original task without the\nserver or benign client noticing. To accomplish this, unlike existing methods\nthat use data poisoning to modify the target model's parameters, HijackFL\nsearches for pixel-level perturbations based on their local model (without\nmodifications) to align hijacking samples with the original ones in the feature\nspace. When performing the hijacking task, the adversary applies these cloaks\nto the hijacking samples, compelling the global model to identify them as\noriginal samples and predict them accordingly. We conduct extensive experiments\non four benchmark datasets and three popular models. Empirical results\ndemonstrate that its attack performance outperforms baselines. We further\ninvestigate the factors that affect its performance and discuss possible\ndefenses to mitigate its impact.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XyZHif24xhUi_09dEhj8yv1mR33pUe1kg0RUuedige4","pdfSize":"1655429","txDigest":"2aXeEChQv7kYtz5vL7b2NbRmR2vXi6VLhHJY1zKvV98X","endEpoch":"1","status":"CERTIFIED"}
