{"id":"2408.10075","title":"Personalizing Reinforcement Learning from Human Feedback with\n  Variational Preference Learning","authors":"Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, Natasha\n  Jaques","authorsParsed":[["Poddar","Sriyash",""],["Wan","Yanming",""],["Ivison","Hamish",""],["Gupta","Abhishek",""],["Jaques","Natasha",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 15:18:30 GMT"}],"updateDate":"2024-08-20","timestamp":1724080710000,"abstract":"  Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for\naligning foundation models to human values and preferences. However, current\nRLHF techniques cannot account for the naturally occurring differences in\nindividual human preferences across a diverse population. When these\ndifferences arise, traditional RLHF frameworks simply average over them,\nleading to inaccurate rewards and poor performance for individual subgroups. To\naddress the need for pluralistic alignment, we develop a class of multimodal\nRLHF methods. Our proposed techniques are based on a latent variable\nformulation - inferring a novel user-specific latent and learning reward models\nand policies conditioned on this latent without additional user-specific data.\nWhile conceptually simple, we show that in practice, this reward modeling\nrequires careful algorithmic considerations around model architecture and\nreward scaling. To empirically validate our proposed technique, we first show\nthat it can provide a way to combat underspecification in simulated control\nproblems, inferring and optimizing user-specific reward functions. Next, we\nconduct experiments on pluralistic language datasets representing diverse user\npreferences and demonstrate improved reward function accuracy. We additionally\nshow the benefits of this probabilistic framework in terms of measuring\nuncertainty, and actively learning user preferences. This work enables learning\nfrom diverse populations of users with divergent preferences, an important\nchallenge that naturally occurs in problems from robot learning to foundation\nmodel alignment.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/"}