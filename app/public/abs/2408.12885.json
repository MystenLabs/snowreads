{"id":"2408.12885","title":"T3M: Text Guided 3D Human Motion Synthesis from Speech","authors":"Wenshuo Peng, Kaipeng Zhang, Sai Qian Zhang","authorsParsed":[["Peng","Wenshuo",""],["Zhang","Kaipeng",""],["Zhang","Sai Qian",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 07:37:26 GMT"}],"updateDate":"2024-08-26","timestamp":1724398646000,"abstract":"  Speech-driven 3D motion synthesis seeks to create lifelike animations based\non human speech, with potential uses in virtual reality, gaming, and the film\nproduction. Existing approaches reply solely on speech audio for motion\ngeneration, leading to inaccurate and inflexible synthesis results. To mitigate\nthis problem, we introduce a novel text-guided 3D human motion synthesis\nmethod, termed \\textit{T3M}. Unlike traditional approaches, T3M allows precise\ncontrol over motion synthesis via textual input, enhancing the degree of\ndiversity and user customization. The experiment results demonstrate that T3M\ncan greatly outperform the state-of-the-art methods in both quantitative\nmetrics and qualitative evaluations. We have publicly released our code at\n\\href{https://github.com/Gloria2tt/T3M.git}{https://github.com/Gloria2tt/T3M.git}\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}