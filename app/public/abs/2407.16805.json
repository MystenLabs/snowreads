{"id":"2407.16805","title":"TAMIGO: Empowering Teaching Assistants using LLM-assisted viva and code\n  assessment in an Advanced Computing Class","authors":"Anishka IIITD, Diksha Sethi, Nipun Gupta, Shikhar Sharma, Srishti\n  Jain, Ujjwal Singhal and Dhruv Kumar","authorsParsed":[["IIITD","Anishka",""],["Sethi","Diksha",""],["Gupta","Nipun",""],["Sharma","Shikhar",""],["Jain","Srishti",""],["Singhal","Ujjwal",""],["Kumar","Dhruv",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 19:12:13 GMT"}],"updateDate":"2024-07-25","timestamp":1721761933000,"abstract":"  Large Language Models (LLMs) have significantly transformed the educational\nlandscape, offering new tools for students, instructors, and teaching\nassistants. This paper investigates the application of LLMs in assisting\nteaching assistants (TAs) with viva and code assessments in an advanced\ncomputing class on distributed systems in an Indian University. We develop\nTAMIGO, an LLM-based system for TAs to evaluate programming assignments.\n  For viva assessment, the TAs generated questions using TAMIGO and circulated\nthese questions to the students for answering. The TAs then used TAMIGO to\ngenerate feedback on student answers. For code assessment, the TAs selected\nspecific code blocks from student code submissions and fed it to TAMIGO to\ngenerate feedback for these code blocks. The TAMIGO-generated feedback for\nstudent answers and code blocks was used by the TAs for further evaluation.\n  We evaluate the quality of LLM-generated viva questions, model answers,\nfeedback on viva answers, and feedback on student code submissions. Our results\nindicate that LLMs are highly effective at generating viva questions when\nprovided with sufficient context and background information. However, the\nresults for LLM-generated feedback on viva answers were mixed; instances of\nhallucination occasionally reduced the accuracy of feedback. Despite this, the\nfeedback was consistent, constructive, comprehensive, balanced, and did not\noverwhelm the TAs. Similarly, for code submissions, the LLM-generated feedback\nwas constructive, comprehensive and balanced, though there was room for\nimprovement in aligning the feedback with the instructor-provided rubric for\ncode evaluation. Our findings contribute to understanding the benefits and\nlimitations of integrating LLMs into educational settings.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Computers and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}