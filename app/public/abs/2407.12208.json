{"id":"2407.12208","title":"Computing $k$-means in mixed precision","authors":"Erin Carson, Xinye Chen, Xiaobo Liu","authorsParsed":[["Carson","Erin",""],["Chen","Xinye",""],["Liu","Xiaobo",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 22:48:35 GMT"}],"updateDate":"2024-07-18","timestamp":1721170115000,"abstract":"  The $k$-means algorithm is one of the most popular and critical techniques in\ndata mining and machine learning, and it has achieved significant success in\nnumerous science and engineering domains. Computing $k$-means to a global\noptimum is NP-hard in Euclidean space, yet there are a variety of efficient\nheuristic algorithms, such as Lloyd's algorithm, that converge to a local\noptimum with superpolynomial complexity in the worst case. Motivated by the\nemergence and prominence of mixed precision capabilities in hardware, a current\ntrend is to develop low and mixed precision variants of algorithms in order to\nimprove the runtime and energy consumption. In this paper we study the\nnumerical stability of Lloyd's $k$-means algorithm, and, in particular, we\nconfirm the stability of the widely used distance computation formula. We\npropose a mixed-precision framework for $k$-means computation and investigate\nthe effects of low-precision distance computation within the framework. Through\nextensive simulations on various data clustering and image segmentation tasks,\nwe verify the applicability and robustness of the mixed precision $k$-means\nmethod. We find that, in $k$-means computation, normalized data is more\ntolerant to the reduction of precision in the distance computation, while for\nnonnormalized data more care is needed in the use of reduced precision, mainly\nto avoid overflow. Our study demonstrates the potential for the use of mixed\nprecision to accelerate the $k$-means computation and offers some insights into\nother distance-based machine learning methods.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}