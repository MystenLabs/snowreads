{"id":"2408.11559","title":"Semi-supervised 3D Semantic Scene Completion with 2D Vision Foundation\n  Model Guidance","authors":"Duc-Hai Pham, Duc Dung Nguyen, Hoang-Anh Pham, Ho Lai Tuan, Phong Ha\n  Nguyen, Khoi Nguyen, Rang Nguyen","authorsParsed":[["Pham","Duc-Hai",""],["Nguyen","Duc Dung",""],["Pham","Hoang-Anh",""],["Tuan","Ho Lai",""],["Nguyen","Phong Ha",""],["Nguyen","Khoi",""],["Nguyen","Rang",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 12:13:18 GMT"},{"version":"v2","created":"Fri, 13 Sep 2024 03:57:47 GMT"}],"updateDate":"2024-09-16","timestamp":1724242398000,"abstract":"  Accurate prediction of 3D semantic occupancy from 2D visual images is vital\nin enabling autonomous agents to comprehend their surroundings for planning and\nnavigation. State-of-the-art methods typically employ fully supervised\napproaches, necessitating a huge labeled dataset acquired through expensive\nLiDAR sensors and meticulous voxel-wise labeling by human annotators. The\nresource-intensive nature of this annotating process significantly hampers the\napplication and scalability of these methods. We introduce a novel\nsemi-supervised framework to alleviate the dependency on densely annotated\ndata. Our approach leverages 2D foundation models to generate essential 3D\nscene geometric and semantic cues, facilitating a more efficient training\nprocess. Our framework exhibits notable properties: (1) Generalizability,\napplicable to various 3D semantic scene completion approaches, including 2D-3D\nlifting and 3D-2D transformer methods. (2) Effectiveness, as demonstrated\nthrough experiments on SemanticKITTI and NYUv2, wherein our method achieves up\nto 85% of the fully-supervised performance using only 10% labeled data. This\napproach not only reduces the cost and labor associated with data annotation\nbut also demonstrates the potential for broader adoption in camera-based\nsystems for 3D semantic occupancy prediction.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}