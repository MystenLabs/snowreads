{"id":"2408.01893","title":"Minimum Gamma Divergence for Regression and Classification Problems","authors":"Shinto Eguchi","authorsParsed":[["Eguchi","Shinto",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 01:07:44 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 06:35:26 GMT"}],"updateDate":"2024-09-04","timestamp":1722733664000,"abstract":"  The book is structured into four main chapters. Chapter 1 introduces the\nfoundational concepts of divergence measures, including the well-known\nKullback-Leibler divergence and its limitations. It then presents a detailed\nexploration of power divergences, such as the $\\alpha$, $\\beta$, and\n$\\gamma$-divergences, highlighting their unique properties and advantages.\nChapter 2 explores minimum divergence methods for regression models,\ndemonstrating how these methods can improve robustness and efficiency in\nstatistical estimation. Chapter 3 extends these methods to Poisson point\nprocesses, with a focus on ecological applications, providing a robust\nframework for modeling species distributions and other spatial phenomena.\nFinally, Chapter 4 explores the use of divergence measures in machine learning,\nincluding applications in Boltzmann machines, AdaBoost, and active learning.\nThe chapter emphasizes the practical benefits of these measures in enhancing\nmodel robustness and performance.\n","subjects":["Statistics/Methodology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}