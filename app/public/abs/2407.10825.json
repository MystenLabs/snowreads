{"id":"2407.10825","title":"Wicked Oddities: Selectively Poisoning for Effective Clean-Label\n  Backdoor Attacks","authors":"Quang H. Nguyen, Nguyen Ngoc-Hieu, The-Anh Ta, Thanh Nguyen-Tang,\n  Kok-Seng Wong, Hoang Thanh-Tung, Khoa D. Doan","authorsParsed":[["Nguyen","Quang H.",""],["Ngoc-Hieu","Nguyen",""],["Ta","The-Anh",""],["Nguyen-Tang","Thanh",""],["Wong","Kok-Seng",""],["Thanh-Tung","Hoang",""],["Doan","Khoa D.",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:38:21 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 04:21:12 GMT"}],"updateDate":"2024-07-17","timestamp":1721057901000,"abstract":"  Deep neural networks are vulnerable to backdoor attacks, a type of\nadversarial attack that poisons the training data to manipulate the behavior of\nmodels trained on such data. Clean-label attacks are a more stealthy form of\nbackdoor attacks that can perform the attack without changing the labels of\npoisoned data. Early works on clean-label attacks added triggers to a random\nsubset of the training set, ignoring the fact that samples contribute unequally\nto the attack's success. This results in high poisoning rates and low attack\nsuccess rates. To alleviate the problem, several supervised learning-based\nsample selection strategies have been proposed. However, these methods assume\naccess to the entire labeled training set and require training, which is\nexpensive and may not always be practical. This work studies a new and more\npractical (but also more challenging) threat model where the attacker only\nprovides data for the target class (e.g., in face recognition systems) and has\nno knowledge of the victim model or any other classes in the training set. We\nstudy different strategies for selectively poisoning a small set of training\nsamples in the target class to boost the attack success rate in this setting.\nOur threat model poses a serious threat in training machine learning models\nwith third-party datasets, since the attack can be performed effectively with\nlimited information. Experiments on benchmark datasets illustrate the\neffectiveness of our strategies in improving clean-label backdoor attacks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}