{"id":"2407.07550","title":"Evaluating the method reproducibility of deep learning models in the\n  biodiversity domain","authors":"Waqas Ahmed, Vamsi Krishna Kommineni, Birgitta K\\\"onig-Ries, Jitendra\n  Gaikwad, Luiz Gadelha, Sheeba Samuel","authorsParsed":[["Ahmed","Waqas",""],["Kommineni","Vamsi Krishna",""],["KÃ¶nig-Ries","Birgitta",""],["Gaikwad","Jitendra",""],["Gadelha","Luiz",""],["Samuel","Sheeba",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 11:19:15 GMT"}],"updateDate":"2024-07-11","timestamp":1720610355000,"abstract":"  Artificial Intelligence (AI) is revolutionizing biodiversity research by\nenabling advanced data analysis, species identification, and habitats\nmonitoring, thereby enhancing conservation efforts. Ensuring reproducibility in\nAI-driven biodiversity research is crucial for fostering transparency,\nverifying results, and promoting the credibility of ecological findings.This\nstudy investigates the reproducibility of deep learning (DL) methods within the\nbiodiversity domain. We design a methodology for evaluating the reproducibility\nof biodiversity-related publications that employ DL techniques across three\nstages. We define ten variables essential for method reproducibility, divided\ninto four categories: resource requirements, methodological information,\nuncontrolled randomness, and statistical considerations. These categories\nsubsequently serve as the basis for defining different levels of\nreproducibility. We manually extract the availability of these variables from a\ncurated dataset comprising 61 publications identified using the keywords\nprovided by biodiversity experts. Our study shows that the dataset is shared in\n47% of the publications; however, a significant number of the publications lack\ncomprehensive information on deep learning methods, including details regarding\nrandomness.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}