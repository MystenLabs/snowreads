{"id":"2407.08199","title":"SRPose: Two-view Relative Pose Estimation with Sparse Keypoints","authors":"Rui Yin, Yulun Zhang, Zherong Pan, Jianjun Zhu, Cheng Wang and Biao\n  Jia","authorsParsed":[["Yin","Rui",""],["Zhang","Yulun",""],["Pan","Zherong",""],["Zhu","Jianjun",""],["Wang","Cheng",""],["Jia","Biao",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 05:46:35 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 05:25:05 GMT"}],"updateDate":"2024-07-19","timestamp":1720676795000,"abstract":"  Two-view pose estimation is essential for map-free visual relocalization and\nobject pose tracking tasks. However, traditional matching methods suffer from\ntime-consuming robust estimators, while deep learning-based pose regressors\nonly cater to camera-to-world pose estimation, lacking generalizability to\ndifferent image sizes and camera intrinsics. In this paper, we propose SRPose,\na sparse keypoint-based framework for two-view relative pose estimation in\ncamera-to-world and object-to-camera scenarios. SRPose consists of a sparse\nkeypoint detector, an intrinsic-calibration position encoder, and promptable\nprior knowledge-guided attention layers. Given two RGB images of a fixed scene\nor a moving object, SRPose estimates the relative camera or 6D object pose\ntransformation. Extensive experiments demonstrate that SRPose achieves\ncompetitive or superior performance compared to state-of-the-art methods in\nterms of accuracy and speed, showing generalizability to both scenarios. It is\nrobust to different image sizes and camera intrinsics, and can be deployed with\nlow computing resources.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}