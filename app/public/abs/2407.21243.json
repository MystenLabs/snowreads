{"id":"2407.21243","title":"Informed Correctors for Discrete Diffusion Models","authors":"Yixiu Zhao, Jiaxin Shi, Lester Mackey, Scott Linderman","authorsParsed":[["Zhao","Yixiu",""],["Shi","Jiaxin",""],["Mackey","Lester",""],["Linderman","Scott",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 23:29:29 GMT"}],"updateDate":"2024-08-01","timestamp":1722382169000,"abstract":"  Discrete diffusion modeling is a promising framework for modeling and\ngenerating data in discrete spaces. To sample from these models, different\nstrategies present trade-offs between computation and sample quality. A\npredominant sampling strategy is predictor-corrector $\\tau$-leaping, which\nsimulates the continuous time generative process with discretized predictor\nsteps and counteracts the accumulation of discretization error via corrector\nsteps. However, for absorbing state diffusion, an important class of discrete\ndiffusion models, the standard forward-backward corrector can be ineffective in\nfixing such errors, resulting in subpar sample quality. To remedy this problem,\nwe propose a family of informed correctors that more reliably counteracts\ndiscretization error by leveraging information learned by the model. For\nfurther efficiency gains, we also propose $k$-Gillespie's, a sampling algorithm\nthat better utilizes each model evaluation, while still enjoying the speed and\nflexibility of $\\tau$-leaping. Across several real and synthetic datasets, we\nshow that $k$-Gillespie's with informed correctors reliably produces higher\nquality samples at lower computational cost.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}