{"id":"2407.20563","title":"Pyramid Coder: Hierarchical Code Generator for Compositional Visual\n  Question Answering","authors":"Ruoyue Shen, Nakamasa Inoue, Koichi Shinoda","authorsParsed":[["Shen","Ruoyue",""],["Inoue","Nakamasa",""],["Shinoda","Koichi",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 05:36:43 GMT"}],"updateDate":"2024-07-31","timestamp":1722317803000,"abstract":"  Visual question answering (VQA) is the task of providing accurate answers to\nnatural language questions based on visual input. Programmatic VQA (PVQA)\nmodels have been gaining attention recently. These use large language models\n(LLMs) to formulate executable programs that address questions requiring\ncomplex visual reasoning. However, there are challenges in enabling LLMs to\ncomprehend the usage of image processing modules and generate relevant code. To\novercome these challenges, this paper introduces PyramidCoder, a novel\nprompting framework for PVQA models. PyramidCoder consists of three\nhierarchical levels, each serving a distinct purpose: query rephrasing, code\ngeneration, and answer aggregation. Notably, PyramidCoder utilizes a single\nfrozen LLM and pre-defined prompts at each level, eliminating the need for\nadditional training and ensuring flexibility across various LLM architectures.\nCompared to the state-of-the-art PVQA model, our approach improves accuracy by\nat least 0.5% on the GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on the\nNLVR2 dataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}