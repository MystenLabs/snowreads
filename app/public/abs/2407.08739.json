{"id":"2407.08739","title":"MAVIS: Mathematical Visual Instruction Tuning","authors":"Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo,\n  Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, Peng Gao,\n  Hongsheng Li","authorsParsed":[["Zhang","Renrui",""],["Wei","Xinyu",""],["Jiang","Dongzhi",""],["Zhang","Yichi",""],["Guo","Ziyu",""],["Tong","Chengzhuo",""],["Liu","Jiaming",""],["Zhou","Aojun",""],["Wei","Bin",""],["Zhang","Shanghang",""],["Gao","Peng",""],["Li","Hongsheng",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:59:47 GMT"}],"updateDate":"2024-07-12","timestamp":1720720787000,"abstract":"  Multi-modal Large Language Models (MLLMs) have recently emerged as a\nsignificant focus in academia and industry. Despite their proficiency in\ngeneral multi-modal scenarios, the mathematical problem-solving capabilities in\nvisual contexts remain insufficiently explored. We identify three key areas\nwithin MLLMs that need to be improved: visual encoding of math diagrams,\ndiagram-language alignment, and mathematical reasoning skills. This draws forth\nan urgent demand for large-scale, high-quality data and training pipelines in\nvisual mathematics. In this paper, we propose MAVIS, the first MAthematical\nVISual instruction tuning paradigm for MLLMs, involving a series of\nmathematical visual datasets and specialized MLLMs. Targeting the three issues,\nMAVIS contains three progressive training stages from scratch. First, we curate\nMAVIS-Caption, consisting of 558K diagram-caption pairs, to fine-tune a\nmath-specific vision encoder (CLIP-Math) through contrastive learning, tailored\nfor improved diagram visual encoding. Second, we utilize MAVIS-Caption to align\nthe CLIP-Math with a large language model (LLM) by a projection layer,\nenhancing vision-language alignment in mathematical domains. Third, we\nintroduce MAVIS-Instruct, including 900K meticulously collected and annotated\nvisual math problems, which is adopted to finally instruct-tune the MLLM for\nrobust mathematical reasoning skills. In MAVIS-Instruct, we incorporate\ncomplete chain-of-thought (CoT) rationales for each problem, and minimize\ntextual redundancy, thereby concentrating the model towards the visual\nelements. Data and Models are released at https://github.com/ZrrSkywalker/MAVIS\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}