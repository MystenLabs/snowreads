{"id":"2407.12431","title":"GLARE: Low Light Image Enhancement via Generative Latent Feature based\n  Codebook Retrieval","authors":"Han Zhou and Wei Dong and Xiaohong Liu and Shuaicheng Liu and Xiongkuo\n  Min and Guangtao Zhai and Jun Chen","authorsParsed":[["Zhou","Han",""],["Dong","Wei",""],["Liu","Xiaohong",""],["Liu","Shuaicheng",""],["Min","Xiongkuo",""],["Zhai","Guangtao",""],["Chen","Jun",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 09:40:15 GMT"}],"updateDate":"2024-07-18","timestamp":1721209215000,"abstract":"  Most existing Low-light Image Enhancement (LLIE) methods either directly map\nLow-Light (LL) to Normal-Light (NL) images or use semantic or illumination maps\nas guides. However, the ill-posed nature of LLIE and the difficulty of semantic\nretrieval from impaired inputs limit these methods, especially in extremely\nlow-light conditions. To address this issue, we present a new LLIE network via\nGenerative LAtent feature based codebook REtrieval (GLARE), in which the\ncodebook prior is derived from undegraded NL images using a Vector Quantization\n(VQ) strategy. More importantly, we develop a generative Invertible Latent\nNormalizing Flow (I-LNF) module to align the LL feature distribution to NL\nlatent representations, guaranteeing the correct code retrieval in the\ncodebook. In addition, a novel Adaptive Feature Transformation (AFT) module,\nfeaturing an adjustable function for users and comprising an Adaptive Mix-up\nBlock (AMB) along with a dual-decoder architecture, is devised to further\nenhance fidelity while preserving the realistic details provided by codebook\nprior. Extensive experiments confirm the superior performance of GLARE on\nvarious benchmark datasets and real-world data. Its effectiveness as a\npreprocessing tool in low-light object detection tasks further validates GLARE\nfor high-level vision applications. Code is released at\nhttps://github.com/LowLevelAI/GLARE.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}