{"id":"2408.11008","title":"Towards a Standardized Representation for Deep Learning Collective\n  Algorithms","authors":"Jinsun Yoo, William Won, Meghan Cowan, Nan Jiang, Benjamin Klenk,\n  Srinivas Sridharan, Tushar Krishna","authorsParsed":[["Yoo","Jinsun",""],["Won","William",""],["Cowan","Meghan",""],["Jiang","Nan",""],["Klenk","Benjamin",""],["Sridharan","Srinivas",""],["Krishna","Tushar",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 17:01:59 GMT"}],"updateDate":"2024-08-21","timestamp":1724173319000,"abstract":"  The explosion of machine learning model size has led to its execution on\ndistributed clusters at a very large scale. Many works have tried to optimize\nthe process of producing collective algorithms and running collective\ncommunications, which act as a bottleneck to distributed machine learning.\nHowever, different works use their own collective algorithm representation,\npushing away from co-optimizing collective communication and the rest of the\nworkload. The lack of a standardized collective algorithm representation has\nalso hindered interoperability between collective algorithm producers and\nconsumers. Additionally, tool-specific conversions and modifications have to be\nmade for each pair of tools producing and consuming collective algorithms which\nadds to engineering efforts.\n  In this position paper, we propose a standardized workflow leveraging a\ncommon collective algorithm representation. Upstream producers and downstream\nconsumers converge to a common representation format based on Chakra Execution\nTrace, a commonly used graph based representation of distributed machine\nlearning workloads. Such a common representation enables us to view collective\ncommunications at the same level as workload operations and decouple producer\nand consumer tools, enhance interoperability, and relieve the user from the\nburden of having to focus on downstream implementations. We provide a\nproof-of-concept of this standardized workflow by simulating collective\nalgorithms generated by the MSCCLang domain-specific language through the\nASTRA-sim distributed machine learning simulator using various network\nconfigurations.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}