{"id":"2408.16947","title":"An Empirical Study of Scaling Laws for Transfer","authors":"Matthew Barnett","authorsParsed":[["Barnett","Matthew",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 00:06:29 GMT"}],"updateDate":"2024-09-02","timestamp":1724976389000,"abstract":"  We present a limited empirical study of scaling laws for transfer learning in\ntransformer models. More specifically, we examine a scaling law that\nincorporates a \"transfer gap\" term, indicating the effectiveness of\npre-training on one distribution when optimizing for downstream performance on\nanother distribution. When the transfer gap is low, pre-training is a\ncost-effective strategy for improving downstream performance. Conversely, when\nthe gap is high, collecting high-quality fine-tuning data becomes relatively\nmore cost effective. Fitting the scaling law to experiments from diverse\ndatasets reveals significant variations in the transfer gap across\ndistributions. In theory, the scaling law can inform optimal data allocation\nstrategies and highlights how the scarcity of downstream data can bottleneck\nperformance. Our findings contribute to a principled way to measure transfer\nlearning efficiency and understand how data availability affects capabilities.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}