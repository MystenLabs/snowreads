{"id":"2407.05679","title":"BEVWorld: A Multimodal World Model for Autonomous Driving via Unified\n  BEV Latent Space","authors":"Yumeng Zhang, Shi Gong, Kaixin Xiong, Xiaoqing Ye, Xiao Tan, Fan Wang,\n  Jizhou Huang, Hua Wu, Haifeng Wang","authorsParsed":[["Zhang","Yumeng",""],["Gong","Shi",""],["Xiong","Kaixin",""],["Ye","Xiaoqing",""],["Tan","Xiao",""],["Wang","Fan",""],["Huang","Jizhou",""],["Wu","Hua",""],["Wang","Haifeng",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 07:26:08 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 08:33:43 GMT"}],"updateDate":"2024-07-19","timestamp":1720423568000,"abstract":"  World models are receiving increasing attention in autonomous driving for\ntheir ability to predict potential future scenarios. In this paper, we present\nBEVWorld, a novel approach that tokenizes multimodal sensor inputs into a\nunified and compact Bird's Eye View (BEV) latent space for environment\nmodeling. The world model consists of two parts: the multi-modal tokenizer and\nthe latent BEV sequence diffusion model. The multi-modal tokenizer first\nencodes multi-modality information and the decoder is able to reconstruct the\nlatent BEV tokens into LiDAR and image observations by ray-casting rendering in\na self-supervised manner. Then the latent BEV sequence diffusion model predicts\nfuture scenarios given action tokens as conditions. Experiments demonstrate the\neffectiveness of BEVWorld in autonomous driving tasks, showcasing its\ncapability in generating future scenes and benefiting downstream tasks such as\nperception and motion prediction. Code will be available at\nhttps://github.com/zympsyche/BevWorld.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}