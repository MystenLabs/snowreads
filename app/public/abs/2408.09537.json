{"id":"2408.09537","title":"Sample-Optimal Large-Scale Optimal Subset Selection","authors":"Zaile Li, Weiwei Fan, L. Jeff Hong","authorsParsed":[["Li","Zaile",""],["Fan","Weiwei",""],["Hong","L. Jeff",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 16:44:41 GMT"}],"updateDate":"2024-08-20","timestamp":1723999481000,"abstract":"  Ranking and selection (R&S) conventionally aims to select the unique best\nalternative with the largest mean performance from a finite set of\nalternatives. However, for better supporting decision making, it may be more\ninformative to deliver a small menu of alternatives whose mean performances are\namong the top $m$. Such problem, called optimal subset selection (OSS), is\ngenerally more challenging to address than the conventional R&S. This challenge\nbecomes even more significant when the number of alternatives is considerably\nlarge. Thus, the focus of this paper is on addressing the large-scale OSS\nproblem. To achieve this goal, we design a top-$m$ greedy selection mechanism\nthat keeps sampling the current top $m$ alternatives with top $m$ running\nsample means and propose the explore-first top-$m$ greedy (EFG-$m$) procedure.\nThrough an extended boundary-crossing framework, we prove that the EFG-$m$\nprocedure is both sample optimal and consistent in terms of the probability of\ngood selection, confirming its effectiveness in solving large-scale OSS\nproblem. Surprisingly, we also demonstrate that the EFG-$m$ procedure enables\nto achieve an indifference-based ranking within the selected subset of\nalternatives at no extra cost. This is highly beneficial as it delivers deeper\ninsights to decision-makers, enabling more informed decision-makings. Lastly,\nnumerical experiments validate our results and demonstrate the efficiency of\nour procedures.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Statistics/Methodology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}