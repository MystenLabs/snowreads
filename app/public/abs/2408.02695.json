{"id":"2408.02695","title":"Distribution-Level Memory Recall for Continual Learning: Preserving\n  Knowledge and Avoiding Confusion","authors":"Shaoxu Cheng, Kanglei Geng, Chiyuan He, Zihuan Qiu, Linfeng Xu, Heqian\n  Qiu, Lanxiao Wang, Qingbo Wu, Fanman Meng, Hongliang Li","authorsParsed":[["Cheng","Shaoxu",""],["Geng","Kanglei",""],["He","Chiyuan",""],["Qiu","Zihuan",""],["Xu","Linfeng",""],["Qiu","Heqian",""],["Wang","Lanxiao",""],["Wu","Qingbo",""],["Meng","Fanman",""],["Li","Hongliang",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 07:37:12 GMT"}],"updateDate":"2024-08-07","timestamp":1722757032000,"abstract":"  Continual Learning (CL) aims to enable Deep Neural Networks (DNNs) to learn\nnew data without forgetting previously learned knowledge. The key to achieving\nthis goal is to avoid confusion at the feature level, i.e., avoiding confusion\nwithin old tasks and between new and old tasks. Previous prototype-based CL\nmethods generate pseudo features for old knowledge replay by adding Gaussian\nnoise to the centroids of old classes. However, the distribution in the feature\nspace exhibits anisotropy during the incremental process, which prevents the\npseudo features from faithfully reproducing the distribution of old knowledge\nin the feature space, leading to confusion in classification boundaries within\nold tasks. To address this issue, we propose the Distribution-Level Memory\nRecall (DMR) method, which uses a Gaussian mixture model to precisely fit the\nfeature distribution of old knowledge at the distribution level and generate\npseudo features in the next stage. Furthermore, resistance to confusion at the\ndistribution level is also crucial for multimodal learning, as the problem of\nmultimodal imbalance results in significant differences in feature responses\nbetween different modalities, exacerbating confusion within old tasks in\nprototype-based CL methods. Therefore, we mitigate the multi-modal imbalance\nproblem by using the Inter-modal Guidance and Intra-modal Mining (IGIM) method\nto guide weaker modalities with prior information from dominant modalities and\nfurther explore useful information within modalities. For the second key, We\npropose the Confusion Index to quantitatively describe a model's ability to\ndistinguish between new and old tasks, and we use the Incremental Mixup Feature\nEnhancement (IMFE) method to enhance pseudo features with new sample features,\nalleviating classification confusion between new and old knowledge.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}