{"id":"2407.03329","title":"$L^{p}$-convergence of Kantorovich-type Max-Min Neural Network Operators","authors":"\\.Ismail Aslan, Stefano De Marchi, Wolfgang Erb","authorsParsed":[["Aslan","Ä°smail",""],["De Marchi","Stefano",""],["Erb","Wolfgang",""]],"versions":[{"version":"v1","created":"Mon, 6 May 2024 06:56:32 GMT"}],"updateDate":"2024-07-08","timestamp":1714978592000,"abstract":"  In this work, we study the Kantorovich variant of max-min neural network\noperators, in which the operator kernel is defined in terms of sigmoidal\nfunctions. Our main aim is to demonstrate the $L^{p}$-convergence of these\nnonlinear operators for $1\\leq p<\\infty$, which makes it possible to obtain\napproximation results for functions that are not necessarily continuous. In\naddition, we will derive quantitative estimates for the rate of approximation\nin the $L^{p}$-norm. We will provide some explicit examples, studying the\napproximation of discontinuous functions with the max-min operator, and varying\nadditionally the underlying sigmoidal function of the kernel. Further, we\nnumerically compare the $L^{p}$-approximation error with the respective error\nof the Kantorovich variants of other popular neural network operators. As a\nfinal application, we show that the Kantorovich variant has advantages compared\nto the sampling variant of the max-min operator and Kantorovich variant of the\nmax-product operator when it comes to approximate noisy functions as for\ninstance biomedical ECG signals.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}