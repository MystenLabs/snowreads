{"id":"2408.04962","title":"DAFT-GAN: Dual Affine Transformation Generative Adversarial Network for\n  Text-Guided Image Inpainting","authors":"Jihoon Lee, Yunhong Min, Hwidong Kim, Sangtae Ahn","authorsParsed":[["Lee","Jihoon",""],["Min","Yunhong",""],["Kim","Hwidong",""],["Ahn","Sangtae",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 09:28:42 GMT"}],"updateDate":"2024-08-12","timestamp":1723195722000,"abstract":"  In recent years, there has been a significant focus on research related to\ntext-guided image inpainting. However, the task remains challenging due to\nseveral constraints, such as ensuring alignment between the image and the text,\nand maintaining consistency in distribution between corrupted and uncorrupted\nregions. In this paper, thus, we propose a dual affine transformation\ngenerative adversarial network (DAFT-GAN) to maintain the semantic consistency\nfor text-guided inpainting. DAFT-GAN integrates two affine transformation\nnetworks to combine text and image features gradually for each decoding block.\nMoreover, we minimize information leakage of uncorrupted features for\nfine-grained image generation by encoding corrupted and uncorrupted regions of\nthe masked image separately. Our proposed model outperforms the existing\nGAN-based models in both qualitative and quantitative assessments with three\nbenchmark datasets (MS-COCO, CUB, and Oxford) for text-guided image inpainting.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}