{"id":"2407.11637","title":"REMM:Rotation-Equivariant Framework for End-to-End Multimodal Image\n  Matching","authors":"Han Nie, Bin Luo, Jun Liu, Zhitao Fu, Weixing Liu and Xin Su","authorsParsed":[["Nie","Han",""],["Luo","Bin",""],["Liu","Jun",""],["Fu","Zhitao",""],["Liu","Weixing",""],["Su","Xin",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 11:58:40 GMT"}],"updateDate":"2024-07-17","timestamp":1721131120000,"abstract":"  We present REMM, a rotation-equivariant framework for end-to-end multimodal\nimage matching, which fully encodes rotational differences of descriptors in\nthe whole matching pipeline. Previous learning-based methods mainly focus on\nextracting modal-invariant descriptors, while consistently ignoring the\nrotational invariance. In this paper, we demonstrate that our REMM is very\nuseful for multimodal image matching, including multimodal feature learning\nmodule and cyclic shift module. We first learn modal-invariant features through\nthe multimodal feature learning module. Then, we design the cyclic shift module\nto rotationally encode the descriptors, greatly improving the performance of\nrotation-equivariant matching, which makes them robust to any angle. To\nvalidate our method, we establish a comprehensive rotation and scale-matching\nbenchmark for evaluating the anti-rotation performance of multimodal images,\nwhich contains a combination of multi-angle and multi-scale transformations\nfrom four publicly available datasets. Extensive experiments show that our\nmethod outperforms existing methods in benchmarking and generalizes well to\nindependent datasets. Additionally, we conducted an in-depth analysis of the\nkey components of the REMM to validate the improvements brought about by the\ncyclic shift module. Code and dataset at https://github.com/HanNieWHU/REMM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}