{"id":"2407.03514","title":"Towards Attention-based Contrastive Learning for Audio Spoof Detection","authors":"Chirag Goel, Surya Koppisetti, Ben Colman, Ali Shahriyari, Gaurav\n  Bharaj","authorsParsed":[["Goel","Chirag",""],["Koppisetti","Surya",""],["Colman","Ben",""],["Shahriyari","Ali",""],["Bharaj","Gaurav",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 21:25:12 GMT"}],"updateDate":"2024-07-08","timestamp":1720041912000,"abstract":"  Vision transformers (ViT) have made substantial progress for classification\ntasks in computer vision. Recently, Gong et. al. '21, introduced\nattention-based modeling for several audio tasks. However, relatively\nunexplored is the use of a ViT for audio spoof detection task. We bridge this\ngap and introduce ViTs for this task. A vanilla baseline built on fine-tuning\nthe SSAST (Gong et. al. '22) audio ViT model achieves sub-optimal equal error\nrates (EERs). To improve performance, we propose a novel attention-based\ncontrastive learning framework (SSAST-CL) that uses cross-attention to aid the\nrepresentation learning. Experiments show that our framework successfully\ndisentangles the bonafide and spoof classes and helps learn better classifiers\nfor the task. With appropriate data augmentations policy, a model trained on\nour framework achieves competitive performance on the ASVSpoof 2021 challenge.\nWe provide comparisons and ablation studies to justify our claim.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}