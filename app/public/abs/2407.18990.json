{"id":"2407.18990","title":"Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM\n  Tuning in Real-World Applications","authors":"Alon Halfon, Shai Gretz, Ofir Arviv, Artem Spector, Orith\n  Toledo-Ronen, Yoav Katz, Liat Ein-Dor, Michal Shmueli-Scheuer, Noam Slonim","authorsParsed":[["Halfon","Alon",""],["Gretz","Shai",""],["Arviv","Ofir",""],["Spector","Artem",""],["Toledo-Ronen","Orith",""],["Katz","Yoav",""],["Ein-Dor","Liat",""],["Shmueli-Scheuer","Michal",""],["Slonim","Noam",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 12:07:55 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 07:46:39 GMT"}],"updateDate":"2024-08-08","timestamp":1721909275000,"abstract":"  Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}