{"id":"2407.18242","title":"LoRA-Pro: Are Low-Rank Adapters Properly Optimized?","authors":"Zhengbo Wang, Jian Liang","authorsParsed":[["Wang","Zhengbo",""],["Liang","Jian",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 17:57:12 GMT"}],"updateDate":"2024-07-26","timestamp":1721930232000,"abstract":"  Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the \"equivalent gradient.\" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}