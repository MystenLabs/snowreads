{"id":"2407.16207","title":"Graph-Structured Speculative Decoding","authors":"Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang,\n  Xunliang Cai, Dongyan Zhao, Rui Yan","authorsParsed":[["Gong","Zhuocheng",""],["Liu","Jiahao",""],["Wang","Ziyue",""],["Wu","Pengfei",""],["Wang","Jingang",""],["Cai","Xunliang",""],["Zhao","Dongyan",""],["Yan","Rui",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 06:21:24 GMT"}],"updateDate":"2024-07-24","timestamp":1721715684000,"abstract":"  Speculative decoding has emerged as a promising technique to accelerate the\ninference of Large Language Models (LLMs) by employing a small language model\nto draft a hypothesis sequence, which is then validated by the LLM. The\neffectiveness of this approach heavily relies on the balance between\nperformance and efficiency of the draft model. In our research, we focus on\nenhancing the proportion of draft tokens that are accepted to the final output\nby generating multiple hypotheses instead of just one. This allows the LLM more\noptions to choose from and select the longest sequence that meets its\nstandards. Our analysis reveals that hypotheses produced by the draft model\nshare many common token sequences, suggesting a potential for optimizing\ncomputation. Leveraging this observation, we introduce an innovative approach\nutilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This\nstructure enables us to efficiently predict and merge recurring token\nsequences, vastly reducing the computational demands of the draft model. We\nterm this approach Graph-structured Speculative Decoding (GSD). We apply GSD\nacross a range of LLMs, including a 70-billion parameter LLaMA-2 model, and\nobserve a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly\nsurpassing standard speculative decoding.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}