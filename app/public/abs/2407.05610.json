{"id":"2407.05610","title":"Described Spatial-Temporal Video Detection","authors":"Wei Ji, Xiangyan Liu, Yingfei Sun, Jiajun Deng, You Qin, Ammar\n  Nuwanna, Mengyao Qiu, Lina Wei, Roger Zimmermann","authorsParsed":[["Ji","Wei",""],["Liu","Xiangyan",""],["Sun","Yingfei",""],["Deng","Jiajun",""],["Qin","You",""],["Nuwanna","Ammar",""],["Qiu","Mengyao",""],["Wei","Lina",""],["Zimmermann","Roger",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 04:54:39 GMT"}],"updateDate":"2024-07-09","timestamp":1720414479000,"abstract":"  Detecting visual content on language expression has become an emerging topic\nin the community. However, in the video domain, the existing setting, i.e.,\nspatial-temporal video grounding (STVG), is formulated to only detect one\npre-existing object in each frame, ignoring the fact that language descriptions\ncan involve none or multiple entities within a video. In this work, we advance\nthe STVG to a more practical setting called described spatial-temporal video\ndetection (DSTVD) by overcoming the above limitation. To facilitate the\nexploration of DSTVD, we first introduce a new benchmark, namely DVD-ST.\nNotably, DVD-ST supports grounding from none to many objects onto the video in\nresponse to queries and encompasses a diverse range of over 150 entities,\nincluding appearance, actions, locations, and interactions. The extensive\nbreadth and diversity of the DVD-ST dataset make it an exemplary testbed for\nthe investigation of DSTVD. In addition to the new benchmark, we further\npresent two baseline methods for our proposed DSTVD task by extending two\nrepresentative STVG models, i.e., TubeDETR, and STCAT. These extended models\ncapitalize on tubelet queries to localize and track referred objects across the\nvideo sequence. Besides, we adjust the training objectives of these models to\noptimize spatial and temporal localization accuracy and multi-class\nclassification capabilities. Furthermore, we benchmark the baselines on the\nintroduced DVD-ST dataset and conduct extensive experimental analysis to guide\nfuture investigation. Our code and benchmark will be publicly available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}