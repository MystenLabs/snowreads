{"id":"2408.07888","title":"Fine-tuning Large Language Models with Human-inspired Learning\n  Strategies in Medical Question Answering","authors":"Yushi Yang, Andrew M. Bean, Robert McCraith, Adam Mahdi","authorsParsed":[["Yang","Yushi",""],["Bean","Andrew M.",""],["McCraith","Robert",""],["Mahdi","Adam",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 02:22:48 GMT"}],"updateDate":"2024-08-16","timestamp":1723688568000,"abstract":"  Training Large Language Models (LLMs) incurs substantial data-related costs,\nmotivating the development of data-efficient training methods through optimised\ndata ordering and selection. Human-inspired learning strategies, such as\ncurriculum learning, offer possibilities for efficient training by organising\ndata according to common human learning practices. Despite evidence that\nfine-tuning with curriculum learning improves the performance of LLMs for\nnatural language understanding tasks, its effectiveness is typically assessed\nusing a single model. In this work, we extend previous research by evaluating\nboth curriculum-based and non-curriculum-based learning strategies across\nmultiple LLMs, using human-defined and automated data labels for medical\nquestion answering. Our results indicate a moderate impact of using\nhuman-inspired learning strategies for fine-tuning LLMs, with maximum accuracy\ngains of 1.77% per model and 1.81% per dataset. Crucially, we demonstrate that\nthe effectiveness of these strategies varies significantly across different\nmodel-dataset combinations, emphasising that the benefits of a specific\nhuman-inspired strategy for fine-tuning LLMs do not generalise. Additionally,\nwe find evidence that curriculum learning using LLM-defined question difficulty\noutperforms human-defined difficulty, highlighting the potential of using\nmodel-generated measures for optimal curriculum design.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}