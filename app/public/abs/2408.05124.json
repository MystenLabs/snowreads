{"id":"2408.05124","title":"Modeling Electromagnetic Signal Injection Attacks on Camera-based Smart\n  Systems: Applications and Mitigation","authors":"Youqian Zhang, Michael Cheung, Chunxi Yang, Xinwei Zhai, Zitong Shen,\n  Xinyu Ji, Eugene Y. Fu, Sze-Yiu Chau, Xiapu Luo","authorsParsed":[["Zhang","Youqian",""],["Cheung","Michael",""],["Yang","Chunxi",""],["Zhai","Xinwei",""],["Shen","Zitong",""],["Ji","Xinyu",""],["Fu","Eugene Y.",""],["Chau","Sze-Yiu",""],["Luo","Xiapu",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 15:33:28 GMT"}],"updateDate":"2024-08-12","timestamp":1723217608000,"abstract":"  Numerous safety- or security-critical systems depend on cameras to perceive\ntheir surroundings, further allowing artificial intelligence (AI) to analyze\nthe captured images to make important decisions. However, a concerning attack\nvector has emerged, namely, electromagnetic waves, which pose a threat to the\nintegrity of these systems. Such attacks enable attackers to manipulate the\nimages remotely, leading to incorrect AI decisions, e.g., autonomous vehicles\nmissing detecting obstacles ahead resulting in collisions. The lack of\nunderstanding regarding how different systems react to such attacks poses a\nsignificant security risk. Furthermore, no effective solutions have been\ndemonstrated to mitigate this threat.\n  To address these gaps, we modeled the attacks and developed a simulation\nmethod for generating adversarial images. Through rigorous analysis, we\nconfirmed that the effects of the simulated adversarial images are\nindistinguishable from those from real attacks. This method enables researchers\nand engineers to rapidly assess the susceptibility of various AI vision\napplications to these attacks, without the need for constructing complicated\nattack devices. In our experiments, most of the models demonstrated\nvulnerabilities to these attacks, emphasizing the need to enhance their\nrobustness. Fortunately, our modeling and simulation method serves as a\nstepping stone toward developing more resilient models. We present a pilot\nstudy on adversarial training to improve their robustness against attacks, and\nour results demonstrate a significant improvement by recovering up to 91%\nperformance, offering a promising direction for mitigating this threat.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}