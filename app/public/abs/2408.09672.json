{"id":"2408.09672","title":"Regularization for Adversarial Robust Learning","authors":"Jie Wang and Rui Gao and Yao Xie","authorsParsed":[["Wang","Jie",""],["Gao","Rui",""],["Xie","Yao",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 03:15:41 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 10:07:50 GMT"}],"updateDate":"2024-08-23","timestamp":1724037341000,"abstract":"  Despite the growing prevalence of artificial neural networks in real-world\napplications, their vulnerability to adversarial attacks remains a significant\nconcern, which motivates us to investigate the robustness of machine learning\nmodels. While various heuristics aim to optimize the distributionally robust\nrisk using the $\\infty$-Wasserstein metric, such a notion of robustness\nfrequently encounters computation intractability. To tackle the computational\nchallenge, we develop a novel approach to adversarial training that integrates\n$\\phi$-divergence regularization into the distributionally robust risk\nfunction. This regularization brings a notable improvement in computation\ncompared with the original formulation. We develop stochastic gradient methods\nwith biased oracles to solve this problem efficiently, achieving the\nnear-optimal sample complexity. Moreover, we establish its regularization\neffects and demonstrate it is asymptotic equivalence to a regularized empirical\nrisk minimization framework, by considering various scaling regimes of the\nregularization parameter and robustness level. These regimes yield gradient\nnorm regularization, variance regularization, or a smoothed gradient norm\nregularization that interpolates between these extremes. We numerically\nvalidate our proposed method in supervised learning, reinforcement learning,\nand contextual learning and showcase its state-of-the-art performance against\nvarious adversarial attacks.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}