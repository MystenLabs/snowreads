{"id":"2407.16168","title":"Progressively Modality Freezing for Multi-Modal Entity Alignment","authors":"Yani Huang, Xuefeng Zhang, Richong Zhang, Junfan Chen, Jaein Kim","authorsParsed":[["Huang","Yani",""],["Zhang","Xuefeng",""],["Zhang","Richong",""],["Chen","Junfan",""],["Kim","Jaein",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 04:22:30 GMT"}],"updateDate":"2024-07-24","timestamp":1721708550000,"abstract":"  Multi-Modal Entity Alignment aims to discover identical entities across\nheterogeneous knowledge graphs. While recent studies have delved into fusion\nparadigms to represent entities holistically, the elimination of features\nirrelevant to alignment and modal inconsistencies is overlooked, which are\ncaused by inherent differences in multi-modal features. To address these\nchallenges, we propose a novel strategy of progressive modality freezing,\ncalled PMF, that focuses on alignmentrelevant features and enhances multi-modal\nfeature fusion. Notably, our approach introduces a pioneering cross-modal\nassociation loss to foster modal consistency. Empirical evaluations across nine\ndatasets confirm PMF's superiority, demonstrating stateof-the-art performance\nand the rationale for freezing modalities. Our code is available at\nhttps://github.com/ninibymilk/PMF-MMEA.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}