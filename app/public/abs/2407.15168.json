{"id":"2407.15168","title":"Mitigating Deep Reinforcement Learning Backdoors in the Neural\n  Activation Space","authors":"Sanyam Vyas, Chris Hicks and Vasilios Mavroudis","authorsParsed":[["Vyas","Sanyam",""],["Hicks","Chris",""],["Mavroudis","Vasilios",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 13:48:23 GMT"}],"updateDate":"2024-07-23","timestamp":1721569703000,"abstract":"  This paper investigates the threat of backdoors in Deep Reinforcement\nLearning (DRL) agent policies and proposes a novel method for their detection\nat runtime. Our study focuses on elusive in-distribution backdoor triggers.\nSuch triggers are designed to induce a deviation in the behaviour of a\nbackdoored agent while blending into the expected data distribution to evade\ndetection. Through experiments conducted in the Atari Breakout environment, we\ndemonstrate the limitations of current sanitisation methods when faced with\nsuch triggers and investigate why they present a challenging defence problem.\nWe then evaluate the hypothesis that backdoor triggers might be easier to\ndetect in the neural activation space of the DRL agent's policy network. Our\nstatistical analysis shows that indeed the activation patterns in the agent's\npolicy network are distinct in the presence of a trigger, regardless of how\nwell the trigger is concealed in the environment. Based on this, we propose a\nnew defence approach that uses a classifier trained on clean environment\nsamples and detects abnormal activations. Our results show that even\nlightweight classifiers can effectively prevent malicious actions with\nconsiderable accuracy, indicating the potential of this research direction even\nagainst sophisticated adversaries.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}