{"id":"2407.10969","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated","authors":"Hongyu Wang, Shuming Ma, Ruiping Wang, Furu Wei","authorsParsed":[["Wang","Hongyu",""],["Ma","Shuming",""],["Wang","Ruiping",""],["Wei","Furu",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 17:59:29 GMT"},{"version":"v2","created":"Sat, 20 Jul 2024 17:57:26 GMT"},{"version":"v3","created":"Wed, 24 Jul 2024 14:57:48 GMT"}],"updateDate":"2024-07-25","timestamp":1721066369000,"abstract":"  We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. We also introduce Block\nQ-Sparse for batch training and inference. The key results from this work are,\n(1) Q-Sparse can achieve results comparable to those of baseline LLMs while\nbeing much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}