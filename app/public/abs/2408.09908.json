{"id":"2408.09908","title":"$p$SVM: Soft-margin SVMs with $p$-norm Hinge Loss","authors":"Haoxiang Sun","authorsParsed":[["Sun","Haoxiang",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 11:30:00 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 12:00:00 GMT"}],"updateDate":"2024-08-21","timestamp":1724067000000,"abstract":"  Support Vector Machines (SVMs) based on hinge loss have been extensively\ndiscussed and applied to various binary classification tasks. These SVMs\nachieve a balance between margin maximization and the minimization of slack due\nto outliers. Although many efforts have been dedicated to enhancing the\nperformance of SVMs with hinge loss, studies on $p$SVMs, soft-margin SVMs with\n$p$-norm hinge loss, remain relatively scarce. In this paper, we explore the\nproperties, performance, and training algorithms of $p$SVMs. We first derive\nthe generalization bound of $p$SVMs, then formulate the dual optimization\nproblem, comparing it with the traditional approach. Furthermore, we discuss a\ngeneralized version of the Sequential Minimal Optimization (SMO) algorithm,\n$p$SMO, to train our $p$SVM model. Comparative experiments on various datasets,\nincluding binary and multi-class classification tasks, demonstrate the\neffectiveness and advantages of our $p$SVM model and the $p$SMO method. Code is\navailable at https://github.com/CoderBak/pSVM.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}