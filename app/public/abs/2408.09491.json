{"id":"2408.09491","title":"A Transcription Prompt-based Efficient Audio Large Language Model for\n  Robust Speech Recognition","authors":"Yangze Li, Xiong Wang, Songjun Cao, Yike Zhang, Long Ma, Lei Xie","authorsParsed":[["Li","Yangze",""],["Wang","Xiong",""],["Cao","Songjun",""],["Zhang","Yike",""],["Ma","Long",""],["Xie","Lei",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 14:10:35 GMT"}],"updateDate":"2024-08-20","timestamp":1723990235000,"abstract":"  Audio-LLM introduces audio modality into a large language model (LLM) to\nenable a powerful LLM to recognize, understand, and generate audio. However,\nduring speech recognition in noisy environments, we observed the presence of\nillusions and repetition issues in audio-LLM, leading to substitution and\ninsertion errors. This paper proposes a transcription prompt-based audio-LLM by\nintroducing an ASR expert as a transcription tokenizer and a hybrid\nAutoregressive (AR) Non-autoregressive (NAR) decoding approach to solve the\nabove problems. Experiments on 10k-hour WenetSpeech Mandarin corpus show that\nour approach decreases 12.2% and 9.6% CER relatively on Test_Net and\nTest_Meeting evaluation sets compared with baseline. Notably, we reduce the\ndecoding repetition rate on the evaluation set to zero, showing that the\ndecoding repetition problem has been solved fundamentally.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}