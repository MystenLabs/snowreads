{"id":"2407.13200","title":"Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual\n  Transformers","authors":"Mengke Li, Da Li, Guoqing Yang, Yiu-ming Cheung, Hui Huang","authorsParsed":[["Li","Mengke",""],["Li","Da",""],["Yang","Guoqing",""],["Cheung","Yiu-ming",""],["Huang","Hui",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 06:32:45 GMT"},{"version":"v2","created":"Sun, 4 Aug 2024 07:06:04 GMT"}],"updateDate":"2024-08-06","timestamp":1721284365000,"abstract":"  Pre-trained large-scale models have exhibited remarkable efficacy in computer\nvision, particularly for 2D image analysis. However, when it comes to 3D point\nclouds, the constrained accessibility of data, in contrast to the vast\nrepositories of images, poses a challenge for the development of 3D pre-trained\nmodels. This paper therefore attempts to directly leverage pre-trained models\nwith 2D prior knowledge to accomplish the tasks for 3D point cloud analysis.\nAccordingly, we propose the Adaptive PointFormer (APF), which fine-tunes\npre-trained 2D models with only a modest number of parameters to directly\nprocess point clouds, obviating the need for mapping to images. Specifically,\nwe convert raw point clouds into point embeddings for aligning dimensions with\nimage tokens. Given the inherent disorder in point clouds, in contrast to the\nstructured nature of images, we then sequence the point embeddings to optimize\nthe utilization of 2D attention priors. To calibrate attention across 3D and 2D\ndomains and reduce computational overhead, a trainable PointFormer with a\nlimited number of parameters is subsequently concatenated to a frozen\npre-trained image model. Extensive experiments on various benchmarks\ndemonstrate the effectiveness of the proposed APF. The source code and more\ndetails are available at https://vcc.tech/research/2024/PointFormer.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}