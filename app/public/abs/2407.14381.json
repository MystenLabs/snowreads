{"id":"2407.14381","title":"Improving GBDT Performance on Imbalanced Datasets: An Empirical Study of\n  Class-Balanced Loss Functions","authors":"Jiaqi Luo, Yuan Yuan, Shixin Xu","authorsParsed":[["Luo","Jiaqi",""],["Yuan","Yuan",""],["Xu","Shixin",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 15:10:46 GMT"}],"updateDate":"2024-07-22","timestamp":1721401846000,"abstract":"  Class imbalance remains a significant challenge in machine learning,\nparticularly for tabular data classification tasks. While Gradient Boosting\nDecision Trees (GBDT) models have proven highly effective for such tasks, their\nperformance can be compromised when dealing with imbalanced datasets. This\npaper presents the first comprehensive study on adapting class-balanced loss\nfunctions to three GBDT algorithms across various tabular classification tasks,\nincluding binary, multi-class, and multi-label classification. We conduct\nextensive experiments on multiple datasets to evaluate the impact of\nclass-balanced losses on different GBDT models, establishing a valuable\nbenchmark. Our results demonstrate the potential of class-balanced loss\nfunctions to enhance GBDT performance on imbalanced datasets, offering a robust\napproach for practitioners facing class imbalance challenges in real-world\napplications. Additionally, we introduce a Python package that facilitates the\nintegration of class-balanced loss functions into GBDT workflows, making these\nadvanced techniques accessible to a wider audience.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}