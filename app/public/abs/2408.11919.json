{"id":"2408.11919","title":"PAL: A Variability-Aware Policy for Scheduling ML Workloads in GPU\n  Clusters","authors":"Rutwik Jain, Brandon Tran, Keting Chen, Matthew D. Sinclair and\n  Shivaram Venkataraman","authorsParsed":[["Jain","Rutwik",""],["Tran","Brandon",""],["Chen","Keting",""],["Sinclair","Matthew D.",""],["Venkataraman","Shivaram",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 18:12:24 GMT"},{"version":"v2","created":"Thu, 19 Sep 2024 12:31:18 GMT"}],"updateDate":"2024-09-20","timestamp":1724263944000,"abstract":"  Large-scale computing systems are increasingly using accelerators such as\nGPUs to enable peta- and exa-scale levels of compute to meet the needs of\nMachine Learning (ML) and scientific computing applications. Given the\nwidespread and growing use of ML, including in some scientific applications,\noptimizing these clusters for ML workloads is particularly important. However,\nrecent work has demonstrated that accelerators in these clusters can suffer\nfrom performance variability and this variability can lead to resource\nunder-utilization and load imbalance. In this work we focus on how clusters\nschedulers, which are used to share accelerator-rich clusters across many\nconcurrent ML jobs, can embrace performance variability to mitigate its\neffects. Our key insight to address this challenge is to characterize which\napplications are more likely to suffer from performance variability and take\nthat into account while placing jobs on the cluster. We design a novel cluster\nscheduler, PAL, which uses performance variability measurements and\napplication-specific profiles to improve job performance and resource\nutilization. PAL also balances performance variability with locality to ensure\njobs are spread across as few nodes as possible. Overall, PAL significantly\nimproves GPU-rich cluster scheduling: across traces for six ML workload\napplications spanning image, language, and vision models with a variety of\nvariability profiles, PAL improves geomean job completion time by 42%, cluster\nutilization by 28%, and makespan by 47% over existing state-of-the-art\nschedulers.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}