{"id":"2408.05169","title":"Weak-Annotation of HAR Datasets using Vision Foundation Models","authors":"Marius Bock, Kristof Van Laerhoven, Michael Moeller","authorsParsed":[["Bock","Marius",""],["Van Laerhoven","Kristof",""],["Moeller","Michael",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 16:46:53 GMT"}],"updateDate":"2024-08-12","timestamp":1723222013000,"abstract":"  As wearable-based data annotation remains, to date, a tedious, time-consuming\ntask requiring researchers to dedicate substantial time, benchmark datasets\nwithin the field of Human Activity Recognition in lack richness and size\ncompared to datasets available within related fields. Recently, vision\nfoundation models such as CLIP have gained significant attention, helping the\nvision community advance in finding robust, generalizable feature\nrepresentations. With the majority of researchers within the wearable community\nrelying on vision modalities to overcome the limited expressiveness of wearable\ndata and accurately label their to-be-released benchmark datasets offline, we\npropose a novel, clustering-based annotation pipeline to significantly reduce\nthe amount of data that needs to be annotated by a human annotator. We show\nthat using our approach, the annotation of centroid clips suffices to achieve\naverage labelling accuracies close to 90% across three publicly available HAR\nbenchmark datasets. Using the weakly annotated datasets, we further demonstrate\nthat we can match the accuracy scores of fully-supervised deep learning\nclassifiers across all three benchmark datasets. Code as well as supplementary\nfigures and results are publicly downloadable via\ngithub.com/mariusbock/weak_har.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}