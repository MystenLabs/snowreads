{"id":"2408.02752","title":"Diffusion Models as Data Mining Tools","authors":"Ioannis Siglidis, Aleksander Holynski, Alexei A. Efros, Mathieu Aubry,\n  Shiry Ginosar","authorsParsed":[["Siglidis","Ioannis",""],["Holynski","Aleksander",""],["Efros","Alexei A.",""],["Aubry","Mathieu",""],["Ginosar","Shiry",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 17:14:31 GMT"}],"updateDate":"2024-08-07","timestamp":1721495671000,"abstract":"  This paper demonstrates how to use generative models trained for image\nsynthesis as tools for visual data mining. Our insight is that since\ncontemporary generative models learn an accurate representation of their\ntraining data, we can use them to summarize the data by mining for visual\npatterns. Concretely, we show that after finetuning conditional diffusion\nmodels to synthesize images from a specific dataset, we can use these models to\ndefine a typicality measure on that dataset. This measure assesses how typical\nvisual elements are for different data labels, such as geographic location,\ntime stamps, semantic labels, or even the presence of a disease. This\nanalysis-by-synthesis approach to data mining has two key advantages. First, it\nscales much better than traditional correspondence-based approaches since it\ndoes not require explicitly comparing all pairs of visual elements. Second,\nwhile most previous works on visual data mining focus on a single dataset, our\napproach works on diverse datasets in terms of content and scale, including a\nhistorical car dataset, a historical face dataset, a large worldwide\nstreet-view dataset, and an even larger scene dataset. Furthermore, our\napproach allows for translating visual elements across class labels and\nanalyzing consistent changes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}