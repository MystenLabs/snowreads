{"id":"2408.05241","title":"Large Model Strategic Thinking, Small Model Efficiency: Transferring\n  Theory of Mind in Large Language Models","authors":"Nunzio Lore, Alireza Sepehr Ilami, Babak Heydari","authorsParsed":[["Lore","Nunzio",""],["Ilami","Alireza Sepehr",""],["Heydari","Babak",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 20:49:48 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 19:09:57 GMT"},{"version":"v3","created":"Tue, 20 Aug 2024 18:58:00 GMT"}],"updateDate":"2024-08-22","timestamp":1722890988000,"abstract":"  As the performance of larger, newer Large Language Models continues to\nimprove for strategic Theory of Mind (ToM) tasks, the demand for these\nstate-of-the-art models increases commensurately. However, their deployment is\ncostly both in terms of processing power and time. In this paper, we\ninvestigate the feasibility of creating smaller, highly-performing specialized\nalgorithms by way of fine-tuning. To do this, we first present a large\npre-trained model with 20 unique scenarios that combine different social\ncontexts with games of varying social dilemmas, record its answers, and use\nthem for Q&A fine-tuning on a smaller model of the same family. Our focus is on\nin-context game-theoretic decision-making, the same domain within which human\ninteraction occurs and that requires both a theory of mind (or a semblance\nthereof) and an understanding of social dynamics. The smaller model is\ntherefore trained not just on the answers provided, but also on the motivations\nprovided by the larger model, which should contain advice and guidelines to\nnavigate both strategic dilemmas and social cues. We find that the fine-tuned\nsmaller language model consistently bridged the gap in performance between the\nsmaller pre-trained version of the model and its larger relative and that its\nimprovements extended in areas and contexts beyond the ones provided in the\ntraining examples, including on out-of-sample scenarios that include completely\ndifferent game structures. On average for all games, through fine-tuning, the\nsmaller model showed a 46% improvement measured as alignment towards the\nbehavior of the larger model, with 100% representing indistinguishable\nbehavior. When presented with out-of-sample social contexts and games, the\nfine-tuned model still displays remarkable levels of alignment, reaching an\nimprovement of 18% and 28% respectively.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society","Computing Research Repository/Emerging Technologies","Computing Research Repository/Computer Science and Game Theory"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}