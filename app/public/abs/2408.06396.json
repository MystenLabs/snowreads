{"id":"2408.06396","title":"Design Proteins Using Large Language Models: Enhancements and\n  Comparative Analyses","authors":"Kamyar Zeinalipour, Neda Jamshidi, Monica Bianchini, Marco Maggini,\n  Marco Gori","authorsParsed":[["Zeinalipour","Kamyar",""],["Jamshidi","Neda",""],["Bianchini","Monica",""],["Maggini","Marco",""],["Gori","Marco",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 08:17:27 GMT"}],"updateDate":"2024-08-14","timestamp":1723450647000,"abstract":"  Pre-trained LLMs have demonstrated substantial capabilities across a range of\nconventional natural language processing (NLP) tasks, such as summarization and\nentity recognition. In this paper, we explore the application of LLMs in the\ngeneration of high-quality protein sequences. Specifically, we adopt a suite of\npre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and\ngemma-7B4, to produce valid protein sequences. All of these models are publicly\navailable.5 Unlike previous work in this field, our approach utilizes a\nrelatively small dataset comprising 42,000 distinct human protein sequences. We\nretrain these models to process protein-related data, ensuring the generation\nof biologically feasible protein structures. Our findings demonstrate that even\nwith limited data, the adapted models exhibit efficiency comparable to\nestablished protein-focused models such as ProGen varieties, ProtGPT2, and\nProLLaMA, which were trained on millions of protein sequences. To validate and\nquantify the performance of our models, we conduct comparative analyses\nemploying standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore,\nwe commit to making the trained versions of all four models publicly available,\nfostering greater transparency and collaboration in the field of computational\nbiology.\n","subjects":["Quantitative Biology/Quantitative Methods","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}