{"id":"2408.06352","title":"Using Large Language Models to Compare Explainable Models for Smart Home\n  Human Activity Recognition","authors":"Michele Fiori, Gabriele Civitarese and Claudio Bettini","authorsParsed":[["Fiori","Michele",""],["Civitarese","Gabriele",""],["Bettini","Claudio",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 12:15:07 GMT"}],"updateDate":"2024-08-14","timestamp":1721823307000,"abstract":"  Recognizing daily activities with unobtrusive sensors in smart environments\nenables various healthcare applications. Monitoring how subjects perform\nactivities at home and their changes over time can reveal early symptoms of\nhealth issues, such as cognitive decline. Most approaches in this field use\ndeep learning models, which are often seen as black boxes mapping sensor data\nto activities. However, non-expert users like clinicians need to trust and\nunderstand these models' outputs. Thus, eXplainable AI (XAI) methods for Human\nActivity Recognition have emerged to provide intuitive natural language\nexplanations from these models. Different XAI methods generate different\nexplanations, and their effectiveness is typically evaluated through user\nsurveys, that are often challenging in terms of costs and fairness. This paper\nproposes an automatic evaluation method using Large Language Models (LLMs) to\nidentify, in a pool of candidates, the best XAI approach for non-expert users.\nOur preliminary results suggest that LLM evaluation aligns with user surveys.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"J5ODurQsyaIPvyUsdnj-7OAxgFKkUpqu9g0hPTFK5WQ","pdfSize":"1301611"}
