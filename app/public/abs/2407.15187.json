{"id":"2407.15187","title":"HoloDreamer: Holistic 3D Panoramic World Generation from Text\n  Descriptions","authors":"Haiyang Zhou, Xinhua Cheng, Wangbo Yu, Yonghong Tian, Li Yuan","authorsParsed":[["Zhou","Haiyang",""],["Cheng","Xinhua",""],["Yu","Wangbo",""],["Tian","Yonghong",""],["Yuan","Li",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 14:52:51 GMT"}],"updateDate":"2024-07-23","timestamp":1721573571000,"abstract":"  3D scene generation is in high demand across various domains, including\nvirtual reality, gaming, and the film industry. Owing to the powerful\ngenerative capabilities of text-to-image diffusion models that provide reliable\npriors, the creation of 3D scenes using only text prompts has become viable,\nthereby significantly advancing researches in text-driven 3D scene generation.\nIn order to obtain multiple-view supervision from 2D diffusion models,\nprevailing methods typically employ the diffusion model to generate an initial\nlocal image, followed by iteratively outpainting the local image using\ndiffusion models to gradually generate scenes. Nevertheless, these\noutpainting-based approaches prone to produce global inconsistent scene\ngeneration results without high degree of completeness, restricting their\nbroader applications. To tackle these problems, we introduce HoloDreamer, a\nframework that first generates high-definition panorama as a holistic\ninitialization of the full 3D scene, then leverage 3D Gaussian Splatting\n(3D-GS) to quickly reconstruct the 3D scene, thereby facilitating the creation\nof view-consistent and fully enclosed 3D scenes. Specifically, we propose\nStylized Equirectangular Panorama Generation, a pipeline that combines multiple\ndiffusion models to enable stylized and detailed equirectangular panorama\ngeneration from complex text prompts. Subsequently, Enhanced Two-Stage Panorama\nReconstruction is introduced, conducting a two-stage optimization of 3D-GS to\ninpaint the missing region and enhance the integrity of the scene.\nComprehensive experiments demonstrated that our method outperforms prior works\nin terms of overall visual consistency and harmony as well as reconstruction\nquality and rendering robustness when generating fully enclosed scenes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}