{"id":"2407.07631","title":"Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning","authors":"Dake Zhang, Boxiang Lyu, Shuang Qiu, Mladen Kolar, Tong Zhang","authorsParsed":[["Zhang","Dake",""],["Lyu","Boxiang",""],["Qiu","Shuang",""],["Kolar","Mladen",""],["Zhang","Tong",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:09:52 GMT"}],"updateDate":"2024-07-11","timestamp":1720616992000,"abstract":"  We study risk-sensitive reinforcement learning (RL), a crucial field due to\nits ability to enhance decision-making in scenarios where it is essential to\nmanage uncertainty and minimize potential adverse outcomes. Particularly, our\nwork focuses on applying the entropic risk measure to RL problems. While\nexisting literature primarily investigates the online setting, there remains a\nlarge gap in understanding how to efficiently derive a near-optimal policy\nbased on this risk measure using only a pre-collected dataset. We center on the\nlinear Markov Decision Process (MDP) setting, a well-regarded theoretical\nframework that has yet to be examined from a risk-sensitive standpoint. In\nresponse, we introduce two provably sample-efficient algorithms. We begin by\npresenting a risk-sensitive pessimistic value iteration algorithm, offering a\ntight analysis by leveraging the structure of the risk-sensitive performance\nmeasure. To further improve the obtained bounds, we propose another pessimistic\nalgorithm that utilizes variance information and reference-advantage\ndecomposition, effectively improving both the dependence on the space dimension\n$d$ and the risk-sensitivity factor. To the best of our knowledge, we obtain\nthe first provably efficient risk-sensitive offline RL algorithms.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}