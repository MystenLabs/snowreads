{"id":"2407.10167","title":"Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model","authors":"Xunyu Zhu, Jian Li, Can Ma, Weiping Wang","authorsParsed":[["Zhu","Xunyu",""],["Li","Jian",""],["Ma","Can",""],["Wang","Weiping",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 11:41:03 GMT"},{"version":"v2","created":"Mon, 22 Jul 2024 10:26:23 GMT"},{"version":"v3","created":"Tue, 30 Jul 2024 08:59:26 GMT"}],"updateDate":"2024-07-31","timestamp":1720957263000,"abstract":"  Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}