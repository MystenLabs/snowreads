{"id":"2407.12820","title":"PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference","authors":"Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao,\n  Xiaonan Nie, Weipeng Chen, Bin Cui","authorsParsed":[["Zhang","Hailin",""],["Ji","Xiaodong",""],["Chen","Yilin",""],["Fu","Fangcheng",""],["Miao","Xupeng",""],["Nie","Xiaonan",""],["Chen","Weipeng",""],["Cui","Bin",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 13:05:42 GMT"}],"updateDate":"2024-07-19","timestamp":1719839142000,"abstract":"  As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), a crucial\ncomponent in LLM inference, has now become the primary memory bottleneck due to\nlimited GPU memory. Current methods selectively determine suitable keys and\nvalues for self-attention computation in LLMs to address the issue. However,\nthey either fall short in maintaining model quality or result in high serving\nlatency. Drawing inspiration from advanced embedding retrieval techniques used\nin the database community, we consider the storage and searching of KVCache as\na typical embedding retrieval problem. We propose PQCache, which employs\nProduct Quantization (PQ) to manage KVCache, maintaining model quality while\nensuring low serving latency. During the prefilling phase, we apply PQ to\ntokens' keys for each LLM layer and head. During the autoregressive decoding\nphase, for each newly generated token, we first identify important tokens\nthrough Maximum Inner-Product Search (MIPS) using PQ codes and centroids, then\nfetch the corresponding key-value pairs for self-attention computation. Through\nmeticulous design of overlapping and caching, we minimize any additional\ncomputation and communication overhead during both phases. Extensive\nexperiments show that PQCache achieves both effectiveness and efficiency. It\nmaintains model quality even when only 1/5 of the tokens are involved in\nattention, while attaining acceptable system latency.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}