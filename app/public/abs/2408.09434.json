{"id":"2408.09434","title":"HySem: A context length optimized LLM pipeline for unstructured tabular\n  extraction","authors":"Narayanan PP, Anantharaman Palacode Narayana Iyer","authorsParsed":[["PP","Narayanan",""],["Iyer","Anantharaman Palacode Narayana",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 10:40:37 GMT"}],"updateDate":"2024-08-20","timestamp":1723977637000,"abstract":"  Regulatory compliance reporting in the pharmaceutical industry relies on\ndetailed tables, but these are often under-utilized beyond compliance due to\ntheir unstructured format and arbitrary content. Extracting and semantically\nrepresenting tabular data is challenging due to diverse table presentations.\nLarge Language Models (LLMs) demonstrate substantial potential for semantic\nrepresentation, yet they encounter challenges related to accuracy and context\nsize limitations, which are crucial considerations for the industry\napplications. We introduce HySem, a pipeline that employs a novel context\nlength optimization technique to generate accurate semantic JSON\nrepresentations from HTML tables. This approach utilizes a custom fine-tuned\nmodel specifically designed for cost- and privacy-sensitive small and medium\npharmaceutical enterprises. Running on commodity hardware and leveraging\nopen-source models, our auto-correcting agents rectify both syntax and semantic\nerrors in LLM-generated content. HySem surpasses its peer open-source models in\naccuracy and provides competitive performance when benchmarked against OpenAI\nGPT-4o and effectively addresses context length limitations, which is a crucial\nfactor for supporting larger tables.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}