{"id":"2408.14380","title":"Probing Causality Manipulation of Large Language Models","authors":"Chenyang Zhang, Haibo Tong, Bin Zhang, Dongyu Zhang","authorsParsed":[["Zhang","Chenyang",""],["Tong","Haibo",""],["Zhang","Bin",""],["Zhang","Dongyu",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 16:00:41 GMT"}],"updateDate":"2024-08-27","timestamp":1724688041000,"abstract":"  Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}