{"id":"2408.10647","title":"Privacy-preserving Universal Adversarial Defense for Black-box Models","authors":"Qiao Li, Cong Wu, Jing Chen, Zijun Zhang, Kun He, Ruiying Du, Xinxin\n  Wang, Qingchuang Zhao, Yang Liu","authorsParsed":[["Li","Qiao",""],["Wu","Cong",""],["Chen","Jing",""],["Zhang","Zijun",""],["He","Kun",""],["Du","Ruiying",""],["Wang","Xinxin",""],["Zhao","Qingchuang",""],["Liu","Yang",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 08:40:39 GMT"}],"updateDate":"2024-08-21","timestamp":1724143239000,"abstract":"  Deep neural networks (DNNs) are increasingly used in critical applications\nsuch as identity authentication and autonomous driving, where robustness\nagainst adversarial attacks is crucial. These attacks can exploit minor\nperturbations to cause significant prediction errors, making it essential to\nenhance the resilience of DNNs. Traditional defense methods often rely on\naccess to detailed model information, which raises privacy concerns, as model\nowners may be reluctant to share such data. In contrast, existing black-box\ndefense methods fail to offer a universal defense against various types of\nadversarial attacks. To address these challenges, we introduce DUCD, a\nuniversal black-box defense method that does not require access to the target\nmodel's parameters or architecture. Our approach involves distilling the target\nmodel by querying it with data, creating a white-box surrogate while preserving\ndata privacy. We further enhance this surrogate model using a certified defense\nbased on randomized smoothing and optimized noise selection, enabling robust\ndefense against a broad range of adversarial attacks. Comparative evaluations\nbetween the certified defenses of the surrogate and target models demonstrate\nthe effectiveness of our approach. Experiments on multiple image classification\ndatasets show that DUCD not only outperforms existing black-box defenses but\nalso matches the accuracy of white-box defenses, all while enhancing data\nprivacy and reducing the success rate of membership inference attacks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"vKd9AfLe5-c1xPGxa7wXEGepXI6wbmow81PvG1BBt7k","pdfSize":"3459725"}
