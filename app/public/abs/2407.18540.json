{"id":"2407.18540","title":"A Universal Prompting Strategy for Extracting Process Model Information\n  from Natural Language Text using Large Language Models","authors":"Julian Neuberger, Lars Ackermann, Han van der Aa, Stefan Jablonski","authorsParsed":[["Neuberger","Julian",""],["Ackermann","Lars",""],["van der Aa","Han",""],["Jablonski","Stefan",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 06:39:35 GMT"}],"updateDate":"2024-07-29","timestamp":1721975975000,"abstract":"  Over the past decade, extensive research efforts have been dedicated to the\nextraction of information from textual process descriptions. Despite the\nremarkable progress witnessed in natural language processing (NLP), information\nextraction within the Business Process Management domain remains predominantly\nreliant on rule-based systems and machine learning methodologies. Data scarcity\nhas so far prevented the successful application of deep learning techniques.\nHowever, the rapid progress in generative large language models (LLMs) makes it\npossible to solve many NLP tasks with very high quality without the need for\nextensive data. Therefore, we systematically investigate the potential of LLMs\nfor extracting information from textual process descriptions, targeting the\ndetection of process elements such as activities and actors, and relations\nbetween them. Using a heuristic algorithm, we demonstrate the suitability of\nthe extracted information for process model generation. Based on a novel\nprompting strategy, we show that LLMs are able to outperform state-of-the-art\nmachine learning approaches with absolute performance improvements of up to 8\\%\n$F_1$ score across three different datasets. We evaluate our prompting strategy\non eight different LLMs, showing it is universally applicable, while also\nanalyzing the impact of certain prompt parts on extraction quality. The number\nof example texts, the specificity of definitions, and the rigour of format\ninstructions are identified as key for improving the accuracy of extracted\ninformation. Our code, prompts, and data are publicly available.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6ayk9uivk12l8ZZrXNIlpzbwSMqS-JWn_vc7T7P5ct8","pdfSize":"829418"}
