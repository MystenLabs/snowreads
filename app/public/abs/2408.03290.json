{"id":"2408.03290","title":"SARA: Singular-Value Based Adaptive Low-Rank Adaption","authors":"Jihao Gu and Shuai Chen and Zelin Wang and Yibo Zhang and Ping Gong","authorsParsed":[["Gu","Jihao",""],["Chen","Shuai",""],["Wang","Zelin",""],["Zhang","Yibo",""],["Gong","Ping",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 16:39:42 GMT"}],"updateDate":"2024-08-07","timestamp":1722962382000,"abstract":"  With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}