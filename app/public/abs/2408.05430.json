{"id":"2408.05430","title":"HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at\n  Kuaishou","authors":"Xu Wang, Jiangxia Cao, Zhiyi Fu, Kun Gai, Guorui Zhou","authorsParsed":[["Wang","Xu",""],["Cao","Jiangxia",""],["Fu","Zhiyi",""],["Gai","Kun",""],["Zhou","Guorui",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 04:25:48 GMT"}],"updateDate":"2024-08-13","timestamp":1723263948000,"abstract":"  In this paper, we present the practical problems and the lessons learned at\nshort-video services from Kuaishou. In industry, a widely-used multi-task\nframework is the Mixture-of-Experts (MoE) paradigm, which always introduces\nsome shared and specific experts for each task and then uses gate networks to\nmeasure related experts' contributions. Although the MoE achieves remarkable\nimprovements, we still observe three anomalies that seriously affect model\nperformances in our iteration: (1) Expert Collapse: We found that experts'\noutput distributions are significantly different, and some experts have over\n90% zero activations with ReLU, making it hard for gate networks to assign fair\nweights to balance experts. (2) Expert Degradation: Ideally, the shared-expert\naims to provide predictive information for all tasks simultaneously.\nNevertheless, we find that some shared-experts are occupied by only one task,\nwhich indicates that shared-experts lost their ability but degenerated into\nsome specific-experts. (3) Expert Underfitting: In our services, we have dozens\nof behavior tasks that need to be predicted, but we find that some data-sparse\nprediction tasks tend to ignore their specific-experts and assign large weights\nto shared-experts. The reason might be that the shared-experts can perceive\nmore gradient updates and knowledge from dense tasks, while specific-experts\neasily fall into underfitting due to their sparse behaviors. Motivated by those\nobservations, we propose HoME to achieve a simple, efficient and balanced MoE\nsystem for multi-task learning.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}