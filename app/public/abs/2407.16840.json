{"id":"2407.16840","title":"Synth4Kws: Synthesized Speech for User Defined Keyword Spotting in Low\n  Resource Environments","authors":"Pai Zhu, Dhruuv Agarwal, Jacob W. Bartel, Kurt Partridge, Hyun Jin\n  Park, Quan Wang","authorsParsed":[["Zhu","Pai",""],["Agarwal","Dhruuv",""],["Bartel","Jacob W.",""],["Partridge","Kurt",""],["Park","Hyun Jin",""],["Wang","Quan",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 21:05:44 GMT"}],"updateDate":"2024-07-25","timestamp":1721768744000,"abstract":"  One of the challenges in developing a high quality custom keyword spotting\n(KWS) model is the lengthy and expensive process of collecting training data\ncovering a wide range of languages, phrases and speaking styles. We introduce\nSynth4Kws - a framework to leverage Text to Speech (TTS) synthesized data for\ncustom KWS in different resource settings. With no real data, we found\nincreasing TTS phrase diversity and utterance sampling monotonically improves\nmodel performance, as evaluated by EER and AUC metrics over 11k utterances of\nthe speech command dataset. In low resource settings, with 50k real utterances\nas a baseline, we found using optimal amounts of TTS data can improve EER by\n30.1% and AUC by 46.7%. Furthermore, we mix TTS data with varying amounts of\nreal data and interpolate the real data needed to achieve various quality\ntargets. Our experiments are based on English and single word utterances but\nthe findings generalize to i18n languages and other keyword types.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}