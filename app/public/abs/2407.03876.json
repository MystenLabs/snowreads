{"id":"2407.03876","title":"DART: Deep Adversarial Automated Red Teaming for LLM Safety","authors":"Bojian Jiang, Yi Jing, Tianhao Shen, Qing Yang, Deyi Xiong","authorsParsed":[["Jiang","Bojian",""],["Jing","Yi",""],["Shen","Tianhao",""],["Yang","Qing",""],["Xiong","Deyi",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 12:14:27 GMT"}],"updateDate":"2024-07-08","timestamp":1720095267000,"abstract":"  Manual Red teaming is a commonly-used method to identify vulnerabilities in\nlarge language models (LLMs), which, is costly and unscalable. In contrast,\nautomated red teaming uses a Red LLM to automatically generate adversarial\nprompts to the Target LLM, offering a scalable way for safety vulnerability\ndetection. However, the difficulty of building a powerful automated Red LLM\nlies in the fact that the safety vulnerabilities of the Target LLM are\ndynamically changing with the evolution of the Target LLM. To mitigate this\nissue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in\nwhich the Red LLM and Target LLM are deeply and dynamically interacting with\neach other in an iterative manner. In each iteration, in order to generate\nsuccessful attacks as many as possible, the Red LLM not only takes into account\nthe responses from the Target LLM, but also adversarially adjust its attacking\ndirections by monitoring the global diversity of generated attacks across\nmultiple iterations. Simultaneously, to explore dynamically changing safety\nvulnerabilities of the Target LLM, we allow the Target LLM to enhance its\nsafety via an active learning based data selection mechanism. Experimential\nresults demonstrate that DART significantly reduces the safety risk of the\ntarget LLM. For human evaluation on Anthropic Harmless dataset, compared to the\ninstruction-tuning target LLM, DART eliminates the violation risks by 53.4\\%.\nWe will release the datasets and codes of DART soon.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}