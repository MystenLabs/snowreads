{"id":"2408.12742","title":"TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based\n  Computing","authors":"Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim and\n  Priyadarshini Panda","authorsParsed":[["Moitra","Abhishek",""],["Bhattacharjee","Abhiroop",""],["Kim","Youngeun",""],["Panda","Priyadarshini",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 21:51:38 GMT"}],"updateDate":"2024-08-26","timestamp":1724363498000,"abstract":"  Due to the high computation overhead of Vision Transformers (ViTs), In-memory\nComputing architectures are being researched towards energy-efficient\ndeployment in edge-computing scenarios. Prior works have proposed efficient\nalgorithm-hardware co-design and IMC-architectural improvements to improve the\nenergy-efficiency of IMC-implemented ViTs. However, all prior works have\nneglected the overhead and co-depencence of attention blocks on the\naccuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose\nTReX- an attention-reuse-driven ViT optimization framework that effectively\nperforms attention reuse in ViT models to achieve optimal\naccuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer\nencoders for attention reuse to achieve near iso-accuracy performance while\nmeeting the user-specified delay requirement. Based on our analysis on the\nImagenet-1k dataset, we find that TReX achieves 2.3x (2.19x) EDAP reduction and\n1.86x (1.79x) TOPS/mm2 improvement with ~1% accuracy drop in case of DeiT-S\n(LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP\nreduction compared to state-of-the-art token pruning and weight sharing\napproaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal\naccuracy compared to baseline at 1.6x lower EDAP.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Hardware Architecture"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gMSV9DK42Lz-bJd5r8VJfER3xRBkx5rRfvWYFj9Nmuk","pdfSize":"2646680"}
