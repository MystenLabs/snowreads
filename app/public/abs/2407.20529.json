{"id":"2407.20529","title":"Can LLMs be Fooled? Investigating Vulnerabilities in LLMs","authors":"Sara Abdali, Jia He, CJ Barberan, Richard Anarfi","authorsParsed":[["Abdali","Sara",""],["He","Jia",""],["Barberan","CJ",""],["Anarfi","Richard",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 04:08:00 GMT"}],"updateDate":"2024-07-31","timestamp":1722312480000,"abstract":"  The advent of Large Language Models (LLMs) has garnered significant\npopularity and wielded immense power across various domains within Natural\nLanguage Processing (NLP). While their capabilities are undeniably impressive,\nit is crucial to identify and scrutinize their vulnerabilities especially when\nthose vulnerabilities can have costly consequences. One such LLM, trained to\nprovide a concise summarization from medical documents could unequivocally leak\npersonal patient data when prompted surreptitiously. This is just one of many\nunfortunate examples that have been unveiled and further research is necessary\nto comprehend the underlying reasons behind such vulnerabilities. In this\nstudy, we delve into multiple sections of vulnerabilities which are\nmodel-based, training-time, inference-time vulnerabilities, and discuss\nmitigation strategies including \"Model Editing\" which aims at modifying LLMs\nbehavior, and \"Chroma Teaming\" which incorporates synergy of multiple teaming\nstrategies to enhance LLMs' resilience. This paper will synthesize the findings\nfrom each vulnerability section and propose new directions of research and\ndevelopment. By understanding the focal points of current vulnerabilities, we\ncan better anticipate and mitigate future risks, paving the road for more\nrobust and secure LLMs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}