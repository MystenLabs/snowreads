{"id":"2407.06947","title":"Audio-Language Datasets of Scenes and Events: A Survey","authors":"Gijs Wijngaard, Elia Formisano, Michele Esposito, Michel Dumontier","authorsParsed":[["Wijngaard","Gijs",""],["Formisano","Elia",""],["Esposito","Michele",""],["Dumontier","Michel",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 15:23:35 GMT"}],"updateDate":"2024-07-10","timestamp":1720538615000,"abstract":"  Audio-language models (ALMs) process sounds to provide a linguistic\ndescription of sound-producing events and scenes. Recent advances in computing\npower and dataset creation have led to significant progress in this domain.\nThis paper surveys existing datasets used for training audio-language models,\nemphasizing the recent trend towards using large, diverse datasets to enhance\nmodel performance. Key sources of these datasets include the Freesound platform\nand AudioSet that have contributed to the field's rapid growth. Although prior\nsurveys primarily address techniques and training details, this survey\ncategorizes and evaluates a wide array of datasets, addressing their origins,\ncharacteristics, and use cases. It also performs a data leak analysis to ensure\ndataset integrity and mitigate bias between datasets. This survey was conducted\nby analyzing research papers up to and including December 2023, and does not\ncontain any papers after that period.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}