{"id":"2407.02911","title":"Non-Adversarial Learning: Vector-Quantized Common Latent Space for\n  Multi-Sequence MRI","authors":"Luyi Han, Tao Tan, Tianyu Zhang, Xin Wang, Yuan Gao, Chunyao Lu,\n  Xinglong Liang, Haoran Dou, Yunzhi Huang, Ritse Mann","authorsParsed":[["Han","Luyi",""],["Tan","Tao",""],["Zhang","Tianyu",""],["Wang","Xin",""],["Gao","Yuan",""],["Lu","Chunyao",""],["Liang","Xinglong",""],["Dou","Haoran",""],["Huang","Yunzhi",""],["Mann","Ritse",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 08:37:01 GMT"}],"updateDate":"2024-07-04","timestamp":1719995821000,"abstract":"  Adversarial learning helps generative models translate MRI from source to\ntarget sequence when lacking paired samples. However, implementing MRI\nsynthesis with adversarial learning in clinical settings is challenging due to\ntraining instability and mode collapse. To address this issue, we leverage\nintermediate sequences to estimate the common latent space among multi-sequence\nMRI, enabling the reconstruction of distinct sequences from the common latent\nspace. We propose a generative model that compresses discrete representations\nof each sequence to estimate the Gaussian distribution of vector-quantized\ncommon (VQC) latent space between multiple sequences. Moreover, we improve the\nlatent space consistency with contrastive learning and increase model stability\nby domain augmentation. Experiments using BraTS2021 dataset show that our\nnon-adversarial model outperforms other GAN-based methods, and VQC latent space\naids our model to achieve (1) anti-interference ability, which can eliminate\nthe effects of noise, bias fields, and artifacts, and (2) solid semantic\nrepresentation ability, with the potential of one-shot segmentation. Our code\nis publicly available.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}