{"id":"2408.11552","title":"Explainable Deep Learning Framework for Human Activity Recognition","authors":"Yiran Huang and Yexu Zhou and Haibin Zhao and Till Riedel and Michael\n  Beigl","authorsParsed":[["Huang","Yiran",""],["Zhou","Yexu",""],["Zhao","Haibin",""],["Riedel","Till",""],["Beigl","Michael",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 11:59:55 GMT"}],"updateDate":"2024-08-22","timestamp":1724241595000,"abstract":"  In the realm of human activity recognition (HAR), the integration of\nexplainable Artificial Intelligence (XAI) emerges as a critical necessity to\nelucidate the decision-making processes of complex models, fostering\ntransparency and trust. Traditional explanatory methods like Class Activation\nMapping (CAM) and attention mechanisms, although effective in highlighting\nregions vital for decisions in various contexts, prove inadequate for HAR. This\ninadequacy stems from the inherently abstract nature of HAR data, rendering\nthese explanations obscure. In contrast, state-of-th-art post-hoc\ninterpretation techniques for time series can explain the model from other\nperspectives. However, this requires extra effort. It usually takes 10 to 20\nseconds to generate an explanation. To overcome these challenges, we proposes a\nnovel, model-agnostic framework that enhances both the interpretability and\nefficacy of HAR models through the strategic use of competitive data\naugmentation. This innovative approach does not rely on any particular model\narchitecture, thereby broadening its applicability across various HAR models.\nBy implementing competitive data augmentation, our framework provides intuitive\nand accessible explanations of model decisions, thereby significantly advancing\nthe interpretability of HAR systems without compromising on performance.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}