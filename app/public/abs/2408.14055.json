{"id":"2408.14055","title":"HAPM -- Hardware Aware Pruning Method for CNN hardware accelerators in\n  resource constrained devices","authors":"Federico Nicolas Peccia, Luciano Ferreyro, Alejandro Furfaro","authorsParsed":[["Peccia","Federico Nicolas",""],["Ferreyro","Luciano",""],["Furfaro","Alejandro",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 07:27:12 GMT"}],"updateDate":"2024-08-27","timestamp":1724657232000,"abstract":"  During the last years, algorithms known as Convolutional Neural Networks\n(CNNs) had become increasingly popular, expanding its application range to\nseveral areas. In particular, the image processing field has experienced a\nremarkable advance thanks to this algorithms. In IoT, a wide research field\naims to develop hardware capable of execute them at the lowest possible energy\ncost, but keeping acceptable image inference time. One can get around this\napparently conflicting objectives by applying design and training techniques.\nThe present work proposes a generic hardware architecture ready to be\nimplemented on FPGA devices, supporting a wide range of configurations which\nallows the system to run different neural network architectures, dynamically\nexploiting the sparsity caused by pruning techniques in the mathematical\noperations present in this kind of algorithms. The inference speed of the\ndesign is evaluated over different resource constrained FPGA devices. Finally,\nthe standard pruning algorithm is compared against a custom pruning technique\nspecifically designed to exploit the scheduling properties of this hardware\naccelerator. We demonstrate that our hardware-aware pruning algorithm achieves\na remarkable improvement of a 45 % in inference time compared to a network\npruned using the standard algorithm.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rNFfScf2AfOIV9ZYgCDdwdq7LLRjSrM1YIrf9tBII7I","pdfSize":"463125"}
