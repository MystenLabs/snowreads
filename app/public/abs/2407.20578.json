{"id":"2407.20578","title":"Comparison of Large Language Models for Generating Contextually Relevant\n  Questions","authors":"Ivo Lodovico Molina, Valdemar \\v{S}v\\'abensk\\'y, Tsubasa Minematsu, Li\n  Chen, Fumiya Okubo, Atsushi Shimada","authorsParsed":[["Molina","Ivo Lodovico",""],["Švábenský","Valdemar",""],["Minematsu","Tsubasa",""],["Chen","Li",""],["Okubo","Fumiya",""],["Shimada","Atsushi",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 06:23:59 GMT"},{"version":"v2","created":"Sun, 15 Sep 2024 07:23:10 GMT"}],"updateDate":"2024-09-17","timestamp":1722320639000,"abstract":"  This study explores the effectiveness of Large Language Models (LLMs) for\nAutomatic Question Generation in educational settings. Three LLMs are compared\nin their ability to create questions from university slide text without\nfine-tuning. Questions were obtained in a two-step pipeline: first, answer\nphrases were extracted from slides using Llama 2-Chat 13B; then, the three\nmodels generated questions for each answer. To analyze whether the questions\nwould be suitable in educational applications for students, a survey was\nconducted with 46 students who evaluated a total of 246 questions across five\nmetrics: clarity, relevance, difficulty, slide relation, and question-answer\nalignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan\nT5 XXL by a small margin, particularly in terms of clarity and question-answer\nalignment. GPT-3.5 especially excels at tailoring questions to match the input\nanswers. The contribution of this research is the analysis of the capacity of\nLLMs for Automatic Question Generation in education.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}