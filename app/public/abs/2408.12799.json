{"id":"2408.12799","title":"Less for More: Enhancing Preference Learning in Generative Language\n  Models with Automated Self-Curation of Training Corpora","authors":"JoonHo Lee, JuYoun Son, Juree Seok, Wooseok Jang and Yeong-Dae Kwon","authorsParsed":[["Lee","JoonHo",""],["Son","JuYoun",""],["Seok","Juree",""],["Jang","Wooseok",""],["Kwon","Yeong-Dae",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 02:27:14 GMT"}],"updateDate":"2024-08-26","timestamp":1724380034000,"abstract":"  Ambiguity in language presents challenges in developing more enhanced\nlanguage models, particularly in preference learning, where variability among\nannotators results in inconsistently annotated datasets used for model\nalignment. To address this issue, we introduce a self-curation method that\npreprocesses annotated datasets by leveraging proxy models trained directly on\nthese datasets. Our method enhances preference learning by automatically\ndetecting and removing ambiguous annotations within the dataset. The proposed\napproach is validated through extensive experiments, demonstrating a marked\nimprovement in performance across various instruction-following tasks. Our work\nprovides a straightforward and reliable method to overcome annotation\ninconsistencies, serving as an initial step towards the development of more\nadvanced preference learning techniques.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}