{"id":"2407.04951","title":"Optimal Quantized Compressed Sensing via Projected Gradient Descent","authors":"Junren Chen, Ming Yuan","authorsParsed":[["Chen","Junren",""],["Yuan","Ming",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 04:06:36 GMT"}],"updateDate":"2024-07-09","timestamp":1720238796000,"abstract":"  This paper provides a unified treatment to the recovery of structured signals\nliving in a star-shaped set from general quantized measurements\n$\\mathcal{Q}(\\mathbf{A}\\mathbf{x}-\\mathbf{\\tau})$, where $\\mathbf{A}$ is a\nsensing matrix, $\\mathbf{\\tau}$ is a vector of (possibly random) quantization\nthresholds, and $\\mathcal{Q}$ denotes an $L$-level quantizer. The ideal\nestimator with consistent quantized measurements is optimal in some important\ninstances but typically infeasible to compute. To this end, we study the\nprojected gradient descent (PGD) algorithm with respect to the one-sided\n$\\ell_1$-loss and identify the conditions under which PGD achieves the same\nerror rate, up to logarithmic factors. For multi-bit case, these conditions\nonly ensure local convergence, and we further develop a complementary approach\nbased on product embedding. When applied to popular models such as 1-bit\ncompressed sensing with Gaussian $\\mathbf{A}$ and zero $\\mathbf{\\tau}$ and the\ndithered 1-bit/multi-bit models with sub-Gaussian $\\mathbf{A}$ and uniform\ndither $\\mathbf{\\tau}$, our unified treatment yields error rates that improve\non or match the sharpest results in all instances. Particularly, PGD achieves\nthe information-theoretic optimal rate $\\tilde{O}(\\frac{k}{mL})$ for recovering\n$k$-sparse signals, and the rate $\\tilde{O}((\\frac{k}{mL})^{1/3})$ for\neffectively sparse signals. For 1-bit compressed sensing of sparse signals, our\nresult recovers the optimality of normalized binary iterative hard thresholding\n(NBIHT) that was proved very recently.\n","subjects":["Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}