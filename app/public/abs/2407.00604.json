{"id":"2407.00604","title":"Fast-OverlaPIM: A Fast Overlap-driven Mapping Framework for Processing\n  In-Memory Neural Network Acceleration","authors":"Xuan Wang, Minxuan Zhou, Tajana Rosing","authorsParsed":[["Wang","Xuan",""],["Zhou","Minxuan",""],["Rosing","Tajana",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 06:13:33 GMT"}],"updateDate":"2024-07-02","timestamp":1719728013000,"abstract":"  Processing in-memory (PIM) is promising to accelerate neural networks (NNs)\nbecause it minimizes data movement and provides large computational\nparallelism. Similar to machine learning accelerators, application mapping,\nwhich determines the operation scheduling and data layout, plays a critical\nrole in the NN acceleration on PIM. The mapping optimization of previous NN\naccelerators focused on optimizing the latency of sequential execution.\nHowever, PIM accelerators feature a distinct design space of application\nmapping from conventional NN accelerators, due to the spatial execution of NN\nlayers across different memory locations. This enables opportunities for\noverlapping execution of consecutive NN layers to improve the latency, where\nthe succeeding layer can start execution before the preceding layer fully\ncompletes the computation. In this paper, we propose Fast-OverlaPIM framework\nthat incorporates the computational overlapping optimization into the DNN\nmapping exploration process on PIM architectures. Fast-OverlaPIM includes\nanalytical algorithms for fast and accurate overlap analysis. Furthermore, it\nproposes a novel mapping search strategy and a transformation mechanism to\nenable efficient design space exploration on the overlap-based mapping for the\nwhole network. Our framework demonstrates a significant improvement in runtime\nperformance from 3.4x to 323.1x compared to the previous state-of-the-art\noverlap-based framework. Our experiments show that Fast-OverlaPIM can\nefficiently produce mappings that are 4.6x to 18.1x faster than the\nstate-of-the-art mapping optimization framework under the same architecture\nconstraints.\n","subjects":["Computing Research Repository/Hardware Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}