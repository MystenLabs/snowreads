{"id":"2408.16450","title":"What to Preserve and What to Transfer: Faithful, Identity-Preserving\n  Diffusion-based Hairstyle Transfer","authors":"Chaeyeon Chung, Sunghyun Park, Jeongho Kim, Jaegul Choo","authorsParsed":[["Chung","Chaeyeon",""],["Park","Sunghyun",""],["Kim","Jeongho",""],["Choo","Jaegul",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 11:30:21 GMT"}],"updateDate":"2024-08-30","timestamp":1724931021000,"abstract":"  Hairstyle transfer is a challenging task in the image editing field that\nmodifies the hairstyle of a given face image while preserving its other\nappearance and background features. The existing hairstyle transfer approaches\nheavily rely on StyleGAN, which is pre-trained on cropped and aligned face\nimages. Hence, they struggle to generalize under challenging conditions such as\nextreme variations of head poses or focal lengths. To address this issue, we\npropose a one-stage hairstyle transfer diffusion model, HairFusion, that\napplies to real-world scenarios. Specifically, we carefully design a\nhair-agnostic representation as the input of the model, where the original hair\ninformation is thoroughly eliminated. Next, we introduce a hair align\ncross-attention (Align-CA) to accurately align the reference hairstyle with the\nface image while considering the difference in their face shape. To enhance the\npreservation of the face image's original features, we leverage adaptive hair\nblending during the inference, where the output's hair regions are estimated by\nthe cross-attention map in Align-CA and blended with non-hair areas of the face\nimage. Our experimental results show that our method achieves state-of-the-art\nperformance compared to the existing methods in preserving the integrity of\nboth the transferred hairstyle and the surrounding features. The codes are\navailable at https://github.com/cychungg/HairFusion.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}