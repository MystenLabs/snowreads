{"id":"2408.05503","title":"Disentangled Noisy Correspondence Learning","authors":"Zhuohang Dang, Minnan Luo, Jihong Wang, Chengyou Jia, Haochen Han,\n  Herun Wan, Guang Dai, Xiaojun Chang and Jingdong Wang","authorsParsed":[["Dang","Zhuohang",""],["Luo","Minnan",""],["Wang","Jihong",""],["Jia","Chengyou",""],["Han","Haochen",""],["Wan","Herun",""],["Dai","Guang",""],["Chang","Xiaojun",""],["Wang","Jingdong",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 09:49:55 GMT"}],"updateDate":"2024-08-13","timestamp":1723283395000,"abstract":"  Cross-modal retrieval is crucial in understanding latent correspondences\nacross modalities. However, existing methods implicitly assume well-matched\ntraining data, which is impractical as real-world data inevitably involves\nimperfect alignments, i.e., noisy correspondences. Although some works explore\nsimilarity-based strategies to address such noise, they suffer from sub-optimal\nsimilarity predictions influenced by modality-exclusive information (MEI),\ne.g., background noise in images and abstract definitions in texts. This issue\narises as MEI is not shared across modalities, thus aligning it in training can\nmarkedly mislead similarity predictions. Moreover, although intuitive, directly\napplying previous cross-modal disentanglement methods suffers from limited\nnoise tolerance and disentanglement efficacy. Inspired by the robustness of\ninformation bottlenecks against noise, we introduce DisNCL, a novel\ninformation-theoretic framework for feature Disentanglement in Noisy\nCorrespondence Learning, to adaptively balance the extraction of MII and MEI\nwith certifiable optimal cross-modal disentanglement efficacy. DisNCL then\nenhances similarity predictions in modality-invariant subspace, thereby greatly\nboosting similarity-based alleviation strategy for noisy correspondences.\nFurthermore, DisNCL introduces soft matching targets to model noisy\nmany-to-many relationships inherent in multi-modal input for noise-robust and\naccurate cross-modal alignment. Extensive experiments confirm DisNCL's efficacy\nby 2% average recall improvement. Mutual information estimation and\nvisualization results show that DisNCL learns meaningful MII/MEI subspaces,\nvalidating our theoretical analyses.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}