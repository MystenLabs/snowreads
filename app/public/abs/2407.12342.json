{"id":"2407.12342","title":"Word Embedding Dimension Reduction via Weakly-Supervised Feature\n  Selection","authors":"Jintang Xue, Yun-Cheng Wang, Chengwei Wei, C.-C. Jay Kuo","authorsParsed":[["Xue","Jintang",""],["Wang","Yun-Cheng",""],["Wei","Chengwei",""],["Kuo","C. -C. Jay",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 06:36:09 GMT"}],"updateDate":"2024-07-18","timestamp":1721198169000,"abstract":"  As a fundamental task in natural language processing, word embedding converts\neach word into a representation in a vector space. A challenge with word\nembedding is that as the vocabulary grows, the vector space's dimension\nincreases and it can lead to a vast model size. Storing and processing word\nvectors are resource-demanding, especially for mobile edge-devices\napplications. This paper explores word embedding dimension reduction. To\nbalance computational costs and performance, we propose an efficient and\neffective weakly-supervised feature selection method, named WordFS. It has two\nvariants, each utilizing novel criteria for feature selection. Experiments\nconducted on various tasks (e.g., word and sentence similarity and binary and\nmulti-class classification) indicate that the proposed WordFS model outperforms\nother dimension reduction methods at lower computational costs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}