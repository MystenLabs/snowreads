{"id":"2408.10848","title":"Perception-guided Jailbreak against Text-to-Image Models","authors":"Yihao Huang, Le Liang, Tianlin Li, Xiaojun Jia, Run Wang, Weikai Miao,\n  Geguang Pu, Yang Liu","authorsParsed":[["Huang","Yihao",""],["Liang","Le",""],["Li","Tianlin",""],["Jia","Xiaojun",""],["Wang","Run",""],["Miao","Weikai",""],["Pu","Geguang",""],["Liu","Yang",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 13:40:25 GMT"},{"version":"v2","created":"Mon, 26 Aug 2024 03:19:45 GMT"}],"updateDate":"2024-08-27","timestamp":1724161225000,"abstract":"  In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}