{"id":"2408.12211","title":"Computer-Aided Fall Recognition Using a Three-Stream Spatial-Temporal\n  GCN Model with Adaptive Feature Aggregation","authors":"Jungpil Shin, Abu Saleh Musa Miah, Rei Egawa1, Koki Hirooka, Md. Al\n  Mehedi Hasan, Yoichi Tomioka, and Yong Seok Hwang","authorsParsed":[["Shin","Jungpil",""],["Miah","Abu Saleh Musa",""],["Egawa1","Rei",""],["Hirooka","Koki",""],["Hasan","Md. Al Mehedi",""],["Tomioka","Yoichi",""],["Hwang","Yong Seok",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 08:40:04 GMT"}],"updateDate":"2024-08-23","timestamp":1724316004000,"abstract":"  The prevention of falls is paramount in modern healthcare, particularly for\nthe elderly, as falls can lead to severe injuries or even fatalities.\nAdditionally, the growing incidence of falls among the elderly, coupled with\nthe urgent need to prevent suicide attempts resulting from medication overdose,\nunderscores the critical importance of accurate and efficient fall detection\nmethods. In this scenario, a computer-aided fall detection system is inevitable\nto save elderly people's lives worldwide. Many researchers have been working to\ndevelop fall detection systems. However, the existing fall detection systems\noften struggle with issues such as unsatisfactory performance accuracy, limited\nrobustness, high computational complexity, and sensitivity to environmental\nfactors due to a lack of effective features. In response to these challenges,\nthis paper proposes a novel three-stream spatial-temporal feature-based fall\ndetection system. Our system incorporates joint skeleton-based spatial and\ntemporal Graph Convolutional Network (GCN) features, joint motion-based spatial\nand temporal GCN features, and residual connections-based features. Each stream\nemploys adaptive graph-based feature aggregation and consecutive separable\nconvolutional neural networks (Sep-TCN), significantly reducing computational\ncomplexity and model parameters compared to prior systems. Experimental results\nacross multiple datasets demonstrate the superior effectiveness and efficiency\nof our proposed system, with accuracies of 99.51\\%, 99.15\\%, 99.79\\% and 99.85\n\\% achieved on the ImViA, UR-Fall, Fall-UP and FU-Kinect datasets,\nrespectively. The remarkable performance of our system highlights its\nsuperiority, efficiency, and generalizability in real-world fall detection\nscenarios, offering significant advancements in healthcare and societal\nwell-being.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yfMb_3KwKWXWhhr-8Ug_UxQGgt83aGJMcz5qclirci4","pdfSize":"1103342"}
