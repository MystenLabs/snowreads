{"id":"2408.16081","title":"Logic-Enhanced Language Model Agents for Trustworthy Social Simulations","authors":"Agnieszka Mensfelt and Kostas Stathis and Vince Trencsenyi","authorsParsed":[["Mensfelt","Agnieszka",""],["Stathis","Kostas",""],["Trencsenyi","Vince",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 18:25:35 GMT"}],"updateDate":"2024-08-30","timestamp":1724869535000,"abstract":"  We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a\nnovel approach to enhance the trustworthiness of social simulations that\nutilize large language models (LLMs). While LLMs have gained attention as\nagents for simulating human behaviour, their applicability in this role is\nlimited by issues such as inherent hallucinations and logical inconsistencies.\nLELMA addresses these challenges by integrating LLMs with symbolic AI, enabling\nlogical verification of the reasoning generated by LLMs. This verification\nprocess provides corrective feedback, refining the reasoning output. The\nframework consists of three main components: an LLM-Reasoner for producing\nstrategic reasoning, an LLM-Translator for mapping natural language reasoning\nto logic queries, and a Solver for evaluating these queries. This study focuses\non decision-making in game-theoretic scenarios as a model of human interaction.\nExperiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt\nhighlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0\nPro, in producing correct reasoning in these contexts. LELMA demonstrates high\naccuracy in error detection and improves the reasoning correctness of LLMs via\nself-refinement, particularly in GPT-4 Omni.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Science and Game Theory","Computing Research Repository/Logic in Computer Science"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}