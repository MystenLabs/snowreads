{"id":"2407.08334","title":"ADMM Based Semi-Structured Pattern Pruning Framework For Transformer","authors":"TianChen Wang","authorsParsed":[["Wang","TianChen",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 09:35:08 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 03:36:01 GMT"},{"version":"v3","created":"Sat, 20 Jul 2024 03:40:43 GMT"},{"version":"v4","created":"Fri, 23 Aug 2024 08:36:41 GMT"}],"updateDate":"2024-08-26","timestamp":1720690508000,"abstract":"  NLP(natural language processsing) has achieved great success through the\ntransformer model.However, the model has hundreds of millions or billions\nparameters,which is huge burden for its deployment on personal computer or\nsmall scale of server.To deal with it, we either make the model's weight matrix\nrelatively sparser, or compress attention layer. Pattern pruning ,one of the\nmost important pruning methods, permits selecting fixed number of parameters in\neach divided pattern block and prunes it. However, the effect of pattern\npruning is strictly limited by the sparsity within a region of weights in each\nlayer. In this paper,we first introduced Alternating Direction Method of\nMultipliers(ADMM) based pattern pruning framework to reshape the distribution\nof activation map. Specifically, we propose to formulate the pattern pruning on\ntransformer as a constrained optimization and use ADMM to optimize the problem.\nIn this way, the initial dense feature maps is transformed to rather regionally\nsparsified ones.Therefore, we can then achieve higher compression ratio with\nbetter performance based on pattern pruning method. Additionally, this paper\nprovides a theoretical derivations of the ADMM with local sparsity. Finally, we\nalso extend the proposed ADMM based framework with SR-STE to demonstrate its\ngeneralization and to avoid gradient vanishing problem. We conduct extensive\nexperiments on classification tasks over GLUE datasets. Significantly, we\nachieve 50% percent compression ratio while maintaining overall score 80.1 on\nGLUE dataset.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}