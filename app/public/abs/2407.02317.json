{"id":"2407.02317","title":"Soft Language Prompts for Language Transfer","authors":"Ivan Vykopal, Simon Ostermann, Mari\\'an \\v{S}imko","authorsParsed":[["Vykopal","Ivan",""],["Ostermann","Simon",""],["Šimko","Marián",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 14:50:03 GMT"}],"updateDate":"2024-07-03","timestamp":1719931803000,"abstract":"  Cross-lingual knowledge transfer, especially between high- and low-resource\nlanguages, remains a challenge in natural language processing (NLP). This study\noffers insights for improving cross-lingual NLP applications through the\ncombination of parameter-efficient fine-tuning methods. We systematically\nexplore strategies for enhancing this cross-lingual transfer through the\nincorporation of language-specific and task-specific adapters and soft prompts.\nWe present a detailed investigation of various combinations of these methods,\nexploring their efficiency across six languages, focusing on three low-resource\nlanguages, including the to our knowledge first use of soft language prompts.\nOur findings demonstrate that in contrast to claims of previous work, a\ncombination of language and task adapters does not always work best; instead,\ncombining a soft language prompt with a task adapter outperforms other\nconfigurations in many cases.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-Wgv6b-_QlwerdebRrSyS2vwF4cb8MMUGnitI-QtvIU","pdfSize":"430260"}
