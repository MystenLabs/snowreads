{"id":"2407.10485","title":"MM-Tracker: Motion Mamba with Margin Loss for UAV-platform Multiple\n  Object Tracking","authors":"Mufeng Yao, Jinlong Peng, Qingdong He, Bo Peng, Hao Chen, Mingmin Chi,\n  Chao Liu, Jon Atli Benediktsson","authorsParsed":[["Yao","Mufeng",""],["Peng","Jinlong",""],["He","Qingdong",""],["Peng","Bo",""],["Chen","Hao",""],["Chi","Mingmin",""],["Liu","Chao",""],["Benediktsson","Jon Atli",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 07:13:27 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 15:42:14 GMT"}],"updateDate":"2024-08-20","timestamp":1721027607000,"abstract":"  Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) platforms\nrequires efficient motion modeling. This is because UAV-MOT faces both local\nobject motion and global camera motion. Motion blur also increases the\ndifficulty of detecting large moving objects. Previous UAV motion modeling\napproaches either focus only on local motion or ignore motion blurring effects,\nthus limiting their tracking performance and speed. To address these issues, we\npropose the Motion Mamba Module, which explores both local and global motion\nfeatures through cross-correlation and bi-directional Mamba Modules for better\nmotion modeling. To address the detection difficulties caused by motion blur,\nwe also design motion margin loss to effectively improve the detection accuracy\nof motion blurred objects. Based on the Motion Mamba module and motion margin\nloss, our proposed MM-Tracker surpasses the state-of-the-art in two widely\nopen-source UAV-MOT datasets. Code will be available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}