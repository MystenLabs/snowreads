{"id":"2407.00614","title":"Learning Granularity-Aware Affordances from Human-Object Interaction for\n  Tool-Based Functional Grasping in Dexterous Robotics","authors":"Fan Yang, Wenrui Chen, Kailun Yang, Haoran Lin, DongSheng Luo, Conghui\n  Tang, Zhiyong Li, Yaonan Wang","authorsParsed":[["Yang","Fan",""],["Chen","Wenrui",""],["Yang","Kailun",""],["Lin","Haoran",""],["Luo","DongSheng",""],["Tang","Conghui",""],["Li","Zhiyong",""],["Wang","Yaonan",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 07:42:57 GMT"}],"updateDate":"2024-07-02","timestamp":1719733377000,"abstract":"  To enable robots to use tools, the initial step is teaching robots to employ\ndexterous gestures for touching specific areas precisely where tasks are\nperformed. Affordance features of objects serve as a bridge in the functional\ninteraction between agents and objects. However, leveraging these affordance\ncues to help robots achieve functional tool grasping remains unresolved. To\naddress this, we propose a granularity-aware affordance feature extraction\nmethod for locating functional affordance areas and predicting dexterous coarse\ngestures. We study the intrinsic mechanisms of human tool use. On one hand, we\nuse fine-grained affordance features of object-functional finger contact areas\nto locate functional affordance regions. On the other hand, we use highly\nactivated coarse-grained affordance features in hand-object interaction regions\nto predict grasp gestures. Additionally, we introduce a model-based\npost-processing module that includes functional finger coordinate localization,\nfinger-to-end coordinate transformation, and force feedback-based\ncoarse-to-fine grasping. This forms a complete dexterous robotic functional\ngrasping framework GAAF-Dex, which learns Granularity-Aware Affordances from\nhuman-object interaction for tool-based Functional grasping in Dexterous\nRobotics. Unlike fully-supervised methods that require extensive data\nannotation, we employ a weakly supervised approach to extract relevant cues\nfrom exocentric (Exo) images of hand-object interactions to supervise feature\nextraction in egocentric (Ego) images. We have constructed a small-scale\ndataset, FAH, which includes near 6K images of functional hand-object\ninteraction Exo- and Ego images of 18 commonly used tools performing 6 tasks.\nExtensive experiments on the dataset demonstrate our method outperforms\nstate-of-the-art methods. The code will be made publicly available at\nhttps://github.com/yangfan293/GAAF-DEX.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}