{"id":"2408.09895","title":"Performance Law of Large Language Models","authors":"Chuhan Wu, Ruiming Tang","authorsParsed":[["Wu","Chuhan",""],["Tang","Ruiming",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 11:09:12 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 12:14:18 GMT"},{"version":"v3","created":"Tue, 10 Sep 2024 02:12:29 GMT"},{"version":"v4","created":"Fri, 13 Sep 2024 12:28:45 GMT"}],"updateDate":"2024-09-16","timestamp":1724065752000,"abstract":"  Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}