{"id":"2408.16653","title":"Optimal Parallelization of Boosting","authors":"Arthur da Cunha, Mikael M{\\o}ller H{\\o}gsgaard, Kasper Green Larsen","authorsParsed":[["da Cunha","Arthur",""],["Høgsgaard","Mikael Møller",""],["Larsen","Kasper Green",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 15:56:22 GMT"}],"updateDate":"2024-08-30","timestamp":1724946982000,"abstract":"  Recent works on the parallel complexity of Boosting have established strong\nlower bounds on the tradeoff between the number of training rounds $p$ and the\ntotal parallel work per round $t$. These works have also presented highly\nnon-trivial parallel algorithms that shed light on different regions of this\ntradeoff. Despite these advancements, a significant gap persists between the\ntheoretical lower bounds and the performance of these algorithms across much of\nthe tradeoff space. In this work, we essentially close this gap by providing\nboth improved lower bounds on the parallel complexity of weak-to-strong\nlearners, and a parallel Boosting algorithm whose performance matches these\nbounds across the entire $p$ vs.~$t$ compromise spectrum, up to logarithmic\nfactors. Ultimately, this work settles the true parallel complexity of Boosting\nalgorithms that are nearly sample-optimal.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}