{"id":"2408.07410","title":"Aquila2 Technical Report","authors":"Bo-Wen Zhang, Liangdong Wang, Jijie Li, Shuhao Gu, Xinya Wu, Zhengduo\n  Zhang, Boyan Gao, Yulong Ao, Guang Liu","authorsParsed":[["Zhang","Bo-Wen",""],["Wang","Liangdong",""],["Li","Jijie",""],["Gu","Shuhao",""],["Wu","Xinya",""],["Zhang","Zhengduo",""],["Gao","Boyan",""],["Ao","Yulong",""],["Liu","Guang",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 09:34:19 GMT"}],"updateDate":"2024-08-15","timestamp":1723628059000,"abstract":"  This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"U0SN410lk76y5B2MkyLVh5Vog4w-SqwFfD-JhlFMh1c","pdfSize":"6767970"}
