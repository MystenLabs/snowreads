{"id":"2408.02014","title":"Unsupervised Representation Learning by Balanced Self Attention Matching","authors":"Daniel Shalam and Simon Korman","authorsParsed":[["Shalam","Daniel",""],["Korman","Simon",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 12:52:44 GMT"}],"updateDate":"2024-08-06","timestamp":1722775964000,"abstract":"  Many leading self-supervised methods for unsupervised representation\nlearning, in particular those for embedding image features, are built on\nvariants of the instance discrimination task, whose optimization is known to be\nprone to instabilities that can lead to feature collapse. Different techniques\nhave been devised to circumvent this issue, including the use of negative pairs\nwith different contrastive losses, the use of external memory banks, and\nbreaking of symmetry by using separate encoding networks with possibly\ndifferent structures. Our method, termed BAM, rather than directly matching\nfeatures of different views (augmentations) of input images, is based on\nmatching their self-attention vectors, which are the distributions of\nsimilarities to the entire set of augmented images of a batch. We obtain rich\nrepresentations and avoid feature collapse by minimizing a loss that matches\nthese distributions to their globally balanced and entropy regularized version,\nwhich is obtained through a simple self-optimal-transport computation. We\nablate and verify our method through a wide set of experiments that show\ncompetitive performance with leading methods on both semi-supervised and\ntransfer-learning benchmarks. Our implementation and pre-trained models are\navailable at github.com/DanielShalam/BAM .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"M4M-lgrdrXP6K2nKP0FWvMbyHckJ-KI6uKUyoAIezeM","pdfSize":"3994315"}
