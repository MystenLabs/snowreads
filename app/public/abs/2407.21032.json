{"id":"2407.21032","title":"Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion","authors":"Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin,\n  Juho Lee","authorsParsed":[["Kim","Sanghyun",""],["Jung","Seohyeon",""],["Kim","Balhae",""],["Choi","Moonseok",""],["Shin","Jinwoo",""],["Lee","Juho",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 05:21:41 GMT"}],"updateDate":"2024-08-01","timestamp":1721193701000,"abstract":"  This paper addresses the societal concerns arising from large-scale\ntext-to-image diffusion models for generating potentially harmful or\ncopyrighted content. Existing models rely heavily on internet-crawled data,\nwherein problematic concepts persist due to incomplete filtration processes.\nWhile previous approaches somewhat alleviate the issue, they often rely on\ntext-specified concepts, introducing challenges in accurately capturing nuanced\nconcepts and aligning model knowledge with human understandings. In response,\nwe propose a framework named Human Feedback Inversion (HFI), where human\nfeedback on model-generated images is condensed into textual tokens guiding the\nmitigation or removal of problematic images. The proposed framework can be\nbuilt upon existing techniques for the same purpose, enhancing their alignment\nwith human judgment. By doing so, we simplify the training objective with a\nself-distillation-based technique, providing a strong baseline for concept\nremoval. Our experimental results demonstrate our framework significantly\nreduces objectionable content generation while preserving image quality,\ncontributing to the ethical deployment of AI in the public sphere.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}