{"id":"2408.11817","title":"GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models","authors":"Jonathan Roberts, Kai Han, Samuel Albanie","authorsParsed":[["Roberts","Jonathan",""],["Han","Kai",""],["Albanie","Samuel",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:59:32 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 17:47:47 GMT"}],"updateDate":"2024-08-30","timestamp":1724263172000,"abstract":"  Large multimodal models (LMMs) have exhibited proficiencies across many\nvisual tasks. Although numerous well-known benchmarks exist to evaluate model\nperformance, they increasingly have insufficient headroom. As such, there is a\npressing need for a new generation of benchmarks challenging enough for the\nnext generation of LMMs. One area that LMMs show potential is graph analysis,\nspecifically, the tasks an analyst might typically perform when interpreting\nfigures such as estimating the mean, intercepts or correlations of functions\nand data series. In this work, we introduce GRAB, a graph analysis benchmark,\nfit for current and future frontier LMMs. Our benchmark is entirely synthetic,\nensuring high-quality, noise-free questions. GRAB is comprised of 2170\nquestions, covering four tasks and 23 graph properties. We evaluate 20 LMMs on\nGRAB, finding it to be a challenging benchmark, with the highest performing\nmodel attaining a score of just 21.7%. Finally, we conduct various ablations to\ninvestigate where the models succeed and struggle. We release GRAB to encourage\nprogress in this important, growing domain.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}