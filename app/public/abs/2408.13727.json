{"id":"2408.13727","title":"LogParser-LLM: Advancing Efficient Log Parsing with Large Language\n  Models","authors":"Aoxiao Zhong, Dengyao Mo, Guiyang Liu, Jinbu Liu, Qingda Lu, Qi Zhou,\n  Jiesheng Wu, Quanzheng Li, Qingsong Wen","authorsParsed":[["Zhong","Aoxiao",""],["Mo","Dengyao",""],["Liu","Guiyang",""],["Liu","Jinbu",""],["Lu","Qingda",""],["Zhou","Qi",""],["Wu","Jiesheng",""],["Li","Quanzheng",""],["Wen","Qingsong",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 05:34:24 GMT"}],"updateDate":"2024-08-27","timestamp":1724564064000,"abstract":"  Logs are ubiquitous digital footprints, playing an indispensable role in\nsystem diagnostics, security analysis, and performance optimization. The\nextraction of actionable insights from logs is critically dependent on the log\nparsing process, which converts raw logs into structured formats for downstream\nanalysis. Yet, the complexities of contemporary systems and the dynamic nature\nof logs pose significant challenges to existing automatic parsing techniques.\nThe emergence of Large Language Models (LLM) offers new horizons. With their\nexpansive knowledge and contextual prowess, LLMs have been transformative\nacross diverse applications. Building on this, we introduce LogParser-LLM, a\nnovel log parser integrated with LLM capabilities. This union seamlessly blends\nsemantic insights with statistical nuances, obviating the need for\nhyper-parameter tuning and labeled training data, while ensuring rapid\nadaptability through online parsing. Further deepening our exploration, we\naddress the intricate challenge of parsing granularity, proposing a new metric\nand integrating human interactions to allow users to calibrate granularity to\ntheir specific needs. Our method's efficacy is empirically demonstrated through\nevaluations on the Loghub-2k and the large-scale LogPub benchmark. In\nevaluations on the LogPub benchmark, involving an average of 3.6 million logs\nper dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM\ninvocations on average, achieving a 90.6% F1 score for grouping accuracy and an\n81.1% for parsing accuracy. These results demonstrate the method's high\nefficiency and accuracy, outperforming current state-of-the-art log parsers,\nincluding pattern-based, neural network-based, and existing LLM-enhanced\napproaches.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}