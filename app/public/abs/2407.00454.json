{"id":"2407.00454","title":"Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language\n  Models via Inherent Capability","authors":"Ryokan Ri, Shun Kiyono, Sho Takase","authorsParsed":[["Ri","Ryokan",""],["Kiyono","Shun",""],["Takase","Sho",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 14:40:23 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 10:04:22 GMT"}],"updateDate":"2024-09-18","timestamp":1719672023000,"abstract":"  Zero-shot cross-lingual transfer by fine-tuning multilingual pretrained\nmodels shows promise for low-resource languages, but often suffers from\nmisalignment of internal representations between languages. We hypothesize that\neven when the model cannot generalize across languages effectively in\nfine-tuning, it still captures cross-lingual correspondence useful for\ncross-lingual transfer. We explore this hypothesis with Self-Translate-Train, a\nmethod that lets large language models (LLMs) to translate training data into\nthe target language and fine-tunes the model on its own generated data. By\ndemonstrating that Self-Translate-Train outperforms zero-shot transfer, we\nencourage further exploration of better methods to elicit cross-lingual\ncapabilities of LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}