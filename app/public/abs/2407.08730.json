{"id":"2407.08730","title":"Evaluating Deep Neural Networks in Deployment (A Comparative and\n  Replicability Study)","authors":"Eduard Pinconschi, Divya Gopinath, Rui Abreu, Corina S. Pasareanu","authorsParsed":[["Pinconschi","Eduard",""],["Gopinath","Divya",""],["Abreu","Rui",""],["Pasareanu","Corina S.",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:58:12 GMT"},{"version":"v2","created":"Sat, 27 Jul 2024 18:27:05 GMT"}],"updateDate":"2024-07-30","timestamp":1720720692000,"abstract":"  As deep neural networks (DNNs) are increasingly used in safety-critical\napplications, there is a growing concern for their reliability. Even highly\ntrained, high-performant networks are not 100% accurate. However, it is very\ndifficult to predict their behavior during deployment without ground truth. In\nthis paper, we provide a comparative and replicability study on recent\napproaches that have been proposed to evaluate the reliability of DNNs in\ndeployment. We find that it is hard to run and reproduce the results for these\napproaches on their replication packages and even more difficult to run them on\nartifacts other than their own. Further, it is difficult to compare the\neffectiveness of the approaches, due to the lack of clearly defined evaluation\nmetrics. Our results indicate that more effort is needed in our research\ncommunity to obtain sound techniques for evaluating the reliability of neural\nnetworks in safety-critical domains. To this end, we contribute an evaluation\nframework that incorporates the considered approaches and enables evaluation on\ncommon benchmarks, using common metrics.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}