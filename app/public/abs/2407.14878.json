{"id":"2407.14878","title":"Modular Sentence Encoders: Separating Language Specialization from\n  Cross-Lingual Alignment","authors":"Yongxin Huang, Kexin Wang, Goran Glava\\v{s}, Iryna Gurevych","authorsParsed":[["Huang","Yongxin",""],["Wang","Kexin",""],["Glava≈°","Goran",""],["Gurevych","Iryna",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 13:56:39 GMT"}],"updateDate":"2024-07-23","timestamp":1721483799000,"abstract":"  Multilingual sentence encoders are commonly obtained by training multilingual\nlanguage models to map sentences from different languages into a shared\nsemantic space. As such, they are subject to curse of multilinguality, a loss\nof monolingual representational accuracy due to parameter sharing. Another\nlimitation of multilingual sentence encoders is the trade-off between\nmonolingual and cross-lingual performance. Training for cross-lingual alignment\nof sentence embeddings distorts the optimal monolingual structure of semantic\nspaces of individual languages, harming the utility of sentence embeddings in\nmonolingual tasks. In this work, we address both issues by modular training of\nsentence encoders, i.e., by separating monolingual specialization from\ncross-lingual alignment. We first efficiently train language-specific sentence\nencoders to avoid negative interference between languages (i.e., the curse). We\nthen align all non-English monolingual encoders to the English encoder by\ntraining a cross-lingual alignment adapter on top of each, preventing\ninterference with monolingual specialization from the first step. In both\nsteps, we resort to contrastive learning on machine-translated paraphrase data.\nMonolingual and cross-lingual evaluations on semantic text\nsimilarity/relatedness and multiple-choice QA render our modular solution more\neffective than multilingual sentence encoders, especially benefiting\nlow-resource languages.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}