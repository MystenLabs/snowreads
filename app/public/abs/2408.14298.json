{"id":"2408.14298","title":"Resource Efficient Asynchronous Federated Learning for Digital Twin\n  Empowered IoT Network","authors":"Shunfeng Chu, Jun Li, Jianxin Wang, Yiyang Ni, Kang Wei, Wen Chen, Shi\n  Jin","authorsParsed":[["Chu","Shunfeng",""],["Li","Jun",""],["Wang","Jianxin",""],["Ni","Yiyang",""],["Wei","Kang",""],["Chen","Wen",""],["Jin","Shi",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 14:28:51 GMT"}],"updateDate":"2024-08-28","timestamp":1724682531000,"abstract":"  As an emerging technology, digital twin (DT) can provide real-time status and\ndynamic topology mapping for Internet of Things (IoT) devices. However, DT and\nits implementation within industrial IoT networks necessitates substantial,\ndistributed data support, which often leads to ``data silos'' and raises\nprivacy concerns. To address these issues, we develop a dynamic resource\nscheduling algorithm tailored for the asynchronous federated learning\n(FL)-based lightweight DT empowered IoT network. Specifically, our approach\naims to minimize a multi-objective function that encompasses both energy\nconsumption and latency by optimizing IoT device selection and transmit power\ncontrol, subject to FL model performance constraints. We utilize the Lyapunov\nmethod to decouple the formulated problem into a series of one-slot\noptimization problems and develop a two-stage optimization algorithm to achieve\nthe optimal transmission power control and IoT device scheduling strategies. In\nthe first stage, we derive closed-form solutions for optimal transmit power on\nthe IoT device side. In the second stage, since partial state information is\nunknown, e.g., the transmitting power and computational frequency of IoT\ndevice, the edge server employs a multi-armed bandit (MAB) framework to model\nthe IoT device selection problem and utilizes an efficient online algorithm,\nnamely the client utility-based upper confidence bound (CU-UCB), to address it.\nNumerical results validate our algorithm's superiority over benchmark schemes,\nand simulations demonstrate that our algorithm achieves faster training speeds\non the Fashion-MNIST and CIFAR-10 datasets within the same training duration.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}