{"id":"2408.02814","title":"Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream\n  Machine Learning Services","authors":"Shaopeng Fu, Xuexue Sun, Ke Qing, Tianhang Zheng, Di Wang","authorsParsed":[["Fu","Shaopeng",""],["Sun","Xuexue",""],["Qing","Ke",""],["Zheng","Tianhang",""],["Wang","Di",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 20:27:54 GMT"}],"updateDate":"2024-08-07","timestamp":1722889674000,"abstract":"  Though pre-trained encoders can be easily accessed online to build downstream\nmachine learning (ML) services quickly, various attacks have been designed to\ncompromise the security and privacy of these encoders. While most attacks\ntarget encoders on the upstream side, it remains unknown how an encoder could\nbe threatened when deployed in a downstream ML service. This paper unveils a\nnew vulnerability: the Pre-trained Encoder Inference (PEI) attack, which posts\nprivacy threats toward encoders hidden behind downstream ML services. By only\nproviding API accesses to a targeted downstream service and a set of candidate\nencoders, the PEI attack can infer which encoder is secretly used by the\ntargeted service based on candidate ones. We evaluate the attack performance of\nPEI against real-world encoders on three downstream tasks: image\nclassification, text classification, and text-to-image generation. Experiments\nshow that the PEI attack succeeds in revealing the hidden encoder in most cases\nand seldom makes mistakes even when the hidden encoder is not in the candidate\nset. We also conducted a case study on one of the most recent vision-language\nmodels, LLaVA, to illustrate that the PEI attack is useful in assisting other\nML attacks such as adversarial attacks. The code is available at\nhttps://github.com/fshp971/encoder-inference.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}