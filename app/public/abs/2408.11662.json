{"id":"2408.11662","title":"Optimizing Federated Graph Learning with Inherent Structural Knowledge\n  and Dual-Densely Connected GNNs","authors":"Longwen Wang and Jianchun Liu and Zhi Liu and Jinyang Huang","authorsParsed":[["Wang","Longwen",""],["Liu","Jianchun",""],["Liu","Zhi",""],["Huang","Jinyang",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 14:37:50 GMT"}],"updateDate":"2024-08-22","timestamp":1724251070000,"abstract":"  Federated Graph Learning (FGL) is an emerging technology that enables clients\nto collaboratively train powerful Graph Neural Networks (GNNs) in a distributed\nmanner without exposing their private data. Nevertheless, FGL still faces the\nchallenge of the severe non-Independent and Identically Distributed (non-IID)\nnature of graphs, which possess diverse node and edge structures, especially\nacross varied domains. Thus, exploring the knowledge inherent in these\nstructures becomes significantly crucial. Existing methods, however, either\noverlook the inherent structural knowledge in graph data or capture it at the\ncost of significantly increased resource demands (e.g., FLOPs and communication\nbandwidth), which can be detrimental to distributed paradigms. Inspired by\nthis, we propose FedDense, a novel FGL framework that optimizes the utilization\nefficiency of inherent structural knowledge. To better acquire knowledge of\ndiverse and underexploited structures, FedDense first explicitly encodes the\nstructural knowledge inherent within graph data itself alongside node features.\nBesides, FedDense introduces a Dual-Densely Connected (DDC) GNN architecture\nthat exploits the multi-scale (i.e., one-hop to multi-hop) feature and\nstructure insights embedded in the aggregated feature maps at each layer. In\naddition to the exploitation of inherent structures, we consider resource\nlimitations in FGL, devising exceedingly narrow layers atop the DDC\narchitecture and adopting a selective parameter sharing strategy to reduce\nresource costs substantially. We conduct extensive experiments using 15\ndatasets across 4 different domains, demonstrating that FedDense consistently\nsurpasses baselines by a large margin in training performance, while demanding\nminimal resources.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}