{"id":"2408.08998","title":"A Confidence Interval for the $\\ell_2$ Expected Calibration Error","authors":"Yan Sun, Pratik Chaudhari, Ian J. Barnett, and Edgar Dobriban","authorsParsed":[["Sun","Yan",""],["Chaudhari","Pratik",""],["Barnett","Ian J.",""],["Dobriban","Edgar",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 20:00:08 GMT"},{"version":"v2","created":"Wed, 4 Sep 2024 03:26:09 GMT"}],"updateDate":"2024-09-05","timestamp":1723838408000,"abstract":"  Recent advances in machine learning have significantly improved prediction\naccuracy in various applications. However, ensuring the calibration of\nprobabilistic predictions remains a significant challenge. Despite efforts to\nenhance model calibration, the rigorous statistical evaluation of model\ncalibration remains less explored. In this work, we develop confidence\nintervals the $\\ell_2$ Expected Calibration Error (ECE). We consider\ntop-1-to-$k$ calibration, which includes both the popular notion of confidence\ncalibration as well as full calibration. For a debiased estimator of the ECE,\nwe show asymptotic normality, but with different convergence rates and\nasymptotic variances for calibrated and miscalibrated models. We develop\nmethods to construct asymptotically valid confidence intervals for the ECE,\naccounting for this behavior as well as non-negativity. Our theoretical\nfindings are supported through extensive experiments, showing that our methods\nproduce valid confidence intervals with shorter lengths compared to those\nobtained by resampling-based methods.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}