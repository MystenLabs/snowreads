{"id":"2408.09775","title":"Faster Adaptive Decentralized Learning Algorithms","authors":"Feihu Huang and Jianyu Zhao","authorsParsed":[["Huang","Feihu",""],["Zhao","Jianyu",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 08:05:33 GMT"}],"updateDate":"2024-08-20","timestamp":1724054733000,"abstract":"  Decentralized learning recently has received increasing attention in machine\nlearning due to its advantages in implementation simplicity and system\nrobustness, data privacy. Meanwhile, the adaptive gradient methods show\nsuperior performances in many machine learning tasks such as training neural\nnetworks. Although some works focus on studying decentralized optimization\nalgorithms with adaptive learning rates, these adaptive decentralized\nalgorithms still suffer from high sample complexity. To fill these gaps, we\npropose a class of faster adaptive decentralized algorithms (i.e., AdaMDOS and\nAdaMDOF) for distributed nonconvex stochastic and finite-sum optimization,\nrespectively. Moreover, we provide a solid convergence analysis framework for\nour methods. In particular, we prove that our AdaMDOS obtains a near-optimal\nsample complexity of $\\tilde{O}(\\epsilon^{-3})$ for finding an\n$\\epsilon$-stationary solution of nonconvex stochastic optimization. Meanwhile,\nour AdaMDOF obtains a near-optimal sample complexity of\n$O(\\sqrt{n}\\epsilon^{-2})$ for finding an $\\epsilon$-stationary solution of\nnonconvex finite-sum optimization, where $n$ denotes the sample size. To the\nbest of our knowledge, our AdaMDOF algorithm is the first adaptive\ndecentralized algorithm for nonconvex finite-sum optimization. Some\nexperimental results demonstrate efficiency of our algorithms.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"zJeprn2TBA3hd1Yi3hRrBCcBddW0DTJCqcnRl5y3A1U","pdfSize":"1122143"}
