{"id":"2408.00359","title":"Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks","authors":"Jy-yong Sohn, Dohyun Kwon, Seoyeon An, Kangwook Lee","authorsParsed":[["Sohn","Jy-yong",""],["Kwon","Dohyun",""],["An","Seoyeon",""],["Lee","Kangwook",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 07:58:51 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 14:15:03 GMT"}],"updateDate":"2024-08-20","timestamp":1722499131000,"abstract":"  Fine-tuning large pre-trained models is a common practice in machine learning\napplications, yet its mathematical analysis remains largely unexplored. In this\npaper, we study fine-tuning through the lens of memorization capacity. Our new\nmeasure, the Fine-Tuning Capacity (FTC), is defined as the maximum number of\nsamples a neural network can fine-tune, or equivalently, as the minimum number\nof neurons ($m$) needed to arbitrarily change $N$ labels among $K$ samples\nconsidered in the fine-tuning process. In essence, FTC extends the memorization\ncapacity concept to the fine-tuning scenario. We analyze FTC for the additive\nfine-tuning scenario where the fine-tuned network is defined as the summation\nof the frozen pre-trained network $f$ and a neural network $g$ (with $m$\nneurons) designed for fine-tuning. When $g$ is a ReLU network with either 2 or\n3 layers, we obtain tight upper and lower bounds on FTC; we show that $N$\nsamples can be fine-tuned with $m=\\Theta(N)$ neurons for 2-layer networks, and\nwith $m=\\Theta(\\sqrt{N})$ neurons for 3-layer networks, no matter how large $K$\nis. Our results recover the known memorization capacity results when $N = K$ as\na special case.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}