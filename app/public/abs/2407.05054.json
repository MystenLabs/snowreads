{"id":"2407.05054","title":"Cross-Lingual Word Alignment for ASEAN Languages with Contrastive\n  Learning","authors":"Jingshen Zhang, Xinying Qiu, Teng Shen, Wenyu Wang, Kailin Zhang,\n  Wenhe Feng","authorsParsed":[["Zhang","Jingshen",""],["Qiu","Xinying",""],["Shen","Teng",""],["Wang","Wenyu",""],["Zhang","Kailin",""],["Feng","Wenhe",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 11:56:41 GMT"}],"updateDate":"2024-07-09","timestamp":1720267001000,"abstract":"  Cross-lingual word alignment plays a crucial role in various natural language\nprocessing tasks, particularly for low-resource languages. Recent study\nproposes a BiLSTM-based encoder-decoder model that outperforms pre-trained\nlanguage models in low-resource settings. However, their model only considers\nthe similarity of word embedding spaces and does not explicitly model the\ndifferences between word embeddings. To address this limitation, we propose\nincorporating contrastive learning into the BiLSTM-based encoder-decoder\nframework. Our approach introduces a multi-view negative sampling strategy to\nlearn the differences between word pairs in the shared cross-lingual embedding\nspace. We evaluate our model on five bilingual aligned datasets spanning four\nASEAN languages: Lao, Vietnamese, Thai, and Indonesian. Experimental results\ndemonstrate that integrating contrastive learning consistently improves word\nalignment accuracy across all datasets, confirming the effectiveness of the\nproposed method in low-resource scenarios. We will release our data set and\ncode to support future research on ASEAN or more low-resource word alignment.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}