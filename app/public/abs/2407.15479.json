{"id":"2407.15479","title":"Affordance Labeling and Exploration: A Manifold-Based Approach","authors":"\\.Ismail \\\"Oz\\c{c}\\.il, A. Bu\\u{g}ra Koku","authorsParsed":[["Özçil","İsmail",""],["Koku","A. Buğra",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 08:46:20 GMT"}],"updateDate":"2024-07-23","timestamp":1721637980000,"abstract":"  The advancement in computing power has significantly reduced the training\ntimes for deep learning, fostering the rapid development of networks designed\nfor object recognition. However, the exploration of object utility, which is\nthe affordance of the object, as opposed to object recognition, has received\ncomparatively less attention. This work focuses on the problem of exploration\nof object affordances using existing networks trained on the object\nclassification dataset. While pre-trained networks have proven to be\ninstrumental in transfer learning for classification tasks, this work diverges\nfrom conventional object classification methods. Instead, it employs\npre-trained networks to discern affordance labels without the need for\nspecialized layers, abstaining from modifying the final layers through the\naddition of classification layers. To facilitate the determination of\naffordance labels without such modifications, two approaches, i.e. subspace\nclustering and manifold curvature methods are tested. These methods offer a\ndistinct perspective on affordance label recognition. Especially, manifold\ncurvature method has been successfully tested with nine distinct pre-trained\nnetworks, each achieving an accuracy exceeding 95%. Moreover, it is observed\nthat manifold curvature and subspace clustering methods explore affordance\nlabels that are not marked in the ground truth, but object affords in various\ncases.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"xmB198JJ6IDhSzrP_NZhU-qfWXtnzB6sGk5MmgxinlI","pdfSize":"699082"}
