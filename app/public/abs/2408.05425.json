{"id":"2408.05425","title":"Modeling Multi-Step Scientific Processes with Graph Transformer Networks","authors":"Amanda A. Volk, Robert W. Epps, Jeffrey G. Ethier, Luke A. Baldwin","authorsParsed":[["Volk","Amanda A.",""],["Epps","Robert W.",""],["Ethier","Jeffrey G.",""],["Baldwin","Luke A.",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 04:03:51 GMT"}],"updateDate":"2024-08-13","timestamp":1723262631000,"abstract":"  This work presents the use of graph learning for the prediction of multi-step\nexperimental outcomes for applications across experimental research, including\nmaterial science, chemistry, and biology. The viability of geometric learning\nfor regression tasks was benchmarked against a collection of linear models\nthrough a combination of simulated and real-world data training studies. First,\na selection of five arbitrarily designed multi-step surrogate functions were\ndeveloped to reflect various features commonly found within experimental\nprocesses. A graph transformer network outperformed all tested linear models in\nscenarios that featured hidden interactions between process steps and sequence\ndependent features, while retaining equivalent performance in sequence agnostic\nscenarios. Then, a similar comparison was applied to real-world literature data\non algorithm guided colloidal atomic layer deposition. Using the complete\nreaction sequence as training data, the graph neural network outperformed all\nlinear models in predicting the three spectral properties for most training set\nsizes. Further implementation of graph neural networks and geometric\nrepresentation of scientific processes for the prediction of experiment\noutcomes could lead to algorithm driven navigation of higher dimension\nparameter spaces and efficient exploration of more dynamic systems.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}