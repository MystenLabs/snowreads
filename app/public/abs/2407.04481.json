{"id":"2407.04481","title":"Using Petri Nets as an Integrated Constraint Mechanism for Reinforcement\n  Learning Tasks","authors":"Timon Sachweh, Pierre Haritz, Thomas Liebig","authorsParsed":[["Sachweh","Timon",""],["Haritz","Pierre",""],["Liebig","Thomas",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 13:04:06 GMT"}],"updateDate":"2024-07-08","timestamp":1720184646000,"abstract":"  The lack of trust in algorithms is usually an issue when using Reinforcement\nLearning (RL) agents for control in real-world domains such as production\nplants, autonomous vehicles, or traffic-related infrastructure, partly due to\nthe lack of verifiability of the model itself. In such scenarios, Petri nets\n(PNs) are often available for flowcharts or process steps, as they are\nversatile and standardized. In order to facilitate integration of RL models and\nas a step towards increasing AI trustworthiness, we propose an approach that\nuses PNs with three main advantages over typical RL approaches: Firstly, the\nagent can now easily be modeled with a combined state including both external\nenvironmental observations and agent-specific state information from a given\nPN. Secondly, we can enforce constraints for state-dependent actions through\nthe inherent PN model. And lastly, we can increase trustworthiness by verifying\nPN properties through techniques such as model checking. We test our approach\non a typical four-way intersection traffic light control setting and present\nour results, beating cycle-based baselines.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}