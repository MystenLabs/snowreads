{"id":"2408.16661","title":"Eigen-Cluster VIS: Improving Weakly-supervised Video Instance\n  Segmentation by Leveraging Spatio-temporal Consistency","authors":"Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei","authorsParsed":[["Arefi","Farnoosh",""],["Mansourian","Amir M.",""],["Kasaei","Shohreh",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 16:05:05 GMT"}],"updateDate":"2024-08-30","timestamp":1724947505000,"abstract":"  The performance of Video Instance Segmentation (VIS) methods has improved\nsignificantly with the advent of transformer networks. However, these networks\noften face challenges in training due to the high annotation cost. To address\nthis, unsupervised and weakly-supervised methods have been developed to reduce\nthe dependency on annotations. This work introduces a novel weakly-supervised\nmethod called Eigen-cluster VIS that, without requiring any mask annotations,\nachieves competitive accuracy compared to other VIS approaches. This method is\nbased on two key innovations: a Temporal Eigenvalue Loss (TEL) and a clip-level\nQuality Cluster Coefficient (QCC). The TEL ensures temporal coherence by\nleveraging the eigenvalues of the Laplacian matrix derived from graph adjacency\nmatrices. By minimizing the mean absolute error (MAE) between the eigenvalues\nof adjacent frames, this loss function promotes smooth transitions and stable\nsegmentation boundaries over time, reducing temporal discontinuities and\nimproving overall segmentation quality. The QCC employs the K-means method to\nensure the quality of spatio-temporal clusters without relying on ground truth\nmasks. Using the Davies-Bouldin score, the QCC provides an unsupervised measure\nof feature discrimination, allowing the model to self-evaluate and adapt to\nvarying object distributions, enhancing robustness during the testing phase.\nThese enhancements are computationally efficient and straightforward, offering\nsignificant performance gains without additional annotated data. The proposed\nEigen-Cluster VIS method is evaluated on the YouTube-VIS 2019/2021 and OVIS\ndatasets, demonstrating that it effectively narrows the performance gap between\nthe fully-supervised and weakly-supervised VIS approaches. The code is\navailable on: https://github.com/farnooshar/EigenClusterVIS\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Ei8y56XdPU-s3b3L0anA18YxPRXZLCtKT6TSwpAs204","pdfSize":"4641890"}
