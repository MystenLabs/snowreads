{"id":"2407.09053","title":"Navi2Gaze: Leveraging Foundation Models for Navigation and Target Gazing","authors":"Jun Zhu, Zihao Du, Haotian Xu, Fengbo Lan, Zilong Zheng, Bo Ma,\n  Shengjie Wang, and Tao Zhang","authorsParsed":[["Zhu","Jun",""],["Du","Zihao",""],["Xu","Haotian",""],["Lan","Fengbo",""],["Zheng","Zilong",""],["Ma","Bo",""],["Wang","Shengjie",""],["Zhang","Tao",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 07:20:05 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 01:42:23 GMT"}],"updateDate":"2024-09-18","timestamp":1720768805000,"abstract":"  Task-aware navigation continues to be a challenging area of research,\nespecially in scenarios involving open vocabulary. Previous studies primarily\nfocus on finding suitable locations for task completion, often overlooking the\nimportance of the robot's pose. However, the robot's orientation is crucial for\nsuccessfully completing tasks because of how objects are arranged (e.g., to\nopen a refrigerator door). Humans intuitively navigate to objects with the\nright orientation using semantics and common sense. For instance, when opening\na refrigerator, we naturally stand in front of it rather than to the side.\nRecent advances suggest that Vision-Language Models (VLMs) can provide robots\nwith similar common sense. Therefore, we develop a VLM-driven method called\nNavigation-to-Gaze (Navi2Gaze) for efficient navigation and object gazing based\non task descriptions. This method uses the VLM to score and select the best\npose from numerous candidates automatically. In evaluations on multiple\nphotorealistic simulation benchmarks, Navi2Gaze significantly outperforms\nexisting approaches by precisely determining the optimal orientation relative\nto target objects, resulting in a 68.8% reduction in Distance to Goal (DTG).\nReal-world video demonstrations can be found on the supplementary website\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/"}