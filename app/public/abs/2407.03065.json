{"id":"2407.03065","title":"Warm-up Free Policy Optimization: Improved Regret in Linear Markov\n  Decision Processes","authors":"Asaf Cassel and Aviv Rosenberg","authorsParsed":[["Cassel","Asaf",""],["Rosenberg","Aviv",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 12:36:24 GMT"}],"updateDate":"2024-07-04","timestamp":1720010184000,"abstract":"  Policy Optimization (PO) methods are among the most popular Reinforcement\nLearning (RL) algorithms in practice. Recently, Sherman et al. [2023a] proposed\na PO-based algorithm with rate-optimal regret guarantees under the linear\nMarkov Decision Process (MDP) model. However, their algorithm relies on a\ncostly pure exploration warm-up phase that is hard to implement in practice.\nThis paper eliminates this undesired warm-up phase, replacing it with a simple\nand efficient contraction mechanism. Our PO algorithm achieves rate-optimal\nregret with improved dependence on the other parameters of the problem (horizon\nand function approximation dimension) in two fundamental settings: adversarial\nlosses with full-information feedback and stochastic losses with bandit\nfeedback.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}