{"id":"2407.01230","title":"DaBiT: Depth and Blur informed Transformer for Joint Refocusing and\n  Super-Resolution","authors":"Crispian Morris, Nantheera Anantrasirichai, Fan Zhang, and David Bull","authorsParsed":[["Morris","Crispian",""],["Anantrasirichai","Nantheera",""],["Zhang","Fan",""],["Bull","David",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 12:22:16 GMT"},{"version":"v2","created":"Wed, 10 Jul 2024 09:19:44 GMT"}],"updateDate":"2024-07-11","timestamp":1719836536000,"abstract":"  In many real-world scenarios, recorded videos suffer from accidental focus\nblur, and while video deblurring methods exist, most specifically target motion\nblur. This paper introduces a framework optimised for the joint task of focal\ndeblurring (refocusing) and video super-resolution (VSR). The proposed method\nemploys novel map guided transformers, in addition to image propagation, to\neffectively leverage the continuous spatial variance of focal blur and restore\nthe footage. We also introduce a flow re-focusing module to efficiently align\nrelevant features between the blurry and sharp domains. Additionally, we\npropose a novel technique for generating synthetic focal blur data, broadening\nthe model's learning capabilities to include a wider array of content. We have\nmade a new benchmark dataset, DAVIS-Blur, available. This dataset, a modified\nextension of the popular DAVIS video segmentation set, provides realistic\nout-of-focus blur degradations as well as the corresponding blur maps.\nComprehensive experiments on DAVIS-Blur demonstrate the superiority of our\napproach. We achieve state-of-the-art results with an average PSNR performance\nover 1.9dB greater than comparable existing video restoration methods. Our\nsource code will be made available at https://github.com/crispianm/DaBiT\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}