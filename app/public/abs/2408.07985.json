{"id":"2408.07985","title":"Analytical Uncertainty-Based Loss Weighting in Multi-Task Learning","authors":"Lukas Kirchdorfer, Cathrin Elich, Simon Kutsche, Heiner\n  Stuckenschmidt, Lukas Schott, Jan M. K\\\"ohler","authorsParsed":[["Kirchdorfer","Lukas",""],["Elich","Cathrin",""],["Kutsche","Simon",""],["Stuckenschmidt","Heiner",""],["Schott","Lukas",""],["KÃ¶hler","Jan M.",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 07:10:17 GMT"}],"updateDate":"2024-08-16","timestamp":1723705817000,"abstract":"  With the rise of neural networks in various domains, multi-task learning\n(MTL) gained significant relevance. A key challenge in MTL is balancing\nindividual task losses during neural network training to improve performance\nand efficiency through knowledge sharing across tasks. To address these\nchallenges, we propose a novel task-weighting method by building on the most\nprevalent approach of Uncertainty Weighting and computing analytically optimal\nuncertainty-based weights, normalized by a softmax function with tunable\ntemperature. Our approach yields comparable results to the combinatorially\nprohibitive, brute-force approach of Scalarization while offering a more\ncost-effective yet high-performing alternative. We conduct an extensive\nbenchmark on various datasets and architectures. Our method consistently\noutperforms six other common weighting methods. Furthermore, we report\nnoteworthy experimental findings for the practical application of MTL. For\nexample, larger networks diminish the influence of weighting methods, and\ntuning the weight decay has a low impact compared to the learning rate.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}