{"id":"2407.07565","title":"On Leakage of Code Generation Evaluation Datasets","authors":"Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone,\n  Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon,\n  Matthias Gall\\'e","authorsParsed":[["Matton","Alexandre",""],["Sherborne","Tom",""],["Aumiller","Dennis",""],["Tommasone","Elena",""],["Alizadeh","Milad",""],["He","Jingyi",""],["Ma","Raymond",""],["Voisin","Maxime",""],["Gilsenan-McMahon","Ellen",""],["Gall√©","Matthias",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 11:50:20 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 15:37:20 GMT"}],"updateDate":"2024-07-12","timestamp":1720612220000,"abstract":"  In this paper we consider contamination by code generation test sets, in\nparticular in their use in modern large language models. We discuss three\npossible sources of such contamination and show findings supporting each of\nthem: (i) direct data leakage, (ii) indirect data leakage through the use of\nsynthetic data and (iii) overfitting to evaluation sets during model selection.\nKey to our findings is a new dataset of 161 prompts with their associated\npython solutions, dataset which is released at\nhttps://huggingface.co/datasets/CohereForAI/lbpp .\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eG9jrZTQPN1z8w-EWDZT7anO7unzRl13qV52JfxeYPE","pdfSize":"328623"}
