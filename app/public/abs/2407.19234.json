{"id":"2407.19234","title":"Ordered Momentum for Asynchronous SGD","authors":"Chang-Wei Shi, Yi-Rui Yang, Wu-Jun Li","authorsParsed":[["Shi","Chang-Wei",""],["Yang","Yi-Rui",""],["Li","Wu-Jun",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 11:35:19 GMT"}],"updateDate":"2024-07-30","timestamp":1722080119000,"abstract":"  Distributed learning is indispensable for training large-scale deep models.\nAsynchronous SGD~(ASGD) and its variants are commonly used distributed learning\nmethods in many scenarios where the computing capabilities of workers in the\ncluster are heterogeneous. Momentum has been acknowledged for its benefits in\nboth optimization and generalization in deep model training. However, existing\nworks have found that naively incorporating momentum into ASGD can impede the\nconvergence. In this paper, we propose a novel method, called ordered momentum\n(OrMo), for ASGD. In OrMo, momentum is incorporated into ASGD by organizing the\ngradients in order based on their iteration indexes. We theoretically prove the\nconvergence of OrMo for non-convex problems. To the best of our knowledge, this\nis the first work to establish the convergence analysis of ASGD with momentum\nwithout relying on the bounded delay assumption. Empirical results demonstrate\nthat OrMo can achieve better convergence performance compared with ASGD and\nother asynchronous methods with momentum.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}