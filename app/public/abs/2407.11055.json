{"id":"2407.11055","title":"Knowledge boosting during low-latency inference","authors":"Vidya Srinivas, Malek Itani, Tuochao Chen, Sefik Emre Eskimez, Takuya\n  Yoshioka and Shyamnath Gollakota","authorsParsed":[["Srinivas","Vidya",""],["Itani","Malek",""],["Chen","Tuochao",""],["Eskimez","Sefik Emre",""],["Yoshioka","Takuya",""],["Gollakota","Shyamnath",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 22:04:23 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 10:43:12 GMT"},{"version":"v3","created":"Thu, 25 Jul 2024 08:26:35 GMT"}],"updateDate":"2024-07-26","timestamp":1720562663000,"abstract":"  Models for low-latency, streaming applications could benefit from the\nknowledge capacity of larger models, but edge devices cannot run these models\ndue to resource constraints. A possible solution is to transfer hints during\ninference from a large model running remotely to a small model running\non-device. However, this incurs a communication delay that breaks real-time\nrequirements and does not guarantee that both models will operate on the same\ndata at the same time. We propose knowledge boosting, a novel technique that\nallows a large model to operate on time-delayed input during inference, while\nstill boosting small model performance. Using a streaming neural network that\nprocesses 8 ms chunks, we evaluate different speech separation and enhancement\ntasks with communication delays of up to six chunks or 48 ms. Our results show\nlarger gains where the performance gap between the small and large models is\nwide, demonstrating a promising method for large-small model collaboration for\nlow-latency applications. Code, dataset, and audio samples available at\nhttps://knowledgeboosting.cs.washington.edu/.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}