{"id":"2407.00219","title":"Evaluating Human Alignment and Model Faithfulness of LLM Rationale","authors":"Mohsen Fayyaz, Fan Yin, Jiao Sun, Nanyun Peng","authorsParsed":[["Fayyaz","Mohsen",""],["Yin","Fan",""],["Sun","Jiao",""],["Peng","Nanyun",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 20:06:30 GMT"}],"updateDate":"2024-07-02","timestamp":1719605190000,"abstract":"  We study how well large language models (LLMs) explain their generations with\nrationales -- a set of tokens extracted from the input texts that reflect the\ndecision process of LLMs. We examine LLM rationales extracted with two methods:\n1) attribution-based methods that use attention or gradients to locate\nimportant tokens, and 2) prompting-based methods that guide LLMs to extract\nrationales using prompts. Through extensive experiments, we show that\nprompting-based rationales align better with human-annotated rationales than\nattribution-based rationales, and demonstrate reasonable alignment with humans\neven when model performance is poor. We additionally find that the faithfulness\nlimitations of prompting-based methods, which are identified in previous work,\nmay be linked to their collapsed predictions. By fine-tuning these models on\nthe corresponding datasets, both prompting and attribution methods demonstrate\nimproved faithfulness. Our study sheds light on more rigorous and fair\nevaluations of LLM rationales, especially for prompting-based ones.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}