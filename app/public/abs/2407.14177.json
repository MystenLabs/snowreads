{"id":"2407.14177","title":"EVLM: An Efficient Vision-Language Model for Visual Understanding","authors":"Kaibing Chen, Dong Shen, Hanwen Zhong, Huasong Zhong, Kui Xia, Di Xu,\n  Wei Yuan, Yifei Hu, Bin Wen, Tianke Zhang, Changyi Liu, Dewen Fan, Huihui\n  Xiao, Jiahong Wu, Fan Yang, Size Li, Di Zhang","authorsParsed":[["Chen","Kaibing",""],["Shen","Dong",""],["Zhong","Hanwen",""],["Zhong","Huasong",""],["Xia","Kui",""],["Xu","Di",""],["Yuan","Wei",""],["Hu","Yifei",""],["Wen","Bin",""],["Zhang","Tianke",""],["Liu","Changyi",""],["Fan","Dewen",""],["Xiao","Huihui",""],["Wu","Jiahong",""],["Yang","Fan",""],["Li","Size",""],["Zhang","Di",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 10:09:51 GMT"}],"updateDate":"2024-07-22","timestamp":1721383791000,"abstract":"  In the field of multi-modal language models, the majority of methods are\nbuilt on an architecture similar to LLaVA. These models use a single-layer ViT\nfeature as a visual prompt, directly feeding it into the language models\nalongside textual tokens. However, when dealing with long sequences of visual\nsignals or inputs such as videos, the self-attention mechanism of language\nmodels can lead to significant computational overhead. Additionally, using\nsingle-layer ViT features makes it challenging for large language models to\nperceive visual signals fully. This paper proposes an efficient multi-modal\nlanguage model to minimize computational costs while enabling the model to\nperceive visual signals as comprehensively as possible. Our method primarily\nincludes: (1) employing cross-attention to image-text interaction similar to\nFlamingo. (2) utilize hierarchical ViT features. (3) introduce the Mixture of\nExperts (MoE) mechanism to enhance model effectiveness. Our model achieves\ncompetitive scores on public multi-modal benchmarks and performs well in tasks\nsuch as image captioning and video captioning.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}