{"id":"2407.19409","title":"LLAVADI: What Matters For Multimodal Large Language Models Distillation","authors":"Shilin Xu, Xiangtai Li, Haobo Yuan, Lu Qi, Yunhai Tong, Ming-Hsuan\n  Yang","authorsParsed":[["Xu","Shilin",""],["Li","Xiangtai",""],["Yuan","Haobo",""],["Qi","Lu",""],["Tong","Yunhai",""],["Yang","Ming-Hsuan",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 06:10:47 GMT"}],"updateDate":"2024-07-30","timestamp":1722147047000,"abstract":"  The recent surge in Multimodal Large Language Models (MLLMs) has showcased\ntheir remarkable potential for achieving generalized intelligence by\nintegrating visual understanding into Large Language Models.Nevertheless, the\nsheer model size of MLLMs leads to substantial memory and computational demands\nthat hinder their widespread deployment. In this work, we do not propose a new\nefficient model structure or train small-scale MLLMs from scratch. Instead, we\nfocus on what matters for training small-scale MLLMs through knowledge\ndistillation, which is the first step from the multimodal distillation\nperspective. Our extensive studies involve training strategies, model choices,\nand distillation algorithms in the knowledge distillation process. These\nresults show that joint alignment for both tokens and logit alignment plays\ncritical roles in teacher-student frameworks. In addition, we draw a series of\nintriguing observations from this study. By evaluating different benchmarks and\nproper strategy, even a 2.7B small-scale model can perform on par with larger\nmodels with 7B or 13B parameters. Our code and models will be publicly\navailable for further research.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"u0lQAwjajcNKNDL00YiWkh5l0TPDnPjbSjDrMGfUm_8","pdfSize":"622058","objectId":"0x28ca92bb412f0c175d630233241c58494283affa89d37de1d5d58bbd7ee939c1","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
