{"id":"2407.11969","title":"Does Refusal Training in LLMs Generalize to the Past Tense?","authors":"Maksym Andriushchenko and Nicolas Flammarion","authorsParsed":[["Andriushchenko","Maksym",""],["Flammarion","Nicolas",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:59:55 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 13:03:01 GMT"}],"updateDate":"2024-07-22","timestamp":1721152795000,"abstract":"  Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, and R2D2 models\nusing GPT-3.5 Turbo as a reformulation model. For example, the success rate of\nthis simple attack on GPT-4o increases from 1% using direct requests to 88%\nusing 20 past tense reformulation attempts on harmful requests from\nJailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find\nthat reformulations in the future tense are less effective, suggesting that\nrefusal guardrails tend to consider past historical questions more benign than\nhypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5\nTurbo show that defending against past reformulations is feasible when past\ntense examples are explicitly included in the fine-tuning data. Overall, our\nfindings highlight that the widely used alignment techniques -- such as SFT,\nRLHF, and adversarial training -- employed to align the studied models can be\nbrittle and do not always generalize as intended. We provide code and jailbreak\nartifacts at https://github.com/tml-epfl/llm-past-tense.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}