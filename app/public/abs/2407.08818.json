{"id":"2407.08818","title":"MAGNET: Improving the Multilingual Fairness of Language Models with\n  Adaptive Gradient-Based Tokenization","authors":"Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Valentin Hoffman, Tomasz\n  Limisiewicz, Yulia Tsvetkov, Noah A. Smith","authorsParsed":[["Ahia","Orevaoghene",""],["Kumar","Sachin",""],["Gonen","Hila",""],["Hoffman","Valentin",""],["Limisiewicz","Tomasz",""],["Tsvetkov","Yulia",""],["Smith","Noah A.",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 18:59:21 GMT"}],"updateDate":"2024-07-15","timestamp":1720724361000,"abstract":"  In multilingual settings, non-Latin scripts and low-resource languages are\nusually disadvantaged in terms of language models' utility, efficiency, and\ncost. Specifically, previous studies have reported multiple modeling biases\nthat the current tokenization algorithms introduce to non-Latin script\nlanguages, the main one being over-segmentation. In this work, we propose\nMAGNET; multilingual adaptive gradient-based tokenization to reduce\nover-segmentation via adaptive gradient-based subword tokenization. MAGNET\nlearns to predict segment boundaries between byte tokens in a sequence via\nsub-modules within the model, which act as internal boundary predictors\n(tokenizers). Previous gradient-based tokenization methods aimed for uniform\ncompression across sequences by integrating a single boundary predictor during\ntraining and optimizing it end-to-end through stochastic reparameterization\nalongside the next token prediction objective. However, this approach still\nresults in over-segmentation for non-Latin script languages in multilingual\nsettings. In contrast, MAGNET offers a customizable architecture where\nbyte-level sequences are routed through language-script-specific predictors,\neach optimized for its respective language script. This modularity enforces\nequitable segmentation granularity across different language scripts compared\nto previous methods. Through extensive experiments, we demonstrate that in\naddition to reducing segmentation disparities, MAGNET also enables faster\nlanguage modelling and improves downstream utility.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8b4UZYRqhR74VKnc0swQpW-VuFsxB5Jrzmh5QlPE85A","pdfSize":"1531248"}
