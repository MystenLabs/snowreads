{"id":"2407.15738","title":"Parallel Split Learning with Global Sampling","authors":"Mohammad Kohankhaki, Ahmad Ayad, Mahdi Barhoush, Anke Schmeink","authorsParsed":[["Kohankhaki","Mohammad",""],["Ayad","Ahmad",""],["Barhoush","Mahdi",""],["Schmeink","Anke",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 15:41:23 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 21:45:57 GMT"}],"updateDate":"2024-08-12","timestamp":1721662883000,"abstract":"  The expansion of IoT devices and the demands of deep learning have\nhighlighted significant challenges in distributed deep learning systems.\nParallel split learning has emerged as a promising derivative of split learning\nwell suited for distributed learning on resource-constrained devices. However,\nparallel split learning faces several challenges, such as large effective batch\nsizes, non-independent and identically distributed data, and the straggler\neffect. We view these issues as a sampling dilemma and propose to address them\nby orchestrating a mini-batch sampling process on the server side. We introduce\na new method called uniform global sampling to decouple the effective batch\nsize from the number of clients and reduce the mini-batch deviation. To address\nthe straggler effect, we introduce a novel method called Latent Dirichlet\nSampling, which generalizes uniform global sampling to balance the trade-off\nbetween batch deviation and training time. Our simulations reveal that our\nproposed methods enhance model accuracy by up to 34.1% in non-independent and\nidentically distributed settings and reduce the training time in the presence\nof stragglers by up to 62%. In particular, Latent Dirichlet Sampling\neffectively mitigates the straggler effect without compromising model accuracy\nor adding significant computational overhead compared to uniform global\nsampling. Our results demonstrate the potential of our methods to mitigate\ncommon challenges in parallel split learning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}