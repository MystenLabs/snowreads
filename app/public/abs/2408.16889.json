{"id":"2408.16889","title":"LLaVA-Chef: A Multi-modal Generative Model for Food Recipes","authors":"Fnu Mohbat and Mohammed J. Zaki","authorsParsed":[["Mohbat","Fnu",""],["Zaki","Mohammed J.",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 20:20:49 GMT"}],"updateDate":"2024-09-02","timestamp":1724962849000,"abstract":"  In the rapidly evolving landscape of online recipe sharing within a\nglobalized context, there has been a notable surge in research towards\ncomprehending and generating food recipes. Recent advancements in large\nlanguage models (LLMs) like GPT-2 and LLaVA have paved the way for Natural\nLanguage Processing (NLP) approaches to delve deeper into various facets of\nfood-related tasks, encompassing ingredient recognition and comprehensive\nrecipe generation. Despite impressive performance and multi-modal adaptability\nof LLMs, domain-specific training remains paramount for their effective\napplication. This work evaluates existing LLMs for recipe generation and\nproposes LLaVA-Chef, a novel model trained on a curated dataset of diverse\nrecipe prompts in a multi-stage approach. First, we refine the mapping of\nvisual food image embeddings to the language space. Second, we adapt LLaVA to\nthe food domain by fine-tuning it on relevant recipe data. Third, we utilize\ndiverse prompts to enhance the model's recipe comprehension. Finally, we\nimprove the linguistic quality of generated recipes by penalizing the model\nwith a custom loss function. LLaVA-Chef demonstrates impressive improvements\nover pretrained LLMs and prior works. A detailed qualitative analysis reveals\nthat LLaVA-Chef generates more detailed recipes with precise ingredient\nmentions, compared to existing approaches.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Ml54u5itg7MHWEqxJKRijAXWlW_KUmWeGnHNFRCu6O0","pdfSize":"1488464"}
