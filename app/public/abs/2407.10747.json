{"id":"2407.10747","title":"Codebook LLMs: Adapting Political Science Codebooks for LLM Use and\n  Adapting LLMs to Follow Codebooks","authors":"Andrew Halterman and Katherine A. Keith","authorsParsed":[["Halterman","Andrew",""],["Keith","Katherine A.",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 14:20:09 GMT"}],"updateDate":"2024-07-16","timestamp":1721053209000,"abstract":"  Codebooks -- documents that operationalize constructs and outline annotation\nprocedures -- are used almost universally by social scientists when coding\nunstructured political texts. Recently, to reduce manual annotation costs,\npolitical scientists have looked to generative large language models (LLMs) to\nlabel and analyze text data. However, previous work using LLMs for\nclassification has implicitly relied on the universal label assumption --\ncorrect classification of documents is possible using only a class label or\nminimal definition and the information that the LLM inductively learns during\nits pre-training. In contrast, we argue that political scientists who care\nabout valid measurement should instead make a codebook-construct label\nassumption -- an LLM should follow the definition and exclusion criteria of a\nconstruct/label provided in a codebook. In this work, we collect and curate\nthree political science datasets and their original codebooks and conduct a set\nof experiments to understand whether LLMs comply with codebook instructions,\nwhether rewriting codebooks improves performance, and whether\ninstruction-tuning LLMs on codebook-document-label tuples improves performance\nover zero-shot classification. Using Mistral 7B Instruct as our LLM, we find\nre-structuring the original codebooks gives modest gains in zero-shot\nperformance but the model still struggles to comply with the constraints of the\ncodebooks. Optimistically, instruction-tuning Mistral on one of our datasets\ngives significant gains over zero-shot inference (0.76 versus 0.53 micro F1).\nWe hope our conceptualization of the codebook-specific task, assumptions, and\ninstruction-tuning pipeline as well our semi-structured LLM codebook format\nwill help political scientists readily adapt to the LLM era.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}