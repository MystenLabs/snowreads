{"id":"2408.11598","title":"Improving Calibration by Relating Focal Loss, Temperature Scaling, and\n  Properness","authors":"Viacheslav Komisarenko and Meelis Kull","authorsParsed":[["Komisarenko","Viacheslav",""],["Kull","Meelis",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 13:10:44 GMT"}],"updateDate":"2024-08-22","timestamp":1724245844000,"abstract":"  Proper losses such as cross-entropy incentivize classifiers to produce class\nprobabilities that are well-calibrated on the training data. Due to the\ngeneralization gap, these classifiers tend to become overconfident on the test\ndata, mandating calibration methods such as temperature scaling. The focal loss\nis not proper, but training with it has been shown to often result in\nclassifiers that are better calibrated on test data. Our first contribution is\na simple explanation about why focal loss training often leads to better\ncalibration than cross-entropy training. For this, we prove that focal loss can\nbe decomposed into a confidence-raising transformation and a proper loss. This\nis why focal loss pushes the model to provide under-confident predictions on\nthe training data, resulting in being better calibrated on the test data, due\nto the generalization gap. Secondly, we reveal a strong connection between\ntemperature scaling and focal loss through its confidence-raising\ntransformation, which we refer to as the focal calibration map. Thirdly, we\npropose focal temperature scaling - a new post-hoc calibration method combining\nfocal calibration and temperature scaling. Our experiments on three image\nclassification datasets demonstrate that focal temperature scaling outperforms\nstandard temperature scaling.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7VABvdWpdRM_FBhp5MvqlYIoJJ256szfwP2eKq87QEg","pdfSize":"1213949"}
