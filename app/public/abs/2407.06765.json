{"id":"2407.06765","title":"A Generalization Bound for Nearly-Linear Networks","authors":"Eugene Golikov","authorsParsed":[["Golikov","Eugene",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 11:20:01 GMT"}],"updateDate":"2024-07-10","timestamp":1720524001000,"abstract":"  We consider nonlinear networks as perturbations of linear ones. Based on this\napproach, we present novel generalization bounds that become non-vacuous for\nnetworks that are close to being linear. The main advantage over the previous\nworks which propose non-vacuous generalization bounds is that our bounds are\na-priori: performing the actual training is not required for evaluating the\nbounds. To the best of our knowledge, they are the first non-vacuous\ngeneralization bounds for neural nets possessing this property.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}