{"id":"2407.00494","title":"Graph Neural Networks Gone Hogwild","authors":"Olga Solodova, Nick Richardson, Deniz Oktay, Ryan P. Adams","authorsParsed":[["Solodova","Olga",""],["Richardson","Nick",""],["Oktay","Deniz",""],["Adams","Ryan P.",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 17:11:09 GMT"}],"updateDate":"2024-07-02","timestamp":1719681069000,"abstract":"  Message passing graph neural networks (GNNs) would appear to be powerful\ntools to learn distributed algorithms via gradient descent, but generate\ncatastrophically incorrect predictions when nodes update asynchronously during\ninference. This failure under asynchrony effectively excludes these\narchitectures from many potential applications, such as learning local\ncommunication policies between resource-constrained agents in, e.g., robotic\nswarms or sensor networks. In this work we explore why this failure occurs in\ncommon GNN architectures, and identify \"implicitly-defined\" GNNs as a class of\narchitectures which is provably robust to partially asynchronous \"hogwild\"\ninference, adapting convergence guarantees from work in asynchronous and\ndistributed optimization, e.g., Bertsekas (1982); Niu et al. (2011). We then\npropose a novel implicitly-defined GNN architecture, which we call an energy\nGNN. We show that this architecture outperforms other GNNs from this class on a\nvariety of synthetic tasks inspired by multi-agent systems, and achieves\ncompetitive performance on real-world datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}