{"id":"2408.14514","title":"Improving Nonlinear Projection Heads using Pretrained Autoencoder\n  Embeddings","authors":"Andreas Schliebitz, Heiko Tapken, Martin Atzmueller","authorsParsed":[["Schliebitz","Andreas",""],["Tapken","Heiko",""],["Atzmueller","Martin",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 11:10:33 GMT"}],"updateDate":"2024-08-28","timestamp":1724584233000,"abstract":"  This empirical study aims at improving the effectiveness of the standard\n2-layer MLP projection head $g(\\cdot)$ featured in the SimCLR framework through\nthe use of pretrained autoencoder embeddings. Given a contrastive learning task\nwith a largely unlabeled image classification dataset, we first train a shallow\nautoencoder architecture and extract its compressed representations contained\nin the encoder's embedding layer. After freezing the weights within this\npretrained layer, we use it as a drop-in replacement for the input layer of\nSimCLR's default projector. Additionally, we also apply further architectural\nchanges to the projector by decreasing its width and changing its activation\nfunction. The different projection heads are then used to contrastively train\nand evaluate a feature extractor $f(\\cdot)$ following the SimCLR protocol,\nwhile also examining the performance impact of Z-score normalized datasets. Our\nexperiments indicate that using a pretrained autoencoder embedding in the\nprojector can not only increase classification accuracy by up to 2.9% or 1.7%\non average but can also significantly decrease the dimensionality of the\nprojection space. Our results also suggest, that using the sigmoid and tanh\nactivation functions within the projector can outperform ReLU in terms of peak\nand average classification accuracy. When applying our presented projectors,\nthen not applying Z-score normalization to datasets often increases peak\nperformance. In contrast, the default projection head can benefit more from\nnormalization. All experiments involving our pretrained projectors are\nconducted with frozen embeddings, since our test results indicate an advantage\ncompared to using their non-frozen counterparts.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"99ez_cu5LtZy1t0NQ-U1GNum-a-hAbSesuS2BBpNUts","pdfSize":"691275"}
