{"id":"2408.03035","title":"Training-Free Condition Video Diffusion Models for single frame\n  Spatial-Semantic Echocardiogram Synthesis","authors":"Van Phi Nguyen, Tri Nhan Luong Ha, Huy Hieu Pham, Quoc Long Tran","authorsParsed":[["Nguyen","Van Phi",""],["Ha","Tri Nhan Luong",""],["Pham","Huy Hieu",""],["Tran","Quoc Long",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 08:31:34 GMT"},{"version":"v2","created":"Fri, 6 Sep 2024 15:52:16 GMT"}],"updateDate":"2024-09-09","timestamp":1722933094000,"abstract":"  Conditional video diffusion models (CDM) have shown promising results for\nvideo synthesis, potentially enabling the generation of realistic\nechocardiograms to address the problem of data scarcity. However, current CDMs\nrequire a paired segmentation map and echocardiogram dataset. We present a new\nmethod called Free-Echo for generating realistic echocardiograms from a single\nend-diastolic segmentation map without additional training data. Our method is\nbased on the 3D-Unet with Temporal Attention Layers model and is conditioned on\nthe segmentation map using a training-free conditioning method based on SDEdit.\nWe evaluate our model on two public echocardiogram datasets, CAMUS and\nEchoNet-Dynamic. We show that our model can generate plausible echocardiograms\nthat are spatially aligned with the input segmentation map, achieving\nperformance comparable to training-based CDMs. Our work opens up new\npossibilities for generating echocardiograms from a single segmentation map,\nwhich can be used for data augmentation, domain adaptation, and other\napplications in medical imaging. Our code is available at\n\\url{https://github.com/gungui98/echo-free}\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}