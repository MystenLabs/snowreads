{"id":"2407.03857","title":"PFGS: High Fidelity Point Cloud Rendering via Feature Splatting","authors":"Jiaxu Wang and Ziyi Zhang and Junhao He and Renjing Xu","authorsParsed":[["Wang","Jiaxu",""],["Zhang","Ziyi",""],["He","Junhao",""],["Xu","Renjing",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 11:42:54 GMT"}],"updateDate":"2024-07-08","timestamp":1720093374000,"abstract":"  Rendering high-fidelity images from sparse point clouds is still challenging.\nExisting learning-based approaches suffer from either hole artifacts, missing\ndetails, or expensive computations. In this paper, we propose a novel framework\nto render high-quality images from sparse points. This method first attempts to\nbridge the 3D Gaussian Splatting and point cloud rendering, which includes\nseveral cascaded modules. We first use a regressor to estimate Gaussian\nproperties in a point-wise manner, the estimated properties are used to\nrasterize neural feature descriptors into 2D planes which are extracted from a\nmultiscale extractor. The projected feature volume is gradually decoded toward\nthe final prediction via a multiscale and progressive decoder. The whole\npipeline experiences a two-stage training and is driven by our well-designed\nprogressive and multiscale reconstruction loss. Experiments on different\nbenchmarks show the superiority of our method in terms of rendering qualities\nand the necessities of our main components.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}