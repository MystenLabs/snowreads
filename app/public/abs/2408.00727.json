{"id":"2408.00727","title":"Improving Retrieval-Augmented Generation in Medicine with Iterative\n  Follow-up Questions","authors":"Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, Aidong\n  Zhang","authorsParsed":[["Xiong","Guangzhi",""],["Jin","Qiao",""],["Wang","Xiao",""],["Zhang","Minjia",""],["Lu","Zhiyong",""],["Zhang","Aidong",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:18:17 GMT"},{"version":"v2","created":"Sun, 8 Sep 2024 02:42:07 GMT"}],"updateDate":"2024-09-10","timestamp":1722532697000,"abstract":"  The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a conventional RAG system\nand they will be further used to guide the query generation in the next\niteration. Our experiments show the improved performance of various LLMs\nbrought by i-MedRAG compared with conventional RAG on complex questions from\nclinical vignettes in the United States Medical Licensing Examination (USMLE),\nas well as various knowledge tests in the Massive Multitask Language\nUnderstanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all\nexisting prompt engineering and fine-tuning methods on GPT-3.5, achieving an\naccuracy of 69.68\\% on the MedQA dataset. In addition, we characterize the\nscaling properties of i-MedRAG with different iterations of follow-up queries\nand different numbers of queries per iteration. Our case studies show that\ni-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing\nan in-depth analysis of medical questions. To the best of our knowledge, this\nis the first-of-its-kind study on incorporating follow-up queries into medical\nRAG.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PIp8zpbNFPnIKdZeVgyy5lS_hmJ856MqOowF3J-I8Dg","pdfSize":"1714901","txDigest":"EcobxiKRR6VkrsncjSSUSFSMfQEFfg3YkyDVWd5DDubB","endEpoch":"1","status":"CERTIFIED"}
