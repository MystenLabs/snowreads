{"id":"2407.20955","title":"Emotion-driven Piano Music Generation via Two-stage Disentanglement and\n  Functional Representation","authors":"Jingyue Huang and Ke Chen and Yi-Hsuan Yang","authorsParsed":[["Huang","Jingyue",""],["Chen","Ke",""],["Yang","Yi-Hsuan",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 16:29:28 GMT"}],"updateDate":"2024-07-31","timestamp":1722356968000,"abstract":"  Managing the emotional aspect remains a challenge in automatic music\ngeneration. Prior works aim to learn various emotions at once, leading to\ninadequate modeling. This paper explores the disentanglement of emotions in\npiano performance generation through a two-stage framework. The first stage\nfocuses on valence modeling of lead sheet, and the second stage addresses\narousal modeling by introducing performance-level attributes. To further\ncapture features that shape valence, an aspect less explored by previous\napproaches, we introduce a novel functional representation of symbolic music.\nThis representation aims to capture the emotional impact of major-minor\ntonality, as well as the interactions among notes, chords, and key signatures.\nObjective and subjective experiments validate the effectiveness of our\nframework in both emotional valence and arousal modeling. We further leverage\nour framework in a novel application of emotional controls, showing a broad\npotential in emotion-driven music generation.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RgW8d7zpVVrUS8y8RbkP515BvbtnGKvno1Lh7rQGelA","pdfSize":"1458062"}
