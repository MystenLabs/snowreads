{"id":"2408.04194","title":"FDI: Attack Neural Code Generation Systems through User Feedback Channel","authors":"Zhensu Sun, Xiaoning Du, Xiapu Luo, Fu Song, David Lo, Li Li","authorsParsed":[["Sun","Zhensu",""],["Du","Xiaoning",""],["Luo","Xiapu",""],["Song","Fu",""],["Lo","David",""],["Li","Li",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 03:28:30 GMT"}],"updateDate":"2024-08-09","timestamp":1723087710000,"abstract":"  Neural code generation systems have recently attracted increasing attention\nto improve developer productivity and speed up software development. Typically,\nthese systems maintain a pre-trained neural model and make it available to\ngeneral users as a service (e.g., through remote APIs) and incorporate a\nfeedback mechanism to extensively collect and utilize the users' reaction to\nthe generated code, i.e., user feedback. However, the security implications of\nsuch feedback have not yet been explored. With a systematic study of current\nfeedback mechanisms, we find that feedback makes these systems vulnerable to\nfeedback data injection (FDI) attacks. We discuss the methodology of FDI\nattacks and present a pre-attack profiling strategy to infer the attack\nconstraints of a targeted system in the black-box setting. We demonstrate two\nproof-of-concept examples utilizing the FDI attack surface to implement prompt\ninjection attacks and backdoor attacks on practical neural code generation\nsystems. The attacker may stealthily manipulate a neural code generation system\nto generate code with vulnerabilities, attack payload, and malicious and spam\nmessages. Our findings reveal the security implications of feedback mechanisms\nin neural code generation systems, paving the way for increasing their\nsecurity.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}