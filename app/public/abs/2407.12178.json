{"id":"2407.12178","title":"Exploration Unbound","authors":"Dilip Arumugam, Wanqiao Xu, Benjamin Van Roy","authorsParsed":[["Arumugam","Dilip",""],["Xu","Wanqiao",""],["Van Roy","Benjamin",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 21:14:43 GMT"}],"updateDate":"2024-07-23","timestamp":1721164483000,"abstract":"  A sequential decision-making agent balances between exploring to gain new\nknowledge about an environment and exploiting current knowledge to maximize\nimmediate reward. For environments studied in the traditional literature,\noptimal decisions gravitate over time toward exploitation as the agent\naccumulates sufficient knowledge and the benefits of further exploration\nvanish. What if, however, the environment offers an unlimited amount of useful\nknowledge and there is large benefit to further exploration no matter how much\nthe agent has learned? We offer a simple, quintessential example of such a\ncomplex environment. In this environment, rewards are unbounded and an agent\ncan always increase the rate at which rewards accumulate by exploring to learn\nmore. Consequently, an optimal agent forever maintains a propensity to explore.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}