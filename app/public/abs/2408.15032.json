{"id":"2408.15032","title":"Mamba2MIL: State Space Duality Based Multiple Instance Learning for\n  Computational Pathology","authors":"Yuqi Zhang, Xiaoqian Zhang, Jiakai Wang, Yuancheng Yang, Taiying Peng,\n  Chao Tong","authorsParsed":[["Zhang","Yuqi",""],["Zhang","Xiaoqian",""],["Wang","Jiakai",""],["Yang","Yuancheng",""],["Peng","Taiying",""],["Tong","Chao",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 13:01:19 GMT"}],"updateDate":"2024-08-28","timestamp":1724763679000,"abstract":"  Computational pathology (CPath) has significantly advanced the clinical\npractice of pathology. Despite the progress made, Multiple Instance Learning\n(MIL), a promising paradigm within CPath, continues to face challenges,\nparticularly related to incomplete information utilization. Existing\nframeworks, such as those based on Convolutional Neural Networks (CNNs),\nattention, and selective scan space state sequential model (SSM), lack\nsufficient flexibility and scalability in fusing diverse features, and cannot\neffectively fuse diverse features. Additionally, current approaches do not\nadequately exploit order-related and order-independent features, resulting in\nsuboptimal utilization of sequence information. To address these limitations,\nwe propose a novel MIL framework called Mamba2MIL. Our framework utilizes the\nstate space duality model (SSD) to model long sequences of patches of whole\nslide images (WSIs), which, combined with weighted feature selection, supports\nthe fusion processing of more branching features and can be extended according\nto specific application needs. Moreover, we introduce a sequence transformation\nmethod tailored to varying WSI sizes, which enhances sequence-independent\nfeatures while preserving local sequence information, thereby improving\nsequence information utilization. Extensive experiments demonstrate that\nMamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive\nexperiments across multiple datasets, achieving improvements in nearly all\nperformance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a\nbinary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the\nBRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an\naccuracy of 0.4981. The code is available at\nhttps://github.com/YuqiZhang-Buaa/Mamba2MIL.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}