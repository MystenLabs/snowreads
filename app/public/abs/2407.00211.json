{"id":"2407.00211","title":"Detection and Measurement of Syntactic Templates in Generated Text","authors":"Chantal Shaib, Yanai Elazar, Junyi Jessy Li, Byron C. Wallace","authorsParsed":[["Shaib","Chantal",""],["Elazar","Yanai",""],["Li","Junyi Jessy",""],["Wallace","Byron C.",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 19:34:23 GMT"}],"updateDate":"2024-07-02","timestamp":1719603263000,"abstract":"  Recent work on evaluating the diversity of text generated by LLMs has focused\non word-level features. Here we offer an analysis of syntactic features to\ncharacterize general repetition in models, beyond frequent n-grams.\nSpecifically, we define syntactic templates and show that models tend to\nproduce templated text in downstream tasks at a higher rate than what is found\nin human-reference texts. We find that most (76%) templates in model-generated\ntext can be found in pre-training data (compared to only 35% of human-authored\ntext), and are not overwritten during fine-tuning processes such as RLHF. This\nconnection to the pre-training data allows us to analyze syntactic templates in\nmodels where we do not have the pre-training data. We also find that templates\nas features are able to differentiate between models, tasks, and domains, and\nare useful for qualitatively evaluating common model constructions. Finally, we\ndemonstrate the use of templates as a useful tool for analyzing style\nmemorization of training data in LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XN_SEenl-FhxLX7zLhgamm1x05Nev0On6k9SeW8y8N8","pdfSize":"795450"}
