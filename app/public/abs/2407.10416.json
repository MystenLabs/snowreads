{"id":"2407.10416","title":"SOFA: A Compute-Memory Optimized Sparsity Accelerator via Cross-Stage\n  Coordinated Tiling","authors":"Huizheng Wang, Jiahao Fang, Xinru Tang, Zhiheng Yue, Jinxi Li, Yubin\n  Qin, Sihan Guan, Qize Yang, Yang Wang, Chao Li, Yang Hu, Shouyi Yin","authorsParsed":[["Wang","Huizheng",""],["Fang","Jiahao",""],["Tang","Xinru",""],["Yue","Zhiheng",""],["Li","Jinxi",""],["Qin","Yubin",""],["Guan","Sihan",""],["Yang","Qize",""],["Wang","Yang",""],["Li","Chao",""],["Hu","Yang",""],["Yin","Shouyi",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 03:44:33 GMT"}],"updateDate":"2024-07-16","timestamp":1721015073000,"abstract":"  Benefiting from the self-attention mechanism, Transformer models have\nattained impressive contextual comprehension capabilities for lengthy texts.\nThe requirements of high-throughput inference arise as the large language\nmodels (LLMs) become increasingly prevalent, which calls for large-scale token\nparallel processing (LTPP). However, existing dynamic sparse accelerators\nstruggle to effectively handle LTPP, as they solely focus on separate stage\noptimization, and with most efforts confined to computational enhancements. By\nre-examining the end-to-end flow of dynamic sparse acceleration, we pinpoint an\never-overlooked opportunity that the LTPP can exploit the intrinsic\ncoordination among stages to avoid excessive memory access and redundant\ncomputation. Motivated by our observation, we present SOFA, a cross-stage\ncompute-memory efficient algorithm-hardware co-design, which is tailored to\ntackle the challenges posed by LTPP of Transformer inference effectively. We\nfirst propose a novel leading zero computing paradigm, which predicts attention\nsparsity by using log-based add-only operations to avoid the significant\noverhead of prediction. Then, a distributed sorting and a sorted updating\nFlashAttention mechanism are proposed with a cross-stage coordinated tiling\nprinciple, which enables fine-grained and lightweight coordination among\nstages, helping optimize memory access and latency. Further, we propose a SOFA\naccelerator to support these optimizations efficiently. Extensive experiments\non 20 benchmarks show that SOFA achieves $9.5\\times$ speed up and $71.5\\times$\nhigher energy efficiency than Nvidia A100 GPU. Compared to 8 SOTA accelerators,\nSOFA achieves an average $15.8\\times$ energy efficiency, $10.3\\times$ area\nefficiency and $9.3\\times$ speed up, respectively.\n","subjects":["Computing Research Repository/Hardware Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}