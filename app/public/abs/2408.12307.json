{"id":"2408.12307","title":"Leveraging Unlabeled Data Sharing through Kernel Function Approximation\n  in Offline Reinforcement Learning","authors":"Yen-Ru Lai, Fu-Chieh Chang, Pei-Yuan Wu","authorsParsed":[["Lai","Yen-Ru",""],["Chang","Fu-Chieh",""],["Wu","Pei-Yuan",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 11:31:51 GMT"}],"updateDate":"2024-08-23","timestamp":1724326311000,"abstract":"  Offline reinforcement learning (RL) learns policies from a fixed dataset, but\noften requires large amounts of data. The challenge arises when labeled\ndatasets are expensive, especially when rewards have to be provided by human\nlabelers for large datasets. In contrast, unlabelled data tends to be less\nexpensive. This situation highlights the importance of finding effective ways\nto use unlabelled data in offline RL, especially when labelled data is limited\nor expensive to obtain. In this paper, we present the algorithm to utilize the\nunlabeled data in the offline RL method with kernel function approximation and\ngive the theoretical guarantee. We present various eigenvalue decay conditions\nof $\\mathcal{H}_k$ which determine the complexity of the algorithm. In summary,\nour work provides a promising approach for exploiting the advantages offered by\nunlabeled data in offline RL, whilst maintaining theoretical assurances.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"46ftiqYSeSteHUuuLFEiOolsmaeXJ7IZItuKovCM2rM","pdfSize":"373332"}
