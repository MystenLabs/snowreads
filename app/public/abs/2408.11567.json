{"id":"2408.11567","title":"Positional Prompt Tuning for Efficient 3D Representation Learning","authors":"Shaochen Zhang, Zekun Qi, Runpei Dong, Xiuxiu Bai, Xing Wei","authorsParsed":[["Zhang","Shaochen",""],["Qi","Zekun",""],["Dong","Runpei",""],["Bai","Xiuxiu",""],["Wei","Xing",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 12:18:34 GMT"}],"updateDate":"2024-08-22","timestamp":1724242714000,"abstract":"  Point cloud analysis has achieved significant development and is\nwell-performed in multiple downstream tasks like point cloud classification and\nsegmentation, etc. Being conscious of the simplicity of the position encoding\nstructure in Transformer-based architectures, we attach importance to the\nposition encoding as a high-dimensional part and the patch encoder to offer\nmulti-scale information. Together with the sequential Transformer, the whole\nmodule with position encoding comprehensively constructs a multi-scale feature\nabstraction module that considers both the local parts from the patch and the\nglobal parts from center points as position encoding. With only a few\nparameters, the position embedding module fits the setting of PEFT\n(Parameter-Efficient Fine-Tuning) tasks pretty well. Thus we unfreeze these\nparameters as a fine-tuning part. At the same time, we review the existing\nprompt and adapter tuning methods, proposing a fresh way of prompts and\nsynthesizing them with adapters as dynamic adjustments. Our Proposed method of\nPEFT tasks, namely PPT, with only 1.05% of parameters for training, gets\nstate-of-the-art results in several mainstream datasets, such as 95.01%\naccuracy in the ScanObjectNN OBJ_BG dataset. Codes will be released at\nhttps://github.com/zsc000722/PPT.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}