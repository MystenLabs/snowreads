{"id":"2407.01378","title":"Beyond Throughput and Compression Ratios: Towards High End-to-end\n  Utility of Gradient Compression","authors":"Wenchen Han, Shay Vargaftik, Michael Mitzenmacher, Brad Karp, Ran Ben\n  Basat","authorsParsed":[["Han","Wenchen",""],["Vargaftik","Shay",""],["Mitzenmacher","Michael",""],["Karp","Brad",""],["Basat","Ran Ben",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 15:32:28 GMT"}],"updateDate":"2024-07-02","timestamp":1719847948000,"abstract":"  Gradient aggregation has long been identified as a major bottleneck in\ntoday's large-scale distributed machine learning training systems. One\npromising solution to mitigate such bottlenecks is gradient compression,\ndirectly reducing communicated gradient data volume. However, in practice, many\ngradient compression schemes do not achieve acceleration of the training\nprocess while also preserving accuracy.\n  In this work, we identify several common issues in previous gradient\ncompression systems and evaluation methods. These issues include excessive\ncomputational overheads; incompatibility with all-reduce; and inappropriate\nevaluation metrics, such as not using an end-to-end metric or using a 32-bit\nbaseline instead of a 16-bit baseline. We propose several general design and\nevaluation techniques to address these issues and provide guidelines for future\nwork. Our preliminary evaluation shows that our techniques enhance the system's\nperformance and provide a clearer understanding of the end-to-end utility of\ngradient compression methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Networking and Internet Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}