{"id":"2408.05426","title":"SAM-FNet: SAM-Guided Fusion Network for Laryngo-Pharyngeal Tumor\n  Detection","authors":"Jia Wei, Yun Li, Meiyu Qiu, Hongyu Chen, Xiaomao Fan, Wenbin Lei","authorsParsed":[["Wei","Jia",""],["Li","Yun",""],["Qiu","Meiyu",""],["Chen","Hongyu",""],["Fan","Xiaomao",""],["Lei","Wenbin",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 04:14:14 GMT"},{"version":"v2","created":"Thu, 15 Aug 2024 03:36:43 GMT"}],"updateDate":"2024-08-16","timestamp":1723263254000,"abstract":"  Laryngo-pharyngeal cancer (LPC) is a highly fatal malignant disease affecting\nthe head and neck region. Previous studies on endoscopic tumor detection,\nparticularly those leveraging dual-branch network architectures, have shown\nsignificant advancements in tumor detection. These studies highlight the\npotential of dual-branch networks in improving diagnostic accuracy by\neffectively integrating global and local (lesion) feature extraction. However,\nthey are still limited in their capabilities to accurately locate the lesion\nregion and capture the discriminative feature information between the global\nand local branches. To address these issues, we propose a novel SAM-guided\nfusion network (SAM-FNet), a dual-branch network for laryngo-pharyngeal tumor\ndetection. By leveraging the powerful object segmentation capabilities of the\nSegment Anything Model (SAM), we introduce the SAM into the SAM-FNet to\naccurately segment the lesion region. Furthermore, we propose a GAN-like\nfeature optimization (GFO) module to capture the discriminative features\nbetween the global and local branches, enhancing the fusion feature\ncomplementarity. Additionally, we collect two LPC datasets from the First\nAffiliated Hospital (FAHSYSU) and the Sixth Affiliated Hospital (SAHSYSU) of\nSun Yat-sen University. The FAHSYSU dataset is used as the internal dataset for\ntraining the model, while the SAHSYSU dataset is used as the external dataset\nfor evaluating the model's performance. Extensive experiments on both datasets\nof FAHSYSU and SAHSYSU demonstrate that the SAM-FNet can achieve competitive\nresults, outperforming the state-of-the-art counterparts. The source code of\nSAM-FNet is available at the URL of https://github.com/VVJia/SAM-FNet.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}