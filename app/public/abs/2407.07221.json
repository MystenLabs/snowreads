{"id":"2407.07221","title":"Tracing Back the Malicious Clients in Poisoning Attacks to Federated\n  Learning","authors":"Yuqi Jia and Minghong Fang and Hongbin Liu and Jinghuai Zhang and Neil\n  Zhenqiang Gong","authorsParsed":[["Jia","Yuqi",""],["Fang","Minghong",""],["Liu","Hongbin",""],["Zhang","Jinghuai",""],["Gong","Neil Zhenqiang",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 20:35:36 GMT"}],"updateDate":"2024-07-11","timestamp":1720557336000,"abstract":"  Poisoning attacks compromise the training phase of federated learning (FL)\nsuch that the learned global model misclassifies attacker-chosen inputs called\ntarget inputs. Existing defenses mainly focus on protecting the training phase\nof FL such that the learnt global model is poison free. However, these defenses\noften achieve limited effectiveness when the clients' local training data is\nhighly non-iid or the number of malicious clients is large, as confirmed in our\nexperiments. In this work, we propose FLForensics, the first poison-forensics\nmethod for FL. FLForensics complements existing training-phase defenses. In\nparticular, when training-phase defenses fail and a poisoned global model is\ndeployed, FLForensics aims to trace back the malicious clients that performed\nthe poisoning attack after a misclassified target input is identified. We\ntheoretically show that FLForensics can accurately distinguish between benign\nand malicious clients under a formal definition of poisoning attack. Moreover,\nwe empirically show the effectiveness of FLForensics at tracing back both\nexisting and adaptive poisoning attacks on five benchmark datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}