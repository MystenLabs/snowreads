{"id":"2408.13055","title":"Atlas Gaussians Diffusion for 3D Generation with Infinite Number of\n  Points","authors":"Haitao Yang, Yuan Dong, Hanwen Jiang, Dejia Xu, Georgios Pavlakos,\n  Qixing Huang","authorsParsed":[["Yang","Haitao",""],["Dong","Yuan",""],["Jiang","Hanwen",""],["Xu","Dejia",""],["Pavlakos","Georgios",""],["Huang","Qixing",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 13:27:27 GMT"}],"updateDate":"2024-08-26","timestamp":1724419647000,"abstract":"  Using the latent diffusion model has proven effective in developing novel 3D\ngeneration techniques. To harness the latent diffusion model, a key challenge\nis designing a high-fidelity and efficient representation that links the latent\nspace and the 3D space. In this paper, we introduce Atlas Gaussians, a novel\nrepresentation for feed-forward native 3D generation. Atlas Gaussians represent\na shape as the union of local patches, and each patch can decode 3D Gaussians.\nWe parameterize a patch as a sequence of feature vectors and design a learnable\nfunction to decode 3D Gaussians from the feature vectors. In this process, we\nincorporate UV-based sampling, enabling the generation of a sufficiently large,\nand theoretically infinite, number of 3D Gaussian points. The large amount of\n3D Gaussians enables high-quality details of generation results. Moreover, due\nto local awareness of the representation, the transformer-based decoding\nprocedure operates on a patch level, ensuring efficiency. We train a\nvariational autoencoder to learn the Atlas Gaussians representation, and then\napply a latent diffusion model on its latent space for learning 3D Generation.\nExperiments show that our approach outperforms the prior arts of feed-forward\nnative 3D generation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}