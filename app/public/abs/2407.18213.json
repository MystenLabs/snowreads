{"id":"2407.18213","title":"Exploring Scaling Trends in LLM Robustness","authors":"Nikolaus Howe, Micha{\\l} Zajac, Ian McKenzie, Oskar Hollinsworth, Tom\n  Tseng, Pierre-Luc Bacon, Adam Gleave","authorsParsed":[["Howe","Nikolaus",""],["Zajac","Micha≈Ç",""],["McKenzie","Ian",""],["Hollinsworth","Oskar",""],["Tseng","Tom",""],["Bacon","Pierre-Luc",""],["Gleave","Adam",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 17:26:41 GMT"},{"version":"v2","created":"Fri, 26 Jul 2024 11:51:58 GMT"}],"updateDate":"2024-07-29","timestamp":1721928401000,"abstract":"  Language model capabilities predictably improve from scaling a model's size\nand training data. Motivated by this, increasingly large language models have\nbeen trained, yielding an array of impressive capabilities. Yet these models\nare vulnerable to adversarial prompts, such as \"jailbreaks\" that hijack models\nto perform undesired behaviors, posing a significant risk of misuse. Prior work\nindicates that computer vision models become more robust with model and data\nscaling, raising the question: does language model robustness also improve with\nscale? We study this question empirically, finding that larger models respond\nsubstantially better to adversarial training, but there is little to no benefit\nfrom model scale in the absence of explicit defenses.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}