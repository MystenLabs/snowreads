{"id":"2407.15421","title":"Planning behavior in a recurrent neural network that plays Sokoban","authors":"Adri\\`a Garriga-Alonso, Mohammad Taufeeque, Adam Gleave","authorsParsed":[["Garriga-Alonso","Adri√†",""],["Taufeeque","Mohammad",""],["Gleave","Adam",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 06:57:34 GMT"}],"updateDate":"2024-07-23","timestamp":1721631454000,"abstract":"  To predict how advanced neural networks generalize to novel situations, it is\nessential to understand how they reason. Guez et al. (2019, \"An investigation\nof model-free planning\") trained a recurrent neural network (RNN) to play\nSokoban with model-free reinforcement learning. They found that adding extra\ncomputation steps to the start of episodes at test time improves the RNN's\nsuccess rate. We further investigate this phenomenon, finding that it rapidly\nemerges early on in training and then slowly fades, but only for comparatively\neasier levels. The RNN also often takes redundant actions at episode starts,\nand these are reduced by adding extra computation steps. Our results suggest\nthat the RNN learns to take time to think by `pacing', despite the per-step\npenalties, indicating that training incentivizes planning capabilities. The\nsmall size (1.29M parameters) and interesting behavior of this model make it an\nexcellent model organism for mechanistic interpretability.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"GFt8rpNkekd1DtA08gnNgl8CoSKOWlZ9cJIB8q-7NPk","pdfSize":"553538"}
