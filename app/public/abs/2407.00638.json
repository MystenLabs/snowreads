{"id":"2407.00638","title":"A Collocation-based Method for Addressing Challenges in Word-level\n  Metric Differential Privacy","authors":"Stephen Meisenbacher, Maulik Chevli, and Florian Matthes","authorsParsed":[["Meisenbacher","Stephen",""],["Chevli","Maulik",""],["Matthes","Florian",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 09:37:34 GMT"}],"updateDate":"2024-07-02","timestamp":1719740254000,"abstract":"  Applications of Differential Privacy (DP) in NLP must distinguish between the\nsyntactic level on which a proposed mechanism operates, often taking the form\nof $\\textit{word-level}$ or $\\textit{document-level}$ privatization. Recently,\nseveral word-level $\\textit{Metric}$ Differential Privacy approaches have been\nproposed, which rely on this generalized DP notion for operating in word\nembedding spaces. These approaches, however, often fail to produce semantically\ncoherent textual outputs, and their application at the sentence- or\ndocument-level is only possible by a basic composition of word perturbations.\nIn this work, we strive to address these challenges by operating\n$\\textit{between}$ the word and sentence levels, namely with\n$\\textit{collocations}$. By perturbing n-grams rather than single words, we\ndevise a method where composed privatized outputs have higher semantic\ncoherence and variable length. This is accomplished by constructing an\nembedding model based on frequently occurring word groups, in which unigram\nwords co-exist with bi- and trigram collocations. We evaluate our method in\nutility and privacy tests, which make a clear case for tokenization strategies\nbeyond the word level.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"BIvbsAFVXR_sceQQ6zf0flrVX3H5MKQFZ9MyjHXESI8","pdfSize":"677043","objectId":"0x0c481a78589a12388eeef1941e25729c496501614eaa5381cfe809f89db67731","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
