{"id":"2407.03636","title":"Diff-Restorer: Unleashing Visual Prompts for Diffusion-based Universal\n  Image Restoration","authors":"Yuhong Zhang, Hengsheng Zhang, Xinning Chai, Zhengxue Cheng, Rong Xie,\n  Li Song, Wenjun Zhang","authorsParsed":[["Zhang","Yuhong",""],["Zhang","Hengsheng",""],["Chai","Xinning",""],["Cheng","Zhengxue",""],["Xie","Rong",""],["Song","Li",""],["Zhang","Wenjun",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 05:01:10 GMT"}],"updateDate":"2024-07-08","timestamp":1720069270000,"abstract":"  Image restoration is a classic low-level problem aimed at recovering\nhigh-quality images from low-quality images with various degradations such as\nblur, noise, rain, haze, etc. However, due to the inherent complexity and\nnon-uniqueness of degradation in real-world images, it is challenging for a\nmodel trained for single tasks to handle real-world restoration problems\neffectively. Moreover, existing methods often suffer from over-smoothing and\nlack of realism in the restored results. To address these issues, we propose\nDiff-Restorer, a universal image restoration method based on the diffusion\nmodel, aiming to leverage the prior knowledge of Stable Diffusion to remove\ndegradation while generating high perceptual quality restoration results.\nSpecifically, we utilize the pre-trained visual language model to extract\nvisual prompts from degraded images, including semantic and degradation\nembeddings. The semantic embeddings serve as content prompts to guide the\ndiffusion model for generation. In contrast, the degradation embeddings\nmodulate the Image-guided Control Module to generate spatial priors for\ncontrolling the spatial structure of the diffusion process, ensuring\nfaithfulness to the original image. Additionally, we design a Degradation-aware\nDecoder to perform structural correction and convert the latent code to the\npixel domain. We conducted comprehensive qualitative and quantitative analysis\non restoration tasks with different degradations, demonstrating the\neffectiveness and superiority of our approach.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}