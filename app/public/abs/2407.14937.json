{"id":"2407.14937","title":"Operationalizing a Threat Model for Red-Teaming Large Language Models\n  (LLMs)","authors":"Apurv Verma, Satyapriya Krishna, Sebastian Gehrmann, Madhavan\n  Seshadri, Anu Pradhan, Tom Ault, Leslie Barrett, David Rabinowitz, John\n  Doucette, NhatHai Phan","authorsParsed":[["Verma","Apurv",""],["Krishna","Satyapriya",""],["Gehrmann","Sebastian",""],["Seshadri","Madhavan",""],["Pradhan","Anu",""],["Ault","Tom",""],["Barrett","Leslie",""],["Rabinowitz","David",""],["Doucette","John",""],["Phan","NhatHai",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 17:05:04 GMT"}],"updateDate":"2024-07-23","timestamp":1721495104000,"abstract":"  Creating secure and resilient applications with large language models (LLM)\nrequires anticipating, adjusting to, and countering unforeseen threats.\nRed-teaming has emerged as a critical technique for identifying vulnerabilities\nin real-world LLM implementations. This paper presents a detailed threat model\nand provides a systematization of knowledge (SoK) of red-teaming attacks on\nLLMs. We develop a taxonomy of attacks based on the stages of the LLM\ndevelopment and deployment process and extract various insights from previous\nresearch. In addition, we compile methods for defense and practical red-teaming\nstrategies for practitioners. By delineating prominent attack motifs and\nshedding light on various entry points, this paper provides a framework for\nimproving the security and robustness of LLM-based systems.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}