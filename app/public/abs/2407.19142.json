{"id":"2407.19142","title":"On the benefits of pixel-based hierarchical policies for task\n  generalization","authors":"Tudor Cristea-Platon, Bogdan Mazoure, Josh Susskind, Walter Talbott","authorsParsed":[["Cristea-Platon","Tudor",""],["Mazoure","Bogdan",""],["Susskind","Josh",""],["Talbott","Walter",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 01:26:26 GMT"}],"updateDate":"2024-07-30","timestamp":1722043586000,"abstract":"  Reinforcement learning practitioners often avoid hierarchical policies,\nespecially in image-based observation spaces. Typically, the single-task\nperformance improvement over flat-policy counterparts does not justify the\nadditional complexity associated with implementing a hierarchy. However, by\nintroducing multiple decision-making levels, hierarchical policies can compose\nlower-level policies to more effectively generalize between tasks, highlighting\nthe need for multi-task evaluations. We analyze the benefits of hierarchy\nthrough simulated multi-task robotic control experiments from pixels. Our\nresults show that hierarchical policies trained with task conditioning can (1)\nincrease performance on training tasks, (2) lead to improved reward and\nstate-space generalizations in similar tasks, and (3) decrease the complexity\nof fine tuning required to solve novel tasks. Thus, we believe that\nhierarchical policies should be considered when building reinforcement learning\narchitectures capable of generalizing between tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}