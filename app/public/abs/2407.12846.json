{"id":"2407.12846","title":"Identifying the Source of Generation for Large Language Models","authors":"Bumjin Park and Jaesik Choi","authorsParsed":[["Park","Bumjin",""],["Choi","Jaesik",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 08:52:15 GMT"}],"updateDate":"2024-07-19","timestamp":1720169535000,"abstract":"  Large language models (LLMs) memorize text from several sources of documents.\nIn pretraining, LLM trains to maximize the likelihood of text but neither\nreceives the source of the text nor memorizes the source. Accordingly, LLM can\nnot provide document information on the generated content, and users do not\nobtain any hint of reliability, which is crucial for factuality or privacy\ninfringement. This work introduces token-level source identification in the\ndecoding step, which maps the token representation to the reference document.\nWe propose a bi-gram source identifier, a multi-layer perceptron with two\nsuccessive token representations as input for better generalization. We conduct\nextensive experiments on Wikipedia and PG19 datasets with several LLMs, layer\nlocations, and identifier sizes. The overall results show a possibility of\ntoken-level source identifiers for tracing the document, a crucial problem for\nthe safe use of LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"A-7qLp5LM95r3Kju0315qc6cPzkJ2QRrVrZVjRZ9EY0","pdfSize":"1592601"}
