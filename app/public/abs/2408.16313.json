{"id":"2408.16313","title":"FA-YOLO: Research On Efficient Feature Selection YOLO Improved Algorithm\n  Based On FMDS and AGMF Modules","authors":"Yukang Huo, Mingyuan Yao, Qingbin Tian, Tonghao Wang, Ruifeng Wang and\n  Haihua Wang","authorsParsed":[["Huo","Yukang",""],["Yao","Mingyuan",""],["Tian","Qingbin",""],["Wang","Tonghao",""],["Wang","Ruifeng",""],["Wang","Haihua",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 07:22:16 GMT"}],"updateDate":"2024-08-30","timestamp":1724916136000,"abstract":"  Over the past few years, the YOLO series of models has emerged as one of the\ndominant methodologies in the realm of object detection. Many studies have\nadvanced these baseline models by modifying their architectures, enhancing data\nquality, and developing new loss functions. However, current models still\nexhibit deficiencies in processing feature maps, such as overlooking the fusion\nof cross-scale features and a static fusion approach that lacks the capability\nfor dynamic feature adjustment. To address these issues, this paper introduces\nan efficient Fine-grained Multi-scale Dynamic Selection Module (FMDS Module),\nwhich applies a more effective dynamic feature selection and fusion method on\nfine-grained multi-scale feature maps, significantly enhancing the detection\naccuracy of small, medium, and large-sized targets in complex environments.\nFurthermore, this paper proposes an Adaptive Gated Multi-branch Focus Fusion\nModule (AGMF Module), which utilizes multiple parallel branches to perform\ncomplementary fusion of various features captured by the gated unit branch,\nFMDS Module branch, and TripletAttention branch. This approach further enhances\nthe comprehensiveness, diversity, and integrity of feature fusion. This paper\nhas integrated the FMDS Module, AGMF Module, into Yolov9 to develop a novel\nobject detection model named FA-YOLO. Extensive experimental results show that\nunder identical experimental conditions, FA-YOLO achieves an outstanding 66.1%\nmean Average Precision (mAP) on the PASCAL VOC 2007 dataset, representing 1.0%\nimprovement over YOLOv9's 65.1%. Additionally, the detection accuracies of\nFA-YOLO for small, medium, and large targets are 44.1%, 54.6%, and 70.8%,\nrespectively, showing improvements of 2.0%, 3.1%, and 0.9% compared to YOLOv9's\n42.1%, 51.5%, and 69.9%.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}