{"id":"2408.09350","title":"E-CGL: An Efficient Continual Graph Learner","authors":"Jianhao Guo, Zixuan Ni, Yun Zhu, Siliang Tang","authorsParsed":[["Guo","Jianhao",""],["Ni","Zixuan",""],["Zhu","Yun",""],["Tang","Siliang",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 04:10:30 GMT"}],"updateDate":"2024-08-20","timestamp":1723954230000,"abstract":"  Continual learning has emerged as a crucial paradigm for learning from\nsequential data while preserving previous knowledge. In the realm of continual\ngraph learning, where graphs continuously evolve based on streaming graph data,\ncontinual graph learning presents unique challenges that require adaptive and\nefficient graph learning methods in addition to the problem of catastrophic\nforgetting. The first challenge arises from the interdependencies between\ndifferent graph data, where previous graphs can influence new data\ndistributions. The second challenge lies in the efficiency concern when dealing\nwith large graphs. To addresses these two problems, we produce an Efficient\nContinual Graph Learner (E-CGL) in this paper. We tackle the interdependencies\nissue by demonstrating the effectiveness of replay strategies and introducing a\ncombined sampling strategy that considers both node importance and diversity.\nTo overcome the limitation of efficiency, E-CGL leverages a simple yet\neffective MLP model that shares weights with a GCN during training, achieving\nacceleration by circumventing the computationally expensive message passing\nprocess. Our method comprehensively surpasses nine baselines on four graph\ncontinual learning datasets under two settings, meanwhile E-CGL largely reduces\nthe catastrophic forgetting problem down to an average of -1.1%. Additionally,\nE-CGL achieves an average of 15.83x training time acceleration and 4.89x\ninference time acceleration across the four datasets. These results indicate\nthat E-CGL not only effectively manages the correlation between different graph\ndata during continual training but also enhances the efficiency of continual\nlearning on large graphs. The code is publicly available at\nhttps://github.com/aubreygjh/E-CGL.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}