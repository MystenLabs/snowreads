{"id":"2408.13452","title":"Data Augmentation for Continual RL via Adversarial Gradient Episodic\n  Memory","authors":"Sihao Wu, Xingyu Zhao, Xiaowei Huang","authorsParsed":[["Wu","Sihao",""],["Zhao","Xingyu",""],["Huang","Xiaowei",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 03:43:35 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 02:19:31 GMT"}],"updateDate":"2024-08-28","timestamp":1724471015000,"abstract":"  Data efficiency of learning, which plays a key role in the Reinforcement\nLearning (RL) training process, becomes even more important in continual RL\nwith sequential environments. In continual RL, the learner interacts with\nnon-stationary, sequential tasks and is required to learn new tasks without\nforgetting previous knowledge. However, there is little work on implementing\ndata augmentation for continual RL. In this paper, we investigate the efficacy\nof data augmentation for continual RL. Specifically, we provide benchmarking\ndata augmentations for continual RL, by (1) summarising existing data\naugmentation methods and (2) including a new augmentation method for continual\nRL: Adversarial Augmentation with Gradient Episodic Memory (Adv-GEM). Extensive\nexperiments show that data augmentations, such as random amplitude scaling,\nstate-switch, mixup, adversarial augmentation, and Adv-GEM, can improve\nexisting continual RL algorithms in terms of their average performance,\ncatastrophic forgetting, and forward transfer, on robot control tasks. All data\naugmentation methods are implemented as plug-in modules for trivial integration\ninto continual RL methods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}