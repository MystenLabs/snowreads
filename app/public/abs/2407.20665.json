{"id":"2407.20665","title":"Powerful A/B-Testing Metrics and Where to Find Them","authors":"Olivier Jeunen, Shubham Baweja, Neeti Pokharna, Aleksei Ustimenko","authorsParsed":[["Jeunen","Olivier",""],["Baweja","Shubham",""],["Pokharna","Neeti",""],["Ustimenko","Aleksei",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 08:59:50 GMT"}],"updateDate":"2024-07-31","timestamp":1722329990000,"abstract":"  Online controlled experiments, colloquially known as A/B-tests, are the bread\nand butter of real-world recommender system evaluation. Typically, end-users\nare randomly assigned some system variant, and a plethora of metrics are then\ntracked, collected, and aggregated throughout the experiment. A North Star\nmetric (e.g. long-term growth or revenue) is used to assess which system\nvariant should be deemed superior. As a result, most collected metrics are\nsupporting in nature, and serve to either (i) provide an understanding of how\nthe experiment impacts user experience, or (ii) allow for confident\ndecision-making when the North Star metric moves insignificantly (i.e. a false\nnegative or type-II error). The latter is not straightforward: suppose a\ntreatment variant leads to fewer but longer sessions, with more views but fewer\nengagements; should this be considered a positive or negative outcome?\n  The question then becomes: how do we assess a supporting metric's utility\nwhen it comes to decision-making using A/B-testing? Online platforms typically\nrun dozens of experiments at any given time. This provides a wealth of\ninformation about interventions and treatment effects that can be used to\nevaluate metrics' utility for online evaluation. We propose to collect this\ninformation and leverage it to quantify type-I, type-II, and type-III errors\nfor the metrics of interest, alongside a distribution of measurements of their\nstatistical power (e.g. $z$-scores and $p$-values). We present results and\ninsights from building this pipeline at scale for two large-scale short-video\nplatforms: ShareChat and Moj; leveraging hundreds of past experiments to find\nonline metrics with high statistical power.\n","subjects":["Computing Research Repository/Information Retrieval","Statistics/Applications"],"license":"http://creativecommons.org/licenses/by/4.0/"}