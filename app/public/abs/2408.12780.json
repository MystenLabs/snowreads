{"id":"2408.12780","title":"Quality or Quantity? On Data Scale and Diversity in Adapting Large\n  Language Models for Low-Resource Translation","authors":"Vivek Iyer, Bhavitvya Malik, Pavel Stepachev, Pinzhen Chen, Barry\n  Haddow, and Alexandra Birch","authorsParsed":[["Iyer","Vivek",""],["Malik","Bhavitvya",""],["Stepachev","Pavel",""],["Chen","Pinzhen",""],["Haddow","Barry",""],["Birch","Alexandra",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 00:59:38 GMT"}],"updateDate":"2024-08-26","timestamp":1724374778000,"abstract":"  Despite the recent popularity of Large Language Models (LLMs) in Machine\nTranslation (MT), their performance in low-resource translation still lags\nsignificantly behind Neural Machine Translation (NMT) models. In this paper, we\nexplore what it would take to adapt LLMs for low-resource settings. In\nparticular, we re-examine the role of two factors: a) the importance and\napplication of parallel data, and b) diversity in Supervised Fine-Tuning (SFT).\nRecently, parallel data has been shown to be less important for MT using LLMs\nthan in previous MT research. Similarly, diversity during SFT has been shown to\npromote significant transfer in LLMs across languages and tasks. However, for\nlow-resource LLM-MT, we show that the opposite is true for both of these\nconsiderations: a) parallel data is critical during both pretraining and SFT,\nand b) diversity tends to cause interference, not transfer. Our experiments,\nconducted with 3 LLMs across 2 low-resourced language groups - indigenous\nAmerican and North-East Indian - reveal consistent patterns in both cases,\nunderscoring the generalizability of our findings. We believe these insights\nwill be valuable for scaling to massively multilingual LLM-MT models that can\neffectively serve lower-resource languages.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}