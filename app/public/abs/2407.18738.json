{"id":"2407.18738","title":"Towards Generalized Offensive Language Identification","authors":"Alphaeus Dmonte, Tejas Arya, Tharindu Ranasinghe, Marcos Zampieri","authorsParsed":[["Dmonte","Alphaeus",""],["Arya","Tejas",""],["Ranasinghe","Tharindu",""],["Zampieri","Marcos",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 13:50:22 GMT"}],"updateDate":"2024-07-29","timestamp":1722001822000,"abstract":"  The prevalence of offensive content on the internet, encompassing hate speech\nand cyberbullying, is a pervasive issue worldwide. Consequently, it has\ngarnered significant attention from the machine learning (ML) and natural\nlanguage processing (NLP) communities. As a result, numerous systems have been\ndeveloped to automatically identify potentially harmful content and mitigate\nits impact. These systems can follow two approaches; (1) Use publicly available\nmodels and application endpoints, including prompting large language models\n(LLMs) (2) Annotate datasets and train ML models on them. However, both\napproaches lack an understanding of how generalizable they are. Furthermore,\nthe applicability of these systems is often questioned in off-domain and\npractical environments. This paper empirically evaluates the generalizability\nof offensive language detection models and datasets across a novel generalized\nbenchmark. We answer three research questions on generalizability. Our findings\nwill be useful in creating robust real-world offensive language detection\nsystems.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}