{"id":"2407.14146","title":"Fine-grained Knowledge Graph-driven Video-Language Learning for Action\n  Recognition","authors":"Rui Zhang, Yafen Lu, Pengli Ji, Junxiao Xue, Xiaoran Yan","authorsParsed":[["Zhang","Rui",""],["Lu","Yafen",""],["Ji","Pengli",""],["Xue","Junxiao",""],["Yan","Xiaoran",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 09:24:43 GMT"}],"updateDate":"2024-07-22","timestamp":1721381083000,"abstract":"  Recent work has explored video action recognition as a video-text matching\nproblem and several effective methods have been proposed based on large-scale\npre-trained vision-language models. However, these approaches primarily operate\nat a coarse-grained level without the detailed and semantic understanding of\naction concepts by exploiting fine-grained semantic connections between actions\nand body movements. To address this gap, we propose a contrastive\nvideo-language learning framework guided by a knowledge graph, termed KG-CLIP,\nwhich incorporates structured information into the CLIP model in the video\ndomain. Specifically, we construct a multi-modal knowledge graph composed of\nmulti-grained concepts by parsing actions based on compositional learning. By\nimplementing a triplet encoder and deviation compensation to adaptively\noptimize the margin in the entity distance function, our model aims to improve\nalignment of entities in the knowledge graph to better suit complex\nrelationship learning. This allows for enhanced video action recognition\ncapabilities by accommodating nuanced associations between graph components. We\ncomprehensively evaluate KG-CLIP on Kinetics-TPS, a large-scale action parsing\ndataset, demonstrating its effectiveness compared to competitive baselines.\nEspecially, our method excels at action recognition with few sample frames or\nlimited training data, which exhibits excellent data utilization and learning\ncapabilities.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}