{"id":"2407.06946","title":"Self-Recognition in Language Models","authors":"Tim R. Davidson, Viacheslav Surkov, Veniamin Veselovsky, Giuseppe\n  Russo, Robert West, Caglar Gulcehre","authorsParsed":[["Davidson","Tim R.",""],["Surkov","Viacheslav",""],["Veselovsky","Veniamin",""],["Russo","Giuseppe",""],["West","Robert",""],["Gulcehre","Caglar",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 15:23:28 GMT"}],"updateDate":"2024-07-10","timestamp":1720538608000,"abstract":"  A rapidly growing number of applications rely on a small set of closed-source\nlanguage models (LMs). This dependency might introduce novel security risks if\nLMs develop self-recognition capabilities. Inspired by human identity\nverification methods, we propose a novel approach for assessing\nself-recognition in LMs using model-generated \"security questions\". Our test\ncan be externally administered to keep track of frontier models as it does not\nrequire access to internal model parameters or output probabilities. We use our\ntest to examine self-recognition in ten of the most capable open- and\nclosed-source LMs currently publicly available. Our extensive experiments found\nno empirical evidence of general or consistent self-recognition in any examined\nLM. Instead, our results suggest that given a set of alternatives, LMs seek to\npick the \"best\" answer, regardless of its origin. Moreover, we find indications\nthat preferences about which models produce the best answers are consistent\nacross LMs. We additionally uncover novel insights on position bias\nconsiderations for LMs in multiple-choice settings.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}