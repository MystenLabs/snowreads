{"id":"2407.14073","title":"LoAS: Fully Temporal-Parallel Dataflow for Dual-Sparse Spiking Neural\n  Networks","authors":"Ruokai Yin, Youngeun Kim, Di Wu, Priyadarshini Panda","authorsParsed":[["Yin","Ruokai",""],["Kim","Youngeun",""],["Wu","Di",""],["Panda","Priyadarshini",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 07:02:26 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 22:25:35 GMT"},{"version":"v3","created":"Sun, 1 Sep 2024 13:26:58 GMT"}],"updateDate":"2024-09-04","timestamp":1721372546000,"abstract":"  Spiking Neural Networks (SNNs) have gained significant research attention in\nthe last decade due to their potential to drive resource-constrained edge\ndevices. Though existing SNN accelerators offer high efficiency in processing\nsparse spikes with dense weights, opportunities are less explored in SNNs with\nsparse weights, i.e., dual-sparsity. In this work, we study the acceleration of\ndual-sparse SNNs, focusing on their core operation, sparse-matrix-sparse-matrix\nmultiplication (spMspM). We observe that naively running a dual-sparse SNN on\nexisting spMspM accelerators designed for dual-sparse Artificial Neural\nNetworks (ANNs) exhibits sub-optimal efficiency. The main challenge is that\nprocessing timesteps, a natural property of SNNs, introduces an extra loop to\nANN spMspM, leading to longer latency and more memory traffic. To address the\nproblem, we propose a fully temporal-parallel (FTP) dataflow, which minimizes\nboth data movement across timesteps and the end-to-end latency of dual-sparse\nSNNs. To maximize the efficiency of FTP dataflow, we propose an FTP-friendly\nspike compression mechanism that efficiently compresses single-bit spikes and\nensures contiguous memory access. We further propose an FTP-friendly inner-join\ncircuit that can lower the cost of the expensive prefix-sum circuits with\nalmost no throughput penalty. All the above techniques for FTP dataflow are\nencapsulated in LoAS, a Low-latency inference Accelerator for dual-sparse SNNs.\nWith FTP dataflow, compression, and inner-join, running dual-sparse SNN\nworkloads on LoAS demonstrates significant speedup (up to $8.51\\times$) and\nenergy reduction (up to $3.68\\times$) compared to running it on prior\ndual-sparse accelerators.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}