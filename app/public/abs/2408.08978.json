{"id":"2408.08978","title":"See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering\n  LLM Weaknesses","authors":"Yulong Chen, Yang Liu, Jianhao Yan, Xuefeng Bai, Ming Zhong, Yinghao\n  Yang, Ziyi Yang, Chenguang Zhu, Yue Zhang","authorsParsed":[["Chen","Yulong",""],["Liu","Yang",""],["Yan","Jianhao",""],["Bai","Xuefeng",""],["Zhong","Ming",""],["Yang","Yinghao",""],["Yang","Ziyi",""],["Zhu","Chenguang",""],["Zhang","Yue",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 19:01:52 GMT"}],"updateDate":"2024-08-20","timestamp":1723834912000,"abstract":"  The impressive performance of Large Language Models (LLMs) has consistently\nsurpassed numerous human-designed benchmarks, presenting new challenges in\nassessing the shortcomings of LLMs. Designing tasks and finding LLMs'\nlimitations are becoming increasingly important. In this paper, we investigate\nthe question of whether an LLM can discover its own limitations from the errors\nit makes. To this end, we propose a Self-Challenge evaluation framework with\nhuman-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we\nprompt GPT-4 to summarize error patterns that can be used to generate new\ninstances and incorporate human feedback on them to refine these patterns for\ngenerating more challenging data, iteratively. We end up with 8 diverse\npatterns, such as text manipulation and questions with assumptions. We then\nbuild a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4\nusing these patterns, with human-annotated gold responses. The SC-G4 serves as\na challenging benchmark that allows for a detailed assessment of LLMs'\nabilities. Our results show that only 44.96\\% of instances in SC-G4 can be\nanswered correctly by GPT-4. Interestingly, our pilot study indicates that\nthese error patterns also challenge other LLMs, such as Claude-3 and Llama-3,\nand cannot be fully resolved through fine-tuning. Our work takes the first step\nto demonstrate that LLMs can autonomously identify their inherent flaws and\nprovide insights for future dynamic and automatic evaluation.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}