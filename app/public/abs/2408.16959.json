{"id":"2408.16959","title":"HiTSR: A Hierarchical Transformer for Reference-based Super-Resolution","authors":"Masoomeh Aslahishahri, Jordan Ubbens, Ian Stavness","authorsParsed":[["Aslahishahri","Masoomeh",""],["Ubbens","Jordan",""],["Stavness","Ian",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 01:16:29 GMT"}],"updateDate":"2024-09-02","timestamp":1724980589000,"abstract":"  In this paper, we propose HiTSR, a hierarchical transformer model for\nreference-based image super-resolution, which enhances low-resolution input\nimages by learning matching correspondences from high-resolution reference\nimages. Diverging from existing multi-network, multi-stage approaches, we\nstreamline the architecture and training pipeline by incorporating the double\nattention block from GAN literature. Processing two visual streams\nindependently, we fuse self-attention and cross-attention blocks through a\ngating attention strategy. The model integrates a squeeze-and-excitation module\nto capture global context from the input images, facilitating long-range\nspatial interactions within window-based attention blocks. Long skip\nconnections between shallow and deep layers further enhance information flow.\nOur model demonstrates superior performance across three datasets including\nSUN80, Urban100, and Manga109. Specifically, on the SUN80 dataset, our model\nachieves PSNR/SSIM values of 30.24/0.821. These results underscore the\neffectiveness of attention mechanisms in reference-based image\nsuper-resolution. The transformer-based model attains state-of-the-art results\nwithout the need for purpose-built subnetworks, knowledge distillation, or\nmulti-stage training, emphasizing the potency of attention in meeting\nreference-based image super-resolution requirements.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}