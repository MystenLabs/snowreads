{"id":"2408.00612","title":"Downstream bias mitigation is all you need","authors":"Arkadeep Baksi, Rahul Singh, Tarun Joshi","authorsParsed":[["Baksi","Arkadeep",""],["Singh","Rahul",""],["Joshi","Tarun",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 14:52:04 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 14:59:31 GMT"}],"updateDate":"2024-08-29","timestamp":1722523924000,"abstract":"  The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}