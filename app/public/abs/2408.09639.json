{"id":"2408.09639","title":"How to Make the Most of LLMs' Grammatical Knowledge for Acceptability\n  Judgments","authors":"Yusuke Ide, Yuto Nishida, Miyu Oba, Yusuke Sakai, Justin Vasselli,\n  Hidetaka Kamigaito, Taro Watanabe","authorsParsed":[["Ide","Yusuke",""],["Nishida","Yuto",""],["Oba","Miyu",""],["Sakai","Yusuke",""],["Vasselli","Justin",""],["Kamigaito","Hidetaka",""],["Watanabe","Taro",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 01:53:47 GMT"}],"updateDate":"2024-08-20","timestamp":1724032427000,"abstract":"  The grammatical knowledge of language models (LMs) is often measured using a\nbenchmark of linguistic minimal pairs, where LMs are presented with a pair of\nacceptable and unacceptable sentences and required to judge which is\nacceptable. The existing dominant approach, however, naively calculates and\ncompares the probabilities of paired sentences using LMs. Additionally, large\nlanguage models (LLMs) have yet to be thoroughly examined in this field. We\nthus investigate how to make the most of LLMs' grammatical knowledge to\ncomprehensively evaluate it. Through extensive experiments of nine judgment\nmethods in English and Chinese, we demonstrate that a probability readout\nmethod, in-template LP, and a prompting-based method, Yes/No probability\ncomputing, achieve particularly high performance, surpassing the conventional\napproach. Our analysis reveals their different strengths, e.g., Yes/No\nprobability computing is robust against token-length bias, suggesting that they\nharness different aspects of LLMs' grammatical knowledge. Consequently, we\nrecommend using diverse judgment methods to evaluate LLMs comprehensively.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"40ggprzz7aUeT5w42OGhdZ5oCshh8wb0XdQWj2eG5dU","pdfSize":"955180"}
