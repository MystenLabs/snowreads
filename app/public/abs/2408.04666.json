{"id":"2408.04666","title":"LLMs are Not Just Next Token Predictors","authors":"Stephen M. Downes, Patrick Forber, and Alex Grzankowski","authorsParsed":[["Downes","Stephen M.",""],["Forber","Patrick",""],["Grzankowski","Alex",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 16:36:28 GMT"}],"updateDate":"2024-08-12","timestamp":1722962188000,"abstract":"  LLMs are statistical models of language learning through stochastic gradient\ndescent with a next token prediction objective. Prompting a popular view among\nAI modelers: LLMs are just next token predictors. While LLMs are engineered\nusing next token prediction, and trained based on their success at this task,\nour view is that a reduction to just next token predictor sells LLMs short.\nMoreover, there are important explanations of LLM behavior and capabilities\nthat are lost when we engage in this kind of reduction. In order to draw this\nout, we will make an analogy with a once prominent research program in biology\nexplaining evolution and development from the gene's eye view.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}