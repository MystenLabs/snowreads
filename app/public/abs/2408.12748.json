{"id":"2408.12748","title":"SLM Meets LLM: Balancing Latency, Interpretability and Consistency in\n  Hallucination Detection","authors":"Mengya Hu and Rui Xu and Deren Lei and Yaxi Li and Mingyu Wang and\n  Emily Ching and Eslam Kamal and Alex Deng","authorsParsed":[["Hu","Mengya",""],["Xu","Rui",""],["Lei","Deren",""],["Li","Yaxi",""],["Wang","Mingyu",""],["Ching","Emily",""],["Kamal","Eslam",""],["Deng","Alex",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 22:13:13 GMT"}],"updateDate":"2024-08-26","timestamp":1724364793000,"abstract":"  Large language models (LLMs) are highly capable but face latency challenges\nin real-time applications, such as conducting online hallucination detection.\nTo overcome this issue, we propose a novel framework that leverages a small\nlanguage model (SLM) classifier for initial detection, followed by a LLM as\nconstrained reasoner to generate detailed explanations for detected\nhallucinated content. This study optimizes the real-time interpretable\nhallucination detection by introducing effective prompting techniques that\nalign LLM-generated explanations with SLM decisions. Empirical experiment\nresults demonstrate its effectiveness, thereby enhancing the overall user\nexperience.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"pX2Vom6TqCrXCtJxmZWyGSAxW4NBo3EytV-jGRr2Yx8","pdfSize":"1075848"}
