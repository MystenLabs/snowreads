{"id":"2408.04883","title":"ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary\n  Segmentation","authors":"Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng,\n  Wayne Zhang","authorsParsed":[["Lan","Mengcheng",""],["Chen","Chaofeng",""],["Ke","Yiping",""],["Wang","Xinjiang",""],["Feng","Litong",""],["Zhang","Wayne",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 06:17:00 GMT"}],"updateDate":"2024-08-12","timestamp":1723184220000,"abstract":"  Open-vocabulary semantic segmentation requires models to effectively\nintegrate visual representations with open-vocabulary semantic labels. While\nContrastive Language-Image Pre-training (CLIP) models shine in recognizing\nvisual concepts from text, they often struggle with segment coherence due to\ntheir limited localization ability. In contrast, Vision Foundation Models\n(VFMs) excel at acquiring spatially consistent local visual representations,\nyet they fall short in semantic understanding. This paper introduces ProxyCLIP,\nan innovative framework designed to harmonize the strengths of both CLIP and\nVFMs, facilitating enhanced open-vocabulary semantic segmentation. ProxyCLIP\nleverages the spatial feature correspondence from VFMs as a form of proxy\nattention to augment CLIP, thereby inheriting the VFMs' robust local\nconsistency and maintaining CLIP's exceptional zero-shot transfer capacity. We\npropose an adaptive normalization and masking strategy to get the proxy\nattention from VFMs, allowing for adaptation across different VFMs. Remarkably,\nas a training-free approach, ProxyCLIP significantly improves the average mean\nIntersection over Union (mIoU) across eight benchmarks from 40.3 to 44.4,\nshowcasing its exceptional efficacy in bridging the gap between spatial\nprecision and semantic richness for the open-vocabulary segmentation task.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}