{"id":"2407.04842","title":"MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for\n  Text-to-Image Generation?","authors":"Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui,\n  Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, Canyu\n  Chen, Qinghao Ye, Zhihong Zhu, Yuqing Zhang, Jiawei Zhou, Zhuokai Zhao,\n  Rafael Rafailov, Chelsea Finn, Huaxiu Yao","authorsParsed":[["Chen","Zhaorun",""],["Du","Yichao",""],["Wen","Zichen",""],["Zhou","Yiyang",""],["Cui","Chenhang",""],["Weng","Zhenzhen",""],["Tu","Haoqin",""],["Wang","Chaoqi",""],["Tong","Zhengwei",""],["Huang","Qinglan",""],["Chen","Canyu",""],["Ye","Qinghao",""],["Zhu","Zhihong",""],["Zhang","Yuqing",""],["Zhou","Jiawei",""],["Zhao","Zhuokai",""],["Rafailov","Rafael",""],["Finn","Chelsea",""],["Yao","Huaxiu",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 20:03:16 GMT"}],"updateDate":"2024-07-09","timestamp":1720209796000,"abstract":"  While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}