{"id":"2408.11318","title":"TWLV-I: Analysis and Insights from Holistic Evaluation on Video\n  Foundation Models","authors":"Hyeongmin Lee, Jin-Young Kim, Kyungjune Baek, Jihwan Kim, Hyojun Go,\n  Seongsu Ha, Seokjin Han, Jiho Jang, Raehyuk Jung, Daewoo Kim, GeunOh Kim,\n  JongMok Kim, Jongseok Kim, Junwan Kim, Soonwoo Kwon, Jangwon Lee, Seungjoon\n  Park, Minjoon Seo, Jay Suh, Jaehyuk Yi, Aiden Lee","authorsParsed":[["Lee","Hyeongmin",""],["Kim","Jin-Young",""],["Baek","Kyungjune",""],["Kim","Jihwan",""],["Go","Hyojun",""],["Ha","Seongsu",""],["Han","Seokjin",""],["Jang","Jiho",""],["Jung","Raehyuk",""],["Kim","Daewoo",""],["Kim","GeunOh",""],["Kim","JongMok",""],["Kim","Jongseok",""],["Kim","Junwan",""],["Kwon","Soonwoo",""],["Lee","Jangwon",""],["Park","Seungjoon",""],["Seo","Minjoon",""],["Suh","Jay",""],["Yi","Jaehyuk",""],["Lee","Aiden",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 03:56:27 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 03:33:17 GMT"}],"updateDate":"2024-08-26","timestamp":1724212587000,"abstract":"  In this work, we discuss evaluating video foundation models in a fair and\nrobust manner. Unlike language or image foundation models, many video\nfoundation models are evaluated with differing parameters (such as sampling\nrate, number of frames, pretraining steps, etc.), making fair and robust\ncomparisons challenging. Therefore, we present a carefully designed evaluation\nframework for measuring two core capabilities of video comprehension:\nappearance and motion understanding. Our findings reveal that existing video\nfoundation models, whether text-supervised like UMT or InternVideo2, or\nself-supervised like V-JEPA, exhibit limitations in at least one of these\ncapabilities. As an alternative, we introduce TWLV-I, a new video foundation\nmodel that constructs robust visual representations for both motion- and\nappearance-based videos. Based on the average top-1 accuracy of linear probing\non five action recognition benchmarks, pretrained only on publicly accessible\ndatasets, our model shows a 4.6%p improvement compared to V-JEPA (ViT-L) and a\n7.7%p improvement compared to UMT (ViT-L). Even when compared to much larger\nmodels, our model demonstrates a 7.2%p improvement compared to DFN (ViT-H), a\n2.7%p improvement compared to V-JEPA (ViT-H) and a 2.8%p improvement compared\nto InternVideo2 (ViT-g). We provide embedding vectors obtained by TWLV-I from\nvideos of several commonly used video benchmarks, along with evaluation source\ncode that can directly utilize these embeddings. The code is available at\nhttps://github.com/twelvelabs-io/video-embeddings-evaluation-framework.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"w6pWr46l-lQUl7pSeDsHOJYFCGTnnjqOp1BG-gjd3_Y","pdfSize":"6011607"}
