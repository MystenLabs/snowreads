{"id":"2408.13008","title":"Focused Discriminative Training For Streaming CTC-Trained Automatic\n  Speech Recognition Models","authors":"Adnan Haider, Xingyu Na, Erik McDermott, Tim Ng, Zhen Huang and\n  Xiaodan Zhuang","authorsParsed":[["Haider","Adnan",""],["Na","Xingyu",""],["McDermott","Erik",""],["Ng","Tim",""],["Huang","Zhen",""],["Zhuang","Xiaodan",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 11:54:25 GMT"}],"updateDate":"2024-08-26","timestamp":1724414065000,"abstract":"  This paper introduces a novel training framework called Focused\nDiscriminative Training (FDT) to further improve streaming word-piece\nend-to-end (E2E) automatic speech recognition (ASR) models trained using either\nCTC or an interpolation of CTC and attention-based encoder-decoder (AED) loss.\nThe proposed approach presents a novel framework to identify and improve a\nmodel's recognition on challenging segments of an audio. Notably, this training\nframework is independent of hidden Markov models (HMMs) and lattices,\neliminating the need for substantial decision-making regarding HMM topology,\nlexicon, and graph generation, as typically required in standard discriminative\ntraining approaches. Compared to additional fine-tuning with MMI or MWER loss\non the encoder, FDT is shown to be more effective in achieving greater\nreductions in Word Error Rate (WER) on streaming models trained on LibriSpeech.\nAdditionally, this method is shown to be effective in further improving a\nconverged word-piece streaming E2E model trained on 600k hours of assistant and\ndictation dataset.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"p9ASSTXKd4fv3mrJrmVRB36z_VtwlB2CCwnlYMVnzJQ","pdfSize":"370113"}
