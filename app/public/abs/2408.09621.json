{"id":"2408.09621","title":"Refining Packing and Shuffling Strategies for Enhanced Performance in\n  Generative Language Models","authors":"Yanbing Chen, Ruilin Wang, Zihao Yang, Lavender Yao Jiang, Eric Karl\n  Oermann","authorsParsed":[["Chen","Yanbing",""],["Wang","Ruilin",""],["Yang","Zihao",""],["Jiang","Lavender Yao",""],["Oermann","Eric Karl",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 00:26:53 GMT"}],"updateDate":"2024-08-20","timestamp":1724027213000,"abstract":"  Packing and shuffling tokens is a common practice in training auto-regressive\nlanguage models (LMs) to prevent overfitting and improve efficiency. Typically\ndocuments are concatenated to chunks of maximum sequence length (MSL) and then\nshuffled. However setting the atom size, the length for each data chunk\naccompanied by random shuffling, to MSL may lead to contextual incoherence due\nto tokens from different documents being packed into the same chunk. An\nalternative approach is to utilize padding, another common data packing\nstrategy, to avoid contextual incoherence by only including one document in\neach shuffled chunk. To optimize both packing strategies (concatenation vs\npadding), we investigated the optimal atom size for shuffling and compared\ntheir performance and efficiency. We found that matching atom size to MSL\noptimizes performance for both packing methods (concatenation and padding), and\npadding yields lower final perplexity (higher performance) than concatenation\nat the cost of more training steps and lower compute efficiency. This trade-off\ninforms the choice of packing methods in training language models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}