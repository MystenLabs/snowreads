{"id":"2407.05890","title":"Affordances-Oriented Planning using Foundation Models for Continuous\n  Vision-Language Navigation","authors":"Jiaqi Chen, Bingqian Lin, Xinmin Liu, Lin Ma, Xiaodan Liang, Kwan-Yee\n  K. Wong","authorsParsed":[["Chen","Jiaqi",""],["Lin","Bingqian",""],["Liu","Xinmin",""],["Ma","Lin",""],["Liang","Xiaodan",""],["Wong","Kwan-Yee K.",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 12:52:46 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 14:51:04 GMT"}],"updateDate":"2024-08-21","timestamp":1720443166000,"abstract":"  LLM-based agents have demonstrated impressive zero-shot performance in\nvision-language navigation (VLN) task. However, existing LLM-based methods\noften focus only on solving high-level task planning by selecting nodes in\npredefined navigation graphs for movements, overlooking low-level control in\nnavigation scenarios. To bridge this gap, we propose AO-Planner, a novel\nAffordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates\nvarious foundation models to achieve affordances-oriented low-level motion\nplanning and high-level decision-making, both performed in a zero-shot setting.\nSpecifically, we employ a Visual Affordances Prompting (VAP) approach, where\nthe visible ground is segmented by SAM to provide navigational affordances,\nbased on which the LLM selects potential candidate waypoints and plans\nlow-level paths towards selected waypoints. We further propose a high-level\nPathAgent which marks planned paths into the image input and reasons the most\nprobable path by comprehending all environmental information. Finally, we\nconvert the selected path into 3D coordinates using camera intrinsic parameters\nand depth information, avoiding challenging 3D predictions for LLMs.\nExperiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner\nachieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our\nmethod can also serve as a data annotator to obtain pseudo-labels, distilling\nits waypoint prediction ability into a learning-based predictor. This new\npredictor does not require any waypoint data from the simulator and achieves\n47% SR competing with supervised methods. We establish an effective connection\nbetween LLM and 3D world, presenting novel prospects for employing foundation\nmodels in low-level motion control.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}