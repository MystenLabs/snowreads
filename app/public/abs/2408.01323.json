{"id":"2408.01323","title":"FANNO: Augmenting High-Quality Instruction Data with Open-Sourced LLMs\n  Only","authors":"He Zhu, Junyou Su, Tianle Lun, Yicheng Tao, Wenjia Zhang, Zipei Fan,\n  Guanhua Chen","authorsParsed":[["Zhu","He",""],["Su","Junyou",""],["Lun","Tianle",""],["Tao","Yicheng",""],["Zhang","Wenjia",""],["Fan","Zipei",""],["Chen","Guanhua",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 15:21:20 GMT"}],"updateDate":"2024-08-05","timestamp":1722612080000,"abstract":"  Instruction fine-tuning stands as a crucial advancement in leveraging large\nlanguage models (LLMs) for enhanced task performance. However, the annotation\nof instruction datasets has traditionally been expensive and laborious, often\nrelying on manual annotations or costly API calls of proprietary LLMs. To\naddress these challenges, we introduce FANNO, a fully autonomous, open-sourced\nframework that revolutionizes the annotation process without the need for\npre-existing annotated data. Utilizing a Mistral-7b-instruct model, FANNO\nefficiently produces diverse and high-quality datasets through a structured\nprocess involving document pre-screening, instruction generation, and response\ngeneration. Experiments on Open LLM Leaderboard and AlpacaEval benchmark show\nthat the FANNO can generate high-quality data with diversity and complexity for\nfree, comparable to human-annotated or cleaned datasets like\nAlpaca-GPT4-Cleaned.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}