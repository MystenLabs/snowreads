{"id":"2407.08683","title":"SEED-Story: Multimodal Long Story Generation with Large Language Model","authors":"Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan,\n  Yingcong Chen","authorsParsed":[["Yang","Shuai",""],["Ge","Yuying",""],["Li","Yang",""],["Chen","Yukang",""],["Ge","Yixiao",""],["Shan","Ying",""],["Chen","Yingcong",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:21:03 GMT"}],"updateDate":"2024-07-12","timestamp":1720718463000,"abstract":"  With the remarkable advancements in image generation and open-form text\ngeneration, the creation of interleaved image-text content has become an\nincreasingly intriguing field. Multimodal story generation, characterized by\nproducing narrative texts and vivid images in an interleaved manner, has\nemerged as a valuable and practical task with broad applications. However, this\ntask poses significant challenges, as it necessitates the comprehension of the\ncomplex interplay between texts and images, and the ability to generate long\nsequences of coherent, contextually relevant texts and visuals. In this work,\nwe propose SEED-Story, a novel method that leverages a Multimodal Large\nLanguage Model (MLLM) to generate extended multimodal stories. Our model, built\nupon the powerful comprehension capability of MLLM, predicts text tokens as\nwell as visual tokens, which are subsequently processed with an adapted visual\nde-tokenizer to produce images with consistent characters and styles. We\nfurther propose multimodal attention sink mechanism to enable the generation of\nstories with up to 25 sequences (only 10 for training) in a highly efficient\nautoregressive manner. Additionally, we present a large-scale and\nhigh-resolution dataset named StoryStream for training our model and\nquantitatively evaluating the task of multimodal story generation in various\naspects.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}