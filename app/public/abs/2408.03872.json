{"id":"2408.03872","title":"Inter-Series Transformer: Attending to Products in Time Series\n  Forecasting","authors":"Rares Cristian, Pavithra Harsha, Clemente Ocejo, Georgia Perakis,\n  Brian Quanz, Ioannis Spantidakis, Hamza Zerhouni","authorsParsed":[["Cristian","Rares",""],["Harsha","Pavithra",""],["Ocejo","Clemente",""],["Perakis","Georgia",""],["Quanz","Brian",""],["Spantidakis","Ioannis",""],["Zerhouni","Hamza",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 16:22:21 GMT"}],"updateDate":"2024-08-08","timestamp":1723047741000,"abstract":"  Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}