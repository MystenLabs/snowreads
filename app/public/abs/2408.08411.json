{"id":"2408.08411","title":"Rater Cohesion and Quality from a Vicarious Perspective","authors":"Deepak Pandita, Tharindu Cyril Weerasooriya, Sujan Dutta, Sarah K.\n  Luger, Tharindu Ranasinghe, Ashiqur R. KhudaBukhsh, Marcos Zampieri,\n  Christopher M. Homan","authorsParsed":[["Pandita","Deepak",""],["Weerasooriya","Tharindu Cyril",""],["Dutta","Sujan",""],["Luger","Sarah K.",""],["Ranasinghe","Tharindu",""],["KhudaBukhsh","Ashiqur R.",""],["Zampieri","Marcos",""],["Homan","Christopher M.",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 20:37:36 GMT"}],"updateDate":"2024-08-19","timestamp":1723754256000,"abstract":"  Human feedback is essential for building human-centered AI systems across\ndomains where disagreement is prevalent, such as AI safety, content moderation,\nor sentiment analysis. Many disagreements, particularly in politically charged\nsettings, arise because raters have opposing values or beliefs. Vicarious\nannotation is a method for breaking down disagreement by asking raters how they\nthink others would annotate the data. In this paper, we explore the use of\nvicarious annotation with analytical methods for moderating rater disagreement.\nWe employ rater cohesion metrics to study the potential influence of political\naffiliations and demographic backgrounds on raters' perceptions of offense.\nAdditionally, we utilize CrowdTruth's rater quality metrics, which consider the\ndemographics of the raters, to score the raters and their annotations. We study\nhow the rater quality metrics influence the in-group and cross-group rater\ncohesion across the personal and vicarious levels.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}