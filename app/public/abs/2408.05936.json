{"id":"2408.05936","title":"Multi-scale Contrastive Adaptor Learning for Segmenting Anything in\n  Underperformed Scenes","authors":"Ke Zhou, Zhongwei Qiu, Dongmei Fu","authorsParsed":[["Zhou","Ke",""],["Qiu","Zhongwei",""],["Fu","Dongmei",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 06:23:10 GMT"}],"updateDate":"2024-08-13","timestamp":1723443790000,"abstract":"  Foundational vision models, such as the Segment Anything Model (SAM), have\nachieved significant breakthroughs through extensive pre-training on\nlarge-scale visual datasets. Despite their general success, these models may\nfall short in specialized tasks with limited data, and fine-tuning such\nlarge-scale models is often not feasible. Current strategies involve\nincorporating adaptors into the pre-trained SAM to facilitate downstream task\nperformance with minimal model adjustment. However, these strategies can be\nhampered by suboptimal learning approaches for the adaptors. In this paper, we\nintroduce a novel Multi-scale Contrastive Adaptor learning method named\nMCA-SAM, which enhances adaptor performance through a meticulously designed\ncontrastive learning framework at both token and sample levels. Our Token-level\nContrastive adaptor (TC-adaptor) focuses on refining local representations by\nimproving the discriminability of patch tokens, while the Sample-level\nContrastive adaptor (SC-adaptor) amplifies global understanding across\ndifferent samples. Together, these adaptors synergistically enhance feature\ncomparison within and across samples, bolstering the model's representational\nstrength and its ability to adapt to new tasks. Empirical results demonstrate\nthat MCA-SAM sets new benchmarks, outperforming existing methods in three\nchallenging domains: camouflage object detection, shadow segmentation, and\npolyp segmentation. Specifically, MCA-SAM exhibits substantial relative\nperformance enhancements, achieving a 20.0% improvement in MAE on the COD10K\ndataset, a 6.0% improvement in MAE on the CAMO dataset, a 15.4% improvement in\nBER on the ISTD dataset, and a 7.9% improvement in mDice on the Kvasir-SEG\ndataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}