{"id":"2408.16370","title":"Efficient Multi-agent Navigation with Lightweight DRL Policy","authors":"Xingrong Diao and Jiankun Wang","authorsParsed":[["Diao","Xingrong",""],["Wang","Jiankun",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 09:28:05 GMT"},{"version":"v2","created":"Wed, 4 Sep 2024 02:39:53 GMT"}],"updateDate":"2024-09-05","timestamp":1724923685000,"abstract":"  In this article, we present an end-to-end collision avoidance policy based on\ndeep reinforcement learning (DRL) for multi-agent systems, demonstrating\nencouraging outcomes in real-world applications. In particular, our policy\ncalculates the control commands of the agent based on the raw LiDAR\nobservation. In addition, the number of parameters of the proposed basic model\nis 140,000, and the size of the parameter file is 3.5 MB, which allows the\nrobot to calculate the actions from the CPU alone. We propose a multi-agent\ntraining platform based on a physics-based simulator to further bridge the gap\nbetween simulation and the real world. The policy is trained on a\npolicy-gradients-based RL algorithm in a dense and messy training environment.\nA novel reward function is introduced to address the issue of agents choosing\nsuboptimal actions in some common scenarios. Although the data used for\ntraining is exclusively from the simulation platform, the policy can be\nsuccessfully transferred and deployed in real-world robots. Finally, our policy\neffectively responds to intentional obstructions and avoids collisions. The\nwebsite is available at\n\\url{https://sites.google.com/view/xingrong2024efficient/%E9%A6%96%E9%A1%B5}.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}