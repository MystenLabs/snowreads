{"id":"2407.19860","title":"Anomalous State Sequence Modeling to Enhance Safety in Reinforcement\n  Learning","authors":"Leen Kweider, Maissa Abou Kassem, Ubai Sandouk","authorsParsed":[["Kweider","Leen",""],["Kassem","Maissa Abou",""],["Sandouk","Ubai",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 10:30:07 GMT"}],"updateDate":"2024-07-30","timestamp":1722249007000,"abstract":"  The deployment of artificial intelligence (AI) in decision-making\napplications requires ensuring an appropriate level of safety and reliability,\nparticularly in changing environments that contain a large number of unknown\nobservations. To address this challenge, we propose a novel safe reinforcement\nlearning (RL) approach that utilizes an anomalous state sequence to enhance RL\nsafety. Our proposed solution Safe Reinforcement Learning with Anomalous State\nSequences (AnoSeqs) consists of two stages. First, we train an agent in a\nnon-safety-critical offline 'source' environment to collect safe state\nsequences. Next, we use these safe sequences to build an anomaly detection\nmodel that can detect potentially unsafe state sequences in a 'target'\nsafety-critical environment where failures can have high costs. The estimated\nrisk from the anomaly detection model is utilized to train a risk-averse RL\npolicy in the target environment; this involves adjusting the reward function\nto penalize the agent for visiting anomalous states deemed unsafe by our\nanomaly model. In experiments on multiple safety-critical benchmarking\nenvironments including self-driving cars, our solution approach successfully\nlearns safer policies and proves that sequential anomaly detection can provide\nan effective supervisory signal for training safety-aware RL agents\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"HMbREYGvFH0G1OFBGRGJ4SfgYFwHn3mo4G16bxwxEZo","pdfSize":"1426198"}
