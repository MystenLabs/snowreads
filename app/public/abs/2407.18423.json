{"id":"2407.18423","title":"HDL-GPT: High-Quality HDL is All You Need","authors":"Bhuvnesh Kumar, Saurav Nanda, Ganapathy Parthasarathy, Pawan Patil,\n  Austin Tsai, Parivesh Choudhary","authorsParsed":[["Kumar","Bhuvnesh",""],["Nanda","Saurav",""],["Parthasarathy","Ganapathy",""],["Patil","Pawan",""],["Tsai","Austin",""],["Choudhary","Parivesh",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 22:48:08 GMT"}],"updateDate":"2024-07-29","timestamp":1721947688000,"abstract":"  This paper presents Hardware Description Language Generative Pre-trained\nTransformers (HDL-GPT), a novel approach that leverages the vast repository of\nopen-source High Definition Language (HDL) codes to train superior quality\nlarge code models. The core premise of this paper is the hypothesis that\nhigh-quality HDL is all you need to create models with exceptional performance\nand broad zero-shot generalization abilities. The paper elucidates the methods\nemployed for the curation and augmentation of large corpora from open-source\nHDL code, transforming highly variable quality data into high-quality data\nthrough careful prompting and context maintenance. We demonstrate that the\ncareful selection, filtering, and augmentation of data across HDLs can yield\npowerful models that surpass current state-of-the-art models. We also explore\nthe impact of different fine-tuning methods on the quality of results. We\ndescribe experimental results across a range of fine-tuned SOTA LLMs,\nsubstantiating our claims. We demonstrate improvements of 50% to 200% over SOTA\nHDL models on current benchmarks in tasks ranging from HDL circuit\nexplanations, code generation, formal and simulation testbench creation,\ntriaging bugs, and fixing them. HDL-GPT opens new avenues for the development\nof advanced model training techniques for circuit design tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}