{"id":"2407.06129","title":"Evaluating the Semantic Profiling Abilities of LLMs for Natural Language\n  Utterances in Data Visualization","authors":"Hannah K. Bako, Arshnoor Bhutani, Xinyi Liu, Kwesi A. Cobbina,\n  Zhicheng Liu","authorsParsed":[["Bako","Hannah K.",""],["Bhutani","Arshnoor",""],["Liu","Xinyi",""],["Cobbina","Kwesi A.",""],["Liu","Zhicheng",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:04:31 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 15:59:29 GMT"}],"updateDate":"2024-07-10","timestamp":1720458271000,"abstract":"  Automatically generating data visualizations in response to human utterances\non datasets necessitates a deep semantic understanding of the data utterance,\nincluding implicit and explicit references to data attributes, visualization\ntasks, and necessary data preparation steps. Natural Language Interfaces (NLIs)\nfor data visualization have explored ways to infer such information, yet\nchallenges persist due to inherent uncertainty in human speech. Recent advances\nin Large Language Models (LLMs) provide an avenue to address these challenges,\nbut their ability to extract the relevant semantic information remains\nunexplored. In this study, we evaluate four publicly available LLMs (GPT-4,\nGemini-Pro, Llama3, and Mixtral), investigating their ability to comprehend\nutterances even in the presence of uncertainty and identify the relevant data\ncontext and visual tasks. Our findings reveal that LLMs are sensitive to\nuncertainties in utterances. Despite this sensitivity, they are able to extract\nthe relevant data context. However, LLMs struggle with inferring visualization\ntasks. Based on these results, we highlight future research directions on using\nLLMs for visualization generation.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}