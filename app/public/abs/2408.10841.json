{"id":"2408.10841","title":"DELIA: Diversity-Enhanced Learning for Instruction Adaptation in Large\n  Language Models","authors":"Yuanhao Zeng, Fei Ren, Xinpeng Zhou, Yihang Wang, Yingxia Shao","authorsParsed":[["Zeng","Yuanhao",""],["Ren","Fei",""],["Zhou","Xinpeng",""],["Wang","Yihang",""],["Shao","Yingxia",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 17:56:06 GMT"}],"updateDate":"2024-08-21","timestamp":1724090166000,"abstract":"  Although instruction tuning is widely used to adjust behavior in Large\nLanguage Models (LLMs), extensive empirical evidence and research indicates\nthat it is primarily a process where the model fits to specific task formats,\nrather than acquiring new knowledge or capabilities. We propose that this\nlimitation stems from biased features learned during instruction tuning, which\ndiffer from ideal task-specfic features, leading to learn less underlying\nsemantics in downstream tasks. However, ideal features are unknown and\nincalculable, constraining past work to rely on prior knowledge to assist\nreasoning or training, which limits LLMs' capabilities to the developers'\nabilities, rather than data-driven scalable learning. In our paper, through our\nnovel data synthesis method, DELIA (Diversity-Enhanced Learning for Instruction\nAdaptation), we leverage the buffering effect of extensive diverse data in LLMs\ntraining to transform biased features in instruction tuning into approximations\nof ideal features, without explicit prior ideal features. Experiments show\nDELIA's better performance compared to common instruction tuning and other\nbaselines. It outperforms common instruction tuning by 17.07%-33.41% on\nIcelandic-English translation bleurt score (WMT-21 dataset, gemma-7b-it) and\nimproves accuracy by 36.1% on formatted text generation (Llama2-7b-chat).\nNotably, among knowledge injection methods we've known, DELIA uniquely align\nthe internal representations of new special tokens with their prior semantics.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}