{"id":"2407.15838","title":"MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with\n  Extensive Diversity","authors":"Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai\n  Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, Yu Qiao, Jifeng Dai","authorsParsed":[["Liu","Yangzhou",""],["Cao","Yue",""],["Gao","Zhangwei",""],["Wang","Weiyun",""],["Chen","Zhe",""],["Wang","Wenhai",""],["Tian","Hao",""],["Lu","Lewei",""],["Zhu","Xizhou",""],["Lu","Tong",""],["Qiao","Yu",""],["Dai","Jifeng",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 17:55:22 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 09:34:25 GMT"}],"updateDate":"2024-08-08","timestamp":1721670922000,"abstract":"  Despite the effectiveness of vision-language supervised fine-tuning in\nenhancing the performance of Vision Large Language Models (VLLMs). However,\nexisting visual instruction tuning datasets include the following limitations:\n(1) Instruction annotation quality: despite existing VLLMs exhibiting strong\nperformance, instructions generated by those advanced VLLMs may still suffer\nfrom inaccuracies, such as hallucinations. (2) Instructions and image\ndiversity: the limited range of instruction types and the lack of diversity in\nimage data may impact the model's ability to generate diversified and closer to\nreal-world scenarios outputs. To address these challenges, we construct a\nhigh-quality, diverse visual instruction tuning dataset MMInstruct, which\nconsists of 973K instructions from 24 domains. There are four instruction\ntypes: Judgement, Multiple-Choice, Long Visual Question Answering and Short\nVisual Question Answering. To construct MMInstruct, we propose an instruction\ngeneration data engine that leverages GPT-4V, GPT-3.5, and manual correction.\nOur instruction generation engine enables semi-automatic, low-cost, and\nmulti-domain instruction generation at 1/6 the cost of manual construction.\nThrough extensive experiment validation and ablation experiments, we\ndemonstrate that MMInstruct could significantly improve the performance of\nVLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-art\nperformance on 10 out of 12 benchmarks. The code and data shall be available at\nhttps://github.com/yuecao0119/MMInstruct.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}