{"id":"2408.11304","title":"FedMoE: Personalized Federated Learning via Heterogeneous Mixture of\n  Experts","authors":"Hanzi Mei, Dongqi Cai, Ao Zhou, Shangguang Wang, Mengwei Xu","authorsParsed":[["Mei","Hanzi",""],["Cai","Dongqi",""],["Zhou","Ao",""],["Wang","Shangguang",""],["Xu","Mengwei",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 03:16:12 GMT"}],"updateDate":"2024-08-22","timestamp":1724210172000,"abstract":"  As Large Language Models (LLMs) push the boundaries of AI capabilities, their\ndemand for data is growing. Much of this data is private and distributed across\nedge devices, making Federated Learning (FL) a de-facto alternative for\nfine-tuning (i.e., FedLLM). However, it faces significant challenges due to the\ninherent heterogeneity among clients, including varying data distributions and\ndiverse task types. Towards a versatile FedLLM, we replace traditional dense\nmodel with a sparsely-activated Mixture-of-Experts (MoE) architecture, whose\nparallel feed-forward networks enable greater flexibility. To make it more\npractical in resource-constrained environments, we present FedMoE, the\nefficient personalized FL framework to address data heterogeneity, constructing\nan optimal sub-MoE for each client and bringing the knowledge back to global\nMoE. FedMoE is composed of two fine-tuning stages. In the first stage, FedMoE\nsimplifies the problem by conducting a heuristic search based on observed\nactivation patterns, which identifies a suboptimal submodel for each client. In\nthe second stage, these submodels are distributed to clients for further\ntraining and returned for server aggregating through a novel modular\naggregation strategy. Meanwhile, FedMoE progressively adjusts the submodels to\noptimal through global expert recommendation. Experimental results demonstrate\nthe superiority of our method over previous personalized FL methods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}