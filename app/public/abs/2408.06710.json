{"id":"2408.06710","title":"Variational Learning of Gaussian Process Latent Variable Models through\n  Stochastic Gradient Annealed Importance Sampling","authors":"Jian Xu, Shian Du, Junmei Yang, Qianli Ma, Delu Zeng","authorsParsed":[["Xu","Jian",""],["Du","Shian",""],["Yang","Junmei",""],["Ma","Qianli",""],["Zeng","Delu",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 08:09:05 GMT"}],"updateDate":"2024-08-14","timestamp":1723536545000,"abstract":"  Gaussian Process Latent Variable Models (GPLVMs) have become increasingly\npopular for unsupervised tasks such as dimensionality reduction and missing\ndata recovery due to their flexibility and non-linear nature. An\nimportance-weighted version of the Bayesian GPLVMs has been proposed to obtain\na tighter variational bound. However, this version of the approach is primarily\nlimited to analyzing simple data structures, as the generation of an effective\nproposal distribution can become quite challenging in high-dimensional spaces\nor with complex data sets. In this work, we propose an Annealed Importance\nSampling (AIS) approach to address these issues. By transforming the posterior\ninto a sequence of intermediate distributions using annealing, we combine the\nstrengths of Sequential Monte Carlo samplers and VI to explore a wider range of\nposterior distributions and gradually approach the target distribution. We\nfurther propose an efficient algorithm by reparameterizing all variables in the\nevidence lower bound (ELBO). Experimental results on both toy and image\ndatasets demonstrate that our method outperforms state-of-the-art methods in\nterms of tighter variational bounds, higher log-likelihoods, and more robust\nconvergence.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Z1crrRb_H6PqYZQF_mbH53LZGfD5lQPld0sJ3ucf1r4","pdfSize":"3362949"}
