{"id":"2408.10497","title":"QUITO-X: An Information Bottleneck-based Compression Algorithm with\n  Cross-Attention","authors":"Yihang Wang, Xu Huang, Bowen Tian, Yixing Fan, Jiafeng Guo","authorsParsed":[["Wang","Yihang",""],["Huang","Xu",""],["Tian","Bowen",""],["Fan","Yixing",""],["Guo","Jiafeng",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 02:44:45 GMT"}],"updateDate":"2024-08-21","timestamp":1724121885000,"abstract":"  Generative LLM have achieved significant success in various industrial tasks\nand can effectively adapt to vertical domains and downstream tasks through ICL.\nHowever, with tasks becoming increasingly complex, the context length required\nby ICL is also getting longer, and two significant issues arise: (i) The\nexcessively long context leads to high costs and inference delays. (ii) A\nsubstantial amount of task-irrelevant information introduced by long contexts\nexacerbates the \"lost in the middle\" problem.\n  Recently, compressing prompts by removing tokens according to some metric\nobtained from some causal language models, such as llama-7b, has emerged as an\neffective approach to mitigate these issues. However, the metric used by prior\nmethod such as self-information or PPL do not fully align with the objective of\ndistinuishing the most important tokens when conditioning on query. In this\nwork, we introduce information bottleneck theory to carefully examine the\nproperties required by the metric. Inspired by this, we use cross-attention in\nencoder-decoder architecture as a new metric. Our simple method leads to\nsignificantly better performance in smaller models with lower latency.\n  We evaluate our method on four datasets: DROP, CoQA, SQuAD, and Quoref. The\nexperimental results show that, while maintaining the same performance, our\ncompression rate can improve by nearly 25% over previous SOTA. Remarkably, in\nexperiments where 25% of the tokens are removed, our model's EM score for\nanswers sometimes even exceeds that of the control group using uncompressed\ntext as context.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8CmdhAGGpwRG0Rd4Hlwtfk7siUykykqQpq9gpdIxvEk","pdfSize":"650546"}
