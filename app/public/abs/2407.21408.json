{"id":"2407.21408","title":"Benchmarking AIGC Video Quality Assessment: A Dataset and Unified Model","authors":"Zhichao Zhang, Xinyue Li, Wei Sun, Jun Jia, Xiongkuo Min, Zicheng\n  Zhang, Chunyi Li, Zijian Chen, Puyi Wang, Zhongpeng Ji, Fengyu Sun, Shangling\n  Jui, and Guangtao Zhai","authorsParsed":[["Zhang","Zhichao",""],["Li","Xinyue",""],["Sun","Wei",""],["Jia","Jun",""],["Min","Xiongkuo",""],["Zhang","Zicheng",""],["Li","Chunyi",""],["Chen","Zijian",""],["Wang","Puyi",""],["Ji","Zhongpeng",""],["Sun","Fengyu",""],["Jui","Shangling",""],["Zhai","Guangtao",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 07:54:26 GMT"}],"updateDate":"2024-08-01","timestamp":1722412466000,"abstract":"  In recent years, artificial intelligence (AI) driven video generation has\ngarnered significant attention due to advancements in stable diffusion and\nlarge language model techniques. Thus, there is a great demand for accurate\nvideo quality assessment (VQA) models to measure the perceptual quality of\nAI-generated content (AIGC) videos as well as optimize video generation\ntechniques. However, assessing the quality of AIGC videos is quite challenging\ndue to the highly complex distortions they exhibit (e.g., unnatural action,\nirrational objects, etc.). Therefore, in this paper, we try to systemically\ninvestigate the AIGC-VQA problem from both subjective and objective quality\nassessment perspectives. For the subjective perspective, we construct a\nLarge-scale Generated Vdeo Quality assessment (LGVQ) dataset, consisting of\n2,808 AIGC videos generated by 6 video generation models using 468 carefully\nselected text prompts. Unlike previous subjective VQA experiments, we evaluate\nthe perceptual quality of AIGC videos from three dimensions: spatial quality,\ntemporal quality, and text-to-video alignment, which hold utmost importance for\ncurrent video generation techniques. For the objective perspective, we\nestablish a benchmark for evaluating existing quality assessment metrics on the\nLGVQ dataset, which reveals that current metrics perform poorly on the LGVQ\ndataset. Thus, we propose a Unify Generated Video Quality assessment (UGVQ)\nmodel to comprehensively and accurately evaluate the quality of AIGC videos\nacross three aspects using a unified model, which uses visual, textual and\nmotion features of video and corresponding prompt, and integrates key features\nto enhance feature expression. We hope that our benchmark can promote the\ndevelopment of quality evaluation metrics for AIGC videos. The LGVQ dataset and\nthe UGVQ metric will be publicly released.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"hq8eCrA8La7a1HaZQJTemFOnxHz-iqKC71rOrNBg-gk","pdfSize":"2051551"}
