{"id":"2407.07671","title":"Why should we ever automate moral decision making?","authors":"Vincent Conitzer","authorsParsed":[["Conitzer","Vincent",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:59:22 GMT"}],"updateDate":"2024-07-11","timestamp":1720619962000,"abstract":"  While people generally trust AI to make decisions in various aspects of their\nlives, concerns arise when AI is involved in decisions with significant moral\nimplications. The absence of a precise mathematical framework for moral\nreasoning intensifies these concerns, as ethics often defies simplistic\nmathematical models. Unlike fields such as logical reasoning, reasoning under\nuncertainty, and strategic decision-making, which have well-defined\nmathematical frameworks, moral reasoning lacks a broadly accepted framework.\nThis absence raises questions about the confidence we can place in AI's moral\ndecision-making capabilities.\n  The environments in which AI systems are typically trained today seem\ninsufficiently rich for such a system to learn ethics from scratch, and even if\nwe had an appropriate environment, it is unclear how we might bring about such\nlearning. An alternative approach involves AI learning from human moral\ndecisions. This learning process can involve aggregating curated human\njudgments or demonstrations in specific domains, or leveraging a foundation\nmodel fed with a wide range of data. Still, concerns persist, given the\nimperfections in human moral decision making.\n  Given this, why should we ever automate moral decision making -- is it not\nbetter to leave all moral decision making to humans? This paper lays out a\nnumber of reasons why we should expect AI systems to engage in decisions with a\nmoral component, with brief discussions of the associated risks.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}