{"id":"2407.00066","title":"Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead","authors":"Rickard Br\\\"uel-Gabrielsson, Jiacheng Zhu, Onkar Bhardwaj, Leshem\n  Choshen, Kristjan Greenewald, Mikhail Yurochkin and Justin Solomon","authorsParsed":[["Br√ºel-Gabrielsson","Rickard",""],["Zhu","Jiacheng",""],["Bhardwaj","Onkar",""],["Choshen","Leshem",""],["Greenewald","Kristjan",""],["Yurochkin","Mikhail",""],["Solomon","Justin",""]],"versions":[{"version":"v1","created":"Mon, 17 Jun 2024 15:21:35 GMT"}],"updateDate":"2024-07-02","timestamp":1718637695000,"abstract":"  Fine-tuning large language models (LLMs) with low-rank adapters (LoRAs) has\nbecome common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of compression when serving LoRA adapters. We consider compressing\nadapters individually via SVD and propose a method for joint compression of\nLoRAs into a shared basis paired with LoRA-specific scaling matrices. Our\nexperiments with up to 500 LoRAs demonstrate that compressed LoRAs preserve\nperformance while offering major throughput gains in realistic serving\nscenarios with over a thousand LoRAs, maintaining 75% of the throughput of\nserving a single LoRA.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}