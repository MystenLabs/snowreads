{"id":"2407.11025","title":"Backdoor Graph Condensation","authors":"Jiahao Wu and Ning Lu and Zeiyu Dai and Wenqi Fan and Shengcai Liu and\n  Qing Li and Ke Tang","authorsParsed":[["Wu","Jiahao",""],["Lu","Ning",""],["Dai","Zeiyu",""],["Fan","Wenqi",""],["Liu","Shengcai",""],["Li","Qing",""],["Tang","Ke",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 06:58:29 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 02:50:04 GMT"}],"updateDate":"2024-07-18","timestamp":1719989909000,"abstract":"  Recently, graph condensation has emerged as a prevalent technique to improve\nthe training efficiency for graph neural networks (GNNs). It condenses a large\ngraph into a small one such that a GNN trained on this small synthetic graph\ncan achieve comparable performance to a GNN trained on a large graph. However,\nwhile existing graph condensation studies mainly focus on the best trade-off\nbetween graph size and the GNNs' performance (model utility), the security\nissues of graph condensation have not been studied. To bridge this research\ngap, we propose the task of backdoor graph condensation. While graph backdoor\nattacks have been extensively explored, applying existing graph backdoor\nmethods for graph condensation is not practical since they can undermine the\nmodel utility and yield low attack success rate.\n  To alleviate these issues, we introduce two primary objectives for backdoor\nattacks against graph condensation: 1) the injection of triggers cannot affect\nthe quality of condensed graphs, maintaining the utility of GNNs trained on\nthem; and 2) the effectiveness of triggers should be preserved throughout the\ncondensation process, achieving high attack success rate. To pursue the\nobjectives, we devise the first backdoor attack against graph condensation,\ndenoted as BGC. Specifically, we inject triggers during condensation and\niteratively update the triggers to ensure effective attacks. Further, we\npropose a poisoned node selection module to minimize the influence of triggers\non condensed graphs' quality. The extensive experiments demonstrate the\neffectiveness of our attack. BGC achieves a high attack success rate (close to\n1.0) and good model utility in all cases. Furthermore, the results demonstrate\nour method's resilience against multiple defense methods. Finally, we conduct\ncomprehensive studies to analyze the factors that influence the attack\nperformance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}