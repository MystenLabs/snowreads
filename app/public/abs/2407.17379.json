{"id":"2407.17379","title":"MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image\n  Relational Association Capabilities in Large Visual Language Models","authors":"Siwei Wu, Kang Zhu, Yu Bai, Yiming Liang, Yizhi Li, Haoning Wu, J.H.\n  Liu, Ruibo Liu, Xingwei Qu, Xuxin Cheng, Ge Zhang, Wenhao Huang, Chenghua Lin","authorsParsed":[["Wu","Siwei",""],["Zhu","Kang",""],["Bai","Yu",""],["Liang","Yiming",""],["Li","Yizhi",""],["Wu","Haoning",""],["Liu","J. H.",""],["Liu","Ruibo",""],["Qu","Xingwei",""],["Cheng","Xuxin",""],["Zhang","Ge",""],["Huang","Wenhao",""],["Lin","Chenghua",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 15:59:01 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 02:44:44 GMT"}],"updateDate":"2024-08-07","timestamp":1721836741000,"abstract":"  Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}