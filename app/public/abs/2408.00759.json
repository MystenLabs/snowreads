{"id":"2408.00759","title":"Text-Guided Video Masked Autoencoder","authors":"David Fan, Jue Wang, Shuai Liao, Zhikang Zhang, Vimal Bhat, Xinyu Li","authorsParsed":[["Fan","David",""],["Wang","Jue",""],["Liao","Shuai",""],["Zhang","Zhikang",""],["Bhat","Vimal",""],["Li","Xinyu",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:58:19 GMT"}],"updateDate":"2024-08-02","timestamp":1722535099000,"abstract":"  Recent video masked autoencoder (MAE) works have designed improved masking\nalgorithms focused on saliency. These works leverage visual cues such as motion\nto mask the most salient regions. However, the robustness of such visual cues\ndepends on how often input videos match underlying assumptions. On the other\nhand, natural language description is an information dense representation of\nvideo that implicitly captures saliency without requiring modality-specific\nassumptions, and has not been explored yet for video MAE. To this end, we\nintroduce a novel text-guided masking algorithm (TGM) that masks the video\nregions with highest correspondence to paired captions. Without leveraging any\nexplicit visual cues for saliency, our TGM is competitive with state-of-the-art\nmasking algorithms such as motion-guided masking. To further benefit from the\nsemantics of natural language for masked reconstruction, we next introduce a\nunified framework for joint MAE and masked video-text contrastive learning. We\nshow that across existing masking algorithms, unifying MAE and masked\nvideo-text contrastive learning improves downstream performance compared to\npure MAE on a variety of video recognition tasks, especially for linear probe.\nWithin this unified framework, our TGM achieves the best relative performance\non five action recognition and one egocentric datasets, highlighting the\ncomplementary nature of natural language for masked video modeling.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}