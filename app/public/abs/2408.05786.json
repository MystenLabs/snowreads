{"id":"2408.05786","title":"HiLight: A Hierarchy-aware Light Global Model with Hierarchical Local\n  ConTrastive Learning","authors":"Zhijian Chen, Zhonghua Li, Jianxin Yang, Ye Qi","authorsParsed":[["Chen","Zhijian",""],["Li","Zhonghua",""],["Yang","Jianxin",""],["Qi","Ye",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 14:26:58 GMT"}],"updateDate":"2024-08-13","timestamp":1723386418000,"abstract":"  Hierarchical text classification (HTC) is a special sub-task of multi-label\nclassification (MLC) whose taxonomy is constructed as a tree and each sample is\nassigned with at least one path in the tree. Latest HTC models contain three\nmodules: a text encoder, a structure encoder and a multi-label classification\nhead. Specially, the structure encoder is designed to encode the hierarchy of\ntaxonomy. However, the structure encoder has scale problem. As the taxonomy\nsize increases, the learnable parameters of recent HTC works grow rapidly.\nRecursive regularization is another widely-used method to introduce\nhierarchical information but it has collapse problem and generally relaxed by\nassigning with a small weight (ie. 1e-6). In this paper, we propose a\nHierarchy-aware Light Global model with Hierarchical local conTrastive learning\n(HiLight), a lightweight and efficient global model only consisting of a text\nencoder and a multi-label classification head. We propose a new learning task\nto introduce the hierarchical information, called Hierarchical Local\nContrastive Learning (HiLCL). Extensive experiments are conducted on two\nbenchmark datasets to demonstrate the effectiveness of our model.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}