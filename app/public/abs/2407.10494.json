{"id":"2407.10494","title":"Learning to Unlearn for Robust Machine Unlearning","authors":"Mark He Huang, Lin Geng Foo, Jun Liu","authorsParsed":[["Huang","Mark He",""],["Foo","Lin Geng",""],["Liu","Jun",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 07:36:00 GMT"}],"updateDate":"2024-07-16","timestamp":1721028960000,"abstract":"  Machine unlearning (MU) seeks to remove knowledge of specific data samples\nfrom trained models without the necessity for complete retraining, a task made\nchallenging by the dual objectives of effective erasure of data and maintaining\nthe overall performance of the model. Despite recent advances in this field,\nbalancing between the dual objectives of unlearning remains challenging. From a\nfresh perspective of generalization, we introduce a novel Learning-to-Unlearn\n(LTU) framework, which adopts a meta-learning approach to optimize the\nunlearning process to improve forgetting and remembering in a unified manner.\nLTU includes a meta-optimization scheme that facilitates models to effectively\npreserve generalizable knowledge with only a small subset of the remaining set,\nwhile thoroughly forgetting the specific data samples. We also introduce a\nGradient Harmonization strategy to align the optimization trajectories for\nremembering and forgetting via mitigating gradient conflicts, thus ensuring\nefficient and effective model updates. Our approach demonstrates improved\nefficiency and efficacy for MU, offering a promising solution to the challenges\nof data rights and model reusability.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}