{"id":"2408.11267","title":"Inverting the Leverage Score Gradient: An Efficient Approximate Newton\n  Method","authors":"Chenyang Li, Zhao Song, Zhaoxing Xu, Junze Yin","authorsParsed":[["Li","Chenyang",""],["Song","Zhao",""],["Xu","Zhaoxing",""],["Yin","Junze",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 01:39:42 GMT"}],"updateDate":"2024-08-22","timestamp":1724204382000,"abstract":"  Leverage scores have become essential in statistics and machine learning,\naiding regression analysis, randomized matrix computations, and various other\ntasks. This paper delves into the inverse problem, aiming to recover the\nintrinsic model parameters given the leverage scores gradient. This endeavor\nnot only enriches the theoretical understanding of models trained with leverage\nscore techniques but also has substantial implications for data privacy and\nadversarial security. We specifically scrutinize the inversion of the leverage\nscore gradient, denoted as $g(x)$. An innovative iterative algorithm is\nintroduced for the approximate resolution of the regularized least squares\nproblem stated as $\\min_{x \\in \\mathbb{R}^d} 0.5 \\|g(x) - c\\|_2^2 +\n0.5\\|\\mathrm{diag}(w)Ax\\|_2^2$. Our algorithm employs subsampled leverage score\ndistributions to compute an approximate Hessian in each iteration, under\nstandard assumptions, considerably mitigating the time complexity. Given that a\ntotal of $T = \\log(\\| x_0 - x^* \\|_2/ \\epsilon)$ iterations are required, the\ncost per iteration is optimized to the order of $O( (\\mathrm{nnz}(A) +\nd^{\\omega} ) \\cdot \\mathrm{poly}(\\log(n/\\delta))$, where $\\mathrm{nnz}(A)$\ndenotes the number of non-zero entries of $A$.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}