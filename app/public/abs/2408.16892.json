{"id":"2408.16892","title":"Tex-ViT: A Generalizable, Robust, Texture-based dual-branch\n  cross-attention deepfake detector","authors":"Deepak Dagar, Dinesh Kumar Vishwakarma","authorsParsed":[["Dagar","Deepak",""],["Vishwakarma","Dinesh Kumar",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 20:26:27 GMT"}],"updateDate":"2024-09-02","timestamp":1724963187000,"abstract":"  Deepfakes, which employ GAN to produce highly realistic facial modification,\nare widely regarded as the prevailing method. Traditional CNN have been able to\nidentify bogus media, but they struggle to perform well on different datasets\nand are vulnerable to adversarial attacks due to their lack of robustness.\nVision transformers have demonstrated potential in the realm of image\nclassification problems, but they require enough training data. Motivated by\nthese limitations, this publication introduces Tex-ViT (Texture-Vision\nTransformer), which enhances CNN features by combining ResNet with a vision\ntransformer. The model combines traditional ResNet features with a texture\nmodule that operates in parallel on sections of ResNet before each\ndown-sampling operation. The texture module then serves as an input to the dual\nbranch of the cross-attention vision transformer. It specifically focuses on\nimproving the global texture module, which extracts feature map correlation.\nEmpirical analysis reveals that fake images exhibit smooth textures that do not\nremain consistent over long distances in manipulations. Experiments were\nperformed on different categories of FF++, such as DF, f2f, FS, and NT,\ntogether with other types of GAN datasets in cross-domain scenarios.\nFurthermore, experiments also conducted on FF++, DFDCPreview, and Celeb-DF\ndataset underwent several post-processing situations, such as blurring,\ncompression, and noise. The model surpassed the most advanced models in terms\nof generalization, achieving a 98% accuracy in cross-domain scenarios. This\ndemonstrates its ability to learn the shared distinguishing textural\ncharacteristics in the manipulated samples. These experiments provide evidence\nthat the proposed model is capable of being applied to various situations and\nis resistant to many post-processing procedures.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}