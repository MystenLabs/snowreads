{"id":"2408.15209","title":"Sec2Sec Co-attention for Video-Based Apparent Affective Prediction","authors":"Mingwei Sun and Kunpeng Zhang","authorsParsed":[["Sun","Mingwei",""],["Zhang","Kunpeng",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 17:18:02 GMT"}],"updateDate":"2024-08-28","timestamp":1724779082000,"abstract":"  Video-based apparent affect detection plays a crucial role in video\nunderstanding, as it encompasses various elements such as vision, audio,\naudio-visual interactions, and spatiotemporal information, which are essential\nfor accurate video predictions. However, existing approaches often focus on\nextracting only a subset of these elements, resulting in the limited predictive\ncapacity of their models. To address this limitation, we propose a novel\nLSTM-based network augmented with a Transformer co-attention mechanism for\npredicting apparent affect in videos. We demonstrate that our proposed Sec2Sec\nCo-attention Transformer surpasses multiple state-of-the-art methods in\npredicting apparent affect on two widely used datasets: LIRIS-ACCEDE and First\nImpressions. Notably, our model offers interpretability, allowing us to examine\nthe contributions of different time points to the overall prediction. The\nimplementation is available at: https://github.com/nestor-sun/sec2sec.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"SvBDiDO2OtvjtT13k2Nhp_2yOTJtQj8Q1RKy5KEAFPw","pdfSize":"720983"}
