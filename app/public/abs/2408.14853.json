{"id":"2408.14853","title":"Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language\n  Models","authors":"Yuhao Du, Zhuo Li, Pengyu Cheng, Xiang Wan, Anningzhe Gao","authorsParsed":[["Du","Yuhao",""],["Li","Zhuo",""],["Cheng","Pengyu",""],["Wan","Xiang",""],["Gao","Anningzhe",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 08:12:08 GMT"}],"updateDate":"2024-08-28","timestamp":1724746328000,"abstract":"  Large Language Models (LLMs) have become a focal point in the rapidly\nevolving field of artificial intelligence. However, a critical concern is the\npresence of toxic content within the pre-training corpus of these models, which\ncan lead to the generation of inappropriate outputs. Investigating methods for\ndetecting internal faults in LLMs can help us understand their limitations and\nimprove their security. Existing methods primarily focus on jailbreaking\nattacks, which involve manually or automatically constructing adversarial\ncontent to prompt the target LLM to generate unexpected responses. These\nmethods rely heavily on prompt engineering, which is time-consuming and usually\nrequires specially designed questions. To address these challenges, this paper\nproposes a target-driven attack paradigm that focuses on directly eliciting the\ntarget response instead of optimizing the prompts. We introduce the use of\nanother LLM as the detector for toxic content, referred to as ToxDet. Given a\ntarget toxic response, ToxDet can generate a possible question and a\npreliminary answer to provoke the target model into producing desired toxic\nresponses with meanings equivalent to the provided one. ToxDet is trained by\ninteracting with the target LLM and receiving reward signals from it, utilizing\nreinforcement learning for the optimization process. While the primary focus of\nthe target models is on open-source LLMs, the fine-tuned ToxDet can also be\ntransferred to attack black-box models such as GPT-4o, achieving notable\nresults. Experimental results on AdvBench and HH-Harmless datasets demonstrate\nthe effectiveness of our methods in detecting the tendencies of target LLMs to\ngenerate harmful responses. This algorithm not only exposes vulnerabilities but\nalso provides a valuable resource for researchers to strengthen their models\nagainst such attacks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GPDZk1OKvTMT_ujUNNpmGiMxsd30gc6Y7GQhtnRxoTo","pdfSize":"5024285"}
