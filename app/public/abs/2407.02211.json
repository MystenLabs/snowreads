{"id":"2407.02211","title":"PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt\n  during Large Language Model Fine-tuning","authors":"Jiaru Zou, Mengyu Zhou, Tao Li, Shi Han, Dongmei Zhang","authorsParsed":[["Zou","Jiaru",""],["Zhou","Mengyu",""],["Li","Tao",""],["Han","Shi",""],["Zhang","Dongmei",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 12:21:14 GMT"}],"updateDate":"2024-07-03","timestamp":1719922874000,"abstract":"  Large language models (LLMs) have played a fundamental role in various\nnatural language processing tasks with powerful prompt techniques. However, in\nreal-world applications, there are often similar prompt components for repeated\nqueries, which causes significant computational burdens during inference.\nExisting prompt compression and direct fine-tuning methods aim to tackle these\nchallenges, yet they frequently struggle to strike an optimal balance between\ncost-efficiency and performance effectiveness, especially in complex tasks such\nas NL2Code. In this paper, we propose a novel method namely PromptIntern to\ninternalize the prompt knowledge into model parameters via progressive\nfine-tuning. Our method enables LLMs to emulate the human learning process for\na new task, where detailed templates and examples in a prompt are gradually\ninternalized and phased out progressively as the model grows accustomed to the\ntask. Extensive experiments demonstrate that our method reduces inference\ntokens over 90%, speedups inference by 4.2 times, and saves 88.3% monetary\ncost.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}