{"id":"2408.03297","title":"KnowPO: Knowledge-aware Preference Optimization for Controllable\n  Knowledge Selection in Retrieval-Augmented Language Models","authors":"Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu\n  Chu, Junfeng Zhao, Yasha Wang","authorsParsed":[["Zhang","Ruizhe",""],["Xu","Yongxin",""],["Xiao","Yuzhen",""],["Zhu","Runchuan",""],["Jiang","Xinke",""],["Chu","Xu",""],["Zhao","Junfeng",""],["Wang","Yasha",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 16:55:54 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 10:38:45 GMT"}],"updateDate":"2024-08-20","timestamp":1722963354000,"abstract":"  By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors such\nas contextual ignorance and contextual overinclusion. To this end, we propose a\nKnowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at\nachieving adaptive knowledge selection based on contextual relevance in real\nretrieval scenarios. Concretely, we proposed a general paradigm for\nconstructing knowledge conflict datasets, which comprehensively cover various\nerror types and learn how to avoid these negative signals through preference\noptimization methods. Simultaneously, we proposed a rewriting strategy and data\nratio optimization strategy to address preference imbalances. Experimental\nresults show that KnowPO outperforms previous methods for handling knowledge\nconflicts by over 37\\%, while also exhibiting robust generalization across\nvarious out-of-distribution datasets.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}