{"id":"2407.06159","title":"A Semantic-Aware and Multi-Guided Network for Infrared-Visible Image\n  Fusion","authors":"Xiaoli Zhang, Liying Wang, Libo Zhao, Xiongfei Li, Siwei Ma","authorsParsed":[["Zhang","Xiaoli",""],["Wang","Liying",""],["Zhao","Libo",""],["Li","Xiongfei",""],["Ma","Siwei",""]],"versions":[{"version":"v1","created":"Tue, 11 Jun 2024 09:32:40 GMT"},{"version":"v2","created":"Sat, 3 Aug 2024 05:46:29 GMT"}],"updateDate":"2024-08-06","timestamp":1718098360000,"abstract":"  Multi-modality image fusion aims at fusing specific-modality and\nshared-modality information from two source images. To tackle the problem of\ninsufficient feature extraction and lack of semantic awareness for complex\nscenes, this paper focuses on how to model correlation-driven decomposing\nfeatures and reason high-level graph representation by efficiently extracting\ncomplementary features and multi-guided feature aggregation. We propose a\nthree-branch encoder-decoder architecture along with corresponding fusion\nlayers as the fusion strategy. The transformer with Multi-Dconv Transposed\nAttention and Local-enhanced Feed Forward network is used to extract shallow\nfeatures after the depthwise convolution. In the three parallel branches\nencoder, Cross Attention and Invertible Block (CAI) enables to extract local\nfeatures and preserve high-frequency texture details. Base feature extraction\nmodule (BFE) with residual connections can capture long-range dependency and\nenhance shared-modality expression capabilities. Graph Reasoning Module (GR) is\nintroduced to reason high-level cross-modality relations and extract low-level\ndetails features as CAI's specific-modality complementary information\nsimultaneously. Experiments demonstrate that our method has obtained\ncompetitive results compared with state-of-the-art methods in visible/infrared\nimage fusion and medical image fusion tasks. Moreover, we surpass other fusion\nmethods in terms of subsequent tasks, averagely scoring 8.27% mAP@0.5 higher in\nobject detection and 5.85% mIoU higher in semantic segmentation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}