{"id":"2408.13092","title":"Diffusion-based Episodes Augmentation for Offline Multi-Agent\n  Reinforcement Learning","authors":"Jihwan Oh, Sungnyun Kim, Gahee Kim, Sunghwan Kim, Se-Young Yun","authorsParsed":[["Oh","Jihwan",""],["Kim","Sungnyun",""],["Kim","Gahee",""],["Kim","Sunghwan",""],["Yun","Se-Young",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 14:17:17 GMT"}],"updateDate":"2024-08-26","timestamp":1724422637000,"abstract":"  Offline multi-agent reinforcement learning (MARL) is increasingly recognized\nas crucial for effectively deploying RL algorithms in environments where\nreal-time interaction is impractical, risky, or costly. In the offline setting,\nlearning from a static dataset of past interactions allows for the development\nof robust and safe policies without the need for live data collection, which\ncan be fraught with challenges. Building on this foundational importance, we\npresent EAQ, Episodes Augmentation guided by Q-total loss, a novel approach for\noffline MARL framework utilizing diffusion models. EAQ integrates the Q-total\nfunction directly into the diffusion model as a guidance to maximize the global\nreturns in an episode, eliminating the need for separate training. Our focus\nprimarily lies on cooperative scenarios, where agents are required to act\ncollectively towards achieving a shared goal-essentially, maximizing global\nreturns. Consequently, we demonstrate that our episodes augmentation in a\ncollaborative manner significantly boosts offline MARL algorithm compared to\nthe original dataset, improving the normalized return by +17.3% and +12.9% for\nmedium and poor behavioral policies in SMAC simulator, respectively.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}