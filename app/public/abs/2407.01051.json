{"id":"2407.01051","title":"On Some Versions of Subspace Optimization Methods with Inexact Gradient\n  Information","authors":"Ilya Kuruzov and Fedor Stonyakin","authorsParsed":[["Kuruzov","Ilya",""],["Stonyakin","Fedor",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 07:58:55 GMT"}],"updateDate":"2024-07-02","timestamp":1719820735000,"abstract":"  It is well-known that accelerated gradient first order methods possess\noptimal complexity estimates for the class of convex smooth minimization\nproblems. In many practical situations, it makes sense to work with inexact\ngradients. However, this can lead to the accumulation of corresponding\ninexactness in the theoretical estimates of the rate of convergence. We propose\nsome modification of the methods for convex optimization with inexact gradient\nbased on the subspace optimization such as Nemirovski's Conjugate Gradients and\nSequential Subspace Optimization. We research the method convergence for\ndifferent condition of inexactness both in gradient value and accuracy of\nsubspace optimization problems. Besides this, we investigate generalization of\nthis result to the class of quasar-convex (weakly-quasi-convex) functions.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/"}