{"id":"2407.09806","title":"Asynchronous Feedback Network for Perceptual Point Cloud Quality\n  Assessment","authors":"Yujie Zhang, Qi Yang, Ziyu Shan, Yiling Xu","authorsParsed":[["Zhang","Yujie",""],["Yang","Qi",""],["Shan","Ziyu",""],["Xu","Yiling",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 08:52:44 GMT"}],"updateDate":"2024-07-16","timestamp":1720860764000,"abstract":"  Recent years have witnessed the success of the deep learning-based technique\nin research of no-reference point cloud quality assessment (NR-PCQA). For a\nmore accurate quality prediction, many previous studies have attempted to\ncapture global and local feature in a bottom-up manner, but ignored the\ninteraction and promotion between them. To solve this problem, we propose a\nnovel asynchronous feedback network (AFNet). Motivated by human visual\nperception mechanisms, AFNet employs a dual-branch structure to deal with\nglobal and local feature, simulating the left and right hemispheres of the\nhuman brain, and constructs a feedback module between them. Specifically, the\ninput point clouds are first fed into a transformer-based global encoder to\ngenerate the attention maps that highlight these semantically rich regions,\nfollowed by being merged into the global feature. Then, we utilize the\ngenerated attention maps to perform dynamic convolution for different semantic\nregions and obtain the local feature. Finally, a coarse-to-fine strategy is\nadopted to merge the two features into the final quality score. We conduct\ncomprehensive experiments on three datasets and achieve superior performance\nover the state-of-the-art approaches on all of these datasets. The code will be\navailable at https://github.com/zhangyujie-1998/AFNet.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}