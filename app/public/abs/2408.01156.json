{"id":"2408.01156","title":"TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for\n  T-Cell Receptor Repertoires Generation","authors":"Yicheng Lin, Dandan Zhang and Yun Liu","authorsParsed":[["Lin","Yicheng",""],["Zhang","Dandan",""],["Liu","Yun",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 10:16:28 GMT"}],"updateDate":"2024-08-05","timestamp":1722593788000,"abstract":"  T-cell receptors (TCRs) play a crucial role in the immune system by\nrecognizing and binding to specific antigens presented by infected or cancerous\ncells. Understanding the sequence patterns of TCRs is essential for developing\ntargeted immune therapies and designing effective vaccines. Language models,\nsuch as auto-regressive transformers, offer a powerful solution to this problem\nby learning the probability distributions of TCR repertoires, enabling the\ngeneration of new TCR sequences that inherit the underlying patterns of the\nrepertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only\ntransformer architecture, designed to uncover and replicate sequence patterns\nin TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring\nsequence probability distributions measured by Pearson correlation coefficient.\nFurthermore, by leveraging Reinforcement Learning(RL), we adapted the\ndistribution of TCR sequences to generate TCRs capable of recognizing specific\npeptides, offering significant potential for advancing targeted immune\ntherapies and vaccine development. With the efficacy of RL, fine-tuned\npretrained TCR-GPT models demonstrated the ability to produce TCR repertoires\nlikely to bind specific peptides, illustrating RL's efficiency in enhancing the\nmodel's adaptability to the probability distributions of biologically relevant\nTCR sequences.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}