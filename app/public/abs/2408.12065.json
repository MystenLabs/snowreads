{"id":"2408.12065","title":"Transformers As Approximations of Solomonoff Induction","authors":"Nathan Young, Michael Witbrock","authorsParsed":[["Young","Nathan",""],["Witbrock","Michael",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 02:05:44 GMT"}],"updateDate":"2024-08-23","timestamp":1724292344000,"abstract":"  Solomonoff Induction is an optimal-in-the-limit unbounded algorithm for\nsequence prediction, representing a Bayesian mixture of every computable\nprobability distribution and performing close to optimally in predicting any\ncomputable sequence.\n  Being an optimal form of computational sequence prediction, it seems\nplausible that it may be used as a model against which other methods of\nsequence prediction might be compared.\n  We put forth and explore the hypothesis that Transformer models - the basis\nof Large Language Models - approximate Solomonoff Induction better than any\nother extant sequence prediction method. We explore evidence for and against\nthis hypothesis, give alternate hypotheses that take this evidence into\naccount, and outline next steps for modelling Transformers and other kinds of\nAI in this way.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}