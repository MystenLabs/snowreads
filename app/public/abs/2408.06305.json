{"id":"2408.06305","title":"From SAM to SAM 2: Exploring Improvements in Meta's Segment Anything\n  Model","authors":"Athulya Sundaresan Geetha and Muhammad Hussain","authorsParsed":[["Geetha","Athulya Sundaresan",""],["Hussain","Muhammad",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 17:17:35 GMT"}],"updateDate":"2024-08-13","timestamp":1723483055000,"abstract":"  The Segment Anything Model (SAM), introduced to the computer vision community\nby Meta in April 2023, is a groundbreaking tool that allows automated\nsegmentation of objects in images based on prompts such as text, clicks, or\nbounding boxes. SAM excels in zero-shot performance, segmenting unseen objects\nwithout additional training, stimulated by a large dataset of over one billion\nimage masks. SAM 2 expands this functionality to video, leveraging memory from\npreceding and subsequent frames to generate accurate segmentation across entire\nvideos, enabling near real-time performance. This comparison shows how SAM has\nevolved to meet the growing need for precise and efficient segmentation in\nvarious applications. The study suggests that future advancements in models\nlike SAM will be crucial for improving computer vision technology.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"tpmZAHrYJhBojpuGO0V29qGpPTT21pAEJI2OT2JBYL4","pdfSize":"1673793"}
