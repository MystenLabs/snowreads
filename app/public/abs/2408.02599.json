{"id":"2408.02599","title":"Progressively Selective Label Enhancement for Language Model Alignment","authors":"Biao Liu, Ning Xu, Xin Geng","authorsParsed":[["Liu","Biao",""],["Xu","Ning",""],["Geng","Xin",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 16:21:17 GMT"}],"updateDate":"2024-08-06","timestamp":1722874877000,"abstract":"  Large Language Models have demonstrated impressive capabilities in various\nlanguage tasks but may produce content that misaligns with human expectations,\nraising ethical and legal concerns. Therefore, it is important to explore the\nlimitations and implement restrictions on the models to ensure safety and\ncompliance, with Reinforcement Learning from Human Feedback (RLHF) being the\nprimary method. Due to challenges in stability and scalability with the RLHF\nstages, researchers are exploring alternative methods to achieve effects\ncomparable to those of RLHF. However, these methods often depend on large\nhigh-quality datasets and inefficiently utilize generated data. To deal with\nthis problem, we propose PSLE, i.e., Progressively Selective Label Enhancement\nfor Language Model Alignment, a framework that fully utilizes all generated\ndata by guiding the model with principles to align outputs with human\nexpectations. Using a dynamically updated threshold, our approach ensures\nefficient data utilization by incorporating all generated responses and\nweighting them based on their corresponding reward scores. Experimental results\non multiple datasets demonstrate the effectiveness of PSLE compared to existing\nlanguage model alignment methods.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}