{"id":"2407.20503","title":"A federated large language model for long-term time series forecasting","authors":"Raed Abdel-Sater and A. Ben Hamza","authorsParsed":[["Abdel-Sater","Raed",""],["Hamza","A. Ben",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 02:38:27 GMT"}],"updateDate":"2024-07-31","timestamp":1722307107000,"abstract":"  Long-term time series forecasting in centralized environments poses unique\nchallenges regarding data privacy, communication overhead, and scalability. To\naddress these challenges, we propose FedTime, a federated large language model\n(LLM) tailored for long-range time series prediction. Specifically, we\nintroduce a federated pre-trained LLM with fine-tuning and alignment\nstrategies. Prior to the learning process, we employ K-means clustering to\npartition edge devices or clients into distinct clusters, thereby facilitating\nmore focused model training. We also incorporate channel independence and\npatching to better preserve local semantic information, ensuring that important\ncontextual details are retained while minimizing the risk of information loss.\nWe demonstrate the effectiveness of our FedTime model through extensive\nexperiments on various real-world forecasting benchmarks, showcasing\nsubstantial improvements over recent approaches. In addition, we demonstrate\nthe efficiency of FedTime in streamlining resource usage, resulting in reduced\ncommunication overhead.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"7Y5Pc4mDNjcEPpmwCf5WdsFgCXe8Mo1U0rb-S2g2510","pdfSize":"510473"}
