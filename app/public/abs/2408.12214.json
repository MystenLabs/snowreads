{"id":"2408.12214","title":"UNCO: Towards Unifying Neural Combinatorial Optimization through Large\n  Language Model","authors":"Xia Jiang, Yaoxin Wu, Yuan Wang, Yingqian Zhang","authorsParsed":[["Jiang","Xia",""],["Wu","Yaoxin",""],["Wang","Yuan",""],["Zhang","Yingqian",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 08:42:44 GMT"}],"updateDate":"2024-08-23","timestamp":1724316164000,"abstract":"  Recently, applying neural networks to address combinatorial optimization\nproblems (COPs) has attracted considerable research attention. The prevailing\nmethods always train deep models independently on specific problems, lacking a\nunified framework for concurrently tackling various COPs. To this end, we\npropose a unified neural combinatorial optimization (UNCO) framework to solve\ndifferent types of COPs by a single model. Specifically, we use natural\nlanguage to formulate text-attributed instances for different COPs and encode\nthem in the same embedding space by the large language model (LLM). The\nobtained embeddings are further advanced by an encoder-decoder model without\nany problem-specific modules, thereby facilitating a unified process of\nsolution construction. We further adopt the conflict gradients erasing\nreinforcement learning (CGERL) algorithm to train the UNCO model, delivering\nbetter performance across different COPs than vanilla multi-objective learning.\nExperiments show that the UNCO model can solve multiple COPs after a\nsingle-session training, and achieves satisfactory performance that is\ncomparable to several traditional or learning-based baselines. Instead of\npursuing the best performance for each COP, we explore the synergy between\ntasks and few-shot generalization based on LLM to inspire future work.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"uYkHjV2fe1hRLy_NbP2gc28U5DlPmMOG2-E9oUyVpRQ","pdfSize":"1264492"}
