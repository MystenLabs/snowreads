{"id":"2408.13030","title":"Real Log Canonical Thresholds at Non-singular Points","authors":"Yuki Kurumadani","authorsParsed":[["Kurumadani","Yuki",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 12:38:22 GMT"}],"updateDate":"2024-08-26","timestamp":1724416702000,"abstract":"  Recent advances have clarified theoretical learning accuracy in Bayesian\ninference, revealing that the asymptotic behavior of metrics such as\ngeneralization loss and free energy, assessing predictive accuracy, is dictated\nby a rational number unique to each statistical model, termed the learning\ncoefficient (real log canonical threshold). For models meeting regularity\nconditions, their learning coefficients are known. However, for singular models\nnot meeting these conditions, exact values of learning coefficients are\nprovided for specific models like reduced-rank regression, but a broadly\napplicable calculation method for these learning coefficients in singular\nmodels remains elusive.\n  This paper extends the application range of the previous work and provides an\napproach that can be applied to many points within the set of realizable\nparameters. Specifically, it provides a formula for calculating the real log\ncanonical threshold at many non-singular points within the set of realizable\nparameters. If this calculation can be performed, it is possible to obtain an\nupper bound for the learning coefficient of the statistical model. Thus, this\napproach can also be used to easily obtain an upper bound for the learning\ncoefficients of statistical models. As an application example, it provides an\nupper bound for the learning coefficient of a mixed binomial model, and\ncalculates the learning coefficient for a specific case of reduced-rank\nregression, confirming that the results are consistent with previous research.\n","subjects":["Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}