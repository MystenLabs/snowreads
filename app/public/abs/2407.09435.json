{"id":"2407.09435","title":"MUSCLE: A Model Update Strategy for Compatible LLM Evolution","authors":"Jessica Echterhoff, Fartash Faghri, Raviteja Vemulapalli, Ting-Yao Hu,\n  Chun-Liang Li, Oncel Tuzel, Hadi Pouransari","authorsParsed":[["Echterhoff","Jessica",""],["Faghri","Fartash",""],["Vemulapalli","Raviteja",""],["Hu","Ting-Yao",""],["Li","Chun-Liang",""],["Tuzel","Oncel",""],["Pouransari","Hadi",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 17:12:48 GMT"}],"updateDate":"2024-07-15","timestamp":1720804368000,"abstract":"  Large Language Models (LLMs) are frequently updated due to data or\narchitecture changes to improve their performance. When updating models,\ndevelopers often focus on increasing overall performance metrics with less\nemphasis on being compatible with previous model versions. However, users often\nbuild a mental model of the functionality and capabilities of a particular\nmachine learning model they are interacting with. They have to adapt their\nmental model with every update -- a draining task that can lead to user\ndissatisfaction. In practice, fine-tuned downstream task adapters rely on\npretrained LLM base models. When these base models are updated, these\nuser-facing downstream task models experience instance regression or negative\nflips -- previously correct instances are now predicted incorrectly. This\nhappens even when the downstream task training procedures remain identical. Our\nwork aims to provide seamless model updates to a user in two ways. First, we\nprovide evaluation metrics for a notion of compatibility to prior model\nversions, specifically for generative tasks but also applicable for\ndiscriminative tasks. We observe regression and inconsistencies between\ndifferent model versions on a diverse set of tasks and model updates. Second,\nwe propose a training strategy to minimize the number of inconsistencies in\nmodel updates, involving training of a compatibility model that can enhance\ntask fine-tuned language models. We reduce negative flips -- instances where a\nprior model version was correct, but a new model incorrect -- by up to 40% from\nLlama 1 to Llama 2.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"sgw-lv_k7BsNGoCn-QCiYyK_a342ccOLeEa7MKqAZaE","pdfSize":"665661"}
