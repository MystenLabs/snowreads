{"id":"2407.10563","title":"Pathformer3D: A 3D Scanpath Transformer for 360{\\deg} Images","authors":"Rong Quan, Yantao Lai, Mengyu Qiu, Dong Liang","authorsParsed":[["Quan","Rong",""],["Lai","Yantao",""],["Qiu","Mengyu",""],["Liang","Dong",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 09:24:27 GMT"}],"updateDate":"2024-07-16","timestamp":1721035467000,"abstract":"  Scanpath prediction in 360{\\deg} images can help realize rapid rendering and\nbetter user interaction in Virtual/Augmented Reality applications. However,\nexisting scanpath prediction models for 360{\\deg} images execute scanpath\nprediction on 2D equirectangular projection plane, which always result in big\ncomputation error owing to the 2D plane's distortion and coordinate\ndiscontinuity. In this work, we perform scanpath prediction for 360{\\deg}\nimages in 3D spherical coordinate system and proposed a novel 3D scanpath\nTransformer named Pathformer3D. Specifically, a 3D Transformer encoder is first\nused to extract 3D contextual feature representation for the 360{\\deg} image.\nThen, the contextual feature representation and historical fixation information\nare input into a Transformer decoder to output current time step's fixation\nembedding, where the self-attention module is used to imitate the visual\nworking memory mechanism of human visual system and directly model the time\ndependencies among the fixations. Finally, a 3D Gaussian distribution is\nlearned from each fixation embedding, from which the fixation position can be\nsampled. Evaluation on four panoramic eye-tracking datasets demonstrates that\nPathformer3D outperforms the current state-of-the-art methods. Code is\navailable at https://github.com/lsztzp/Pathformer3D .\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}