{"id":"2407.07168","title":"Statistical mechanics of transfer learning in fully-connected networks\n  in the proportional limit","authors":"Alessandro Ingrosso, Rosalba Pacelli, Pietro Rotondo, Federica Gerace","authorsParsed":[["Ingrosso","Alessandro",""],["Pacelli","Rosalba",""],["Rotondo","Pietro",""],["Gerace","Federica",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 18:14:18 GMT"}],"updateDate":"2024-07-11","timestamp":1720548858000,"abstract":"  Transfer learning (TL) is a well-established machine learning technique to\nboost the generalization performance on a specific (target) task using\ninformation gained from a related (source) task, and it crucially depends on\nthe ability of a network to learn useful features. Leveraging recent analytical\nprogress in the proportional regime of deep learning theory (i.e. the limit\nwhere the size of the training set $P$ and the size of the hidden layers $N$\nare taken to infinity keeping their ratio $\\alpha = P/N$ finite), in this work\nwe develop a novel single-instance Franz-Parisi formalism that yields an\neffective theory for TL in fully-connected neural networks. Unlike the\n(lazy-training) infinite-width limit, where TL is ineffective, we demonstrate\nthat in the proportional limit TL occurs due to a renormalized source-target\nkernel that quantifies their relatedness and determines whether TL is\nbeneficial for generalization.\n","subjects":["Condensed Matter/Disordered Systems and Neural Networks","Condensed Matter/Statistical Mechanics"],"license":"http://creativecommons.org/licenses/by/4.0/"}