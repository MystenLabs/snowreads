{"id":"2407.14503","title":"Catastrophic Goodhart: regularizing RLHF with KL divergence does not\n  mitigate heavy-tailed reward misspecification","authors":"Thomas Kwa, Drake Thomas, Adri\\`a Garriga-Alonso","authorsParsed":[["Kwa","Thomas",""],["Thomas","Drake",""],["Garriga-Alonso","Adri√†",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:57:59 GMT"}],"updateDate":"2024-07-22","timestamp":1721411879000,"abstract":"  When applying reinforcement learning from human feedback (RLHF), the reward\nis learned from data and, therefore, always has some error. It is common to\nmitigate this by regularizing the policy with KL divergence from a base model,\nwith the hope that balancing reward with regularization will achieve desirable\noutcomes despite this reward misspecification. We show that when the reward\nfunction has light-tailed error, optimal policies under less restrictive KL\npenalties achieve arbitrarily high utility. However, if error is heavy-tailed,\nsome policies obtain arbitrarily high reward despite achieving no more utility\nthan the base model--a phenomenon we call catastrophic Goodhart. We adapt a\ndiscrete optimization method to measure the tails of reward models, finding\nthat they are consistent with light-tailed error. However, the pervasiveness of\nheavy-tailed distributions in many real-world applications indicates that\nfuture sources of RL reward could have heavy-tailed error, increasing the\nlikelihood of reward hacking even with KL regularization.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}