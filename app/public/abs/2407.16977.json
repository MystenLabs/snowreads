{"id":"2407.16977","title":"Selective Vision-Language Subspace Projection for Few-shot CLIP","authors":"Xingyu Zhu, Beier Zhu, Yi Tan, Shuo Wang, Yanbin Hao, Hanwang Zhang","authorsParsed":[["Zhu","Xingyu",""],["Zhu","Beier",""],["Tan","Yi",""],["Wang","Shuo",""],["Hao","Yanbin",""],["Zhang","Hanwang",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 03:45:35 GMT"},{"version":"v2","created":"Fri, 26 Jul 2024 15:52:46 GMT"}],"updateDate":"2024-07-29","timestamp":1721792735000,"abstract":"  Vision-language models such as CLIP are capable of mapping the different\nmodality data into a unified feature space, enabling zero/few-shot inference by\nmeasuring the similarity of given images and texts. However, most existing\nmethods overlook modality gaps in CLIP's encoded features, which is shown as\nthe text and image features lie far apart from each other, resulting in limited\nclassification performance. To tackle this issue, we introduce a method called\nSelective Vision-Language Subspace Projection (SSP), which incorporates local\nimage features and utilizes them as a bridge to enhance the alignment between\nimage-text pairs. Specifically, our SSP framework comprises two parallel\nmodules: a vision projector and a language projector. Both projectors utilize\nlocal image features to span the respective subspaces for image and texts,\nthereby projecting the image and text features into their respective subspaces\nto achieve alignment. Moreover, our approach entails only training-free matrix\ncalculations and can be seamlessly integrated into advanced CLIP-based few-shot\nlearning frameworks. Extensive experiments on 11 datasets have demonstrated\nSSP's superior text-image alignment capabilities, outperforming the\nstate-of-the-art alignment methods. The code is available at\nhttps://github.com/zhuhsingyuu/SSP\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}