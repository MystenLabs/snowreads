{"id":"2408.04547","title":"Emotional Cues Extraction and Fusion for Multi-modal Emotion Prediction\n  and Recognition in Conversation","authors":"Haoxiang Shi, Ziqi Liang and Jun Yu","authorsParsed":[["Shi","Haoxiang",""],["Liang","Ziqi",""],["Yu","Jun",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 15:55:35 GMT"}],"updateDate":"2024-08-09","timestamp":1723132535000,"abstract":"  Emotion Prediction in Conversation (EPC) aims to forecast the emotions of\nforthcoming utterances by utilizing preceding dialogues. Previous EPC\napproaches relied on simple context modeling for emotion extraction,\noverlooking fine-grained emotion cues at the word level. Additionally, prior\nworks failed to account for the intrinsic differences between modalities,\nresulting in redundant information. To overcome these limitations, we propose\nan emotional cues extraction and fusion network, which consists of two stages:\na modality-specific learning stage that utilizes word-level labels and prosody\nlearning to construct emotion embedding spaces for each modality, and a\ntwo-step fusion stage for integrating multi-modal features. Moreover, the\nemotion features extracted by our model are also applicable to the Emotion\nRecognition in Conversation (ERC) task. Experimental results validate the\nefficacy of the proposed method, demonstrating superior performance on both\nIEMOCAP and MELD datasets.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}