{"id":"2407.14266","title":"L^2CL: Embarrassingly Simple Layer-to-Layer Contrastive Learning for\n  Graph Collaborative Filtering","authors":"Xinzhou Jin, Jintang Li, Liang Chen, Chenyun Yu, Yuanzhen Xie, Tao\n  Xie, Chengxiang Zhuo, Zang Li, Zibin Zheng","authorsParsed":[["Jin","Xinzhou",""],["Li","Jintang",""],["Chen","Liang",""],["Yu","Chenyun",""],["Xie","Yuanzhen",""],["Xie","Tao",""],["Zhuo","Chengxiang",""],["Li","Zang",""],["Zheng","Zibin",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 12:45:21 GMT"}],"updateDate":"2024-07-22","timestamp":1721393121000,"abstract":"  Graph neural networks (GNNs) have recently emerged as an effective approach\nto model neighborhood signals in collaborative filtering. Towards this research\nline, graph contrastive learning (GCL) demonstrates robust capabilities to\naddress the supervision label shortage issue through generating massive\nself-supervised signals. Despite its effectiveness, GCL for recommendation\nsuffers seriously from two main challenges: i) GCL relies on graph augmentation\nto generate semantically different views for contrasting, which could\npotentially disrupt key information and introduce unwanted noise; ii) current\nworks for GCL primarily focus on contrasting representations using\nsophisticated networks architecture (usually deep) to capture high-order\ninteractions, which leads to increased computational complexity and suboptimal\ntraining efficiency. To this end, we propose L2CL, a principled Layer-to-Layer\nContrastive Learning framework that contrasts representations from different\nlayers. By aligning the semantic similarities between different layers, L2CL\nenables the learning of complex structural relationships and gets rid of the\nnoise perturbation in stochastic data augmentation. Surprisingly, we find that\nL2CL, using only one-hop contrastive learning paradigm, is able to capture\nintrinsic semantic structures and improve the quality of node representation,\nleading to a simple yet effective architecture. We also provide theoretical\nguarantees for L2CL in minimizing task-irrelevant information. Extensive\nexperiments on five real-world datasets demonstrate the superiority of our\nmodel over various state-of-the-art collaborative filtering methods. Our code\nis available at https://github.com/downeykking/L2CL.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}