{"id":"2407.12075","title":"Tiled Bit Networks: Sub-Bit Neural Network Compression Through Reuse of\n  Learnable Binary Vectors","authors":"Matt Gorbett, Hossein Shirazi, Indrakshi Ray","authorsParsed":[["Gorbett","Matt",""],["Shirazi","Hossein",""],["Ray","Indrakshi",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:55:38 GMT"}],"updateDate":"2024-07-18","timestamp":1721145338000,"abstract":"  Binary Neural Networks (BNNs) enable efficient deep learning by saving on\nstorage and computational costs. However, as the size of neural networks\ncontinues to grow, meeting computational requirements remains a challenge. In\nthis work, we propose a new form of quantization to tile neural network layers\nwith sequences of bits to achieve sub-bit compression of binary-weighted neural\nnetworks. The method learns binary vectors (i.e. tiles) to populate each layer\nof a model via aggregation and reshaping operations. During inference, the\nmethod reuses a single tile per layer to represent the full tensor. We employ\nthe approach to both fully-connected and convolutional layers, which make up\nthe breadth of space in most neural architectures. Empirically, the approach\nachieves near fullprecision performance on a diverse range of architectures\n(CNNs, Transformers, MLPs) and tasks (classification, segmentation, and time\nseries forecasting) with up to an 8x reduction in size compared to\nbinary-weighted models. We provide two implementations for Tiled Bit Networks:\n1) we deploy the model to a microcontroller to assess its feasibility in\nresource-constrained environments, and 2) a GPU-compatible inference kernel to\nfacilitate the reuse of a single tile per layer in memory.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WttUzyInmGd-jLfLpKAJvpH9o9LxohEDWwofG-RTupw","pdfSize":"954010"}
