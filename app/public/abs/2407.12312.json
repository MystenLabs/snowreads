{"id":"2407.12312","title":"Shap-Mix: Shapley Value Guided Mixing for Long-Tailed Skeleton Based\n  Action Recognition","authors":"Jiahang Zhang, Lilang Lin, Jiaying Liu","authorsParsed":[["Zhang","Jiahang",""],["Lin","Lilang",""],["Liu","Jiaying",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 04:25:51 GMT"}],"updateDate":"2024-07-18","timestamp":1721190351000,"abstract":"  In real-world scenarios, human actions often fall into a long-tailed\ndistribution. It makes the existing skeleton-based action recognition works,\nwhich are mostly designed based on balanced datasets, suffer from a sharp\nperformance degradation. Recently, many efforts have been madeto image/video\nlong-tailed learning. However, directly applying them to skeleton data can be\nsub-optimal due to the lack of consideration of the crucial spatial-temporal\nmotion patterns, especially for some modality-specific methodologies such as\ndata augmentation. To this end, considering the crucial role of the body parts\nin the spatially concentrated human actions, we attend to the mixing\naugmentations and propose a novel method, Shap-Mix, which improves long-tailed\nlearning by mining representative motion patterns for tail categories.\nSpecifically, we first develop an effective spatial-temporal mixing strategy\nfor the skeleton to boost representation quality. Then, the employed saliency\nguidance method is presented, consisting of the saliency estimation based on\nShapley value and a tail-aware mixing policy. It preserves the salient motion\nparts of minority classes in mixed data, explicitly establishing the\nrelationships between crucial body structure cues and high-level semantics.\nExtensive experiments on three large-scale skeleton datasets show our\nremarkable performance improvement under both long-tailed and balanced\nsettings. Our project is publicly available at:\nhttps://jhang2020.github.io/Projects/Shap-Mix/Shap-Mix.html.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}