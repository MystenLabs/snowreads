{"id":"2407.03641","title":"Learning Scalable Model Soup on a Single GPU: An Efficient Subspace\n  Training Strategy","authors":"Tao Li, Weisen Jiang, Fanghui Liu, Xiaolin Huang, James T. Kwok","authorsParsed":[["Li","Tao",""],["Jiang","Weisen",""],["Liu","Fanghui",""],["Huang","Xiaolin",""],["Kwok","James T.",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 05:23:22 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 06:19:21 GMT"}],"updateDate":"2024-07-24","timestamp":1720070602000,"abstract":"  Pre-training followed by fine-tuning is widely adopted among practitioners.\nThe performance can be improved by \"model soups\"~\\cite{wortsman2022model} via\nexploring various hyperparameter configurations.The Learned-Soup, a variant of\nmodel soups, significantly improves the performance but suffers from\nsubstantial memory and time costs due to the requirements of (i) having to load\nall fine-tuned models simultaneously, and (ii) a large computational graph\nencompassing all fine-tuned models. In this paper, we propose Memory Efficient\nHyperplane Learned Soup (MEHL-Soup) to tackle this issue by formulating the\nlearned soup as a hyperplane optimization problem and introducing block\ncoordinate gradient descent to learn the mixing coefficients. At each\niteration, MEHL-Soup only needs to load a few fine-tuned models and build a\ncomputational graph with one combined model. We further extend MEHL-Soup to\nMEHL-Soup+ in a layer-wise manner. Experimental results on various ViT models\nand data sets show that MEHL-Soup(+) outperforms Learned-Soup(+) in terms of\ntest accuracy, and also reduces memory usage by more than $13\\times$. Moreover,\nMEHL-Soup(+) can be run on a single GPU and achieves $9\\times$ speed up in soup\nconstruction compared with the Learned-Soup. The code is released at\nhttps://github.com/nblt/MEHL-Soup.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}