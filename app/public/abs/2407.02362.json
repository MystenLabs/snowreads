{"id":"2407.02362","title":"Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication\n  on FPGA","authors":"Xuqi Zhu, Huaizhi Zhang, JunKyu Lee, Jiacheng Zhu, Chandrajit Pal,\n  Sangeet Saha, Klaus D. McDonald-Maier and Xiaojun Zhai","authorsParsed":[["Zhu","Xuqi",""],["Zhang","Huaizhi",""],["Lee","JunKyu",""],["Zhu","Jiacheng",""],["Pal","Chandrajit",""],["Saha","Sangeet",""],["McDonald-Maier","Klaus D.",""],["Zhai","Xiaojun",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 15:28:10 GMT"},{"version":"v2","created":"Sun, 7 Jul 2024 17:20:51 GMT"}],"updateDate":"2024-07-09","timestamp":1719934090000,"abstract":"  Modern Neural Network (NN) architectures heavily rely on vast numbers of\nmultiply-accumulate arithmetic operations, constituting the predominant\ncomputational cost. Therefore, this paper proposes a high-throughput, scalable\nand energy efficient non-element-wise matrix multiplication unit on FPGAs as a\nbasic component of the NNs. We firstly streamline inter-layer and intra-layer\nredundancies of MADDNESS algorithm, a LUT-based approximate matrix\nmultiplication, to design a fast, efficient scalable approximate matrix\nmultiplication module termed \"Approximate Multiplication Unit (AMU)\". The AMU\noptimizes LUT-based matrix multiplications further through dedicated memory\nmanagement and access design, decoupling computational overhead from input\nresolution and boosting FPGA-based NN accelerator efficiency significantly. The\nexperimental results show that using our AMU achieves up to 9x higher\nthroughput and 112x higher energy efficiency over the state-of-the-art\nsolutions for the FPGA-based Quantised Neural Network (QNN) accelerators.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}