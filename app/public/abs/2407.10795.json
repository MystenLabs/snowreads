{"id":"2407.10795","title":"Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping","authors":"Wenhao Zhu, Sizhe Liu, Shujian Huang, Shuaijie She, Chris Wendler,\n  Jiajun Chen","authorsParsed":[["Zhu","Wenhao",""],["Liu","Sizhe",""],["Huang","Shujian",""],["She","Shuaijie",""],["Wendler","Chris",""],["Chen","Jiajun",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:14:01 GMT"}],"updateDate":"2024-07-16","timestamp":1721056441000,"abstract":"  Decoding by contrasting layers (DoLa), is designed to improve the generation\nquality of large language models (LLMs) by contrasting the prediction\nprobabilities between an early exit output (amateur logits) and the final\noutput (expert logits). However, we find that this approach does not work well\non non-English tasks. Inspired by previous interpretability work on language\ntransition during the model's forward pass, we discover that this issue arises\nfrom a language mismatch between early exit output and final output. In this\nwork, we propose an improved contrastive decoding algorithm that is effective\nfor diverse languages beyond English. To obtain more helpful amateur logits, we\ndevise two strategies to skip a set of bottom, language-agnostic layers based\non our preliminary analysis. Experimental results on multilingual reasoning\nbenchmarks demonstrate that our proposed method outperforms previous\ncontrastive decoding baselines and substantially improves LLM's\nchain-of-thought reasoning accuracy across 11 languages. The project will be\navailable at: https://github.com/NJUNLP/SkipLayerCD.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}