{"id":"2408.08742","title":"A lifted Bregman strategy for training unfolded proximal neural network\n  Gaussian denoisers","authors":"Xiaoyu Wang, Martin Benning, Audrey Repetti","authorsParsed":[["Wang","Xiaoyu",""],["Benning","Martin",""],["Repetti","Audrey",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 13:41:34 GMT"}],"updateDate":"2024-08-19","timestamp":1723815694000,"abstract":"  Unfolded proximal neural networks (PNNs) form a family of methods that\ncombines deep learning and proximal optimization approaches. They consist in\ndesigning a neural network for a specific task by unrolling a proximal\nalgorithm for a fixed number of iterations, where linearities can be learned\nfrom prior training procedure. PNNs have shown to be more robust than\ntraditional deep learning approaches while reaching at least as good\nperformances, in particular in computational imaging. However, training PNNs\nstill depends on the efficiency of available training algorithms. In this work,\nwe propose a lifted training formulation based on Bregman distances for\nunfolded PNNs. Leveraging the deterministic mini-batch block-coordinate\nforward-backward method, we design a bespoke computational strategy beyond\ntraditional back-propagation methods for solving the resulting learning problem\nefficiently. We assess the behaviour of the proposed training approach for PNNs\nthrough numerical simulations on image denoising, considering a denoising PNN\nwhose structure is based on dual proximal-gradient iterations.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}