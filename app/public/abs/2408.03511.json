{"id":"2408.03511","title":"MoExtend: Tuning New Experts for Modality and Task Extension","authors":"Shanshan Zhong, Shanghua Gao, Zhongzhan Huang, Wushao Wen, Marinka\n  Zitnik, Pan Zhou","authorsParsed":[["Zhong","Shanshan",""],["Gao","Shanghua",""],["Huang","Zhongzhan",""],["Wen","Wushao",""],["Zitnik","Marinka",""],["Zhou","Pan",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 02:28:37 GMT"}],"updateDate":"2024-08-08","timestamp":1722997717000,"abstract":"  Large language models (LLMs) excel in various tasks but are primarily trained\non text data, limiting their application scope. Expanding LLM capabilities to\ninclude vision-language understanding is vital, yet training them on multimodal\ndata from scratch is challenging and costly. Existing instruction tuning\nmethods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs\nvia fully fine-tuning LLMs to bridge the modality gap. However, full\nfine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous\nknowledge, and high training costs particularly in the era of increasing tasks\nand modalities. To solve this issue, we introduce MoExtend, an effective\nframework designed to streamline the modality adaptation and extension of\nMixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts\ninto pre-trained MoE models, endowing them with novel knowledge without the\nneed to tune pretrained models such as MoE and vision encoders. This approach\nenables rapid adaptation and extension to new modal data or tasks, effectively\naddressing the challenge of accommodating new modalities within LLMs.\nFurthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk\nof catastrophic forgetting. Experimental results demonstrate the efficacy and\nefficiency of MoExtend in enhancing the multimodal capabilities of LLMs,\ncontributing to advancements in multimodal AI research. Code:\nhttps://github.com/zhongshsh/MoExtend.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}