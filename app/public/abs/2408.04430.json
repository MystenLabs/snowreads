{"id":"2408.04430","title":"Large Language Models for cross-language code clone detection","authors":"Micheline B\\'en\\'edicte Moumoula and Abdoul Kader Kabore and Jacques\n  Klein and Tegawend\\'e Bissyande","authorsParsed":[["Moumoula","Micheline Bénédicte",""],["Kabore","Abdoul Kader",""],["Klein","Jacques",""],["Bissyande","Tegawendé",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 12:57:14 GMT"}],"updateDate":"2024-08-13","timestamp":1723121834000,"abstract":"  With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction with the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection.\n  We investigate the capabilities of four (04) LLMs and eight (08) prompts for\nthe identification of cross-lingual code clones. Additionally, we evaluate a\npre-trained embedding model to assess the effectiveness of the generated\nrepresentations for classifying clone and non-clone pairs. Both studies (based\non LLMs and Embedding models) are evaluated using two widely used cross-lingual\ndatasets, XLCoST and CodeNet. Our results show that LLMs can achieve high F1\nscores, up to 0.98, for straightforward programming examples (e.g., from\nXLCoST). However, they not only perform less well on programs associated with\ncomplex programming challenges but also do not necessarily understand the\nmeaning of code clones in a cross-lingual setting. We show that embedding\nmodels used to represent code fragments from different programming languages in\nthe same representation space enable the training of a basic classifier that\noutperforms all LLMs by ~2 and ~24 percentage points on the XLCoST and CodeNet\ndatasets, respectively. This finding suggests that, despite the apparent\ncapabilities of LLMs, embeddings provided by embedding models offer suitable\nrepresentations to achieve state-of-the-art performance in cross-lingual code\nclone detection.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"kGNCaMiCCTWiD8TDIfwTmetrAmHqbSP5Ap-x1CCgmOI","pdfSize":"1626421"}
