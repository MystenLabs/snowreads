{"id":"2407.16237","title":"OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and\n  Self-Reflection","authors":"Fan Cui, Chenyang Yin, Kexing Zhou, Youwei Xiao, Guangyu Sun, Qiang\n  Xu, Qipeng Guo, Demin Song, Dahua Lin, Xingcheng Zhang and Yun (Eric) Liang","authorsParsed":[["Cui","Fan","","Eric"],["Yin","Chenyang","","Eric"],["Zhou","Kexing","","Eric"],["Xiao","Youwei","","Eric"],["Sun","Guangyu","","Eric"],["Xu","Qiang","","Eric"],["Guo","Qipeng","","Eric"],["Song","Demin","","Eric"],["Lin","Dahua","","Eric"],["Zhang","Xingcheng","","Eric"],["Yun","","","Eric"],["Liang","",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 07:22:25 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 07:25:21 GMT"}],"updateDate":"2024-09-04","timestamp":1721719345000,"abstract":"  Recent studies have demonstrated the significant potential of Large Language\nModels (LLMs) in generating Register Transfer Level (RTL) code, with notable\nadvancements showcased by commercial models such as GPT-4 and Claude3-Opus.\nHowever, these proprietary LLMs often raise concerns regarding privacy and\nsecurity. While open-source LLMs offer solutions to these concerns, they\ntypically underperform commercial models in RTL code generation tasks,\nprimarily due to the scarcity of high-quality open-source RTL datasets. To\naddress this challenge, we introduce OriGen , a fully open-source framework\nthat incorporates self-reflection capabilities and a novel dataset augmentation\nmethodology for generating high-quality, large-scale RTL code. Our approach\nemploys a code-tocode augmentation technique to enhance the quality of\nopen-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors\nthrough a self-reflection process that leverages compiler feedback.\nExperimental results demonstrate that OriGen significantly outperforms other\nopen-source alternatives in RTL code generation. It surpasses the previous\nbest-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the\npass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits\nsuperior capabilities in self-reflection and error correction, outperforming\nGPT-4 by 19.9% on a benchmark designed to evaluate self-reflection\ncapabilities.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"GX4KNehwSxEVCG_aH9JdyA6fBwUzhNCd59Dcczct2I4","pdfSize":"1002059"}
