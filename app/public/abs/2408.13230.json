{"id":"2408.13230","title":"Amortized Bayesian Multilevel Models","authors":"Daniel Habermann, Marvin Schmitt, Lars K\\\"uhmichel, Andreas Bulling,\n  Stefan T. Radev, Paul-Christian B\\\"urkner","authorsParsed":[["Habermann","Daniel",""],["Schmitt","Marvin",""],["Kühmichel","Lars",""],["Bulling","Andreas",""],["Radev","Stefan T.",""],["Bürkner","Paul-Christian",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 17:11:04 GMT"}],"updateDate":"2024-08-26","timestamp":1724433064000,"abstract":"  Multilevel models (MLMs) are a central building block of the Bayesian\nworkflow. They enable joint, interpretable modeling of data across hierarchical\nlevels and provide a fully probabilistic quantification of uncertainty. Despite\ntheir well-recognized advantages, MLMs pose significant computational\nchallenges, often rendering their estimation and evaluation intractable within\nreasonable time constraints. Recent advances in simulation-based inference\noffer promising solutions for addressing complex probabilistic models using\ndeep generative networks. However, the utility and reliability of deep learning\nmethods for estimating Bayesian MLMs remains largely unexplored, especially\nwhen compared with gold-standard samplers. To this end, we explore a family of\nneural network architectures that leverage the probabilistic factorization of\nmultilevel models to facilitate efficient neural network training and\nsubsequent near-instant posterior inference on unseen data sets. We test our\nmethod on several real-world case studies and provide comprehensive comparisons\nto Stan as a gold-standard method where possible. Finally, we provide an\nopen-source implementation of our methods to stimulate further research in the\nnascent field of amortized Bayesian inference.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Statistics/Computation"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}