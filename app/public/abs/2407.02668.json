{"id":"2407.02668","title":"MomentsNeRF: Leveraging Orthogonal Moments for Few-Shot Neural Rendering","authors":"Ahmad AlMughrabi, Ricardo Marques, Petia Radeva","authorsParsed":[["AlMughrabi","Ahmad",""],["Marques","Ricardo",""],["Radeva","Petia",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 21:02:48 GMT"}],"updateDate":"2024-07-04","timestamp":1719954168000,"abstract":"  We propose MomentsNeRF, a novel framework for one- and few-shot neural\nrendering that predicts a neural representation of a 3D scene using Orthogonal\nMoments. Our architecture offers a new transfer learning method to train on\nmulti-scenes and incorporate a per-scene optimization using one or a few images\nat test time. Our approach is the first to successfully harness features\nextracted from Gabor and Zernike moments, seamlessly integrating them into the\nNeRF architecture. We show that MomentsNeRF performs better in synthesizing\nimages with complex textures and shapes, achieving a significant noise\nreduction, artifact elimination, and completing the missing parts compared to\nthe recent one- and few-shot neural rendering frameworks. Extensive experiments\non the DTU and Shapenet datasets show that MomentsNeRF improves the\nstate-of-the-art by {3.39\\;dB\\;PSNR}, 11.1% SSIM, 17.9% LPIPS, and 8.3% DISTS\nmetrics. Moreover, it outperforms state-of-the-art performance for both novel\nview synthesis and single-image 3D view reconstruction. The source code is\naccessible at: https://amughrabi.github.io/momentsnerf/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}