{"id":"2408.05452","title":"EV-MGDispNet: Motion-Guided Event-Based Stereo Disparity Estimation\n  Network with Left-Right Consistency","authors":"Junjie Jiang, Hao Zhuang, Xinjie Huang, Delei Kong, Zheng Fang","authorsParsed":[["Jiang","Junjie",""],["Zhuang","Hao",""],["Huang","Xinjie",""],["Kong","Delei",""],["Fang","Zheng",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 06:13:37 GMT"}],"updateDate":"2024-08-13","timestamp":1723270417000,"abstract":"  Event cameras have the potential to revolutionize the field of robot vision,\nparticularly in areas like stereo disparity estimation, owing to their high\ntemporal resolution and high dynamic range. Many studies use deep learning for\nevent camera stereo disparity estimation. However, these methods fail to fully\nexploit the temporal information in the event stream to acquire clear event\nrepresentations. Additionally, there is room for further reduction in pixel\nshifts in the feature maps before constructing the cost volume. In this paper,\nwe propose EV-MGDispNet, a novel event-based stereo disparity estimation\nmethod. Firstly, we propose an edge-aware aggregation (EAA) module, which fuses\nevent frames and motion confidence maps to generate a novel clear event\nrepresentation. Then, we propose a motion-guided attention (MGA) module, where\nmotion confidence maps utilize deformable transformer encoders to enhance the\nfeature map with more accurate edges. Finally, we also add a census left-right\nconsistency loss function to enhance the left-right consistency of stereo event\nrepresentation. Through conducting experiments within challenging real-world\ndriving scenarios, we validate that our method outperforms currently known\nstate-of-the-art methods in terms of mean absolute error (MAE) and root mean\nsquare error (RMSE) metrics.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}