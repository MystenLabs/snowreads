{"id":"2408.02584","title":"Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality\n  Aspect-Based Summarization","authors":"Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Aditya\n  Vempaty, Pawan Goyal, Niloy Ganguly, Prasenjit Dey, Ravi Kokku","authorsParsed":[["Mullick","Ankan",""],["Bose","Sombit",""],["Saha","Rounak",""],["Bhowmick","Ayan Kumar",""],["Vempaty","Aditya",""],["Goyal","Pawan",""],["Ganguly","Niloy",""],["Dey","Prasenjit",""],["Kokku","Ravi",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 16:00:21 GMT"}],"updateDate":"2024-08-06","timestamp":1722873621000,"abstract":"  The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"RV7OQOuG2wQAmfPelTBIvvljUYvpJQm1MMQfJV-3DMc","pdfSize":"391462"}
