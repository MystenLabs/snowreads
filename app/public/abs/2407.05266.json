{"id":"2407.05266","title":"CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training\n  Quantization of ViTs","authors":"Akshat Ramachandran, Souvik Kundu, Tushar Krishna","authorsParsed":[["Ramachandran","Akshat",""],["Kundu","Souvik",""],["Krishna","Tushar",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 05:39:25 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 00:08:36 GMT"}],"updateDate":"2024-09-10","timestamp":1720330765000,"abstract":"  We present CLAMP-ViT, a data-free post-training quantization method for\nvision transformers (ViTs). We identify the limitations of recent techniques,\nnotably their inability to leverage meaningful inter-patch relationships,\nleading to the generation of simplistic and semantically vague data, impacting\nquantization accuracy. CLAMP-ViT employs a two-stage approach, cyclically\nadapting between data generation and model quantization. Specifically, we\nincorporate a patch-level contrastive learning scheme to generate richer,\nsemantically meaningful data. Furthermore, we leverage contrastive learning in\nlayer-wise evolutionary search for fixed- and mixed-precision quantization to\nidentify optimal quantization parameters while mitigating the effects of a\nnon-smooth loss landscape. Extensive evaluations across various vision tasks\ndemonstrate the superiority of CLAMP-ViT, with performance improvements of up\nto 3% in top-1 accuracy for classification, 0.6 mAP for object detection, and\n1.5 mIoU for segmentation at similar or better compression ratio over existing\nalternatives. Code is available at\nhttps://github.com/georgia-tech-synergy-lab/CLAMP-ViT.git\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}