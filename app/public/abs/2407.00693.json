{"id":"2407.00693","title":"BAPO: Base-Anchored Preference Optimization for Personalized Alignment\n  in Large Language Models","authors":"Gihun Lee, Minchan Jeong, Yujin Kim, Hojung Jung, Jaehoon Oh, Sangmook\n  Kim, Se-Young Yun","authorsParsed":[["Lee","Gihun",""],["Jeong","Minchan",""],["Kim","Yujin",""],["Jung","Hojung",""],["Oh","Jaehoon",""],["Kim","Sangmook",""],["Yun","Se-Young",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 13:30:04 GMT"}],"updateDate":"2024-07-02","timestamp":1719754204000,"abstract":"  While learning to align Large Language Models (LLMs) with human preferences\nhas shown remarkable success, aligning these models to meet the diverse user\npreferences presents further challenges in preserving previous knowledge. This\npaper examines the impact of personalized preference optimization on LLMs,\nrevealing that the extent of knowledge loss varies significantly with\npreference heterogeneity. Although previous approaches have utilized the KL\nconstraint between the reference model and the policy model, we observe that\nthey fail to maintain general knowledge and alignment when facing personalized\npreferences. To this end, we introduce Base-Anchored Preference Optimization\n(BAPO), a simple yet effective approach that utilizes the initial responses of\nreference model to mitigate forgetting while accommodating personalized\nalignment. BAPO effectively adapts to diverse user preferences while minimally\naffecting global knowledge or general alignment. Our experiments demonstrate\nthe efficacy of BAPO in various setups.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4-hhzmiodjUGCStYHCHpZpnngD6BqLfR_-ExyfVkZsw","pdfSize":"1797337"}
