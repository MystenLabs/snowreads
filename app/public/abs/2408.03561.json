{"id":"2408.03561","title":"MPC-Minimized Secure LLM Inference","authors":"Deevashwer Rathee, Dacheng Li, Ion Stoica, Hao Zhang, Raluca Popa","authorsParsed":[["Rathee","Deevashwer",""],["Li","Dacheng",""],["Stoica","Ion",""],["Zhang","Hao",""],["Popa","Raluca",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 05:50:17 GMT"}],"updateDate":"2024-08-08","timestamp":1723009817000,"abstract":"  Many inference services based on large language models (LLMs) pose a privacy\nconcern, either revealing user prompts to the service or the proprietary\nweights to the user. Secure inference offers a solution to this problem through\nsecure multi-party computation (MPC), however, it is still impractical for\nmodern LLM workload due to the large overhead imposed by MPC. To address this\noverhead, we propose Marill, a framework that adapts LLM fine-tuning to\nminimize MPC usage during secure inference. Marill introduces high-level\narchitectural changes during fine-tuning that significantly reduce the number\nof expensive operations needed within MPC during inference, by removing some\nand relocating others outside MPC without compromising security. As a result,\nMarill-generated models are more efficient across all secure inference\nprotocols and our approach complements MPC-friendly approximations for such\noperations. Compared to standard fine-tuning, Marill results in 3.6-11.3x\nbetter runtime and 2.4-6.9x better communication during secure inference across\nvarious MPC settings, while typically preserving over 90% performance across\ndownstream tasks.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}