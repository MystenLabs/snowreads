{"id":"2408.12332","title":"Simplifying Random Forests' Probabilistic Forecasts","authors":"Nils Koster and Fabian Kr\\\"uger","authorsParsed":[["Koster","Nils",""],["Kr√ºger","Fabian",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 12:20:17 GMT"}],"updateDate":"2024-08-23","timestamp":1724329217000,"abstract":"  Since their introduction by Breiman, Random Forests (RFs) have proven to be\nuseful for both classification and regression tasks. The RF prediction of a\npreviously unseen observation can be represented as a weighted sum of all\ntraining sample observations. This nearest-neighbor-type representation is\nuseful, among other things, for constructing forecast distributions\n(Meinshausen, 2006). In this paper, we consider simplifying RF-based forecast\ndistributions by sparsifying them. That is, we focus on a small subset of\nnearest neighbors while setting the remaining weights to zero. This\nsparsification step greatly improves the interpretability of RF predictions. It\ncan be applied to any forecasting task without re-training existing RF models.\nIn empirical experiments, we document that the simplified predictions can be\nsimilar to or exceed the original ones in terms of forecasting performance. We\nexplore the statistical sources of this finding via a stylized analytical model\nof RFs. The model suggests that simplification is particularly promising if the\nunknown true forecast distribution contains many small weights that are\nestimated imprecisely.\n","subjects":["Statistics/Applications","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}