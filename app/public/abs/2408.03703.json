{"id":"2408.03703","title":"CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications","authors":"Tianfang Zhang, Lei Li, Yang Zhou, Wentao Liu, Chen Qian, Xiangyang Ji","authorsParsed":[["Zhang","Tianfang",""],["Li","Lei",""],["Zhou","Yang",""],["Liu","Wentao",""],["Qian","Chen",""],["Ji","Xiangyang",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 11:33:46 GMT"}],"updateDate":"2024-08-08","timestamp":1723030426000,"abstract":"  Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we construct a novel additive\nsimilarity function following this paradigm and present an efficient\nimplementation named Convolutional Additive Token Mixer (CATM). This\nsimplification leads to a significant reduction in computational overhead. We\nevaluate CAS-ViT across a variety of vision tasks, including image\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Our experiments, conducted on GPUs, ONNX, and iPhones,\ndemonstrate that CAS-ViT achieves a competitive performance when compared to\nother state-of-the-art backbones, establishing it as a viable option for\nefficient mobile vision applications. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}