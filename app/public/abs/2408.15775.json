{"id":"2408.15775","title":"Easy, Interpretable, Effective: openSMILE for voice deepfake detection","authors":"Octavian Pascu, Dan Oneata, Horia Cucu, Nicolas M. M\\\"uller","authorsParsed":[["Pascu","Octavian",""],["Oneata","Dan",""],["Cucu","Horia",""],["MÃ¼ller","Nicolas M.",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 13:14:18 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 11:58:35 GMT"}],"updateDate":"2024-08-30","timestamp":1724850858000,"abstract":"  In this paper, we demonstrate that attacks in the latest ASVspoof5 dataset --\na de facto standard in the field of voice authenticity and deepfake detection\n-- can be identified with surprising accuracy using a small subset of very\nsimplistic features. These are derived from the openSMILE library, and are\nscalar-valued, easy to compute, and human interpretable. For example, attack\nA10`s unvoiced segments have a mean length of 0.09 +- 0.02, while bona fide\ninstances have a mean length of 0.18 +- 0.07. Using this feature alone, a\nthreshold classifier achieves an Equal Error Rate (EER) of 10.3% for attack\nA10. Similarly, across all attacks, we achieve up to 0.8% EER, with an overall\nEER of 15.7 +- 6.0%. We explore the generalization capabilities of these\nfeatures and find that some of them transfer effectively between attacks,\nprimarily when the attacks originate from similar Text-to-Speech (TTS)\narchitectures. This finding may indicate that voice anti-spoofing is, in part,\na problem of identifying and remembering signatures or fingerprints of\nindividual TTS systems. This allows to better understand anti-spoofing models\nand their challenges in real-world application.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}