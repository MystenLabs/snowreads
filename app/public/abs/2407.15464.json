{"id":"2407.15464","title":"The Diversity Bonus: Learning from Dissimilar Distributed Clients in\n  Personalized Federated Learning","authors":"Xinghao Wu, Xuefeng Liu, Jianwei Niu, Guogang Zhu, Shaojie Tang,\n  Xiaotian Li, and Jiannong Cao","authorsParsed":[["Wu","Xinghao",""],["Liu","Xuefeng",""],["Niu","Jianwei",""],["Zhu","Guogang",""],["Tang","Shaojie",""],["Li","Xiaotian",""],["Cao","Jiannong",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 08:24:45 GMT"}],"updateDate":"2024-07-23","timestamp":1721636685000,"abstract":"  Personalized Federated Learning (PFL) is a commonly used framework that\nallows clients to collaboratively train their personalized models. PFL is\nparticularly useful for handling situations where data from different clients\nare not independent and identically distributed (non-IID). Previous research in\nPFL implicitly assumes that clients can gain more benefits from those with\nsimilar data distributions. Correspondingly, methods such as personalized\nweight aggregation are developed to assign higher weights to similar clients\nduring training. We pose a question: can a client benefit from other clients\nwith dissimilar data distributions and if so, how? This question is\nparticularly relevant in scenarios with a high degree of non-IID, where clients\nhave widely different data distributions, and learning from only similar\nclients will lose knowledge from many other clients. We note that when dealing\nwith clients with similar data distributions, methods such as personalized\nweight aggregation tend to enforce their models to be close in the parameter\nspace. It is reasonable to conjecture that a client can benefit from dissimilar\nclients if we allow their models to depart from each other. Based on this idea,\nwe propose DiversiFed which allows each client to learn from clients with\ndiversified data distribution in personalized federated learning. DiversiFed\npushes personalized models of clients with dissimilar data distributions apart\nin the parameter space while pulling together those with similar distributions.\nIn addition, to achieve the above effect without using prior knowledge of data\ndistribution, we design a loss function that leverages the model similarity to\ndetermine the degree of attraction and repulsion between any two models.\nExperiments on several datasets show that DiversiFed can benefit from\ndissimilar clients and thus outperform the state-of-the-art methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}