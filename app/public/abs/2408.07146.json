{"id":"2408.07146","title":"Vision Language Model for Interpretable and Fine-grained Detection of\n  Safety Compliance in Diverse Workplaces","authors":"Zhiling Chen, Hanning Chen, Mohsen Imani, Ruimin Chen, and Farhad\n  Imani","authorsParsed":[["Chen","Zhiling",""],["Chen","Hanning",""],["Imani","Mohsen",""],["Chen","Ruimin",""],["Imani","Farhad",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 18:32:06 GMT"}],"updateDate":"2024-08-15","timestamp":1723573926000,"abstract":"  Workplace accidents due to personal protective equipment (PPE) non-compliance\nraise serious safety concerns and lead to legal liabilities, financial\npenalties, and reputational damage. While object detection models have shown\nthe capability to address this issue by identifying safety items, most existing\nmodels, such as YOLO, Faster R-CNN, and SSD, are limited in verifying the\nfine-grained attributes of PPE across diverse workplace scenarios. Vision\nlanguage models (VLMs) are gaining traction for detection tasks by leveraging\nthe synergy between visual and textual information, offering a promising\nsolution to traditional object detection limitations in PPE recognition.\nNonetheless, VLMs face challenges in consistently verifying PPE attributes due\nto the complexity and variability of workplace environments, requiring them to\ninterpret context-specific language and visual cues simultaneously. We\nintroduce Clip2Safety, an interpretable detection framework for diverse\nworkplace safety compliance, which comprises four main modules: scene\nrecognition, the visual prompt, safety items detection, and fine-grained\nverification. The scene recognition identifies the current scenario to\ndetermine the necessary safety gear. The visual prompt formulates the specific\nvisual prompts needed for the detection process. The safety items detection\nidentifies whether the required safety gear is being worn according to the\nspecified scenario. Lastly, the fine-grained verification assesses whether the\nworn safety equipment meets the fine-grained attribute requirements. We conduct\nreal-world case studies across six different scenarios. The results show that\nClip2Safety not only demonstrates an accuracy improvement over state-of-the-art\nquestion-answering based VLMs but also achieves inference times two hundred\ntimes faster.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}