{"id":"2408.08855","title":"DPA: Dual Prototypes Alignment for Unsupervised Adaptation of\n  Vision-Language Models","authors":"Eman Ali and Sathira Silva and Muhammad Haris Khan","authorsParsed":[["Ali","Eman",""],["Silva","Sathira",""],["Khan","Muhammad Haris",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 17:30:27 GMT"}],"updateDate":"2024-08-19","timestamp":1723829427000,"abstract":"  Vision-language models (VLMs), e.g., CLIP, have shown remarkable potential in\nzero-shot image classification. However, adapting these models to new domains\nremains challenging, especially in unsupervised settings where labelled data is\nunavailable. Recent research has proposed pseudo-labelling approaches to adapt\nCLIP in an unsupervised manner using unlabelled target data. Nonetheless, these\nmethods struggle due to noisy pseudo-labels resulting from the misalignment\nbetween CLIP's visual and textual representations. This study introduces DPA,\nan unsupervised domain adaptation method for VLMs. DPA introduces the concept\nof dual prototypes, acting as distinct classifiers, along with the convex\ncombination of their outputs, thereby leading to accurate pseudo-label\nconstruction. Next, it ranks pseudo-labels to facilitate robust self-training,\nparticularly during early training. Finally, it addresses visual-textual\nmisalignment by aligning textual prototypes with image prototypes to further\nimprove the adaptation performance. Experiments on 13 downstream vision tasks\ndemonstrate that DPA significantly outperforms zero-shot CLIP and the\nstate-of-the-art unsupervised adaptation baselines.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}