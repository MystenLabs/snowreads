{"id":"2407.17493","title":"ReDiFine: Reusable Diffusion Finetuning for Mitigating Degradation in\n  the Chain of Diffusion","authors":"Youngseok Yoon, Dainong Hu, Iain Weissburg, Yao Qin, Haewon Jeong","authorsParsed":[["Yoon","Youngseok",""],["Hu","Dainong",""],["Weissburg","Iain",""],["Qin","Yao",""],["Jeong","Haewon",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 13:41:54 GMT"}],"updateDate":"2024-07-26","timestamp":1720100514000,"abstract":"  Diffusion models have achieved tremendous improvements in generative modeling\nfor images, enabling high-quality generation that is indistinguishable by\nhumans from real images. The qualities of images have reached a threshold at\nwhich we can reuse synthetic images for training machine learning models again.\nThis attracts the area as it can relieve the high cost of data collection and\nfundamentally solve many problems in data-limited areas. In this paper, we\nfocus on a practical scenario in which pretrained text-to-image diffusion\nmodels are iteratively finetuned using a set of synthetic images, which we call\nthe Chain of Diffusion. Finetuned models generate images that are used for the\nnext iteration of finetuning. We first demonstrate how these iterative\nprocesses result in severe degradation in image qualities. Thorough\ninvestigations reveal the most impactful factor for the degradation, and we\npropose finetuning and generation strategies that can effectively resolve the\ndegradation. Our method, Reusable Diffusion Finetuning (ReDiFine), combines\ncondition drop finetuning and CFG scheduling to maintain the qualities of\ngenerated images throughout iterations. ReDiFine works effectively for multiple\ndatasets and models without further hyperparameter search, making synthetic\nimages reusable to finetune future generative models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}