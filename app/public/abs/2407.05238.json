{"id":"2407.05238","title":"P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for\n  LiDAR Point Clouds","authors":"Jiahao Nie, Fei Xie, Sifan Zhou, Xueyi Zhou, Dong-Kyu Chae, Zhiwei He","authorsParsed":[["Nie","Jiahao",""],["Xie","Fei",""],["Zhou","Sifan",""],["Zhou","Xueyi",""],["Chae","Dong-Kyu",""],["He","Zhiwei",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 02:37:24 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 02:15:52 GMT"}],"updateDate":"2024-07-10","timestamp":1720319844000,"abstract":"  3D single object tracking (SOT) methods based on appearance matching has long\nsuffered from insufficient appearance information incurred by incomplete,\ntextureless and semantically deficient LiDAR point clouds. While motion\nparadigm exploits motion cues instead of appearance matching for tracking, it\nincurs complex multi-stage processing and segmentation module. In this paper,\nwe first provide in-depth explorations on motion paradigm, which proves that\n(\\textbf{i}) it is feasible to directly infer target relative motion from point\nclouds across consecutive frames; (\\textbf{ii}) fine-grained information\ncomparison between consecutive point clouds facilitates target motion modeling.\nWe thereby propose to perform part-to-part motion modeling for consecutive\npoint clouds and introduce a novel tracking framework, termed \\textbf{P2P}. The\nnovel framework fuses each corresponding part information between consecutive\npoint clouds, effectively exploring detailed information changes and thus\nmodeling accurate target-related motion cues. Following this framework, we\npresent P2P-point and P2P-voxel models, incorporating implicit and explicit\npart-to-part motion modeling by point- and voxel-based representation,\nrespectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art\nperformance ($\\sim$\\textbf{89\\%}, \\textbf{72\\%} and \\textbf{63\\%} precision on\nKITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same\npoint-based representation, P2P-point outperforms the previous motion tracker\nM$^2$Track by \\textbf{3.3\\%} and \\textbf{6.7\\%} on the KITTI and NuScenes,\nwhile running at a considerably high speed of \\textbf{107 Fps} on a single\nRTX3090 GPU. The source code and pre-trained models are available at\n\\url{https://github.com/haooozi/P2P}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}