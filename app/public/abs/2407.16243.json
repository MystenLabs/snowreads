{"id":"2407.16243","title":"Chameleon: Images Are What You Need For Multimodal Learning Robust To\n  Missing Modalities","authors":"Muhammad Irzam Liaqat, Shah Nawaz, Muhammad Zaigham Zaheer, Muhammad\n  Saad Saeed, Hassan Sajjad, Tom De Schepper, Karthik Nandakumar, Muhammad\n  Haris Khan Markus Schedl","authorsParsed":[["Liaqat","Muhammad Irzam",""],["Nawaz","Shah",""],["Zaheer","Muhammad Zaigham",""],["Saeed","Muhammad Saad",""],["Sajjad","Hassan",""],["De Schepper","Tom",""],["Nandakumar","Karthik",""],["Schedl","Muhammad Haris Khan Markus",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 07:29:57 GMT"}],"updateDate":"2024-07-24","timestamp":1721719797000,"abstract":"  Multimodal learning has demonstrated remarkable performance improvements over\nunimodal architectures. However, multimodal learning methods often exhibit\ndeteriorated performances if one or more modalities are missing. This may be\nattributed to the commonly used multi-branch design containing\nmodality-specific streams making the models reliant on the availability of a\ncomplete set of modalities. In this work, we propose a robust textual-visual\nmultimodal learning method, Chameleon, that completely deviates from the\nconventional multi-branch design. To enable this, we present the unification of\ninput modalities into one format by encoding textual modality into visual\nrepresentations. As a result, our approach does not require modality-specific\nbranches to learn modality-independent multimodal representations making it\nrobust to missing modalities. Extensive experiments are performed on four\npopular challenging datasets including Hateful Memes, UPMC Food-101, MM-IMDb,\nand Ferramenta. Chameleon not only achieves superior performance when all\nmodalities are present at train/test time but also demonstrates notable\nresilience in the case of missing modalities.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}