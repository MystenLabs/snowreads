{"id":"2407.10026","title":"Conditional Entropies of k-Deletion/Insertion Channels","authors":"Shubhransh Singhvi, Omer Sabary, Daniella Bar-Lev and Eitan Yaakobi","authorsParsed":[["Singhvi","Shubhransh",""],["Sabary","Omer",""],["Bar-Lev","Daniella",""],["Yaakobi","Eitan",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 23:04:56 GMT"}],"updateDate":"2024-07-16","timestamp":1720911896000,"abstract":"  The channel output entropy of a transmitted sequence is the entropy of the\npossible channel outputs and similarly the channel input entropy of a received\nsequence is the entropy of all possible transmitted sequences. The goal of this\nwork is to study these entropy values for the k-deletion, k-insertion channels,\nwhere exactly k symbols are deleted, inserted in the transmitted sequence,\nrespectively. If all possible sequences are transmitted with the same\nprobability then studying the input and output entropies is equivalent. For\nboth the 1-deletion and 1-insertion channels, it is proved that among all\nsequences with a fixed number of runs, the input entropy is minimized for\nsequences with a skewed distribution of their run lengths and it is maximized\nfor sequences with a balanced distribution of their run lengths. Among our\nresults, we establish a conjecture by Atashpendar et al. which claims that for\nthe 1-deletion channel, the input entropy is maximized by the alternating\nsequences over all binary sequences. This conjecture is also verified for the\n2-deletion channel, where it is proved that constant sequences with a single\nrun minimize the input entropy.\n","subjects":["Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}