{"id":"2407.12798","title":"Multi-Granularity and Multi-modal Feature Interaction Approach for Text\n  Video Retrieval","authors":"Wenjun Li, Shudong Wang, Dong Zhao, Shenghui Xu, Zhaoming Pan, Zhimin\n  Zhang","authorsParsed":[["Li","Wenjun",""],["Wang","Shudong",""],["Zhao","Dong",""],["Xu","Shenghui",""],["Pan","Zhaoming",""],["Zhang","Zhimin",""]],"versions":[{"version":"v1","created":"Fri, 21 Jun 2024 02:28:06 GMT"}],"updateDate":"2024-07-19","timestamp":1718936886000,"abstract":"  The key of the text-to-video retrieval (TVR) task lies in learning the unique\nsimilarity between each pair of text (consisting of words) and video\n(consisting of audio and image frames) representations. However, some problems\nexist in the representation alignment of video and text, such as a text, and\nfurther each word, are of different importance for video frames. Besides, audio\nusually carries additional or critical information for TVR in the case that\nframes carry little valid information. Therefore, in TVR task,\nmulti-granularity representation of text, including whole sentence and every\nword, and the modal of audio are salutary which are underutilized in most\nexisting works. To address this, we propose a novel multi-granularity feature\ninteraction module called MGFI, consisting of text-frame and word-frame, for\nvideo-text representations alignment. Moreover, we introduce a cross-modal\nfeature interaction module of audio and text called CMFI to solve the problem\nof insufficient expression of frames in the video. Experiments on benchmark\ndatasets such as MSR-VTT, MSVD, DiDeMo show that the proposed method\noutperforms the existing state-of-the-art methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}