{"id":"2408.13976","title":"Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates","authors":"Zhihong Sun, Yao Wan, Jia Li, Hongyu Zhang, Zhi Jin, Ge Li, Chen Lyu","authorsParsed":[["Sun","Zhihong",""],["Wan","Yao",""],["Li","Jia",""],["Zhang","Hongyu",""],["Jin","Zhi",""],["Li","Ge",""],["Lyu","Chen",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 01:48:57 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 06:49:19 GMT"},{"version":"v3","created":"Thu, 19 Sep 2024 13:16:34 GMT"}],"updateDate":"2024-09-20","timestamp":1724636937000,"abstract":"  Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Existing approaches\ntypically generate multiple candidate solutions using LLMs to increase the\nlikelihood of producing correct code. However, selecting the correct code from\nthese candidates-a process known as code ranking-remains a major challenge.\nCurrent research on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to truly comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}