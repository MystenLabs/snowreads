{"id":"2407.04075","title":"Sparsest Models Elude Pruning: An Expos\\'e of Pruning's Current\n  Capabilities","authors":"Stephen Zhang, Vardan Papyan","authorsParsed":[["Zhang","Stephen",""],["Papyan","Vardan",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 17:33:15 GMT"}],"updateDate":"2024-07-08","timestamp":1720114395000,"abstract":"  Pruning has emerged as a promising approach for compressing large-scale\nmodels, yet its effectiveness in recovering the sparsest of models has not yet\nbeen explored. We conducted an extensive series of 485,838 experiments,\napplying a range of state-of-the-art pruning algorithms to a synthetic dataset\nwe created, named the Cubist Spiral. Our findings reveal a significant gap in\nperformance compared to ideal sparse networks, which we identified through a\nnovel combinatorial search algorithm. We attribute this performance gap to\ncurrent pruning algorithms' poor behaviour under overparameterization, their\ntendency to induce disconnected paths throughout the network, and their\npropensity to get stuck at suboptimal solutions, even when given the optimal\nwidth and initialization. This gap is concerning, given the simplicity of the\nnetwork architectures and datasets used in our study. We hope that our research\nencourages further investigation into new pruning techniques that strive for\ntrue network sparsity.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}