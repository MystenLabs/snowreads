{"id":"2408.12337","title":"Fine-tuning Smaller Language Models for Question Answering over\n  Financial Documents","authors":"Karmvir Singh Phogat, Sai Akhil Puranam, Sridhar Dasaratha, Chetan\n  Harsha, Shashishekar Ramakrishna","authorsParsed":[["Phogat","Karmvir Singh",""],["Puranam","Sai Akhil",""],["Dasaratha","Sridhar",""],["Harsha","Chetan",""],["Ramakrishna","Shashishekar",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 12:23:29 GMT"}],"updateDate":"2024-08-23","timestamp":1724329409000,"abstract":"  Recent research has shown that smaller language models can acquire\nsubstantial reasoning abilities when fine-tuned with reasoning exemplars\ncrafted by a significantly larger teacher model. We explore this paradigm for\nthe financial domain, focusing on the challenge of answering questions that\nrequire multi-hop numerical reasoning over financial texts. We assess the\nperformance of several smaller models that have been fine-tuned to generate\nprograms that encode the required financial reasoning and calculations. Our\nfindings demonstrate that these fine-tuned smaller models approach the\nperformance of the teacher model.\n  To provide a granular analysis of model performance, we propose an approach\nto investigate the specific student model capabilities that are enhanced by\nfine-tuning. Our empirical analysis indicates that fine-tuning refines the\nstudent models ability to express and apply the required financial concepts\nalong with adapting the entity extraction for the specific data format. In\naddition, we hypothesize and demonstrate that comparable financial reasoning\ncapability can be induced using relatively smaller datasets.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}