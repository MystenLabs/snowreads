{"id":"2408.09765","title":"Baby Bear: Seeking a Just Right Rating Scale for Scalar Annotations","authors":"Xu Han, Felix Yu, Joao Sedoc, Benjamin Van Durme","authorsParsed":[["Han","Xu",""],["Yu","Felix",""],["Sedoc","Joao",""],["Van Durme","Benjamin",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 07:53:50 GMT"}],"updateDate":"2024-08-20","timestamp":1724054030000,"abstract":"  Our goal is a mechanism for efficiently assigning scalar ratings to each of a\nlarge set of elements. For example, \"what percent positive or negative is this\nproduct review?\" When sample sizes are small, prior work has advocated for\nmethods such as Best Worst Scaling (BWS) as being more robust than direct\nordinal annotation (\"Likert scales\"). Here we first introduce IBWS, which\niteratively collects annotations through Best-Worst Scaling, resulting in\nrobustly ranked crowd-sourced data. While effective, IBWS is too expensive for\nlarge-scale tasks. Using the results of IBWS as a best-desired outcome, we\nevaluate various direct assessment methods to determine what is both\ncost-efficient and best correlating to a large scale BWS annotation strategy.\nFinally, we illustrate in the domains of dialogue and sentiment how these\nannotations can support robust learning-to-rank models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}