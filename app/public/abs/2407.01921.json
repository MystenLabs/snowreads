{"id":"2407.01921","title":"GVDIFF: Grounded Text-to-Video Generation with Diffusion Models","authors":"Huanzhang Dou, Ruixiang Li, Wei Su, and Xi Li","authorsParsed":[["Dou","Huanzhang",""],["Li","Ruixiang",""],["Su","Wei",""],["Li","Xi",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 03:36:23 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 06:54:12 GMT"}],"updateDate":"2024-07-08","timestamp":1719891383000,"abstract":"  In text-to-video (T2V) generation, significant attention has been directed\ntoward its development, yet unifying discrete and continuous grounding\nconditions in T2V generation remains under-explored. This paper proposes a\nGrounded text-to-Video generation framework, termed GVDIFF. First, we inject\nthe grounding condition into the self-attention through an uncertainty-based\nrepresentation to explicitly guide the focus of the network. Second, we\nintroduce a spatial-temporal grounding layer that connects the grounding\ncondition with target objects and enables the model with the grounded\ngeneration capacity in the spatial-temporal domain. Third, our dynamic gate\nnetwork adaptively skips the redundant grounding process to selectively extract\ngrounding information and semantics while improving efficiency. We extensively\nevaluate the grounded generation capacity of GVDIFF and demonstrate its\nversatility in applications, including long-range video generation, sequential\nprompts, and object-specific editing.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}