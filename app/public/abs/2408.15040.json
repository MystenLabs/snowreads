{"id":"2408.15040","title":"A Survey of Large Language Models for European Languages","authors":"Wazir Ali and Sampo Pyysalo","authorsParsed":[["Ali","Wazir",""],["Pyysalo","Sampo",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 13:10:05 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 03:56:37 GMT"}],"updateDate":"2024-08-29","timestamp":1724764205000,"abstract":"  Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}