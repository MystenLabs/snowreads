{"id":"2407.15155","title":"Distilling Vision-Language Foundation Models: A Data-Free Approach via\n  Prompt Diversification","authors":"Yunyi Xuan, Weijie Chen, Shicai Yang, Di Xie, Luojun Lin, Yueting\n  Zhuang","authorsParsed":[["Xuan","Yunyi",""],["Chen","Weijie",""],["Yang","Shicai",""],["Xie","Di",""],["Lin","Luojun",""],["Zhuang","Yueting",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 13:26:30 GMT"}],"updateDate":"2024-07-23","timestamp":1721568390000,"abstract":"  Data-Free Knowledge Distillation (DFKD) has shown great potential in creating\na compact student model while alleviating the dependency on real training data\nby synthesizing surrogate data. However, prior arts are seldom discussed under\ndistribution shifts, which may be vulnerable in real-world applications. Recent\nVision-Language Foundation Models, e.g., CLIP, have demonstrated remarkable\nperformance in zero-shot out-of-distribution generalization, yet consuming\nheavy computation resources. In this paper, we discuss the extension of DFKD to\nVision-Language Foundation Models without access to the billion-level\nimage-text datasets. The objective is to customize a student model for\ndistribution-agnostic downstream tasks with given category concepts, inheriting\nthe out-of-distribution generalization capability from the pre-trained\nfoundation models. In order to avoid generalization degradation, the primary\nchallenge of this task lies in synthesizing diverse surrogate images driven by\ntext prompts. Since not only category concepts but also style information are\nencoded in text prompts, we propose three novel Prompt Diversification methods\nto encourage image synthesis with diverse styles, namely Mix-Prompt,\nRandom-Prompt, and Contrastive-Prompt. Experiments on out-of-distribution\ngeneralization datasets demonstrate the effectiveness of the proposed methods,\nwith Contrastive-Prompt performing the best.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}