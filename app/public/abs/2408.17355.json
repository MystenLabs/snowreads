{"id":"2408.17355","title":"Bidirectional Decoding: Improving Action Chunking via Closed-Loop\n  Resampling","authors":"Yuejiang Liu, Jubayer Ibn Hamid, Annie Xie, Yoonho Lee, Maximilian Du,\n  Chelsea Finn","authorsParsed":[["Liu","Yuejiang",""],["Hamid","Jubayer Ibn",""],["Xie","Annie",""],["Lee","Yoonho",""],["Du","Maximilian",""],["Finn","Chelsea",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 15:39:34 GMT"}],"updateDate":"2024-09-02","timestamp":1725032374000,"abstract":"  Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. However, its effects on learned policies remain\npuzzling: some studies highlight its importance for achieving strong\nperformance, while others observe detrimental effects. In this paper, we first\ndissect the role of action chunking by analyzing the divergence between the\nlearner and the demonstrator. We find that longer action chunks enable a policy\nto better capture temporal dependencies by taking into account more past states\nand actions within the chunk. However, this advantage comes at the cost of\nexacerbating errors in stochastic environments due to fewer observations of\nrecent states. To address this, we propose Bidirectional Decoding (BID), a\ntest-time inference algorithm that bridges action chunking with closed-loop\noperations. BID samples multiple predictions at each time step and searches for\nthe optimal one based on two criteria: (i) backward coherence, which favors\nsamples aligned with previous decisions, (ii) forward contrast, which favors\nsamples close to outputs of a stronger policy and distant from those of a\nweaker policy. By coupling decisions within and across action chunks, BID\nenhances temporal consistency over extended sequences while enabling adaptive\nreplanning in stochastic environments. Experimental results show that BID\nsubstantially outperforms conventional closed-loop operations of two\nstate-of-the-art generative policies across seven simulation benchmarks and two\nreal-world tasks.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}