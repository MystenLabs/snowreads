{"id":"2408.13790","title":"CV-MOS: A Cross-View Model for Motion Segmentation","authors":"Xiaoyu Tang, Zeyu Chen, Jintao Cheng, Xieyuanli Chen, Jin Wu, Bohuan\n  Xue","authorsParsed":[["Tang","Xiaoyu",""],["Chen","Zeyu",""],["Cheng","Jintao",""],["Chen","Xieyuanli",""],["Wu","Jin",""],["Xue","Bohuan",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 09:39:26 GMT"}],"updateDate":"2024-08-27","timestamp":1724578766000,"abstract":"  In autonomous driving, accurately distinguishing between static and moving\nobjects is crucial for the autonomous driving system. When performing the\nmotion object segmentation (MOS) task, effectively leveraging motion\ninformation from objects becomes a primary challenge in improving the\nrecognition of moving objects. Previous methods either utilized range view (RV)\nor bird's eye view (BEV) residual maps to capture motion information. Unlike\ntraditional approaches, we propose combining RV and BEV residual maps to\nexploit a greater potential of motion information jointly. Thus, we introduce\nCV-MOS, a cross-view model for moving object segmentation. Novelty, we decouple\nspatial-temporal information by capturing the motion from BEV and RV residual\nmaps and generating semantic features from range images, which are used as\nmoving object guidance for the motion branch. Our direct and unique solution\nmaximizes the use of range images and RV and BEV residual maps, significantly\nenhancing the performance of LiDAR-based MOS task. Our method achieved leading\nIoU(\\%) scores of 77.5\\% and 79.2\\% on the validation and test sets of the\nSemanticKitti dataset. In particular, CV-MOS demonstrates SOTA performance to\ndate on various datasets. The CV-MOS implementation is available at\nhttps://github.com/SCNU-RISLAB/CV-MOS\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}