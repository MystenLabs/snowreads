{"id":"2408.01766","title":"MultiFuser: Multimodal Fusion Transformer for Enhanced Driver Action\n  Recognition","authors":"Ruoyu Wang, Wenqian Wang, Jianjun Gao, Dan Lin, Kim-Hui Yap, Bingbing\n  Li","authorsParsed":[["Wang","Ruoyu",""],["Wang","Wenqian",""],["Gao","Jianjun",""],["Lin","Dan",""],["Yap","Kim-Hui",""],["Li","Bingbing",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 12:33:21 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 09:32:36 GMT"}],"updateDate":"2024-08-20","timestamp":1722688401000,"abstract":"  Driver action recognition, aiming to accurately identify drivers' behaviours,\nis crucial for enhancing driver-vehicle interactions and ensuring driving\nsafety. Unlike general action recognition, drivers' environments are often\nchallenging, being gloomy and dark, and with the development of sensors,\nvarious cameras such as IR and depth cameras have emerged for analyzing\ndrivers' behaviors. Therefore, in this paper, we propose a novel multimodal\nfusion transformer, named MultiFuser, which identifies cross-modal\ninterrelations and interactions among multimodal car cabin videos and\nadaptively integrates different modalities for improved representations.\nSpecifically, MultiFuser comprises layers of Bi-decomposed Modules to model\nspatiotemporal features, with a modality synthesizer for multimodal features\nintegration. Each Bi-decomposed Module includes a Modal Expertise ViT block for\nextracting modality-specific features and a Patch-wise Adaptive Fusion block\nfor efficient cross-modal fusion. Extensive experiments are conducted on\nDrive&Act dataset and the results demonstrate the efficacy of our proposed\napproach.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6Ro6uj2YDfot0sZ4tRm0aKqpxUzpOGqhk3PjKMID6KA","pdfSize":"974715"}
