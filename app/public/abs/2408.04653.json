{"id":"2408.04653","title":"Batching BPE Tokenization Merges","authors":"Alexander P. Morgan","authorsParsed":[["Morgan","Alexander P.",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 09:37:21 GMT"}],"updateDate":"2024-08-12","timestamp":1722850641000,"abstract":"  The Byte Pair Encoding algorithm can be safely batched to merge hundreds of\npairs of tokens at a time when building up a tokenizer's vocabulary. This\ntechnique combined with reducing the memory footprint of text used in\nvocabulary training make it feasible to train a high quality tokenizer on a\nbasic laptop. This paper presents BatchBPE, an open-source pure Python\nimplementation of these concepts, with the goal of making experimenting with\nnew tokenization strategies more accessible especially in compute- and\nmemory-constrained contexts. BatchBPE's usefulness and malleability are\ndemonstrated through the training of several token vocabularies to explore the\nbatch merging process and experiment with preprocessing a stop word list and\nignoring the least common text chunks in a dataset. Resultant encoded lengths\nof texts are used as a basic evaluation metric.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VYEuvltZ0N5IHw-ljfK5zOSM9EhawSKSpXqKiCwmFCc","pdfSize":"661893"}
