{"id":"2408.10729","title":"Towards Efficient Large Language Models for Scientific Text: A Review","authors":"Huy Quoc To, Ming Liu, Guangyan Huang","authorsParsed":[["To","Huy Quoc",""],["Liu","Ming",""],["Huang","Guangyan",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 10:57:34 GMT"}],"updateDate":"2024-08-21","timestamp":1724151454000,"abstract":"  Large language models (LLMs) have ushered in a new era for processing complex\ninformation in various fields, including science. The increasing amount of\nscientific literature allows these models to acquire and understand scientific\nknowledge effectively, thus improving their performance in a wide range of\ntasks. Due to the power of LLMs, they require extremely expensive computational\nresources, intense amounts of data, and training time. Therefore, in recent\nyears, researchers have proposed various methodologies to make scientific LLMs\nmore affordable. The most well-known approaches align in two directions. It can\nbe either focusing on the size of the models or enhancing the quality of data.\nTo date, a comprehensive review of these two families of methods has not yet\nbeen undertaken. In this paper, we (I) summarize the current advances in the\nemerging abilities of LLMs into more accessible AI solutions for science, and\n(II) investigate the challenges and opportunities of developing affordable\nsolutions for scientific domains using LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"HMl4PL1ecOVE0gqpUKflpPXLTsm9HHCn1vXfWuHG3GM","pdfSize":"271899"}
