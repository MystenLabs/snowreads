{"id":"2408.13940","title":"CoT Rerailer: Enhancing the Reliability of Large Language Models in\n  Complex Reasoning Tasks through Error Detection and Correction","authors":"Guangya Wan, Yuqi Wu, Jie Chen, Sheng Li","authorsParsed":[["Wan","Guangya",""],["Wu","Yuqi",""],["Chen","Jie",""],["Li","Sheng",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 21:20:17 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 22:19:17 GMT"}],"updateDate":"2024-09-19","timestamp":1724620817000,"abstract":"  Chain-of-Thought (CoT) prompting enhances Large Language Models (LLMs)\ncomplex reasoning abilities by generating intermediate steps. However, these\nsteps can introduce hallucinations and accumulate errors. We propose the CoT\nRerailer to address these challenges, employing self-consistency and\nmulti-agent debate systems to identify and rectify errors in the reasoning\nprocess. The CoT Rerailer first selects the most logically correct Reasoning\nPath (RP) using consistency checks and critical evaluation by automated agents.\nIt then engages a multi-agent debate system to propose and validate corrections\nto ensure the generation of an error-free intermediate logical path. The\ncorrected steps are then used to generate a revised reasoning chain to further\nreduce hallucinations and enhance answer quality. We demonstrate the\neffectiveness of our approach across diverse question-answering datasets in\nvarious knowledge domains. The CoT Rerailer enhances the reliability of\nLLM-generated reasoning, contributing to more trustworthy AI driven\ndecision-making processes.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}