{"id":"2407.09590","title":"Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse\n  Mixture-of-Experts","authors":"Zeliang Zhang, Xiaodong Liu, Hao Cheng, Chenliang Xu, Jianfeng Gao","authorsParsed":[["Zhang","Zeliang",""],["Liu","Xiaodong",""],["Cheng","Hao",""],["Xu","Chenliang",""],["Gao","Jianfeng",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 17:25:02 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 13:48:50 GMT"}],"updateDate":"2024-09-18","timestamp":1720805102000,"abstract":"  By increasing model parameters but activating them sparsely when performing a\ntask, the use of Mixture-of-Experts (MoE) architecture significantly improves\nthe performance of Large Language Models (LLMs) without increasing the\ninference cost. However, the memory consumption due to the growing number of\nexperts presents a challenge to the deployment of these models in many real\nworld settings. Our empirical study reveals that some experts encode redundant\nknowledge during pre-training. We thus propose a method of grouping and pruning\nsimilar experts to improve the model's parameter efficiency. We validate the\neffectiveness of our method by pruning three state-of-the-art MoE\narchitectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows\nthat our method outperforms other model pruning methods on a range of natural\nlanguage tasks. We will release our code to facilitate future research.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bw-vABCXBdmLLwd0G8f2GpaocwfEaVaXrdwGcXHM15c","pdfSize":"2159336"}
