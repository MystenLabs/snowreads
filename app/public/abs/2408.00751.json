{"id":"2408.00751","title":"A Policy-Gradient Approach to Solving Imperfect-Information Games with\n  Iterate Convergence","authors":"Mingyang Liu, Gabriele Farina, Asuman Ozdaglar","authorsParsed":[["Liu","Mingyang",""],["Farina","Gabriele",""],["Ozdaglar","Asuman",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:54:01 GMT"}],"updateDate":"2024-08-02","timestamp":1722534841000,"abstract":"  Policy gradient methods have become a staple of any single-agent\nreinforcement learning toolbox, due to their combination of desirable\nproperties: iterate convergence, efficient use of stochastic trajectory\nfeedback, and theoretically-sound avoidance of importance sampling corrections.\nIn multi-agent imperfect-information settings (extensive-form games), however,\nit is still unknown whether the same desiderata can be guaranteed while\nretaining theoretical guarantees. Instead, sound methods for extensive-form\ngames rely on approximating counterfactual values (as opposed to Q values),\nwhich are incompatible with policy gradient methodologies. In this paper, we\ninvestigate whether policy gradient can be safely used in two-player zero-sum\nimperfect-information extensive-form games (EFGs). We establish positive\nresults, showing for the first time that a policy gradient method leads to\nprovable best-iterate convergence to a regularized Nash equilibrium in\nself-play.\n","subjects":["Computing Research Repository/Computer Science and Game Theory","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Q44RhVA0oECPThj9GC3J38VAmpuyKukxinm5b6vaRXE","pdfSize":"1182864"}
