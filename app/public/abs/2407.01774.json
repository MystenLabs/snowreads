{"id":"2407.01774","title":"Audio-Visual Approach For Multimodal Concurrent Speaker Detection","authors":"Amit Eliav, Sharon Gannot","authorsParsed":[["Eliav","Amit",""],["Gannot","Sharon",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 20:06:57 GMT"}],"updateDate":"2024-07-03","timestamp":1719864417000,"abstract":"  Concurrent Speaker Detection (CSD), the task of identifying the presence and\noverlap of active speakers in an audio signal, is crucial for many audio tasks\nsuch as meeting transcription, speaker diarization, and speech separation. This\nstudy introduces a multimodal deep learning approach that leverages both audio\nand visual information. The proposed model employs an early fusion strategy\ncombining audio and visual features through cross-modal attention mechanisms,\nwith a learnable [CLS] token capturing the relevant audio-visual relationships.\n  The model is extensively evaluated on two real-world datasets, AMI and the\nrecently introduced EasyCom dataset. Experiments validate the effectiveness of\nthe multimodal fusion strategy. Ablation studies further support the design\nchoices and the training procedure of the model. As this is the first work\nreporting CSD results on the challenging EasyCom dataset, the findings\ndemonstrate the potential of the proposed multimodal approach for CSD in\nreal-world scenarios.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}