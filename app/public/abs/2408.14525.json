{"id":"2408.14525","title":"Estimating Uncertainty with Implicit Quantile Network","authors":"Yi Hung Lim","authorsParsed":[["Lim","Yi Hung",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 13:33:14 GMT"}],"updateDate":"2024-08-28","timestamp":1724679194000,"abstract":"  Uncertainty quantification is an important part of many performance critical\napplications. This paper provides a simple alternative to existing approaches\nsuch as ensemble learning and bayesian neural networks. By directly modeling\nthe loss distribution with an Implicit Quantile Network, we get an estimate of\nhow uncertain the model is of its predictions. For experiments with MNIST and\nCIFAR datasets, the mean of the estimated loss distribution is 2x higher for\nincorrect predictions. When data with high estimated uncertainty is removed\nfrom the test dataset, the accuracy of the model goes up as much as 10%. This\nmethod is simple to implement while offering important information to\napplications where the user has to know when the model could be wrong (e.g.\ndeep learning for healthcare).\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}