{"id":"2408.00624","title":"SynesLM: A Unified Approach for Audio-visual Speech Recognition and\n  Translation via Language Model and Synthetic Data","authors":"Yichen Lu, Jiaqi Song, Xuankai Chang, Hengwei Bian, Soumi Maiti,\n  Shinji Watanabe","authorsParsed":[["Lu","Yichen",""],["Song","Jiaqi",""],["Chang","Xuankai",""],["Bian","Hengwei",""],["Maiti","Soumi",""],["Watanabe","Shinji",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 15:09:32 GMT"}],"updateDate":"2024-08-02","timestamp":1722524972000,"abstract":"  In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}