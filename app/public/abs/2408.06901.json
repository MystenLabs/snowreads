{"id":"2408.06901","title":"Divide and Conquer: Improving Multi-Camera 3D Perception with 2D\n  Semantic-Depth Priors and Input-Dependent Queries","authors":"Qi Song, Qingyong Hu, Chi Zhang, Yongquan Chen, Rui Huang","authorsParsed":[["Song","Qi",""],["Hu","Qingyong",""],["Zhang","Chi",""],["Chen","Yongquan",""],["Huang","Rui",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 13:51:34 GMT"}],"updateDate":"2024-08-14","timestamp":1723557094000,"abstract":"  3D perception tasks, such as 3D object detection and Bird's-Eye-View (BEV)\nsegmentation using multi-camera images, have drawn significant attention\nrecently. Despite the fact that accurately estimating both semantic and 3D\nscene layouts are crucial for this task, existing techniques often neglect the\nsynergistic effects of semantic and depth cues, leading to the occurrence of\nclassification and position estimation errors. Additionally, the\ninput-independent nature of initial queries also limits the learning capacity\nof Transformer-based models. To tackle these challenges, we propose an\ninput-aware Transformer framework that leverages Semantics and Depth as priors\n(named SDTR). Our approach involves the use of an S-D Encoder that explicitly\nmodels semantic and depth priors, thereby disentangling the learning process of\nobject categorization and position estimation. Moreover, we introduce a\nPrior-guided Query Builder that incorporates the semantic prior into the\ninitial queries of the Transformer, resulting in more effective input-aware\nqueries. Extensive experiments on the nuScenes and Lyft benchmarks demonstrate\nthe state-of-the-art performance of our method in both 3D object detection and\nBEV segmentation tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OabHnImrqcn2bxJPFSonZLSFOBLVSIIMQ5nHbWvMty8","pdfSize":"4661703"}
