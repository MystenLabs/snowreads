{"id":"2408.13139","title":"Optimally Solving Simultaneous-Move Dec-POMDPs: The Sequential Central\n  Planning Approach","authors":"Johan Peralez, Aur\\'elien Delage, Jacopo Castellini, Rafael F. Cunha,\n  Jilles S. Dibangoye","authorsParsed":[["Peralez","Johan",""],["Delage","Aur√©lien",""],["Castellini","Jacopo",""],["Cunha","Rafael F.",""],["Dibangoye","Jilles S.",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 15:01:37 GMT"}],"updateDate":"2024-08-26","timestamp":1724425297000,"abstract":"  Centralized training for decentralized execution paradigm emerged as the\nstate-of-the-art approach to epsilon-optimally solving decentralized partially\nobservable Markov decision processes. However, scalability remains a\nsignificant issue. This paper presents a novel and more scalable alternative,\nnamely sequential-move centralized training for decentralized execution. This\nparadigm further pushes the applicability of Bellman's principle of optimality,\nraising three new properties. First, it allows a central planner to reason upon\nsufficient sequential-move statistics instead of prior simultaneous-move ones.\nNext, it proves that epsilon-optimal value functions are piecewise linear and\nconvex in sufficient sequential-move statistics. Finally, it drops the\ncomplexity of the backup operators from double exponential to polynomial at the\nexpense of longer planning horizons. Besides, it makes it easy to use\nsingle-agent methods, e.g., SARSA algorithm enhanced with these findings\napplies while still preserving convergence guarantees. Experiments on two- as\nwell as many-agent domains from the literature against epsilon-optimal\nsimultaneous-move solvers confirm the superiority of the novel approach. This\nparadigm opens the door for efficient planning and reinforcement learning\nmethods for multi-agent systems.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}