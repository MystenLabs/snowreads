{"id":"2407.10335","title":"Towards Adapting Reinforcement Learning Agents to New Tasks: Insights\n  from Q-Values","authors":"Ashwin Ramaswamy and Ransalu Senanayake","authorsParsed":[["Ramaswamy","Ashwin",""],["Senanayake","Ransalu",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 21:28:27 GMT"}],"updateDate":"2024-07-16","timestamp":1720992507000,"abstract":"  While contemporary reinforcement learning research and applications have\nembraced policy gradient methods as the panacea of solving learning problems,\nvalue-based methods can still be useful in many domains as long as we can\nwrangle with how to exploit them in a sample efficient way. In this paper, we\nexplore the chaotic nature of DQNs in reinforcement learning, while\nunderstanding how the information that they retain when trained can be\nrepurposed for adapting a model to different tasks. We start by designing a\nsimple experiment in which we are able to observe the Q-values for each state\nand action in an environment. Then we train in eight different ways to explore\nhow these training algorithms affect the way that accurate Q-values are learned\n(or not learned). We tested the adaptability of each trained model when\nretrained to accomplish a slightly modified task. We then scaled our setup to\ntest the larger problem of an autonomous vehicle at an unprotected\nintersection. We observed that the model is able to adapt to new tasks quicker\nwhen the base model's Q-value estimates are closer to the true Q-values. The\nresults provide some insights and guidelines into what algorithms are useful\nfor sample efficient task adaptation.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}