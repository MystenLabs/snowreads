{"id":"2408.12161","title":"Rebalancing Multi-Label Class-Incremental Learning","authors":"Kaile Du, Yifan Zhou, Fan Lyu, Yuyang Li, Junzhou Xie, Yixi Shen,\n  Fuyuan Hu, Guangcan Liu","authorsParsed":[["Du","Kaile",""],["Zhou","Yifan",""],["Lyu","Fan",""],["Li","Yuyang",""],["Xie","Junzhou",""],["Shen","Yixi",""],["Hu","Fuyuan",""],["Liu","Guangcan",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 07:04:22 GMT"}],"updateDate":"2024-08-23","timestamp":1724310262000,"abstract":"  Multi-label class-incremental learning (MLCIL) is essential for real-world\nmulti-label applications, allowing models to learn new labels while retaining\npreviously learned knowledge continuously. However, recent MLCIL approaches can\nonly achieve suboptimal performance due to the oversight of the\npositive-negative imbalance problem, which manifests at both the label and loss\nlevels because of the task-level partial label issue. The imbalance at the\nlabel level arises from the substantial absence of negative labels, while the\nimbalance at the loss level stems from the asymmetric contributions of the\npositive and negative loss parts to the optimization. To address the issue\nabove, we propose a Rebalance framework for both the Loss and Label levels\n(RebLL), which integrates two key modules: asymmetric knowledge distillation\n(AKD) and online relabeling (OR). AKD is proposed to rebalance at the loss\nlevel by emphasizing the negative label learning in classification loss and\ndown-weighting the contribution of overconfident predictions in distillation\nloss. OR is designed for label rebalance, which restores the original class\ndistribution in memory by online relabeling the missing classes. Our\ncomprehensive experiments on the PASCAL VOC and MS-COCO datasets demonstrate\nthat this rebalancing strategy significantly improves performance, achieving\nnew state-of-the-art results even with a vanilla CNN backbone.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}