{"id":"2408.10188","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","authors":"Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li,\n  Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin,\n  Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han","authorsParsed":[["Xue","Fuzhao",""],["Chen","Yukang",""],["Li","Dacheng",""],["Hu","Qinghao",""],["Zhu","Ligeng",""],["Li","Xiuyu",""],["Fang","Yunhao",""],["Tang","Haotian",""],["Yang","Shang",""],["Liu","Zhijian",""],["He","Ethan",""],["Yin","Hongxu",""],["Molchanov","Pavlo",""],["Kautz","Jan",""],["Fan","Linxi",""],["Zhu","Yuke",""],["Lu","Yao",""],["Han","Song",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 17:48:08 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 17:56:24 GMT"},{"version":"v3","created":"Wed, 21 Aug 2024 17:47:33 GMT"}],"updateDate":"2024-08-22","timestamp":1724089688000,"abstract":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long supervised fine-tuning. However, training on long video is\ncomputationally and memory intensive. We introduce the long-context Multi-Modal\nSequence Parallelism (MM-SP) system that efficiently parallelizes long video\ntraining and inference, enabling 2M context length training on 256 GPUs without\nany gradient checkpointing. LongVILA efficiently extends the number of video\nframes of VILA from 8 to 1024, improving the long video captioning score from\n2.00 to 3.26 (out of 5), achieving 99.5% accuracy in 1400-frame (274k context\nlength) video needle-in-a-haystack. LongVILA-8B demonstrates consistent\naccuracy improvements on long videos in the VideoMME benchmark as the number of\nframes increases. Besides, MM-SP is 2.1x - 5.7x faster than ring sequence\nparallelism and 1.1x - 1.4x faster than Megatron with context parallelism +\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}