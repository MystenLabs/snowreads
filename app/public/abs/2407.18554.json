{"id":"2407.18554","title":"Skin Cancer Detection utilizing Deep Learning: Classification of Skin\n  Lesion Images using a Vision Transformer","authors":"Carolin Flosdorf, Justin Engelker, Igor Keller, Nicolas Mohr","authorsParsed":[["Flosdorf","Carolin",""],["Engelker","Justin",""],["Keller","Igor",""],["Mohr","Nicolas",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 07:06:42 GMT"},{"version":"v2","created":"Sun, 25 Aug 2024 08:09:04 GMT"}],"updateDate":"2024-08-27","timestamp":1721977602000,"abstract":"  Skin cancer detection still represents a major challenge in healthcare.\nCommon detection methods can be lengthy and require human assistance which\nfalls short in many countries. Previous research demonstrates how convolutional\nneural networks (CNNs) can help effectively through both automation and an\naccuracy that is comparable to the human level. However, despite the progress\nin previous decades, the precision is still limited, leading to substantial\nmisclassifications that have a serious impact on people's health. Hence, we\nemploy a Vision Transformer (ViT) that has been developed in recent years based\non the idea of a self-attention mechanism, specifically two configurations of a\npre-trained ViT. We generally find superior metrics for classifying skin\nlesions after comparing them to base models such as decision tree classifier\nand k-nearest neighbor (KNN) classifier, as well as to CNNs and less complex\nViTs. In particular, we attach greater importance to the performance of\nmelanoma, which is the most lethal type of skin cancer. The ViT-L32 model\nachieves an accuracy of 91.57% and a melanoma recall of 58.54%, while ViT-L16\nachieves an accuracy of 92.79% and a melanoma recall of 56.10%. This offers a\npotential tool for faster and more accurate diagnoses and an overall\nimprovement for the healthcare sector.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}