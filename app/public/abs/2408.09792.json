{"id":"2408.09792","title":"Unsupervised Composable Representations for Audio","authors":"Giovanni Bindi, Philippe Esling","authorsParsed":[["Bindi","Giovanni",""],["Esling","Philippe",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 08:41:09 GMT"}],"updateDate":"2024-08-20","timestamp":1724056869000,"abstract":"  Current generative models are able to generate high-quality artefacts but\nhave been shown to struggle with compositional reasoning, which can be defined\nas the ability to generate complex structures from simpler elements. In this\npaper, we focus on the problem of compositional representation learning for\nmusic data, specifically targeting the fully-unsupervised setting. We propose a\nsimple and extensible framework that leverages an explicit compositional\ninductive bias, defined by a flexible auto-encoding objective that can leverage\nany of the current state-of-art generative models. We demonstrate that our\nframework, used with diffusion models, naturally addresses the task of\nunsupervised audio source separation, showing that our model is able to perform\nhigh-quality separation. Our findings reveal that our proposal achieves\ncomparable or superior performance with respect to other blind source\nseparation methods and, furthermore, it even surpasses current state-of-art\nsupervised baselines on signal-to-interference ratio metrics. Additionally, by\nlearning an a-posteriori masking diffusion model in the space of composable\nrepresentations, we achieve a system capable of seamlessly performing\nunsupervised source separation, unconditional generation, and variation\ngeneration. Finally, as our proposal works in the latent space of pre-trained\nneural audio codecs, it also provides a lower computational cost with respect\nto other neural baselines.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}