{"id":"2407.20647","title":"Image Re-Identification: Where Self-supervision Meets Vision-Language\n  Learning","authors":"Bin Wang, Yuying Liang, Lei Cai, Huakun Huang, and Huanqiang Zeng","authorsParsed":[["Wang","Bin",""],["Liang","Yuying",""],["Cai","Lei",""],["Huang","Huakun",""],["Zeng","Huanqiang",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 08:43:53 GMT"}],"updateDate":"2024-07-31","timestamp":1722329033000,"abstract":"  Recently, large-scale vision-language pre-trained models like CLIP have shown\nimpressive performance in image re-identification (ReID). In this work, we\nexplore whether self-supervision can aid in the use of CLIP for image ReID\ntasks. Specifically, we propose SVLL-ReID, the first attempt to integrate\nself-supervision and pre-trained CLIP via two training stages to facilitate the\nimage ReID. We observe that: 1) incorporating language self-supervision in the\nfirst training stage can make the learnable text prompts more distinguishable,\nand 2) incorporating vision self-supervision in the second training stage can\nmake the image features learned by the image encoder more discriminative. These\nobservations imply that: 1) the text prompt learning in the first stage can\nbenefit from the language self-supervision, and 2) the image feature learning\nin the second stage can benefit from the vision self-supervision. These\nbenefits jointly facilitate the performance gain of the proposed SVLL-ReID. By\nconducting experiments on six image ReID benchmark datasets without any\nconcrete text labels, we find that the proposed SVLL-ReID achieves the overall\nbest performances compared with state-of-the-arts. Codes will be publicly\navailable at https://github.com/BinWangGzhu/SVLL-ReID.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}