{"id":"2407.09127","title":"Robustness of Explainable Artificial Intelligence in Industrial Process\n  Modelling","authors":"Benedikt Kantz, Clemens Staudinger, Christoph Feilmayr, Johannes\n  Wachlmayr, Alexander Haberl, Stefan Schuster, Franz Pernkopf","authorsParsed":[["Kantz","Benedikt",""],["Staudinger","Clemens",""],["Feilmayr","Christoph",""],["Wachlmayr","Johannes",""],["Haberl","Alexander",""],["Schuster","Stefan",""],["Pernkopf","Franz",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 09:46:26 GMT"}],"updateDate":"2024-07-15","timestamp":1720777586000,"abstract":"  eXplainable Artificial Intelligence (XAI) aims at providing understandable\nexplanations of black box models. In this paper, we evaluate current XAI\nmethods by scoring them based on ground truth simulations and sensitivity\nanalysis. To this end, we used an Electric Arc Furnace (EAF) model to better\nunderstand the limits and robustness characteristics of XAI methods such as\nSHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic\nExplanations (LIME), as well as Averaged Local Effects (ALE) or Smooth\nGradients (SG) in a highly topical setting. These XAI methods were applied to\nvarious types of black-box models and then scored based on their correctness\ncompared to the ground-truth sensitivity of the data-generating processes using\na novel scoring evaluation methodology over a range of simulated additive\nnoise. The resulting evaluation shows that the capability of the Machine\nLearning (ML) models to capture the process accurately is, indeed, coupled with\nthe correctness of the explainability of the underlying data-generating\nprocess. We furthermore show the differences between XAI methods in their\nability to correctly predict the true sensitivity of the modeled industrial\nprocess.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}