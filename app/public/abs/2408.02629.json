{"id":"2408.02629","title":"VidGen-1M: A Large-Scale Dataset for Text-to-video Generation","authors":"Zhiyu Tan, Xiaomeng Yang, Luozheng Qin and Hao Li","authorsParsed":[["Tan","Zhiyu",""],["Yang","Xiaomeng",""],["Qin","Luozheng",""],["Li","Hao",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 16:53:23 GMT"}],"updateDate":"2024-08-06","timestamp":1722876803000,"abstract":"  The quality of video-text pairs fundamentally determines the upper bound of\ntext-to-video models. Currently, the datasets used for training these models\nsuffer from significant shortcomings, including low temporal consistency,\npoor-quality captions, substandard video quality, and imbalanced data\ndistribution. The prevailing video curation process, which depends on image\nmodels for tagging and manual rule-based curation, leads to a high\ncomputational load and leaves behind unclean data. As a result, there is a lack\nof appropriate training datasets for text-to-video models. To address this\nproblem, we present VidGen-1M, a superior training dataset for text-to-video\nmodels. Produced through a coarse-to-fine curation strategy, this dataset\nguarantees high-quality videos and detailed captions with excellent temporal\nconsistency. When used to train the video generation model, this dataset has\nled to experimental results that surpass those obtained with other models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TVq37H-OT87vr7pvLzpGdVW2NkGH-kJsZtIkqrvHXmE","pdfSize":"2737139"}
