{"id":"2407.03233","title":"Asynchronous Parallel Policy Gradient Methods for the Linear Quadratic\n  Regulator","authors":"Xingyu Sha, Feiran Zhao, Keyou You","authorsParsed":[["Sha","Xingyu",""],["Zhao","Feiran",""],["You","Keyou",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 16:03:13 GMT"}],"updateDate":"2024-07-04","timestamp":1720022593000,"abstract":"  Learning policies in an asynchronous parallel way is essential to the\nnumerous successes of RL for solving large-scale problems. However, their\nconvergence performance is still not rigorously evaluated. To this end, we\nadopt the asynchronous parallel zero-order policy gradient (AZOPG) method to\nsolve the continuous-time linear quadratic regulation problem. Specifically, as\nin the celebrated A3C algorithm, there are multiple parallel workers to\nasynchronously estimate PGs which are then sent to a central master for policy\nupdates. Via quantifying its convergence rate of policy iterations, we show the\nlinear speedup property of the AZOPG, both in theory and simulation, which\nclearly reveals the advantages of using parallel workers for learning policies.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}