{"id":"2408.13885","title":"Neural Spacetimes for DAG Representation Learning","authors":"Haitz S\\'aez de Oc\\'ariz Borde, Anastasis Kratsios, Marc T. Law,\n  Xiaowen Dong, Michael Bronstein","authorsParsed":[["Borde","Haitz Sáez de Ocáriz",""],["Kratsios","Anastasis",""],["Law","Marc T.",""],["Dong","Xiaowen",""],["Bronstein","Michael",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 16:26:55 GMT"}],"updateDate":"2024-08-27","timestamp":1724603215000,"abstract":"  We propose a class of trainable deep learning-based geometries called Neural\nSpacetimes (NSTs), which can universally represent nodes in weighted directed\nacyclic graphs (DAGs) as events in a spacetime manifold. While most works in\nthe literature focus on undirected graph representation learning or causality\nembedding separately, our differentiable geometry can encode both graph edge\nweights in its spatial dimensions and causality in the form of edge\ndirectionality in its temporal dimensions. We use a product manifold that\ncombines a quasi-metric (for space) and a partial order (for time). NSTs are\nimplemented as three neural networks trained in an end-to-end manner: an\nembedding network, which learns to optimize the location of nodes as events in\nthe spacetime manifold, and two other networks that optimize the space and time\ngeometries in parallel, which we call a neural (quasi-)metric and a neural\npartial order, respectively. The latter two networks leverage recent ideas at\nthe intersection of fractal geometry and deep learning to shape the geometry of\nthe representation space in a data-driven fashion, unlike other works in the\nliterature that use fixed spacetime manifolds such as Minkowski space or De\nSitter space to embed DAGs. Our main theoretical guarantee is a universal\nembedding theorem, showing that any $k$-point DAG can be embedded into an NST\nwith $1+\\mathcal{O}(\\log(k))$ distortion while exactly preserving its causal\nstructure. The total number of parameters defining the NST is sub-cubic in $k$\nand linear in the width of the DAG. If the DAG has a planar Hasse diagram, this\nis improved to $\\mathcal{O}(\\log(k)) + 2)$ spatial and 2 temporal dimensions.\nWe validate our framework computationally with synthetic weighted DAGs and\nreal-world network embeddings; in both cases, the NSTs achieve lower embedding\ndistortions than their counterparts using fixed spacetime geometries.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Discrete Mathematics","Computing Research Repository/Neural and Evolutionary Computing","Mathematics/Metric Geometry","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"sHDVb3DnOWJ0zdwOsMP8d9Y6BEFuY_-ARviCXvXcx2Y","pdfSize":"1021497"}
