{"id":"2408.07852","title":"Training Language Models on the Knowledge Graph: Insights on\n  Hallucinations and Their Detectability","authors":"Jiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne Liu, Ben Adlam,\n  Maxwell Bileschi, Bernd Bohnet, JD Co-Reyes, Noah Fiedel, C. Daniel Freeman,\n  Izzeddin Gur, Kathleen Kenealy, Jaehoon Lee, Peter J. Liu, Gaurav Mishra,\n  Igor Mordatch, Azade Nova, Roman Novak, Aaron Parisi, Jeffrey Pennington,\n  Alex Rizkowsky, Isabelle Simpson, Hanie Sedghi, Jascha Sohl-dickstein, Kevin\n  Swersky, Sharad Vikram, Tris Warkentin, Lechao Xiao, Kelvin Xu, Jasper Snoek,\n  Simon Kornblith","authorsParsed":[["Hron","Jiri",""],["Culp","Laura",""],["Elsayed","Gamaleldin",""],["Liu","Rosanne",""],["Adlam","Ben",""],["Bileschi","Maxwell",""],["Bohnet","Bernd",""],["Co-Reyes","JD",""],["Fiedel","Noah",""],["Freeman","C. Daniel",""],["Gur","Izzeddin",""],["Kenealy","Kathleen",""],["Lee","Jaehoon",""],["Liu","Peter J.",""],["Mishra","Gaurav",""],["Mordatch","Igor",""],["Nova","Azade",""],["Novak","Roman",""],["Parisi","Aaron",""],["Pennington","Jeffrey",""],["Rizkowsky","Alex",""],["Simpson","Isabelle",""],["Sedghi","Hanie",""],["Sohl-dickstein","Jascha",""],["Swersky","Kevin",""],["Vikram","Sharad",""],["Warkentin","Tris",""],["Xiao","Lechao",""],["Xu","Kelvin",""],["Snoek","Jasper",""],["Kornblith","Simon",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 23:34:28 GMT"}],"updateDate":"2024-08-16","timestamp":1723678468000,"abstract":"  While many capabilities of language models (LMs) improve with increased\ntraining budget, the influence of scale on hallucinations is not yet fully\nunderstood. Hallucinations come in many forms, and there is no universally\naccepted definition. We thus focus on studying only those hallucinations where\na correct answer appears verbatim in the training set. To fully control the\ntraining data content, we construct a knowledge graph (KG)-based dataset, and\nuse it to train a set of increasingly large LMs. We find that for a fixed\ndataset, larger and longer-trained LMs hallucinate less. However, hallucinating\non $\\leq5$% of the training data requires an order of magnitude larger model,\nand thus an order of magnitude more compute, than Hoffmann et al. (2022)\nreported was optimal. Given this costliness, we study how hallucination\ndetectors depend on scale. While we see detector size improves performance on\nfixed LM's outputs, we find an inverse relationship between the scale of the LM\nand the detectability of its hallucinations.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UyyHHqCMo1-gBHoof5Ne2pLXcYMMVcqX5XtzCCSg77U","pdfSize":"1183776"}
