{"id":"2407.12844","title":"$\\texttt{metabench}$ -- A Sparse Benchmark to Measure General Ability in\n  Large Language Models","authors":"Alex Kipnis, Konstantinos Voudouris, Luca M. Schulze Buschoff, Eric\n  Schulz","authorsParsed":[["Kipnis","Alex",""],["Voudouris","Konstantinos",""],["Buschoff","Luca M. Schulze",""],["Schulz","Eric",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 17:57:38 GMT"}],"updateDate":"2024-07-19","timestamp":1720115858000,"abstract":"  Large Language Models (LLMs) vary in their abilities on a range of tasks.\nInitiatives such as the $\\texttt{Open LLM Leaderboard}$ aim to quantify these\ndifferences with several large benchmarks (sets of test items to which an LLM\ncan respond either correctly or incorrectly). However, high correlations within\nand between benchmark scores suggest that (1) there exists a small set of\ncommon underlying abilities that these benchmarks measure, and (2) items tap\ninto redundant information and the benchmarks may thus be considerably\ncompressed. We use data from $n > 5000$ LLMs to identify the most informative\nitems of six benchmarks, ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande\n(with $d=28,632$ items in total). From them we distill a sparse benchmark,\n$\\texttt{metabench}$, that has less than $3\\%$ of the original size of all six\nbenchmarks combined. This new sparse benchmark goes beyond point scores by\nyielding estimators of the underlying benchmark-specific abilities. We show\nthat these estimators (1) can be used to reconstruct each original\n$\\textit{individual}$ benchmark score with, on average, $1.5\\%$ root mean\nsquare error (RMSE), (2) reconstruct the original $\\textit{total}$ score with\n$0.8\\%$ RMSE, and (3) have a single underlying common factor whose Spearman\ncorrelation with the total score is $r = 0.93$.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}