{"id":"2407.18698","title":"Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended\n  Text Generation","authors":"Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian\n  Heumann, Matthias A{\\ss}enmacher","authorsParsed":[["Arias","Esteban Garces",""],["Rodemann","Julian",""],["Li","Meimingwei",""],["Heumann","Christian",""],["AÃŸenmacher","Matthias",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 12:23:54 GMT"}],"updateDate":"2024-07-29","timestamp":1721996634000,"abstract":"  Decoding from the output distributions of large language models to produce\nhigh-quality text is a complex challenge in language modeling. Various\napproaches, such as beam search, sampling with temperature, $k-$sampling,\nnucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive\nsearch, have been proposed to address this problem, aiming to improve\ncoherence, diversity, as well as resemblance to human-generated text. In this\nstudy, we introduce adaptive contrastive search, a novel decoding strategy\nextending contrastive search by incorporating an adaptive degeneration penalty,\nguided by the estimated uncertainty of the model at each generation step. This\nstrategy is designed to enhance both the creativity and diversity of the\nlanguage modeling process while at the same time producing coherent and\nhigh-quality generated text output. Our findings indicate performance\nenhancement in both aspects, across different model architectures and datasets,\nunderscoring the effectiveness of our method in text generation tasks. Our code\nbase, datasets, and models are publicly available.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Statistics/Methodology","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}