{"id":"2407.08965","title":"Lite-SAM Is Actually What You Need for Segment Everything","authors":"Jianhai Fu, Yuanjie Yu, Ningchuan Li, Yi Zhang, Qichao Chen, Jianping\n  Xiong, Jun Yin, Zhiyu Xiang","authorsParsed":[["Fu","Jianhai",""],["Yu","Yuanjie",""],["Li","Ningchuan",""],["Zhang","Yi",""],["Chen","Qichao",""],["Xiong","Jianping",""],["Yin","Jun",""],["Xiang","Zhiyu",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 03:28:46 GMT"}],"updateDate":"2024-07-15","timestamp":1720754926000,"abstract":"  This paper introduces Lite-SAM, an efficient end-to-end solution for the\nSegEvery task designed to reduce computational costs and redundancy. Lite-SAM\nis composed of four main components: a streamlined CNN-Transformer hybrid\nencoder (LiteViT), an automated prompt proposal network (AutoPPN), a\ntraditional prompt encoder, and a mask decoder. All these components are\nintegrated within the SAM framework. Our LiteViT, a high-performance\nlightweight backbone network, has only 1.16M parameters, which is a 23%\nreduction compared to the lightest existing backbone network Shufflenet. We\nalso introduce AutoPPN, an innovative end-to-end method for prompt boxes and\npoints generation. This is an improvement over traditional grid search sampling\nmethods, and its unique design allows for easy integration into any SAM series\nalgorithm, extending its usability. we have thoroughly benchmarked Lite-SAM\nacross a plethora of both public and private datasets. The evaluation\nencompassed a broad spectrum of universal metrics, including the number of\nparameters, SegEvery execution time, and accuracy. The findings reveal that\nLite-SAM, operating with a lean 4.2M parameters, significantly outpaces its\ncounterparts, demonstrating performance improvements of 43x, 31x, 20x, 21x, and\n1.6x over SAM, MobileSAM, Edge-SAM, EfficientViT-SAM, and MobileSAM-v2\nrespectively, all the while maintaining competitive accuracy. This underscores\nLite-SAM's prowess in achieving an optimal equilibrium between performance and\nprecision, thereby setting a new state-of-the-art(SOTA) benchmark in the\ndomain.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}