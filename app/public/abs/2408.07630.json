{"id":"2408.07630","title":"Towards Fair and Rigorous Evaluations: Hyperparameter Optimization for\n  Top-N Recommendation Task with Implicit Feedback","authors":"Hui Fang, Xu Feng, Lu Qin and Zhu Sun","authorsParsed":[["Fang","Hui",""],["Feng","Xu",""],["Qin","Lu",""],["Sun","Zhu",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 15:56:27 GMT"}],"updateDate":"2024-08-15","timestamp":1723650987000,"abstract":"  The widespread use of the internet has led to an overwhelming amount of data,\nwhich has resulted in the problem of information overload. Recommender systems\nhave emerged as a solution to this problem by providing personalized\nrecommendations to users based on their preferences and historical data.\nHowever, as recommendation models become increasingly complex, finding the best\nhyperparameter combination for different models has become a challenge. The\nhigh-dimensional hyperparameter search space poses numerous challenges for\nresearchers, and failure to disclose hyperparameter settings may impede the\nreproducibility of research results. In this paper, we investigate the Top-N\nimplicit recommendation problem and focus on optimizing the benchmark\nrecommendation algorithm commonly used in comparative experiments using\nhyperparameter optimization algorithms. We propose a research methodology that\nfollows the principles of a fair comparison, employing seven types of\nhyperparameter search algorithms to fine-tune six common recommendation\nalgorithms on three datasets. We have identified the most suitable\nhyperparameter search algorithms for various recommendation algorithms on\ndifferent types of datasets as a reference for later study. This study\ncontributes to algorithmic research in recommender systems based on\nhyperparameter optimization, providing a fair basis for comparison.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}