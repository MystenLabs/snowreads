{"id":"2408.01977","title":"Label Augmentation for Neural Networks Robustness","authors":"Fatemeh Amerehi, Patrick Healy","authorsParsed":[["Amerehi","Fatemeh",""],["Healy","Patrick",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 09:51:14 GMT"}],"updateDate":"2024-08-06","timestamp":1722765074000,"abstract":"  Out-of-distribution generalization can be categorized into two types: common\nperturbations arising from natural variations in the real world and adversarial\nperturbations that are intentionally crafted to deceive neural networks. While\ndeep neural networks excel in accuracy under the assumption of identical\ndistributions between training and test data, they often encounter\nout-of-distribution scenarios resulting in a significant decline in accuracy.\nData augmentation methods can effectively enhance robustness against common\ncorruptions, but they typically fall short in improving robustness against\nadversarial perturbations. In this study, we develop Label Augmentation (LA),\nwhich enhances robustness against both common and intentional perturbations and\nimproves uncertainty estimation. Our findings indicate a Clean error rate\nimprovement of up to 23.29% when employing LA in comparisons to the baseline.\nAdditionally, it enhances robustness under common corruptions benchmark by up\nto 24.23%. When tested against FGSM and PGD attacks, improvements in\nadversarial robustness are noticeable, with enhancements of up to 53.18% for\nFGSM and 24.46% for PGD attacks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}