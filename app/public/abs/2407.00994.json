{"id":"2407.00994","title":"LLM Uncertainty Quantification through Directional Entailment Graph and\n  Claim Level Response Augmentation","authors":"Longchao Da, Tiejin Chen, Lu Cheng, Hua Wei","authorsParsed":[["Da","Longchao",""],["Chen","Tiejin",""],["Cheng","Lu",""],["Wei","Hua",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 06:11:30 GMT"},{"version":"v2","created":"Mon, 8 Jul 2024 04:52:23 GMT"}],"updateDate":"2024-07-09","timestamp":1719814290000,"abstract":"  The Large language models (LLMs) have showcased superior capabilities in\nsophisticated tasks across various domains, stemming from basic question-answer\n(QA), they are nowadays used as decision assistants or explainers for\nunfamiliar content. However, they are not always correct due to the data\nsparsity in specific domain corpus, or the model's hallucination problems.\nGiven this, how much should we trust the responses from LLMs? This paper\npresents a novel way to evaluate the uncertainty that captures the directional\ninstability, by constructing a directional graph from entailment probabilities,\nand we innovatively conduct Random Walk Laplacian given the asymmetric property\nof a constructed directed graph, then the uncertainty is aggregated by the\nderived eigenvalues from the Laplacian process. We also provide a way to\nincorporate the existing work's semantics uncertainty with our proposed layer.\nBesides, this paper identifies the vagueness issues in the raw response set and\nproposes an augmentation approach to mitigate such a problem, we conducted\nextensive empirical experiments and demonstrated the superiority of our\nproposed solutions.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}