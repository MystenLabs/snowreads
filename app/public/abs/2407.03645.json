{"id":"2407.03645","title":"Continual Learning Optimizations for Auto-regressive Decoder of\n  Multilingual ASR systems","authors":"Chin Yuen Kwok, Jia Qi Yip, Eng Siong Chng","authorsParsed":[["Kwok","Chin Yuen",""],["Yip","Jia Qi",""],["Chng","Eng Siong",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 05:35:47 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 03:07:04 GMT"}],"updateDate":"2024-07-15","timestamp":1720071347000,"abstract":"  Continual Learning (CL) involves fine-tuning pre-trained models with new data\nwhile maintaining the performance on the pre-trained data. This is particularly\nrelevant for expanding multilingual ASR (MASR) capabilities. However, existing\nCL methods, mainly designed for computer vision and reinforcement learning\ntasks, often yield sub-optimal results when directly applied to MASR. We\nhypothesise that this is because CL of the auto-regressive decoder in the MASR\nmodel is difficult. To verify this, we propose four optimizations on the\ndecoder. They include decoder-layer gradient surgery, freezing unused token\nembeddings, suppressing output of newly added tokens, and learning rate\nre-scaling. Our experiments on adapting Whisper to 10 unseen languages from the\nCommon Voice dataset demonstrate that these optimizations reduce the Average\nWord Error Rate (AWER) of pretrained languages from 14.2% to 12.4% compared\nwith Experience Replay, without compromising the AWER of new languages.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}