{"id":"2407.13885","title":"Attention in SRAM on Tenstorrent Grayskull","authors":"Moritz Th\\\"uning","authorsParsed":[["Th√ºning","Moritz",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 20:19:36 GMT"}],"updateDate":"2024-07-22","timestamp":1721333976000,"abstract":"  When implementations of the Transformer's self-attention layer utilize SRAM\ninstead of DRAM, they can achieve significant speedups. The Tenstorrent\nGrayskull architecture provides a large SRAM, distributed across a grid of\ncores. This work presents a fused kernel for Grayskull, that exclusively\nutilizes its large SRAM by combining matrix multiplication, attention score\nscaling and Softmax operations. Additionally, a dedicated Softmax kernel\nutilizing the SRAM and a CPU implementation serving as a baseline are\npresented. The Softmax operation consumes most of the runtime in the\ncomputation of attention weights from queries and keys on Grayskull. The\nspeedup of the dedicated Softmax kernel compared to the CPU implementation is\nup to $10 \\times$, and the Softmax implementation inside the fused kernel is\napproximately $1.8 \\times$ faster than the dedicated Softmax kernel. The time\nand memory complexity of all implementations is quadratic in sequence length.\nCurrently, the Grayskull e150 is approximately $30 \\times$ cheaper for the\ngeneral public than an Nvidia H100 PCIe (a state-of-the-art GPU) and offers\napproximately $1.5 \\times$ more SRAM.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Performance"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}