{"id":"2407.19816","title":"Comparative Analysis of Encoder-Based NER and Large Language Models for\n  Skill Extraction from Russian Job Vacancies","authors":"Nikita Matkin, Aleksei Smirnov, Mikhail Usanin, Egor Ivanov, Kirill\n  Sobyanin, Sofiia Paklina, Petr Parshakov","authorsParsed":[["Matkin","Nikita",""],["Smirnov","Aleksei",""],["Usanin","Mikhail",""],["Ivanov","Egor",""],["Sobyanin","Kirill",""],["Paklina","Sofiia",""],["Parshakov","Petr",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 09:08:40 GMT"},{"version":"v2","created":"Sun, 15 Sep 2024 05:02:37 GMT"}],"updateDate":"2024-09-17","timestamp":1722244120000,"abstract":"  The labor market is undergoing rapid changes, with increasing demands on job\nseekers and a surge in job openings. Identifying essential skills and\ncompetencies from job descriptions is challenging due to varying employer\nrequirements and the omission of key skills. This study addresses these\nchallenges by comparing traditional Named Entity Recognition (NER) methods\nbased on encoders with Large Language Models (LLMs) for extracting skills from\nRussian job vacancies. Using a labeled dataset of 4,000 job vacancies for\ntraining and 1,472 for testing, the performance of both approaches is\nevaluated. Results indicate that traditional NER models, especially DeepPavlov\nRuBERT NER tuned, outperform LLMs across various metrics including accuracy,\nprecision, recall, and inference time. The findings suggest that traditional\nNER models provide more effective and efficient solutions for skill extraction,\nenhancing job requirement clarity and aiding job seekers in aligning their\nqualifications with employer expectations. This research contributes to the\nfield of natural language processing (NLP) and its application in the labor\nmarket, particularly in non-English contexts.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"S66f0AzUlZ4naAAKkZHQz_YI_19iXKQdzZA2-8iFr9M","pdfSize":"431278"}
