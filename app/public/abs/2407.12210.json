{"id":"2407.12210","title":"A Closer Look at Benchmarking Self-Supervised Pre-training with Image\n  Classification","authors":"Markus Marks, Manuel Knott, Neehar Kondapaneni, Elijah Cole, Thijs\n  Defraeye, Fernando Perez-Cruz, Pietro Perona","authorsParsed":[["Marks","Markus",""],["Knott","Manuel",""],["Kondapaneni","Neehar",""],["Cole","Elijah",""],["Defraeye","Thijs",""],["Perez-Cruz","Fernando",""],["Perona","Pietro",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 23:17:36 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 00:18:44 GMT"}],"updateDate":"2024-07-19","timestamp":1721171856000,"abstract":"  Self-supervised learning (SSL) is a machine learning approach where the data\nitself provides supervision, eliminating the need for external labels. The\nmodel is forced to learn about the data structure or context by solving a\npretext task. With SSL, models can learn from abundant and cheap unlabeled\ndata, significantly reducing the cost of training models where labels are\nexpensive or inaccessible. In Computer Vision, SSL is widely used as\npre-training followed by a downstream task, such as supervised transfer,\nfew-shot learning on smaller labeled data sets, and/or unsupervised clustering.\nUnfortunately, it is infeasible to evaluate SSL methods on all possible\ndownstream tasks and objectively measure the quality of the learned\nrepresentation. Instead, SSL methods are evaluated using in-domain evaluation\nprotocols, such as fine-tuning, linear probing, and k-nearest neighbors (kNN).\nHowever, it is not well understood how well these evaluation protocols estimate\nthe representation quality of a pre-trained model for different downstream\ntasks under different conditions, such as dataset, metric, and model\narchitecture. We study how classification-based evaluation protocols for SSL\ncorrelate and how well they predict downstream performance on different dataset\ntypes. Our study includes eleven common image datasets and 26 models that were\npre-trained with different SSL methods or have different model backbones. We\nfind that in-domain linear/kNN probing protocols are, on average, the best\ngeneral predictors for out-of-domain performance. We further investigate the\nimportance of batch normalization and evaluate how robust correlations are for\ndifferent kinds of dataset domain shifts. We challenge assumptions about the\nrelationship between discriminative and generative self-supervised methods,\nfinding that most of their performance differences can be explained by changes\nto model backbones.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}