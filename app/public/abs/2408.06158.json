{"id":"2408.06158","title":"OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal\n  Omni-Scale Feature Learning","authors":"Mushui Liu, Bozheng Li, Yunlong Yu","authorsParsed":[["Liu","Mushui",""],["Li","Bozheng",""],["Yu","Yunlong",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 13:55:46 GMT"}],"updateDate":"2024-08-13","timestamp":1723470946000,"abstract":"  Recent Vision-Language Models (VLMs) \\textit{e.g.} CLIP have made great\nprogress in video recognition. Despite the improvement brought by the strong\nvisual backbone in extracting spatial features, CLIP still falls short in\ncapturing and integrating spatial-temporal features which is essential for\nvideo recognition. In this paper, we propose OmniCLIP, a framework that adapts\nCLIP for video recognition by focusing on learning comprehensive features\nencompassing spatial, temporal, and dynamic spatial-temporal scales, which we\nrefer to as omni-scale features. This is achieved through the design of\nspatial-temporal blocks that include parallel temporal adapters (PTA), enabling\nefficient temporal modeling. Additionally, we introduce a self-prompt generator\n(SPG) module to capture dynamic object spatial features. The synergy between\nPTA and SPG allows OmniCLIP to discern varying spatial information across\nframes and assess object scales over time. We have conducted extensive\nexperiments in supervised video recognition, few-shot video recognition, and\nzero-shot recognition tasks. The results demonstrate the effectiveness of our\nmethod, especially with OmniCLIP achieving a top-1 accuracy of 74.30\\% on\nHMDB51 in a 16-shot setting, surpassing the recent MotionPrompt approach even\nwith full training data. The code is available at\n\\url{https://github.com/XiaoBuL/OmniCLIP}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"b_sPv8T769JFfc4xitAb-ryRHlbb9iI6stkZ01C5B4o","pdfSize":"10049756"}
