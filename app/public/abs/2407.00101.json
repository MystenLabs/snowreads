{"id":"2407.00101","title":"Hybrid Approach to Parallel Stochastic Gradient Descent","authors":"Aakash Sudhirbhai Vora and Dhrumil Chetankumar Joshi and Aksh\n  Kantibhai Patel","authorsParsed":[["Vora","Aakash Sudhirbhai",""],["Joshi","Dhrumil Chetankumar",""],["Patel","Aksh Kantibhai",""]],"versions":[{"version":"v1","created":"Thu, 27 Jun 2024 06:28:30 GMT"}],"updateDate":"2024-07-02","timestamp":1719469710000,"abstract":"  Stochastic Gradient Descent is used for large datasets to train models to\nreduce the training time. On top of that data parallelism is widely used as a\nmethod to efficiently train neural networks using multiple worker nodes in\nparallel. Synchronous and asynchronous approach to data parallelism is used by\nmost systems to train the model in parallel. However, both of them have their\ndrawbacks. We propose a third approach to data parallelism which is a hybrid\nbetween synchronous and asynchronous approaches, using both approaches to train\nthe neural network. When the threshold function is selected appropriately to\ngradually shift all parameter aggregation from asynchronous to synchronous, we\nshow that in a given time period our hybrid approach outperforms both\nasynchronous and synchronous approaches.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computational Complexity","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}