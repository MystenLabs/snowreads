{"id":"2408.07519","title":"Whitening Consistently Improves Self-Supervised Learning","authors":"Andr\\'as Kalapos, B\\'alint Gyires-T\\'oth","authorsParsed":[["Kalapos","András",""],["Gyires-Tóth","Bálint",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 12:52:13 GMT"}],"updateDate":"2024-08-15","timestamp":1723639933000,"abstract":"  Self-supervised learning (SSL) has been shown to be a powerful approach for\nlearning visual representations. In this study, we propose incorporating ZCA\nwhitening as the final layer of the encoder in self-supervised learning to\nenhance the quality of learned features by normalizing and decorrelating them.\nAlthough whitening has been utilized in SSL in previous works, its potential to\nuniversally improve any SSL model has not been explored. We demonstrate that\nadding whitening as the last layer of SSL pretrained encoders is independent of\nthe self-supervised learning method and encoder architecture, thus it improves\nperformance for a wide range of SSL methods across multiple encoder\narchitectures and datasets. Our experiments show that whitening is capable of\nimproving linear and k-NN probing accuracy by 1-5%. Additionally, we propose\nmetrics that allow for a comprehensive analysis of the learned features,\nprovide insights into the quality of the representations and help identify\ncollapse patterns.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}