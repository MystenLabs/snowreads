{"id":"2407.18365","title":"FADAS: Towards Federated Adaptive Asynchronous Optimization","authors":"Yujia Wang, Shiqiang Wang, Songtao Lu, Jinghui Chen","authorsParsed":[["Wang","Yujia",""],["Wang","Shiqiang",""],["Lu","Songtao",""],["Chen","Jinghui",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 20:02:57 GMT"}],"updateDate":"2024-07-29","timestamp":1721937777000,"abstract":"  Federated learning (FL) has emerged as a widely adopted training paradigm for\nprivacy-preserving machine learning. While the SGD-based FL algorithms have\ndemonstrated considerable success in the past, there is a growing trend towards\nadopting adaptive federated optimization methods, particularly for training\nlarge-scale models. However, the conventional synchronous aggregation design\nposes a significant challenge to the practical deployment of those adaptive\nfederated optimization methods, particularly in the presence of straggler\nclients. To fill this research gap, this paper introduces federated adaptive\nasynchronous optimization, named FADAS, a novel method that incorporates\nasynchronous updates into adaptive federated optimization with provable\nguarantees. To further enhance the efficiency and resilience of our proposed\nmethod in scenarios with significant asynchronous delays, we also extend FADAS\nwith a delay-adaptive learning adjustment strategy. We rigorously establish the\nconvergence rate of the proposed algorithms and empirical results demonstrate\nthe superior performance of FADAS over other asynchronous FL baselines.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}