{"id":"2407.06692","title":"Deep-Motion-Net: GNN-based volumetric organ shape reconstruction from\n  single-view 2D projections","authors":"Isuru Wijesinghe, Michael Nix, Arezoo Zakeri, Alireza Hokmabadi,\n  Bashar Al-Qaisieh, Ali Gooya, Zeike A. Taylor","authorsParsed":[["Wijesinghe","Isuru",""],["Nix","Michael",""],["Zakeri","Arezoo",""],["Hokmabadi","Alireza",""],["Al-Qaisieh","Bashar",""],["Gooya","Ali",""],["Taylor","Zeike A.",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 09:07:18 GMT"}],"updateDate":"2024-07-10","timestamp":1720516038000,"abstract":"  We propose Deep-Motion-Net: an end-to-end graph neural network (GNN)\narchitecture that enables 3D (volumetric) organ shape reconstruction from a\nsingle in-treatment kV planar X-ray image acquired at any arbitrary projection\nangle. Estimating and compensating for true anatomical motion during\nradiotherapy is essential for improving the delivery of planned radiation dose\nto target volumes while sparing organs-at-risk, and thereby improving the\ntherapeutic ratio. Achieving this using only limited imaging available during\nirradiation and without the use of surrogate signals or invasive fiducial\nmarkers is attractive. The proposed model learns the mesh regression from a\npatient-specific template and deep features extracted from kV images at\narbitrary projection angles. A 2D-CNN encoder extracts image features, and four\nfeature pooling networks fuse these features to the 3D template organ mesh. A\nResNet-based graph attention network then deforms the feature-encoded mesh. The\nmodel is trained using synthetically generated organ motion instances and\ncorresponding kV images. The latter is generated by deforming a reference CT\nvolume aligned with the template mesh, creating digitally reconstructed\nradiographs (DRRs) at required projection angles, and DRR-to-kV style\ntransferring with a conditional CycleGAN model. The overall framework was\ntested quantitatively on synthetic respiratory motion scenarios and\nqualitatively on in-treatment images acquired over full scan series for liver\ncancer patients. Overall mean prediction errors for synthetic motion test\ndatasets were 0.16$\\pm$0.13 mm, 0.18$\\pm$0.19 mm, 0.22$\\pm$0.34 mm, and\n0.12$\\pm$0.11 mm. Mean peak prediction errors were 1.39 mm, 1.99 mm, 3.29 mm,\nand 1.16 mm.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"9oZjiBBl6F19BLqVXJRmr6p-v_C8Ie7FEZy9JvTC5mE","pdfSize":"3321494"}
