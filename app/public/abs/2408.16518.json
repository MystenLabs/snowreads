{"id":"2408.16518","title":"CNIMA: A Universal Evaluation Framework and Automated Approach for\n  Assessing Second Language Dialogues","authors":"Rena Gao, Jingxuan Wu, Carsten Roever, Xuetong Wu, Jing Wu, Long Lv,\n  Jey Han Lau","authorsParsed":[["Gao","Rena",""],["Wu","Jingxuan",""],["Roever","Carsten",""],["Wu","Xuetong",""],["Wu","Jing",""],["Lv","Long",""],["Lau","Jey Han",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 13:28:52 GMT"}],"updateDate":"2024-08-30","timestamp":1724938132000,"abstract":"  We develop CNIMA (Chinese Non-Native Interactivity Measurement and\nAutomation), a Chinese-as-a-second-language labelled dataset with 10K\ndialogues. We annotate CNIMA using an evaluation framework -- originally\nintroduced for English-as-a-second-language dialogues -- that assesses\nmicro-level features (e.g.\\ backchannels) and macro-level interactivity labels\n(e.g.\\ topic management) and test the framework's transferability from English\nto Chinese. We found the framework robust across languages and revealed\nuniversal and language-specific relationships between micro-level and\nmacro-level features. Next, we propose an approach to automate the evaluation\nand find strong performance, creating a new tool for automated second language\nassessment. Our system can be adapted to other languages easily as it uses\nlarge language models and as such does not require large-scale annotated\ntraining data.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HTHQDugX0-4SQVjTASxwxGnmtyJHlgiMfD72u1dwlTg","pdfSize":"1618492"}
