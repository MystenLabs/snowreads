{"id":"2407.19674","title":"Advancing Prompt Learning through an External Layer","authors":"Fangming Cui, Xun Yang, Chao Wu, Liang Xiao, Xinmei Tian","authorsParsed":[["Cui","Fangming",""],["Yang","Xun",""],["Wu","Chao",""],["Xiao","Liang",""],["Tian","Xinmei",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 03:30:09 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 05:26:13 GMT"},{"version":"v3","created":"Wed, 7 Aug 2024 17:45:05 GMT"},{"version":"v4","created":"Thu, 8 Aug 2024 02:39:15 GMT"},{"version":"v5","created":"Fri, 9 Aug 2024 06:09:44 GMT"}],"updateDate":"2024-08-12","timestamp":1722223809000,"abstract":"  Prompt learning represents a promising method for adapting pre-trained\nvision-language models (VLMs) to various downstream tasks by learning a set of\ntext embeddings. One challenge inherent to these methods is the poor\ngeneralization performance due to the invalidity of the learned text embeddings\nfor unseen tasks. A straightforward approach to bridge this gap is to freeze\nthe text embeddings in prompts, which results in a lack of capacity to adapt\nVLMs for downstream tasks. To address this dilemma, we propose a paradigm\ncalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose a\ntextual external layer and learnable visual embeddings for adapting VLMs to\ndownstream tasks. The learnable external layer is built upon valid embeddings\nof pre-trained CLIP. This design considers the balance of learning capabilities\nbetween the two branches. To align the textual and visual features, we propose\na novel two-pronged approach: i) we introduce the optimal transport as the\ndiscrepancy metric to align the vision and text modalities, and ii) we\nintroduce a novel strengthening feature to enhance the interaction between\nthese two modalities. Four representative experiments (i.e., base-to-novel\ngeneralization, few-shot learning, cross-dataset generalization, domain shifts\ngeneralization) across 15 datasets demonstrate that our method outperforms the\nexisting prompt learning method.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}