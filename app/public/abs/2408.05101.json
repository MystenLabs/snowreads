{"id":"2408.05101","title":"MooER: LLM-based Speech Recognition and Translation Models from Moore\n  Threads","authors":"Junhao Xu, Zhenlin Liang, Yi Liu, Yichao Hu, Jian Li, Yajun Zheng,\n  Meng Cai, Hua Wang","authorsParsed":[["Xu","Junhao",""],["Liang","Zhenlin",""],["Liu","Yi",""],["Hu","Yichao",""],["Li","Jian",""],["Zheng","Yajun",""],["Cai","Meng",""],["Wang","Hua",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 14:43:56 GMT"}],"updateDate":"2024-08-12","timestamp":1723214636000,"abstract":"  In this paper, we present MooER, a LLM-based large-scale automatic speech\nrecognition (ASR) / automatic speech translation (AST) model of Moore Threads.\nA 5000h pseudo labeled dataset containing open source and self collected speech\ndata is used for training. We achieve performance comparable to other open\nsource models trained with up to hundreds of thousands of hours of labeled\nspeech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest\nthat our model outperforms other open source Speech LLMs. A BLEU score of 25.2\ncan be obtained. The main contributions of this paper are summarized as\nfollows. First, this paper presents a training strategy for encoders and LLMs\non speech related tasks (including ASR and AST) using a small size of pseudo\nlabeled data without any extra manual annotation and selection. Second, we\nrelease our ASR and AST models and plan to open-source our training code and\nstrategy in the near future. Moreover, a model trained on 8wh scale training\ndata is planned to be released later on.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}