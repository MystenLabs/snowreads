{"id":"2408.14834","title":"Strategic Optimization and Challenges of Large Language Models in\n  Object-Oriented Programming","authors":"Zinan Wang","authorsParsed":[["Wang","Zinan",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 07:44:16 GMT"}],"updateDate":"2024-08-28","timestamp":1724744656000,"abstract":"  In the area of code generation research, the emphasis has transitioned from\ncrafting individual functions to developing class-level method code that\nintegrates contextual information. This shift has brought several benchmarks\nsuch as ClassEval and CoderEval, which consider class-level contexts.\nNevertheless, the influence of specific contextual factors at the method level\nremains less explored.\n  This research focused on method-level code generation within the\nObject-Oriented Programming (OOP) framework. Based on CoderEval, we devised\nexperiments that varied the extent of contextual information in the prompts,\nranging from method-specific to project-wide details. We introduced the\ninnovative metric of \"Prompt-Token Cost-Effectiveness\" to evaluate the economic\nviability of incorporating additional contextual layers. Our findings indicate\nthat prompts enriched with method invocation details yield the highest\ncost-effectiveness. Additionally, our study revealed disparities among Large\nLanguage Models (LLMs) regarding error type distributions and the level of\nassistance they provide to developers. Notably, larger LLMs do not invariably\nperform better. We also observed that tasks with higher degrees of coupling\npresent more substantial challenges, suggesting that the choice of LLM should\nbe tailored to the task's coupling degree. For example, GPT-4 exhibited\nimproved performance in low-coupling scenarios, whereas GPT-3.5 seemed better\nsuited for tasks with high coupling. By meticulously curating prompt content\nand selecting the appropriate LLM, developers can optimize code quality while\nmaximizing cost-efficiency during the development process.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}