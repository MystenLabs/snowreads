{"id":"2407.11061","title":"Exploring the Boundaries of On-Device Inference: When Tiny Falls Short,\n  Go Hierarchical","authors":"Adarsh Prasad Behera, Paulius Daubaris, I\\~naki Bravo, Jos\\'e Gallego,\n  Roberto Morabito, Joerg Widmer, Jaya Prakash Varma Champati","authorsParsed":[["Behera","Adarsh Prasad",""],["Daubaris","Paulius",""],["Bravo","Iñaki",""],["Gallego","José",""],["Morabito","Roberto",""],["Widmer","Joerg",""],["Champati","Jaya Prakash Varma",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:05:43 GMT"}],"updateDate":"2024-07-17","timestamp":1720627543000,"abstract":"  On-device inference holds great potential for increased energy efficiency,\nresponsiveness, and privacy in edge ML systems. However, due to less capable ML\nmodels that can be embedded in resource-limited devices, use cases are limited\nto simple inference tasks such as visual keyword spotting, gesture recognition,\nand predictive analytics. In this context, the Hierarchical Inference (HI)\nsystem has emerged as a promising solution that augments the capabilities of\nthe local ML by offloading selected samples to an edge server or cloud for\nremote ML inference. Existing works demonstrate through simulation that HI\nimproves accuracy. However, they do not account for the latency and energy\nconsumption on the device, nor do they consider three key heterogeneous\ndimensions that characterize ML systems: hardware, network connectivity, and\nmodels. In contrast, this paper systematically compares the performance of HI\nwith on-device inference based on measurements of accuracy, latency, and energy\nfor running embedded ML models on five devices with different capabilities and\nthree image classification datasets. For a given accuracy requirement, the HI\nsystems we designed achieved up to 73% lower latency and up to 77% lower device\nenergy consumption than an on-device inference system. The key to building an\nefficient HI system is the availability of small-size, reasonably accurate\non-device models whose outputs can be effectively differentiated for samples\nthat require remote inference. Despite the performance gains, HI requires\non-device inference for all samples, which adds a fixed overhead to its latency\nand energy consumption. Therefore, we design a hybrid system, Early Exit with\nHI (EE-HI), and demonstrate that compared to HI, EE-HI reduces the latency by\nup to 59.7% and lowers the device's energy consumption by up to 60.4%.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"zNthXj0EV0WnyiN1w8RqZU6XFOqnSD7yQPhMLRvlVow","pdfSize":"5489758"}
