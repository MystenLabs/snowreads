{"id":"2407.11569","title":"SFPNet: Sparse Focal Point Network for Semantic Segmentation on General\n  LiDAR Point Clouds","authors":"Yanbo Wang, Wentao Zhao, Chuan Cao, Tianchen Deng, Jingchuan Wang,\n  Weidong Chen","authorsParsed":[["Wang","Yanbo",""],["Zhao","Wentao",""],["Cao","Chuan",""],["Deng","Tianchen",""],["Wang","Jingchuan",""],["Chen","Weidong",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 10:22:09 GMT"}],"updateDate":"2024-07-17","timestamp":1721125329000,"abstract":"  Although LiDAR semantic segmentation advances rapidly, state-of-the-art\nmethods often incorporate specifically designed inductive bias derived from\nbenchmarks originating from mechanical spinning LiDAR. This can limit model\ngeneralizability to other kinds of LiDAR technologies and make hyperparameter\ntuning more complex. To tackle these issues, we propose a generalized framework\nto accommodate various types of LiDAR prevalent in the market by replacing\nwindow-attention with our sparse focal point modulation. Our SFPNet is capable\nof extracting multi-level contexts and dynamically aggregating them using a\ngate mechanism. By implementing a channel-wise information query, features that\nincorporate both local and global contexts are encoded. We also introduce a\nnovel large-scale hybrid-solid LiDAR semantic segmentation dataset for robotic\napplications. SFPNet demonstrates competitive performance on conventional\nbenchmarks derived from mechanical spinning LiDAR, while achieving\nstate-of-the-art results on benchmark derived from solid-state LiDAR.\nAdditionally, it outperforms existing methods on our novel dataset sourced from\nhybrid-solid LiDAR. Code and dataset are available at\nhttps://github.com/Cavendish518/SFPNet and https://www.semanticindustry.top.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}