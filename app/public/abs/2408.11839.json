{"id":"2408.11839","title":"Adaptive Friction in Deep Learning: Enhancing Optimizers with Sigmoid\n  and Tanh Function","authors":"Hongye Zheng, Bingxing Wang, Minheng Xiao, Honglin Qin, Zhizhong Wu,\n  Lianghao Tan","authorsParsed":[["Zheng","Hongye",""],["Wang","Bingxing",""],["Xiao","Minheng",""],["Qin","Honglin",""],["Wu","Zhizhong",""],["Tan","Lianghao",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 03:20:46 GMT"}],"updateDate":"2024-08-23","timestamp":1723000846000,"abstract":"  Adaptive optimizers are pivotal in guiding the weight updates of deep neural\nnetworks, yet they often face challenges such as poor generalization and\noscillation issues. To counter these, we introduce sigSignGrad and\ntanhSignGrad, two novel optimizers that integrate adaptive friction\ncoefficients based on the Sigmoid and Tanh functions, respectively. These\nalgorithms leverage short-term gradient information, a feature overlooked in\ntraditional Adam variants like diffGrad and AngularGrad, to enhance parameter\nupdates and convergence.Our theoretical analysis demonstrates the wide-ranging\nadjustment capability of the friction coefficient S, which aligns with targeted\nparameter update strategies and outperforms existing methods in both\noptimization trajectory smoothness and convergence rate. Extensive experiments\non CIFAR-10, CIFAR-100, and Mini-ImageNet datasets using ResNet50 and ViT\narchitectures confirm the superior performance of our proposed optimizers,\nshowcasing improved accuracy and reduced training time. The innovative approach\nof integrating adaptive friction coefficients as plug-ins into existing\noptimizers, exemplified by the sigSignAdamW and sigSignAdamP variants, presents\na promising strategy for boosting the optimization performance of established\nalgorithms. The findings of this study contribute to the advancement of\noptimizer design in deep learning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}