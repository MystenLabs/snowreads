{"id":"2407.20341","title":"BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger\n  Visual Cues","authors":"Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara","authorsParsed":[["Sarto","Sara",""],["Cornia","Marcella",""],["Baraldi","Lorenzo",""],["Cucchiara","Rita",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 18:00:17 GMT"}],"updateDate":"2024-07-31","timestamp":1722276017000,"abstract":"  Effectively aligning with human judgment when evaluating machine-generated\nimage captions represents a complex yet intriguing challenge. Existing\nevaluation metrics like CIDEr or CLIP-Score fall short in this regard as they\ndo not take into account the corresponding image or lack the capability of\nencoding fine-grained details and penalizing hallucinations. To overcome these\nissues, in this paper, we propose BRIDGE, a new learnable and reference-free\nimage captioning metric that employs a novel module to map visual features into\ndense vectors and integrates them into multi-modal pseudo-captions which are\nbuilt during the evaluation process. This approach results in a multimodal\nmetric that properly incorporates information from the input image without\nrelying on reference captions, bridging the gap between human judgment and\nmachine-generated image captions. Experiments spanning several datasets\ndemonstrate that our proposal achieves state-of-the-art results compared to\nexisting reference-free evaluation scores. Our source code and trained models\nare publicly available at: https://github.com/aimagelab/bridge-score.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}