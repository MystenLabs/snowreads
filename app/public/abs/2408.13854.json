{"id":"2408.13854","title":"Tangram: A Challenging Benchmark for Geometric Element Recognizing","authors":"Jiamin Tang and Chao Zhang and Xudong Zhu and Mengchi Liu","authorsParsed":[["Tang","Jiamin",""],["Zhang","Chao",""],["Zhu","Xudong",""],["Liu","Mengchi",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 14:47:25 GMT"}],"updateDate":"2024-08-27","timestamp":1724597245000,"abstract":"  Significant advancements in Large Multimodal Models (LMMs) have enabled them\nto tackle complex problems involving visual-mathematical reasoning. However,\ntheir ability to identify geometric elements remains understudied. To bridge\nthis gap, we introduce Tangram, a novel benchmark designed to evaluate the\nperformance of LMMs on geometric element recognition. Tangram includes 1,080\ndiverse geometric diagrams sourced from primary and secondary school exams,\ncompetitions, and textbooks, covering from simple basic geometric shapes to\ncomplex combinations. Each diagram is associated with four questions, resulting\nin a total of 4,320 visual-question-answer pairs. Unlike existing benchmarks\nthat seek higher-level cognition and reasoning, Tangram focuses on the\nunderstanding of geometric elements, requiring models to perform a \"simple but\ninteresting\" counting task. Systematic evaluation of 10 prominent LMMs, such as\nGPT-4o and Claude 3.5 Sonnet, shows that even in the seemingly simple task,\nthese models still face significant challenges. Notably, the overall accuracy\nof the top performer across all tested models is only 56.8%, marking a\nsignificant gap when compared to human performance. These findings highlight\nthe limitations of current multimodal artificial intelligence systems in\nhandling basic perception tasks, and will inspire the development of the next\ngeneration of expert-level multimodal foundational models. The Tangram and\nevaluation code will be available soon.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}