{"id":"2408.01708","title":"AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual\n  Segmentation","authors":"Zili Wang, Qi Yang, Linsu Shi, Jiazhong Yu, Qinghua Liang, Fei Li and\n  Shiming Xiang","authorsParsed":[["Wang","Zili",""],["Yang","Qi",""],["Shi","Linsu",""],["Yu","Jiazhong",""],["Liang","Qinghua",""],["Li","Fei",""],["Xiang","Shiming",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 08:25:26 GMT"}],"updateDate":"2024-08-06","timestamp":1722673526000,"abstract":"  Recently, transformer-based models have demonstrated remarkable performance\non audio-visual segmentation (AVS) tasks. However, their expensive\ncomputational cost makes real-time inference impractical. By characterizing\nattention maps of the network, we identify two key obstacles in AVS models: 1)\nattention dissipation, corresponding to the over-concentrated attention weights\nby Softmax within restricted frames, and 2) inefficient, burdensome transformer\ndecoder, caused by narrow focus patterns in early stages. In this paper, we\nintroduce AVESFormer, the first real-time Audio-Visual Efficient Segmentation\ntransformer that achieves fast, efficient and light-weight simultaneously. Our\nmodel leverages an efficient prompt query generator to correct the behaviour of\ncross-attention. Additionally, we propose ELF decoder to bring greater\nefficiency by facilitating convolutions suitable for local features to reduce\ncomputational burdens. Extensive experiments demonstrate that our AVESFormer\nsignificantly enhances model performance, achieving 79.9% on S4, 57.9% on MS3\nand 31.2% on AVSS, outperforming previous state-of-the-art and achieving an\nexcellent trade-off between performance and speed. Code can be found at\nhttps://github.com/MarkXCloud/AVESFormer.git.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}