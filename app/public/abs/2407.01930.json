{"id":"2407.01930","title":"Self-Cooperation Knowledge Distillation for Novel Class Discovery","authors":"Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Yunquan Sun, and Lizhe Qi","authorsParsed":[["Wang","Yuzheng",""],["Chen","Zhaoyu",""],["Yang","Dingkang",""],["Sun","Yunquan",""],["Qi","Lizhe",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 03:49:48 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 14:51:09 GMT"}],"updateDate":"2024-07-04","timestamp":1719892188000,"abstract":"  Novel Class Discovery (NCD) aims to discover unknown and novel classes in an\nunlabeled set by leveraging knowledge already learned about known classes.\nExisting works focus on instance-level or class-level knowledge representation\nand build a shared representation space to achieve performance improvements.\nHowever, a long-neglected issue is the potential imbalanced number of samples\nfrom known and novel classes, pushing the model towards dominant classes.\nTherefore, these methods suffer from a challenging trade-off between reviewing\nknown classes and discovering novel classes. Based on this observation, we\npropose a Self-Cooperation Knowledge Distillation (SCKD) method to utilize each\ntraining sample (whether known or novel, labeled or unlabeled) for both review\nand discovery. Specifically, the model's feature representations of known and\nnovel classes are used to construct two disjoint representation spaces. Through\nspatial mutual information, we design a self-cooperation learning to encourage\nmodel learning from the two feature representation spaces from itself.\nExtensive experiments on six datasets demonstrate that our method can achieve\nsignificant performance improvements, achieving state-of-the-art performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}