{"id":"2408.08430","title":"Random Gradient Masking as a Defensive Measure to Deep Leakage in\n  Federated Learning","authors":"Joon Kim, Sejin Park","authorsParsed":[["Kim","Joon",""],["Park","Sejin",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 21:43:26 GMT"}],"updateDate":"2024-08-19","timestamp":1723758206000,"abstract":"  Federated Learning(FL), in theory, preserves privacy of individual clients'\ndata while producing quality machine learning models. However, attacks such as\nDeep Leakage from Gradients(DLG) severely question the practicality of FL. In\nthis paper, we empirically evaluate the efficacy of four defensive methods\nagainst DLG: Masking, Clipping, Pruning, and Noising. Masking, while only\npreviously studied as a way to compress information during parameter transfer,\nshows surprisingly robust defensive utility when compared to the other three\nestablished methods. Our experimentation is two-fold. We first evaluate the\nminimum hyperparameter threshold for each method across MNIST, CIFAR-10, and\nlfw datasets. Then, we train FL clients with each method and their minimum\nthreshold values to investigate the trade-off between DLG defense and training\nperformance. Results reveal that Masking and Clipping show near to none\ndegradation in performance while obfuscating enough information to effectively\ndefend against DLG.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nf4lKOxhTxAPGL4UQW-E6wPXasGdvPr7-7PvCChf8Vk","pdfSize":"1140581"}
