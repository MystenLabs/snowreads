{"id":"2408.05517","title":"SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning","authors":"Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze\n  Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou and Yingda\n  Chen","authorsParsed":[["Zhao","Yuze",""],["Huang","Jintao",""],["Hu","Jinghan",""],["Wang","Xingjun",""],["Mao","Yunlin",""],["Zhang","Daoze",""],["Jiang","Zeyinzi",""],["Wu","Zhikai",""],["Ai","Baole",""],["Wang","Ang",""],["Zhou","Wenmeng",""],["Chen","Yingda",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 11:00:13 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 09:22:21 GMT"},{"version":"v3","created":"Mon, 19 Aug 2024 03:21:21 GMT"}],"updateDate":"2024-08-20","timestamp":1723287613000,"abstract":"  Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the most\ncomprehensive support for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8m-0cwP9BGW2KbTdPxBCwkZJ8VSu3Z3HkC7FGLnLGuQ","pdfSize":"939998"}
