{"id":"2408.10456","title":"Differentially Private Stochastic Gradient Descent with Fixed-Size\n  Minibatches: Tighter RDP Guarantees with or without Replacement","authors":"Jeremiah Birrell, Reza Ebrahimi, Rouzbeh Behnia, Jason Pacheco","authorsParsed":[["Birrell","Jeremiah",""],["Ebrahimi","Reza",""],["Behnia","Rouzbeh",""],["Pacheco","Jason",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 23:57:31 GMT"}],"updateDate":"2024-08-21","timestamp":1724111851000,"abstract":"  Differentially private stochastic gradient descent (DP-SGD) has been\ninstrumental in privately training deep learning models by providing a\nframework to control and track the privacy loss incurred during training. At\nthe core of this computation lies a subsampling method that uses a privacy\namplification lemma to enhance the privacy guarantees provided by the additive\nnoise. Fixed size subsampling is appealing for its constant memory usage,\nunlike the variable sized minibatches in Poisson subsampling. It is also of\ninterest in addressing class imbalance and federated learning. However, the\ncurrent computable guarantees for fixed-size subsampling are not tight and do\nnot consider both add/remove and replace-one adjacency relationships. We\npresent a new and holistic R{\\'e}nyi differential privacy (RDP) accountant for\nDP-SGD with fixed-size subsampling without replacement (FSwoR) and with\nreplacement (FSwR). For FSwoR we consider both add/remove and replace-one\nadjacency. Our FSwoR results improves on the best current computable bound by a\nfactor of $4$. We also show for the first time that the widely-used Poisson\nsubsampling and FSwoR with replace-one adjacency have the same privacy to\nleading order in the sampling probability. Accordingly, our work suggests that\nFSwoR is often preferable to Poisson subsampling due to constant memory usage.\nOur FSwR accountant includes explicit non-asymptotic upper and lower bounds\nand, to the authors' knowledge, is the first such analysis of fixed-size RDP\nwith replacement for DP-SGD. We analytically and empirically compare fixed size\nand Poisson subsampling, and show that DP-SGD gradients in a fixed-size\nsubsampling regime exhibit lower variance in practice in addition to memory\nusage benefits.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}