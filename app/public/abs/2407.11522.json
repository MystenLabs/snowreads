{"id":"2407.11522","title":"FIRE: A Dataset for Feedback Integration and Refinement Evaluation of\n  Multimodal Models","authors":"Pengxiang Li, Zhi Gao, Bofei Zhang, Tao Yuan, Yuwei Wu, Mehrtash\n  Harandi, Yunde Jia, Song-Chun Zhu, Qing Li","authorsParsed":[["Li","Pengxiang",""],["Gao","Zhi",""],["Zhang","Bofei",""],["Yuan","Tao",""],["Wu","Yuwei",""],["Harandi","Mehrtash",""],["Jia","Yunde",""],["Zhu","Song-Chun",""],["Li","Qing",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 09:00:45 GMT"}],"updateDate":"2024-07-17","timestamp":1721120445000,"abstract":"  Vision language models (VLMs) have achieved impressive progress in diverse\napplications, becoming a prevalent research direction. In this paper, we build\nFIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn\nconversations that are derived from 27 source datasets, empowering VLMs to\nspontaneously refine their responses based on user feedback across diverse\ntasks. To scale up the data collection, FIRE is collected in two components:\nFIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is\nfreely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a\nbenchmark to comprehensively evaluate the feedback-refining capability of VLMs,\nwhich contains 11K feedback-refinement conversations as the test data, two\nevaluation settings, and a model to provide feedback for VLMs. We develop the\nFIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows\nremarkable feedback-refining capability on FIRE-Bench and outperforms untrained\nVLMs by 50%, making more efficient user-agent interactions and underscoring the\nsignificance of the FIRE dataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}