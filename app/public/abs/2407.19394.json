{"id":"2407.19394","title":"Depth-Wise Convolutions in Vision Transformers for Efficient Training on\n  Small Datasets","authors":"Tianxiao Zhang, Wenju Xu, Bo Luo, Guanghui Wang","authorsParsed":[["Zhang","Tianxiao",""],["Xu","Wenju",""],["Luo","Bo",""],["Wang","Guanghui",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 04:23:40 GMT"},{"version":"v2","created":"Thu, 1 Aug 2024 04:22:29 GMT"},{"version":"v3","created":"Fri, 2 Aug 2024 10:05:03 GMT"}],"updateDate":"2024-08-05","timestamp":1722140620000,"abstract":"  The Vision Transformer (ViT) leverages the Transformer's encoder to capture\nglobal information by dividing images into patches and achieves superior\nperformance across various computer vision tasks. However, the self-attention\nmechanism of ViT captures the global context from the outset, overlooking the\ninherent relationships between neighboring pixels in images or videos.\nTransformers mainly focus on global information while ignoring the fine-grained\nlocal details. Consequently, ViT lacks inductive bias during image or video\ndataset training. In contrast, convolutional neural networks (CNNs), with their\nreliance on local filters, possess an inherent inductive bias, making them more\nefficient and quicker to converge than ViT with less data. In this paper, we\npresent a lightweight Depth-Wise Convolution module as a shortcut in ViT\nmodels, bypassing entire Transformer blocks to ensure the models capture both\nlocal and global information with minimal overhead. Additionally, we introduce\ntwo architecture variants, allowing the Depth-Wise Convolution modules to be\napplied to multiple Transformer blocks for parameter savings, and incorporating\nindependent parallel Depth-Wise Convolution modules with different kernels to\nenhance the acquisition of local information. The proposed approach\nsignificantly boosts the performance of ViT models on image classification,\nobject detection and instance segmentation by a large margin, especially on\nsmall datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet\nfor image classification, and COCO for object detection and instance\nsegmentation. The source code can be accessed at\nhttps://github.com/ZTX-100/Efficient_ViT_with_DW.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}