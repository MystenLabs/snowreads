{"id":"2407.08980","title":"Enabling Elastic Model Serving with MultiWorld","authors":"Myungjin Lee, Akshay Jajoo, Ramana Rao Kompella","authorsParsed":[["Lee","Myungjin",""],["Jajoo","Akshay",""],["Kompella","Ramana Rao",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 04:31:55 GMT"}],"updateDate":"2024-07-15","timestamp":1720758715000,"abstract":"  Machine learning models have been exponentially growing in terms of their\nparameter size over the past few years. We are now seeing the rise of\ntrillion-parameter models. The large models cannot fit into a single GPU and\nthus require partitioned deployment across GPUs and even hosts. A\nhigh-performance collective communication library (CCL) such as NCCL is\nessential to fully utilize expensive GPU resources. However, CCL is not a great\nfit for inference. Unlike training for which a fixed amount of GPU resources is\nused for fixed workloads (e.g., input datasets), the inference workloads can\nchange dynamically over time. Failures at the serving time can also impact\nindividual user's experiences directly. In contrast, workers in a CCL process\ngroup share a single fault domain and the process group cannot grow as the\nworkloads increase. The gap between the unique characteristics of model serving\nand CCL's nature makes it hard to serve large models elastically. To bridge the\ngap, we propose MultiWorld that enables fault tolerance and online scaling at\nthe granularity of workers for model serving. Our evaluation showcases that\nenabling these new functionalities incurs small overheads (1.4-4.3% throughput\nloss) for most of the scenarios we tested.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}