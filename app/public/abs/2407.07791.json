{"id":"2407.07791","title":"Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent\n  Communities","authors":"Tianjie Ju, Yiting Wang, Xinbei Ma, Pengzhou Cheng, Haodong Zhao,\n  Yulong Wang, Lifeng Liu, Jian Xie, Zhuosheng Zhang, Gongshen Liu","authorsParsed":[["Ju","Tianjie",""],["Wang","Yiting",""],["Ma","Xinbei",""],["Cheng","Pengzhou",""],["Zhao","Haodong",""],["Wang","Yulong",""],["Liu","Lifeng",""],["Xie","Jian",""],["Zhang","Zhuosheng",""],["Liu","Gongshen",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:08:46 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 01:59:54 GMT"}],"updateDate":"2024-07-24","timestamp":1720627726000,"abstract":"  The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}