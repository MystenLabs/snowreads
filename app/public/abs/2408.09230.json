{"id":"2408.09230","title":"Siamese Multiple Attention Temporal Convolution Networks for Human\n  Mobility Signature Identification","authors":"Zhipeng Zheng, Yuchen Jiang, Shiyao Zhang, and Xuetao Wei","authorsParsed":[["Zheng","Zhipeng",""],["Jiang","Yuchen",""],["Zhang","Shiyao",""],["Wei","Xuetao",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 15:27:38 GMT"}],"updateDate":"2024-08-20","timestamp":1723908458000,"abstract":"  The Human Mobility Signature Identification (HuMID) problem stands as a\nfundamental task within the realm of driving style representation, dedicated to\ndiscerning latent driving behaviors and preferences from diverse driver\ntrajectories for driver identification. Its solutions hold significant\nimplications across various domains (e.g., ride-hailing, insurance), wherein\ntheir application serves to safeguard users and mitigate potential fraudulent\nactivities. Present HuMID solutions often exhibit limitations in adaptability\nwhen confronted with lengthy trajectories, consequently incurring substantial\ncomputational overhead. Furthermore, their inability to effectively extract\ncrucial local information further impedes their performance. To address this\nproblem, we propose a Siamese Multiple Attention Temporal Convolutional Network\n(Siamese MA-TCN) to capitalize on the strengths of both TCN architecture and\nmulti-head self-attention, enabling the proficient extraction of both local and\nlong-term dependencies. Additionally, we devise a novel attention mechanism\ntailored for the efficient aggregation of multi-scale representations derived\nfrom our model. Experimental evaluations conducted on two real-world taxi\ntrajectory datasets reveal that our proposed model effectively extracts both\nlocal key information and long-term dependencies. These findings highlight the\nmodel's outstanding generalization capabilities, demonstrating its robustness\nand adaptability across datasets of varying sizes.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}