{"id":"2407.09150","title":"Evaluating the Adversarial Robustness of Semantic Segmentation: Trying\n  Harder Pays Off","authors":"Levente Halmosi, B\\'alint Mohos, M\\'ark Jelasity","authorsParsed":[["Halmosi","Levente",""],["Mohos","Bálint",""],["Jelasity","Márk",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 10:32:53 GMT"}],"updateDate":"2024-07-15","timestamp":1720780373000,"abstract":"  Machine learning models are vulnerable to tiny adversarial input\nperturbations optimized to cause a very large output error. To measure this\nvulnerability, we need reliable methods that can find such adversarial\nperturbations. For image classification models, evaluation methodologies have\nemerged that have stood the test of time. However, we argue that in the area of\nsemantic segmentation, a good approximation of the sensitivity to adversarial\nperturbations requires significantly more effort than what is currently\nconsidered satisfactory. To support this claim, we re-evaluate a number of\nwell-known robust segmentation models in an extensive empirical study. We\npropose new attacks and combine them with the strongest attacks available in\nthe literature. We also analyze the sensitivity of the models in fine detail.\nThe results indicate that most of the state-of-the-art models have a\ndramatically larger sensitivity to adversarial perturbations than previously\nreported. We also demonstrate a size-bias: small objects are often more easily\nattacked, even if the large objects are robust, a phenomenon not revealed by\ncurrent evaluation metrics. Our results also demonstrate that a diverse set of\nstrong attacks is necessary, because different models are often vulnerable to\ndifferent attacks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"2tjR7oAQ-baie3IdN7sBo6ERiSb_Odv_GvdpNFRJmFk","pdfSize":"15487942"}
