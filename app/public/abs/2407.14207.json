{"id":"2407.14207","title":"Longhorn: State Space Models are Amortized Online Learners","authors":"Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qiang Liu","authorsParsed":[["Liu","Bo",""],["Wang","Rui",""],["Wu","Lemeng",""],["Feng","Yihao",""],["Stone","Peter",""],["Liu","Qiang",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 11:12:08 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 16:24:59 GMT"},{"version":"v3","created":"Fri, 26 Jul 2024 02:03:00 GMT"},{"version":"v4","created":"Wed, 31 Jul 2024 22:09:50 GMT"}],"updateDate":"2024-08-02","timestamp":1721387528000,"abstract":"  The most fundamental capability of modern AI methods such as Large Language\nModels (LLMs) is the ability to predict the next token in a long sequence of\ntokens, known as ``sequence modeling.\" Although the Transformers model is the\ncurrent dominant approach to sequence modeling, its quadratic computational\ncost with respect to sequence length is a significant drawback. State-space\nmodels (SSMs) offer a promising alternative due to their linear decoding\nefficiency and high parallelizability during training. However, existing SSMs\noften rely on seemingly ad hoc linear recurrence designs. In this work, we\nexplore SSM design through the lens of online learning, conceptualizing SSMs as\nmeta-modules for specific online learning problems. This approach links SSM\ndesign to formulating precise online learning objectives, with state transition\nrules derived from optimizing these objectives. Based on this insight, we\nintroduce a novel deep SSM architecture based on the implicit update for\noptimizing an online regression objective. Our experimental results show that\nour models outperform state-of-the-art SSMs, including the Mamba model, on\nstandard sequence modeling benchmarks and language modeling tasks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}