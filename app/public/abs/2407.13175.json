{"id":"2407.13175","title":"OVGNet: A Unified Visual-Linguistic Framework for Open-Vocabulary\n  Robotic Grasping","authors":"Li Meng, Zhao Qi, Lyu Shuchang, Wang Chunlei, Ma Yujing, Cheng\n  Guangliang, Yang Chenguang","authorsParsed":[["Meng","Li",""],["Qi","Zhao",""],["Shuchang","Lyu",""],["Chunlei","Wang",""],["Yujing","Ma",""],["Guangliang","Cheng",""],["Chenguang","Yang",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 05:30:00 GMT"}],"updateDate":"2024-07-19","timestamp":1721280600000,"abstract":"  Recognizing and grasping novel-category objects remains a crucial yet\nchallenging problem in real-world robotic applications. Despite its\nsignificance, limited research has been conducted in this specific domain. To\naddress this, we seamlessly propose a novel framework that integrates\nopen-vocabulary learning into the domain of robotic grasping, empowering robots\nwith the capability to adeptly handle novel objects. Our contributions are\nthreefold. Firstly, we present a large-scale benchmark dataset specifically\ntailored for evaluating the performance of open-vocabulary grasping tasks.\nSecondly, we propose a unified visual-linguistic framework that serves as a\nguide for robots in successfully grasping both base and novel objects. Thirdly,\nwe introduce two alignment modules designed to enhance visual-linguistic\nperception in the robotic grasping process. Extensive experiments validate the\nefficacy and utility of our approach. Notably, our framework achieves an\naverage accuracy of 71.2\\% and 64.4\\% on base and novel categories in our new\ndataset, respectively.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"m61daFnovRwwUaR3G3p0NQiEzaG45NN3pFuajT06U30","pdfSize":"2498489","objectId":"0x09c2fd8424fb986ca1663784618f93b665f70cb2af237720179fea78891b4edb","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
