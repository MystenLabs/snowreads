{"id":"2408.14051","title":"Let Video Teaches You More: Video-to-Image Knowledge Distillation using\n  DEtection TRansformer for Medical Video Lesion Detection","authors":"Yuncheng Jiang, Zixun Zhang, Jun Wei, Chun-Mei Feng, Guanbin Li, Xiang\n  Wan, Shuguang Cui, Zhen Li","authorsParsed":[["Jiang","Yuncheng",""],["Zhang","Zixun",""],["Wei","Jun",""],["Feng","Chun-Mei",""],["Li","Guanbin",""],["Wan","Xiang",""],["Cui","Shuguang",""],["Li","Zhen",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 07:17:05 GMT"}],"updateDate":"2024-08-27","timestamp":1724656625000,"abstract":"  AI-assisted lesion detection models play a crucial role in the early\nscreening of cancer. However, previous image-based models ignore the\ninter-frame contextual information present in videos. On the other hand,\nvideo-based models capture the inter-frame context but are computationally\nexpensive. To mitigate this contradiction, we delve into Video-to-Image\nknowledge distillation leveraging DEtection TRansformer (V2I-DETR) for the task\nof medical video lesion detection. V2I-DETR adopts a teacher-student network\nparadigm. The teacher network aims at extracting temporal contexts from\nmultiple frames and transferring them to the student network, and the student\nnetwork is an image-based model dedicated to fast prediction in inference. By\ndistilling multi-frame contexts into a single frame, the proposed V2I-DETR\ncombines the advantages of utilizing temporal contexts from video-based models\nand the inference speed of image-based models. Through extensive experiments,\nV2I-DETR outperforms previous state-of-the-art methods by a large margin while\nachieving the real-time inference speed (30 FPS) as the image-based model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}