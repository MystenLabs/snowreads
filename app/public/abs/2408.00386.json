{"id":"2408.00386","title":"What comes after transformers? -- A selective survey connecting ideas in\n  deep learning","authors":"Johannes Schneider","authorsParsed":[["Schneider","Johannes",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 08:50:25 GMT"}],"updateDate":"2024-08-02","timestamp":1722502225000,"abstract":"  Transformers have become the de-facto standard model in artificial\nintelligence since 2017 despite numerous shortcomings ranging from energy\ninefficiency to hallucinations. Research has made a lot of progress in\nimproving elements of transformers, and, more generally, deep learning\nmanifesting in many proposals for architectures, layers, optimization\nobjectives, and optimization techniques. For researchers it is difficult to\nkeep track of such developments on a broader level. We provide a comprehensive\noverview of the many important, recent works in these areas to those who\nalready have a basic understanding of deep learning. Our focus differs from\nother works, as we target specifically novel, alternative potentially\ndisruptive approaches to transformers as well as successful ideas of recent\ndeep learning. We hope that such a holistic and unified treatment of\ninfluential, recent works and novel ideas helps researchers to form new\nconnections between diverse areas of deep learning. We identify and discuss\nmultiple patterns that summarize the key strategies for successful innovations\nover the last decade as well as works that can be seen as rising stars.\nEspecially, we discuss attempts on how to improve on transformers covering\n(partially) proven methods such as state space models but also including\nfar-out ideas in deep learning that seem promising despite not achieving\nstate-of-the-art results. We also cover a discussion on recent state-of-the-art\nmodels such as OpenAI's GPT series and Meta's LLama models and, Google's Gemini\nmodel family.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3dcR9fyAPzAIVu-sIn8g3C97yylQrFnfigRRk0ugpF8","pdfSize":"473758","txDigest":"ESoaMKs7nyHRcYYKao6focSjZYgiR2nX7CvGmeUyXuGb","endEpoch":"1","status":"CERTIFIED"}
