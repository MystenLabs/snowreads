{"id":"2407.17636","title":"IgnitionInnovators at \"Discharge Me!\": Chain-of-Thought Instruction\n  Finetuning Large Language Models for Discharge Summaries","authors":"An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh","authorsParsed":[["Tang","An Quang",""],["Zhang","Xiuzhen",""],["Dinh","Minh Ngoc",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 21:02:53 GMT"}],"updateDate":"2024-07-26","timestamp":1721854973000,"abstract":"  This paper presents our proposed approach to the Discharge Me! shared task,\ncollocated with the 23th Workshop on Biomedical Natural Language Processing\n(BioNLP). In this work, we develop an LLM-based framework for solving the\nDischarge Summary Documentation (DSD) task, i.e., generating the two critical\ntarget sections `Brief Hospital Course' and `Discharge Instructions' in the\ndischarge summary. By streamlining the recent instruction-finetuning process on\nLLMs, we explore several prompting strategies for optimally adapting LLMs to\nspecific generation task of DSD. Experimental results show that providing a\nclear output structure, complimented by a set of comprehensive\nChain-of-Thoughts (CoT) questions, effectively improves the model's reasoning\ncapability, and thereby, enhancing the structural correctness and faithfulness\nof clinical information in the generated text. Source code is available at:\nhttps://github.com/antangrocket1312/Discharge_LLM\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"lFYfHgzAJgkmb5eKFD2t-ZobRvgg-vV6URPcEQZr_yI","pdfSize":"407387","objectId":"0xbc0a5ee6e7fd4911e9ff23a5f89e3c9ffd336fbaa98b4fd72448d10a3294be97","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
