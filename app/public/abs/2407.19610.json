{"id":"2407.19610","title":"Mixture of Modular Experts: Distilling Knowledge from a Multilingual\n  Teacher into Specialized Modular Language Models","authors":"Mohammed Al-Maamari, Mehdi Ben Amor, Michael Granitzer","authorsParsed":[["Al-Maamari","Mohammed",""],["Amor","Mehdi Ben",""],["Granitzer","Michael",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 23:42:09 GMT"}],"updateDate":"2024-07-30","timestamp":1722210129000,"abstract":"  This research combines Knowledge Distillation (KD) and Mixture of Experts\n(MoE) to develop modular, efficient multilingual language models. Key\nobjectives include evaluating adaptive versus fixed alpha methods in KD and\ncomparing modular MoE architectures for handling multi-domain inputs and\npreventing catastrophic forgetting. KD compresses large language models (LLMs)\ninto smaller, efficient models, while MoE enhances modularity with specialized\ntasks. Experiments showed similar performance for both KD methods, with\nmarginal improvements from adaptive alpha. A combined loss approach provided\nmore stable learning. The router, trained to classify input sequences into\nEnglish, French, German, or Python, achieved 99.95% precision, recall, and F1\nscore, with Logistic Regression being the most effective classifier.\nEvaluations of modular MoE architectures revealed that Pre-trained Language\nExperts (PLE) and Joint Expert Embedding Training (JEET) performed similarly,\nwhile the MoE with Common Expert (MoE-CE) setup showed slightly lower\nperformance. Including a common expert in MoE-CE improved its performance.\nStudies on catastrophic forgetting indicated that sequential training led to\nsignificant forgetting, while single-session training with balanced batches and\nthe MoE approach mitigated this issue. The MoE architecture preserved knowledge\nacross multiple languages effectively.\n  The research contributes open-sourced resources including the dataset\n(https://zenodo.org/doi/10.5281/zenodo.12677631), a balanced dataset creation\ntool (https://github.com/padas-lab-de/multi-language-dataset-creator), and the\nresearch codebase (https://github.com/ModMaamari/mixture-modular-experts).\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"5DBjvC7S85kfhdmAgDgVJpc938q6S1lvxrWAmpSKwTA","pdfSize":"1089143","objectId":"0x216b2dfe1e05df8f16ffc4517329113dda560c4a5adae2f9235010b85fca0de7","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
