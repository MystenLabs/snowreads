{"id":"2408.09880","title":"Fast Hermitian Diagonalization with Nearly Optimal Precision","authors":"Rikhav Shah","authorsParsed":[["Shah","Rikhav",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 10:43:07 GMT"}],"updateDate":"2024-08-20","timestamp":1724064187000,"abstract":"  Algorithms for numerical tasks in finite precision simultaneously seek to\nminimize the number of floating point operations performed, and also the number\nof bits of precision required by each floating point operation. This paper\npresents an algorithm for Hermitian diagonalization requiring only\n$\\lg(1/\\varepsilon)+O(\\log(n)+\\log\\log(1/\\varepsilon))$ bits of precision where\n$n$ is the size of the input matrix and $\\varepsilon$ is the target error.\nFurthermore, it runs in near matrix multiplication time.\n  In the general setting, the first complete analysis of the stability of a\nnear matrix multiplication time algorithm for diagonalization is that of Banks\net al. [BGVKS20]. They exhibit an algorithm for diagonalizing an arbitrary\nmatrix up to $\\varepsilon$ backward error using only\n$O(\\log^4(n/\\varepsilon)\\log(n))$ bits of precision. This work focuses on the\nHermitian setting, where we determine a dramatically improved bound on the\nnumber of bits needed. In particular, the result is close to providing a\npractical bound. The exact bit count depends on the specific implementation of\nmatrix multiplication and QR decomposition one wishes to use, but if one uses\nsuitable $O(n^3)$-time implementations, then for $\\varepsilon=10^{-15},n=4000$,\nwe show 92 bits of precision suffice (and 59 are necessary). By comparison, the\nsame parameters in [BGVKS20] does not even show that 682,916,525,000 bits\nsuffice.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}