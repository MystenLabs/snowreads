{"id":"2407.10068","title":"Multi-Granularity Semantic Revision for Large Language Model\n  Distillation","authors":"Xiaoyu Liu, Yun Zhang, Wei Li, Simiao Li, Xudong Huang, Hanting Chen,\n  Yehui Tang, Jie Hu, Zhiwei Xiong, Yunhe Wang","authorsParsed":[["Liu","Xiaoyu",""],["Zhang","Yun",""],["Li","Wei",""],["Li","Simiao",""],["Huang","Xudong",""],["Chen","Hanting",""],["Tang","Yehui",""],["Hu","Jie",""],["Xiong","Zhiwei",""],["Wang","Yunhe",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 03:51:49 GMT"}],"updateDate":"2024-07-16","timestamp":1720929109000,"abstract":"  Knowledge distillation plays a key role in compressing the Large Language\nModels (LLMs), which boosts a small-size student model under large teacher\nmodels' guidance. However, existing LLM distillation methods overly rely on\nstudent-generated outputs, which may introduce generation errors and misguide\nthe distillation process. Moreover, the distillation loss functions introduced\nin previous art struggle to align the most informative part due to the complex\ndistribution of LLMs' outputs. To address these problems, we propose a\nmulti-granularity semantic revision method for LLM distillation. At the\nsequence level, we propose a sequence correction and re-generation (SCRG)\nstrategy. SCRG first calculates the semantic cognitive difference between the\nteacher and student to detect the error token, then corrects it with the\nteacher-generated one, and re-generates the sequence to reduce generation\nerrors and enhance generation diversity. At the token level, we design a\ndistribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the\ndistillation objective function. DAC-KL loss exploits a learnable sub-network\nto adaptively extract semantically dense areas from the teacher's output,\navoiding the interference of redundant information in the distillation process.\nFinally, at the span level, we leverage the span priors of a sequence to\ncompute the probability correlations within spans, and constrain the teacher\nand student's probability correlations to be consistent, further enhancing the\ntransfer of semantic information. Extensive experiments across different model\nfamilies with parameters ranging from 0.1B to 13B demonstrate the superiority\nof our method compared to existing methods.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}