{"id":"2407.03736","title":"Semantic Grouping Network for Audio Source Separation","authors":"Shentong Mo, Yapeng Tian","authorsParsed":[["Mo","Shentong",""],["Tian","Yapeng",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 08:37:47 GMT"}],"updateDate":"2024-07-08","timestamp":1720082267000,"abstract":"  Recently, audio-visual separation approaches have taken advantage of the\nnatural synchronization between the two modalities to boost audio source\nseparation performance. They extracted high-level semantics from visual inputs\nas the guidance to help disentangle sound representation for individual\nsources. Can we directly learn to disentangle the individual semantics from the\nsound itself? The dilemma is that multiple sound sources are mixed together in\nthe original space. To tackle the difficulty, in this paper, we present a novel\nSemantic Grouping Network, termed as SGN, that can directly disentangle sound\nrepresentations and extract high-level semantic information for each source\nfrom input audio mixture. Specifically, SGN aggregates category-wise source\nfeatures through learnable class tokens of sounds. Then, the aggregated\nsemantic features can be used as the guidance to separate the corresponding\naudio sources from the mixture. We conducted extensive experiments on\nmusic-only and universal sound separation benchmarks: MUSIC, FUSS, MUSDB18, and\nVGG-Sound. The results demonstrate that our SGN significantly outperforms\nprevious audio-only methods and audio-visual models without utilizing\nadditional visual cues.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}