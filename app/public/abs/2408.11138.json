{"id":"2408.11138","title":"Target-Oriented Object Grasping via Multimodal Human Guidance","authors":"Pengwei Xie, Siang Chen, Dingchang Hu, Yixiang Dai, Kaiqin Yang,\n  Guijin Wang","authorsParsed":[["Xie","Pengwei",""],["Chen","Siang",""],["Hu","Dingchang",""],["Dai","Yixiang",""],["Yang","Kaiqin",""],["Wang","Guijin",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 18:42:56 GMT"}],"updateDate":"2024-08-22","timestamp":1724179376000,"abstract":"  In the context of human-robot interaction and collaboration scenarios,\nrobotic grasping still encounters numerous challenges. Traditional grasp\ndetection methods generally analyze the entire scene to predict grasps, leading\nto redundancy and inefficiency. In this work, we reconsider 6-DoF grasp\ndetection from a target-referenced perspective and propose a Target-Oriented\nGrasp Network (TOGNet). TOGNet specifically targets local, object-agnostic\nregion patches to predict grasps more efficiently. It integrates seamlessly\nwith multimodal human guidance, including language instructions, pointing\ngestures, and interactive clicks. Thus our system comprises two primary\nfunctional modules: a guidance module that identifies the target object in 3D\nspace and TOGNet, which detects region-focal 6-DoF grasps around the target,\nfacilitating subsequent motion planning. Through 50 target-grasping simulation\nexperiments in cluttered scenes, our system achieves a success rate improvement\nof about 13.7%. In real-world experiments, we demonstrate that our method\nexcels in various target-oriented grasping scenarios.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}