{"id":"2407.12346","title":"Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval","authors":"Naoya Sogi, Takashi Shibata, Makoto Terao","authorsParsed":[["Sogi","Naoya",""],["Shibata","Takashi",""],["Terao","Makoto",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 06:42:14 GMT"}],"updateDate":"2024-07-18","timestamp":1721198534000,"abstract":"  The pre-trained vision and language (V\\&L) models have substantially improved\nthe performance of cross-modal image-text retrieval. In general, however, V\\&L\nmodels have limited retrieval performance for small objects because of the\nrough alignment between words and the small objects in the image. In contrast,\nit is known that human cognition is object-centric, and we pay more attention\nto important objects, even if they are small. To bridge this gap between the\nhuman cognition and the V\\&L model's capability, we propose a cross-modal\nimage-text retrieval framework based on ``object-aware query perturbation.''\nThe proposed method generates a key feature subspace of the detected objects\nand perturbs the corresponding queries using this subspace to improve the\nobject awareness in the image. In our proposed method, object-aware cross-modal\nimage-text retrieval is possible while keeping the rich expressive power and\nretrieval performance of existing V\\&L models without additional fine-tuning.\nComprehensive experiments on four public datasets show that our method\noutperforms conventional algorithms.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}