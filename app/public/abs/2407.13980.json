{"id":"2407.13980","title":"Byzantine-tolerant distributed learning of finite mixture models","authors":"Qiong Zhang, Jiahua Chen","authorsParsed":[["Zhang","Qiong",""],["Chen","Jiahua",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 02:11:26 GMT"}],"updateDate":"2024-07-22","timestamp":1721355086000,"abstract":"  This paper proposes two split-and-conquer (SC) learning estimators for finite\nmixture models that are tolerant to Byzantine failures. In SC learning,\nindividual machines obtain local estimates, which are then transmitted to a\ncentral server for aggregation. During this communication, the server may\nreceive malicious or incorrect information from some local machines, a scenario\nknown as Byzantine failures. While SC learning approaches have been devised to\nmitigate Byzantine failures in statistical models with Euclidean parameters,\ndeveloping Byzantine-tolerant methods for finite mixture models with\nnon-Euclidean parameters requires a distinct strategy. Our proposed\ndistance-based methods are hyperparameter tuning free, unlike existing methods,\nand are resilient to Byzantine failures while achieving high statistical\nefficiency. We validate the effectiveness of our methods both theoretically and\nempirically via experiments on simulated and real data from machine learning\napplications for digit recognition. The code for the experiment can be found at\nhttps://github.com/SarahQiong/RobustSCGMM.\n","subjects":["Statistics/Methodology","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}