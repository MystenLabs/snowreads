{"id":"2408.04526","title":"Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs","authors":"Kevin Tan, Wei Fan, Yuting Wei","authorsParsed":[["Tan","Kevin",""],["Fan","Wei",""],["Wei","Yuting",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 15:26:18 GMT"}],"updateDate":"2024-08-09","timestamp":1723130778000,"abstract":"  Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}