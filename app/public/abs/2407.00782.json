{"id":"2407.00782","title":"Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical\n  Reasoning","authors":"Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan,\n  Mingjie Zhan, Hongsheng Li","authorsParsed":[["Lu","Zimu",""],["Zhou","Aojun",""],["Wang","Ke",""],["Ren","Houxing",""],["Shi","Weikang",""],["Pan","Junting",""],["Zhan","Mingjie",""],["Li","Hongsheng",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 17:59:07 GMT"},{"version":"v2","created":"Tue, 2 Jul 2024 04:46:35 GMT"},{"version":"v3","created":"Mon, 15 Jul 2024 02:03:54 GMT"}],"updateDate":"2024-07-16","timestamp":1719770347000,"abstract":"  Direct Preference Optimization (DPO) has proven effective at improving the\nperformance of large language models (LLMs) on downstream tasks such as\nreasoning and alignment. In this work, we propose Step-Controlled DPO (SCDPO),\na method for automatically providing stepwise error supervision by creating\nnegative samples of mathematical reasoning rationales that start making errors\nat a specified step. By applying these samples in DPO training, SCDPO can\nbetter align the model to understand reasoning errors and output accurate\nreasoning steps. We apply SCDPO to both code-integrated and chain-of-thought\nsolutions, empirically showing that it consistently improves the performance\ncompared to naive DPO on three different SFT models, including one existing SFT\nmodel and two models we finetuned. Qualitative analysis of the credit\nassignment of SCDPO and DPO demonstrates the effectiveness of SCDPO at\nidentifying errors in mathematical solutions. We then apply SCDPO to an\nInternLM2-20B model, resulting in a 20B model that achieves high scores of\n88.5% on GSM8K and 58.1% on MATH, rivaling all other open-source LLMs, showing\nthe great potential of our method.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AK92d6QtXMdA4W98neAYPcV3OQGYlvS5SsKQUspsA1k","pdfSize":"1190078"}
