{"id":"2407.06146","title":"Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling\n  Tasks","authors":"Lukas Netz, Jan Reimer, Bernhard Rumpe","authorsParsed":[["Netz","Lukas",""],["Reimer","Jan",""],["Rumpe","Bernhard",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:19:59 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 07:08:11 GMT"}],"updateDate":"2024-07-10","timestamp":1720459199000,"abstract":"  We present and evaluate a method called grammar masking, which is used to\nguide large language models (LLMs) toward producing syntactically correct\nmodels for a given context-free grammar. Prompt engineering methods such as\nfew-shot learning or priming can be used to improve the chances of an LLM\nproducing correct syntax, but the more complex the grammar, the more\ntime-consuming and less promising these methods become. Previous work is\nfocused primarily on the usage of either language model training or prompt\nengineering. In this work, a method is presented that restricts the output to a\ngiven grammar using constrained decoding to ensure the output adheres to a\nvalid syntax. We use several DSLs built with MontiCore and task multiple LLMs\nto produce models with and without constrained decoding. A corresponding parser\nis used to confirm the syntactic correctness of each model. We show that\ngrammar masking can dramatically improve the modeling capabilities of several\nLLMs, reducing the need for well-refined prompting while increasing the chance\nof producing correct models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/"}