{"id":"2407.06654","title":"SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language\n  Model Pre-training","authors":"Nan He, Weichen Xiong, Hanwen Liu, Yi Liao, Lei Ding, Kai Zhang,\n  Guohua Tang, Xiao Han, Wei Yang","authorsParsed":[["He","Nan",""],["Xiong","Weichen",""],["Liu","Hanwen",""],["Liao","Yi",""],["Ding","Lei",""],["Zhang","Kai",""],["Tang","Guohua",""],["Han","Xiao",""],["Yang","Wei",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 08:26:39 GMT"}],"updateDate":"2024-07-10","timestamp":1720513599000,"abstract":"  The effectiveness of large language models (LLMs) is often hindered by\nduplicated data in their extensive pre-training datasets. Current approaches\nprimarily focus on detecting and removing duplicates, which risks the loss of\nvaluable information and neglects the varying degrees of duplication. To\naddress this, we propose a soft deduplication method that maintains dataset\nintegrity while selectively reducing the sampling weight of data with high\ncommonness. Central to our approach is the concept of \"data commonness\", a\nmetric we introduce to quantify the degree of duplication by measuring the\noccurrence probabilities of samples using an n-gram model. Empirical analysis\nshows that this method significantly improves training efficiency, achieving\ncomparable perplexity scores with at least a 26% reduction in required training\nsteps. Additionally, it enhances average few-shot downstream accuracy by 1.77%\nwhen trained for an equivalent duration. Importantly, this approach\nconsistently improves performance, even on rigorously deduplicated datasets,\nindicating its potential to complement existing methods and become a standard\npre-training process for LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}