{"id":"2408.08137","title":"Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature\n  Attribution Explainability","authors":"Joakim Edin, Andreas Geert Motzfeldt, Casper L. Christensen, Tuukka\n  Ruotsalo, Lars Maal{\\o}e, Maria Maistro","authorsParsed":[["Edin","Joakim",""],["Motzfeldt","Andreas Geert",""],["Christensen","Casper L.",""],["Ruotsalo","Tuukka",""],["Maal√∏e","Lars",""],["Maistro","Maria",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 13:13:17 GMT"}],"updateDate":"2024-08-16","timestamp":1723727597000,"abstract":"  Deep neural network predictions are notoriously difficult to interpret.\nFeature attribution methods aim to explain these predictions by identifying the\ncontribution of each input feature. Faithfulness, often evaluated using the\narea over the perturbation curve (AOPC), reflects feature attributions'\naccuracy in describing the internal mechanisms of deep neural networks.\nHowever, many studies rely on AOPC to compare faithfulness across different\nmodels, which we show can lead to false conclusions about models' faithfulness.\nSpecifically, we find that AOPC is sensitive to variations in the model,\nresulting in unreliable cross-model comparisons. Moreover, AOPC scores are\ndifficult to interpret in isolation without knowing the model-specific lower\nand upper limits. To address these issues, we propose a normalization approach,\nNormalized AOPC (NAOPC), enabling consistent cross-model evaluations and more\nmeaningful interpretation of individual scores. Our experiments demonstrate\nthat this normalization can radically change AOPC results, questioning the\nconclusions of earlier studies and offering a more robust framework for\nassessing feature attribution faithfulness.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}