{"id":"2408.01099","title":"Contribution-based Low-Rank Adaptation with Pre-training Model for Real\n  Image Restoration","authors":"Donwon Park, Hayeon Kim, Se Young Chun","authorsParsed":[["Park","Donwon",""],["Kim","Hayeon",""],["Chun","Se Young",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 08:24:05 GMT"}],"updateDate":"2024-08-05","timestamp":1722587045000,"abstract":"  Recently, pre-trained model and efficient parameter tuning have achieved\nremarkable success in natural language processing and high-level computer\nvision with the aid of masked modeling and prompt tuning. In low-level computer\nvision, however, there have been limited investigations on pre-trained models\nand even efficient fine-tuning strategy has not yet been explored despite its\nimportance and benefit in various real-world tasks such as alleviating memory\ninflation issue when integrating new tasks on AI edge devices. Here, we propose\na novel efficient parameter tuning approach dubbed contribution-based low-rank\nadaptation (CoLoRA) for multiple image restorations along with effective\npre-training method with random order degradations (PROD). Unlike prior arts\nthat tune all network parameters, our CoLoRA effectively fine-tunes small\namount of parameters by leveraging LoRA (low-rank adaptation) for each new\nvision task with our contribution-based method to adaptively determine layer by\nlayer capacity for that task to yield comparable performance to full tuning.\nFurthermore, our PROD strategy allows to extend the capability of pre-trained\nmodels with improved performance as well as robustness to bridge synthetic\npre-training and real-world fine-tuning. Our CoLoRA with PROD has demonstrated\nits superior performance in various image restoration tasks across diverse\ndegradation types on both synthetic and real-world datasets for known and novel\ntasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}