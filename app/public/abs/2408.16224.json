{"id":"2408.16224","title":"LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in\n  Vision-Language Models","authors":"Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng","authorsParsed":[["Wang","Jingyi",""],["Ju","Jianzhong",""],["Luan","Jian",""],["Deng","Zhidong",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 02:43:20 GMT"},{"version":"v2","created":"Fri, 30 Aug 2024 02:49:40 GMT"}],"updateDate":"2024-09-02","timestamp":1724899400000,"abstract":"  Recent advances in large vision-language models (VLMs) typically employ\nvision encoders based on the Vision Transformer (ViT) architecture. The\ndivision of the images into patches by ViT results in a fragmented perception,\nthereby hindering the visual understanding capabilities of VLMs. In this paper,\nwe propose an innovative enhancement to address this limitation by introducing\na Scene Graph Expression (SGE) module in VLMs. This module extracts and\nstructurally expresses the complex semantic information within images, thereby\nimproving the foundational perception and understanding abilities of VLMs.\nExtensive experiments demonstrate that integrating our SGE module significantly\nenhances the VLM's performance in vision-language tasks, indicating its\neffectiveness in preserving intricate semantic details and facilitating better\nvisual understanding.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}