{"id":"2408.15715","title":"Autoregressive model path dependence near Ising criticality","authors":"Yi Hong Teoh and Roger G. Melko","authorsParsed":[["Teoh","Yi Hong",""],["Melko","Roger G.",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 11:21:33 GMT"}],"updateDate":"2024-08-29","timestamp":1724844093000,"abstract":"  Autoregressive models are a class of generative model that probabilistically\npredict the next output of a sequence based on previous inputs. The\nautoregressive sequence is by definition one-dimensional (1D), which is natural\nfor language tasks and hence an important component of modern architectures\nlike recurrent neural networks (RNNs) and transformers. However, when language\nmodels are used to predict outputs on physical systems that are not\nintrinsically 1D, the question arises of which choice of autoregressive\nsequence -- if any -- is optimal. In this paper, we study the reconstruction of\ncritical correlations in the two-dimensional (2D) Ising model, using RNNs and\ntransformers trained on binary spin data obtained near the thermal phase\ntransition. We compare the training performance for a number of different 1D\nautoregressive sequences imposed on finite-size 2D lattices. We find that paths\nwith long 1D segments are more efficient at training the autoregressive models\ncompared to space-filling curves that better preserve the 2D locality. Our\nresults illustrate the potential importance in choosing the optimal\nautoregressive sequence ordering when training modern language models for tasks\nin physics.\n","subjects":["Computing Research Repository/Machine Learning","Condensed Matter/Disordered Systems and Neural Networks"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RG6bYjYOubHi9SMiCeTZNbWwy6HFUg8RLmgl0hl-B1Y","pdfSize":"1223134"}
