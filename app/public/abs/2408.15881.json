{"id":"2408.15881","title":"LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation","authors":"Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Guanghao Zhang, Haonan\n  Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, Haoyuan Li, Bolin Li,\n  Zhelun Yu, Si Liu, Hongsheng Li, Hao Jiang","authorsParsed":[["Shu","Fangxun",""],["Liao","Yue",""],["Zhuo","Le",""],["Xu","Chenning",""],["Zhang","Guanghao",""],["Shi","Haonan",""],["Chen","Long",""],["Zhong","Tao",""],["He","Wanggui",""],["Fu","Siming",""],["Li","Haoyuan",""],["Li","Bolin",""],["Yu","Zhelun",""],["Liu","Si",""],["Li","Hongsheng",""],["Jiang","Hao",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 15:52:23 GMT"}],"updateDate":"2024-08-29","timestamp":1724860343000,"abstract":"  We introduce LLaVA-MoD, a novel framework designed to enable the efficient\ntraining of small-scale Multimodal Language Models (s-MLLM) by distilling\nknowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamental\nchallenges in MLLM distillation. First, we optimize the network structure of\ns-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the\nlanguage model, striking a balance between computational efficiency and model\nexpressiveness. Second, we propose a progressive knowledge transfer strategy to\nensure comprehensive knowledge migration. This strategy begins with mimic\ndistillation, where we minimize the Kullback-Leibler (KL) divergence between\noutput distributions to enable the student model to emulate the teacher\nnetwork's understanding. Following this, we introduce preference distillation\nvia Direct Preference Optimization (DPO), where the key lies in treating l-MLLM\nas the reference model. During this phase, the s-MLLM's ability to discriminate\nbetween superior and inferior examples is significantly enhanced beyond l-MLLM,\nleading to a better student that surpasses its teacher, particularly in\nhallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoD\noutperforms existing models across various multimodal benchmarks while\nmaintaining a minimal number of activated parameters and low computational\ncosts. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpasses\nQwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% of\nthe training data and 23% trainable parameters. These results underscore\nLLaVA-MoD's ability to effectively distill comprehensive knowledge from its\nteacher model, paving the way for the development of more efficient MLLMs. The\ncode will be available on: https://github.com/shufangxun/LLaVA-MoD.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8CT8tufxJSWWDnmFqUyJrsoGjQJXOUbFkIPNn5uyLZ0","pdfSize":"1139041"}
