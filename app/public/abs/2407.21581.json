{"id":"2407.21581","title":"InScope: A New Real-world 3D Infrastructure-side Collaborative\n  Perception Dataset for Open Traffic Scenarios","authors":"Xiaofei Zhang, Yining Li, Jinping Wang, Xiangyi Qin, Ying Shen,\n  Zhengping Fan, and Xiaojun Tan","authorsParsed":[["Zhang","Xiaofei",""],["Li","Yining",""],["Wang","Jinping",""],["Qin","Xiangyi",""],["Shen","Ying",""],["Fan","Zhengping",""],["Tan","Xiaojun",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 13:11:14 GMT"}],"updateDate":"2024-08-01","timestamp":1722431474000,"abstract":"  Perception systems of autonomous vehicles are susceptible to occlusion,\nespecially when examined from a vehicle-centric perspective. Such occlusion can\nlead to overlooked object detections, e.g., larger vehicles such as trucks or\nbuses may create blind spots where cyclists or pedestrians could be obscured,\naccentuating the safety concerns associated with such perception system\nlimitations. To mitigate these challenges, the vehicle-to-everything (V2X)\nparadigm suggests employing an infrastructure-side perception system (IPS) to\ncomplement autonomous vehicles with a broader perceptual scope. Nevertheless,\nthe scarcity of real-world 3D infrastructure-side datasets constrains the\nadvancement of V2X technologies. To bridge these gaps, this paper introduces a\nnew 3D infrastructure-side collaborative perception dataset, abbreviated as\ninscope. Notably, InScope is the first dataset dedicated to addressing\nocclusion challenges by strategically deploying multiple-position Light\nDetection and Ranging (LiDAR) systems on the infrastructure side. Specifically,\nInScope encapsulates a 20-day capture duration with 303 tracking trajectories\nand 187,787 3D bounding boxes annotated by experts. Through analysis of\nbenchmarks, four different benchmarks are presented for open traffic scenarios,\nincluding collaborative 3D object detection, multisource data fusion, data\ndomain transfer, and 3D multiobject tracking tasks. Additionally, a new metric\nis designed to quantify the impact of occlusion, facilitating the evaluation of\ndetection degradation ratios among various algorithms. The Experimental\nfindings showcase the enhanced performance of leveraging InScope to assist in\ndetecting and tracking 3D multiobjects in real-world scenarios, particularly in\ntracking obscured, small, and distant objects. The dataset and benchmarks are\navailable at https://github.com/xf-zh/InScope.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"U59e5Ooh0HKlJbRbO7-yQ9UhBsOJLBcG8wP_q301BJk","pdfSize":"14055631"}
