{"id":"2408.13237","title":"JacNet: Learning Functions with Structured Jacobians","authors":"Jonathan Lorraine, Safwan Hossain","authorsParsed":[["Lorraine","Jonathan",""],["Hossain","Safwan",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 17:21:44 GMT"}],"updateDate":"2024-08-26","timestamp":1724433704000,"abstract":"  Neural networks are trained to learn an approximate mapping from an input\ndomain to a target domain. Incorporating prior knowledge about true mappings is\ncritical to learning a useful approximation. With current architectures, it is\nchallenging to enforce structure on the derivatives of the input-output\nmapping. We propose to use a neural network to directly learn the Jacobian of\nthe input-output function, which allows easy control of the derivative. We\nfocus on structuring the derivative to allow invertibility and also demonstrate\nthat other useful priors, such as $k$-Lipschitz, can be enforced. Using this\napproach, we can learn approximations to simple functions that are guaranteed\nto be invertible and easily compute the inverse. We also show similar results\nfor 1-Lipschitz functions.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}