{"id":"2408.17422","title":"Open-vocabulary Temporal Action Localization using VLMs","authors":"Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu,\n  Katsushi Ikeuchi","authorsParsed":[["Wake","Naoki",""],["Kanehira","Atsushi",""],["Sasabuchi","Kazuhiro",""],["Takamatsu","Jun",""],["Ikeuchi","Katsushi",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 17:12:14 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 16:00:58 GMT"},{"version":"v3","created":"Sun, 8 Sep 2024 01:12:25 GMT"}],"updateDate":"2024-09-10","timestamp":1725037934000,"abstract":"  Video action localization aims to find timings of a specific action from a\nlong video. Although existing learning-based approaches have been successful,\nthose require annotating videos that come with a considerable labor cost. This\npaper proposes a learning-free, open-vocabulary approach based on emerging\noff-the-shelf vision-language models (VLM). The challenge stems from the fact\nthat VLMs are neither designed to process long videos nor tailored for finding\nactions. We overcome these problems by extending an iterative visual prompting\ntechnique. Specifically, we sample video frames into a concatenated image with\nframe index labels, making a VLM guess a frame that is considered to be closest\nto the start/end of the action. Iterating this process by narrowing a sampling\ntime window results in finding a specific frame of start and end of an action.\nWe demonstrate that this sampling technique yields reasonable results,\nillustrating a practical extension of VLMs for understanding videos. A sample\ncode is available at\nhttps://microsoft.github.io/VLM-Video-Action-Localization/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}