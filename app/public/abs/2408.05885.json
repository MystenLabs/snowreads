{"id":"2408.05885","title":"GFlowNet Training by Policy Gradients","authors":"Puhua Niu, Shili Wu, Mingzhou Fan, Xiaoning Qian","authorsParsed":[["Niu","Puhua",""],["Wu","Shili",""],["Fan","Mingzhou",""],["Qian","Xiaoning",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 01:24:49 GMT"}],"updateDate":"2024-08-13","timestamp":1723425889000,"abstract":"  Generative Flow Networks (GFlowNets) have been shown effective to generate\ncombinatorial objects with desired properties. We here propose a new GFlowNet\ntraining framework, with policy-dependent rewards, that bridges keeping flow\nbalance of GFlowNets to optimizing the expected accumulated reward in\ntraditional Reinforcement-Learning (RL). This enables the derivation of new\npolicy-based GFlowNet training methods, in contrast to existing ones resembling\nvalue-based RL. It is known that the design of backward policies in GFlowNet\ntraining affects efficiency. We further develop a coupled training strategy\nthat jointly solves GFlowNet forward policy training and backward policy\ndesign. Performance analysis is provided with a theoretical guarantee of our\npolicy-based GFlowNet training. Experiments on both simulated and real-world\ndatasets verify that our policy-based strategies provide advanced RL\nperspectives for robust gradient estimation to improve GFlowNet performance.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"sRnyXlywsulyEJHK3E7CnbEXsgYu2mKv0f1EAWCN6KU","pdfSize":"3072008"}
