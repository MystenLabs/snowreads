{"id":"2407.08521","title":"Emergent Visual-Semantic Hierarchies in Image-Text Representations","authors":"Morris Alper and Hadar Averbuch-Elor","authorsParsed":[["Alper","Morris",""],["Averbuch-Elor","Hadar",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 14:09:42 GMT"},{"version":"v2","created":"Mon, 15 Jul 2024 19:40:44 GMT"}],"updateDate":"2024-07-17","timestamp":1720706982000,"abstract":"  While recent vision-and-language models (VLMs) like CLIP are a powerful tool\nfor analyzing text and images in a shared semantic space, they do not\nexplicitly model the hierarchical nature of the set of texts which may describe\nan image. Conversely, existing multimodal hierarchical representation learning\nmethods require costly training from scratch, failing to leverage the knowledge\nencoded by state-of-the-art multimodal foundation models. In this work, we\nstudy the knowledge of existing foundation models, finding that they exhibit\nemergent understanding of visual-semantic hierarchies despite not being\ndirectly trained for this purpose. We propose the Radial Embedding (RE)\nframework for probing and optimizing hierarchical understanding, and contribute\nthe HierarCaps dataset, a benchmark facilitating the study of hierarchical\nknowledge in image--text representations, constructed automatically via large\nlanguage models. Our results show that foundation VLMs exhibit zero-shot\nhierarchical understanding, surpassing the performance of prior models\nexplicitly designed for this purpose. Furthermore, we show that foundation\nmodels may be better aligned to hierarchical reasoning via a text-only\nfine-tuning phase, while retaining pretraining knowledge.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}