{"id":"2407.15814","title":"Perceptions of Linguistic Uncertainty by Language Models and Humans","authors":"Catarina G Belem, Markelle Kelly, Mark Steyvers, Sameer Singh,\n  Padhraic Smyth","authorsParsed":[["Belem","Catarina G",""],["Kelly","Markelle",""],["Steyvers","Mark",""],["Singh","Sameer",""],["Smyth","Padhraic",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 17:26:12 GMT"}],"updateDate":"2024-07-23","timestamp":1721669172000,"abstract":"  Uncertainty expressions such as ``probably'' or ``highly unlikely'' are\npervasive in human language. While prior work has established that there is\npopulation-level agreement in terms of how humans interpret these expressions,\nthere has been little inquiry into the abilities of language models to\ninterpret such expressions. In this paper, we investigate how language models\nmap linguistic expressions of uncertainty to numerical responses. Our approach\nassesses whether language models can employ theory of mind in this setting:\nunderstanding the uncertainty of another agent about a particular statement,\nindependently of the model's own certainty about that statement. We evaluate\nboth humans and 10 popular language models on a task created to assess these\nabilities. Unexpectedly, we find that 8 out of 10 models are able to map\nuncertainty expressions to probabilistic responses in a human-like manner.\nHowever, we observe systematically different behavior depending on whether a\nstatement is actually true or false. This sensitivity indicates that language\nmodels are substantially more susceptible to bias based on their prior\nknowledge (as compared to humans). These findings raise important questions and\nhave broad implications for human-AI alignment and AI-AI communication.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}