{"id":"2408.11481","title":"E-Bench: Subjective-Aligned Benchmark Suite for Text-Driven Video\n  Editing Quality Assessment","authors":"Shangkun Sun, Xiaoyu Liang, Songlin Fan, Wenxu Gao and Wei Gao","authorsParsed":[["Sun","Shangkun",""],["Liang","Xiaoyu",""],["Fan","Songlin",""],["Gao","Wenxu",""],["Gao","Wei",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 09:49:32 GMT"}],"updateDate":"2024-08-22","timestamp":1724233772000,"abstract":"  Text-driven video editing has recently experienced rapid development. Despite\nthis, evaluating edited videos remains a considerable challenge. Current\nmetrics tend to fail to align with human perceptions, and effective\nquantitative metrics for video editing are still notably absent. To address\nthis, we introduce E-Bench, a benchmark suite tailored to the assessment of\ntext-driven video editing. This suite includes E-Bench DB, a video quality\nassessment (VQA) database for video editing. E-Bench DB encompasses a diverse\nset of source videos featuring various motions and subjects, along with\nmultiple distinct editing prompts, editing results from 8 different models, and\nthe corresponding Mean Opinion Scores (MOS) from 24 human annotators. Based on\nE-Bench DB, we further propose E-Bench QA, a quantitative human-aligned\nmeasurement for the text-driven video editing task. In addition to the\naesthetic, distortion, and other visual quality indicators that traditional VQA\nmethods emphasize, E-Bench QA focuses on the text-video alignment and the\nrelevance modeling between source and edited videos. It proposes a new\nassessment network for video editing that attains superior performance in\nalignment with human preferences. To the best of our knowledge, E-Bench\nintroduces the first quality assessment dataset for video editing and an\neffective subjective-aligned quantitative metric for this domain. All data and\ncode will be publicly available at https://github.com/littlespray/E-Bench.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}