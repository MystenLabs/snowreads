{"id":"2408.11374","title":"A Unified Framework for Continual Learning and Machine Unlearning","authors":"Romit Chatterjee, Vikram Chundawat, Ayush Tarun, Ankur Mali, Murari\n  Mandal","authorsParsed":[["Chatterjee","Romit",""],["Chundawat","Vikram",""],["Tarun","Ayush",""],["Mali","Ankur",""],["Mandal","Murari",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 06:49:59 GMT"}],"updateDate":"2024-08-22","timestamp":1724222999000,"abstract":"  Continual learning and machine unlearning are crucial challenges in machine\nlearning, typically addressed separately. Continual learning focuses on\nadapting to new knowledge while preserving past information, whereas unlearning\ninvolves selectively forgetting specific subsets of data. In this paper, we\nintroduce a novel framework that jointly tackles both tasks by leveraging\ncontrolled knowledge distillation. Our approach enables efficient learning with\nminimal forgetting and effective targeted unlearning. By incorporating a fixed\nmemory buffer, the system supports learning new concepts while retaining prior\nknowledge. The distillation process is carefully managed to ensure a balance\nbetween acquiring new information and forgetting specific data as needed.\nExperimental results on benchmark datasets show that our method matches or\nexceeds the performance of existing approaches in both continual learning and\nmachine unlearning. This unified framework is the first to address both\nchallenges simultaneously, paving the way for adaptable models capable of\ndynamic learning and forgetting while maintaining strong overall performance.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}