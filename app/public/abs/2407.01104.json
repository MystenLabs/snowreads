{"id":"2407.01104","title":"Semantic-guided Adversarial Diffusion Model for Self-supervised Shadow\n  Removal","authors":"Ziqi Zeng, Chen Zhao, Weiling Cai, Chenyu Dong","authorsParsed":[["Zeng","Ziqi",""],["Zhao","Chen",""],["Cai","Weiling",""],["Dong","Chenyu",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 09:14:38 GMT"}],"updateDate":"2024-07-02","timestamp":1719825278000,"abstract":"  Existing unsupervised methods have addressed the challenges of inconsistent\npaired data and tedious acquisition of ground-truth labels in shadow removal\ntasks. However, GAN-based training often faces issues such as mode collapse and\nunstable optimization. Furthermore, due to the complex mapping between shadow\nand shadow-free domains, merely relying on adversarial learning is not enough\nto capture the underlying relationship between two domains, resulting in low\nquality of the generated images. To address these problems, we propose a\nsemantic-guided adversarial diffusion framework for self-supervised shadow\nremoval, which consists of two stages. At first stage a semantic-guided\ngenerative adversarial network (SG-GAN) is proposed to carry out a coarse\nresult and construct paired synthetic data through a cycle-consistent\nstructure. Then the coarse result is refined with a diffusion-based restoration\nmodule (DBRM) to enhance the texture details and edge artifact at second stage.\nMeanwhile, we propose a multi-modal semantic prompter (MSP) that aids in\nextracting accurate semantic information from real images and text, guiding the\nshadow removal network to restore images better in SG-GAN. We conduct\nexperiments on multiple public datasets, and the experimental results\ndemonstrate the effectiveness of our method.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}