{"id":"2407.12871","title":"MetaTool: Facilitating Large Language Models to Master Tools with\n  Meta-task Augmentation","authors":"Xiaohan Wang, Dian Li, Yilin Zhao, Sinbadliu, Hui Wang","authorsParsed":[["Wang","Xiaohan",""],["Li","Dian",""],["Zhao","Yilin",""],["Sinbadliu","",""],["Wang","Hui",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 10:15:41 GMT"}],"updateDate":"2024-07-19","timestamp":1721038541000,"abstract":"  Utilizing complex tools with Large Language Models (LLMs) is a critical\ncomponent for grounding AI agents in various real-world scenarios. The core\nchallenge of manipulating tools lies in understanding their usage and\nfunctionality. The prevailing approach involves few-shot prompting with\ndemonstrations or fine-tuning on expert trajectories. However, for complex\ntools and tasks, mere in-context demonstrations may fail to cover sufficient\nknowledge. Training-based methods are also constrained by the high cost of\ndataset construction and limited generalizability. In this paper, we introduce\na new tool learning methodology (MetaTool) that is generalizable for mastering\nany reusable toolset. Our approach includes a self-supervised data augmentation\ntechnique that enables LLMs to gain a comprehensive understanding of various\ntools, thereby improving their ability to complete tasks effectively. We\ndevelop a series of meta-tasks that involve predicting masked factors of tool\nexecution. These self-supervised tasks enable the automatic generation of\nhigh-quality QA data concerning tool comprehension. By incorporating meta-task\ndata into the instruction tuning process, the proposed MetaTool model achieves\nsignificant superiority to open-source models and is comparable to\nGPT-4/GPT-3.5 on multiple tool-oriented tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}