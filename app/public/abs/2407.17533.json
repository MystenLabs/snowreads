{"id":"2407.17533","title":"SFPrompt: Communication-Efficient Split Federated Fine-Tuning for Large\n  Pre-Trained Models over Resource-Limited Devices","authors":"Linxiao Cao, Yifei Zhu and Wei Gong","authorsParsed":[["Cao","Linxiao",""],["Zhu","Yifei",""],["Gong","Wei",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 04:22:37 GMT"}],"updateDate":"2024-07-26","timestamp":1721794957000,"abstract":"  Large pre-trained models have exhibited remarkable achievements across\nvarious domains. The substantial training costs associated with these models\nhave led to wide studies of fine-tuning for effectively harnessing their\ncapabilities in solving downstream tasks. Yet, conventional fine-tuning\napproaches become infeasible when the model lacks access to downstream data due\nto privacy concerns. Naively integrating fine-tuning approaches with the\nemerging federated learning frameworks incurs substantial communication\noverhead and exerts high demand on local computing resources, making it\nimpractical for common resource-limited devices. In this paper, we introduce\nSFPrompt, an innovative privacy-preserving fine-tuning method tailored for the\nfederated setting where direct uploading of raw data is prohibited and local\ndevices are resource-constrained to run a complete pre-trained model. In\nessence, SFPrompt judiciously combines split learning with federated learning\nto handle these challenges. Specifically, the pre-trained model is first\npartitioned into client and server components, thereby streamlining the\nclient-side model and substantially alleviating computational demands on local\nresources. SFPrompt then introduces soft prompts into the federated model to\nenhance the fine-tuning performance. To further reduce communication costs, a\nnovel dataset pruning algorithm and a local-loss update strategy are devised\nduring the fine-tuning process. Extensive experiments demonstrate that SFPrompt\ndelivers competitive performance as the federated full fine-tuning approach\nwhile consuming a mere 0.46% of local computing resources and incurring 53%\nless communication cost.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}