{"id":"2408.00426","title":"A Cross-Domain Benchmark for Active Learning","authors":"Thorben Werner, Johannes Burchert, Maximilian Stubbemann, Lars\n  Schmidt-Thieme","authorsParsed":[["Werner","Thorben",""],["Burchert","Johannes",""],["Stubbemann","Maximilian",""],["Schmidt-Thieme","Lars",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 09:57:48 GMT"}],"updateDate":"2024-08-02","timestamp":1722506268000,"abstract":"  Active Learning (AL) deals with identifying the most informative samples for\nlabeling to reduce data annotation costs for supervised learning tasks. AL\nresearch suffers from the fact that lifts from literature generalize poorly and\nthat only a small number of repetitions of experiments are conducted. To\novercome these obstacles, we propose \\emph{CDALBench}, the first active\nlearning benchmark which includes tasks in computer vision, natural language\nprocessing and tabular learning. Furthermore, by providing an efficient, greedy\noracle, \\emph{CDALBench} can be evaluated with 50 runs for each experiment. We\nshow, that both the cross-domain character and a large amount of repetitions\nare crucial for sophisticated evaluation of AL research. Concretely, we show\nthat the superiority of specific methods varies over the different domains,\nmaking it important to evaluate Active Learning with a cross-domain benchmark.\nAdditionally, we show that having a large amount of runs is crucial. With only\nconducting three runs as often done in the literature, the superiority of\nspecific methods can strongly vary with the specific runs. This effect is so\nstrong, that, depending on the seed, even a well-established method's\nperformance can be significantly better and significantly worse than random for\nthe same dataset.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}