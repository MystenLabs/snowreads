{"id":"2408.03043","title":"Targeted Visual Prompting for Medical Visual Question Answering","authors":"Sergio Tascon-Morales and Pablo M\\'arquez-Neila and Raphael Sznitman","authorsParsed":[["Tascon-Morales","Sergio",""],["MÃ¡rquez-Neila","Pablo",""],["Sznitman","Raphael",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 08:58:20 GMT"}],"updateDate":"2024-08-07","timestamp":1722934700000,"abstract":"  With growing interest in recent years, medical visual question answering\n(Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs)\nemerging as an alternative to classical model architectures. Specifically,\ntheir ability to add visual information to the input of pre-trained LLMs brings\nnew capabilities for image interpretation. However, simple visual errors cast\ndoubt on the actual visual understanding abilities of these models. To address\nthis, region-based questions have been proposed as a means to assess and\nenhance actual visual understanding through compositional evaluation. To\ncombine these two perspectives, this paper introduces targeted visual prompting\nto equip MLLMs with region-based questioning capabilities. By presenting the\nmodel with both the isolated region and the region in its context in a\ncustomized visual prompt, we show the effectiveness of our method across\nmultiple datasets while comparing it to several baseline models. Our code and\ndata are available at https://github.com/sergiotasconmorales/locvqallm.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VkSLkXwzgZF0b0MTciAqjD7rTqL297bULLeYV1Bis08","pdfSize":"21608273"}
