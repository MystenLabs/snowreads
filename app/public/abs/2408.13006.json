{"id":"2408.13006","title":"Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks:\n  Explainable Metrics and Diverse Prompt Templates","authors":"Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, Mei Han","authorsParsed":[["Wei","Hui",""],["He","Shenghua",""],["Xia","Tian",""],["Wong","Andy",""],["Lin","Jingyang",""],["Han","Mei",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 11:49:01 GMT"}],"updateDate":"2024-08-26","timestamp":1724413741000,"abstract":"  Alignment approaches such as RLHF and DPO are actively investigated to align\nlarge language models (LLMs) with human preferences. Commercial large language\nmodels (LLMs) like GPT-4 have been recently employed to evaluate and compare\ndifferent LLM alignment approaches. These models act as surrogates for human\nevaluators due to their promising abilities to approximate human preferences\nwith remarkably faster feedback and lower costs. This methodology is referred\nto as LLM-as-a-judge. However, concerns regarding its reliability have emerged,\nattributed to LLM judges' biases and inconsistent decision-making. Previous\nresearch has sought to develop robust evaluation frameworks for assessing the\nreliability of LLM judges and their alignment with human preferences. However,\nthe employed evaluation metrics often lack adequate explainability and fail to\naddress the internal inconsistency of LLMs. Additionally, existing studies\ninadequately explore the impact of various prompt templates when applying\nLLM-as-a-judge methods, which leads to potentially inconsistent comparisons\nbetween different alignment algorithms. In this work, we systematically\nevaluate LLM judges on alignment tasks (e.g. summarization) by defining\nevaluation metrics with improved theoretical interpretability and disentangling\nreliability metrics with LLM internal inconsistency. We develop a framework to\nevaluate, compare, and visualize the reliability and alignment of LLM judges to\nprovide informative observations that help choose LLM judges for alignment\ntasks. Our results indicate a significant impact of prompt templates on LLM\njudge performance, as well as a mediocre alignment level between the tested LLM\njudges and human evaluators.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"4DlFodPEp4rDS-gbhI81q0mdr3HQHiEe0CHL83SgHkk","pdfSize":"3811182"}
