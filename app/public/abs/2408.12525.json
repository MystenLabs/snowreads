{"id":"2408.12525","title":"PCGRL+: Scaling, Control and Generalization in Reinforcement Learning\n  Level Generators","authors":"Sam Earle, Zehua Jiang, Julian Togelius","authorsParsed":[["Earle","Sam",""],["Jiang","Zehua",""],["Togelius","Julian",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 16:30:24 GMT"}],"updateDate":"2024-08-23","timestamp":1724344224000,"abstract":"  Procedural Content Generation via Reinforcement Learning (PCGRL) has been\nintroduced as a means by which controllable designer agents can be trained\nbased only on a set of computable metrics acting as a proxy for the level's\nquality and key characteristics. While PCGRL offers a unique set of affordances\nfor game designers, it is constrained by the compute-intensive process of\ntraining RL agents, and has so far been limited to generating relatively small\nlevels. To address this issue of scale, we implement several PCGRL environments\nin Jax so that all aspects of learning and simulation happen in parallel on the\nGPU, resulting in faster environment simulation; removing the CPU-GPU transfer\nof information bottleneck during RL training; and ultimately resulting in\nsignificantly improved training speed. We replicate several key results from\nprior works in this new framework, letting models train for much longer than\npreviously studied, and evaluating their behavior after 1 billion timesteps.\nAiming for greater control for human designers, we introduce randomized level\nsizes and frozen \"pinpoints\" of pivotal game tiles as further ways of\ncountering overfitting. To test the generalization ability of learned\ngenerators, we evaluate models on large, out-of-distribution map sizes, and\nfind that partial observation sizes learn more robust design strategies.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}