{"id":"2407.03026","title":"Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End\n  Multi-Accent Speech Recognition","authors":"Jinming Chen, Jingyi Fang, Yuanzhong Zheng, Yaoxuan Wang, Haojun Fei","authorsParsed":[["Chen","Jinming",""],["Fang","Jingyi",""],["Zheng","Yuanzhong",""],["Wang","Yaoxuan",""],["Fei","Haojun",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 11:35:52 GMT"}],"updateDate":"2024-07-04","timestamp":1720006552000,"abstract":"  Currently, end-to-end (E2E) speech recognition methods have achieved\npromising performance. However, auto speech recognition (ASR) models still face\nchallenges in recognizing multi-accent speech accurately. We propose a\nlayer-adapted fusion (LAF) model, called Qifusion-Net, which does not require\nany prior knowledge about the target accent. Based on dynamic chunk strategy,\nour approach enables streaming decoding and can extract frame-level acoustic\nfeature, facilitating fine-grained information fusion. Experiment results\ndemonstrate that our proposed methods outperform the baseline with relative\nreductions of 22.1$\\%$ and 17.2$\\%$ in character error rate (CER) across multi\naccent test datasets on KeSpeech and MagicData-RMAC.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}