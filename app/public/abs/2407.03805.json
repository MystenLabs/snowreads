{"id":"2407.03805","title":"Cognitive Modeling with Scaffolded LLMs: A Case Study of Referential\n  Expression Generation","authors":"Polina Tsvilodub, Michael Franke, Fausto Carcassi","authorsParsed":[["Tsvilodub","Polina",""],["Franke","Michael",""],["Carcassi","Fausto",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 10:28:48 GMT"},{"version":"v2","created":"Mon, 8 Jul 2024 09:42:20 GMT"}],"updateDate":"2024-07-09","timestamp":1720088928000,"abstract":"  To what extent can LLMs be used as part of a cognitive model of language\ngeneration? In this paper, we approach this question by exploring a\nneuro-symbolic implementation of an algorithmic cognitive model of referential\nexpression generation by Dale & Reiter (1995). The symbolic task analysis\nimplements the generation as an iterative procedure that scaffolds symbolic and\ngpt-3.5-turbo-based modules. We compare this implementation to an ablated model\nand a one-shot LLM-only baseline on the A3DS dataset (Tsvilodub & Franke,\n2023). We find that our hybrid approach is cognitively plausible and performs\nwell in complex contexts, while allowing for more open-ended modeling of\nlanguage generation in a larger domain.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}