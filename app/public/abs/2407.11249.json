{"id":"2407.11249","title":"Disentangling Representations in RNNs through Multi-task Learning","authors":"Pantelis Vafidis, Aman Bhargava, Antonio Rangel","authorsParsed":[["Vafidis","Pantelis",""],["Bhargava","Aman",""],["Rangel","Antonio",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 21:32:58 GMT"}],"updateDate":"2024-07-17","timestamp":1721079178000,"abstract":"  Abstract, or disentangled, representations are a promising mathematical\nframework for efficient and effective generalization in both biological and\nartificial systems. We investigate abstract representations in the context of\nmulti-task classification over noisy evidence streams -- a canonical\ndecision-making neuroscience paradigm. We derive theoretical bounds that\nguarantee the emergence of disentangled representations in the latent state of\nany optimal multi-task classifier, when the number of tasks exceeds the\ndimensionality of the state space. We experimentally confirm that RNNs trained\non multi-task classification learn disentangled representations in the form of\ncontinuous attractors, leading to zero-shot out-of-distribution (OOD)\ngeneralization. We demonstrate the flexibility of the abstract RNN\nrepresentations across various decision boundary geometries and in tasks\nrequiring classification confidence estimation. Our framework suggests a\ngeneral principle for the formation of cognitive maps that organize knowledge\nto enable flexible generalization in biological and artificial systems alike,\nand closely relates to representations found in humans and animals during\ndecision-making and spatial reasoning tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Quantitative Biology/Neurons and Cognition","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}