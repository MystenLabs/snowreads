{"id":"2407.00118","title":"From Efficient Multimodal Models to World Models: A Survey","authors":"Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan\n  Kang, Yan Wang, Wenqiang Zhang","authorsParsed":[["Mai","Xinji",""],["Tao","Zeng",""],["Lin","Junxiong",""],["Wang","Haoran",""],["Chang","Yang",""],["Kang","Yanlan",""],["Wang","Yan",""],["Zhang","Wenqiang",""]],"versions":[{"version":"v1","created":"Thu, 27 Jun 2024 15:36:43 GMT"}],"updateDate":"2024-07-02","timestamp":1719502603000,"abstract":"  Multimodal Large Models (MLMs) are becoming a significant research focus,\ncombining powerful large language models with multimodal learning to perform\ncomplex tasks across different data modalities. This review explores the latest\ndevelopments and challenges in MLMs, emphasizing their potential in achieving\nartificial general intelligence and as a pathway to world models. We provide an\noverview of key techniques such as Multimodal Chain of Thought (M-COT),\nMultimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning\n(M-ICL). Additionally, we discuss both the fundamental and specific\ntechnologies of multimodal models, highlighting their applications,\ninput/output modalities, and design characteristics. Despite significant\nadvancements, the development of a unified multimodal model remains elusive. We\ndiscuss the integration of 3D generation and embodied intelligence to enhance\nworld simulation capabilities and propose incorporating external rule systems\nfor improved reasoning and decision-making. Finally, we outline future research\ndirections to address these challenges and advance the field.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}