{"id":"2408.01890","title":"Cross-layer Attention Sharing for Large Language Models","authors":"Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Qiaozhi\n  He, Murun Yang, Tong Xiao and Jingbo Zhu","authorsParsed":[["Mu","Yongyu",""],["Wu","Yuzhang",""],["Fan","Yuchun",""],["Wang","Chenglong",""],["Li","Hengyu",""],["He","Qiaozhi",""],["Yang","Murun",""],["Xiao","Tong",""],["Zhu","Jingbo",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 00:38:34 GMT"}],"updateDate":"2024-08-06","timestamp":1722731914000,"abstract":"  As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}