{"id":"2407.15131","title":"Token-Picker: Accelerating Attention in Text Generation with Minimized\n  Memory Transfer via Probability Estimation","authors":"Junyoung Park, Myeonggu Kang, Yunki Han, Yanggon Kim, Jaekang Shin,\n  Lee-Sup Kim","authorsParsed":[["Park","Junyoung",""],["Kang","Myeonggu",""],["Han","Yunki",""],["Kim","Yanggon",""],["Shin","Jaekang",""],["Kim","Lee-Sup",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 11:56:54 GMT"}],"updateDate":"2024-07-23","timestamp":1721563014000,"abstract":"  The attention mechanism in text generation is memory-bounded due to its\nsequential characteristics. Therefore, off-chip memory accesses should be\nminimized for faster execution. Although previous methods addressed this by\npruning unimportant tokens, they fall short in selectively removing tokens with\nnear-zero attention probabilities in each instance. Our method estimates the\nprobability before the softmax function, effectively removing low probability\ntokens and achieving an 12.1x pruning ratio without fine-tuning. Additionally,\nwe present a hardware design supporting seamless on-demand off-chip access. Our\napproach shows 2.6x reduced memory accesses, leading to an average 2.3x speedup\nand a 2.4x energy efficiency.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}