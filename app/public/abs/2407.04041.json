{"id":"2407.04041","title":"Towards Cross-View-Consistent Self-Supervised Surround Depth Estimation","authors":"Laiyan Ding, Hualie Jiang, Jie Li, Yongquan Chen, Rui Huang","authorsParsed":[["Ding","Laiyan",""],["Jiang","Hualie",""],["Li","Jie",""],["Chen","Yongquan",""],["Huang","Rui",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 16:29:05 GMT"}],"updateDate":"2024-07-08","timestamp":1720110545000,"abstract":"  Depth estimation is a cornerstone for autonomous driving, yet acquiring\nper-pixel depth ground truth for supervised learning is challenging.\nSelf-Supervised Surround Depth Estimation (SSSDE) from consecutive images\noffers an economical alternative. While previous SSSDE methods have proposed\ndifferent mechanisms to fuse information across images, few of them explicitly\nconsider the cross-view constraints, leading to inferior performance,\nparticularly in overlapping regions. This paper proposes an efficient and\nconsistent pose estimation design and two loss functions to enhance cross-view\nconsistency for SSSDE. For pose estimation, we propose to use only front-view\nimages to reduce training memory and sustain pose estimation consistency. The\nfirst loss function is the dense depth consistency loss, which penalizes the\ndifference between predicted depths in overlapping regions. The second one is\nthe multi-view reconstruction consistency loss, which aims to maintain\nconsistency between reconstruction from spatial and spatial-temporal contexts.\nAdditionally, we introduce a novel flipping augmentation to improve the\nperformance further. Our techniques enable a simple neural model to achieve\nstate-of-the-art performance on the DDAD and nuScenes datasets. Last but not\nleast, our proposed techniques can be easily applied to other methods. The code\nwill be made public.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}