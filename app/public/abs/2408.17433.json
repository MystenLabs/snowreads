{"id":"2408.17433","title":"DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised\n  Vector-LoRA of the Foundation Model","authors":"Mona Sheikh Zeinoddin, Chiara Lena, Jiongqi Qu, Luca Carlini, Mattia\n  Magro, Seunghoi Kim, Elena De Momi, Sophia Bano, Matthew Grech-Sollars,\n  Evangelos Mazomenos, Daniel C. Alexander, Danail Stoyanov, Matthew J.\n  Clarkson, Mobarakol Islam","authorsParsed":[["Zeinoddin","Mona Sheikh",""],["Lena","Chiara",""],["Qu","Jiongqi",""],["Carlini","Luca",""],["Magro","Mattia",""],["Kim","Seunghoi",""],["De Momi","Elena",""],["Bano","Sophia",""],["Grech-Sollars","Matthew",""],["Mazomenos","Evangelos",""],["Alexander","Daniel C.",""],["Stoyanov","Danail",""],["Clarkson","Matthew J.",""],["Islam","Mobarakol",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 17:35:06 GMT"}],"updateDate":"2024-09-02","timestamp":1725039306000,"abstract":"  Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D\nreconstruction and visualization. While foundation models like Depth Anything\nModels (DAM) show promise, directly applying them to surgery often yields\nsuboptimal results. Fully fine-tuning on limited surgical data can cause\noverfitting and catastrophic forgetting, compromising model robustness and\ngeneralization. Although Low-Rank Adaptation (LoRA) addresses some adaptation\nissues, its uniform parameter distribution neglects the inherent feature\nhierarchy, where earlier layers, learning more general features, require more\nparameters than later ones. To tackle this issue, we introduce Depth Anything\nin Robotic Endoscopic Surgery (DARES), a novel approach that employs a new\nadaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to\nperform self-supervised monocular depth estimation in RAS scenes. To enhance\nlearning efficiency, we introduce Vector-LoRA by integrating more parameters in\nearlier layers and gradually decreasing parameters in later layers. We also\ndesign a reprojection loss based on the multi-scale SSIM error to enhance depth\nperception by better tailoring the foundation model to the specific\nrequirements of the surgical environment. The proposed method is validated on\nthe SCARED dataset and demonstrates superior performance over recent\nstate-of-the-art self-supervised monocular depth estimation techniques,\nachieving an improvement of 13.3% in the absolute relative error metric. The\ncode and pre-trained weights are available at\nhttps://github.com/mobarakol/DARES.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}