{"id":"2407.03391","title":"Soft Begging: Modular and Efficient Shielding of LLMs against Prompt\n  Injection and Jailbreaking based on Prompt Tuning","authors":"Simon Ostermann, Kevin Baum, Christoph Endres, Julia Masloh, Patrick\n  Schramowski","authorsParsed":[["Ostermann","Simon",""],["Baum","Kevin",""],["Endres","Christoph",""],["Masloh","Julia",""],["Schramowski","Patrick",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:52:09 GMT"}],"updateDate":"2024-07-08","timestamp":1720018329000,"abstract":"  Prompt injection (both direct and indirect) and jailbreaking are now\nrecognized as significant issues for large language models (LLMs), particularly\ndue to their potential for harm in application-integrated contexts. This\nextended abstract explores a novel approach to protecting LLMs from such\nattacks, termed \"soft begging.\" This method involves training soft prompts to\ncounteract the effects of corrupted prompts on the LLM's output. We provide an\noverview of prompt injections and jailbreaking, introduce the theoretical basis\nof the \"soft begging\" technique, and discuss an evaluation of its\neffectiveness.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}