{"id":"2407.18559","title":"VSSD: Vision Mamba with Non-Causal State Space Duality","authors":"Yuheng Shi, Minjing Dong, Mingjia Li, Chang Xu","authorsParsed":[["Shi","Yuheng",""],["Dong","Minjing",""],["Li","Mingjia",""],["Xu","Chang",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 07:16:52 GMT"},{"version":"v2","created":"Sun, 4 Aug 2024 04:08:59 GMT"}],"updateDate":"2024-08-06","timestamp":1721978212000,"abstract":"  Vision transformers have significantly advanced the field of computer vision,\noffering robust modeling capabilities and global receptive field. However,\ntheir high computational demands limit their applicability in processing long\nsequences. To tackle this issue, State Space Models (SSMs) have gained\nprominence in vision tasks as they offer linear computational complexity.\nRecently, State Space Duality (SSD), an improved variant of SSMs, was\nintroduced in Mamba2 to enhance model performance and efficiency. However, the\ninherent causal nature of SSD/SSMs restricts their applications in non-causal\nvision tasks. To address this limitation, we introduce Visual State Space\nDuality (VSSD) model, which has a non-causal format of SSD. Specifically, we\npropose to discard the magnitude of interactions between the hidden state and\ntokens while preserving their relative weights, which relieves the dependencies\nof token contribution on previous tokens. Together with the involvement of\nmulti-scan strategies, we show that the scanning results can be integrated to\nachieve non-causality, which not only improves the performance of SSD in vision\ntasks but also enhances its efficiency. We conduct extensive experiments on\nvarious benchmarks including image classification, detection, and segmentation,\nwhere VSSD surpasses existing state-of-the-art SSM-based models. Code and\nweights are available at \\url{https://github.com/YuHengsss/VSSD}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}