{"id":"2408.15237","title":"The Mamba in the Llama: Distilling and Accelerating Hybrid Models","authors":"Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, and Tri\n  Dao","authorsParsed":[["Wang","Junxiong",""],["Paliotta","Daniele",""],["May","Avner",""],["Rush","Alexander M.",""],["Dao","Tri",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 17:56:11 GMT"}],"updateDate":"2024-08-28","timestamp":1724781371000,"abstract":"  Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}