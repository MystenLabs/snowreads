{"id":"2408.09657","title":"Impact of Large Language Models of Code on Fault Localization","authors":"Suhwan Ji, Sanghwa Lee, Changsup Lee, Hyeonseung Im, Yo-Sub Han","authorsParsed":[["Ji","Suhwan",""],["Lee","Sanghwa",""],["Lee","Changsup",""],["Im","Hyeonseung",""],["Han","Yo-Sub",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 02:36:07 GMT"}],"updateDate":"2024-08-20","timestamp":1724034967000,"abstract":"  Identifying the point of error is imperative in software debugging.\nTraditional fault localization (FL) techniques rely on executing the program\nand using the code coverage matrix in tandem with test case results to\ncalculate a suspiciousness score for each function or line. Recently,\nlearning-based FL techniques have harnessed machine learning models to extract\nmeaningful features from the code coverage matrix and improve FL performance.\nThese techniques, however, require compilable source code, existing test cases,\nand specialized tools for generating the code coverage matrix for each\nprogramming language of interest.\n  In this paper, we propose, for the first time, a simple but effective\nsequence generation approach for fine-tuning large language models of code\n(LLMCs) for FL tasks. LLMCs have recently received much attention for various\nsoftware engineering problems. In line with these, we leverage the innate\nunderstanding of code that LLMCs have acquired through pre-training on large\ncode corpora. Specifically, we fine-tune representative encoder,\nencoder-decoder, and decoder-based 13 LLMCs for FL tasks. Unlike previous\napproaches, LLMCs can analyze code sequences even with syntactic errors, since\nthey do not rely on compiled input. Still, they have a limitation on the length\nof the input data. Therefore, for a fair comparison with existing FL\ntechniques, we extract methods with errors from the project-level benchmark,\nDefects4J, and analyze them at the line level. Experimental results show that\nLLMCs fine-tuned with our approach successfully pinpoint error positions in\n50.6\\%, 64.2\\%, and 72.3\\% of 1,291 methods in Defects4J for Top-1/3/5\nprediction, outperforming the best learning-based state-of-the-art technique by\nup to 1.35, 1.12, and 1.08 times, respectively. Our findings suggest promising\nresearch directions for FL and automated program repair tasks using LLMCs.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/"}