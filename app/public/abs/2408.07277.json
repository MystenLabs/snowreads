{"id":"2408.07277","title":"Speech vs. Transcript: Does It Matter for Human Annotators in Speech\n  Summarization?","authors":"Roshan Sharma, Suwon Shon, Mark Lindsey, Hira Dhamyal, Rita Singh and\n  Bhiksha Raj","authorsParsed":[["Sharma","Roshan",""],["Shon","Suwon",""],["Lindsey","Mark",""],["Dhamyal","Hira",""],["Singh","Rita",""],["Raj","Bhiksha",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 13:25:53 GMT"}],"updateDate":"2024-08-15","timestamp":1723469153000,"abstract":"  Reference summaries for abstractive speech summarization require human\nannotation, which can be performed by listening to an audio recording or by\nreading textual transcripts of the recording. In this paper, we examine whether\nsummaries based on annotators listening to the recordings differ from those\nbased on annotators reading transcripts. Using existing intrinsic evaluation\nbased on human evaluation, automatic metrics, LLM-based evaluation, and a\nretrieval-based reference-free method. We find that summaries are indeed\ndifferent based on the source modality, and that speech-based summaries are\nmore factually consistent and information-selective than transcript-based\nsummaries. Meanwhile, transcript-based summaries are impacted by recognition\nerrors in the source, and expert-written summaries are more informative and\nreliable. We make all the collected data and analysis code\npublic(https://github.com/cmu-mlsp/interview_humanssum) to facilitate the\nreproduction of our work and advance research in this area.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}