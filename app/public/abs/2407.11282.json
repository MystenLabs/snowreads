{"id":"2407.11282","title":"Uncertainty is Fragile: Manipulating Uncertainty in Large Language\n  Models","authors":"Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua,\n  Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix\n  Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang","authorsParsed":[["Zeng","Qingcheng",""],["Jin","Mingyu",""],["Yu","Qinkai",""],["Wang","Zhenting",""],["Hua","Wenyue",""],["Zhou","Zihao",""],["Sun","Guangyan",""],["Meng","Yanda",""],["Ma","Shiqing",""],["Wang","Qifan",""],["Juefei-Xu","Felix",""],["Ding","Kaize",""],["Yang","Fan",""],["Tang","Ruixiang",""],["Zhang","Yongfeng",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 23:41:11 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 02:34:45 GMT"},{"version":"v3","created":"Fri, 19 Jul 2024 14:16:35 GMT"}],"updateDate":"2024-07-22","timestamp":1721086871000,"abstract":"  Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}