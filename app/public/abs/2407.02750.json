{"id":"2407.02750","title":"Learning to Reduce: Towards Improving Performance of Large Language\n  Models on Structured Data","authors":"Younghun Lee, Sungchul Kim, Ryan A. Rossi, Tong Yu, Xiang Chen","authorsParsed":[["Lee","Younghun",""],["Kim","Sungchul",""],["Rossi","Ryan A.",""],["Yu","Tong",""],["Chen","Xiang",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 01:51:50 GMT"}],"updateDate":"2024-07-04","timestamp":1719971510000,"abstract":"  Large Language Models (LLMs) have been achieving competent performance on a\nwide range of downstream tasks, yet existing work shows that inference on\nstructured data is challenging for LLMs. This is because LLMs need to either\nunderstand long structured data or select the most relevant evidence before\ninference, and both approaches are not trivial. This paper proposes a\nframework, Learning to Reduce, that fine-tunes a language model with On-Policy\nLearning to generate a reduced version of an input structured data. When\ncompared to state-of-the-art LLMs like GPT-4, Learning to Reduce not only\nachieves outstanding performance in reducing the input, but shows\ngeneralizability on different datasets. We further show that the model\nfine-tuned with our framework helps LLMs better perform on table QA tasks\nespecially when the context is longer.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}