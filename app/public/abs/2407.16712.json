{"id":"2407.16712","title":"Rapid Switching and Multi-Adapter Fusion via Sparse High Rank Adapters","authors":"Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Viswanath\n  Ganapathy, Rafael Esteves, Shreya Kadambi, Shubhankar Borse, Paul Whatmough,\n  Risheek Garrepalli, Mart Van Baalen, Harris Teague, Markus Nagel","authorsParsed":[["Bhardwaj","Kartikeya",""],["Pandey","Nilesh Prasad",""],["Priyadarshi","Sweta",""],["Ganapathy","Viswanath",""],["Esteves","Rafael",""],["Kadambi","Shreya",""],["Borse","Shubhankar",""],["Whatmough","Paul",""],["Garrepalli","Risheek",""],["Van Baalen","Mart",""],["Teague","Harris",""],["Nagel","Markus",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 22:46:36 GMT"}],"updateDate":"2024-07-25","timestamp":1721688396000,"abstract":"  In this paper, we propose Sparse High Rank Adapters (SHiRA) that directly\nfinetune 1-2% of the base model weights while leaving others unchanged, thus,\nresulting in a highly sparse adapter. This high sparsity incurs no inference\noverhead, enables rapid switching directly in the fused mode, and significantly\nreduces concept-loss during multi-adapter fusion. Our extensive experiments on\nLVMs and LLMs demonstrate that finetuning merely 1-2% parameters in the base\nmodel is sufficient for many adapter tasks and significantly outperforms Low\nRank Adaptation (LoRA). We also show that SHiRA is orthogonal to advanced LoRA\nmethods such as DoRA and can be easily combined with existing techniques.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"s-JhA41FBcYeIizt5eisH56AR0zjC_Z2pAdTDhbXn_o","pdfSize":"14778021","objectId":"0xd7bb095b862fc005dec06a9c62b3fd20cf3ea67336fab42597ece39c755bf34a","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
