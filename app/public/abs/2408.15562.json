{"id":"2408.15562","title":"Boosting Lossless Speculative Decoding via Feature Sampling and Partial\n  Alignment Distillation","authors":"Lujun Gui, Bin Xiao, Lei Su, Weipeng Chen","authorsParsed":[["Gui","Lujun",""],["Xiao","Bin",""],["Su","Lei",""],["Chen","Weipeng",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 06:28:01 GMT"}],"updateDate":"2024-08-29","timestamp":1724826481000,"abstract":"  Lossless speculative decoding accelerates target large language model (LLM)\ninference by employing a lightweight draft model for generating tree-structured\ncandidates, which are subsequently verified in parallel by the target LLM.\nCurrently, effective approaches leverage feature-level rather than token-level\nautoregression within the draft model to facilitate more straightforward\npredictions and enhanced knowledge distillation. In this paper, we reassess\nthese approaches and propose FSPAD (Feature Sampling and Partial Alignment\nDistillation for Lossless Speculative Decoding), which introduces two\nstraightforward and effective components within the existing framework to boost\nlossless speculative decoding. Firstly, FSPAD utilizes token embeddings to\nsample features of the target LLM in high-dimensional space before feeding them\ninto the draft model, due to the inherent uncertainty of the features\npreventing the draft model from obtaining the specific token output by the\ntarget LLM. Secondly, FSPAD introduces partial alignment distillation to weaken\nthe draft model's connection between features and logits, aiming to reduce the\nconflict between feature alignment and logit confidence during training. Our\nexperiments include both greedy and non-greedy decoding on the largest and\nsmallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in\nmulti-turn conversation, translation, summarization, question answering,\nmathematical reasoning, and retrieval-augmented generation. The results show\nthat FSPAD outperforms the state-of-the-art method across all the\naforementioned tasks and target LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2G7UAC2TzBj1ybpkyfkyGj29M1SbDQKC2qtU1cR7wDw","pdfSize":"1041201"}
