{"id":"2408.12570","title":"Jamba-1.5: Hybrid Transformer-Mamba Models at Scale","authors":"Jamba Team: Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich,\n  Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel\n  Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M Gerber, Elad Dolev,\n  Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim\n  Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan\n  Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman,\n  Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer\n  Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i\n  Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal\n  Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz,\n  Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, Yoav\n  Shoham","authorsParsed":[["Jamba Team","",""],["Lenz","Barak",""],["Arazi","Alan",""],["Bergman","Amir",""],["Manevich","Avshalom",""],["Peleg","Barak",""],["Aviram","Ben",""],["Almagor","Chen",""],["Fridman","Clara",""],["Padnos","Dan",""],["Gissin","Daniel",""],["Jannai","Daniel",""],["Muhlgay","Dor",""],["Zimberg","Dor",""],["Gerber","Edden M",""],["Dolev","Elad",""],["Krakovsky","Eran",""],["Safahi","Erez",""],["Schwartz","Erez",""],["Cohen","Gal",""],["Shachaf","Gal",""],["Rozenblum","Haim",""],["Bata","Hofit",""],["Blass","Ido",""],["Magar","Inbal",""],["Dalmedigos","Itay",""],["Osin","Jhonathan",""],["Fadlon","Julie",""],["Rozman","Maria",""],["Danos","Matan",""],["Gokhman","Michael",""],["Zusman","Mor",""],["Gidron","Naama",""],["Ratner","Nir",""],["Gat","Noam",""],["Rozen","Noam",""],["Fried","Oded",""],["Leshno","Ohad",""],["Antverg","Omer",""],["Abend","Omri",""],["Lieber","Opher",""],["Dagan","Or",""],["Cohavi","Orit",""],["Alon","Raz",""],["Belson","Ro'i",""],["Cohen","Roi",""],["Gilad","Rom",""],["Glozman","Roman",""],["Lev","Shahar",""],["Meirom","Shaked",""],["Delbari","Tal",""],["Ness","Tal",""],["Asida","Tomer",""],["Gal","Tom Ben",""],["Braude","Tom",""],["Pumerantz","Uriya",""],["Cohen","Yehoshua",""],["Belinkov","Yonatan",""],["Globerson","Yuval",""],["Levy","Yuval Peleg",""],["Shoham","Yoav",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 17:38:59 GMT"}],"updateDate":"2024-08-23","timestamp":1724348339000,"abstract":"  We present Jamba-1.5, new instruction-tuned large language models based on\nour Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts\narchitecture, providing high throughput and low memory usage across context\nlengths, while retaining the same or better quality as Transformer models. We\nrelease two model sizes: Jamba-1.5-Large, with 94B active parameters, and\nJamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a\nvariety of conversational and instruction-following capabilties, and have an\neffective context length of 256K tokens, the largest amongst open-weight\nmodels. To support cost-effective inference, we introduce ExpertsInt8, a novel\nquantization technique that allows fitting Jamba-1.5-Large on a machine with 8\n80GB GPUs when processing 256K-token contexts without loss of quality. When\nevaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models\nachieve excellent results while providing high throughput and outperforming\nother open-weight models on long-context benchmarks. The model weights for both\nsizes are publicly available under the Jamba Open Model License and we release\nExpertsInt8 as open source.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}