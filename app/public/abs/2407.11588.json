{"id":"2407.11588","title":"Progressive Pretext Task Learning for Human Trajectory Prediction","authors":"Xiaotong Lin, Tianming Liang, Jianhuang Lai, and Jian-Fang Hu","authorsParsed":[["Lin","Xiaotong",""],["Liang","Tianming",""],["Lai","Jianhuang",""],["Hu","Jian-Fang",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 10:48:18 GMT"}],"updateDate":"2024-07-17","timestamp":1721126898000,"abstract":"  Human trajectory prediction is a practical task of predicting the future\npositions of pedestrians on the road, which typically covers all temporal\nranges from short-term to long-term within a trajectory. However, existing\nworks attempt to address the entire trajectory prediction with a singular,\nuniform training paradigm, neglecting the distinction between short-term and\nlong-term dynamics in human trajectories. To overcome this limitation, we\nintroduce a novel Progressive Pretext Task learning (PPT) framework, which\nprogressively enhances the model's capacity of capturing short-term dynamics\nand long-term dependencies for the final entire trajectory prediction.\nSpecifically, we elaborately design three stages of training tasks in the PPT\nframework. In the first stage, the model learns to comprehend the short-term\ndynamics through a stepwise next-position prediction task. In the second stage,\nthe model is further enhanced to understand long-term dependencies through a\ndestination prediction task. In the final stage, the model aims to address the\nentire future trajectory task by taking full advantage of the knowledge from\nprevious stages. To alleviate the knowledge forgetting, we further apply a\ncross-task knowledge distillation. Additionally, we design a Transformer-based\ntrajectory predictor, which is able to achieve highly efficient two-step\nreasoning by integrating a destination-driven prediction strategy and a group\nof learnable prompt embeddings. Extensive experiments on popular benchmarks\nhave demonstrated that our proposed approach achieves state-of-the-art\nperformance with high efficiency. Code is available at\nhttps://github.com/iSEE-Laboratory/PPT.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}