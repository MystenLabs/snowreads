{"id":"2407.10597","title":"Multilevel Regularized Newton Methods with Fast Convergence Rates","authors":"Nick Tsipinakis, Panos Parpas","authorsParsed":[["Tsipinakis","Nick",""],["Parpas","Panos",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 10:20:00 GMT"}],"updateDate":"2024-07-16","timestamp":1721038800000,"abstract":"  We introduce new multilevel methods for solving large-scale unconstrained\noptimization problems. Specifically, the philosophy of multilevel methods is\napplied to Newton-type methods that regularize the Newton sub-problem using\nsecond order information from a coarse (low dimensional) sub-problem. The new\n\\emph{regularized multilevel methods} provably converge from any initialization\npoint and enjoy faster convergence rates than Gradient Descent. In particular,\nfor arbitrary functions with Lipschitz continuous Hessians, we show that their\nconvergence rate interpolates between the rate of Gradient Descent and that of\nthe cubic Newton method. If, additionally, the objective function is assumed to\nbe convex, then the proposed method converges with the fast\n$\\mathcal{O}(k^{-2})$ rate. Hence, since the updates are generated using a\n\\emph{coarse} model in low dimensions, the theoretical results of this paper\nsignificantly speed-up the convergence of Newton-type or preconditioned\ngradient methods in practical applications. Preliminary numerical results\nsuggest that the proposed multilevel algorithms are significantly faster than\ncurrent state-of-the-art methods.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/"}