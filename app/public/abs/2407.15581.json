{"id":"2407.15581","title":"vLSM: Low tail latency and I/O amplification in LSM-based KV stores","authors":"Giorgos Xanthakis, Antonios Katsarakis, Giorgos Saloustros, and\n  Angelos Bilas","authorsParsed":[["Xanthakis","Giorgos",""],["Katsarakis","Antonios",""],["Saloustros","Giorgos",""],["Bilas","Angelos",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 12:17:01 GMT"}],"updateDate":"2024-07-23","timestamp":1721650621000,"abstract":"  LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.\n","subjects":["Computing Research Repository/Databases"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}