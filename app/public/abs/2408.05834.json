{"id":"2408.05834","title":"Divide-and-Conquer Predictive Coding: a structured Bayesian inference\n  algorithm","authors":"Eli Sennesh, Hao Wu, Tommaso Salvatori","authorsParsed":[["Sennesh","Eli",""],["Wu","Hao",""],["Salvatori","Tommaso",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 17:29:03 GMT"}],"updateDate":"2024-08-13","timestamp":1723397343000,"abstract":"  Unexpected stimuli induce \"error\" or \"surprise\" signals in the brain. The\ntheory of predictive coding promises to explain these observations in terms of\nBayesian inference by suggesting that the cortex implements variational\ninference in a probabilistic graphical model. However, when applied to machine\nlearning tasks, this family of algorithms has yet to perform on par with other\nvariational approaches in high-dimensional, structured inference problems. To\naddress this, we introduce a novel predictive coding algorithm for structured\ngenerative models, that we call divide-and-conquer predictive coding (DCPC).\nDCPC differs from other formulations of predictive coding, as it respects the\ncorrelation structure of the generative model and provably performs\nmaximum-likelihood updates of model parameters, all without sacrificing\nbiological plausibility. Empirically, DCPC achieves better numerical\nperformance than competing algorithms and provides accurate inference in a\nnumber of problems not previously addressed with predictive coding. We provide\nan open implementation of DCPC in Pyro on Github.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Quantitative Biology/Neurons and Cognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}