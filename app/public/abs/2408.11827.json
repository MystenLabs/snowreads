{"id":"2408.11827","title":"The Mechanics of Conceptual Interpretation in GPT Models: Interpretative\n  Insights","authors":"Nura Aljaafari, Danilo S. Carvalho, Andr\\'e Freitas","authorsParsed":[["Aljaafari","Nura",""],["Carvalho","Danilo S.",""],["Freitas","Andr√©",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 18:50:08 GMT"}],"updateDate":"2024-08-23","timestamp":1722883808000,"abstract":"  Locating and editing knowledge in large language models (LLMs) is crucial for\nenhancing their accuracy, safety, and inference rationale. We introduce\n``concept editing'', an innovative variation of knowledge editing that uncovers\nconceptualisation mechanisms within these models. Using the reverse dictionary\ntask, inference tracing, and input abstraction, we analyse the Multi-Layer\nPerceptron (MLP), Multi-Head Attention (MHA), and hidden state components of\ntransformer models. Our results reveal distinct patterns: MLP layers employ\nkey-value retrieval mechanism and context-dependent processing, which are\nhighly associated with relative input tokens. MHA layers demonstrate a\ndistributed nature with significant higher-level activations, suggesting\nsophisticated semantic integration. Hidden states emphasise the importance of\nthe last token and top layers in the inference process. We observe evidence of\ngradual information building and distributed representation. These observations\nelucidate how transformer models process semantic information, paving the way\nfor targeted interventions and improved interpretability techniques. Our work\nhighlights the complex, layered nature of semantic processing in LLMs and the\nchallenges of isolating and modifying specific concepts within these models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"LTcJJcuBuxTUumXztdic3qJRjyIdgDRE3jPoS3nclDQ","pdfSize":"9194633"}
