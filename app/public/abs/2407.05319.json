{"id":"2407.05319","title":"Rethinking Targeted Adversarial Attacks For Neural Machine Translation","authors":"Junjie Wu, Lemao Liu, Wei Bi, Dit-Yan Yeung","authorsParsed":[["Wu","Junjie",""],["Liu","Lemao",""],["Bi","Wei",""],["Yeung","Dit-Yan",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 10:16:06 GMT"}],"updateDate":"2024-07-09","timestamp":1720347366000,"abstract":"  Targeted adversarial attacks are widely used to evaluate the robustness of\nneural machine translation systems. Unfortunately, this paper first identifies\na critical issue in the existing settings of NMT targeted adversarial attacks,\nwhere their attacking results are largely overestimated. To this end, this\npaper presents a new setting for NMT targeted adversarial attacks that could\nlead to reliable attacking results. Under the new setting, it then proposes a\nTargeted Word Gradient adversarial Attack (TWGA) method to craft adversarial\nexamples. Experimental results demonstrate that our proposed setting could\nprovide faithful attacking results for targeted adversarial attacks on NMT\nsystems, and the proposed TWGA method can effectively attack such victim NMT\nsystems. In-depth analyses on a large-scale dataset further illustrate some\nvaluable findings. 1 Our code and data are available at\nhttps://github.com/wujunjie1998/TWGA.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}