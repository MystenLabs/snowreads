{"id":"2407.10328","title":"The Interpretation Gap in Text-to-Music Generation Models","authors":"Yongyi Zang and Yixiao Zhang","authorsParsed":[["Zang","Yongyi",""],["Zhang","Yixiao",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 20:51:08 GMT"}],"updateDate":"2024-07-16","timestamp":1720990268000,"abstract":"  Large-scale text-to-music generation models have significantly enhanced music\ncreation capabilities, offering unprecedented creative freedom. However, their\nability to collaborate effectively with human musicians remains limited. In\nthis paper, we propose a framework to describe the musical interaction process,\nwhich includes expression, interpretation, and execution of controls. Following\nthis framework, we argue that the primary gap between existing text-to-music\nmodels and musicians lies in the interpretation stage, where models lack the\nability to interpret controls from musicians. We also propose two strategies to\naddress this gap and call on the music information retrieval community to\ntackle the interpretation challenge to improve human-AI musical collaboration.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}