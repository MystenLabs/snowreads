{"id":"2408.16100","title":"LLMSecCode: Evaluating Large Language Models for Secure Coding","authors":"Anton Ryd\\'en, Erik N\\\"aslund, Elad Michael Schiller, Magnus Almgren","authorsParsed":[["Rydén","Anton",""],["Näslund","Erik",""],["Schiller","Elad Michael",""],["Almgren","Magnus",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 19:07:08 GMT"}],"updateDate":"2024-08-30","timestamp":1724872028000,"abstract":"  The rapid deployment of Large Language Models (LLMs) requires careful\nconsideration of their effect on cybersecurity. Our work aims to improve the\nselection process of LLMs that are suitable for facilitating Secure Coding\n(SC). This raises challenging research questions, such as (RQ1) Which\nfunctionality can streamline the LLM evaluation? (RQ2) What should the\nevaluation measure? (RQ3) How to attest that the evaluation process is\nimpartial? To address these questions, we introduce LLMSecCode, an open-source\nevaluation framework designed to assess LLM SC capabilities objectively.\n  We validate the LLMSecCode implementation through experiments. When varying\nparameters and prompts, we find a 10% and 9% difference in performance,\nrespectively. We also compare some results to reliable external actors, where\nour results show a 5% difference.\n  We strive to ensure the ease of use of our open-source framework and\nencourage further development by external actors. With LLMSecCode, we hope to\nencourage the standardization and benchmarking of LLMs' capabilities in\nsecurity-oriented code and tasks.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}