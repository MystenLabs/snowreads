{"id":"2408.15637","title":"Transfer Learning from Simulated to Real Scenes for Monocular 3D Object\n  Detection","authors":"Sondos Mohamed, Walter Zimmer, Ross Greer, Ahmed Alaaeldin Ghita,\n  Modesto Castrill\\'on-Santana, Mohan Trivedi, Alois Knoll, Salvatore Mario\n  Carta, Mirko Marras","authorsParsed":[["Mohamed","Sondos",""],["Zimmer","Walter",""],["Greer","Ross",""],["Ghita","Ahmed Alaaeldin",""],["Castrill√≥n-Santana","Modesto",""],["Trivedi","Mohan",""],["Knoll","Alois",""],["Carta","Salvatore Mario",""],["Marras","Mirko",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 08:44:58 GMT"}],"updateDate":"2024-08-29","timestamp":1724834698000,"abstract":"  Accurately detecting 3D objects from monocular images in dynamic roadside\nscenarios remains a challenging problem due to varying camera perspectives and\nunpredictable scene conditions. This paper introduces a two-stage training\nstrategy to address these challenges. Our approach initially trains a model on\nthe large-scale synthetic dataset, RoadSense3D, which offers a diverse range of\nscenarios for robust feature learning. Subsequently, we fine-tune the model on\na combination of real-world datasets to enhance its adaptability to practical\nconditions. Experimental results of the Cube R-CNN model on challenging public\nbenchmarks show a remarkable improvement in detection performance, with a mean\naverage precision rising from 0.26 to 12.76 on the TUM Traffic A9 Highway\ndataset and from 2.09 to 6.60 on the DAIR-V2X-I dataset when performing\ntransfer learning. Code, data, and qualitative video results are available on\nthe project website: https://roadsense3d.github.io.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"47zfdKekFTOu6oahnKTgteDM8HMBP23gcmFQCf8vOvY","pdfSize":"4803093"}
