{"id":"2407.03748","title":"Argument Mining in Data Scarce Settings: Cross-lingual Transfer and\n  Few-shot Techniques","authors":"Anar Yeginbergen and Maite Oronoz and Rodrigo Agerri","authorsParsed":[["Yeginbergen","Anar",""],["Oronoz","Maite",""],["Agerri","Rodrigo",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 08:59:17 GMT"}],"updateDate":"2024-07-30","timestamp":1720083557000,"abstract":"  Recent research on sequence labelling has been exploring different strategies\nto mitigate the lack of manually annotated data for the large majority of the\nworld languages. Among others, the most successful approaches have been based\non (i) the cross-lingual transfer capabilities of multilingual pre-trained\nlanguage models (model-transfer), (ii) data translation and label projection\n(data-transfer) and (iii), prompt-based learning by reusing the mask objective\nto exploit the few-shot capabilities of pre-trained language models (few-shot).\nPrevious work seems to conclude that model-transfer outperforms data-transfer\nmethods and that few-shot techniques based on prompting are superior to\nupdating the model's weights via fine-tuning. In this paper, we empirically\ndemonstrate that, for Argument Mining, a sequence labelling task which requires\nthe detection of long and complex discourse structures, previous insights on\ncross-lingual transfer or few-shot learning do not apply. Contrary to previous\nwork, we show that for Argument Mining data transfer obtains better results\nthan model-transfer and that fine-tuning outperforms few-shot methods.\nRegarding the former, the domain of the dataset used for data-transfer seems to\nbe a deciding factor, while, for few-shot, the type of task (length and\ncomplexity of the sequence spans) and sampling method prove to be crucial.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}