{"id":"2408.10277","title":"Increasing transformer token length with a Maximum Entropy Principle\n  Method","authors":"R. I. Cukier","authorsParsed":[["Cukier","R. I.",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 15:47:39 GMT"}],"updateDate":"2024-08-21","timestamp":1723909659000,"abstract":"  Transformers suffer from the computational overhead of their quadratic\ndependence on the length of sequences processed. We present three methods, all\nadding an intermediate step between training and inference/generation, which\nextend the autoregressive length of transformers. All rely on a Maximum Entropy\nPrinciple (MEP) whereby entropy is maximized in the presence of suitable\nconstraints, accounted for by use of Lagrange Multipliers. These constraint\nmethods extend the autoregressive character from T to 2T tokens in a\nlinear-with-T fashion. There is overhead associated with this added step, but\nthey should still be faster than the standard methods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6vYVe8t0Fqo2ZluLXxsUyLakJ5yAtwgcRD8049cJ9JM","pdfSize":"1730326"}
