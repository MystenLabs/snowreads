{"id":"2407.13184","title":"HSEmotion Team at the 7th ABAW Challenge: Multi-Task Learning and\n  Compound Facial Expression Recognition","authors":"Andrey V. Savchenko","authorsParsed":[["Savchenko","Andrey V.",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 05:47:49 GMT"}],"updateDate":"2024-07-19","timestamp":1721281669000,"abstract":"  In this paper, we describe the results of the HSEmotion team in two tasks of\nthe seventh Affective Behavior Analysis in-the-wild (ABAW) competition, namely,\nmulti-task learning for simultaneous prediction of facial expression, valence,\narousal, and detection of action units, and compound expression recognition. We\npropose an efficient pipeline based on frame-level facial feature extractors\npre-trained in multi-task settings to estimate valence-arousal and basic facial\nexpressions given a facial photo. We ensure the privacy-awareness of our\ntechniques by using the lightweight architectures of neural networks, such as\nMT-EmotiDDAMFN, MT-EmotiEffNet, and MT-EmotiMobileFaceNet, that can run even on\na mobile device without the need to send facial video to a remote server. It\nwas demonstrated that a significant step in improving the overall accuracy is\nthe smoothing of neural network output scores using Gaussian or box filters. It\nwas experimentally demonstrated that such a simple post-processing of\npredictions from simple blending of two top visual models improves the F1-score\nof facial expression recognition up to 7%. At the same time, the mean\nConcordance Correlation Coefficient (CCC) of valence and arousal is increased\nby up to 1.25 times compared to each model's frame-level predictions. As a\nresult, our final performance score on the validation set from the multi-task\nlearning challenge is 4.5 times higher than the baseline (1.494 vs 0.32).\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}