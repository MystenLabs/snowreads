{"id":"2407.07794","title":"Reinforcement Learning of Adaptive Acquisition Policies for Inverse\n  Problems","authors":"Gianluigi Silvestri, Fabio Valerio Massoli, Tribhuvanesh Orekondy,\n  Afshin Abdi, Arash Behboodi","authorsParsed":[["Silvestri","Gianluigi",""],["Massoli","Fabio Valerio",""],["Orekondy","Tribhuvanesh",""],["Abdi","Afshin",""],["Behboodi","Arash",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:12:09 GMT"}],"updateDate":"2024-07-11","timestamp":1720627929000,"abstract":"  A promising way to mitigate the expensive process of obtaining a\nhigh-dimensional signal is to acquire a limited number of low-dimensional\nmeasurements and solve an under-determined inverse problem by utilizing the\nstructural prior about the signal. In this paper, we focus on adaptive\nacquisition schemes to save further the number of measurements. To this end, we\npropose a reinforcement learning-based approach that sequentially collects\nmeasurements to better recover the underlying signal by acquiring fewer\nmeasurements. Our approach applies to general inverse problems with continuous\naction spaces and jointly learns the recovery algorithm. Using insights\nobtained from theoretical analysis, we also provide a probabilistic design for\nour methods using variational formulation. We evaluate our approach on multiple\ndatasets and with two measurement spaces (Gaussian, Radon). Our results confirm\nthe benefits of adaptive strategies in low-acquisition horizon settings.\n","subjects":["Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bzUFz2cDTXp4CJw0NiHzA4w31I77LgNWuspqvkmL4IQ","pdfSize":"1187696"}
