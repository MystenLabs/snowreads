{"id":"2407.16382","title":"TookaBERT: A Step Forward for Persian NLU","authors":"MohammadAli SadraeiJavaheri and Ali Moghaddaszadeh and Milad Molazadeh\n  and Fariba Naeiji and Farnaz Aghababaloo and Hamideh Rafiee and Zahra\n  Amirmahani and Tohid Abedini and Fatemeh Zahra Sheikhi and Amirmohammad\n  Salehoof","authorsParsed":[["SadraeiJavaheri","MohammadAli",""],["Moghaddaszadeh","Ali",""],["Molazadeh","Milad",""],["Naeiji","Fariba",""],["Aghababaloo","Farnaz",""],["Rafiee","Hamideh",""],["Amirmahani","Zahra",""],["Abedini","Tohid",""],["Sheikhi","Fatemeh Zahra",""],["Salehoof","Amirmohammad",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 11:12:47 GMT"}],"updateDate":"2024-07-24","timestamp":1721733167000,"abstract":"  The field of natural language processing (NLP) has seen remarkable\nadvancements, thanks to the power of deep learning and foundation models.\nLanguage models, and specifically BERT, have been key players in this progress.\nIn this study, we trained and introduced two new BERT models using Persian\ndata. We put our models to the test, comparing them to seven existing models\nacross 14 diverse Persian natural language understanding (NLU) tasks. The\nresults speak for themselves: our larger model outperforms the competition,\nshowing an average improvement of at least +2.8 points. This highlights the\neffectiveness and potential of our new BERT models for Persian NLU tasks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}