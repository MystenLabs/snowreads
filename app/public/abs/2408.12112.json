{"id":"2408.12112","title":"Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards","authors":"Shresth Verma, Niclas Boehmer, Lingkai Kong, Milind Tambe","authorsParsed":[["Verma","Shresth",""],["Boehmer","Niclas",""],["Kong","Lingkai",""],["Tambe","Milind",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 03:54:08 GMT"},{"version":"v2","created":"Sun, 15 Sep 2024 07:16:38 GMT"}],"updateDate":"2024-09-17","timestamp":1724298848000,"abstract":"  LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/"}