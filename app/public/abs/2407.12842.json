{"id":"2407.12842","title":"MS2SL: Multimodal Spoken Data-Driven Continuous Sign Language Production","authors":"Jian Ma, Wenguan Wang, Yi Yang, Feng Zheng","authorsParsed":[["Ma","Jian",""],["Wang","Wenguan",""],["Yang","Yi",""],["Zheng","Feng",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 13:53:50 GMT"}],"updateDate":"2024-07-19","timestamp":1720101230000,"abstract":"  Sign language understanding has made significant strides; however, there is\nstill no viable solution for generating sign sequences directly from entire\nspoken content, e.g., text or speech. In this paper, we propose a unified\nframework for continuous sign language production, easing communication between\nsign and non-sign language users. In particular, a sequence diffusion model,\nutilizing embeddings extracted from text or speech, is crafted to generate sign\npredictions step by step. Moreover, by creating a joint embedding space for\ntext, audio, and sign, we bind these modalities and leverage the semantic\nconsistency among them to provide informative feedback for the model training.\nThis embedding-consistency learning strategy minimizes the reliance on sign\ntriplets and ensures continuous model refinement, even with a missing audio\nmodality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our\nmodel achieves competitive performance in sign language production.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}