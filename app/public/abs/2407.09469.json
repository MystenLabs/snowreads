{"id":"2407.09469","title":"Learning Coordinated Maneuver in Adversarial Environments","authors":"Zechen Hu, Manshi Limbu, Daigo Shishika, Xuesu Xiao, and Xuan Wang","authorsParsed":[["Hu","Zechen",""],["Limbu","Manshi",""],["Shishika","Daigo",""],["Xiao","Xuesu",""],["Wang","Xuan",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 17:49:42 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 05:30:44 GMT"}],"updateDate":"2024-08-22","timestamp":1720806582000,"abstract":"  This paper aims to solve the coordination of a team of robots traversing a\nroute in the presence of adversaries with random positions. Our goal is to\nminimize the overall cost of the team, which is determined by (i) the\naccumulated risk when robots stay in adversary-impacted zones and (ii) the\nmission completion time. During traversal, robots can reduce their speed and\nact as a `guard' (the slower, the better), which will decrease the risks\ncertain adversary incurs. This leads to a trade-off between the robots'\nguarding behaviors and their travel speeds. The formulated problem is highly\nnon-convex and cannot be efficiently solved by existing algorithms. Our\napproach includes a theoretical analysis of the robots' behaviors for the\nsingle-adversary case. As the scale of the problem expands, solving the optimal\nsolution using optimization approaches is challenging, therefore, we employ\nreinforcement learning techniques by developing new encoding and\npolicy-generating methods. Simulations demonstrate that our learning methods\ncan efficiently produce team coordination behaviors. We discuss the reasoning\nbehind these behaviors and explain why they reduce the overall team cost.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Yjr_oqNgT8gv90XdwrSDyga6yz05UbrrrvBmrrB8ERg","pdfSize":"1390194"}
