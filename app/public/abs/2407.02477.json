{"id":"2407.02477","title":"Understanding Alignment in Multimodal LLMs: A Comprehensive Study","authors":"Elmira Amirloo, Jean-Philippe Fauconnier, Christoph Roesmann,\n  Christian Kerl, Rinu Boney, Yusu Qian, Zirui Wang, Afshin Dehghan, Yinfei\n  Yang, Zhe Gan, Peter Grasch","authorsParsed":[["Amirloo","Elmira",""],["Fauconnier","Jean-Philippe",""],["Roesmann","Christoph",""],["Kerl","Christian",""],["Boney","Rinu",""],["Qian","Yusu",""],["Wang","Zirui",""],["Dehghan","Afshin",""],["Yang","Yinfei",""],["Gan","Zhe",""],["Grasch","Peter",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:55:03 GMT"}],"updateDate":"2024-07-03","timestamp":1719942903000,"abstract":"  Preference alignment has become a crucial component in enhancing the\nperformance of Large Language Models (LLMs), yet its impact in Multimodal Large\nLanguage Models (MLLMs) remains comparatively underexplored. Similar to\nlanguage models, MLLMs for image understanding tasks encounter challenges like\nhallucination. In MLLMs, hallucination can occur not only by stating incorrect\nfacts but also by producing responses that are inconsistent with the image\ncontent. A primary objective of alignment for MLLMs is to encourage these\nmodels to align responses more closely with image information. Recently,\nmultiple works have introduced preference datasets for MLLMs and examined\ndifferent alignment methods, including Direct Preference Optimization (DPO) and\nProximal Policy Optimization (PPO). However, due to variations in datasets,\nbase model types, and alignment methods, it remains unclear which specific\nelements contribute most significantly to the reported improvements in these\nworks. In this paper, we independently analyze each aspect of preference\nalignment in MLLMs. We start by categorizing the alignment algorithms into two\ngroups, offline (such as DPO), and online (such as online-DPO), and show that\ncombining offline and online methods can improve the performance of the model\nin certain scenarios. We review a variety of published multimodal preference\ndatasets and discuss how the details of their construction impact model\nperformance. Based on these insights, we introduce a novel way of creating\nmultimodal preference data called Bias-Driven Hallucination Sampling (BDHS)\nthat needs neither additional annotation nor external models, and show that it\ncan achieve competitive performance to previously published alignment work for\nmultimodal models across a range of benchmarks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}