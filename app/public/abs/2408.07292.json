{"id":"2408.07292","title":"LiPCoT: Linear Predictive Coding based Tokenizer for Self-supervised\n  Learning of Time Series Data via Language Models","authors":"Md Fahim Anjum","authorsParsed":[["Anjum","Md Fahim",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 04:51:33 GMT"}],"updateDate":"2024-08-15","timestamp":1723611093000,"abstract":"  Language models have achieved remarkable success in various natural language\nprocessing tasks. However, their application to time series data, a crucial\ncomponent in many domains, remains limited. This paper proposes LiPCoT (Linear\nPredictive Coding based Tokenizer for time series), a novel tokenizer that\nencodes time series data into a sequence of tokens, enabling self-supervised\nlearning of time series using existing Language model architectures such as\nBERT. Unlike traditional time series tokenizers that rely heavily on CNN\nencoder for time series feature generation, LiPCoT employs stochastic modeling\nthrough linear predictive coding to create a latent space for time series\nproviding a compact yet rich representation of the inherent stochastic nature\nof the data. Furthermore, LiPCoT is computationally efficient and can\neffectively handle time series data with varying sampling rates and lengths,\novercoming common limitations of existing time series tokenizers. In this\nproof-of-concept work, we present the effectiveness of LiPCoT in classifying\nParkinson's disease (PD) using an EEG dataset from 46 participants. In\nparticular, we utilize LiPCoT to encode EEG data into a small vocabulary of\ntokens and then use BERT for self-supervised learning and the downstream task\nof PD classification. We benchmark our approach against several\nstate-of-the-art CNN-based deep learning architectures for PD detection. Our\nresults reveal that BERT models utilizing self-supervised learning outperformed\nthe best-performing existing method by 7.1% in precision, 2.3% in recall, 5.5%\nin accuracy, 4% in AUC, and 5% in F1-score highlighting the potential for\nself-supervised learning even on small datasets. Our work will inform future\nfoundational models for time series, particularly for self-supervised learning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}