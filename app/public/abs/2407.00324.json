{"id":"2407.00324","title":"Revisiting Sparse Rewards for Goal-Reaching Reinforcement Learning","authors":"Gautham Vasan, Yan Wang, Fahim Shahriar, James Bergstra, Martin\n  Jagersand, A. Rupam Mahmood","authorsParsed":[["Vasan","Gautham",""],["Wang","Yan",""],["Shahriar","Fahim",""],["Bergstra","James",""],["Jagersand","Martin",""],["Mahmood","A. Rupam",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 05:55:33 GMT"},{"version":"v2","created":"Mon, 8 Jul 2024 20:15:46 GMT"}],"updateDate":"2024-07-10","timestamp":1719640533000,"abstract":"  Many real-world robot learning problems, such as pick-and-place or arriving\nat a destination, can be seen as a problem of reaching a goal state as soon as\npossible. These problems, when formulated as episodic reinforcement learning\ntasks, can easily be specified to align well with our intended goal: -1 reward\nevery time step with termination upon reaching the goal state, called\nminimum-time tasks. Despite this simplicity, such formulations are often\noverlooked in favor of dense rewards due to their perceived difficulty and lack\nof informativeness. Our studies contrast the two reward paradigms, revealing\nthat the minimum-time task specification not only facilitates learning\nhigher-quality policies but can also surpass dense-reward-based policies on\ntheir own performance metrics. Crucially, we also identify the goal-hit rate of\nthe initial policy as a robust early indicator for learning success in such\nsparse feedback settings. Finally, using four distinct real-robotic platforms,\nwe show that it is possible to learn pixel-based policies from scratch within\ntwo to three hours using constant negative rewards.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TUvxUaovtgG61Ht7TEUWi6sbjfX1T6-dprqrTbTtt9w","pdfSize":"7989822","objectId":"0x2a78e1bab318ddc5d3248dbda15f2f60fca8d2278dabebfa90a52e3dad7964d7","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
