{"id":"2407.02170","title":"GMM-ResNet2: Ensemble of Group ResNet Networks for Synthetic Speech\n  Detection","authors":"Zhenchun Lei, Hui Yan, Changhong Liu, Yong Zhou, Minglei Ma","authorsParsed":[["Lei","Zhenchun",""],["Yan","Hui",""],["Liu","Changhong",""],["Zhou","Yong",""],["Ma","Minglei",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 11:25:42 GMT"}],"updateDate":"2024-07-03","timestamp":1719919542000,"abstract":"  Deep learning models are widely used for speaker recognition and spoofing\nspeech detection. We propose the GMM-ResNet2 for synthesis speech detection.\nCompared with the previous GMM-ResNet model, GMM-ResNet2 has four improvements.\nFirstly, the different order GMMs have different capabilities to form smooth\napproximations to the feature distribution, and multiple GMMs are used to\nextract multi-scale Log Gaussian Probability features. Secondly, the grouping\ntechnique is used to improve the classification accuracy by exposing the group\ncardinality while reducing both the number of parameters and the training time.\nThe final score is obtained by ensemble of all group classifier outputs using\nthe averaging method. Thirdly, the residual block is improved by including one\nactivation function and one batch normalization layer. Finally, an\nensemble-aware loss function is proposed to integrate the independent loss\nfunctions of all ensemble members. On the ASVspoof 2019 LA task, the\nGMM-ResNet2 achieves a minimum t-DCF of 0.0227 and an EER of 0.79\\%. On the\nASVspoof 2021 LA task, the GMM-ResNet2 achieves a minimum t-DCF of 0.2362 and\nan EER of 2.19\\%, and represents a relative reductions of 31.4\\% and 76.3\\%\ncompared with the LFCC-LCNN baseline.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}