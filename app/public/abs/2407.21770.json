{"id":"2407.21770","title":"MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts","authors":"Xi Victoria Lin and Akshat Shrivastava and Liang Luo and Srinivasan\n  Iyer and Mike Lewis and Gargi Ghosh and Luke Zettlemoyer and Armen Aghajanyan","authorsParsed":[["Lin","Xi Victoria",""],["Shrivastava","Akshat",""],["Luo","Liang",""],["Iyer","Srinivasan",""],["Lewis","Mike",""],["Ghosh","Gargi",""],["Zettlemoyer","Luke",""],["Aghajanyan","Armen",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 17:46:51 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 17:57:41 GMT"},{"version":"v3","created":"Mon, 12 Aug 2024 16:20:37 GMT"}],"updateDate":"2024-08-13","timestamp":1722448011000,"abstract":"  We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cG0Bzixu-5diFlKB32dpfT6A4EqhUNonQEplE3zmuU0","pdfSize":"2735834"}
