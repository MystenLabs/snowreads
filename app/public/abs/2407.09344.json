{"id":"2407.09344","title":"Pre-training Point Cloud Compact Model with Partial-aware Reconstruction","authors":"Yaohua Zha,Yanzi Wang,Tao Dai,Shu-Tao Xia","authorsParsed":[["Zha","Yaohua",""],["Wang","Yanzi",""],["Dai","Tao",""],["Xia","Shu-Tao",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 15:18:14 GMT"}],"updateDate":"2024-07-15","timestamp":1720797494000,"abstract":"  The pre-trained point cloud model based on Masked Point Modeling (MPM) has\nexhibited substantial improvements across various tasks. However, two drawbacks\nhinder their practical application. Firstly, the positional embedding of masked\npatches in the decoder results in the leakage of their central coordinates,\nleading to limited 3D representations. Secondly, the excessive model size of\nexisting MPM methods results in higher demands for devices. To address these,\nwe propose to pre-train Point cloud Compact Model with Partial-aware\n\\textbf{R}econstruction, named Point-CPR. Specifically, in the decoder, we\ncouple the vanilla masked tokens with their positional embeddings as randomly\nmasked queries and introduce a partial-aware prediction module before each\ndecoder layer to predict them from the unmasked partial. It prevents the\ndecoder from creating a shortcut between the central coordinates of masked\npatches and their reconstructed coordinates, enhancing the robustness of\nmodels. We also devise a compact encoder composed of local aggregation and\nMLPs, reducing the parameters and computational requirements compared to\nexisting Transformer-based encoders. Extensive experiments demonstrate that our\nmodel exhibits strong performance across various tasks, especially surpassing\nthe leading MPM-based model PointGPT-B with only 2% of its parameters.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HJyOnsVPl8LIHEsm6Oi53dYCDuKjJ2S9-sduyHQY9UY","pdfSize":"929026"}
