{"id":"2407.01290","title":"Hypformer: Exploring Efficient Hyperbolic Transformer Fully in\n  Hyperbolic Space","authors":"Menglin Yang, Harshit Verma, Delvin Ce Zhang, Jiahong Liu, Irwin King,\n  Rex Ying","authorsParsed":[["Yang","Menglin",""],["Verma","Harshit",""],["Zhang","Delvin Ce",""],["Liu","Jiahong",""],["King","Irwin",""],["Ying","Rex",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 13:44:38 GMT"}],"updateDate":"2024-07-02","timestamp":1719841478000,"abstract":"  Hyperbolic geometry have shown significant potential in modeling complex\nstructured data, particularly those with underlying tree-like and hierarchical\nstructures. Despite the impressive performance of various hyperbolic neural\nnetworks across numerous domains, research on adapting the Transformer to\nhyperbolic space remains limited. Previous attempts have mainly focused on\nmodifying self-attention modules in the Transformer. However, these efforts\nhave fallen short of developing a complete hyperbolic Transformer. This stems\nprimarily from: (i) the absence of well-defined modules in hyperbolic space,\nincluding linear transformation layers, LayerNorm layers, activation functions,\ndropout operations, etc. (ii) the quadratic time complexity of the existing\nhyperbolic self-attention module w.r.t the number of input tokens, which\nhinders its scalability. To address these challenges, we propose, Hypformer, a\nnovel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry.\nIn Hypformer, we introduce two foundational blocks that define the essential\nmodules of the Transformer in hyperbolic space. Furthermore, we develop a\nlinear self-attention mechanism in hyperbolic space, enabling hyperbolic\nTransformer to process billion-scale graph data and long-sequence inputs for\nthe first time. Our experimental results confirm the effectiveness and\nefficiency of Hypformer across various datasets, demonstrating its potential as\nan effective and scalable solution for large-scale data representation and\nlarge models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"f9QSsAZHFxFanKgSoXtH4WaudGH5K6vJXMY3AqkIZXc","pdfSize":"1306818"}
