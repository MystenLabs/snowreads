{"id":"2407.06581","title":"Vision language models are blind","authors":"Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh\n  Totti Nguyen","authorsParsed":[["Rahmanzadehgervi","Pooyan",""],["Bolton","Logan",""],["Taesiri","Mohammad Reza",""],["Nguyen","Anh Totti",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 06:20:17 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 15:33:10 GMT"},{"version":"v3","created":"Fri, 12 Jul 2024 04:55:18 GMT"},{"version":"v4","created":"Thu, 25 Jul 2024 04:19:58 GMT"},{"version":"v5","created":"Fri, 26 Jul 2024 03:27:58 GMT"}],"updateDate":"2024-07-29","timestamp":1720506017000,"abstract":"  While large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro, are powering various image-text applications and scoring high\non many vision-understanding benchmarks, we find that they are surprisingly\nstill struggling with low-level vision tasks that are easy to humans.\nSpecifically, on BlindTest, our suite of 7 very simple tasks such as\nidentifying (a) whether two circles overlap; (b) whether two lines intersect;\n(c) which letter is being circled in a word; and (d) counting circles in an\nOlympic-like logo, four state-of-the-art VLMs are only 58.57% accurate on\naverage. Claude 3.5 Sonnet performs the best at 74.94% accuracy, but this is\nstill far from the human expected accuracy of 100%. Across different image\nresolutions and line widths, VLMs consistently struggle with tasks that require\nprecise spatial information and recognizing geometric primitives that overlap\nor are close together. Code and data are available at:\nhttps://vlmsareblind.github.io\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}