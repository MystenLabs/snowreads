{"id":"2407.15023","title":"ViT LoS V2X: Vision Transformers for Environment-aware LoS Blockage\n  Prediction for 6G Vehicular Networks","authors":"Ghazi Gharsallah and Georges Kaddoum","authorsParsed":[["Gharsallah","Ghazi",""],["Kaddoum","Georges",""]],"versions":[{"version":"v1","created":"Thu, 27 Jun 2024 01:38:09 GMT"}],"updateDate":"2024-09-18","timestamp":1719452289000,"abstract":"  As wireless communication technology progresses towards the sixth generation\n(6G), high-frequency millimeter-wave (mmWave) communication has emerged as a\npromising candidate for enabling vehicular networks. It offers high data rates\nand low-latency communication. However, obstacles such as buildings, trees, and\nother vehicles can cause signal attenuation and blockage, leading to\ncommunication failures that can result in fatal accidents or traffic\ncongestion. Predicting blockages is crucial for ensuring reliable and efficient\ncommunications. Furthermore, the advent of 6G technology is anticipated to\nintegrate advanced sensing capabilities, utilizing a variety of sensor types.\nThese sensors, ranging from traditional RF sensors to cameras and Lidar\nsensors, are expected to provide access to rich multimodal data, thereby\nenriching communication systems with a wealth of additional contextual\ninformation. Leveraging this multimodal data becomes essential for making\nprecise network management decisions, including the crucial task of blockage\ndetection. In this paper, we propose a Deep Learning (DL)-based approach that\ncombines Convolutional Neural Networks (CNNs) and customized Vision\nTransformers (ViTs) to effectively extract essential information from\nmultimodal data and predict blockages in vehicular networks. Our method\ncapitalizes on the synergistic strengths of CNNs and ViTs to extract features\nfrom time-series multimodal data, which include images and beam vectors. To\ncapture temporal dependencies between the extracted features and the blockage\nstate at future time steps, we employ a Gated Recurrent Unit (GRU)-based\narchitecture. Our results show that the proposed approach achieves high\naccuracy and outperforms state-of-the-art solutions, achieving more than $95\\%$\naccurate predictions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Networking and Internet Architecture"],"license":"http://creativecommons.org/licenses/by/4.0/"}