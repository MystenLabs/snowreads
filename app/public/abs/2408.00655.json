{"id":"2408.00655","title":"SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context","authors":"Hongjun An, Yifan Chen, Zhe Sun, and Xuelong Li","authorsParsed":[["An","Hongjun",""],["Chen","Yifan",""],["Sun","Zhe",""],["Li","Xuelong",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 15:45:19 GMT"},{"version":"v2","created":"Fri, 2 Aug 2024 08:27:08 GMT"},{"version":"v3","created":"Tue, 6 Aug 2024 13:38:50 GMT"},{"version":"v4","created":"Wed, 7 Aug 2024 12:23:14 GMT"},{"version":"v5","created":"Wed, 14 Aug 2024 07:34:44 GMT"}],"updateDate":"2024-08-15","timestamp":1722527119000,"abstract":"  Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aiming at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), which includes a Sentence\nEncoder to compress multiple tokens in a sentence into a single token, and a\nSentence Decoder to reconstruct it. By integrating SentenceVAE into the input\nand output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference method. In addition, the SentenceVAE module of\nSLLMs can maintain the integrity of the original semantic content by segmenting\nthe context into sentences, thereby improving accuracy while boosting inference\nspeed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over\nequivalent context length, significantly reducing memory demands for\nself-attention computation and facilitating the handling of longer context.\nExtensive experiments on Wanjuan dataset have revealed that the proposed method\ncan accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%\nof its original metric, and decrease memory overhead by 86~91% for the\nequivalent context length, compared to previous token-by-token methods.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}