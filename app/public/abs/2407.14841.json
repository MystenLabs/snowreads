{"id":"2407.14841","title":"Text-based Talking Video Editing with Cascaded Conditional Diffusion","authors":"Bo Han, Heqing Zou, Haoyang Li, Guangcong Wang, Chng Eng Siong","authorsParsed":[["Han","Bo",""],["Zou","Heqing",""],["Li","Haoyang",""],["Wang","Guangcong",""],["Siong","Chng Eng",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 10:55:19 GMT"}],"updateDate":"2024-07-23","timestamp":1721472919000,"abstract":"  Text-based talking-head video editing aims to efficiently insert, delete, and\nsubstitute segments of talking videos through a user-friendly text editing\napproach. It is challenging because of \\textbf{1)} generalizable talking-face\nrepresentation, \\textbf{2)} seamless audio-visual transitions, and \\textbf{3)}\nidentity-preserved talking faces. Previous works either require minutes of\ntalking-face video training data and expensive test-time optimization for\ncustomized talking video editing or directly generate a video sequence without\nconsidering in-context information, leading to a poor generalizable\nrepresentation, or incoherent transitions, or even inconsistent identity. In\nthis paper, we propose an efficient cascaded conditional diffusion-based\nframework, which consists of two stages: audio to dense-landmark motion and\nmotion to video. \\textit{\\textbf{In the first stage}}, we first propose a\ndynamic weighted in-context diffusion module to synthesize dense-landmark\nmotions given an edited audio. \\textit{\\textbf{In the second stage}}, we\nintroduce a warping-guided conditional diffusion module. The module first\ninterpolates between the start and end frames of the editing interval to\ngenerate smooth intermediate frames. Then, with the help of the audio-to-dense\nmotion images, these intermediate frames are warped to obtain coarse\nintermediate frames. Conditioned on the warped intermedia frames, a diffusion\nmodel is adopted to generate detailed and high-resolution target frames, which\nguarantees coherent and identity-preserved transitions. The cascaded\nconditional diffusion model decomposes the complex talking editing task into\ntwo flexible generation tasks, which provides a generalizable talking-face\nrepresentation, seamless audio-visual transitions, and identity-preserved faces\non a small dataset. Experiments show the effectiveness and superiority of the\nproposed method.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}