{"id":"2407.01425","title":"FORA: Fast-Forward Caching in Diffusion Transformer Acceleration","authors":"Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, Luming\n  Liang","authorsParsed":[["Selvaraju","Pratheba",""],["Ding","Tianyu",""],["Chen","Tianyi",""],["Zharkov","Ilya",""],["Liang","Luming",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 16:14:37 GMT"}],"updateDate":"2024-07-02","timestamp":1719850477000,"abstract":"  Diffusion transformers (DiT) have become the de facto choice for generating\nhigh-quality images and videos, largely due to their scalability, which enables\nthe construction of larger models for enhanced performance. However, the\nincreased size of these models leads to higher inference costs, making them\nless attractive for real-time applications. We present Fast-FORward CAching\n(FORA), a simple yet effective approach designed to accelerate DiT by\nexploiting the repetitive nature of the diffusion process. FORA implements a\ncaching mechanism that stores and reuses intermediate outputs from the\nattention and MLP layers across denoising steps, thereby reducing computational\noverhead. This approach does not require model retraining and seamlessly\nintegrates with existing transformer-based diffusion models. Experiments show\nthat FORA can speed up diffusion transformers several times over while only\nminimally affecting performance metrics such as the IS Score and FID. By\nenabling faster processing with minimal trade-offs in quality, FORA represents\na significant advancement in deploying diffusion transformers for real-time\napplications. Code will be made publicly available at:\nhttps://github.com/prathebaselva/FORA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}