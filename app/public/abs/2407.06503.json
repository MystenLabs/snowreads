{"id":"2407.06503","title":"Preference-Guided Reinforcement Learning for Efficient Exploration","authors":"Guojian Wang, Faguo Wu, Xiao Zhang, Tianyuan Chen, Xuyang Chen, Lin\n  Zhao","authorsParsed":[["Wang","Guojian",""],["Wu","Faguo",""],["Zhang","Xiao",""],["Chen","Tianyuan",""],["Chen","Xuyang",""],["Zhao","Lin",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 02:11:12 GMT"}],"updateDate":"2024-07-10","timestamp":1720491072000,"abstract":"  In this paper, we investigate preference-based reinforcement learning (PbRL)\nthat allows reinforcement learning (RL) agents to learn from human feedback.\nThis is particularly valuable when defining a fine-grain reward function is not\nfeasible. However, this approach is inefficient and impractical for promoting\ndeep exploration in hard-exploration tasks with long horizons and sparse\nrewards. To tackle this issue, we introduce LOPE: Learning Online with\ntrajectory Preference guidancE, an end-to-end preference-guided RL framework\nthat enhances exploration efficiency in hard-exploration tasks. Our intuition\nis that LOPE directly adjusts the focus of online exploration by considering\nhuman feedback as guidance, avoiding learning a separate reward model from\npreferences. Specifically, LOPE includes a two-step sequential policy\noptimization process consisting of trust-region-based policy improvement and\npreference guidance steps. We reformulate preference guidance as a novel\ntrajectory-wise state marginal matching problem that minimizes the maximum mean\ndiscrepancy distance between the preferred trajectories and the learned policy.\nFurthermore, we provide a theoretical analysis to characterize the performance\nimprovement bound and evaluate the LOPE's effectiveness. When assessed in\nvarious challenging hard-exploration environments, LOPE outperforms several\nstate-of-the-art methods regarding convergence rate and overall performance.\nThe code used in this study is available at\n\\url{https://github.com/buaawgj/LOPE}.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}