{"id":"2408.12136","title":"Domain Adaptation for Offline Reinforcement Learning with Limited\n  Samples","authors":"Weiqin Chen, Sandipan Mishra and Santiago Paternain","authorsParsed":[["Chen","Weiqin",""],["Mishra","Sandipan",""],["Paternain","Santiago",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 05:38:48 GMT"}],"updateDate":"2024-08-23","timestamp":1724305128000,"abstract":"  Offline reinforcement learning (RL) learns effective policies from a static\ntarget dataset. Despite state-of-the-art (SOTA) offline RL algorithms being\npromising, they highly rely on the quality of the target dataset. The\nperformance of SOTA algorithms can degrade in scenarios with limited samples in\nthe target dataset, which is often the case in real-world applications. To\naddress this issue, domain adaptation that leverages auxiliary samples from\nrelated source datasets (such as simulators) can be beneficial. In this\ncontext, determining the optimal way to trade off the source and target\ndatasets remains a critical challenge in offline RL. To the best of our\nknowledge, this paper proposes the first framework that theoretically and\nexperimentally explores how the weight assigned to each dataset affects the\nperformance of offline RL. We establish the performance bounds and convergence\nneighborhood of our framework, both of which depend on the selection of the\nweight. Furthermore, we identify the existence of an optimal weight for\nbalancing the two datasets. All theoretical guarantees and optimal weight\ndepend on the quality of the source dataset and the size of the target dataset.\nOur empirical results on the well-known Procgen Benchmark substantiate our\ntheoretical contributions.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}