{"id":"2408.12895","title":"Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion\n  Recognition","authors":"Cam-Van Thi Nguyen, The-Son Le, Anh-Tuan Mai, Duc-Trong Le","authorsParsed":[["Nguyen","Cam-Van Thi",""],["Le","The-Son",""],["Mai","Anh-Tuan",""],["Le","Duc-Trong",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 07:59:51 GMT"}],"updateDate":"2024-08-26","timestamp":1724399991000,"abstract":"  Multimodal Emotion Recognition in Conversations (ERC) is a typical multimodal\nlearning task in exploiting various data modalities concurrently. Prior studies\non effective multimodal ERC encounter challenges in addressing modality\nimbalances and optimizing learning across modalities. Dealing with these\nproblems, we present a novel framework named Ada2I, which consists of two\ninseparable modules namely Adaptive Feature Weighting (AFW) and Adaptive\nModality Weighting (AMW) for feature-level and modality-level balancing\nrespectively via leveraging both Inter- and Intra-modal interactions.\nAdditionally, we introduce a refined disparity ratio as part of our training\noptimization strategy, a simple yet effective measure to assess the overall\ndiscrepancy of the model's learning process when handling multiple modalities\nsimultaneously. Experimental results validate the effectiveness of Ada2I with\nstate-of-the-art performance compared to baselines on three benchmark datasets,\nparticularly in addressing modality imbalances.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7Ikyb9bLtZp6-yUMCV78yjyh_uFA-dAaI4HfdcDctgA","pdfSize":"1338420"}
