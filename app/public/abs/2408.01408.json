{"id":"2408.01408","title":"Derivation of Back-propagation for Graph Convolutional Networks using\n  Matrix Calculus and its Application to Explainable Artificial Intelligence","authors":"Yen-Che Hsiao, Rongting Yue, Abhishek Dutta","authorsParsed":[["Hsiao","Yen-Che",""],["Yue","Rongting",""],["Dutta","Abhishek",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 17:33:52 GMT"}],"updateDate":"2024-08-05","timestamp":1722620032000,"abstract":"  This paper provides a comprehensive and detailed derivation of the\nbackpropagation algorithm for graph convolutional neural networks using matrix\ncalculus. The derivation is extended to include arbitrary element-wise\nactivation functions and an arbitrary number of layers. The study addresses two\nfundamental problems, namely node classification and link prediction. To\nvalidate our method, we compare it with reverse-mode automatic differentiation.\nThe experimental results demonstrate that the median sum of squared errors of\nthe updated weight matrices, when comparing our method to the approach using\nreverse-mode automatic differentiation, falls within the range of $10^{-18}$ to\n$10^{-14}$. These outcomes are obtained from conducting experiments on a\nfive-layer graph convolutional network, applied to a node classification\nproblem on Zachary's karate club social network and a link prediction problem\non a drug-drug interaction network. Finally, we show how the derived\nclosed-form solution can facilitate the development of explainable AI and\nsensitivity analysis.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gLQADfJqlAglzb9yM4QRxehpRD4OaiOSeMclFviawyI","pdfSize":"7958926","txDigest":"AgC62EGwp61qZHnH7efeKY5WKgBNrzFpZTB6ks1waFzk","endEpoch":"1","status":"CERTIFIED"}
