{"id":"2408.06047","title":"BooW-VTON: Boosting In-the-Wild Virtual Try-On via Mask-Free Pseudo Data\n  Training","authors":"Xuanpu Zhang and Dan Song and Pengxin Zhan and Qingguo Chen and Zhao\n  Xu and Weihua Luo and Kaifu Zhang and Anan Liu","authorsParsed":[["Zhang","Xuanpu",""],["Song","Dan",""],["Zhan","Pengxin",""],["Chen","Qingguo",""],["Xu","Zhao",""],["Luo","Weihua",""],["Zhang","Kaifu",""],["Liu","Anan",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 10:39:59 GMT"}],"updateDate":"2024-08-13","timestamp":1723459199000,"abstract":"  Image-based virtual try-on is an increasingly popular and important task to\ngenerate realistic try-on images of specific person. Existing methods always\nemploy an accurate mask to remove the original garment in the source image,\nthus achieving realistic synthesized images in simple and conventional try-on\nscenarios based on powerful diffusion model. Therefore, acquiring suitable mask\nis vital to the try-on performance of these methods. However, obtaining precise\ninpainting masks, especially for complex wild try-on data containing diverse\nforeground occlusions and person poses, is not easy as Figure 1-Top shows. This\ndifficulty often results in poor performance in more practical and challenging\nreal-life scenarios, such as the selfie scene shown in Figure 1-Bottom. To this\nend, we propose a novel training paradigm combined with an efficient data\naugmentation method to acquire large-scale unpaired training data from wild\nscenarios, thereby significantly facilitating the try-on performance of our\nmodel without the need for additional inpainting masks. Besides, a try-on\nlocalization loss is designed to localize a more accurate try-on area to obtain\nmore reasonable try-on results. It is noted that our method only needs the\nreference cloth image, source pose image and source person image as input,\nwhich is more cost-effective and user-friendly compared to existing methods.\nExtensive qualitative and quantitative experiments have demonstrated superior\nperformance in wild scenarios with such a low-demand input.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}