{"id":"2407.13987","title":"RealViformer: Investigating Attention for Real-World Video\n  Super-Resolution","authors":"Yuehan Zhang and Angela Yao","authorsParsed":[["Zhang","Yuehan",""],["Yao","Angela",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 02:32:49 GMT"}],"updateDate":"2024-07-22","timestamp":1721356369000,"abstract":"  In real-world video super-resolution (VSR), videos suffer from in-the-wild\ndegradations and artifacts. VSR methods, especially recurrent ones, tend to\npropagate artifacts over time in the real-world setting and are more vulnerable\nthan image super-resolution. This paper investigates the influence of artifacts\non commonly used covariance-based attention mechanisms in VSR. Comparing the\nwidely-used spatial attention, which computes covariance over space, versus the\nchannel attention, we observe that the latter is less sensitive to artifacts.\nHowever, channel attention leads to feature redundancy, as evidenced by the\nhigher covariance among output channels. As such, we explore simple techniques\nsuch as the squeeze-excite mechanism and covariance-based rescaling to counter\nthe effects of high channel covariance. Based on our findings, we propose\nRealViformer. This channel-attention-based real-world VSR framework surpasses\nstate-of-the-art on two real-world VSR datasets with fewer parameters and\nfaster runtimes. The source code is available at\nhttps://github.com/Yuehan717/RealViformer.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}