{"id":"2407.13164","title":"Translate-and-Revise: Boosting Large Language Models for Constrained\n  Translation","authors":"Pengcheng Huang and Yongyu Mu and Yuzhang Wu and Bei Li and Chunyang\n  Xiao and Tong Xiao and Jingbo Zhu","authorsParsed":[["Huang","Pengcheng",""],["Mu","Yongyu",""],["Wu","Yuzhang",""],["Li","Bei",""],["Xiao","Chunyang",""],["Xiao","Tong",""],["Zhu","Jingbo",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 05:08:09 GMT"}],"updateDate":"2024-07-19","timestamp":1721279289000,"abstract":"  Imposing constraints on machine translation systems presents a challenging\nissue because these systems are not trained to make use of constraints in\ngenerating adequate, fluent translations. In this paper, we leverage the\ncapabilities of large language models (LLMs) for constrained translation, given\nthat LLMs can easily adapt to this task by taking translation instructions and\nconstraints as prompts. However, LLMs cannot always guarantee the adequacy of\ntranslation, and, in some cases, ignore the given constraints. This is in part\nbecause LLMs might be overly confident in their predictions, overriding the\ninfluence of the constraints. To overcome this overiding behaviour, we propose\nto add a revision process that encourages LLMs to correct the outputs by\nprompting them about the constraints that have not yet been met. We evaluate\nour approach on four constrained translation tasks, encompassing both lexical\nand structural constraints in multiple constraint domains. Experiments show\n15\\% improvement in constraint-based translation accuracy over standard LLMs\nand the approach also significantly outperforms neural machine translation\n(NMT) state-of-the-art methods.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}