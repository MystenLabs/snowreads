{"id":"2407.03825","title":"StreamLTS: Query-based Temporal-Spatial LiDAR Fusion for Cooperative\n  Object Detection","authors":"Yunshuang Yuan and Monika Sester","authorsParsed":[["Yuan","Yunshuang",""],["Sester","Monika",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 10:56:10 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 15:40:42 GMT"}],"updateDate":"2024-08-23","timestamp":1720090570000,"abstract":"  Cooperative perception via communication among intelligent traffic agents has\ngreat potential to improve the safety of autonomous driving. However, limited\ncommunication bandwidth, localization errors and asynchronized capturing time\nof sensor data, all introduce difficulties to the data fusion of different\nagents. To some extend, previous works have attempted to reduce the shared data\nsize, mitigate the spatial feature misalignment caused by localization errors\nand communication delay. However, none of them have considered the\nasynchronized sensor ticking times, which can lead to dynamic object\nmisplacement of more than one meter during data fusion. In this work, we\npropose Time-Aligned COoperative Object Detection (TA-COOD), for which we adapt\nwidely used dataset OPV2V and DairV2X with considering asynchronous LiDAR\nsensor ticking times and build an efficient fully sparse framework with\nmodeling the temporal information of individual objects with query-based\ntechniques. The experiment results confirmed the superior efficiency of our\nfully sparse framework compared to the state-of-the-art dense models. More\nimportantly, they show that the point-wise observation timestamps of the\ndynamic objects are crucial for accurate modeling the object temporal context\nand the predictability of their time-related locations. The official code is\navailable at \\url{https://github.com/YuanYunshuang/CoSense3D}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/"}