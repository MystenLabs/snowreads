{"id":"2407.13362","title":"Open Vocabulary 3D Scene Understanding via Geometry Guided\n  Self-Distillation","authors":"Pengfei Wang and Yuxi Wang and Shuai Li and Zhaoxiang Zhang and Zhen\n  Lei and Lei Zhang","authorsParsed":[["Wang","Pengfei",""],["Wang","Yuxi",""],["Li","Shuai",""],["Zhang","Zhaoxiang",""],["Lei","Zhen",""],["Zhang","Lei",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 10:13:56 GMT"}],"updateDate":"2024-07-19","timestamp":1721297636000,"abstract":"  The scarcity of large-scale 3D-text paired data poses a great challenge on\nopen vocabulary 3D scene understanding, and hence it is popular to leverage\ninternet-scale 2D data and transfer their open vocabulary capabilities to 3D\nmodels through knowledge distillation. However, the existing distillation-based\n3D scene understanding approaches rely on the representation capacity of 2D\nmodels, disregarding the exploration of geometric priors and inherent\nrepresentational advantages offered by 3D data. In this paper, we propose an\neffective approach, namely Geometry Guided Self-Distillation (GGSD), to learn\nsuperior 3D representations from 2D pre-trained models. Specifically, we first\ndesign a geometry guided distillation module to distill knowledge from 2D\nmodels, and then leverage the 3D geometric priors to alleviate the inherent\nnoise in 2D models and enhance the representation learning process. Due to the\nadvantages of 3D representation, the performance of the distilled 3D student\nmodel can significantly surpass that of the 2D teacher model. This motivates us\nto further leverage the representation advantages of 3D data through\nself-distillation. As a result, our proposed GGSD approach outperforms the\nexisting open vocabulary 3D scene understanding methods by a large margin, as\ndemonstrated by our experiments on both indoor and outdoor benchmark datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}