{"id":"2407.03851","title":"Implicit Hypersurface Approximation Capacity in Deep ReLU Networks","authors":"Jonatan Vallin, Karl Larsson, Mats G. Larson","authorsParsed":[["Vallin","Jonatan",""],["Larsson","Karl",""],["Larson","Mats G.",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 11:34:42 GMT"}],"updateDate":"2024-07-08","timestamp":1720092882000,"abstract":"  We develop a geometric approximation theory for deep feed-forward neural\nnetworks with ReLU activations. Given a $d$-dimensional hypersurface in\n$\\mathbb{R}^{d+1}$ represented as the graph of a $C^2$-function $\\phi$, we show\nthat a deep fully-connected ReLU network of width $d+1$ can implicitly\nconstruct an approximation as its zero contour with a precision bound depending\non the number of layers. This result is directly applicable to the binary\nclassification setting where the sign of the network is trained as a\nclassifier, with the network's zero contour as a decision boundary. Our proof\nis constructive and relies on the geometrical structure of ReLU layers provided\nin [doi:10.48550/arXiv.2310.03482]. Inspired by this geometrical description,\nwe define a new equivalent network architecture that is easier to interpret\ngeometrically, where the action of each hidden layer is a projection onto a\npolyhedral cone derived from the layer's parameters. By repeatedly adding such\nlayers, with parameters chosen such that we project small parts of the graph of\n$\\phi$ from the outside in, we, in a controlled way, construct a network that\nimplicitly approximates the graph over a ball of radius $R$. The accuracy of\nthis construction is controlled by a discretization parameter $\\delta$ and we\nshow that the tolerance in the resulting error bound scales as\n$(d-1)R^{3/2}\\delta^{1/2}$ and the required number of layers is of order\n$d\\big(\\frac{32R}{\\delta}\\big)^{\\frac{d+1}{2}}$.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}