{"id":"2407.09033","title":"Textual Query-Driven Mask Transformer for Domain Generalized\n  Segmentation","authors":"Byeonghyun Pak, Byeongju Woo, Sunghwan Kim, Dae-hwan Kim, Hoseong Kim","authorsParsed":[["Pak","Byeonghyun",""],["Woo","Byeongju",""],["Kim","Sunghwan",""],["Kim","Dae-hwan",""],["Kim","Hoseong",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 06:49:16 GMT"},{"version":"v2","created":"Wed, 31 Jul 2024 14:27:06 GMT"}],"updateDate":"2024-08-01","timestamp":1720766956000,"abstract":"  In this paper, we introduce a method to tackle Domain Generalized Semantic\nSegmentation (DGSS) by utilizing domain-invariant semantic knowledge from text\nembeddings of vision-language models. We employ the text embeddings as object\nqueries within a transformer-based segmentation framework (textual object\nqueries). These queries are regarded as a domain-invariant basis for pixel\ngrouping in DGSS. To leverage the power of textual object queries, we introduce\na novel framework named the textual query-driven mask transformer (tqdm). Our\ntqdm aims to (1) generate textual object queries that maximally encode\ndomain-invariant semantics and (2) enhance the semantic clarity of dense visual\nfeatures. Additionally, we suggest three regularization losses to improve the\nefficacy of tqdm by aligning between visual and textual features. By utilizing\nour method, the model can comprehend inherent semantic information for classes\nof interest, enabling it to generalize to extreme domains (e.g., sketch style).\nOur tqdm achieves 68.9 mIoU on GTA5$\\rightarrow$Cityscapes, outperforming the\nprior state-of-the-art method by 2.5 mIoU. The project page is available at\nhttps://byeonghyunpak.github.io/tqdm.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}