{"id":"2408.03459","title":"On the Generalization of Preference Learning with DPO","authors":"Shawn Im, Yixuan Li","authorsParsed":[["Im","Shawn",""],["Li","Yixuan",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 22:11:00 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 14:45:34 GMT"}],"updateDate":"2024-08-13","timestamp":1722982260000,"abstract":"  Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Ze06htxF64Ba9Z1oAszQic8Ok-gonOor0pGuuZAYIv0","pdfSize":"1344949"}
