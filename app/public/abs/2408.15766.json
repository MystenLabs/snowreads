{"id":"2408.15766","title":"Learning Harmonized Representations for Speculative Sampling","authors":"Lefan Zhang, Xiaodan Wang, Yanhua Huang, Ruiwen Xu","authorsParsed":[["Zhang","Lefan",""],["Wang","Xiaodan",""],["Huang","Yanhua",""],["Xu","Ruiwen",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 12:59:12 GMT"},{"version":"v2","created":"Thu, 19 Sep 2024 15:46:57 GMT"}],"updateDate":"2024-09-20","timestamp":1724849952000,"abstract":"  Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}