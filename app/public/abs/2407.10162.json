{"id":"2407.10162","title":"ChatLogic: Integrating Logic Programming with Large Language Models for\n  Multi-Step Reasoning","authors":"Zhongsheng Wang, Jiamou Liu, Qiming Bao, Hongfei Rong, Jingfeng Zhang","authorsParsed":[["Wang","Zhongsheng",""],["Liu","Jiamou",""],["Bao","Qiming",""],["Rong","Hongfei",""],["Zhang","Jingfeng",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 11:06:43 GMT"}],"updateDate":"2024-07-16","timestamp":1720955203000,"abstract":"  Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated\nimpressive capabilities in various generative tasks. However, their performance\nis often hampered by limitations in accessing and leveraging long-term memory,\nleading to specific vulnerabilities and biases, especially during long\ninteractions. This paper introduces ChatLogic, an innovative framework\nspecifically targeted at LLM reasoning tasks that can enhance the performance\nof LLMs in multi-step deductive reasoning tasks by integrating logic\nprogramming. In ChatLogic, the language model plays a central role, acting as a\ncontroller and participating in every system operation stage. We propose a\nnovel method of converting logic problems into symbolic integration with an\ninference engine. This approach leverages large language models' situational\nunderstanding and imitation skills and uses symbolic memory to enhance\nmulti-step deductive reasoning capabilities. Our results show that the\nChatLogic framework significantly improves the multi-step reasoning\ncapabilities of LLMs. The source code and data are available at\n\\url{https://github.com/Strong-AI-Lab/ChatLogic}\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}