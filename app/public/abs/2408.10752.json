{"id":"2408.10752","title":"Security Assessment of Hierarchical Federated Deep Learning","authors":"D Alqattan, R Sun, H Liang, G Nicosia, V Snasel, R Ranjan, and V Ojha","authorsParsed":[["Alqattan","D",""],["Sun","R",""],["Liang","H",""],["Nicosia","G",""],["Snasel","V",""],["Ranjan","R",""],["Ojha","V",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 11:34:23 GMT"}],"updateDate":"2024-08-21","timestamp":1724153663000,"abstract":"  Hierarchical federated learning (HFL) is a promising distributed deep\nlearning model training paradigm, but it has crucial security concerns arising\nfrom adversarial attacks. This research investigates and assesses the security\nof HFL using a novel methodology by focusing on its resilience against\nadversarial attacks inference-time and training-time. Through a series of\nextensive experiments across diverse datasets and attack scenarios, we uncover\nthat HFL demonstrates robustness against untargeted training-time attacks due\nto its hierarchical structure. However, targeted attacks, particularly backdoor\nattacks, exploit this architecture, especially when malicious clients are\npositioned in the overlapping coverage areas of edge servers. Consequently, HFL\nshows a dual nature in its resilience, showcasing its capability to recover\nfrom attacks thanks to its hierarchical aggregation that strengthens its\nsuitability for adversarial training, thereby reinforcing its resistance\nagainst inference-time attacks. These insights underscore the necessity for\nbalanced security strategies in HFL systems, leveraging their inherent\nstrengths while effectively mitigating vulnerabilities.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}