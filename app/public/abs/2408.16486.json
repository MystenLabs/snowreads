{"id":"2408.16486","title":"Adapting Vision-Language Models to Open Classes via Test-Time Prompt\n  Tuning","authors":"Zhengqing Gao and Xiang Ao and Xu-Yao Zhang and Cheng-Lin Liu","authorsParsed":[["Gao","Zhengqing",""],["Ao","Xiang",""],["Zhang","Xu-Yao",""],["Liu","Cheng-Lin",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 12:34:01 GMT"}],"updateDate":"2024-08-30","timestamp":1724934841000,"abstract":"  Adapting pre-trained models to open classes is a challenging problem in\nmachine learning. Vision-language models fully explore the knowledge of text\nmodality, demonstrating strong zero-shot recognition performance, which is\nnaturally suited for various open-set problems. More recently, some research\nfocuses on fine-tuning such models to downstream tasks. Prompt tuning methods\nachieved huge improvements by learning context vectors on few-shot data.\nHowever, through the evaluation under open-set adaptation setting with the test\ndata including new classes, we find that there exists a dilemma that learned\nprompts have worse generalization abilities than hand-crafted prompts. In this\npaper, we consider combining the advantages of both and come up with a\ntest-time prompt tuning approach, which leverages the maximum concept matching\n(MCM) scores as dynamic weights to generate an input-conditioned prompt for\neach image during test. Through extensive experiments on 11 different datasets,\nwe show that our proposed method outperforms all comparison methods on average\nconsidering both base and new classes. The code is available at\nhttps://github.com/gaozhengqing/TTPT\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}