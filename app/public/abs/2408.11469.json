{"id":"2408.11469","title":"The Self-Contained Negation Test Set","authors":"David Kletz (Lattice, LLF - UMR7110, UPCit\\'e), Pascal Amsili\n  (Lattice), Marie Candito (LLF UMR7110, UPCit\\'e)","authorsParsed":[["Kletz","David","","Lattice, LLF - UMR7110, UPCité"],["Amsili","Pascal","","Lattice"],["Candito","Marie","","LLF UMR7110, UPCité"]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 09:38:15 GMT"}],"updateDate":"2024-08-22","timestamp":1724233095000,"abstract":"  Several methodologies have recently been proposed to evaluate the ability of\nPretrained Language Models (PLMs) to interpret negation. In this article, we\nbuild on Gubelmann and Handschuh (2022), which studies the modification of\nPLMs' predictions as a function of the polarity of inputs, in English.\nCrucially, this test uses ``self-contained'' inputs ending with a masked\nposition: depending on the polarity of a verb in the input, a particular token\nis either semantically ruled out or allowed at the masked position. By\nreplicating Gubelmann and Handschuh (2022) experiments, we have uncovered flaws\nthat weaken the conclusions that can be drawn from this test. We thus propose\nan improved version, the Self-Contained Neg Test, which is more controlled,\nmore systematic, and entirely based on examples forming minimal pairs varying\nonly in the presence or absence of verbal negation in English. When applying\nour test to the roberta and bert base and large models, we show that only\nroberta-large shows trends that match the expectations, while bert-base is\nmostly insensitive to negation. For all the tested models though, in a\nsignificant number of test instances the top-1 prediction remains the token\nthat is semantically forbidden by the context, which shows how much room for\nimprovement remains for a proper treatment of the negation phenomenon.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}