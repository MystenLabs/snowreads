{"id":"2407.04955","title":"Asynchronous Multimodal Video Sequence Fusion via Learning\n  Modality-Exclusive and -Agnostic Representations","authors":"Dingkang Yang, Mingcheng Li, Linhao Qu, Kun Yang, Peng Zhai, Song\n  Wang, Lihua Zhang","authorsParsed":[["Yang","Dingkang",""],["Li","Mingcheng",""],["Qu","Linhao",""],["Yang","Kun",""],["Zhai","Peng",""],["Wang","Song",""],["Zhang","Lihua",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 04:36:48 GMT"}],"updateDate":"2024-07-09","timestamp":1720240608000,"abstract":"  Understanding human intentions (e.g., emotions) from videos has received\nconsiderable attention recently. Video streams generally constitute a blend of\ntemporal data stemming from distinct modalities, including natural language,\nfacial expressions, and auditory clues. Despite the impressive advancements of\nprevious works via attention-based paradigms, the inherent temporal asynchrony\nand modality heterogeneity challenges remain in multimodal sequence fusion,\ncausing adverse performance bottlenecks. To tackle these issues, we propose a\nMultimodal fusion approach for learning modality-Exclusive and\nmodality-Agnostic representations (MEA) to refine multimodal features and\nleverage the complementarity across distinct modalities. On the one hand, MEA\nintroduces a predictive self-attention module to capture reliable context\ndynamics within modalities and reinforce unique features over the\nmodality-exclusive spaces. On the other hand, a hierarchical cross-modal\nattention module is designed to explore valuable element correlations among\nmodalities over the modality-agnostic space. Meanwhile, a double-discriminator\nstrategy is presented to ensure the production of distinct representations in\nan adversarial manner. Eventually, we propose a decoupled graph fusion\nmechanism to enhance knowledge exchange across heterogeneous modalities and\nlearn robust multimodal representations for downstream tasks. Numerous\nexperiments are implemented on three multimodal datasets with asynchronous\nsequences. Systematic analyses show the necessity of our approach.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}