{"id":"2407.11780","title":"SwitchCIT: Switching for Continual Instruction Tuning of Large Language\n  Models","authors":"Xinbo Wu, Max Hartman, Vidhata Arjun Jayaraman, Lav R. Varshney","authorsParsed":[["Wu","Xinbo",""],["Hartman","Max",""],["Jayaraman","Vidhata Arjun",""],["Varshney","Lav R.",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:37:33 GMT"}],"updateDate":"2024-07-17","timestamp":1721140653000,"abstract":"  Large language models (LLMs) have exhibited impressive capabilities in\nvarious domains, particularly in general language understanding. However these\nmodels, trained on massive text data, may not be finely optimized for specific\ntasks triggered by instructions. Continual instruction tuning is crucial to\nadapt LLMs to evolving tasks and domains, ensuring their effectiveness and\nrelevance across a wide range of applications. In the context of continual\ninstruction tuning, where models are sequentially trained on different tasks,\ncatastrophic forgetting can occur, leading to performance degradation on\npreviously learned tasks. This work addresses the catastrophic forgetting in\ncontinual instruction learning for LLMs through a switching mechanism for\nrouting computations to parameter-efficient tuned models. We demonstrate the\neffectiveness of our method through experiments on continual instruction tuning\nof different natural language generation tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}