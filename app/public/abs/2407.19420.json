{"id":"2407.19420","title":"UniGAP: A Universal and Adaptive Graph Upsampling Approach to Mitigate\n  Over-Smoothing in Node Classification Tasks","authors":"Xiaotang Wang, Yun Zhu, Haizhou Shi, Yongchao Liu, Chuntao Hong","authorsParsed":[["Wang","Xiaotang",""],["Zhu","Yun",""],["Shi","Haizhou",""],["Liu","Yongchao",""],["Hong","Chuntao",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 07:44:13 GMT"}],"updateDate":"2024-07-30","timestamp":1722152653000,"abstract":"  In the graph domain, deep graph networks based on Message Passing Neural\nNetworks (MPNNs) or Graph Transformers often cause over-smoothing of node\nfeatures, limiting their expressive capacity. Many upsampling techniques\ninvolving node and edge manipulation have been proposed to mitigate this issue.\nHowever, these methods often require extensive manual labor, resulting in\nsuboptimal performance and lacking a universal integration strategy. In this\nstudy, we introduce UniGAP, a universal and adaptive graph upsampling technique\nfor graph data. It provides a universal framework for graph upsampling,\nencompassing most current methods as variants. Moreover, UniGAP serves as a\nplug-in component that can be seamlessly and adaptively integrated with\nexisting GNNs to enhance performance and mitigate the over-smoothing problem.\nThrough extensive experiments, UniGAP demonstrates significant improvements\nover heuristic data augmentation methods across various datasets and metrics.\nWe analyze how graph structure evolves with UniGAP, identifying key bottlenecks\nwhere over-smoothing occurs, and providing insights into how UniGAP addresses\nthis issue. Lastly, we show the potential of combining UniGAP with large\nlanguage models (LLMs) to further improve downstream performance. Our code is\navailable at: https://github.com/wangxiaotang0906/UniGAP\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}