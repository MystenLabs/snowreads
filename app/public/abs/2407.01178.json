{"id":"2407.01178","title":"$\\text{Memory}^3$: Language Modeling with Explicit Memory","authors":"Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang,\n  Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai\n  Chen, Feiyu Xiong, Linpeng Tang, Weinan E","authorsParsed":[["Yang","Hongkang",""],["Lin","Zehao",""],["Wang","Wenjin",""],["Wu","Hao",""],["Li","Zhiyu",""],["Tang","Bo",""],["Wei","Wenqiang",""],["Wang","Jinbo",""],["Tang","Zeyun",""],["Song","Shichao",""],["Xi","Chenyang",""],["Yu","Yu",""],["Chen","Kai",""],["Xiong","Feiyu",""],["Tang","Linpeng",""],["E","Weinan",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 11:07:23 GMT"}],"updateDate":"2024-07-02","timestamp":1719832043000,"abstract":"  The training and inference of large language models (LLMs) are together a\ncostly process that transports knowledge from raw data to meaningful\ncomputation. Inspired by the memory hierarchy of the human brain, we reduce\nthis cost by equipping LLMs with explicit memory, a memory format cheaper than\nmodel parameters and text retrieval-augmented generation (RAG). Conceptually,\nwith most of its knowledge externalized to explicit memories, the LLM can enjoy\na smaller parameter size, training cost, and inference cost, all proportional\nto the amount of remaining \"abstract knowledge\". As a preliminary proof of\nconcept, we train from scratch a 2.4B LLM, which achieves better performance\nthan much larger LLMs as well as RAG models, and maintains higher decoding\nspeed than RAG. The model is named $\\text{Memory}^3$, since explicit memory is\nthe third form of memory in LLMs after implicit memory (model parameters) and\nworking memory (context key-values). We introduce a memory circuitry theory to\nsupport the externalization of knowledge, and present novel techniques\nincluding a memory sparsification mechanism that makes storage tractable and a\ntwo-stage pretraining scheme that facilitates memory formation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}