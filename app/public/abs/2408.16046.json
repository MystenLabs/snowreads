{"id":"2408.16046","title":"Scaling Up Diffusion and Flow-based XGBoost Models","authors":"Jesse C. Cresswell, Taewoo Kim","authorsParsed":[["Cresswell","Jesse C.",""],["Kim","Taewoo",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 18:00:00 GMT"}],"updateDate":"2024-08-30","timestamp":1724868000000,"abstract":"  Novel machine learning methods for tabular data generation are often\ndeveloped on small datasets which do not match the scale required for\nscientific applications. We investigate a recent proposal to use XGBoost as the\nfunction approximator in diffusion and flow-matching models on tabular data,\nwhich proved to be extremely memory intensive, even on tiny datasets. In this\nwork, we conduct a critical analysis of the existing implementation from an\nengineering perspective, and show that these limitations are not fundamental to\nthe method; with better implementation it can be scaled to datasets 370x larger\nthan previously used. Our efficient implementation also unlocks scaling models\nto much larger sizes which we show directly leads to improved performance on\nbenchmark tasks. We also propose algorithmic improvements that can further\nbenefit resource usage and model performance, including multi-output trees\nwhich are well-suited to generative modeling. Finally, we present results on\nlarge-scale scientific datasets derived from experimental particle physics as\npart of the Fast Calorimeter Simulation Challenge. Code is available at\nhttps://github.com/layer6ai-labs/calo-forest.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}