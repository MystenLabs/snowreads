{"id":"2408.16594","title":"Continuous Gaussian mixture solution for linear Bayesian inversion with\n  application to Laplace priors","authors":"Rafael Flock, Yiqiu Dong, Felipe Uribe, Olivier Zahm","authorsParsed":[["Flock","Rafael",""],["Dong","Yiqiu",""],["Uribe","Felipe",""],["Zahm","Olivier",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 14:59:33 GMT"}],"updateDate":"2024-08-30","timestamp":1724943573000,"abstract":"  We focus on Bayesian inverse problems with Gaussian likelihood, linear\nforward model, and priors that can be formulated as a Gaussian mixture. Such a\nmixture is expressed as an integral of Gaussian density functions weighted by a\nmixing density over the mixing variables. Within this framework, the\ncorresponding posterior distribution also takes the form of a Gaussian mixture,\nand we derive the closed-form expression for its posterior mixing density. To\nsample from the posterior Gaussian mixture, we propose a two-step sampling\nmethod. First, we sample the mixture variables from the posterior mixing\ndensity, and then we sample the variables of interest from Gaussian densities\nconditioned on the sampled mixing variables. However, the posterior mixing\ndensity is relatively difficult to sample from, especially in high dimensions.\nTherefore, we propose to replace the posterior mixing density by a\ndimension-reduced approximation, and we provide a bound in the Hellinger\ndistance for the resulting approximate posterior. We apply the proposed\napproach to a posterior with Laplace prior, where we introduce two\ndimension-reduced approximations for the posterior mixing density. Our\nnumerical experiments indicate that samples generated via the proposed\napproximations have very low correlation and are close to the exact posterior.\n","subjects":["Statistics/Computation","Mathematics/Probability","Statistics/Applications"],"license":"http://creativecommons.org/licenses/by/4.0/"}