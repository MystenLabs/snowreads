{"id":"2407.13377","title":"Linear-Complexity Self-Supervised Learning for Speech Processing","authors":"Shucong Zhang, Titouan Parcollet, Rogier van Dalen, Sourav\n  Bhattacharya","authorsParsed":[["Zhang","Shucong",""],["Parcollet","Titouan",""],["van Dalen","Rogier",""],["Bhattacharya","Sourav",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 10:34:33 GMT"}],"updateDate":"2024-07-19","timestamp":1721298873000,"abstract":"  Self-supervised learning (SSL) models usually require weeks of pre-training\nwith dozens of high-end GPUs. These models typically have a multi-headed\nself-attention (MHSA) context encoder. However, MHSA takes quadratic time and\nspace in the input length, contributing to the high pre-training cost.\nLinear-complexity alternatives to MHSA have been proposed. For instance, in\nsupervised training, the SummaryMixing model is the first to outperform MHSA\nacross multiple speech processing tasks. However, these cheaper alternatives\nhave not been explored for SSL yet. This paper studies a linear-complexity\ncontext encoder for SSL for the first time. With better or equivalent\nperformance for the downstream tasks of the MP3S benchmark, SummaryMixing\nreduces the pre-training time and peak VRAM of wav2vec 2.0 model by 18% and by\n23%, respectively, leading to the pre-training of a 155M wav2vec 2.0 model\nfinished within one week with 4 Tesla A100 GPUs. Code is available at\nhttps://github.com/SamsungLabs/SummaryMixing.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}