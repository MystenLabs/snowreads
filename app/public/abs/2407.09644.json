{"id":"2407.09644","title":"OXN -- Automated Observability Assessments for Cloud-Native Applications","authors":"Maria C. Borges, Joshua Bauer, Sebastian Werner","authorsParsed":[["Borges","Maria C.",""],["Bauer","Joshua",""],["Werner","Sebastian",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 19:04:13 GMT"}],"updateDate":"2024-07-16","timestamp":1720811053000,"abstract":"  Observability is important to ensure the reliability of microservice\napplications. These applications are often prone to failures, since they have\nmany independent services deployed on heterogeneous environments. When employed\n\"correctly\", observability can help developers identify and troubleshoot faults\nquickly. However, instrumenting and configuring the observability of a\nmicroservice application is not trivial but tool-dependent and tied to costs.\nPractitioners need to understand observability-related trade-offs in order to\nweigh between different observability design alternatives. Still, these\narchitectural design decisions are not supported by systematic methods and\ntypically just rely on \"professional intuition\".\n  To assess observability design trade-offs with concrete evidence, we advocate\nfor conducting experiments that compare various design alternatives. Achieving\na systematic and repeatable experiment process necessitates automation. We\npresent a proof-of-concept implementation of an experiment tool - Observability\neXperiment eNgine (OXN). OXN is able to inject arbitrary faults into an\napplication, similar to Chaos Engineering, but also possesses the unique\ncapability to modify the observability configuration, allowing for the\nstraightforward assessment of design decisions that were previously left\nunexplored.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}