{"id":"2408.13909","title":"LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages\n  in Multimodal Image Retrieval Task","authors":"Ali Asgarov, Samir Rustamov","authorsParsed":[["Asgarov","Ali",""],["Rustamov","Samir",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 18:10:16 GMT"}],"updateDate":"2024-08-27","timestamp":1724609416000,"abstract":"  This research explores the development of multimodal vision-language models\nfor image retrieval in low-resource languages, specifically Azerbaijani.\nExisting vision-language models primarily support high-resource languages, and\nfine-tuning them remains computationally demanding. To address challenges in\nvision-language retrieval for low-resource languages, we integrated the CLIP\nmodel architecture and employed several techniques to balance computational\nefficiency with performance. These techniques include synthetic data generation\nthrough machine translation, image augmentation, and further training the\nattention mechanisms of transformer-based models with domain-specific data. We\nintegrated Multilingual BERT as a text encoder with image encoders like\nResNet50, EfficientNet0, Vision Transformer (ViT), and Tiny Swin Transformer.\nOur study found that models like EfficientNet0 and Tiny Swin Transformer\nperform best on the datasets they were trained on, such as COCO, Flickr30k, and\nFlickr8k. Augmentation techniques boosted EfficientNet0 MAP on Flickr30k from\n0.84 to 0.87 and ResNet50 MAP on MSCOCO from 0.70 to 0.80, contributing to a\nnew state of the art in vision-language retrieval. We share our configurations\nand results to support further research. Code and pre-trained models are\navailable at https://github.com/aliasgerovs/azclip.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}