{"id":"2408.04154","title":"The Data Addition Dilemma","authors":"Judy Hanwen Shen, Inioluwa Deborah Raji, Irene Y. Chen","authorsParsed":[["Shen","Judy Hanwen",""],["Raji","Inioluwa Deborah",""],["Chen","Irene Y.",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 01:42:31 GMT"}],"updateDate":"2024-08-09","timestamp":1723081351000,"abstract":"  In many machine learning for healthcare tasks, standard datasets are\nconstructed by amassing data across many, often fundamentally dissimilar,\nsources. But when does adding more data help, and when does it hinder progress\non desired model outcomes in real-world settings? We identify this situation as\nthe \\textit{Data Addition Dilemma}, demonstrating that adding training data in\nthis multi-source scaling context can at times result in reduced overall\naccuracy, uncertain fairness outcomes, and reduced worst-subgroup performance.\nWe find that this possibly arises from an empirically observed trade-off\nbetween model performance improvements due to data scaling and model\ndeterioration from distribution shift. We thus establish baseline strategies\nfor navigating this dilemma, introducing distribution shift heuristics to guide\ndecision-making on which data sources to add in data scaling, in order to yield\nthe expected model performance improvements. We conclude with a discussion of\nthe required considerations for data collection and suggestions for studying\ndata composition and scale in the age of increasingly larger models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"x0jOocoqOaK-dbFKJAOJgXWhK7aUuAlmK-UimxrhTxU","pdfSize":"1746004"}
