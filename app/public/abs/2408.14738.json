{"id":"2408.14738","title":"Learning Differentially Private Diffusion Models via Stochastic\n  Adversarial Distillation","authors":"Bochao Liu, Pengju Wang and Shiming Ge","authorsParsed":[["Liu","Bochao",""],["Wang","Pengju",""],["Ge","Shiming",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 02:29:29 GMT"}],"updateDate":"2024-08-28","timestamp":1724725769000,"abstract":"  While the success of deep learning relies on large amounts of training\ndatasets, data is often limited in privacy-sensitive domains. To address this\nchallenge, generative model learning with differential privacy has emerged as a\nsolution to train private generative models for desensitized data generation.\nHowever, the quality of the images generated by existing methods is limited due\nto the complexity of modeling data distribution. We build on the success of\ndiffusion models and introduce DP-SAD, which trains a private diffusion model\nby a stochastic adversarial distillation method. Specifically, we first train a\ndiffusion model as a teacher and then train a student by distillation, in which\nwe achieve differential privacy by adding noise to the gradients from other\nmodels to the student. For better generation quality, we introduce a\ndiscriminator to distinguish whether an image is from the teacher or the\nstudent, which forms the adversarial training. Extensive experiments and\nanalysis clearly demonstrate the effectiveness of our proposed method.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}