{"id":"2407.15703","title":"Estimating Probability Densities with Transformer and Denoising\n  Diffusion","authors":"Henry W. Leung, Jo Bovy, Joshua S. Speagle","authorsParsed":[["Leung","Henry W.",""],["Bovy","Jo",""],["Speagle","Joshua S.",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 15:10:41 GMT"}],"updateDate":"2024-07-23","timestamp":1721661041000,"abstract":"  Transformers are often the go-to architecture to build foundation models that\ningest a large amount of training data. But these models do not estimate the\nprobability density distribution when trained on regression problems, yet\nobtaining full probabilistic outputs is crucial to many fields of science,\nwhere the probability distribution of the answer can be non-Gaussian and\nmultimodal. In this work, we demonstrate that training a probabilistic model\nusing a denoising diffusion head on top of the Transformer provides reasonable\nprobability density estimation even for high-dimensional inputs. The combined\nTransformer+Denoising Diffusion model allows conditioning the output\nprobability density on arbitrary combinations of inputs and it is thus a highly\nflexible density function emulator of all possible input/output combinations.\nWe illustrate our Transformer+Denoising Diffusion model by training it on a\nlarge dataset of astronomical observations and measured labels of stars within\nour Galaxy and we apply it to a variety of inference tasks to show that the\nmodel can infer labels accurately with reasonable distributions.\n","subjects":["Computing Research Repository/Machine Learning","Astrophysics/Instrumentation and Methods for Astrophysics","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}