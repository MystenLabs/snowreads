{"id":"2407.14655","title":"LORTSAR: Low-Rank Transformer for Skeleton-based Action Recognition","authors":"Soroush Oraki, Harry Zhuang, Jie Liang","authorsParsed":[["Oraki","Soroush",""],["Zhuang","Harry",""],["Liang","Jie",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 20:19:41 GMT"}],"updateDate":"2024-07-23","timestamp":1721420381000,"abstract":"  The complexity of state-of-the-art Transformer-based models for\nskeleton-based action recognition poses significant challenges in terms of\ncomputational efficiency and resource utilization. In this paper, we explore\nthe application of Singular Value Decomposition (SVD) to effectively reduce the\nmodel sizes of these pre-trained models, aiming to minimize their resource\nconsumption while preserving accuracy. Our method, LORTSAR (LOw-Rank\nTransformer for Skeleton-based Action Recognition), also includes a fine-tuning\nstep to compensate for any potential accuracy degradation caused by model\ncompression, and is applied to two leading Transformer-based models,\n\"Hyperformer\" and \"STEP-CATFormer\". Experimental results on the \"NTU RGB+D\" and\n\"NTU RGB+D 120\" datasets show that our method can reduce the number of model\nparameters substantially with negligible degradation or even performance\nincrease in recognition accuracy. This confirms that SVD combined with\npost-compression fine-tuning can boost model efficiency, paving the way for\nmore sustainable, lightweight, and high-performance technologies in human\naction recognition.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}