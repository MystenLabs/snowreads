{"id":"2408.01654","title":"Deep Patch Visual SLAM","authors":"Lahav Lipson, Zachary Teed, Jia Deng","authorsParsed":[["Lipson","Lahav",""],["Teed","Zachary",""],["Deng","Jia",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 03:51:47 GMT"}],"updateDate":"2024-08-06","timestamp":1722657107000,"abstract":"  Recent work in visual SLAM has shown the effectiveness of using deep network\nbackbones. Despite excellent accuracy, however, such approaches are often\nexpensive to run or do not generalize well zero-shot. Their runtime can also\nfluctuate wildly while their frontend and backend fight for access to GPU\nresources. To address these problems, we introduce Deep Patch Visual (DPV)\nSLAM, a method for monocular visual SLAM on a single GPU. DPV-SLAM maintains a\nhigh minimum framerate and small memory overhead (5-7G) compared to existing\ndeep SLAM systems. On real-world datasets, DPV-SLAM runs at 1x-4x real-time\nframerates. We achieve comparable accuracy to DROID-SLAM on EuRoC and TartanAir\nwhile running 2.5x faster using a fraction of the memory. DPV-SLAM is an\nextension to the DPVO visual odometry system; its code can be found in the same\nrepository: https://github.com/princeton-vl/DPVO\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}