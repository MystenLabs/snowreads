{"id":"2408.02451","title":"An investigation on the use of Large Language Models for hyperparameter\n  tuning in Evolutionary Algorithms","authors":"Leonardo Lucio Custode, Fabio Caraffini, Anil Yaman, Giovanni Iacca","authorsParsed":[["Custode","Leonardo Lucio",""],["Caraffini","Fabio",""],["Yaman","Anil",""],["Iacca","Giovanni",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 13:20:41 GMT"}],"updateDate":"2024-08-06","timestamp":1722864041000,"abstract":"  Hyperparameter optimization is a crucial problem in Evolutionary Computation.\nIn fact, the values of the hyperparameters directly impact the trajectory taken\nby the optimization process, and their choice requires extensive reasoning by\nhuman operators. Although a variety of self-adaptive Evolutionary Algorithms\nhave been proposed in the literature, no definitive solution has been found. In\nthis work, we perform a preliminary investigation to automate the reasoning\nprocess that leads to the choice of hyperparameter values. We employ two\nopen-source Large Language Models (LLMs), namely Llama2-70b and Mixtral, to\nanalyze the optimization logs online and provide novel real-time hyperparameter\nrecommendations. We study our approach in the context of step-size adaptation\nfor (1+1)-ES. The results suggest that LLMs can be an effective method for\noptimizing hyperparameters in Evolution Strategies, encouraging further\nresearch in this direction.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}