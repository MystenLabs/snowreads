{"id":"2408.11743","title":"MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large\n  Language Models","authors":"Elias Frantar, Roberto L. Castro, Jiale Chen, Torsten Hoefler, Dan\n  Alistarh","authorsParsed":[["Frantar","Elias",""],["Castro","Roberto L.",""],["Chen","Jiale",""],["Hoefler","Torsten",""],["Alistarh","Dan",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 16:10:41 GMT"}],"updateDate":"2024-08-22","timestamp":1724256641000,"abstract":"  As inference on Large Language Models (LLMs) emerges as an important workload\nin machine learning applications, weight quantization has become a standard\ntechnique for efficient GPU deployment. Quantization not only reduces model\nsize, but has also been shown to yield substantial speedups for single-user\ninference, due to reduced memory movement, with low accuracy impact. Yet, it\nremains open whether speedups are achievable also in \\emph{batched} settings\nwith multiple parallel clients, which are highly relevant for practical\nserving. It is unclear whether GPU kernels can be designed to remain\npractically memory-bound, while supporting the substantially increased compute\nrequirements of batched workloads.\n  This paper resolves this question positively by describing the design of\nMixed-precision Auto-Regressive LINear kernels, called MARLIN. Concretely,\ngiven a model whose weights are compressed via quantization to, e.g., 4 bits\nper element, MARLIN shows that batchsizes up to 16-32 can be supported with\nclose to maximum ($4\\times$) quantization speedup, and larger batchsizes up to\n64-128 with gradually decreasing, but still significant, acceleration. MARLIN\naccomplishes this via a combination of techniques, such as asynchronous memory\naccess, complex task scheduling and pipelining, and bespoke quantization\nsupport. Our experiments show that MARLIN's near-optimal performance on\nindividual LLM layers across different scenarios can also lead to end-to-end\nLLM inference speedups (of up to $2.8\\times$) when integrated with the popular\nvLLM serving engine. Finally, MARLIN is extensible to further compression\ntechniques, like NVIDIA 2:4 sparsity, leading to additional speedups.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KXmJrPdcWEc7_SPnJ9tpLA8QnOAAJktXhLdJEdATujo","pdfSize":"890112"}
