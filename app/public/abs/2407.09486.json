{"id":"2407.09486","title":"ENOVA: Autoscaling towards Cost-effective and Stable Serverless LLM\n  Serving","authors":"Tao Huang, Pengfei Chen, Kyoka Gong, Jocky Hawk, Zachary Bright,\n  Wenxin Xie, Kecheng Huang, Zhi Ji","authorsParsed":[["Huang","Tao",""],["Chen","Pengfei",""],["Gong","Kyoka",""],["Hawk","Jocky",""],["Bright","Zachary",""],["Xie","Wenxin",""],["Huang","Kecheng",""],["Ji","Zhi",""]],"versions":[{"version":"v1","created":"Fri, 17 May 2024 09:48:31 GMT"}],"updateDate":"2024-07-16","timestamp":1715939311000,"abstract":"  Since the increasing popularity of large language model (LLM) backend\nsystems, it is common and necessary to deploy stable serverless serving of LLM\non multi-GPU clusters with autoscaling. However, there exist challenges because\nthe diversity and co-location of applications in multi-GPU clusters will lead\nto low service quality and GPU utilization. To address them, we build ENOVA, a\ndeployment, monitoring and autoscaling service towards serverless LLM serving.\nENOVA deconstructs the execution process of LLM service comprehensively, based\non which ENOVA designs a configuration recommendation module for automatic\ndeployment on any GPU clusters and a performance detection module for\nautoscaling. On top of them, ENOVA implements a deployment execution engine for\nmulti-GPU cluster scheduling. The experiment results show that ENOVA\nsignificantly outperforms other state-of-the-art methods and is suitable for\nwide deployment in large online systems.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}