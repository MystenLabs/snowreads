{"id":"2407.16026","title":"KWT-Tiny: RISC-V Accelerated, Embedded Keyword Spotting Transformer","authors":"Aness Al-Qawlaq, Ajay Kumar M, Deepu John","authorsParsed":[["Al-Qawlaq","Aness",""],["M","Ajay Kumar",""],["John","Deepu",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 20:07:21 GMT"}],"updateDate":"2024-07-24","timestamp":1721678841000,"abstract":"  This paper explores the adaptation of Transformerbased models for edge\ndevices through the quantisation and hardware acceleration of the ARM Keyword\nTransformer (KWT) model on a RISC-V platform. The model was targeted to run on\n64kB RAM in bare-metal C using a custom-developed edge AI library. KWT-1 was\nretrained to be 369 times smaller, with only a 10% loss in accuracy through\nreducing output classes from 35 to 2. The retraining and quantisation reduced\nmodel size from 2.42 MB to 1.65 kB. The integration of custom RISC-V\ninstructions that accelerated GELU and SoftMax operations enabled a 5x speedup\nand thus ~5x power reduction in inference, with inference clock cycle counts\ndecreasing from 26 million to 5.5 million clock cycles while incurring a small\narea overhead of approximately 29%. The results demonstrate a viable method for\nporting and accelerating Transformer-based models in low-power IoT devices.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Performance"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"6hIzvVSJRPxaLpBYiU6-IE5iNav3hncEKTrkQXKHHTg","pdfSize":"618861","objectId":"0xddb42d06a709d73ddf20125698b3e27743a2275d781f58f41db23d80a491d37e","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
