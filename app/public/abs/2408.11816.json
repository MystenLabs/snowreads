{"id":"2408.11816","title":"Efficient Exploration and Discriminative World Model Learning with an\n  Object-Centric Abstraction","authors":"Anthony GX-Chen, Kenneth Marino, Rob Fergus","authorsParsed":[["GX-Chen","Anthony",""],["Marino","Kenneth",""],["Fergus","Rob",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:59:31 GMT"}],"updateDate":"2024-08-22","timestamp":1724263171000,"abstract":"  In the face of difficult exploration problems in reinforcement learning, we\nstudy whether giving an agent an object-centric mapping (describing a set of\nitems and their attributes) allow for more efficient learning. We found this\nproblem is best solved hierarchically by modelling items at a higher level of\nstate abstraction to pixels, and attribute change at a higher level of temporal\nabstraction to primitive actions. This abstraction simplifies the transition\ndynamic by making specific future states easier to predict. We make use of this\nto propose a fully model-based algorithm that learns a discriminative world\nmodel, plans to explore efficiently with only a count-based intrinsic reward,\nand can subsequently plan to reach any discovered (abstract) states.\n  We demonstrate the model's ability to (i) efficiently solve single tasks,\n(ii) transfer zero-shot and few-shot across item types and environments, and\n(iii) plan across long horizons. Across a suite of 2D crafting and MiniHack\nenvironments, we empirically show our model significantly out-performs\nstate-of-the-art low-level methods (without abstraction), as well as performant\nmodel-free and model-based methods using the same abstraction. Finally, we show\nhow to reinforce learn low level object-perturbing policies, as well as\nsupervise learn the object mapping itself.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}