{"id":"2408.00838","title":"Calibrating Bayesian Generative Machine Learning for Bayesiamplification","authors":"Sebastian Bieringer, Sascha Diefenbacher, Gregor Kasieczka, Mathias\n  Trabs","authorsParsed":[["Bieringer","Sebastian",""],["Diefenbacher","Sascha",""],["Kasieczka","Gregor",""],["Trabs","Mathias",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 18:00:05 GMT"}],"updateDate":"2024-08-05","timestamp":1722535205000,"abstract":"  Recently, combinations of generative and Bayesian machine learning have been\nintroduced in particle physics for both fast detector simulation and inference\ntasks. These neural networks aim to quantify the uncertainty on the generated\ndistribution originating from limited training statistics. The interpretation\nof a distribution-wide uncertainty however remains ill-defined. We show a clear\nscheme for quantifying the calibration of Bayesian generative machine learning\nmodels. For a Continuous Normalizing Flow applied to a low-dimensional toy\nexample, we evaluate the calibration of Bayesian uncertainties from either a\nmean-field Gaussian weight posterior, or Monte Carlo sampling network weights,\nto gauge their behaviour on unsteady distribution edges. Well calibrated\nuncertainties can then be used to roughly estimate the number of uncorrelated\ntruth samples that are equivalent to the generated sample and clearly indicate\ndata amplification for smooth features of the distribution.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Physics/High Energy Physics - Phenomenology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}