{"id":"2407.11030","title":"DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMs","authors":"Zhen Tan, Daize Dong, Xinyu Zhao, Jie Peng, Yu Cheng and Tianlong Chen","authorsParsed":[["Tan","Zhen",""],["Dong","Daize",""],["Zhao","Xinyu",""],["Peng","Jie",""],["Cheng","Yu",""],["Chen","Tianlong",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 18:34:08 GMT"}],"updateDate":"2024-07-17","timestamp":1720031648000,"abstract":"  In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach\nfor vertically scaling transformer-based Large Language Models (LLMs) by\ndynamically expanding, activating, or skipping layers using a sophisticated\nrouting policy based on layerwise feature similarity. Unlike traditional\nMixture-of-Experts (MoE) methods that focus on extending the model width, our\napproach targets model depth, addressing the redundancy observed across layer\nrepresentations for various input samples. Our framework is integrated with the\nSupervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive\nContinual Pre-Training (CPT). Experimental results demonstrate that DLO not\nonly outperforms the original unscaled models but also achieves comparable\nresults to densely expanded models with significantly improved efficiency. Our\nwork offers a promising direction for building efficient yet powerful LLMs. We\nwill release our implementation and model weights upon acceptance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}