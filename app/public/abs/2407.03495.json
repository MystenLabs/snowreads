{"id":"2407.03495","title":"Codec-ASR: Training Performant Automatic Speech Recognition Systems with\n  Discrete Speech Representations","authors":"Kunal Dhawan, Nithin Rao Koluguri, Ante Juki\\'c, Ryan Langman,\n  Jagadeesh Balam, Boris Ginsburg","authorsParsed":[["Dhawan","Kunal",""],["Koluguri","Nithin Rao",""],["JukiÄ‡","Ante",""],["Langman","Ryan",""],["Balam","Jagadeesh",""],["Ginsburg","Boris",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 20:51:41 GMT"}],"updateDate":"2024-07-08","timestamp":1720039901000,"abstract":"  Discrete speech representations have garnered recent attention for their\nefficacy in training transformer-based models for various speech-related tasks\nsuch as automatic speech recognition (ASR), translation, speaker verification,\nand joint speech-text foundational models. In this work, we present a\ncomprehensive analysis on building ASR systems with discrete codes. We\ninvestigate different methods for codec training such as quantization schemes\nand time-domain vs spectral feature encodings. We further explore ASR training\ntechniques aimed at enhancing performance, training efficiency, and noise\nrobustness. Drawing upon our findings, we introduce a codec ASR pipeline that\noutperforms Encodec at similar bit-rate. Remarkably, it also surpasses the\nstate-of-the-art results achieved by strong self-supervised models on the 143\nlanguages ML-SUPERB benchmark despite being smaller in size and pretrained on\nsignificantly less data.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}