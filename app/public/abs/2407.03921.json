{"id":"2407.03921","title":"Concept Bottleneck Models Without Predefined Concepts","authors":"Simon Schrodi, Julian Schur, Max Argus, Thomas Brox","authorsParsed":[["Schrodi","Simon",""],["Schur","Julian",""],["Argus","Max",""],["Brox","Thomas",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 13:34:50 GMT"}],"updateDate":"2024-07-08","timestamp":1720100090000,"abstract":"  There has been considerable recent interest in interpretable concept-based\nmodels such as Concept Bottleneck Models (CBMs), which first predict\nhuman-interpretable concepts and then map them to output classes. To reduce\nreliance on human-annotated concepts, recent works have converted pretrained\nblack-box models into interpretable CBMs post-hoc. However, these approaches\npredefine a set of concepts, assuming which concepts a black-box model encodes\nin its representations. In this work, we eliminate this assumption by\nleveraging unsupervised concept discovery to automatically extract concepts\nwithout human annotations or a predefined set of concepts. We further introduce\nan input-dependent concept selection mechanism that ensures only a small subset\nof concepts is used across all classes. We show that our approach improves\ndownstream performance and narrows the performance gap to black-box models,\nwhile using significantly fewer concepts in the classification. Finally, we\ndemonstrate how large vision-language models can intervene on the final model\nweights to correct model errors.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}