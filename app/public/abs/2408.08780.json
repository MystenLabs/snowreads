{"id":"2408.08780","title":"Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions","authors":"Chenming Tang, Zhixiang Wang and Yunfang Wu","authorsParsed":[["Tang","Chenming",""],["Wang","Zhixiang",""],["Wu","Yunfang",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 14:49:04 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 09:01:09 GMT"},{"version":"v3","created":"Thu, 22 Aug 2024 02:52:28 GMT"}],"updateDate":"2024-08-23","timestamp":1723819744000,"abstract":"  With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}