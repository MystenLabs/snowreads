{"id":"2407.04219","title":"Semi-supervised Learning for Code-Switching ASR with Large Language\n  Model Filter","authors":"Yu Xi, Wen Ding, Kai Yu, Junjie Lai","authorsParsed":[["Xi","Yu",""],["Ding","Wen",""],["Yu","Kai",""],["Lai","Junjie",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 02:14:28 GMT"}],"updateDate":"2024-07-08","timestamp":1720145668000,"abstract":"  Code-switching (CS) phenomenon occurs when words or phrases from different\nlanguages are alternated in a single sentence. Due to data scarcity, building\nan effective CS Automatic Speech Recognition (ASR) system remains challenging.\nIn this paper, we propose to enhance CS-ASR systems by utilizing rich\nunsupervised monolingual speech data within a semi-supervised learning\nframework, particularly when access to CS data is limited. To achieve this, we\nestablish a general paradigm for applying noisy student training (NST) to the\nCS-ASR task. Specifically, we introduce the LLM-Filter, which leverages\nwell-designed prompt templates to activate the correction capability of large\nlanguage models (LLMs) for monolingual data selection and pseudo-labels\nrefinement during NST. Our experiments on the supervised ASRU-CS and\nunsupervised AISHELL-2 and LibriSpeech datasets show that our method not only\nachieves significant improvements over supervised and semi-supervised learning\nbaselines for the CS task, but also attains better performance compared with\nthe fully-supervised oracle upper-bound on the CS English part. Additionally,\nwe further investigate the influence of accent on AESRC dataset and demonstrate\nthat our method can get achieve additional benefits when the monolingual data\ncontains relevant linguistic characteristic.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}