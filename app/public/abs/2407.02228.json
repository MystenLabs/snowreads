{"id":"2407.02228","title":"MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based\n  Decoders","authors":"Baijiong Lin, Weisen Jiang, Pengguang Chen, Yu Zhang, Shu Liu, and\n  Ying-Cong Chen","authorsParsed":[["Lin","Baijiong",""],["Jiang","Weisen",""],["Chen","Pengguang",""],["Zhang","Yu",""],["Liu","Shu",""],["Chen","Ying-Cong",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 12:52:18 GMT"},{"version":"v2","created":"Sun, 14 Jul 2024 07:50:04 GMT"}],"updateDate":"2024-07-16","timestamp":1719924738000,"abstract":"  Multi-task dense scene understanding, which learns a model for multiple dense\nprediction tasks, has a wide range of application scenarios. Modeling\nlong-range dependency and enhancing cross-task interactions are crucial to\nmulti-task dense prediction. In this paper, we propose MTMamba, a novel\nMamba-based architecture for multi-task scene understanding. It contains two\ntypes of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM)\nblock. STM handles long-range dependency by leveraging Mamba, while CTM\nexplicitly models task interactions to facilitate information exchange across\ntasks. Experiments on NYUDv2 and PASCAL-Context datasets demonstrate the\nsuperior performance of MTMamba over Transformer-based and CNN-based methods.\nNotably, on the PASCAL-Context dataset, MTMamba achieves improvements of +2.08,\n+5.01, and +4.90 over the previous best methods in the tasks of semantic\nsegmentation, human parsing, and object boundary detection, respectively. The\ncode is available at https://github.com/EnVision-Research/MTMamba.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}