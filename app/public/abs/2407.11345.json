{"id":"2407.11345","title":"Beyond Binary: Multiclass Paraphasia Detection with Generative\n  Pretrained Transformers and End-to-End Models","authors":"Matthew Perez, Aneesha Sampath, Minxue Niu, Emily Mower Provost","authorsParsed":[["Perez","Matthew",""],["Sampath","Aneesha",""],["Niu","Minxue",""],["Provost","Emily Mower",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 03:24:51 GMT"}],"updateDate":"2024-07-17","timestamp":1721100291000,"abstract":"  Aphasia is a language disorder that can lead to speech errors known as\nparaphasias, which involve the misuse, substitution, or invention of words.\nAutomatic paraphasia detection can help those with Aphasia by facilitating\nclinical assessment and treatment planning options. However, most automatic\nparaphasia detection works have focused solely on binary detection, which\ninvolves recognizing only the presence or absence of a paraphasia. Multiclass\nparaphasia detection represents an unexplored area of research that focuses on\nidentifying multiple types of paraphasias and where they occur in a given\nspeech segment. We present novel approaches that use a generative pretrained\ntransformer (GPT) to identify paraphasias from transcripts as well as two\nend-to-end approaches that focus on modeling both automatic speech recognition\n(ASR) and paraphasia classification as multiple sequences vs. a single\nsequence. We demonstrate that a single sequence model outperforms GPT baselines\nfor multiclass paraphasia detection.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}