{"id":"2408.14677","title":"Can Optimization Trajectories Explain Multi-Task Transfer?","authors":"David Mueller, Mark Dredze, Nicholas Andrews","authorsParsed":[["Mueller","David",""],["Dredze","Mark",""],["Andrews","Nicholas",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 22:57:01 GMT"}],"updateDate":"2024-08-28","timestamp":1724713021000,"abstract":"  Despite the widespread adoption of multi-task training in deep learning,\nlittle is understood about how multi-task learning (MTL) affects\ngeneralization. Prior work has conjectured that the negative effects of MTL are\ndue to optimization challenges that arise during training, and many\noptimization methods have been proposed to improve multi-task performance.\nHowever, recent work has shown that these methods fail to consistently improve\nmulti-task generalization. In this work, we seek to improve our understanding\nof these failures by empirically studying how MTL impacts the optimization of\ntasks, and whether this impact can explain the effects of MTL on\ngeneralization. We show that MTL results in a generalization gap-a gap in\ngeneralization at comparable training loss-between single-task and multi-task\ntrajectories early into training. However, we find that factors of the\noptimization trajectory previously proposed to explain generalization gaps in\nsingle-task settings cannot explain the generalization gaps between single-task\nand multi-task models. Moreover, we show that the amount of gradient conflict\nbetween tasks is correlated with negative effects to task optimization, but is\nnot predictive of generalization. Our work sheds light on the underlying causes\nfor failures in MTL and, importantly, raises questions about the role of\ngeneral purpose multi-task optimization algorithms.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}