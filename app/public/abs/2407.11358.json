{"id":"2407.11358","title":"SES: Bridging the Gap Between Explainability and Prediction of Graph\n  Neural Networks","authors":"Zhenhua Huang, Kunhao Li, Shaojie Wang, Zhaohong Jia, Wentao Zhu,\n  Sharad Mehrotra","authorsParsed":[["Huang","Zhenhua",""],["Li","Kunhao",""],["Wang","Shaojie",""],["Jia","Zhaohong",""],["Zhu","Wentao",""],["Mehrotra","Sharad",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 03:46:57 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 04:20:12 GMT"}],"updateDate":"2024-07-26","timestamp":1721101617000,"abstract":"  Despite the Graph Neural Networks' (GNNs) proficiency in analyzing graph\ndata, achieving high-accuracy and interpretable predictions remains\nchallenging. Existing GNN interpreters typically provide post-hoc explanations\ndisjointed from GNNs' predictions, resulting in misrepresentations.\nSelf-explainable GNNs offer built-in explanations during the training process.\nHowever, they cannot exploit the explanatory outcomes to augment prediction\nperformance, and they fail to provide high-quality explanations of node\nfeatures and require additional processes to generate explainable subgraphs,\nwhich is costly. To address the aforementioned limitations, we propose a\nself-explained and self-supervised graph neural network (SES) to bridge the gap\nbetween explainability and prediction. SES comprises two processes: explainable\ntraining and enhanced predictive learning. During explainable training, SES\nemploys a global mask generator co-trained with a graph encoder and directly\nproduces crucial structure and feature masks, reducing time consumption and\nproviding node feature and subgraph explanations. In the enhanced predictive\nlearning phase, mask-based positive-negative pairs are constructed utilizing\nthe explanations to compute a triplet loss and enhance the node representations\nby contrastive learning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}