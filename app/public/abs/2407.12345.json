{"id":"2407.12345","title":"VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual\n  Descriptions","authors":"Seokha Moon, Hyun Woo, Hongbeen Park, Haeji Jung, Reza Mahjourian,\n  Hyung-gun Chi, Hyerin Lim, Sangpil Kim, Jinkyu Kim","authorsParsed":[["Moon","Seokha",""],["Woo","Hyun",""],["Park","Hongbeen",""],["Jung","Haeji",""],["Mahjourian","Reza",""],["Chi","Hyung-gun",""],["Lim","Hyerin",""],["Kim","Sangpil",""],["Kim","Jinkyu",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 06:39:52 GMT"}],"updateDate":"2024-07-18","timestamp":1721198392000,"abstract":"  Predicting future trajectories for other road agents is an essential task for\nautonomous vehicles. Established trajectory prediction methods primarily use\nagent tracks generated by a detection and tracking system and HD map as inputs.\nIn this work, we propose a novel method that also incorporates visual input\nfrom surround-view cameras, allowing the model to utilize visual cues such as\nhuman gazes and gestures, road conditions, vehicle turn signals, etc, which are\ntypically hidden from the model in prior methods. Furthermore, we use textual\ndescriptions generated by a Vision-Language Model (VLM) and refined by a Large\nLanguage Model (LLM) as supervision during training to guide the model on what\nto learn from the input data. Despite using these extra inputs, our method\nachieves a latency of 53 ms, making it feasible for real-time processing, which\nis significantly faster than that of previous single-agent prediction methods\nwith similar performance. Our experiments show that both the visual inputs and\nthe textual descriptions contribute to improvements in trajectory prediction\nperformance, and our qualitative analysis highlights how the model is able to\nexploit these additional inputs. Lastly, in this work we create and release the\nnuScenes-Text dataset, which augments the established nuScenes dataset with\nrich textual annotations for every scene, demonstrating the positive impact of\nutilizing VLM on trajectory prediction. Our project page is at\nhttps://moonseokha.github.io/VisionTrap/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}