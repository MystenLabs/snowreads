{"id":"2407.16560","title":"COALA: A Practical and Vision-Centric Federated Learning Platform","authors":"Weiming Zhuang, Jian Xu, Chen Chen, Jingtao Li, Lingjuan Lyu","authorsParsed":[["Zhuang","Weiming",""],["Xu","Jian",""],["Chen","Chen",""],["Li","Jingtao",""],["Lyu","Lingjuan",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 15:14:39 GMT"}],"updateDate":"2024-07-24","timestamp":1721747679000,"abstract":"  We present COALA, a vision-centric Federated Learning (FL) platform, and a\nsuite of benchmarks for practical FL scenarios, which we categorize into three\nlevels: task, data, and model. At the task level, COALA extends support from\nsimple classification to 15 computer vision tasks, including object detection,\nsegmentation, pose estimation, and more. It also facilitates federated\nmultiple-task learning, allowing clients to tackle multiple tasks\nsimultaneously. At the data level, COALA goes beyond supervised FL to benchmark\nboth semi-supervised FL and unsupervised FL. It also benchmarks feature\ndistribution shifts other than commonly considered label distribution shifts.\nIn addition to dealing with static data, it supports federated continual\nlearning for continuously changing data in real-world scenarios. At the model\nlevel, COALA benchmarks FL with split models and different models in different\nclients. COALA platform offers three degrees of customization for these\npractical FL scenarios, including configuration customization, components\ncustomization, and workflow customization. We conduct systematic benchmarking\nexperiments for the practical FL scenarios and highlight potential\nopportunities for further advancements in FL. Codes are open sourced at\nhttps://github.com/SonyResearch/COALA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}