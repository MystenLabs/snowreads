{"id":"2407.07425","title":"Out-of-distribution generalisation in spoken language understanding","authors":"Dejan Porjazovski, Anssi Moisio, Mikko Kurimo","authorsParsed":[["Porjazovski","Dejan",""],["Moisio","Anssi",""],["Kurimo","Mikko",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 07:27:38 GMT"}],"updateDate":"2024-07-11","timestamp":1720596458000,"abstract":"  Test data is said to be out-of-distribution (OOD) when it unexpectedly\ndiffers from the training data, a common challenge in real-world use cases of\nmachine learning. Although OOD generalisation has gained interest in recent\nyears, few works have focused on OOD generalisation in spoken language\nunderstanding (SLU) tasks. To facilitate research on this topic, we introduce a\nmodified version of the popular SLU dataset SLURP, featuring data splits for\ntesting OOD generalisation in the SLU task. We call our modified dataset SLURP\nFor OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find\nend-to-end SLU models to have limited capacity for generalisation. Furthermore,\nby employing model interpretability techniques, we shed light on the factors\ncontributing to the generalisation difficulties of the models. To improve the\ngeneralisation, we experiment with two techniques, which improve the results on\nsome, but not all the splits, emphasising the need for new techniques.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}