{"id":"2407.02920","title":"EgoFlowNet: Non-Rigid Scene Flow from Point Clouds with Ego-Motion\n  Support","authors":"Ramy Battrawy, Ren\\'e Schuster, Didier Stricker","authorsParsed":[["Battrawy","Ramy",""],["Schuster","Ren√©",""],["Stricker","Didier",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 08:53:50 GMT"}],"updateDate":"2024-07-04","timestamp":1719996830000,"abstract":"  Recent weakly-supervised methods for scene flow estimation from LiDAR point\nclouds are limited to explicit reasoning on object-level. These methods perform\nmultiple iterative optimizations for each rigid object, which makes them\nvulnerable to clustering robustness. In this paper, we propose our EgoFlowNet -\na point-level scene flow estimation network trained in a weakly-supervised\nmanner and without object-based abstraction. Our approach predicts a binary\nsegmentation mask that implicitly drives two parallel branches for ego-motion\nand scene flow. Unlike previous methods, we provide both branches with all\ninput points and carefully integrate the binary mask into the feature\nextraction and losses. We also use a shared cost volume with local refinement\nthat is updated at multiple scales without explicit clustering or rigidity\nassumptions. On realistic KITTI scenes, we show that our EgoFlowNet performs\nbetter than state-of-the-art methods in the presence of ground surface points.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}