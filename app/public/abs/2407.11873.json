{"id":"2407.11873","title":"Variance Norms for Kernelized Anomaly Detection","authors":"Thomas Cass, Lukas Gonon, Nikita Zozoulenko","authorsParsed":[["Cass","Thomas",""],["Gonon","Lukas",""],["Zozoulenko","Nikita",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:59:49 GMT"}],"updateDate":"2024-07-17","timestamp":1721145589000,"abstract":"  We present a unified theory for Mahalanobis-type anomaly detection on Banach\nspaces, using ideas from Cameron-Martin theory applied to non-Gaussian\nmeasures. This approach leads to a basis-free, data-driven notion of anomaly\ndistance through the so-called variance norm of a probability measure, which\ncan be consistently estimated using empirical measures. Our framework\ngeneralizes the classical $\\mathbb{R}^d$, functional $(L^2[0,1])^d$, and\nkernelized settings, including the general case of non-injective covariance\noperator. We prove that the variance norm depends solely on the inner product\nin a given Hilbert space, and hence that the kernelized Mahalanobis distance\ncan naturally be recovered by working on reproducing kernel Hilbert spaces.\n  Using the variance norm, we introduce the notion of a kernelized\nnearest-neighbour Mahalanobis distance for semi-supervised anomaly detection.\nIn an empirical study on 12 real-world datasets, we demonstrate that the\nkernelized nearest-neighbour Mahalanobis distance outperforms the traditional\nkernelized Mahalanobis distance for multivariate time series anomaly detection,\nusing state-of-the-art time series kernels such as the signature, global\nalignment, and Volterra reservoir kernels. Moreover, we provide an initial\ntheoretical justification of nearest-neighbour Mahalanobis distances by\ndeveloping concentration inequalities in the finite-dimensional Gaussian case.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Probability"],"license":"http://creativecommons.org/licenses/by/4.0/"}