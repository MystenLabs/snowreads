{"id":"2407.01034","title":"Enhancing Speech-Driven 3D Facial Animation with Audio-Visual Guidance\n  from Lip Reading Expert","authors":"Han EunGi, Oh Hyun-Bin, Kim Sung-Bin, Corentin Nivelet Etcheberry,\n  Suekyeong Nam, Janghoon Joo, Tae-Hyun Oh","authorsParsed":[["EunGi","Han",""],["Hyun-Bin","Oh",""],["Sung-Bin","Kim",""],["Etcheberry","Corentin Nivelet",""],["Nam","Suekyeong",""],["Joo","Janghoon",""],["Oh","Tae-Hyun",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 07:39:28 GMT"}],"updateDate":"2024-07-02","timestamp":1719819568000,"abstract":"  Speech-driven 3D facial animation has recently garnered attention due to its\ncost-effective usability in multimedia production. However, most current\nadvances overlook the intelligibility of lip movements, limiting the realism of\nfacial expressions. In this paper, we introduce a method for speech-driven 3D\nfacial animation to generate accurate lip movements, proposing an audio-visual\nmultimodal perceptual loss. This loss provides guidance to train the\nspeech-driven 3D facial animators to generate plausible lip motions aligned\nwith the spoken transcripts. Furthermore, to incorporate the proposed\naudio-visual perceptual loss, we devise an audio-visual lip reading expert\nleveraging its prior knowledge about correlations between speech and lip\nmotions. We validate the effectiveness of our approach through broad\nexperiments, showing noticeable improvements in lip synchronization and lip\nreadability performance. Codes are available at\nhttps://3d-talking-head-avguide.github.io/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}