{"id":"2407.02320","title":"Exploring the Role of Transliteration in In-Context Learning for\n  Low-resource Languages Written in Non-Latin Scripts","authors":"Chunlan Ma, Yihong Liu, Haotian Ye and Hinrich Sch\\\"utze","authorsParsed":[["Ma","Chunlan",""],["Liu","Yihong",""],["Ye","Haotian",""],["Sch√ºtze","Hinrich",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 14:51:20 GMT"}],"updateDate":"2024-07-03","timestamp":1719931880000,"abstract":"  Decoder-only large language models (LLMs) excel in high-resource languages\nacross various tasks through few-shot or even zero-shot in-context learning\n(ICL). However, their performance often does not transfer well to low-resource\nlanguages, especially those written in non-Latin scripts. Inspired by recent\nwork that leverages transliteration in encoder-only models, we investigate\nwhether transliteration is also effective in improving LLMs' performance for\nlow-resource languages written in non-Latin scripts. To this end, we propose\nthree prompt templates, where the target-language text is represented in (1)\nits original script, (2) Latin script, or (3) both. We apply these methods to\nseveral representative LLMs of different sizes on various tasks including text\nclassification and sequential labeling. Our findings show that the\neffectiveness of transliteration varies by task type and model size. For\ninstance, all models benefit from transliterations for sequential labeling\n(with increases of up to 25%).\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}