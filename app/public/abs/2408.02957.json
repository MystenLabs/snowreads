{"id":"2408.02957","title":"Online Temporal Action Localization with Memory-Augmented Transformer","authors":"Youngkil Song, Dongkeun Kim, Minsu Cho, and Suha Kwak","authorsParsed":[["Song","Youngkil",""],["Kim","Dongkeun",""],["Cho","Minsu",""],["Kwak","Suha",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 04:55:33 GMT"}],"updateDate":"2024-08-07","timestamp":1722920133000,"abstract":"  Online temporal action localization (On-TAL) is the task of identifying\nmultiple action instances given a streaming video. Since existing methods take\nas input only a video segment of fixed size per iteration, they are limited in\nconsidering long-term context and require tuning the segment size carefully. To\novercome these limitations, we propose memory-augmented transformer (MATR).\nMATR utilizes the memory queue that selectively preserves the past segment\nfeatures, allowing to leverage long-term context for inference. We also propose\na novel action localization method that observes the current input segment to\npredict the end time of the ongoing action and accesses the memory queue to\nestimate the start time of the action. Our method outperformed existing methods\non two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the\nonline setting but also some offline TAL methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}