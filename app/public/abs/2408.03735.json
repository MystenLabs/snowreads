{"id":"2408.03735","title":"Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation","authors":"Jingjing Xie, Yuxin Zhang, Mingbao Lin, Liujuan Cao, Rongrong Ji","authorsParsed":[["Xie","Jingjing",""],["Zhang","Yuxin",""],["Lin","Mingbao",""],["Cao","Liujuan",""],["Ji","Rongrong",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 12:42:09 GMT"}],"updateDate":"2024-08-08","timestamp":1723034529000,"abstract":"  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dpls_l-HKnrcxejXcvczJ55w-r33UwRcHlbchgk35To","pdfSize":"2921487"}
