{"id":"2408.12245","title":"Scalable Autoregressive Image Generation with Mamba","authors":"Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li,\n  Guoqi Li","authorsParsed":[["Li","Haopeng",""],["Yang","Jinyue",""],["Wang","Kexin",""],["Qiu","Xuerui",""],["Chou","Yuhong",""],["Li","Xin",""],["Li","Guoqi",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 09:27:49 GMT"}],"updateDate":"2024-08-23","timestamp":1724318869000,"abstract":"  We introduce AiM, an autoregressive (AR) image generative model based on\nMamba architecture. AiM employs Mamba, a novel state-space model characterized\nby its exceptional performance for long-sequence modeling with linear time\ncomplexity, to supplant the commonly utilized Transformers in AR image\ngeneration models, aiming to achieve both superior generation quality and\nenhanced inference speed. Unlike existing methods that adapt Mamba to handle\ntwo-dimensional signals via multi-directional scan, AiM directly utilizes the\nnext-token prediction paradigm for autoregressive image generation. This\napproach circumvents the need for extensive modifications to enable Mamba to\nlearn 2D spatial representations. By implementing straightforward yet\nstrategically targeted modifications for visual generative tasks, we preserve\nMamba's core structure, fully exploiting its efficient long-sequence modeling\ncapabilities and scalability. We provide AiM models in various scales, with\nparameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256\nbenchmark, our best AiM model achieves a FID of 2.21, surpassing all existing\nAR models of comparable parameter counts and demonstrating significant\ncompetitiveness against diffusion models, with 2 to 10 times faster inference\nspeed. Code is available at https://github.com/hp-l33/AiM\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}