{"id":"2407.00029","title":"Distributed Inference Performance Optimization for LLMs on CPUs","authors":"Pujiang He, Shan Zhou, Changqing Li, Wenhuan Huang, Weifei Yu, Duyi\n  Wang, Chen Meng, Sheng Gui","authorsParsed":[["He","Pujiang",""],["Zhou","Shan",""],["Li","Changqing",""],["Huang","Wenhuan",""],["Yu","Weifei",""],["Wang","Duyi",""],["Meng","Chen",""],["Gui","Sheng",""]],"versions":[{"version":"v1","created":"Thu, 16 May 2024 08:39:37 GMT"}],"updateDate":"2024-07-02","timestamp":1715848777000,"abstract":"  Large language models (LLMs) hold tremendous potential for addressing\nnumerous real-world challenges, yet they typically demand significant\ncomputational resources and memory. Deploying LLMs onto a resource-limited\nhardware device with restricted memory capacity presents considerable\nchallenges. Distributed computing emerges as a prevalent strategy to mitigate\nsingle-node memory constraints and expedite LLM inference performance. To\nreduce the hardware limitation burden, we proposed an efficient distributed\ninference optimization solution for LLMs on CPUs. We conduct experiments with\nthe proposed solution on 5th Gen Intel Xeon Scalable Processors, and the result\nshows the time per output token for the LLM with 72B parameter is 140 ms/token,\nmuch faster than the average human reading speed about 200ms per token.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-Jz1oBXbtNc_pZ7aWhk0g16whXeJdBbq2RRtiOB3tRk","pdfSize":"275219"}
