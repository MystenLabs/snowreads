{"id":"2408.12353","title":"Distributed quasi-Newton robust estimation under differential privacy","authors":"Chuhan Wang, Lixing Zhu, and Xuehu Zhu","authorsParsed":[["Wang","Chuhan",""],["Zhu","Lixing",""],["Zhu","Xuehu",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 12:51:28 GMT"}],"updateDate":"2024-08-23","timestamp":1724331088000,"abstract":"  For distributed computing with Byzantine machines under Privacy Protection\n(PP) constraints, this paper develops a robust PP distributed quasi-Newton\nestimation, which only requires the node machines to transmit five vectors to\nthe central processor with high asymptotic relative efficiency. Compared with\nthe gradient descent strategy which requires more rounds of transmission and\nthe Newton iteration strategy which requires the entire Hessian matrix to be\ntransmitted, the novel quasi-Newton iteration has advantages in reducing\nprivacy budgeting and transmission cost. Moreover, our PP algorithm does not\ndepend on the boundedness of gradients and second-order derivatives. When\ngradients and second-order derivatives follow sub-exponential distributions, we\noffer a mechanism that can ensure PP with a sufficiently high probability.\nFurthermore, this novel estimator can achieve the optimal convergence rate and\nthe asymptotic normality. The numerical studies on synthetic and real data sets\nevaluate the performance of the proposed algorithm.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}