{"id":"2407.21331","title":"CAMAv2: A Vision-Centric Approach for Static Map Element Annotation","authors":"Shiyuan Chen, Jiaxin Zhang, Ruohong Mei, Yingfeng Cai, Haoran Yin, Tao\n  Chen, Wei Sui and Cong Yang","authorsParsed":[["Chen","Shiyuan",""],["Zhang","Jiaxin",""],["Mei","Ruohong",""],["Cai","Yingfeng",""],["Yin","Haoran",""],["Chen","Tao",""],["Sui","Wei",""],["Yang","Cong",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 04:41:49 GMT"}],"updateDate":"2024-08-01","timestamp":1722400909000,"abstract":"  The recent development of online static map element (a.k.a. HD map)\nconstruction algorithms has raised a vast demand for data with ground truth\nannotations. However, available public datasets currently cannot provide\nhigh-quality training data regarding consistency and accuracy. For instance,\nthe manual labelled (low efficiency) nuScenes still contains misalignment and\ninconsistency between the HD maps and images (e.g., around 8.03 pixels\nreprojection error on average). To this end, we present CAMAv2: a\nvision-centric approach for Consistent and Accurate Map Annotation. Without\nLiDAR inputs, our proposed framework can still generate high-quality 3D\nannotations of static map elements. Specifically, the annotation can achieve\nhigh reprojection accuracy across all surrounding cameras and is\nspatial-temporal consistent across the whole sequence. We apply our proposed\nframework to the popular nuScenes dataset to provide efficient and highly\naccurate annotations. Compared with the original nuScenes static map element,\nour CAMAv2 annotations achieve lower reprojection errors (e.g., 4.96 vs. 8.03\npixels). Models trained with annotations from CAMAv2 also achieve lower\nreprojection errors (e.g., 5.62 vs. 8.43 pixels).\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}