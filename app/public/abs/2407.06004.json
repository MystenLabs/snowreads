{"id":"2407.06004","title":"Perceptions to Beliefs: Exploring Precursory Inferences for Theory of\n  Mind in Large Language Models","authors":"Chani Jung, Dongkwan Kim, Jiho Jin, Jiseon Kim, Yeon Seonwoo, Yejin\n  Choi, Alice Oh, Hyunwoo Kim","authorsParsed":[["Jung","Chani",""],["Kim","Dongkwan",""],["Jin","Jiho",""],["Kim","Jiseon",""],["Seonwoo","Yeon",""],["Choi","Yejin",""],["Oh","Alice",""],["Kim","Hyunwoo",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 14:58:29 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 09:11:18 GMT"}],"updateDate":"2024-07-10","timestamp":1720450709000,"abstract":"  While humans naturally develop theory of mind (ToM), the capability to\nunderstand other people's mental states and beliefs, state-of-the-art large\nlanguage models (LLMs) underperform on simple ToM benchmarks. We posit that we\ncan extend our understanding of LLMs' ToM abilities by evaluating key human ToM\nprecursors -- perception inference and perception-to-belief inference -- in\nLLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate\nthese precursory inferences for ToM in LLMs by annotating characters'\nperceptions on ToMi and FANToM, respectively. Our evaluation of eight\nstate-of-the-art LLMs reveals that the models generally perform well in\nperception inference while exhibiting limited capability in\nperception-to-belief inference (e.g., lack of inhibitory control). Based on\nthese results, we present PercepToM, a novel ToM method leveraging LLMs' strong\nperception inference capability while supplementing their limited\nperception-to-belief inference. Experimental results demonstrate that PercepToM\nsignificantly enhances LLM's performance, especially in false belief scenarios.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}