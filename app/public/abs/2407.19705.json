{"id":"2407.19705","title":"CollectiveSFT: Scaling Large Language Models for Chinese Medical\n  Benchmark with Collective Instructions in Healthcare","authors":"Jingwei Zhu, Minghuan Tan, Min Yang, Ruixue Li, Hamid Alinejad-Rokny","authorsParsed":[["Zhu","Jingwei",""],["Tan","Minghuan",""],["Yang","Min",""],["Li","Ruixue",""],["Alinejad-Rokny","Hamid",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 05:00:48 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 08:23:05 GMT"}],"updateDate":"2024-07-31","timestamp":1722229248000,"abstract":"  The rapid progress in Large Language Models (LLMs) has prompted the creation\nof numerous benchmarks to evaluate their capabilities.This study focuses on the\nComprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset\ndiversity and distribution in supervised fine-tuning (SFT) may enhance LLM\nperformance.Remarkably, We successfully trained a smaller base model to achieve\nscores comparable to larger models, indicating that a diverse and\nwell-distributed dataset can optimize performance regardless of model size.This\nstudy suggests that even smaller models may reach high performance levels with\ncarefully curated and varied datasets. By integrating a wide range of\ninstructional content, our approach addresses potential issues such as data\nquality inconsistencies. Our results imply that a broader spectrum of training\ndata may enhance a model's ability to generalize and perform effectively across\ndifferent medical scenarios, highlighting the importance of dataset quality and\ndiversity in fine-tuning processes. We open-source the model for future\nresearch at https://github.com/CAS-SIAT-XinHai/CollectiveSFT\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"STBYB7_v88DGuZXuI_0MK0FNE3suZL24DDXLtwzmILk","pdfSize":"124762"}
