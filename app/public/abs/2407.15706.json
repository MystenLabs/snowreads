{"id":"2407.15706","title":"Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition","authors":"Jinfu Liu, Chen Chen, Mengyuan Liu","authorsParsed":[["Liu","Jinfu",""],["Chen","Chen",""],["Liu","Mengyuan",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 15:16:47 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 14:52:05 GMT"},{"version":"v3","created":"Thu, 25 Jul 2024 03:51:18 GMT"},{"version":"v4","created":"Tue, 30 Jul 2024 03:13:16 GMT"},{"version":"v5","created":"Tue, 6 Aug 2024 13:20:16 GMT"},{"version":"v6","created":"Thu, 15 Aug 2024 12:25:39 GMT"}],"updateDate":"2024-08-16","timestamp":1721661407000,"abstract":"  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}