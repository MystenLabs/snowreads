{"id":"2408.15491","title":"Enhancing and Accelerating Large Language Models via Instruction-Aware\n  Contextual Compression","authors":"Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, Fei Yu","authorsParsed":[["Hou","Haowen",""],["Ma","Fei",""],["Bai","Binwen",""],["Zhu","Xinxin",""],["Yu","Fei",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 02:31:15 GMT"}],"updateDate":"2024-08-29","timestamp":1724812275000,"abstract":"  Large Language Models (LLMs) have garnered widespread attention due to their\nremarkable performance across various tasks. However, to mitigate the issue of\nhallucinations, LLMs often incorporate retrieval-augmented pipeline to provide\nthem with rich external knowledge and context. Nevertheless, challenges stem\nfrom inaccurate and coarse-grained context retrieved from the retriever.\nSupplying irrelevant context to the LLMs can result in poorer responses,\nincreased inference latency, and higher costs. This paper introduces a method\ncalled Instruction-Aware Contextual Compression, which filters out less\ninformative content, thereby accelerating and enhancing the use of LLMs. The\nexperimental results demonstrate that Instruction-Aware Contextual Compression\nnotably reduces memory consumption and minimizes generation latency while\nmaintaining performance levels comparable to those achieved with the use of the\nfull context. Specifically, we achieved a 50% reduction in context-related\ncosts, resulting in a 5% reduction in inference memory usage and a 2.2-fold\nincrease in inference speed, with only a minor drop of 0.047 in Rouge-1. These\nfindings suggest that our method strikes an effective balance between\nefficiency and performance.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}