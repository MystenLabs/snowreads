{"id":"2407.12835","title":"Regurgitative Training: The Value of Real Data in Training Large\n  Language Models","authors":"Jinghui Zhang, Dandan Qiao, Mochen Yang, Qiang Wei","authorsParsed":[["Zhang","Jinghui",""],["Qiao","Dandan",""],["Yang","Mochen",""],["Wei","Qiang",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 18:42:55 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 16:50:58 GMT"}],"updateDate":"2024-07-26","timestamp":1720032175000,"abstract":"  What happens if we train a new Large Language Model (LLM) using data that are\nat least partially generated by other LLMs? The explosive success of LLMs means\nthat a substantial amount of content online will be generated by LLMs rather\nthan humans, which will inevitably enter the training datasets of\nnext-generation LLMs. We evaluate the implications of such \"regurgitative\ntraining\" on LLM performance. Through fine-tuning GPT-3.5 with data generated\neither by itself or by other LLMs in a machine translation task, we find strong\nevidence that regurgitative training clearly handicaps the performance of LLMs.\nThe same performance loss of regurgitative training is observed on transformer\nmodels that we train from scratch. We find suggestive evidence that the\nperformance disadvantage of regurgitative training can be attributed to at\nleast two mechanisms: (1) higher error rates and (2) lower lexical diversity in\nLLM-generated data as compared to real data. Based on these mechanisms, we\npropose and evaluate three different strategies to mitigate the performance\nloss of regurgitative training. First, we devise data-driven metrics to gauge\nthe quality of each LLM-generated data instance, and then carry out an ordered\ntraining process where high-quality data are added before low-quality ones.\nSecond, we combine data generated by multiple different LLMs (as an attempt to\nincrease lexical diversity). Third, we train an AI detection classifier to\ndifferentiate between LLM- and human-generated data, and include LLM-generated\ndata in the order of resemblance to human-generated data. All three strategies\ncan improve the performance of regurgitative training to some extent but are\nnot always able to fully close the gap from training with real data. Our\nresults highlight the value of real, human-generated data in training LLMs,\nwhich cannot be easily substituted by synthetic, LLM-generated data.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"v94W2FtO8etthilT8k5kNNdI7xYsdPKCh-s6L6N81GE","pdfSize":"524754"}
