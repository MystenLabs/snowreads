{"id":"2408.16195","title":"DLM-VMTL:A Double Layer Mapper for heterogeneous data video Multi-task\n  prompt learning","authors":"Zeyi Bo (1), Wuxi Sun (1), Ye Jin (1) ((1) Harbin Institute of\n  Technology)","authorsParsed":[["Bo","Zeyi",""],["Sun","Wuxi",""],["Jin","Ye",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 01:25:36 GMT"}],"updateDate":"2024-08-30","timestamp":1724894736000,"abstract":"  In recent years, the parameters of backbones of Video Understanding tasks\ncontinue to increase and even reach billion-level. Whether fine-tuning a\nspecific task on the Video Foundation Model or pre-training the model designed\nfor the specific task, incurs a lot of overhead. How to make these models play\nother values than their own tasks becomes a worthy question. Multi-Task\nLearning(MTL) makes the visual task acquire the rich shareable knowledge from\nother tasks while joint training. It is fully explored in Image Recognition\ntasks especially dense predict tasks. Nevertheless, it is rarely used in video\ndomain due to the lack of multi-labels video data. In this paper, a\nheterogenous data video multi-task prompt learning (VMTL) method is proposed to\naddress above problem. It's different from it in image domain, a Double-Layers\nMapper(DLM) is proposed to extract the shareable knowledge into visual promptS\nand align it with representation of primary task. Extensive experiments prove\nthat our DLM-VMTL performs better than baselines on 6 different video\nunderstanding tasks and 11 datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"s8TE8cQRuxWrYaiSVcGGuUqxisiY1Rb4XThAAiQtG5I","pdfSize":"934503"}
