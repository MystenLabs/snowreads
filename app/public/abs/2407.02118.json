{"id":"2407.02118","title":"Breaking Language Barriers: Cross-Lingual Continual Pre-Training at\n  Scale","authors":"Wenzhen Zheng, Wenbo Pan, Xu Xu, Libo Qin, Li Yue, Ming Zhou","authorsParsed":[["Zheng","Wenzhen",""],["Pan","Wenbo",""],["Xu","Xu",""],["Qin","Libo",""],["Yue","Li",""],["Zhou","Ming",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 10:06:41 GMT"}],"updateDate":"2024-07-03","timestamp":1719914801000,"abstract":"  In recent years, Large Language Models (LLMs) have made significant strides\ntowards Artificial General Intelligence. However, training these models from\nscratch requires substantial computational resources and vast amounts of text\ndata. In this paper, we explore an alternative approach to constructing an LLM\nfor a new language by continually pretraining (CPT) from existing pretrained\nLLMs, instead of using randomly initialized parameters. Based on parallel\nexperiments on 40 model sizes ranging from 40M to 5B parameters, we find that\n1) CPT converges faster and saves significant resources in a scalable manner;\n2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)\nwith a joint data-parameter scaling term; 3) The compute-optimal data-parameter\nallocation for CPT markedly differs based on our estimated scaling factors; 4)\nThe effectiveness of transfer at scale is influenced by training duration and\nlinguistic properties, while robust to data replaying, a method that\neffectively mitigates catastrophic forgetting in CPT. We hope our findings\nprovide deeper insights into the transferability of LLMs at scale for the\nresearch community.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"az1GuZZZBI6iLuV8zXPpkoq3kQkskeeRnhzO7xxWRzQ","pdfSize":"757920"}
