{"id":"2407.02678","title":"Reasoning in Large Language Models: A Geometric Perspective","authors":"Romain Cosentino, Sarath Shekkizhar","authorsParsed":[["Cosentino","Romain",""],["Shekkizhar","Sarath",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 21:39:53 GMT"}],"updateDate":"2024-07-04","timestamp":1719956393000,"abstract":"  The advancement of large language models (LLMs) for real-world applications\nhinges critically on enhancing their reasoning capabilities. In this work, we\nexplore the reasoning abilities of large language models (LLMs) through their\ngeometrical understanding. We establish a connection between the expressive\npower of LLMs and the density of their self-attention graphs. Our analysis\ndemonstrates that the density of these graphs defines the intrinsic dimension\nof the inputs to the MLP blocks. We demonstrate through theoretical analysis\nand toy examples that a higher intrinsic dimension implies a greater expressive\ncapacity of the LLM. We further provide empirical evidence linking this\ngeometric framework to recent advancements in methods aimed at enhancing the\nreasoning capabilities of LLMs.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}