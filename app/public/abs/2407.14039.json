{"id":"2407.14039","title":"BERTer: The Efficient One","authors":"Pradyumna Saligram, Andrew Lanpouthakoun","authorsParsed":[["Saligram","Pradyumna",""],["Lanpouthakoun","Andrew",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 05:33:09 GMT"}],"updateDate":"2024-07-22","timestamp":1721367189000,"abstract":"  We explore advanced fine-tuning techniques to boost BERT's performance in\nsentiment analysis, paraphrase detection, and semantic textual similarity. Our\napproach leverages SMART regularization to combat overfitting, improves\nhyperparameter choices, employs a cross-embedding Siamese architecture for\nimproved sentence embeddings, and introduces innovative early exiting methods.\nOur fine-tuning findings currently reveal substantial improvements in model\nefficiency and effectiveness when combining multiple fine-tuning architectures,\nachieving a state-of-the-art performance score of on the test set, surpassing\ncurrent benchmarks and highlighting BERT's adaptability in multifaceted\nlinguistic tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}