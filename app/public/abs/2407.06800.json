{"id":"2407.06800","title":"Learn and Don't Forget: Adding a New Language to ASR Foundation Models","authors":"Mengjie Qian, Siyuan Tang, Rao Ma, Kate M. Knill, Mark J. F. Gales","authorsParsed":[["Qian","Mengjie",""],["Tang","Siyuan",""],["Ma","Rao",""],["Knill","Kate M.",""],["Gales","Mark J. F.",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 12:14:48 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 13:07:06 GMT"}],"updateDate":"2024-07-22","timestamp":1720527288000,"abstract":"  Foundation ASR models often support many languages, e.g. 100 languages in\nWhisper. However, there has been limited work on integrating an additional,\ntypically low-resource, language, while maintaining performance on the original\nlanguage set. Fine-tuning, while simple, may degrade the accuracy of the\noriginal set. We compare three approaches that exploit adaptation parameters:\nsoft language code tuning, train only the language code; soft prompt tuning,\ntrain prepended tokens; and LoRA where a small set of additional parameters are\noptimised. Elastic Weight Consolidation (EWC) offers an alternative compromise\nwith the potential to maintain performance in specific target languages.\nResults show that direct fine-tuning yields the best performance for the new\nlanguage but degrades existing language capabilities. EWC can address this\nissue for specific languages. If only adaptation parameters are used, the\nlanguage capabilities are maintained but at the cost of performance in the new\nlanguage.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}