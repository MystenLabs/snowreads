{"id":"2407.03250","title":"When big data actually are low-rank, or entrywise approximation of\n  certain function-generated matrices","authors":"Stanislav Budzinskiy","authorsParsed":[["Budzinskiy","Stanislav",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 16:29:47 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 10:56:45 GMT"},{"version":"v3","created":"Fri, 6 Sep 2024 11:56:16 GMT"}],"updateDate":"2024-09-09","timestamp":1720024187000,"abstract":"  The article concerns low-rank approximation of matrices generated by sampling\na smooth function of two $m$-dimensional variables. We refute an argument made\nin the literature to prove that, for a specific class of analytic functions,\nsuch matrices admit accurate entrywise approximation of rank that is\nindependent of $m$ -- a claim known as \"big-data matrices are approximately\nlow-rank\". We provide a theoretical explanation of the numerical results\npresented in support of this claim, describing three narrower classes of\nfunctions for which $n \\times n$ function-generated matrices can be\napproximated within an entrywise error of order $\\varepsilon$ with rank\n$\\mathcal{O}(\\log(n) \\varepsilon^{-2} \\mathrm{polylog}(\\varepsilon^{-1}))$ that\nis independent of the dimension $m$: (i) functions of the inner product of the\ntwo variables, (ii) functions of the Euclidean distance between the variables,\nand (iii) shift-invariant positive-definite kernels. We extend our argument to\ntensor-train approximation of tensors generated with functions of the\nmulti-linear product of their $m$-dimensional variables. We discuss our results\nin the context of low-rank approximation of (a) growing datasets and (b)\nattention in transformer neural networks.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"reGzfO2cc7FRAQOBjMR8ZcK67i6mhQPqIVobGdoFucM","pdfSize":"1004359"}
