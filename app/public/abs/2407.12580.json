{"id":"2407.12580","title":"E5-V: Universal Embeddings with Multimodal Large Language Models","authors":"Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng,\n  Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang","authorsParsed":[["Jiang","Ting",""],["Song","Minghui",""],["Zhang","Zihan",""],["Huang","Haizhen",""],["Deng","Weiwei",""],["Sun","Feng",""],["Zhang","Qi",""],["Wang","Deqing",""],["Zhuang","Fuzhen",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 14:04:12 GMT"}],"updateDate":"2024-07-18","timestamp":1721225052000,"abstract":"  Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}