{"id":"2408.09053","title":"Learning to Route for Dynamic Adapter Composition in Continual Learning\n  with Language Models","authors":"Vladimir Araujo, Marie-Francine Moens, Tinne Tuytelaars","authorsParsed":[["Araujo","Vladimir",""],["Moens","Marie-Francine",""],["Tuytelaars","Tinne",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 23:57:29 GMT"}],"updateDate":"2024-08-20","timestamp":1723852649000,"abstract":"  Parameter-efficient fine-tuning (PEFT) methods are increasingly used with\npre-trained language models (PLMs) for continual learning (CL). These methods\ninvolve training a PEFT module for each new task and using similarity-based\nselection to route modules during inference. However, they face two major\nlimitations: 1) interference with already learned modules and 2) suboptimal\nrouting when composing modules. In this paper, we introduce a method that\nisolates the training of PEFT modules for task specialization. Then, before\nevaluation, it learns to compose the previously learned modules by training a\nrouter that leverages samples from a small memory. We evaluate our method in\ntwo CL setups using several benchmarks. Our results show that our method\nprovides a better composition of PEFT modules, leading to better generalization\nand performance compared to previous methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}