{"id":"2407.04208","title":"AMD: Automatic Multi-step Distillation of Large-scale Vision Models","authors":"Cheng Han and Qifan Wang and Sohail A. Dianat and Majid Rabbani and\n  Raghuveer M. Rao and Yi Fang and Qiang Guan and Lifu Huang and Dongfang Liu","authorsParsed":[["Han","Cheng",""],["Wang","Qifan",""],["Dianat","Sohail A.",""],["Rabbani","Majid",""],["Rao","Raghuveer M.",""],["Fang","Yi",""],["Guan","Qiang",""],["Huang","Lifu",""],["Liu","Dongfang",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 01:35:42 GMT"}],"updateDate":"2024-07-08","timestamp":1720143342000,"abstract":"  Transformer-based architectures have become the de-facto standard models for\ndiverse vision tasks owing to their superior performance. As the size of the\nmodels continues to scale up, model distillation becomes extremely important in\nvarious real applications, particularly on devices limited by computational\nresources. However, prevailing knowledge distillation methods exhibit\ndiminished efficacy when confronted with a large capacity gap between the\nteacher and the student, e.g, 10x compression rate. In this paper, we present a\nnovel approach named Automatic Multi-step Distillation (AMD) for large-scale\nvision model compression. In particular, our distillation process unfolds\nacross multiple steps. Initially, the teacher undergoes distillation to form an\nintermediate teacher-assistant model, which is subsequently distilled further\nto the student. An efficient and effective optimization framework is introduced\nto automatically identify the optimal teacher-assistant that leads to the\nmaximal student performance. We conduct extensive experiments on multiple image\nclassification datasets, including CIFAR-10, CIFAR-100, and ImageNet. The\nfindings consistently reveal that our approach outperforms several established\nbaselines, paving a path for future knowledge distillation methods on\nlarge-scale vision models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}