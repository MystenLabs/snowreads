{"id":"2408.05918","title":"PAFormer: Part Aware Transformer for Person Re-identification","authors":"Hyeono Jung, Jangwon Lee, Jiwon Yoo, Dami Ko, Gyeonghwan Kim","authorsParsed":[["Jung","Hyeono",""],["Lee","Jangwon",""],["Yoo","Jiwon",""],["Ko","Dami",""],["Kim","Gyeonghwan",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 04:46:55 GMT"}],"updateDate":"2024-08-13","timestamp":1723438015000,"abstract":"  Within the domain of person re-identification (ReID), partial ReID methods\nare considered mainstream, aiming to measure feature distances through\ncomparisons of body parts between samples. However, in practice, previous\nmethods often lack sufficient awareness of anatomical aspect of body parts,\nresulting in the failure to capture features of the same body parts across\ndifferent samples. To address this issue, we introduce \\textbf{Part Aware\nTransformer (PAFormer)}, a pose estimation based ReID model which can perform\nprecise part-to-part comparison. In order to inject part awareness to pose\ntokens, we introduce learnable parameters called `pose token' which estimate\nthe correlation between each body part and partial regions of the image.\nNotably, at inference phase, PAFormer operates without additional modules\nrelated to body part localization, which is commonly used in previous ReID\nmethodologies leveraging pose estimation models. Additionally, leveraging the\nenhanced awareness of body parts, PAFormer suggests the use of a learning-based\nvisibility predictor to estimate the degree of occlusion for each body part.\nAlso, we introduce a teacher forcing technique using ground truth visibility\nscores which enables PAFormer to be trained only with visible parts. A set of\nextensive experiments show that our method outperforms existing approaches on\nwell-known ReID benchmark datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}