{"id":"2408.08632","title":"A Survey on Benchmarks of Multimodal Large Language Models","authors":"Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang\n  Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, Ying Tai, Wankou Yang, Yabiao Wang,\n  Chengjie Wang","authorsParsed":[["Li","Jian",""],["Lu","Weiheng",""],["Fei","Hao",""],["Luo","Meng",""],["Dai","Ming",""],["Xia","Min",""],["Jin","Yizhang",""],["Gan","Zhenye",""],["Qi","Ding",""],["Fu","Chaoyou",""],["Tai","Ying",""],["Yang","Wankou",""],["Wang","Yabiao",""],["Wang","Chengjie",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 09:52:02 GMT"},{"version":"v2","created":"Fri, 6 Sep 2024 11:20:13 GMT"}],"updateDate":"2024-09-09","timestamp":1723801922000,"abstract":"  Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on\n(1)perception and understanding, (2)cognition and reasoning, (3)specific\ndomains, (4)key capabilities, and (5)other modalities. Finally, we discuss the\nlimitations of the current evaluation methods for MLLMs and explore promising\nfuture directions. Our key argument is that evaluation should be regarded as a\ncrucial discipline to support the development of MLLMs better. For more\ndetails, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}