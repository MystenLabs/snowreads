{"id":"2407.12522","title":"Struct-X: Enhancing Large Language Models Reasoning with Structured Data","authors":"Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu,\n  Yuan Qi","authorsParsed":[["Tan","Xiaoyu",""],["Wang","Haoyu",""],["Qiu","Xihe",""],["Cheng","Yuan",""],["Xu","Yinghui",""],["Chu","Wei",""],["Qi","Yuan",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 13:06:25 GMT"}],"updateDate":"2024-07-18","timestamp":1721221585000,"abstract":"  Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}