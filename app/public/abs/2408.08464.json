{"id":"2408.08464","title":"$\\textit{MMJ-Bench}$: A Comprehensive Study on Jailbreak Attacks and\n  Defenses for Vision Language Models","authors":"Fenghua Weng, Yue Xu, Chengyan Fu, Wenjie Wang","authorsParsed":[["Weng","Fenghua",""],["Xu","Yue",""],["Fu","Chengyan",""],["Wang","Wenjie",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 00:18:23 GMT"},{"version":"v2","created":"Wed, 4 Sep 2024 01:30:23 GMT"}],"updateDate":"2024-09-05","timestamp":1723767503000,"abstract":"  As deep learning advances, Large Language Models (LLMs) and their multimodal\ncounterparts, Vision-Language Models (VLMs), have shown exceptional performance\nin many real-world tasks. However, VLMs face significant security challenges,\nsuch as jailbreak attacks, where attackers attempt to bypass the model's safety\nalignment to elicit harmful responses. The threat of jailbreak attacks on VLMs\narises from both the inherent vulnerabilities of LLMs and the multiple\ninformation channels that VLMs process. While various attacks and defenses have\nbeen proposed, there is a notable gap in unified and comprehensive evaluations,\nas each method is evaluated on different dataset and metrics, making it\nimpossible to compare the effectiveness of each method. To address this gap, we\nintroduce \\textit{MMJ-Bench}, a unified pipeline for evaluating jailbreak\nattacks and defense techniques for VLMs. Through extensive experiments, we\nassess the effectiveness of various attack methods against SoTA VLMs and\nevaluate the impact of defense mechanisms on both defense effectiveness and\nmodel utility for normal tasks. Our comprehensive evaluation contribute to the\nfield by offering a unified and systematic evaluation framework and the first\npublic-available benchmark for VLM jailbreak research. We also demonstrate\nseveral insightful findings that highlights directions for future studies.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}