{"id":"2407.05449","title":"SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers\n  for Text Detoxification","authors":"Elisei Rykov, Konstantin Zaytsev, Ivan Anisimov, Alexandr Voronin","authorsParsed":[["Rykov","Elisei",""],["Zaytsev","Konstantin",""],["Anisimov","Ivan",""],["Voronin","Alexandr",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 17:19:34 GMT"},{"version":"v2","created":"Wed, 10 Jul 2024 14:44:18 GMT"}],"updateDate":"2024-07-11","timestamp":1720372774000,"abstract":"  This paper presents a solution for the Multilingual Text Detoxification task\nin the PAN-2024 competition of the SmurfCat team. Using data augmentation\nthrough machine translation and a special filtering procedure, we collected an\nadditional multilingual parallel dataset for text detoxification. Using the\nobtained data, we fine-tuned several multilingual sequence-to-sequence models,\nsuch as mT0 and Aya, on a text detoxification task. We applied the ORPO\nalignment technique to the final model. Our final model has only 3.7 billion\nparameters and achieves state-of-the-art results for the Ukrainian language and\nnear state-of-the-art results for other languages. In the competition, our team\nachieved first place in the automated evaluation with a score of 0.52 and\nsecond place in the final human evaluation with a score of 0.74.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}