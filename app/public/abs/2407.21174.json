{"id":"2407.21174","title":"AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal\n  Image Captioning","authors":"Maisha Binte Rashid and Pablo Rivas","authorsParsed":[["Rashid","Maisha Binte",""],["Rivas","Pablo",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 20:28:31 GMT"}],"updateDate":"2024-08-01","timestamp":1722371311000,"abstract":"  Multimodal machine learning models that combine visual and textual data are\nincreasingly being deployed in critical applications, raising significant\nsafety and security concerns due to their vulnerability to adversarial attacks.\nThis paper presents an effective strategy to enhance the robustness of\nmultimodal image captioning models against such attacks. By leveraging the Fast\nGradient Sign Method (FGSM) to generate adversarial examples and incorporating\nadversarial training techniques, we demonstrate improved model robustness on\ntwo benchmark datasets: Flickr8k and COCO. Our findings indicate that\nselectively training only the text decoder of the multimodal architecture shows\nperformance comparable to full adversarial training while offering increased\ncomputational efficiency. This targeted approach suggests a balance between\nrobustness and training costs, facilitating the ethical deployment of\nmultimodal AI systems across various domains.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}