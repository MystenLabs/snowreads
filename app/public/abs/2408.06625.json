{"id":"2408.06625","title":"DePatch: Towards Robust Adversarial Patch for Evading Person Detectors\n  in the Real World","authors":"Jikang Cheng, Ying Zhang, Zhongyuan Wang, Zou Qin, and Chen Li","authorsParsed":[["Cheng","Jikang",""],["Zhang","Ying",""],["Wang","Zhongyuan",""],["Qin","Zou",""],["Li","Chen",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 04:25:13 GMT"}],"updateDate":"2024-08-14","timestamp":1723523113000,"abstract":"  Recent years have seen an increasing interest in physical adversarial\nattacks, which aim to craft deployable patterns for deceiving deep neural\nnetworks, especially for person detectors. However, the adversarial patterns of\nexisting patch-based attacks heavily suffer from the self-coupling issue, where\na degradation, caused by physical transformations, in any small patch segment\ncan result in a complete adversarial dysfunction, leading to poor robustness in\nthe complex real world. Upon this observation, we introduce the Decoupled\nadversarial Patch (DePatch) attack to address the self-coupling issue of\nadversarial patches. Specifically, we divide the adversarial patch into\nblock-wise segments, and reduce the inter-dependency among these segments\nthrough randomly erasing out some segments during the optimization. We further\nintroduce a border shifting operation and a progressive decoupling strategy to\nimprove the overall attack capabilities. Extensive experiments demonstrate the\nsuperior performance of our method over other physical adversarial attacks,\nespecially in the real world.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"osTAi3DMVBGpc5dwCPw2bd4R9PpJ339_5mh4CNaYw-U","pdfSize":"3096698"}
