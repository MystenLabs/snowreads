{"id":"2408.00764","title":"AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation","authors":"Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jianguang Lou, Qingwei\n  Lin, Ping Luo, Saravan Rajmohan, Dongmei Zhang","authorsParsed":[["Hu","Mengkang",""],["Zhao","Pu",""],["Xu","Can",""],["Sun","Qingfeng",""],["Lou","Jianguang",""],["Lin","Qingwei",""],["Luo","Ping",""],["Rajmohan","Saravan",""],["Zhang","Dongmei",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:59:46 GMT"}],"updateDate":"2024-08-02","timestamp":1722535186000,"abstract":"  Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}