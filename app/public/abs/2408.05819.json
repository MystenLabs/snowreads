{"id":"2408.05819","title":"On the Convergence of a Federated Expectation-Maximization Algorithm","authors":"Zhixu Tao, Rajita Chandak, Sanjeev Kulkarni","authorsParsed":[["Tao","Zhixu",""],["Chandak","Rajita",""],["Kulkarni","Sanjeev",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 16:46:42 GMT"}],"updateDate":"2024-08-13","timestamp":1723394802000,"abstract":"  Data heterogeneity has been a long-standing bottleneck in studying the\nconvergence rates of Federated Learning algorithms. In order to better\nunderstand the issue of data heterogeneity, we study the convergence rate of\nthe Expectation-Maximization (EM) algorithm for the Federated Mixture of $K$\nLinear Regressions model. We fully characterize the convergence rate of the EM\nalgorithm under all regimes of $m/n$ where $m$ is the number of clients and $n$\nis the number of data points per client. We show that with a\nsignal-to-noise-ratio (SNR) of order $\\Omega(\\sqrt{K})$, the well-initialized\nEM algorithm converges within the minimax distance of the ground truth under\neach of the regimes. Interestingly, we identify that when $m$ grows\nexponentially in $n$, the EM algorithm only requires a constant number of\niterations to converge. We perform experiments on synthetic datasets to\nillustrate our results. Surprisingly, the results show that rather than being a\nbottleneck, data heterogeneity can accelerate the convergence of federated\nlearning algorithms.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}