{"id":"2408.03077","title":"Model-free optimal controller for discrete-time Markovian jump linear\n  systems: A Q-learning approach","authors":"Ehsan Badfar, Babak Tavassoli","authorsParsed":[["Badfar","Ehsan",""],["Tavassoli","Babak",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 10:13:22 GMT"}],"updateDate":"2024-08-07","timestamp":1722939202000,"abstract":"  This research paper introduces a model-free optimal controller for\ndiscrete-time Markovian jump linear systems (MJLSs), employing principles from\nthe methodology of reinforcement learning (RL). While Q-learning methods have\ndemonstrated efficacy in determining optimal controller gains for deterministic\nsystems, their application to systems with Markovian switching remains\nunexplored. To address this research gap, we propose a Q-function involving the\nMarkovian mode. Subsequently, a Q-learning algorithm is proposed to learn the\nunknown kernel matrix using raw input-state information from the system.\nNotably, the study proves the convergence of the proposed Q-learning optimal\ncontroller gains to the model-based optimal controller gains after proving the\nconvergence of a value iteration algorithm as the first step. Addition of\nexcitation noise to input which is required to ensure the leaning performance\ndoes not lead to any bias. Unlike the conventional optimal controller, the\nproposed method does not require any knowledge on system dynamics and\neliminates the need for solving coupled algebraic Riccati equations arising in\noptimal control of MJLSs. Finally, the efficiency of the proposed method is\ndemonstrated through a simulation study.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/"}