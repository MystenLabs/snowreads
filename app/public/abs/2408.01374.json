{"id":"2408.01374","title":"Hybrid Coordinate Descent for Efficient Neural Network Learning Using\n  Line Search and Gradient Descent","authors":"Yen-Che Hsiao and Abhishek Dutta","authorsParsed":[["Hsiao","Yen-Che",""],["Dutta","Abhishek",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 16:29:54 GMT"}],"updateDate":"2024-08-05","timestamp":1722616194000,"abstract":"  This paper presents a novel coordinate descent algorithm leveraging a\ncombination of one-directional line search and gradient information for\nparameter updates for a squared error loss function. Each parameter undergoes\nupdates determined by either the line search or gradient method, contingent\nupon whether the modulus of the gradient of the loss with respect to that\nparameter surpasses a predefined threshold. Notably, a larger threshold value\nenhances algorithmic efficiency. Despite the potentially slower nature of the\nline search method relative to gradient descent, its parallelizability\nfacilitates computational time reduction. Experimental validation conducted on\na 2-layer Rectified Linear Unit network with synthetic data elucidates the\nimpact of hyperparameters on convergence rates and computational efficiency.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UMZeFL1FKzdxh_7HiW-IvCw3BOZK9O5qW_OYBfKxPqM","pdfSize":"1459935"}
