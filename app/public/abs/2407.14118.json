{"id":"2407.14118","title":"Beyond Code Generation: Assessing Code LLM Maturity with Postconditions","authors":"Fusen He, Juan Zhai, Minxue Pan","authorsParsed":[["He","Fusen",""],["Zhai","Juan",""],["Pan","Minxue",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 08:34:30 GMT"}],"updateDate":"2024-07-22","timestamp":1721378070000,"abstract":"  Most existing code Large Language Model (LLM) benchmarks, e.g., EvalPlus,\nfocus on the code generation tasks. Namely, they contain a natural language\ndescription of a problem and ask the LLM to write code to solve the problem. We\nargue that they do not capture all capabilities needed to assess the quality of\na code LLM. In this paper, we propose a code LLM maturity model, based on the\npostcondition generation problem, to access a more complete set of code LLM\ncapabilities. We choose the postcondition generation problem as it requires the\ncode LLM to understand the code including semantics, natural language, and also\nhave the capability to generate unambiguous postconditions in programming\nlanguages (i.e., the generation capablity). Moreover, postconditions have\nvarious types, requiring different levels of these capabilities, making it\nsuitable to evaluate the maturity of the code LLM. Based on our designed\nmaturity model, we augment the EvalPlus dataset to a postcondition testing\nbenchmark, and evaluated several open-sourced models. Our results highlight the\nnecessary improvements needed for better LLMs for code. Code:\nhttps://github.com/MatureModel/PostcondGen\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}