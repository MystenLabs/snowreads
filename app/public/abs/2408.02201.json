{"id":"2408.02201","title":"Evaluating the Performance of Large Language Models for SDG Mapping\n  (Technical Report)","authors":"Hui Yin, Amir Aryani, Nakul Nambiar","authorsParsed":[["Yin","Hui",""],["Aryani","Amir",""],["Nambiar","Nakul",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 03:05:02 GMT"}],"updateDate":"2024-08-06","timestamp":1722827102000,"abstract":"  The use of large language models (LLMs) is expanding rapidly, and open-source\nversions are becoming available, offering users safer and more adaptable\noptions. These models enable users to protect data privacy by eliminating the\nneed to provide data to third parties and can be customized for specific tasks.\nIn this study, we compare the performance of various language models on the\nSustainable Development Goal (SDG) mapping task, using the output of GPT-4o as\nthe baseline. The selected open-source models for comparison include Mixtral,\nLLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more\nspecialized version of GPT-4o, was included to extend the comparison. Given the\nmulti-label nature of the SDG mapping task, we employed metrics such as F1\nscore, precision, and recall with micro-averaging to evaluate different aspects\nof the models' performance. These metrics are derived from the confusion matrix\nto ensure a comprehensive evaluation. We provide a clear observation and\nanalysis of each model's performance by plotting curves based on F1 score,\nprecision, and recall at different thresholds. According to the results of this\nexperiment, LLaMA 2 and Gemma still have significant room for improvement. The\nother four models do not exhibit particularly large differences in performance.\nThe outputs from all seven models are available on Zenodo:\nhttps://doi.org/10.5281/zenodo.12789375.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7wWJn4hPsRpUFMg6iyKvyUa8Jxe4S5v26tEO74NbdTg","pdfSize":"501162"}
