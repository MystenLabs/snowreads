{"id":"2408.02111","title":"Understanding Deep Learning via Notions of Rank","authors":"Noam Razin","authorsParsed":[["Razin","Noam",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 18:47:55 GMT"}],"updateDate":"2024-08-06","timestamp":1722797275000,"abstract":"  Despite the extreme popularity of deep learning in science and industry, its\nformal understanding is limited. This thesis puts forth notions of rank as key\nfor developing a theory of deep learning, focusing on the fundamental aspects\nof generalization and expressiveness. In particular, we establish that\ngradient-based training can induce an implicit regularization towards low rank\nfor several neural network architectures, and demonstrate empirically that this\nphenomenon may facilitate an explanation of generalization over natural data\n(e.g., audio, images, and text). Then, we characterize the ability of graph\nneural networks to model interactions via a notion of rank, which is commonly\nused for quantifying entanglement in quantum physics. A central tool underlying\nthese results is a connection between neural networks and tensor\nfactorizations. Practical implications of our theory for designing explicit\nregularization schemes and data preprocessing algorithms are presented.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}