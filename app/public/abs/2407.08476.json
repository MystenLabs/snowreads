{"id":"2407.08476","title":"VideoMamba: Spatio-Temporal Selective State Space Model","authors":"Jinyoung Park, Hee-Seon Kim, Kangwook Ko, Minbeom Kim, Changick Kim","authorsParsed":[["Park","Jinyoung",""],["Kim","Hee-Seon",""],["Ko","Kangwook",""],["Kim","Minbeom",""],["Kim","Changick",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:11:21 GMT"}],"updateDate":"2024-07-12","timestamp":1720703481000,"abstract":"  We introduce VideoMamba, a novel adaptation of the pure Mamba architecture,\nspecifically designed for video recognition. Unlike transformers that rely on\nself-attention mechanisms leading to high computational costs by quadratic\ncomplexity, VideoMamba leverages Mamba's linear complexity and selective SSM\nmechanism for more efficient processing. The proposed Spatio-Temporal Forward\nand Backward SSM allows the model to effectively capture the complex\nrelationship between non-sequential spatial and sequential temporal information\nin video. Consequently, VideoMamba is not only resource-efficient but also\neffective in capturing long-range dependency in videos, demonstrated by\ncompetitive performance and outstanding efficiency on a variety of video\nunderstanding benchmarks. Our work highlights the potential of VideoMamba as a\npowerful tool for video understanding, offering a simple yet effective baseline\nfor future research in video analysis.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}