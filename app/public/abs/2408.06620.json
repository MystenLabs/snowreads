{"id":"2408.06620","title":"Unveiling the Flaws: A Critical Analysis of Initialization Effect on\n  Time Series Anomaly Detection","authors":"Alex Koran, Hadi Hojjati, Narges Armanfard","authorsParsed":[["Koran","Alex",""],["Hojjati","Hadi",""],["Armanfard","Narges",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 04:08:17 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 09:14:40 GMT"}],"updateDate":"2024-09-18","timestamp":1723522097000,"abstract":"  Deep learning for time-series anomaly detection (TSAD) has gained significant\nattention over the past decade. Despite the reported improvements in several\npapers, the practical application of these models remains limited. Recent\nstudies have cast doubt on these models, attributing their results to flawed\nevaluation techniques. However, the impact of initialization has largely been\noverlooked. This paper provides a critical analysis of the initialization\neffects on TSAD model performance. Our extensive experiments reveal that TSAD\nmodels are highly sensitive to hyperparameters such as window size, seed\nnumber, and normalization. This sensitivity often leads to significant\nvariability in performance, which can be exploited to artificially inflate the\nreported efficacy of these models. We demonstrate that even minor changes in\ninitialization parameters can result in performance variations that overshadow\nthe claimed improvements from novel model architectures. Our findings highlight\nthe need for rigorous evaluation protocols and transparent reporting of\npreprocessing steps to ensure the reliability and fairness of anomaly detection\nmethods. This paper calls for a more cautious interpretation of TSAD\nadvancements and encourages the development of more robust and transparent\nevaluation practices to advance the field and its practical applications.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"IahmK8PE0OcwjvPobZ9Ct_GBc0cqg19wLAbPWXWh7ig","pdfSize":"472219"}
