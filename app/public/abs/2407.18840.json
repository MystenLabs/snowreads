{"id":"2407.18840","title":"The Cross-environment Hyperparameter Setting Benchmark for Reinforcement\n  Learning","authors":"Andrew Patterson, Samuel Neumann, Raksha Kumaraswamy, Martha White,\n  Adam White","authorsParsed":[["Patterson","Andrew",""],["Neumann","Samuel",""],["Kumaraswamy","Raksha",""],["White","Martha",""],["White","Adam",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 16:04:40 GMT"}],"updateDate":"2024-07-29","timestamp":1722009880000,"abstract":"  This paper introduces a new empirical methodology, the Cross-environment\nHyperparameter Setting Benchmark, that compares RL algorithms across\nenvironments using a single hyperparameter setting, encouraging algorithmic\ndevelopment which is insensitive to hyperparameters. We demonstrate that this\nbenchmark is robust to statistical noise and obtains qualitatively similar\nresults across repeated applications, even when using few samples. This\nrobustness makes the benchmark computationally cheap to apply, allowing\nstatistically sound insights at low cost. We demonstrate two example\ninstantiations of the CHS, on a set of six small control environments (SC-CHS)\nand on the entire DM Control suite of 28 environments (DMC-CHS). Finally, to\nillustrate the applicability of the CHS to modern RL algorithms on challenging\nenvironments, we conduct a novel empirical study of an open question in the\ncontinuous control literature. We show, with high confidence, that there is no\nmeaningful difference in performance between Ornstein-Uhlenbeck noise and\nuncorrelated Gaussian noise for exploration with the DDPG algorithm on the\nDMC-CHS.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}