{"id":"2407.04242","title":"Fine-grained Context and Multi-modal Alignment for Freehand 3D\n  Ultrasound Reconstruction","authors":"Zhongnuo Yan, Xin Yang, Mingyuan Luo, Jiongquan Chen, Rusi Chen, Lian\n  Liu, Dong Ni","authorsParsed":[["Yan","Zhongnuo",""],["Yang","Xin",""],["Luo","Mingyuan",""],["Chen","Jiongquan",""],["Chen","Rusi",""],["Liu","Lian",""],["Ni","Dong",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 04:09:30 GMT"}],"updateDate":"2024-07-08","timestamp":1720152570000,"abstract":"  Fine-grained spatio-temporal learning is crucial for freehand 3D ultrasound\nreconstruction. Previous works mainly resorted to the coarse-grained spatial\nfeatures and the separated temporal dependency learning and struggles for\nfine-grained spatio-temporal learning. Mining spatio-temporal information in\nfine-grained scales is extremely challenging due to learning difficulties in\nlong-range dependencies. In this context, we propose a novel method to exploit\nthe long-range dependency management capabilities of the state space model\n(SSM) to address the above challenge. Our contribution is three-fold. First, we\npropose ReMamba, which mines multi-scale spatio-temporal information by\ndevising a multi-directional SSM. Second, we propose an adaptive fusion\nstrategy that introduces multiple inertial measurement units as auxiliary\ntemporal information to enhance spatio-temporal perception. Last, we design an\nonline alignment strategy that encodes the temporal information as pseudo\nlabels for multi-modal alignment to further improve reconstruction performance.\nExtensive experimental validations on two large-scale datasets show remarkable\nimprovement from our method over competitors.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}