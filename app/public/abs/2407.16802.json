{"id":"2407.16802","title":"Distribution-Aware Robust Learning from Long-Tailed Data with Noisy\n  Labels","authors":"Jae Soon Baik, In Young Yoon, Kun Hoon Kim, and Jun Won Choi","authorsParsed":[["Baik","Jae Soon",""],["Yoon","In Young",""],["Kim","Kun Hoon",""],["Choi","Jun Won",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 19:06:15 GMT"}],"updateDate":"2024-07-25","timestamp":1721761575000,"abstract":"  Deep neural networks have demonstrated remarkable advancements in various\nfields using large, well-annotated datasets. However, real-world data often\nexhibit long-tailed distributions and label noise, significantly degrading\ngeneralization performance. Recent studies addressing these issues have focused\non noisy sample selection methods that estimate the centroid of each class\nbased on high-confidence samples within each target class. The performance of\nthese methods is limited because they use only the training samples within each\nclass for class centroid estimation, making the quality of centroids\nsusceptible to long-tailed distributions and noisy labels. In this study, we\npresent a robust training framework called Distribution-aware Sample Selection\nand Contrastive Learning (DaSC). Specifically, DaSC introduces a\nDistribution-aware Class Centroid Estimation (DaCC) to generate enhanced class\ncentroids. DaCC performs weighted averaging of the features from all samples,\nwith weights determined based on model predictions. Additionally, we propose a\nconfidence-aware contrastive learning strategy to obtain balanced and robust\nrepresentations. The training samples are categorized into high-confidence and\nlow-confidence samples. Our method then applies Semi-supervised Balanced\nContrastive Loss (SBCL) using high-confidence samples, leveraging reliable\nlabel information to mitigate class bias. For the low-confidence samples, our\nmethod computes Mixup-enhanced Instance Discrimination Loss (MIDL) to improve\ntheir representations in a self-supervised manner. Our experimental results on\nCIFAR and real-world noisy-label datasets demonstrate the superior performance\nof the proposed DaSC compared to previous approaches.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}