{"id":"2408.09882","title":"GINO-Q: Learning an Asymptotically Optimal Index Policy for Restless\n  Multi-armed Bandits","authors":"Gongpu Chen, Soung Chang Liew, Deniz Gunduz","authorsParsed":[["Chen","Gongpu",""],["Liew","Soung Chang",""],["Gunduz","Deniz",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 10:50:45 GMT"}],"updateDate":"2024-08-20","timestamp":1724064645000,"abstract":"  The restless multi-armed bandit (RMAB) framework is a popular model with\napplications across a wide variety of fields. However, its solution is hindered\nby the exponentially growing state space (with respect to the number of arms)\nand the combinatorial action space, making traditional reinforcement learning\nmethods infeasible for large-scale instances. In this paper, we propose GINO-Q,\na three-timescale stochastic approximation algorithm designed to learn an\nasymptotically optimal index policy for RMABs. GINO-Q mitigates the curse of\ndimensionality by decomposing the RMAB into a series of subproblems, each with\nthe same dimension as a single arm, ensuring that complexity increases linearly\nwith the number of arms. Unlike recently developed Whittle-index-based\nalgorithms, GINO-Q does not require RMABs to be indexable, enhancing its\nflexibility and applicability. Our experimental results demonstrate that GINO-Q\nconsistently learns near-optimal policies, even for non-indexable RMABs where\nWhittle-index-based algorithms perform poorly, and it converges significantly\nfaster than existing baselines.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}