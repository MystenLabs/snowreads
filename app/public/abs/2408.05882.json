{"id":"2408.05882","title":"Creating Arabic LLM Prompts at Scale","authors":"Abdelrahman El-Sheikh and Ahmed Elmogtaba and Kareem Darwish and\n  Muhammad Elmallah and Ashraf Elneima and Hassan Sawaf","authorsParsed":[["El-Sheikh","Abdelrahman",""],["Elmogtaba","Ahmed",""],["Darwish","Kareem",""],["Elmallah","Muhammad",""],["Elneima","Ashraf",""],["Sawaf","Hassan",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 00:46:39 GMT"}],"updateDate":"2024-08-13","timestamp":1723423599000,"abstract":"  The debut of chatGPT and BARD has popularized instruction following text\ngeneration using LLMs, where a user can interrogate an LLM using natural\nlanguage requests and obtain natural language answers that matches their\nrequests. Training LLMs to respond in this manner requires a large number of\nworked out examples of user requests (aka prompts) with corresponding gold\nresponses. In this paper, we introduce two methods for creating such prompts\nfor Arabic cheaply and quickly. The first methods entails automatically\ntranslating existing prompt datasets from English, such as PromptSource and\nSuper-NaturalInstructions, and then using machine translation quality\nestimation to retain high quality translations only. The second method involves\ncreating natural language prompts on top of existing Arabic NLP datasets. Using\nthese two methods we were able to create more than 67.4 million Arabic prompts\nthat cover a variety of tasks including summarization, headline generation,\ngrammar checking, open/closed question answering, creative writing, etc. We\nshow that fine tuning an open 7 billion parameter large language model, namely\nbase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter\ninstruction tuned model, namely Llama3 70B, in handling Arabic prompts.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"FDMKnmWcucpvkkZhYGKEDb5lPoqcS9aULoxZmig3VaU","pdfSize":"298646"}
