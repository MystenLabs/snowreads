{"id":"2407.08507","title":"Bootstrapping Vision-language Models for Self-supervised Remote\n  Physiological Measurement","authors":"Zijie Yue, Miaojing Shi, Hanli Wang, Shuai Ding, Qijun Chen, Shanlin\n  Yang","authorsParsed":[["Yue","Zijie",""],["Shi","Miaojing",""],["Wang","Hanli",""],["Ding","Shuai",""],["Chen","Qijun",""],["Yang","Shanlin",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:45:50 GMT"}],"updateDate":"2024-07-12","timestamp":1720705550000,"abstract":"  Facial video-based remote physiological measurement is a promising research\narea for detecting human vital signs (e.g., heart rate, respiration frequency)\nin a non-contact way. Conventional approaches are mostly supervised learning,\nrequiring extensive collections of facial videos and synchronously recorded\nphotoplethysmography (PPG) signals. To tackle it, self-supervised learning has\nrecently gained attentions; due to the lack of ground truth PPG signals, its\nperformance is however limited. In this paper, we propose a novel\nself-supervised framework that successfully integrates the popular\nvision-language models (VLMs) into the remote physiological measurement task.\nGiven a facial video, we first augment its positive and negative video samples\nwith varying rPPG signal frequencies. Next, we introduce a frequency-oriented\nvision-text pair generation method by carefully creating contrastive\nspatio-temporal maps from positive and negative samples and designing proper\ntext prompts to describe their relative ratios of signal frequencies. A\npre-trained VLM is employed to extract features for these formed vision-text\npairs and estimate rPPG signals thereafter. We develop a series of generative\nand contrastive learning mechanisms to optimize the VLM, including the\ntext-guided visual map reconstruction task, the vision-text contrastive\nlearning task, and the frequency contrastive and ranking task. Overall, our\nmethod for the first time adapts VLMs to digest and align the frequency-related\nknowledge in vision and text modalities. Extensive experiments on four\nbenchmark datasets demonstrate that it significantly outperforms state of the\nart self-supervised methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}