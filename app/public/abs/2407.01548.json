{"id":"2407.01548","title":"From Cognition to Computation: A Comparative Review of Human Attention\n  and Transformer Architectures","authors":"Minglu Zhao, Dehong Xu, Tao Gao","authorsParsed":[["Zhao","Minglu",""],["Xu","Dehong",""],["Gao","Tao",""]],"versions":[{"version":"v1","created":"Thu, 25 Apr 2024 05:13:38 GMT"}],"updateDate":"2024-07-03","timestamp":1714022018000,"abstract":"  Attention is a cornerstone of human cognition that facilitates the efficient\nextraction of information in everyday life. Recent developments in artificial\nintelligence like the Transformer architecture also incorporate the idea of\nattention in model designs. However, despite the shared fundamental principle\nof selectively attending to information, human attention and the Transformer\nmodel display notable differences, particularly in their capacity constraints,\nattention pathways, and intentional mechanisms. Our review aims to provide a\ncomparative analysis of these mechanisms from a cognitive-functional\nperspective, thereby shedding light on several open research questions. The\nexploration encourages interdisciplinary efforts to derive insights from human\nattention mechanisms in the pursuit of developing more generalized artificial\nintelligence.\n","subjects":["Quantitative Biology/Other Quantitative Biology","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}