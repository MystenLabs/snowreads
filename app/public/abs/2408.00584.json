{"id":"2408.00584","title":"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian\n  Rebuses","authors":"Gabriele Sarti, Tommaso Caselli, Malvina Nissim, Arianna Bisazza","authorsParsed":[["Sarti","Gabriele",""],["Caselli","Tommaso",""],["Nissim","Malvina",""],["Bisazza","Arianna",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 14:14:15 GMT"}],"updateDate":"2024-08-02","timestamp":1722521655000,"abstract":"  Rebuses are puzzles requiring constrained multi-step reasoning to identify a\nhidden phrase from a set of images and letters. In this work, we introduce a\nlarge collection of verbalized rebuses for the Italian language and use it to\nassess the rebus-solving capabilities of state-of-the-art large language\nmodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly\non this task, ad-hoc fine-tuning seems to improve models' performance. However,\nwe find that performance gains from training are largely motivated by\nmemorization. Our results suggest that rebus solving remains a challenging test\nbed to evaluate large language models' linguistic proficiency and sequential\ninstruction-following skills.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}