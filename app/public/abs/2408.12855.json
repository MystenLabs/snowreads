{"id":"2408.12855","title":"Efficient Training Approaches for Performance Anomaly Detection Models\n  in Edge Computing Environments","authors":"Duneesha Fernando, Maria A. Rodriguez, Patricia Arroba, Leila Ismail,\n  Rajkumar Buyya","authorsParsed":[["Fernando","Duneesha",""],["Rodriguez","Maria A.",""],["Arroba","Patricia",""],["Ismail","Leila",""],["Buyya","Rajkumar",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 05:50:12 GMT"}],"updateDate":"2024-08-26","timestamp":1724392212000,"abstract":"  Microservice architectures are increasingly used to modularize IoT\napplications and deploy them in distributed and heterogeneous edge computing\nenvironments. Over time, these microservice-based IoT applications are\nsusceptible to performance anomalies caused by resource hogging (e.g., CPU or\nmemory), resource contention, etc., which can negatively impact their Quality\nof Service and violate their Service Level Agreements. Existing research on\nperformance anomaly detection for edge computing environments focuses on model\ntraining approaches that either achieve high accuracy at the expense of a\ntime-consuming and resource-intensive training process or prioritize training\nefficiency at the cost of lower accuracy. To address this gap, while\nconsidering the resource constraints and the large number of devices in modern\nedge platforms, we propose two clustering-based model training approaches : (1)\nintra-cluster parameter transfer learning-based model training (ICPTL) and (2)\ncluster-level model training (CM). These approaches aim to find a trade-off\nbetween the training efficiency of anomaly detection models and their accuracy.\nWe compared the models trained under ICPTL and CM to models trained for\nspecific devices (most accurate, least efficient) and a single general model\ntrained for all devices (least accurate, most efficient). Our findings show\nthat the model accuracy of ICPTL is comparable to that of the model per device\napproach while requiring only 40% of the training time. In addition, CM further\nimproves training efficiency by requiring 23% less training time and reducing\nthe number of trained models by approximately 66% compared to ICPTL, yet\nachieving a higher accuracy than a single general model.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}