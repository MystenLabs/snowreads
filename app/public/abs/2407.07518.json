{"id":"2407.07518","title":"Multi-modal Crowd Counting via a Broker Modality","authors":"Haoliang Meng and Xiaopeng Hong and Chenhao Wang and Miao Shang and\n  Wangmeng Zuo","authorsParsed":[["Meng","Haoliang",""],["Hong","Xiaopeng",""],["Wang","Chenhao",""],["Shang","Miao",""],["Zuo","Wangmeng",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 10:13:11 GMT"}],"updateDate":"2024-07-11","timestamp":1720606391000,"abstract":"  Multi-modal crowd counting involves estimating crowd density from both visual\nand thermal/depth images. This task is challenging due to the significant gap\nbetween these distinct modalities. In this paper, we propose a novel approach\nby introducing an auxiliary broker modality and on this basis frame the task as\na triple-modal learning problem. We devise a fusion-based method to generate\nthis broker modality, leveraging a non-diffusion, lightweight counterpart of\nmodern denoising diffusion-based fusion models. Additionally, we identify and\naddress the ghosting effect caused by direct cross-modal image fusion in\nmulti-modal crowd counting. Through extensive experimental evaluations on\npopular multi-modal crowd-counting datasets, we demonstrate the effectiveness\nof our method, which introduces only 4 million additional parameters, yet\nachieves promising results. The code is available at\nhttps://github.com/HenryCilence/Broker-Modality-Crowd-Counting.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}