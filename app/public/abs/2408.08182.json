{"id":"2408.08182","title":"Your Turn: At Home Turning Angle Estimation for Parkinson's Disease\n  Severity Assessment","authors":"Qiushuo Cheng, Catherine Morgan, Arindam Sikdar, Alessandro Masullo,\n  Alan Whone, Majid Mirmehdi","authorsParsed":[["Cheng","Qiushuo",""],["Morgan","Catherine",""],["Sikdar","Arindam",""],["Masullo","Alessandro",""],["Whone","Alan",""],["Mirmehdi","Majid",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 14:36:07 GMT"},{"version":"v2","created":"Sat, 24 Aug 2024 16:18:50 GMT"}],"updateDate":"2024-08-27","timestamp":1723732567000,"abstract":"  People with Parkinson's Disease (PD) often experience progressively worsening\ngait, including changes in how they turn around, as the disease progresses.\nExisting clinical rating tools are not capable of capturing hour-by-hour\nvariations of PD symptoms, as they are confined to brief assessments within\nclinic settings. Measuring gait turning angles continuously and passively is a\ncomponent step towards using gait characteristics as sensitive indicators of\ndisease progression in PD. This paper presents a deep learning-based approach\nto automatically quantify turning angles by extracting 3D skeletons from videos\nand calculating the rotation of hip and knee joints. We utilise\nstate-of-the-art human pose estimation models, Fastpose and Strided\nTransformer, on a total of 1386 turning video clips from 24 subjects (12 people\nwith PD and 12 healthy control volunteers), trimmed from a PD dataset of\nunscripted free-living videos in a home-like setting (Turn-REMAP). We also\ncurate a turning video dataset, Turn-H3.6M, from the public Human3.6M human\npose benchmark with 3D ground truth, to further validate our method. Previous\ngait research has primarily taken place in clinics or laboratories evaluating\nscripted gait outcomes, but this work focuses on free-living home settings\nwhere complexities exist, such as baggy clothing and poor lighting. Due to\ndifficulties in obtaining accurate ground truth data in a free-living setting,\nwe quantise the angle into the nearest bin $45^\\circ$ based on the manual\nlabelling of expert clinicians. Our method achieves a turning calculation\naccuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\\deg}, and a weighted\nprecision WPrec of 68.3% for Turn-REMAP. This is the first work to explore the\nuse of single monocular camera data to quantify turns by PD patients in a home\nsetting.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"zIcEpTU9uviLjAWpoEADQ83z59cKvn_XyaIVJsTTVjE","pdfSize":"6633460"}
