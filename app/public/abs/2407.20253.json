{"id":"2407.20253","title":"Improving EEG Classification Through Randomly Reassembling Original and\n  Generated Data with Transformer-based Diffusion Models","authors":"Mingzhi Chen, Yiyu Gui, Yuqi Su, Yuesheng Zhu, Guibo Luo, Yuchao Yang","authorsParsed":[["Chen","Mingzhi",""],["Gui","Yiyu",""],["Su","Yuqi",""],["Zhu","Yuesheng",""],["Luo","Guibo",""],["Yang","Yuchao",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 06:58:14 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 17:13:25 GMT"}],"updateDate":"2024-08-20","timestamp":1721458694000,"abstract":"  Electroencephalogram (EEG) classification has been widely used in various\nmedical and engineering applications, where it is important for understanding\nbrain function, diagnosing diseases, and assessing mental health conditions.\nHowever, the scarcity of EEG data severely restricts the performance of EEG\nclassification networks, and generative model-based data augmentation methods\nhave emerged as potential solutions to overcome this challenge. There are two\nproblems with existing methods: (1) The quality of the generated EEG signals is\nnot high; (2) The enhancement of EEG classification networks is not effective.\nIn this paper, we propose a Transformer-based denoising diffusion probabilistic\nmodel and a generated data-based augmentation method to address the above two\nproblems. For the characteristics of EEG signals, we propose a constant-factor\nscaling method to preprocess the signals, which reduces the loss of\ninformation. We incorporated Multi-Scale Convolution and Dynamic Fourier\nSpectrum Information modules into the model, improving the stability of the\ntraining process and the quality of the generated data. The proposed\naugmentation method randomly reassemble the generated data with original data\nin the time-domain to obtain vicinal data, which improves the model performance\nby minimizing the empirical risk and the vicinal risk. We verify the proposed\naugmentation method on four EEG datasets for four tasks and observe significant\naccuracy performance improvements: 14.00% on the Bonn dataset; 6.38% on the\nSleepEDF-20 dataset; 9.42% on the FACED dataset; 2.5% on the Shu dataset. We\nwill make the code of our method publicly accessible soon.\n","subjects":["Electrical Engineering and Systems Science/Signal Processing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}