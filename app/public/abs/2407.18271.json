{"id":"2407.18271","title":"Large Language Model for Verilog Generation with Golden Code Feedback","authors":"Ning Wang, Bingkun Yao, Jie Zhou, Xi Wang, Zhe Jiang and Nan Guan","authorsParsed":[["Wang","Ning",""],["Yao","Bingkun",""],["Zhou","Jie",""],["Wang","Xi",""],["Jiang","Zhe",""],["Guan","Nan",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 11:25:21 GMT"},{"version":"v2","created":"Mon, 5 Aug 2024 06:12:46 GMT"}],"updateDate":"2024-08-06","timestamp":1721561121000,"abstract":"  Recent advancements in large language models (LLMs) have catalyzed\nsignificant interest in the automatic generation of Register-Transfer Level\n(RTL) code, particularly Verilog, from natural language instructions. While\ncommercial LLMs like ChatGPT have dominated this domain, open-source\nalternatives have lagged considerably in performance, limiting the flexibility\nand data privacy of this emerging technology. This study introduces a novel\napproach utilizing reinforcement learning with golden code feedback to enhance\nthe performance of pre-trained models. Leveraging open-source data and base\nmodels, we have achieved state-of-the-art (SOTA) results with a substantial\nmargin. Notably, our 6.7B parameter model \\ours{} demonstrates superior\nperformance compared to current best-in-class 13B and 16B models. Furthermore,\nthrough a comprehensive analysis of the limitations in direct fine-tuning and\nthe training dynamics of reinforcement learning, we posit that the development\nof comprehensive supervisory signals, which are align with the inherent\nparallel semantics of Verilog code, is critical to effective generation. The\ncode and data associated with this research are publicly available at\n\\url{https://github.com/CatIIIIIIII/veriseek}. The model weights can be\naccessed at \\url{https://huggingface.co/WANGNingroci/VeriSeek}.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}