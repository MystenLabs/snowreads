{"id":"2408.14352","title":"Assessing Contamination in Large Language Models: Introducing the\n  LogProber method","authors":"Nicolas Yax and Pierre-Yves Oudeyer and Stefano Palminteri","authorsParsed":[["Yax","Nicolas",""],["Oudeyer","Pierre-Yves",""],["Palminteri","Stefano",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 15:29:34 GMT"}],"updateDate":"2024-08-27","timestamp":1724686174000,"abstract":"  In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yJcG0No3AbmOKLXXOUaEkt92wLxBzRboxDG-Kwjy2c0","pdfSize":"1698379"}
