{"id":"2408.02718","title":"MMIU: Multimodal Multi-image Understanding for Evaluating Large\n  Vision-Language Models","authors":"Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi\n  Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, Kaipeng Zhang, Wenqi Shao","authorsParsed":[["Meng","Fanqing",""],["Wang","Jin",""],["Li","Chuanhao",""],["Lu","Quanfeng",""],["Tian","Hao",""],["Liao","Jiaqi",""],["Zhu","Xizhou",""],["Dai","Jifeng",""],["Qiao","Yu",""],["Luo","Ping",""],["Zhang","Kaipeng",""],["Shao","Wenqi",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 17:56:41 GMT"}],"updateDate":"2024-08-07","timestamp":1722880601000,"abstract":"  The capability to process multiple images is crucial for Large\nVision-Language Models (LVLMs) to develop a more thorough and nuanced\nunderstanding of a scene. Recent multi-image LVLMs have begun to address this\nneed. However, their evaluation has not kept pace with their development. To\nfill this gap, we introduce the Multimodal Multi-image Understanding (MMIU)\nbenchmark, a comprehensive evaluation suite designed to assess LVLMs across a\nwide range of multi-image tasks. MMIU encompasses 7 types of multi-image\nrelationships, 52 tasks, 77K images, and 11K meticulously curated\nmultiple-choice questions, making it the most extensive benchmark of its kind.\nOur evaluation of 24 popular LVLMs, including both open-source and proprietary\nmodels, reveals significant challenges in multi-image comprehension,\nparticularly in tasks involving spatial understanding. Even the most advanced\nmodels, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Through\nmulti-faceted analytical experiments, we identify key performance gaps and\nlimitations, providing valuable insights for future model and data\nimprovements. We aim for MMIU to advance the frontier of LVLM research and\ndevelopment, moving us toward achieving sophisticated multimodal multi-image\nuser interactions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}