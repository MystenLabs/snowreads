{"id":"2408.11239","title":"A Little Confidence Goes a Long Way","authors":"John Scoville, Shang Gao, Devanshu Agrawal, Javed Qadrud-Din","authorsParsed":[["Scoville","John",""],["Gao","Shang",""],["Agrawal","Devanshu",""],["Qadrud-Din","Javed",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 23:36:00 GMT"}],"updateDate":"2024-08-22","timestamp":1724196960000,"abstract":"  We introduce a group of related methods for binary classification tasks using\nprobes of the hidden state activations in large language models (LLMs).\nPerformance is on par with the largest and most advanced LLMs currently\navailable, but requiring orders of magnitude fewer computational resources and\nnot requiring labeled data. This approach involves translating class labels\ninto a semantically rich description, spontaneous symmetry breaking of\nmultilayer perceptron probes for unsupervised learning and inference, training\nprobes to generate confidence scores (prior probabilities) from hidden state\nactivations subject to known constraints via entropy maximization, and\nselecting the most confident probe model from an ensemble for prediction. These\ntechniques are evaluated on four datasets using five base LLMs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Information Theory","Computing Research Repository/Neural and Evolutionary Computing","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}