{"id":"2408.04958","title":"Surgical-VQLA++: Adversarial Contrastive Learning for Calibrated Robust\n  Visual Question-Localized Answering in Robotic Surgery","authors":"Long Bai, Guankun Wang, Mobarakol Islam, Lalithkumar Seenivasan, An\n  Wang, Hongliang Ren","authorsParsed":[["Bai","Long",""],["Wang","Guankun",""],["Islam","Mobarakol",""],["Seenivasan","Lalithkumar",""],["Wang","An",""],["Ren","Hongliang",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 09:23:07 GMT"},{"version":"v2","created":"Sun, 1 Sep 2024 14:43:05 GMT"}],"updateDate":"2024-09-04","timestamp":1723195387000,"abstract":"  Medical visual question answering (VQA) bridges the gap between visual\ninformation and clinical decision-making, enabling doctors to extract\nunderstanding from clinical images and videos. In particular, surgical VQA can\nenhance the interpretation of surgical data, aiding in accurate diagnoses,\neffective education, and clinical interventions. However, the inability of VQA\nmodels to visually indicate the regions of interest corresponding to the given\nquestions results in incomplete comprehension of the surgical scene. To tackle\nthis, we propose the surgical visual question localized-answering (VQLA) for\nprecise and context-aware responses to specific queries regarding surgical\nimages. Furthermore, to address the strong demand for safety in surgical\nscenarios and potential corruptions in image acquisition and transmission, we\npropose a novel approach called Calibrated Co-Attention Gated Vision-Language\n(C$^2$G-ViL) embedding to integrate and align multimodal information\neffectively. Additionally, we leverage the adversarial sample-based contrastive\nlearning strategy to boost our performance and robustness. We also extend our\nEndoVis-18-VQLA and EndoVis-17-VQLA datasets to broaden the scope and\napplication of our data. Extensive experiments on the aforementioned datasets\ndemonstrate the remarkable performance and robustness of our solution. Our\nsolution can effectively combat real-world image corruption. Thus, our proposed\napproach can serve as an effective tool for assisting surgical education,\npatient care, and enhancing surgical outcomes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}