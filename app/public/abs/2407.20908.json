{"id":"2407.20908","title":"Dynamic Scene Understanding through Object-Centric Voxelization and\n  Neural Rendering","authors":"Yanpeng Zhao, Yiwei Hao, Siyu Gao, Yunbo Wang, Xiaokang Yang","authorsParsed":[["Zhao","Yanpeng",""],["Hao","Yiwei",""],["Gao","Siyu",""],["Wang","Yunbo",""],["Yang","Xiaokang",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 15:33:58 GMT"}],"updateDate":"2024-07-31","timestamp":1722353638000,"abstract":"  Learning object-centric representations from unsupervised videos is\nchallenging. Unlike most previous approaches that focus on decomposing 2D\nimages, we present a 3D generative model named DynaVol-S for dynamic scenes\nthat enables object-centric learning within a differentiable volume rendering\nframework. The key idea is to perform object-centric voxelization to capture\nthe 3D nature of the scene, which infers per-object occupancy probabilities at\nindividual spatial locations. These voxel features evolve through a\ncanonical-space deformation function and are optimized in an inverse rendering\npipeline with a compositional NeRF. Additionally, our approach integrates 2D\nsemantic features to create 3D semantic grids, representing the scene through\nmultiple disentangled voxel grids. DynaVol-S significantly outperforms existing\nmodels in both novel view synthesis and unsupervised decomposition tasks for\ndynamic scenes. By jointly considering geometric structures and semantic\nfeatures, it effectively addresses challenging real-world scenarios involving\ncomplex object interactions. Furthermore, once trained, the explicitly\nmeaningful voxel features enable additional capabilities that 2D scene\ndecomposition methods cannot achieve, such as novel scene generation through\nediting geometric shapes or manipulating the motion trajectories of objects.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}