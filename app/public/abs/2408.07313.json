{"id":"2408.07313","title":"Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal\n  Data for Mental Health","authors":"Yongquan Hu, Shuning Zhang, Ting Dang, Hong Jia, Flora D. Salim, Wen\n  Hu, Aaron J. Quigley","authorsParsed":[["Hu","Yongquan",""],["Zhang","Shuning",""],["Dang","Ting",""],["Jia","Hong",""],["Salim","Flora D.",""],["Hu","Wen",""],["Quigley","Aaron J.",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 06:14:02 GMT"}],"updateDate":"2024-08-15","timestamp":1723616042000,"abstract":"  Integrating physiological signals such as electroencephalogram (EEG), with\nother data such as interview audio, may offer valuable multimodal insights into\npsychological states or neurological disorders. Recent advancements with Large\nLanguage Models (LLMs) position them as prospective ``health agents'' for\nmental health assessment. However, current research predominantly focus on\nsingle data modalities, presenting an opportunity to advance understanding\nthrough multimodal data. Our study aims to advance this approach by\ninvestigating multimodal data using LLMs for mental health assessment,\nspecifically through zero-shot and few-shot prompting. Three datasets are\nadopted for depression and emotion classifications incorporating EEG, facial\nexpressions, and audio (text). The results indicate that multimodal information\nconfers substantial advantages over single modality approaches in mental health\nassessment. Notably, integrating EEG alongside commonly used LLM modalities\nsuch as audio and images demonstrates promising potential. Moreover, our\nfindings reveal that 1-shot learning offers greater benefits compared to\nzero-shot learning methods.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HB06xJ8TvEMECnU7HTB9_MTiky8y2Mh58y53-CGED3Y","pdfSize":"2647159"}
