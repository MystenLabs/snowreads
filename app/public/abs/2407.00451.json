{"id":"2407.00451","title":"Language-Guided Object-Centric Diffusion Policy for Collision-Aware\n  Robotic Manipulation","authors":"Hang Li, Qian Feng, Zhi Zheng, Jianxiang Feng, Alois Knoll","authorsParsed":[["Li","Hang",""],["Feng","Qian",""],["Zheng","Zhi",""],["Feng","Jianxiang",""],["Knoll","Alois",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 14:35:21 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 21:45:02 GMT"}],"updateDate":"2024-07-08","timestamp":1719671721000,"abstract":"  Learning from demonstrations faces challenges in generalizing beyond the\ntraining data and is fragile even to slight visual variations. To tackle this\nproblem, we introduce Lan-o3dp, a language guided object centric diffusion\npolicy that takes 3d representation of task relevant objects as conditional\ninput and can be guided by cost function for safety constraints at inference\ntime. Lan-o3dp enables strong generalization in various aspects, such as\nbackground changes, visual ambiguity and can avoid novel obstacles that are\nunseen during the demonstration process. Specifically, We first train a\ndiffusion policy conditioned on point clouds of target objects and then harness\na large language model to decompose the user instruction into task related\nunits consisting of target objects and obstacles, which can be used as visual\nobservation for the policy network or converted to a cost function, guiding the\ngeneration of trajectory towards collision free region at test time. Our\nproposed method shows training efficiency and higher success rates compared\nwith the baselines in simulation experiments. In real world experiments, our\nmethod exhibits strong generalization performance towards unseen instances,\ncluttered scenes, scenes of multiple similar objects and demonstrates training\nfree capability of obstacle avoidance.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}