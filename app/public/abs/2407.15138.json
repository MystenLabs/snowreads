{"id":"2407.15138","title":"D$^4$M: Dataset Distillation via Disentangled Diffusion Model","authors":"Duo Su, Junjie Hou, Weizhi Gao, Yingjie Tian, Bowen Tang","authorsParsed":[["Su","Duo",""],["Hou","Junjie",""],["Gao","Weizhi",""],["Tian","Yingjie",""],["Tang","Bowen",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 12:16:20 GMT"}],"updateDate":"2024-07-23","timestamp":1721564180000,"abstract":"  Dataset distillation offers a lightweight synthetic dataset for fast network\ntraining with promising test accuracy. To imitate the performance of the\noriginal dataset, most approaches employ bi-level optimization and the\ndistillation space relies on the matching architecture. Nevertheless, these\napproaches either suffer significant computational costs on large-scale\ndatasets or experience performance decline on cross-architectures. We advocate\nfor designing an economical dataset distillation framework that is independent\nof the matching architectures. With empirical observations, we argue that\nconstraining the consistency of the real and synthetic image spaces will\nenhance the cross-architecture generalization. Motivated by this, we introduce\nDataset Distillation via Disentangled Diffusion Model (D$^4$M), an efficient\nframework for dataset distillation. Compared to architecture-dependent methods,\nD$^4$M employs latent diffusion model to guarantee consistency and incorporates\nlabel information into category prototypes. The distilled datasets are\nversatile, eliminating the need for repeated generation of distinct datasets\nfor various architectures. Through comprehensive experiments, D$^4$M\ndemonstrates superior performance and robust generalization, surpassing the\nSOTA methods across most aspects.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}