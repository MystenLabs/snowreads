{"id":"2408.13787","title":"Mask-Encoded Sparsification: Mitigating Biased Gradients in\n  Communication-Efficient Split Learning","authors":"Wenxuan Zhou, Zhihao Qu, Shen-Huan Lyu, Miao Cai, Baoliu Ye","authorsParsed":[["Zhou","Wenxuan",""],["Qu","Zhihao",""],["Lyu","Shen-Huan",""],["Cai","Miao",""],["Ye","Baoliu",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 09:30:34 GMT"},{"version":"v2","created":"Wed, 18 Sep 2024 06:44:48 GMT"}],"updateDate":"2024-09-19","timestamp":1724578234000,"abstract":"  This paper introduces a novel framework designed to achieve a high\ncompression ratio in Split Learning (SL) scenarios where resource-constrained\ndevices are involved in large-scale model training. Our investigations\ndemonstrate that compressing feature maps within SL leads to biased gradients\nthat can negatively impact the convergence rates and diminish the\ngeneralization capabilities of the resulting models. Our theoretical analysis\nprovides insights into how compression errors critically hinder SL performance,\nwhich previous methodologies underestimate. To address these challenges, we\nemploy a narrow bit-width encoded mask to compensate for the sparsification\nerror without increasing the order of time complexity. Supported by rigorous\ntheoretical analysis, our framework significantly reduces compression errors\nand accelerates the convergence. Extensive experiments also verify that our\nmethod outperforms existing solutions regarding training efficiency and\ncommunication complexity.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}