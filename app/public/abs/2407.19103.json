{"id":"2407.19103","title":"FedAR: Addressing Client Unavailability in Federated Learning with Local\n  Update Approximation and Rectification","authors":"Chutian Jiang, Hansong Zhou, Xiaonan Zhang, and Shayok Chakraborty","authorsParsed":[["Jiang","Chutian",""],["Zhou","Hansong",""],["Zhang","Xiaonan",""],["Chakraborty","Shayok",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 21:56:52 GMT"}],"updateDate":"2024-07-30","timestamp":1722031012000,"abstract":"  Federated learning (FL) enables clients to collaboratively train machine\nlearning models under the coordination of a server in a privacy-preserving\nmanner. One of the main challenges in FL is that the server may not receive\nlocal updates from each client in each round due to client resource limitations\nand intermittent network connectivity. The existence of unavailable clients\nseverely deteriorates the overall FL performance. In this paper, we propose , a\nnovel client update Approximation and Rectification algorithm for FL to address\nthe client unavailability issue. FedAR can get all clients involved in the\nglobal model update to achieve a high-quality global model on the server, which\nalso furnishes accurate predictions for each client. To this end, the server\nuses the latest update from each client as a surrogate for its current update.\nIt then assigns a different weight to each client's surrogate update to derive\nthe global model, in order to guarantee contributions from both available and\nunavailable clients. Our theoretical analysis proves that FedAR achieves\noptimal convergence rates on non-IID datasets for both convex and non-convex\nsmooth loss functions. Extensive empirical studies show that FedAR\ncomprehensively outperforms state-of-the-art FL baselines including FedAvg,\nMIFA, FedVARP and Scaffold in terms of the training loss, test accuracy, and\nbias mitigation. Moreover, FedAR also depicts impressive performance in the\npresence of a large number of clients with severe client unavailability.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}