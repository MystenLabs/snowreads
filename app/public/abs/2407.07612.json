{"id":"2407.07612","title":"Teaching Transformers Causal Reasoning through Axiomatic Training","authors":"Aniket Vashishtha, Abhinav Kumar, Abbavaram Gowtham Reddy, Vineeth N\n  Balasubramanian, Amit Sharma","authorsParsed":[["Vashishtha","Aniket",""],["Kumar","Abhinav",""],["Reddy","Abbavaram Gowtham",""],["Balasubramanian","Vineeth N",""],["Sharma","Amit",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 12:50:44 GMT"}],"updateDate":"2024-07-11","timestamp":1720615844000,"abstract":"  For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since interventional data is costly to generate, we study\nto what extent an agent can learn causal reasoning from passive data.\nSpecifically, we consider an axiomatic training setup where an agent learns\nfrom multiple demonstrations of a causal axiom (or rule), rather than\nincorporating the axiom as an inductive bias or inferring it from data values.\nA key question is whether the agent would learn to generalize from the axiom\ndemonstrations to new scenarios. For example, if a transformer model is trained\non demonstrations of the causal transitivity axiom over small graphs, would it\ngeneralize to applying the transitivity axiom over large graphs? Our results,\nbased on a novel axiomatic training scheme, indicate that such generalization\nis possible. We consider the task of inferring whether a variable causes\nanother variable, given a causal graph structure. We find that a 67 million\nparameter transformer model, when trained on linear causal chains (along with\nsome noisy variations) can generalize well to new kinds of graphs, including\nlonger causal chains, causal chains with reversed order, and graphs with\nbranching; even when it is not explicitly trained for such settings. Our model\nperforms at par (or even better) than many larger language models such as\nGPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework\nprovides a new paradigm of learning causal reasoning from passive data that can\nbe used to learn arbitrary axioms, as long as sufficient demonstrations can be\ngenerated.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Ib9B9hfaq-3hpLgnPKz2jKNCvO8oBHas5yYV_F2feqQ","pdfSize":"473515"}
