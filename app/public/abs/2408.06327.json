{"id":"2408.06327","title":"VisualAgentBench: Towards Large Multimodal Models as Visual Foundation\n  Agents","authors":"Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song,\n  Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, Jiadai Sun, Xinyue Yang, Yu\n  Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu,\n  Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng,\n  Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, Jie Tang","authorsParsed":[["Liu","Xiao",""],["Zhang","Tianjie",""],["Gu","Yu",""],["Iong","Iat Long",""],["Xu","Yifan",""],["Song","Xixuan",""],["Zhang","Shudan",""],["Lai","Hanyu",""],["Liu","Xinyi",""],["Zhao","Hanlin",""],["Sun","Jiadai",""],["Yang","Xinyue",""],["Yang","Yu",""],["Qi","Zehan",""],["Yao","Shuntian",""],["Sun","Xueqiao",""],["Cheng","Siyi",""],["Zheng","Qinkai",""],["Yu","Hao",""],["Zhang","Hanchen",""],["Hong","Wenyi",""],["Ding","Ming",""],["Pan","Lihang",""],["Gu","Xiaotao",""],["Zeng","Aohan",""],["Du","Zhengxiao",""],["Song","Chan Hee",""],["Su","Yu",""],["Dong","Yuxiao",""],["Tang","Jie",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 17:44:17 GMT"}],"updateDate":"2024-08-13","timestamp":1723484657000,"abstract":"  Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}