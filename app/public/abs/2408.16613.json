{"id":"2408.16613","title":"Blending Low and High-Level Semantics of Time Series for Better Masked\n  Time Series Generation","authors":"Johan Vik Mathisen, Erlend Lokna, Daesoo Lee and Erlend Aune","authorsParsed":[["Mathisen","Johan Vik",""],["Lokna","Erlend",""],["Lee","Daesoo",""],["Aune","Erlend",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 15:20:17 GMT"}],"updateDate":"2024-08-30","timestamp":1724944817000,"abstract":"  State-of-the-art approaches in time series generation (TSG), such as\nTimeVQVAE, utilize vector quantization-based tokenization to effectively model\ncomplex distributions of time series. These approaches first learn to transform\ntime series into a sequence of discrete latent vectors, and then a prior model\nis learned to model the sequence. The discrete latent vectors, however, only\ncapture low-level semantics (\\textit{e.g.,} shapes). We hypothesize that\nhigher-fidelity time series can be generated by training a prior model on more\ninformative discrete latent vectors that contain both low and high-level\nsemantics (\\textit{e.g.,} characteristic dynamics). In this paper, we introduce\na novel framework, termed NC-VQVAE, to integrate self-supervised learning into\nthose TSG methods to derive a discrete latent space where low and high-level\nsemantics are captured. Our experimental results demonstrate that NC-VQVAE\nresults in a considerable improvement in the quality of synthetic samples.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"W-YwVV1i3FEISpKxlU27VQyxLKjn2JYf3FjO5_d1xrQ","pdfSize":"10148569"}
