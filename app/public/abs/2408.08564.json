{"id":"2408.08564","title":"Collaborative Cross-modal Fusion with Large Language Model for\n  Recommendation","authors":"Zhongzhou Liu, Hao Zhang, Kuicai Dong, Yuan Fang","authorsParsed":[["Liu","Zhongzhou",""],["Zhang","Hao",""],["Dong","Kuicai",""],["Fang","Yuan",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 06:54:10 GMT"}],"updateDate":"2024-08-19","timestamp":1723791250000,"abstract":"  Despite the success of conventional collaborative filtering (CF) approaches\nfor recommendation systems, they exhibit limitations in leveraging semantic\nknowledge within the textual attributes of users and items. Recent focus on the\napplication of large language models for recommendation (LLM4Rec) has\nhighlighted their capability for effective semantic knowledge capture. However,\nthese methods often overlook the collaborative signals in user behaviors. Some\nsimply instruct-tune a language model, while others directly inject the\nembeddings of a CF-based model, lacking a synergistic fusion of different\nmodalities. To address these issues, we propose a framework of Collaborative\nCross-modal Fusion with Large Language Models, termed CCF-LLM, for\nrecommendation. In this framework, we translate the user-item interactions into\na hybrid prompt to encode both semantic knowledge and collaborative signals,\nand then employ an attentive cross-modal fusion strategy to effectively fuse\nlatent embeddings of both modalities. Extensive experiments demonstrate that\nCCF-LLM outperforms existing methods by effectively utilizing semantic and\ncollaborative signals in the LLM4Rec context.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}