{"id":"2408.02946","title":"Scaling Laws for Data Poisoning in LLMs","authors":"Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam\n  Gleave, Kellin Pelrine","authorsParsed":[["Bowen","Dillon",""],["Murphy","Brendan",""],["Cai","Will",""],["Khachaturov","David",""],["Gleave","Adam",""],["Pelrine","Kellin",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 04:14:29 GMT"},{"version":"v2","created":"Fri, 30 Aug 2024 20:22:18 GMT"}],"updateDate":"2024-09-04","timestamp":1722917669000,"abstract":"  Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\nsignificantly more quickly than smaller LLMs with even minimal data poisoning.\nThese results underscore the need for robust safeguards against data poisoning\nin larger LLMs.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZkHLQPg-8A6M_FK83EU2VCvMM39lABrDgViAwkbeKng","pdfSize":"772791"}
