{"id":"2407.05078","title":"Function and derivative approximation by shallow neural networks","authors":"Yuanyuan Li and Shuai Lu","authorsParsed":[["Li","Yuanyuan",""],["Lu","Shuai",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 13:46:23 GMT"}],"updateDate":"2024-07-09","timestamp":1720273583000,"abstract":"  We investigate a Tikhonov regularization scheme specifically tailored for\nshallow neural networks within the context of solving a classic inverse\nproblem: approximating an unknown function and its derivatives within a unit\ncubic domain based on noisy measurements. The proposed Tikhonov regularization\nscheme incorporates a penalty term that takes three distinct yet intricately\nrelated network (semi)norms: the extended Barron norm, the variation norm, and\nthe Radon-BV seminorm. These choices of the penalty term are contingent upon\nthe specific architecture of the neural network being utilized. We establish\nthe connection between various network norms and particularly trace the\ndependence of the dimensionality index, aiming to deepen our understanding of\nhow these norms interplay with each other. We revisit the universality of\nfunction approximation through various norms, establish rigorous error-bound\nanalysis for the Tikhonov regularization scheme, and explicitly elucidate the\ndependency of the dimensionality index, providing a clearer understanding of\nhow the dimensionality affects the approximation performance and how one\ndesigns a neural network with diverse approximating tasks.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}