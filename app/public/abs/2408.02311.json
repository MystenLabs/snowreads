{"id":"2408.02311","title":"PTM4Tag+: Tag Recommendation of Stack Overflow Posts with Pre-trained\n  Models","authors":"Junda He, Bowen Xu, Zhou Yang, DongGyun Han, Chengran Yang, Jiakun\n  Liu, Zhipeng Zhao, David Lo","authorsParsed":[["He","Junda",""],["Xu","Bowen",""],["Yang","Zhou",""],["Han","DongGyun",""],["Yang","Chengran",""],["Liu","Jiakun",""],["Zhao","Zhipeng",""],["Lo","David",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 08:45:27 GMT"}],"updateDate":"2024-08-06","timestamp":1722847527000,"abstract":"  Stack Overflow is one of the most influential Software Question & Answer\n(SQA) websites, hosting millions of programming-related questions and answers.\nTags play a critical role in efficiently organizing the contents in Stack\nOverflow and are vital to support a range of site operations, e.g., querying\nrelevant content. Poorly selected tags often raise problems like tag ambiguity\nand tag explosion. Thus, a precise and accurate automated tag recommendation\ntechnique is demanded.\n  Inspired by the recent success of pre-trained models (PTMs) in natural\nlanguage processing (NLP), we present PTM4Tag+, a tag recommendation framework\nfor Stack Overflow posts that utilizes PTMs in language modeling. PTM4Tag+ is\nimplemented with a triplet architecture, which considers three key components\nof a post, i.e., Title, Description, and Code, with independent PTMs. We\nutilize a number of popular pre-trained models, including the BERT-based models\n(e.g., BERT, RoBERTa, CodeBERT, BERTOverflow, and ALBERT), and encoder-decoder\nmodels (e.g., PLBART, CoTexT, and CodeT5). Our results show that leveraging\nCodeT5 under the PTM4Tag+ framework achieves the best performance among the\neight considered PTMs and outperforms the state-of-the-art Convolutional Neural\nNetwork-based approach by a substantial margin in terms of average P\nrecision@k, Recall@k, and F1-score@k (k ranges from 1 to 5). Specifically,\nCodeT5 improves the performance of F1-score@1-5 by 8.8%, 12.4%, 15.3%, 16.4%,\nand 16.6%. Moreover, to address the concern with inference latency, we\nexperiment PTM4Tag+ with smaller PTM models (i.e., DistilBERT, DistilRoBERTa,\nCodeBERT-small, and CodeT5-small). We find that although smaller PTMs cannot\noutperform larger PTMs, they still maintain over 93.96% of the performance on\naverage, meanwhile shortening the mean inference time by more than 47.2%\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rxJk8il3otSkaLYXCFpzWNLN4Z6q6LBlMKcaJo4NoJQ","pdfSize":"1036874"}
