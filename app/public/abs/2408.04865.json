{"id":"2408.04865","title":"TEAdapter: Supply abundant guidance for controllable text-to-music\n  generation","authors":"Jialing Zou, Jiahao Mei, Xudong Nan, Jinghua Li, Daoguo Dong, Liang He","authorsParsed":[["Zou","Jialing",""],["Mei","Jiahao",""],["Nan","Xudong",""],["Li","Jinghua",""],["Dong","Daoguo",""],["He","Liang",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 05:04:13 GMT"}],"updateDate":"2024-08-12","timestamp":1723179853000,"abstract":"  Although current text-guided music generation technology can cope with simple\ncreative scenarios, achieving fine-grained control over individual\ntext-modality conditions remains challenging as user demands become more\nintricate. Accordingly, we introduce the TEAcher Adapter (TEAdapter), a compact\nplugin designed to guide the generation process with diverse control\ninformation provided by users. In addition, we explore the controllable\ngeneration of extended music by leveraging TEAdapter control groups trained on\ndata of distinct structural functionalities. In general, we consider controls\nover global, elemental, and structural levels. Experimental results demonstrate\nthat the proposed TEAdapter enables multiple precise controls and ensures\nhigh-quality music generation. Our module is also lightweight and transferable\nto any diffusion model architecture. Available code and demos will be found\nsoon at https://github.com/Ashley1101/TEAdapter.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}