{"id":"2407.01902","title":"SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak\n  Attack","authors":"Yan Yang, Zeguan Xiao, Xin Lu, Hongru Wang, Hailiang Huang, Guanhua\n  Chen, Yun Chen","authorsParsed":[["Yang","Yan",""],["Xiao","Zeguan",""],["Lu","Xin",""],["Wang","Hongru",""],["Huang","Hailiang",""],["Chen","Guanhua",""],["Chen","Yun",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 02:58:29 GMT"}],"updateDate":"2024-07-03","timestamp":1719889109000,"abstract":"  The widespread applications of large language models (LLMs) have brought\nabout concerns regarding their potential misuse. Although aligned with human\npreference data before release, LLMs remain vulnerable to various malicious\nattacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety\nand introduce SoP, a simple yet effective framework to design jailbreak prompts\nautomatically. Inspired by the social facilitation concept, SoP generates and\noptimizes multiple jailbreak characters to bypass the guardrails of the target\nLLM. Different from previous work which relies on proprietary LLMs or seed\njailbreak templates crafted by human expertise, SoP can generate and optimize\nthe jailbreak prompt in a cold-start scenario using open-sourced LLMs without\nany seed jailbreak templates. Experimental results show that SoP achieves\nattack success rates of 88% and 60% in bypassing the safety alignment of\nGPT-3.5-1106 and GPT-4, respectively. Furthermore, we extensively evaluate the\ntransferability of the generated templates across different LLMs and held-out\nmalicious requests, while also exploring defense strategies against the\njailbreak attack designed by SoP. Code is available at\nhttps://github.com/Yang-Yan-Yang-Yan/SoP.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}