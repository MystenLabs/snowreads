{"id":"2407.08946","title":"Your Diffusion Model is Secretly a Noise Classifier and Benefits from\n  Contrastive Training","authors":"Yunshu Wu, Yingtao Luo, Xianghao Kong, Evangelos E. Papalexakis, Greg\n  Ver Steeg","authorsParsed":[["Wu","Yunshu",""],["Luo","Yingtao",""],["Kong","Xianghao",""],["Papalexakis","Evangelos E.",""],["Steeg","Greg Ver",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 03:03:50 GMT"}],"updateDate":"2024-07-15","timestamp":1720753430000,"abstract":"  Diffusion models learn to denoise data and the trained denoiser is then used\nto generate new samples from the data distribution. In this paper, we revisit\nthe diffusion sampling process and identify a fundamental cause of sample\nquality degradation: the denoiser is poorly estimated in regions that are far\nOutside Of the training Distribution (OOD), and the sampling process inevitably\nevaluates in these OOD regions. This can become problematic for all sampling\nmethods, especially when we move to parallel sampling which requires us to\ninitialize and update the entire sample trajectory of dynamics in parallel,\nleading to many OOD evaluations. To address this problem, we introduce a new\nself-supervised training objective that differentiates the levels of noise\nadded to a sample, leading to improved OOD denoising performance. The approach\nis based on our observation that diffusion models implicitly define a\nlog-likelihood ratio that distinguishes distributions with different amounts of\nnoise, and this expression depends on denoiser performance outside the standard\ntraining distribution. We show by diverse experiments that the proposed\ncontrastive diffusion training is effective for both sequential and parallel\nsettings, and it improves the performance and speed of parallel samplers\nsignificantly.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}