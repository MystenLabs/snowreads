{"id":"2408.00161","title":"Automatic Generation of Behavioral Test Cases For Natural Language\n  Processing Using Clustering and Prompting","authors":"Ying Li, Rahul Singh, Tarun Joshi, Agus Sudjianto","authorsParsed":[["Li","Ying",""],["Singh","Rahul",""],["Joshi","Tarun",""],["Sudjianto","Agus",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 21:12:21 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 16:31:05 GMT"}],"updateDate":"2024-08-09","timestamp":1722460341000,"abstract":"  Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Emerging Technologies","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}