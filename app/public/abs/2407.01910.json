{"id":"2407.01910","title":"MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog\n  Generation","authors":"Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, Yingyan Celine Lin","authorsParsed":[["Zhang","Yongan",""],["Yu","Zhongzhi",""],["Fu","Yonggan",""],["Wan","Cheng",""],["Lin","Yingyan Celine",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 03:21:24 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 15:15:20 GMT"}],"updateDate":"2024-07-04","timestamp":1719890484000,"abstract":"  Large Language Models (LLMs) have recently shown promise in streamlining\nhardware design processes by encapsulating vast amounts of domain-specific\ndata. In addition, they allow users to interact with the design processes\nthrough natural language instructions, thus making hardware design more\naccessible to developers. However, effectively leveraging LLMs in hardware\ndesign necessitates providing domain-specific data during inference (e.g.,\nthrough in-context learning), fine-tuning, or pre-training. Unfortunately,\nexisting publicly available hardware datasets are often limited in size,\ncomplexity, or detail, which hinders the effectiveness of LLMs in hardware\ndesign tasks. To address this issue, we first propose a set of criteria for\ncreating high-quality hardware datasets that can effectively enhance\nLLM-assisted hardware design. Based on these criteria, we propose a\nMulti-Grained-Verilog (MG-Verilog) dataset, which encompasses descriptions at\nvarious levels of detail and corresponding code samples. To benefit the broader\nhardware design community, we have developed an open-source infrastructure that\nfacilitates easy access, integration, and extension of the dataset to meet\nspecific project needs. Furthermore, to fully exploit the potential of the\nMG-Verilog dataset, which varies in complexity and detail, we introduce a\nbalanced fine-tuning scheme. This scheme serves as a unique use case to\nleverage the diverse levels of detail provided by the dataset. Extensive\nexperiments demonstrate that the proposed dataset and fine-tuning scheme\nconsistently improve the performance of LLMs in hardware design tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Hardware Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}