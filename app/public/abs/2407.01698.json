{"id":"2407.01698","title":"Column and row subset selection using nuclear scores: algorithms and\n  theory for Nystr\\\"{o}m approximation, CUR decomposition, and graph Laplacian\n  reduction","authors":"Mark Fornace and Michael Lindsey","authorsParsed":[["Fornace","Mark",""],["Lindsey","Michael",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 18:10:19 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 22:31:10 GMT"}],"updateDate":"2024-08-09","timestamp":1719857419000,"abstract":"  Column selection is an essential tool for structure-preserving low-rank\napproximation, with wide-ranging applications across many fields, such as data\nscience, machine learning, and theoretical chemistry. In this work, we develop\nunified methodologies for fast, efficient, and theoretically guaranteed column\nselection. First we derive and implement a sparsity-exploiting deterministic\nalgorithm applicable to tasks including kernel approximation and CUR\ndecomposition. Next, we develop a matrix-free formalism relying on a\nrandomization scheme satisfying guaranteed concentration bounds, applying this\nconstruction both to CUR decomposition and to the approximation of matrix\nfunctions of graph Laplacians. Importantly, the randomization is only relevant\nfor the computation of the scores that we use for column selection, not the\nselection itself given these scores. For both deterministic and matrix-free\nalgorithms, we bound the performance favorably relative to the expected\nperformance of determinantal point process (DPP) sampling and, in select\nscenarios, that of exactly optimal subset selection. The general case requires\nnew analysis of the DPP expectation. Finally, we demonstrate strong real-world\nperformance of our algorithms on a diverse set of example approximation tasks.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}