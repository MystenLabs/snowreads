{"id":"2408.09859","title":"OccMamba: Semantic Occupancy Prediction with State Space Models","authors":"Heng Li, Yuenan Hou, Xiaohan Xing, Xiao Sun, Yanyong Zhang","authorsParsed":[["Li","Heng",""],["Hou","Yuenan",""],["Xing","Xiaohan",""],["Sun","Xiao",""],["Zhang","Yanyong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 10:07:00 GMT"}],"updateDate":"2024-08-20","timestamp":1724062020000,"abstract":"  Training deep learning models for semantic occupancy prediction is\nchallenging due to factors such as a large number of occupancy cells, severe\nocclusion, limited visual cues, complicated driving scenarios, etc. Recent\nmethods often adopt transformer-based architectures given their strong\ncapability in learning input-conditioned weights and long-range relationships.\nHowever, transformer-based networks are notorious for their quadratic\ncomputation complexity, seriously undermining their efficacy and deployment in\nsemantic occupancy prediction. Inspired by the global modeling and linear\ncomputation complexity of the Mamba architecture, we present the first\nMamba-based network for semantic occupancy prediction, termed OccMamba.\nHowever, directly applying the Mamba architecture to the occupancy prediction\ntask yields unsatisfactory performance due to the inherent domain gap between\nthe linguistic and 3D domains. To relieve this problem, we present a simple yet\neffective 3D-to-1D reordering operation, i.e., height-prioritized 2D Hilbert\nexpansion. It can maximally retain the spatial structure of point clouds as\nwell as facilitate the processing of Mamba blocks. Our OccMamba achieves\nstate-of-the-art performance on three prevalent occupancy prediction\nbenchmarks, including OpenOccupancy, SemanticKITTI and SemanticPOSS. Notably,\non OpenOccupancy, our OccMamba outperforms the previous state-of-the-art Co-Occ\nby 3.1% IoU and 3.2% mIoU, respectively. Codes will be released upon\npublication.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}