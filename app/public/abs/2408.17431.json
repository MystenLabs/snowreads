{"id":"2408.17431","title":"Advancing Multi-talker ASR Performance with Large Language Models","authors":"Mohan Shi, Zengrui Jin, Yaoxun Xu, Yong Xu, Shi-Xiong Zhang, Kun Wei,\n  Yiwen Shao, Chunlei Zhang, Dong Yu","authorsParsed":[["Shi","Mohan",""],["Jin","Zengrui",""],["Xu","Yaoxun",""],["Xu","Yong",""],["Zhang","Shi-Xiong",""],["Wei","Kun",""],["Shao","Yiwen",""],["Zhang","Chunlei",""],["Yu","Dong",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 17:29:25 GMT"}],"updateDate":"2024-09-02","timestamp":1725038965000,"abstract":"  Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}