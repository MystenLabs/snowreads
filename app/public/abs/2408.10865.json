{"id":"2408.10865","title":"Multi-agent Multi-armed Bandits with Stochastic Sharable Arm Capacities","authors":"Hong Xie, Jinyu Mo, Defu Lian, Jie Wang, Enhong Chen","authorsParsed":[["Xie","Hong",""],["Mo","Jinyu",""],["Lian","Defu",""],["Wang","Jie",""],["Chen","Enhong",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 13:57:00 GMT"}],"updateDate":"2024-08-21","timestamp":1724162220000,"abstract":"  Motivated by distributed selection problems, we formulate a new variant of\nmulti-player multi-armed bandit (MAB) model, which captures stochastic arrival\nof requests to each arm, as well as the policy of allocating requests to\nplayers. The challenge is how to design a distributed learning algorithm such\nthat players select arms according to the optimal arm pulling profile (an arm\npulling profile prescribes the number of players at each arm) without\ncommunicating to each other. We first design a greedy algorithm, which locates\none of the optimal arm pulling profiles with a polynomial computational\ncomplexity. We also design an iterative distributed algorithm for players to\ncommit to an optimal arm pulling profile with a constant number of rounds in\nexpectation. We apply the explore then commit (ETC) framework to address the\nonline setting when model parameters are unknown. We design an exploration\nstrategy for players to estimate the optimal arm pulling profile. Since such\nestimates can be different across different players, it is challenging for\nplayers to commit. We then design an iterative distributed algorithm, which\nguarantees that players can arrive at a consensus on the optimal arm pulling\nprofile in only M rounds. We conduct experiments to validate our algorithm.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KqPQBZQcsWSHcGcW5GiEc13uSWeOdgRM2YgeZRVSCyQ","pdfSize":"1966410"}
