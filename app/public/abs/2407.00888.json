{"id":"2407.00888","title":"Papez: Resource-Efficient Speech Separation with Auditory Working Memory","authors":"Hyunseok Oh, Juheon Yi, Youngki Lee","authorsParsed":[["Oh","Hyunseok",""],["Yi","Juheon",""],["Lee","Youngki",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 01:23:06 GMT"}],"updateDate":"2024-07-02","timestamp":1719796986000,"abstract":"  Transformer-based models recently reached state-of-the-art single-channel\nspeech separation accuracy; However, their extreme computational load makes it\ndifficult to deploy them in resource-constrained mobile or IoT devices. We thus\npresent Papez, a lightweight and computation-efficient single-channel speech\nseparation model. Papez is based on three key techniques. We first replace the\ninter-chunk Transformer with small-sized auditory working memory. Second, we\nadaptively prune the input tokens that do not need further processing. Finally,\nwe reduce the number of parameters through the recurrent transformer. Our\nextensive evaluation shows that Papez achieves the best resource and accuracy\ntradeoffs with a large margin. We publicly share our source code at\n\\texttt{https://github.com/snuhcs/Papez}\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}