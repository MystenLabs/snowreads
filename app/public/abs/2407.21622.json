{"id":"2407.21622","title":"Extended Fiducial Inference: Toward an Automated Process of Statistical\n  Inference","authors":"Faming Liang, Sehwan Kim, Yan Sun","authorsParsed":[["Liang","Faming",""],["Kim","Sehwan",""],["Sun","Yan",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 14:15:42 GMT"}],"updateDate":"2024-08-01","timestamp":1722435342000,"abstract":"  While fiducial inference was widely considered a big blunder by R.A. Fisher,\nthe goal he initially set --`inferring the uncertainty of model parameters on\nthe basis of observations' -- has been continually pursued by many\nstatisticians. To this end, we develop a new statistical inference method\ncalled extended Fiducial inference (EFI). The new method achieves the goal of\nfiducial inference by leveraging advanced statistical computing techniques\nwhile remaining scalable for big data. EFI involves jointly imputing random\nerrors realized in observations using stochastic gradient Markov chain Monte\nCarlo and estimating the inverse function using a sparse deep neural network\n(DNN). The consistency of the sparse DNN estimator ensures that the uncertainty\nembedded in observations is properly propagated to model parameters through the\nestimated inverse function, thereby validating downstream statistical\ninference. Compared to frequentist and Bayesian methods, EFI offers significant\nadvantages in parameter estimation and hypothesis testing. Specifically, EFI\nprovides higher fidelity in parameter estimation, especially when outliers are\npresent in the observations; and eliminates the need for theoretical reference\ndistributions in hypothesis testing, thereby automating the statistical\ninference process. EFI also provides an innovative framework for\nsemi-supervised learning.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}