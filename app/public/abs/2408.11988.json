{"id":"2408.11988","title":"Distributed-Memory Parallel Algorithms for Sparse Matrix and Sparse\n  Tall-and-Skinny Matrix Multiplication","authors":"Isuru Ranawaka, Md Taufique Hussain, Charles Block, Gerasimos\n  Gerogiannis, Josep Torrellas, Ariful Azad","authorsParsed":[["Ranawaka","Isuru",""],["Hussain","Md Taufique",""],["Block","Charles",""],["Gerogiannis","Gerasimos",""],["Torrellas","Josep",""],["Azad","Ariful",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 20:54:39 GMT"}],"updateDate":"2024-08-23","timestamp":1724273679000,"abstract":"  We consider a sparse matrix-matrix multiplication (SpGEMM) setting where one\nmatrix is square and the other is tall and skinny. This special variant, called\nTS-SpGEMM, has important applications in multi-source breadth-first search,\ninfluence maximization, sparse graph embedding, and algebraic multigrid\nsolvers. Unfortunately, popular distributed algorithms like sparse SUMMA\ndeliver suboptimal performance for TS-SpGEMM. To address this limitation, we\ndevelop a novel distributed-memory algorithm tailored for TS-SpGEMM. Our\napproach employs customized 1D partitioning for all matrices involved and\nleverages sparsity-aware tiling for efficient data transfers. In addition, it\nminimizes communication overhead by incorporating both local and remote\ncomputations. On average, our TS-SpGEMM algorithm attains 5x performance gains\nover 2D and 3D SUMMA. Furthermore, we use our algorithm to implement\nmulti-source breadth-first search and sparse graph embedding algorithms and\ndemonstrate their scalability up to 512 Nodes (or 65,536 cores) on NERSC\nPerlmutter.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DA7duFNzdobuJ08TPtuNlugxW2By61mbxSLrXlBwags","pdfSize":"2078416"}
