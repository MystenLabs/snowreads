{"id":"2408.03746","title":"Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion\n  Posterior Sampling","authors":"Jian Xu, Zhiqi Lin, Shigui Li, Min Chen, Junmei Yang, Delu Zeng, John\n  Paisley","authorsParsed":[["Xu","Jian",""],["Lin","Zhiqi",""],["Li","Shigui",""],["Chen","Min",""],["Yang","Junmei",""],["Zeng","Delu",""],["Paisley","John",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 12:59:58 GMT"}],"updateDate":"2024-08-08","timestamp":1723035598000,"abstract":"  Bayesian Last Layer (BLL) models focus solely on uncertainty in the output\nlayer of neural networks, demonstrating comparable performance to more complex\nBayesian models. However, the use of Gaussian priors for last layer weights in\nBayesian Last Layer (BLL) models limits their expressive capacity when faced\nwith non-Gaussian, outlier-rich, or high-dimensional datasets. To address this\nshortfall, we introduce a novel approach that combines diffusion techniques and\nimplicit priors for variational learning of Bayesian last layer weights. This\nmethod leverages implicit distributions for modeling weight priors in BLL,\ncoupled with diffusion samplers for approximating true posterior predictions,\nthereby establishing a comprehensive Bayesian prior and posterior estimation\nstrategy. By delivering an explicit and computationally efficient variational\nlower bound, our method aims to augment the expressive abilities of BLL models,\nenhancing model accuracy, calibration, and out-of-distribution detection\nproficiency. Through detailed exploration and experimental validation, We\nshowcase the method's potential for improving predictive accuracy and\nuncertainty quantification while ensuring computational efficiency.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}