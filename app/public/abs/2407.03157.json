{"id":"2407.03157","title":"Let the Code LLM Edit Itself When You Edit the Code","authors":"Zhenyu He, Jun Zhang, Shengjie Luo, Jingjing Xu, Zhi Zhang, Di He","authorsParsed":[["He","Zhenyu",""],["Zhang","Jun",""],["Luo","Shengjie",""],["Xu","Jingjing",""],["Zhang","Zhi",""],["He","Di",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:34:03 GMT"}],"updateDate":"2024-07-04","timestamp":1720017243000,"abstract":"  In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}