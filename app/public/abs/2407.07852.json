{"id":"2407.07852","title":"OpenDiLoCo: An Open-Source Framework for Globally Distributed\n  Low-Communication Training","authors":"Sami Jaghouar, Jack Min Ong and Johannes Hagemann","authorsParsed":[["Jaghouar","Sami",""],["Ong","Jack Min",""],["Hagemann","Johannes",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 17:13:17 GMT"}],"updateDate":"2024-07-11","timestamp":1720631597000,"abstract":"  OpenDiLoCo is an open-source implementation and replication of the\nDistributed Low-Communication (DiLoCo) training method for large language\nmodels. We provide a reproducible implementation of the DiLoCo experiments,\noffering it within a scalable, decentralized training framework using the\nHivemind library. We demonstrate its effectiveness by training a model across\ntwo continents and three countries, while maintaining 90-95% compute\nutilization. Additionally, we conduct ablations studies focusing on the\nalgorithm's compute efficiency, scalability in the number of workers and show\nthat its gradients can be all-reduced using FP16 without any performance\ndegradation. Furthermore, we scale OpenDiLoCo to 3x the size of the original\nwork, demonstrating its effectiveness for billion parameter models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ujJrdw6ez-hykdKrJ9HAnoqGjx9lF4q9gfh3xlT-wrY","pdfSize":"931737"}
