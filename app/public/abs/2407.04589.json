{"id":"2407.04589","title":"Remembering Everything Makes You Vulnerable: A Limelight on Machine\n  Unlearning for Personalized Healthcare Sector","authors":"Ahan Chatterjee, Sai Anirudh Aryasomayajula, Rajat Chaudhari, Subhajit\n  Paul, Vishwa Mohan Singh","authorsParsed":[["Chatterjee","Ahan",""],["Aryasomayajula","Sai Anirudh",""],["Chaudhari","Rajat",""],["Paul","Subhajit",""],["Singh","Vishwa Mohan",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 15:38:36 GMT"}],"updateDate":"2024-07-08","timestamp":1720193916000,"abstract":"  As the prevalence of data-driven technologies in healthcare continues to\nrise, concerns regarding data privacy and security become increasingly\nparamount. This thesis aims to address the vulnerability of personalized\nhealthcare models, particularly in the context of ECG monitoring, to\nadversarial attacks that compromise patient privacy. We propose an approach\ntermed \"Machine Unlearning\" to mitigate the impact of exposed data points on\nmachine learning models, thereby enhancing model robustness against adversarial\nattacks while preserving individual privacy. Specifically, we investigate the\nefficacy of Machine Unlearning in the context of personalized ECG monitoring,\nutilizing a dataset of clinical ECG recordings. Our methodology involves\ntraining a deep neural classifier on ECG data and fine-tuning the model for\nindividual patients. We demonstrate the susceptibility of fine-tuned models to\nadversarial attacks, such as the Fast Gradient Sign Method (FGSM), which can\nexploit additional data points in personalized models. To address this\nvulnerability, we propose a Machine Unlearning algorithm that selectively\nremoves sensitive data points from fine-tuned models, effectively enhancing\nmodel resilience against adversarial manipulation. Experimental results\ndemonstrate the effectiveness of our approach in mitigating the impact of\nadversarial attacks while maintaining the pre-trained model accuracy.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}