{"id":"2408.08655","title":"Mitigating Backdoor Attacks in Federated Learning via Flipping Weight\n  Updates of Low-Activation Input Neurons","authors":"Binbin Ding, Penghui Yang, Zeqing Ge, Shengjun Huang","authorsParsed":[["Ding","Binbin",""],["Yang","Penghui",""],["Ge","Zeqing",""],["Huang","Shengjun",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 10:44:14 GMT"}],"updateDate":"2024-08-19","timestamp":1723805054000,"abstract":"  Federated learning enables multiple clients to collaboratively train machine\nlearning models under the overall planning of the server while adhering to\nprivacy requirements. However, the server cannot directly oversee the local\ntraining process, creating an opportunity for malicious clients to introduce\nbackdoors. Existing research shows that backdoor attacks activate specific\nneurons in the compromised model, which remain dormant when processing clean\ndata. Leveraging this insight, we propose a method called Flipping Weight\nUpdates of Low-Activation Input Neurons (FLAIN) to defend against backdoor\nattacks in federated learning. Specifically, after completing global training,\nwe employ an auxiliary dataset to identify low-activation input neurons and\nflip the associated weight updates. We incrementally raise the threshold for\nlow-activation inputs and flip the weight updates iteratively, until the\nperformance degradation on the auxiliary data becomes unacceptable. Extensive\nexperiments validate that our method can effectively reduce the success rate of\nbackdoor attacks to a low level in various attack scenarios including those\nwith non-IID data distribution or high MCRs, causing only minimal performance\ndegradation on clean data.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}