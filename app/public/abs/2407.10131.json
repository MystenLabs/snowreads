{"id":"2407.10131","title":"WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation\n  Models","authors":"Xinjian Wu, Ruisong Zhang, Jie Qin, Shijie Ma, Cheng-Lin Liu","authorsParsed":[["Wu","Xinjian",""],["Zhang","Ruisong",""],["Qin","Jie",""],["Ma","Shijie",""],["Liu","Cheng-Lin",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 09:31:21 GMT"}],"updateDate":"2024-07-16","timestamp":1720949481000,"abstract":"  Segmenting and recognizing diverse object parts is crucial in computer vision\nand robotics. Despite significant progress in object segmentation, part-level\nsegmentation remains underexplored due to complex boundaries and scarce\nannotated data. To address this, we propose a novel Weakly-supervised Part\nSegmentation (WPS) setting and an approach called WPS-SAM, built on the\nlarge-scale pre-trained vision foundation model, Segment Anything Model (SAM).\nWPS-SAM is an end-to-end framework designed to extract prompt tokens directly\nfrom images and perform pixel-level segmentation of part regions. During its\ntraining phase, it only uses weakly supervised labels in the form of bounding\nboxes or points. Extensive experiments demonstrate that, through exploiting the\nrich knowledge embedded in pre-trained foundation models, WPS-SAM outperforms\nother segmentation models trained with pixel-level strong annotations.\nSpecifically, WPS-SAM achieves 68.93% mIOU and 79.53% mACC on the PartImageNet\ndataset, surpassing state-of-the-art fully supervised methods by approximately\n4% in terms of mIOU.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}