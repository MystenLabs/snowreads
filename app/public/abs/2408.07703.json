{"id":"2408.07703","title":"Knowledge Distillation with Refined Logits","authors":"Wujie Sun, Defang Chen, Siwei Lyu, Genlang Chen, Chun Chen, Can Wang","authorsParsed":[["Sun","Wujie",""],["Chen","Defang",""],["Lyu","Siwei",""],["Chen","Genlang",""],["Chen","Chun",""],["Wang","Can",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 17:59:32 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 07:52:15 GMT"}],"updateDate":"2024-08-20","timestamp":1723658372000,"abstract":"  Recent research on knowledge distillation has increasingly focused on logit\ndistillation because of its simplicity, effectiveness, and versatility in model\ncompression. In this paper, we introduce Refined Logit Distillation (RLD) to\naddress the limitations of current logit distillation methods. Our approach is\nmotivated by the observation that even high-performing teacher models can make\nincorrect predictions, creating a conflict between the standard distillation\nloss and the cross-entropy loss. This conflict can undermine the consistency of\nthe student model's learning objectives. Previous attempts to use labels to\nempirically correct teacher predictions may undermine the class correlation. In\ncontrast, our RLD employs labeling information to dynamically refine teacher\nlogits. In this way, our method can effectively eliminate misleading\ninformation from the teacher while preserving crucial class correlations, thus\nenhancing the value and efficiency of distilled knowledge. Experimental results\non CIFAR-100 and ImageNet demonstrate its superiority over existing methods.\nThe code is provided at \\text{https://github.com/zju-SWJ/RLD}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}