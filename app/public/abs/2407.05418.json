{"id":"2407.05418","title":"EMBANet: A Flexible Efffcient Multi-branch Attention Network","authors":"Keke Zu and Hu Zhang and Jian Lu and Lei Zhang and Chen Xu","authorsParsed":[["Zu","Keke",""],["Zhang","Hu",""],["Lu","Jian",""],["Zhang","Lei",""],["Xu","Chen",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 15:50:01 GMT"}],"updateDate":"2024-07-09","timestamp":1720367401000,"abstract":"  This work presents a novel module, namely multi-branch concat (MBC), to\nprocess the input tensor and obtain the multi-scale feature map. The proposed\nMBC module brings new degrees of freedom (DoF) for the design of attention\nnetworks by allowing the type of transformation operators and the number of\nbranches to be flexibly adjusted. Two important transformation operators,\nmultiplex and split, are considered in this work, both of which can represent\nmulti-scale features at a more granular level and increase the range of\nreceptive fields. By integrating the MBC and attention module, a multi-branch\nattention (MBA) module is consequently developed to capture the channel-wise\ninteraction of feature maps for establishing the long-range channel dependency.\nBy substituting the 3x3 convolutions in the bottleneck blocks of the ResNet\nwith the proposed MBA, a novel block namely efficient multi-branch attention\n(EMBA) is obtained, which can be easily plugged into the state-of-the-art\nbackbone CNN models. Furthermore, a new backbone network called EMBANet is\nestablished by stacking the EMBA blocks. The proposed EMBANet is extensively\nevaluated on representative computer vision tasks including: classification,\ndetection, and segmentation. And it demonstrates consistently superior\nperformance over the popular backbones.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}