{"id":"2407.00326","title":"Teola: Towards End-to-End Optimization of LLM-based Applications","authors":"Xin Tan, Yimin Jiang, Yitao Yang and Hong Xu","authorsParsed":[["Tan","Xin",""],["Jiang","Yimin",""],["Yang","Yitao",""],["Xu","Hong",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 05:59:53 GMT"}],"updateDate":"2024-07-02","timestamp":1719640793000,"abstract":"  Large language model (LLM)-based applications consist of both LLM and non-LLM\ncomponents, each contributing to the end-to-end latency. Despite great efforts\nto optimize LLM inference, end-to-end workflow optimization has been\noverlooked. Existing frameworks employ coarse-grained orchestration with task\nmodules, which confines optimizations to within each module and yields\nsuboptimal scheduling decisions. We propose fine-grained end-to-end\norchestration, which utilizes task primitives as the basic units and represents\neach query's workflow as a primitive-level dataflow graph. This explicitly\nexposes a much larger design space, enables optimizations in parallelization\nand pipelining across primitives of different modules, and enhances scheduling\nto improve application-level performance. We build Teola, a novel orchestration\nframework for LLM-based applications that implements this scheme. Comprehensive\nexperiments show that Teola can achieve up to 2.09x speedup over existing\nsystems across various popular LLM applications.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Networking and Internet Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}