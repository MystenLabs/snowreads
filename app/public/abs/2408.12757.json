{"id":"2408.12757","title":"NanoFlow: Towards Optimal Large Language Model Serving Throughput","authors":"Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie,\n  Yufei Gao, Qinyu Xu, Tian Tang, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin,\n  Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci","authorsParsed":[["Zhu","Kan",""],["Zhao","Yilong",""],["Zhao","Liangyu",""],["Zuo","Gefei",""],["Gu","Yile",""],["Xie","Dedong",""],["Gao","Yufei",""],["Xu","Qinyu",""],["Tang","Tian",""],["Ye","Zihao",""],["Kamahori","Keisuke",""],["Lin","Chien-Yu",""],["Wang","Stephanie",""],["Krishnamurthy","Arvind",""],["Kasikci","Baris",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 23:00:40 GMT"}],"updateDate":"2024-08-26","timestamp":1724367640000,"abstract":"  The increasing usage of Large Language Models (LLMs) has resulted in a\nsurging demand for planet-scale serving systems, where tens of thousands of\nGPUs continuously serve hundreds of millions of users. Consequently, throughput\n(under reasonable latency constraints) has emerged as a key metric that\ndetermines serving systems' performance. To boost throughput, various methods\nof inter-device parallelism (e.g., data, tensor, pipeline) have been explored.\nHowever, existing methods do not consider overlapping the utilization of\ndifferent resources within a single device, leading to underutilization and\nsub-optimal performance.\n  We propose NanoFlow, a novel serving framework that exploits intra-device\nparallelism, which overlaps the usage of resources including compute, memory,\nand network within a single device through operation co-scheduling. To exploit\nintra-device parallelism, NanoFlow introduces two key innovations: First,\nNanoFlow splits requests into nano-batches at the granularity of operations,\nwhich breaks the dependency of sequential operations in LLM inference and\nenables overlapping; then, to get benefit from overlapping, NanoFlow uses an\noperation-level pipeline with execution unit scheduling, which partitions the\ndevice's functional units and simultaneously executes different operations in\neach unit. NanoFlow automates the pipeline setup using a parameter search\nalgorithm, which enables easily porting NanoFlow to different models. We\nimplement NanoFlow on NVIDIA GPUs and evaluate end-to-end serving throughput on\nseveral popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc..\nWith practical workloads, NanoFlow provides 1.91x throughput boost compared to\nstate-of-the-art serving systems achieving 59% to 72% of optimal throughput\nacross ported models.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}