{"id":"2408.08408","title":"Oja's plasticity rule overcomes several challenges of training neural\n  networks under biological constraints","authors":"Navid Shervani-Tabar, Marzieh Alireza Mirhoseini, Robert Rosenbaum","authorsParsed":[["Shervani-Tabar","Navid",""],["Mirhoseini","Marzieh Alireza",""],["Rosenbaum","Robert",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 20:26:47 GMT"}],"updateDate":"2024-08-19","timestamp":1723753607000,"abstract":"  There is a large literature on the similarities and differences between\nbiological neural circuits and deep artificial neural networks (DNNs). However,\nmodern training of DNNs relies on several engineering tricks such as data\nbatching, normalization, adaptive optimizers, and precise weight\ninitialization. Despite their critical role in training DNNs, these engineering\ntricks are often overlooked when drawing parallels between biological and\nartificial networks, potentially due to a lack of evidence for their direct\nbiological implementation. In this study, we show that Oja's plasticity rule\npartly overcomes the need for some engineering tricks. Specifically, under\ndifficult, but biologically realistic learning scenarios such as online\nlearning, deep architectures, and sub-optimal weight initialization, Oja's rule\ncan substantially improve the performance of pure backpropagation. Our results\ndemonstrate that simple synaptic plasticity rules can overcome challenges to\nlearning that are typically overcome using less biologically plausible\napproaches when training DNNs.\n","subjects":["Quantitative Biology/Neurons and Cognition","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}