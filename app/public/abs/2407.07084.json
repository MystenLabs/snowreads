{"id":"2407.07084","title":"Stabilized Proximal-Point Methods for Federated Optimization","authors":"Xiaowen Jiang, Anton Rodomanov, Sebastian U. Stich","authorsParsed":[["Jiang","Xiaowen",""],["Rodomanov","Anton",""],["Stich","Sebastian U.",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 17:56:29 GMT"}],"updateDate":"2024-07-10","timestamp":1720547789000,"abstract":"  In developing efficient optimization algorithms, it is crucial to account for\ncommunication constraints -- a significant challenge in modern federated\nlearning settings. The best-known communication complexity among\nnon-accelerated algorithms is achieved by DANE, a distributed proximal-point\nalgorithm that solves local subproblems in each iteration and that can exploit\nsecond-order similarity among individual functions. However, to achieve such\ncommunication efficiency, the accuracy requirement for solving the local\nsubproblems is slightly sub-optimal. Inspired by the hybrid projection-proximal\npoint method, in this work, we i) propose a novel distributed algorithm S-DANE.\nThis method adopts a more stabilized prox-center in the proximal step compared\nwith DANE, and matches its deterministic communication complexity. Moreover,\nthe accuracy condition of the subproblem is milder, leading to enhanced local\ncomputation efficiency. Furthermore, it supports partial client participation\nand arbitrary stochastic local solvers, making it more attractive in practice.\nWe further ii) accelerate S-DANE, and show that the resulting algorithm\nachieves the best-known communication complexity among all existing methods for\ndistributed convex optimization, with the same improved local computation\nefficiency as S-DANE.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}