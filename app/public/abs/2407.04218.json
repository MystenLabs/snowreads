{"id":"2407.04218","title":"Batch Transformer: Look for Attention in Batch","authors":"Myung Beom Her, Jisu Jeong, Hojoon Song, and Ji-Hyeong Han","authorsParsed":[["Her","Myung Beom",""],["Jeong","Jisu",""],["Song","Hojoon",""],["Han","Ji-Hyeong",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 02:13:47 GMT"}],"updateDate":"2024-07-08","timestamp":1720145627000,"abstract":"  Facial expression recognition (FER) has received considerable attention in\ncomputer vision, with \"in-the-wild\" environments such as human-computer\ninteraction. However, FER images contain uncertainties such as occlusion, low\nresolution, pose variation, illumination variation, and subjectivity, which\nincludes some expressions that do not match the target label. Consequently,\nlittle information is obtained from a noisy single image and it is not trusted.\nThis could significantly degrade the performance of the FER task. To address\nthis issue, we propose a batch transformer (BT), which consists of the proposed\nclass batch attention (CBA) module, to prevent overfitting in noisy data and\nextract trustworthy information by training on features reflected from several\nimages in a batch, rather than information from a single image. We also propose\nmulti-level attention (MLA) to prevent overfitting the specific features by\ncapturing correlations between each level. In this paper, we present a batch\ntransformer network (BTN) that combines the above proposals. Experimental\nresults on various FER benchmark datasets show that the proposed BTN\nconsistently outperforms the state-ofthe-art in FER datasets. Representative\nresults demonstrate the promise of the proposed BTN for FER.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"SOLQF2E5wC1Zliqz60JVlyCom6MUHdwpo2qwEKmk6_w","pdfSize":"1293281"}
