{"id":"2407.15083","title":"Rocket Landing Control with Random Annealing Jump Start Reinforcement\n  Learning","authors":"Yuxuan Jiang, Yujie Yang, Zhiqian Lan, Guojian Zhan, Shengbo Eben Li,\n  Qi Sun, Jian Ma, Tianwen Yu, Changwu Zhang","authorsParsed":[["Jiang","Yuxuan",""],["Yang","Yujie",""],["Lan","Zhiqian",""],["Zhan","Guojian",""],["Li","Shengbo Eben",""],["Sun","Qi",""],["Ma","Jian",""],["Yu","Tianwen",""],["Zhang","Changwu",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 07:47:53 GMT"}],"updateDate":"2024-07-23","timestamp":1721548073000,"abstract":"  Rocket recycling is a crucial pursuit in aerospace technology, aimed at\nreducing costs and environmental impact in space exploration. The primary focus\ncenters on rocket landing control, involving the guidance of a nonlinear\nunderactuated rocket with limited fuel in real-time. This challenging task\nprompts the application of reinforcement learning (RL), yet goal-oriented\nnature of the problem poses difficulties for standard RL algorithms due to the\nabsence of intermediate reward signals. This paper, for the first time,\nsignificantly elevates the success rate of rocket landing control from 8% with\na baseline controller to 97% on a high-fidelity rocket model using RL. Our\napproach, called Random Annealing Jump Start (RAJS), is tailored for real-world\ngoal-oriented problems by leveraging prior feedback controllers as guide policy\nto facilitate environmental exploration and policy learning in RL. In each\nepisode, the guide policy navigates the environment for the guide horizon,\nfollowed by the exploration policy taking charge to complete remaining steps.\nThis jump-start strategy prunes exploration space, rendering the problem more\ntractable to RL algorithms. The guide horizon is sampled from a uniform\ndistribution, with its upper bound annealing to zero based on performance\nmetrics, mitigating distribution shift and mismatch issues in existing methods.\nAdditional enhancements, including cascading jump start, refined reward and\nterminal condition, and action smoothness regulation, further improve policy\nperformance and practical applicability. The proposed method is validated\nthrough extensive evaluation and Hardware-in-the-Loop testing, affirming the\neffectiveness, real-time feasibility, and smoothness of the proposed\ncontroller.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}