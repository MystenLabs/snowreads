{"id":"2408.13881","title":"Safe Policy Exploration Improvement via Subgoals","authors":"Brian Angulo, Gregory Gorbov, Aleksandr Panov, Konstantin Yakovlev","authorsParsed":[["Angulo","Brian",""],["Gorbov","Gregory",""],["Panov","Aleksandr",""],["Yakovlev","Konstantin",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 16:12:49 GMT"}],"updateDate":"2024-08-27","timestamp":1724602369000,"abstract":"  Reinforcement learning is a widely used approach to autonomous navigation,\nshowing potential in various tasks and robotic setups. Still, it often\nstruggles to reach distant goals when safety constraints are imposed (e.g., the\nwheeled robot is prohibited from moving close to the obstacles). One of the\nmain reasons for poor performance in such setups, which is common in practice,\nis that the need to respect the safety constraints degrades the exploration\ncapabilities of an RL agent. To this end, we introduce a novel learnable\nalgorithm that is based on decomposing the initial problem into smaller\nsub-problems via intermediate goals, on the one hand, and respects the limit of\nthe cumulative safety constraints, on the other hand -- SPEIS(Safe Policy\nExploration Improvement via Subgoals). It comprises the two coupled policies\ntrained end-to-end: subgoal and safe. The subgoal policy is trained to generate\nthe subgoal based on the transitions from the buffer of the safe (main) policy\nthat helps the safe policy to reach distant goals. Simultaneously, the safe\npolicy maximizes its rewards while attempting not to violate the limit of the\ncumulative safety constraints, thus providing a certain level of safety. We\nevaluate SPEIS in a wide range of challenging (simulated) environments that\ninvolve different types of robots in two different environments: autonomous\nvehicles from the POLAMP environment and car, point, doggo, and sweep from the\nsafety-gym environment. We demonstrate that our method consistently outperforms\nstate-of-the-art competitors and can significantly reduce the collision rate\nwhile maintaining high success rates (higher by 80% compared to the\nbest-performing methods).\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}