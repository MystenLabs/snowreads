{"id":"2407.10633","title":"Evaluating Model Bias Requires Characterizing its Mistakes","authors":"Isabela Albuquerque, Jessica Schrouff, David Warde-Farley, Taylan\n  Cemgil, Sven Gowal, and Olivia Wiles","authorsParsed":[["Albuquerque","Isabela",""],["Schrouff","Jessica",""],["Warde-Farley","David",""],["Cemgil","Taylan",""],["Gowal","Sven",""],["Wiles","Olivia",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 11:46:21 GMT"}],"updateDate":"2024-07-16","timestamp":1721043981000,"abstract":"  The ability to properly benchmark model performance in the face of spurious\ncorrelations is important to both build better predictors and increase\nconfidence that models are operating as intended. We demonstrate that\ncharacterizing (as opposed to simply quantifying) model mistakes across\nsubgroups is pivotal to properly reflect model biases, which are ignored by\nstandard metrics such as worst-group accuracy or accuracy gap. Inspired by the\nhypothesis testing framework, we introduce SkewSize, a principled and flexible\nmetric that captures bias from mistakes in a model's predictions. It can be\nused in multi-class settings or generalised to the open vocabulary setting of\ngenerative models. SkewSize is an aggregation of the effect size of the\ninteraction between two categorical variables: the spurious variable\nrepresenting the bias attribute and the model's prediction. We demonstrate the\nutility of SkewSize in multiple settings including: standard vision models\ntrained on synthetic data, vision models trained on ImageNet, and large scale\nvision-and-language models from the BLIP-2 family. In each case, the proposed\nSkewSize is able to highlight biases not captured by other metrics, while also\nproviding insights on the impact of recently proposed techniques, such as\ninstruction tuning.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}