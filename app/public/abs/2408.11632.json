{"id":"2408.11632","title":"Optimizing Interpretable Decision Tree Policies for Reinforcement\n  Learning","authors":"Dani\\\"el Vos and Sicco Verwer","authorsParsed":[["Vos","DaniÃ«l",""],["Verwer","Sicco",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 14:04:00 GMT"}],"updateDate":"2024-08-22","timestamp":1724249040000,"abstract":"  Reinforcement learning techniques leveraging deep learning have made\ntremendous progress in recent years. However, the complexity of neural networks\nprevents practitioners from understanding their behavior. Decision trees have\ngained increased attention in supervised learning for their inherent\ninterpretability, enabling modelers to understand the exact prediction process\nafter learning. This paper considers the problem of optimizing interpretable\ndecision tree policies to replace neural networks in reinforcement learning\nsettings. Previous works have relaxed the tree structure, restricted to\noptimizing only tree leaves, or applied imitation learning techniques to\napproximately copy the behavior of a neural network policy with a decision\ntree. We propose the Decision Tree Policy Optimization (DTPO) algorithm that\ndirectly optimizes the complete decision tree using policy gradients. Our\ntechnique uses established decision tree heuristics for regression to perform\npolicy optimization. We empirically show that DTPO is a competitive algorithm\ncompared to imitation learning algorithms for optimizing decision tree policies\nin reinforcement learning.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}