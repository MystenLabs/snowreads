{"id":"2408.09886","title":"SAM-UNet:Enhancing Zero-Shot Segmentation of SAM for Universal Medical\n  Images","authors":"Sihan Yang, Haixia Bi, Hai Zhang and Jian Sun","authorsParsed":[["Yang","Sihan",""],["Bi","Haixia",""],["Zhang","Hai",""],["Sun","Jian",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 11:01:00 GMT"}],"updateDate":"2024-08-20","timestamp":1724065260000,"abstract":"  Segment Anything Model (SAM) has demonstrated impressive performance on a\nwide range of natural image segmentation tasks. However, its performance\nsignificantly deteriorates when directly applied to medical domain, due to the\nremarkable differences between natural images and medical images. Some\nresearchers have attempted to train SAM on large scale medical datasets.\nHowever, poor zero-shot performance is observed from the experimental results.\nIn this context, inspired by the superior performance of U-Net-like models in\nmedical image segmentation, we propose SAMUNet, a new foundation model which\nincorporates U-Net to the original SAM, to fully leverage the powerful\ncontextual modeling ability of convolutions. To be specific, we parallel a\nconvolutional branch in the image encoder, which is trained independently with\nthe vision Transformer branch frozen. Additionally, we employ multi-scale\nfusion in the mask decoder, to facilitate accurate segmentation of objects with\ndifferent scales. We train SAM-UNet on SA-Med2D-16M, the largest 2-dimensional\nmedical image segmentation dataset to date, yielding a universal pretrained\nmodel for medical images. Extensive experiments are conducted to evaluate the\nperformance of the model, and state-of-the-art result is achieved, with a dice\nsimilarity coefficient score of 0.883 on SA-Med2D-16M dataset. Specifically, in\nzero-shot segmentation experiments, our model not only significantly\noutperforms previous large medical SAM models across all modalities, but also\nsubstantially mitigates the performance degradation seen on unseen modalities.\nIt should be highlighted that SAM-UNet is an efficient and extensible\nfoundation model, which can be further fine-tuned for other downstream tasks in\nmedical community. The code is available at\nhttps://github.com/Hhankyangg/sam-unet.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}