{"id":"2407.06411","title":"If You Don't Understand It, Don't Use It: Eliminating Trojans with\n  Filters Between Layers","authors":"Adriano Hernandez","authorsParsed":[["Hernandez","Adriano",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 21:40:23 GMT"}],"updateDate":"2024-07-10","timestamp":1720474823000,"abstract":"  Large language models (LLMs) sometimes exhibit dangerous unintended\nbehaviors. Finding and fixing these is challenging because the attack surface\nis massive -- it is not tractable to exhaustively search for all possible\ninputs that may elicit such behavior. One specific and particularly challenging\ncase is that if data-poisoning-injected trojans, since there is no way to know\nwhat they are to search for them. To our knowledge, there is no generally\napplicable method to unlearn unknown trojans injected during pre-training. This\nwork seeks to provide a general purpose recipe (filters) and a specific\nimplementation (LoRA) filters that work in practice on small to medium sized\nmodels. The focus is primarily empirical, though some perplexing behavior opens\nthe door to the fundamental question of how LLMs store and process information.\nNot unexpectedly, we find that our filters work best on the residual stream and\nthe latest layers.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}