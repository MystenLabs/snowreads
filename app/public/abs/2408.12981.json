{"id":"2408.12981","title":"QD-VMR: Query Debiasing with Contextual Understanding Enhancement for\n  Video Moment Retrieval","authors":"Chenghua Gao, Min Li, Jianshuo Liu, Junxing Ren, Lin Chen, Haoyu Liu,\n  Bo Meng, Jitao Fu, Wenwen Su","authorsParsed":[["Gao","Chenghua",""],["Li","Min",""],["Liu","Jianshuo",""],["Ren","Junxing",""],["Chen","Lin",""],["Liu","Haoyu",""],["Meng","Bo",""],["Fu","Jitao",""],["Su","Wenwen",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 10:56:42 GMT"}],"updateDate":"2024-08-26","timestamp":1724410602000,"abstract":"  Video Moment Retrieval (VMR) aims to retrieve relevant moments of an\nuntrimmed video corresponding to the query. While cross-modal interaction\napproaches have shown progress in filtering out query-irrelevant information in\nvideos, they assume the precise alignment between the query semantics and the\ncorresponding video moments, potentially overlooking the misunderstanding of\nthe natural language semantics. To address this challenge, we propose a novel\nmodel called \\textit{QD-VMR}, a query debiasing model with enhanced contextual\nunderstanding. Firstly, we leverage a Global Partial Aligner module via video\nclip and query features alignment and video-query contrastive learning to\nenhance the cross-modal understanding capabilities of the model. Subsequently,\nwe employ a Query Debiasing Module to obtain debiased query features\nefficiently, and a Visual Enhancement module to refine the video features\nrelated to the query. Finally, we adopt the DETR structure to predict the\npossible target video moments. Through extensive evaluations of three benchmark\ndatasets, QD-VMR achieves state-of-the-art performance, proving its potential\nto improve the accuracy of VMR. Further analytical experiments demonstrate the\neffectiveness of our proposed module. Our code will be released to facilitate\nfuture research.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}