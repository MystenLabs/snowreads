{"id":"2408.00791","title":"Improving Audio Spectrogram Transformers for Sound Event Detection\n  Through Multi-Stage Training","authors":"Florian Schmid, Paul Primus, Tobias Morocutti, Jonathan Greif, Gerhard\n  Widmer","authorsParsed":[["Schmid","Florian",""],["Primus","Paul",""],["Morocutti","Tobias",""],["Greif","Jonathan",""],["Widmer","Gerhard",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 20:36:46 GMT"}],"updateDate":"2024-08-05","timestamp":1721248606000,"abstract":"  This technical report describes the CP-JKU team's submission for Task 4 Sound\nEvent Detection with Heterogeneous Training Datasets and Potentially Missing\nLabels of the DCASE 24 Challenge. We fine-tune three large Audio Spectrogram\nTransformers, PaSST, BEATs, and ATST, on the joint DESED and MAESTRO datasets\nin a two-stage training procedure. The first stage closely matches the baseline\nsystem setup and trains a CRNN model while keeping the large pre-trained\ntransformer model frozen. In the second stage, both CRNN and transformer are\nfine-tuned using heavily weighted self-supervised losses. After the second\nstage, we compute strong pseudo-labels for all audio clips in the training set\nusing an ensemble of all three fine-tuned transformers. Then, in a second\niteration, we repeat the two-stage training process and include a distillation\nloss based on the pseudo-labels, boosting single-model performance\nsubstantially. Additionally, we pre-train PaSST and ATST on the subset of\nAudioSet that comes with strong temporal labels, before fine-tuning them on the\nTask 4 datasets.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}