{"id":"2407.03615","title":"Visualizing Dialogues: Enhancing Image Selection through Dialogue\n  Understanding with Large Language Models","authors":"Chang-Sheng Kao, Yun-Nung Chen","authorsParsed":[["Kao","Chang-Sheng",""],["Chen","Yun-Nung",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 03:50:30 GMT"}],"updateDate":"2024-07-08","timestamp":1720065030000,"abstract":"  Recent advancements in dialogue systems have highlighted the significance of\nintegrating multimodal responses, which enable conveying ideas through diverse\nmodalities rather than solely relying on text-based interactions. This\nenrichment not only improves overall communicative efficacy but also enhances\nthe quality of conversational experiences. However, existing methods for\ndialogue-to-image retrieval face limitations due to the constraints of\npre-trained vision language models (VLMs) in comprehending complex dialogues\naccurately. To address this, we present a novel approach leveraging the robust\nreasoning capabilities of large language models (LLMs) to generate precise\ndialogue-associated visual descriptors, facilitating seamless connection with\nimages. Extensive experiments conducted on benchmark data validate the\neffectiveness of our proposed approach in deriving concise and accurate visual\ndescriptors, leading to significant enhancements in dialogue-to-image retrieval\nperformance. Furthermore, our findings demonstrate the method's\ngeneralizability across diverse visual cues, various LLMs, and different\ndatasets, underscoring its practicality and potential impact in real-world\napplications.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}