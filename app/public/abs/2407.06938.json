{"id":"2407.06938","title":"RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models","authors":"Bowen Zhang, Yiji Cheng, Chunyu Wang, Ting Zhang, Jiaolong Yang,\n  Yansong Tang, Feng Zhao, Dong Chen, Baining Guo","authorsParsed":[["Zhang","Bowen",""],["Cheng","Yiji",""],["Wang","Chunyu",""],["Zhang","Ting",""],["Yang","Jiaolong",""],["Tang","Yansong",""],["Zhao","Feng",""],["Chen","Dong",""],["Guo","Baining",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 15:14:45 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 03:46:45 GMT"}],"updateDate":"2024-07-12","timestamp":1720538085000,"abstract":"  We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}