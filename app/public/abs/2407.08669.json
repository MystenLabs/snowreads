{"id":"2407.08669","title":"Segmentation-guided Attention for Visual Question Answering from Remote\n  Sensing Images","authors":"Lucrezia Tosato, Hichem Boussaid, Flora Weissgerber, Camille Kurtz,\n  Laurent Wendling, Sylvain Lobry","authorsParsed":[["Tosato","Lucrezia",""],["Boussaid","Hichem",""],["Weissgerber","Flora",""],["Kurtz","Camille",""],["Wendling","Laurent",""],["Lobry","Sylvain",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 16:59:32 GMT"}],"updateDate":"2024-07-12","timestamp":1720717172000,"abstract":"  Visual Question Answering for Remote Sensing (RSVQA) is a task that aims at\nanswering natural language questions about the content of a remote sensing\nimage. The visual features extraction is therefore an essential step in a VQA\npipeline. By incorporating attention mechanisms into this process, models gain\nthe ability to focus selectively on salient regions of the image, prioritizing\nthe most relevant visual information for a given question. In this work, we\npropose to embed an attention mechanism guided by segmentation into a RSVQA\npipeline. We argue that segmentation plays a crucial role in guiding attention\nby providing a contextual understanding of the visual information, underlying\nspecific objects or areas of interest. To evaluate this methodology, we provide\na new VQA dataset that exploits very high-resolution RGB orthophotos annotated\nwith 16 segmentation classes and question/answer pairs. Our study shows\npromising results of our new methodology, gaining almost 10% of overall\naccuracy compared to a classical method on the proposed dataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}