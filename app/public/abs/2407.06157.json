{"id":"2407.06157","title":"Temporal Grounding of Activities using Multimodal Large Language Models","authors":"Young Chol Song","authorsParsed":[["Song","Young Chol",""]],"versions":[{"version":"v1","created":"Thu, 30 May 2024 09:11:02 GMT"}],"updateDate":"2024-07-09","timestamp":1717060262000,"abstract":"  Temporal grounding of activities, the identification of specific time\nintervals of actions within a larger event context, is a critical task in video\nunderstanding. Recent advancements in multimodal large language models (LLMs)\noffer new opportunities for enhancing temporal reasoning capabilities. In this\npaper, we evaluate the effectiveness of combining image-based and text-based\nlarge language models (LLMs) in a two-stage approach for temporal activity\nlocalization. We demonstrate that our method outperforms existing video-based\nLLMs. Furthermore, we explore the impact of instruction-tuning on a smaller\nmultimodal LLM, showing that refining its ability to process action queries\nleads to more expressive and informative outputs, thereby enhancing its\nperformance in identifying specific time intervals of activities. Our\nexperimental results on the Charades-STA dataset highlight the potential of\nthis approach in advancing the field of temporal activity localization and\nvideo understanding.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}