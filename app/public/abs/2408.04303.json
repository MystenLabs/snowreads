{"id":"2408.04303","title":"Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language\n  Adaptation of LLMs for Low-Resource NLP","authors":"Fran\\c{c}ois Remy, Pieter Delobelle, Hayastan Avetisyan, Alfiya\n  Khabibullina, Miryam de Lhoneux, Thomas Demeester","authorsParsed":[["Remy","Fran√ßois",""],["Delobelle","Pieter",""],["Avetisyan","Hayastan",""],["Khabibullina","Alfiya",""],["de Lhoneux","Miryam",""],["Demeester","Thomas",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 08:37:28 GMT"}],"updateDate":"2024-08-09","timestamp":1723106248000,"abstract":"  The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}