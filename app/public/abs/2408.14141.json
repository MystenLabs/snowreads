{"id":"2408.14141","title":"Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in\n  Subjective Tasks?","authors":"Urja Khurana, Eric Nalisnick, Antske Fokkens, Swabha Swayamdipta","authorsParsed":[["Khurana","Urja",""],["Nalisnick","Eric",""],["Fokkens","Antske",""],["Swayamdipta","Swabha",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 09:37:42 GMT"}],"updateDate":"2024-08-27","timestamp":1724665062000,"abstract":"  Subjective tasks in NLP have been mostly relegated to objective standards,\nwhere the gold label is decided by taking the majority vote. This obfuscates\nannotator disagreement and the inherent uncertainty of the label. We argue that\nsubjectivity should factor into model decisions and play a direct role via\ncalibration under a selective prediction setting. Specifically, instead of\ncalibrating confidence purely from the model's perspective, we calibrate models\nfor subjective tasks based on crowd worker agreement. Our method,\nCrowd-Calibrator, models the distance between the distribution of crowd worker\nlabels and the model's own distribution over labels to inform whether the model\nshould abstain from a decision. On two highly subjective tasks, hate speech\ndetection and natural language inference, our experiments show Crowd-Calibrator\neither outperforms or achieves competitive performance with existing selective\nprediction baselines. Our findings highlight the value of bringing human\ndecision-making into model predictions.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_xli1-5POcKuLvZVdO-LG8XH9NvwMnYOxSMYdCKTC2A","pdfSize":"3293130"}
