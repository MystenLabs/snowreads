{"id":"2407.05733","title":"Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative\n  Judgment Approach Based on Rater Cognition","authors":"Seungju Kim and Meounggun Jo","authorsParsed":[["Kim","Seungju",""],["Jo","Meounggun",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 08:37:00 GMT"}],"updateDate":"2024-07-09","timestamp":1720427820000,"abstract":"  Large Language Models (LLMs) have shown promise in Automated Essay Scoring\n(AES), but their zero-shot and few-shot performance often falls short compared\nto state-of-the-art models and human raters. However, fine-tuning LLMs for each\nspecific task is impractical due to the variety of essay prompts and rubrics\nused in real-world educational contexts. This study proposes a novel approach\ncombining LLMs and Comparative Judgment (CJ) for AES, using zero-shot prompting\nto choose between two essays. We demonstrate that a CJ method surpasses\ntraditional rubric-based scoring in essay scoring using LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"-lAaJKVEL5FSx_8SZHHZv7pVITRlYHwfnyT2YuYrdBo","pdfSize":"1240755"}
