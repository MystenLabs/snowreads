{"id":"2408.08305","title":"Towards Flexible Visual Relationship Segmentation","authors":"Fangrui Zhu, Jianwei Yang, Huaizu Jiang","authorsParsed":[["Zhu","Fangrui",""],["Yang","Jianwei",""],["Jiang","Huaizu",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:57:38 GMT"}],"updateDate":"2024-08-16","timestamp":1723744658000,"abstract":"  Visual relationship understanding has been studied separately in human-object\ninteraction(HOI) detection, scene graph generation(SGG), and referring\nrelationships(RR) tasks. Given the complexity and interconnectedness of these\ntasks, it is crucial to have a flexible framework that can effectively address\nthese tasks in a cohesive manner. In this work, we propose FleVRS, a single\nmodel that seamlessly integrates the above three aspects in standard and\npromptable visual relationship segmentation, and further possesses the\ncapability for open-vocabulary segmentation to adapt to novel scenarios. FleVRS\nleverages the synergy between text and image modalities, to ground various\ntypes of relationships from images and use textual features from\nvision-language models to visual conceptual understanding. Empirical validation\nacross various datasets demonstrates that our framework outperforms existing\nmodels in standard, promptable, and open-vocabulary tasks, e.g., +1.9 $mAP$ on\nHICO-DET, +11.4 $Acc$ on VRD, +4.7 $mAP$ on unseen HICO-DET. Our FleVRS\nrepresents a significant step towards a more intuitive, comprehensive, and\nscalable understanding of visual relationships.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}