{"id":"2407.15662","title":"How to Shrink Confidence Sets for Many Equivalent Discrete\n  Distributions?","authors":"Odalric-Ambrym Maillard and Mohammad Sadegh Talebi","authorsParsed":[["Maillard","Odalric-Ambrym",""],["Talebi","Mohammad Sadegh",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 14:19:19 GMT"}],"updateDate":"2024-07-23","timestamp":1721657959000,"abstract":"  We consider the situation when a learner faces a set of unknown discrete\ndistributions $(p_k)_{k\\in \\mathcal K}$ defined over a common alphabet\n$\\mathcal X$, and can build for each distribution $p_k$ an individual\nhigh-probability confidence set thanks to $n_k$ observations sampled from\n$p_k$. The set $(p_k)_{k\\in \\mathcal K}$ is structured: each distribution $p_k$\nis obtained from the same common, but unknown, distribution q via applying an\nunknown permutation to $\\mathcal X$. We call this\n\\emph{permutation-equivalence}. The goal is to build refined confidence sets\n\\emph{exploiting} this structural property. Like other popular notions of\nstructure (Lipschitz smoothness, Linearity, etc.) permutation-equivalence\nnaturally appears in machine learning problems, and to benefit from its\npotential gain calls for a specific approach. We present a strategy to\neffectively exploit permutation-equivalence, and provide a finite-time\nhigh-probability bound on the size of the refined confidence sets output by the\nstrategy. Since a refinement is not possible for too few observations in\ngeneral, under mild technical assumptions, our finite-time analysis establish\nwhen the number of observations $(n_k)_{k\\in \\mathcal K}$ are large enough so\nthat the output confidence sets improve over initial individual sets. We\ncarefully characterize this event and the corresponding improvement. Further,\nour result implies that the size of confidence sets shrink at asymptotic rates\nof $O(1/\\sqrt{\\sum_{k\\in \\mathcal K} n_k})$ and $O(1/\\max_{k\\in K} n_{k})$,\nrespectively for elements inside and outside the support of q, when the size of\neach individual confidence set shrinks at respective rates of $O(1/\\sqrt{n_k})$\nand $O(1/n_k)$. We illustrate the practical benefit of exploiting permutation\nequivalence on a reinforcement learning task.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}