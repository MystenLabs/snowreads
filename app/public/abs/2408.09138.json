{"id":"2408.09138","title":"StylePrompter: Enhancing Domain Generalization with Test-Time Style\n  Priors","authors":"Jiao Zhang, Jian Xu, Xu-Yao Zhang, Cheng-Lin Liu","authorsParsed":[["Zhang","Jiao",""],["Xu","Jian",""],["Zhang","Xu-Yao",""],["Liu","Cheng-Lin",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 08:35:43 GMT"}],"updateDate":"2024-08-20","timestamp":1723883743000,"abstract":"  In real-world applications, the sample distribution at the inference stage\noften differs from the one at the training stage, causing performance\ndegradation of trained deep models. The research on domain generalization (DG)\naims to develop robust algorithms that can improve the generalized performance\nin unseen domains by training on a few domains. However, the domain-agnostic\nvision model, trained on a limited number of domains using traditional domain\ngeneralization methods, cannot guarantee its effectiveness in dealing with\nunseen domains. The introduction of language can break the closed cognition\nspace of the vision model, providing additional semantic information that\ncannot be inferred from vision-only datasets. In this paper, we propose to\novercome the challenge in previous DG methods by introducing the style prompt\nin the language modality to adapt the trained model dynamically. In particular,\nwe train a style prompter to extract style information of the current image\ninto an embedding in the token embedding space and place it in front of the\ncandidate category words as prior knowledge to prompt the model. Our open space\npartition of the style token embedding space and the hand-crafted style\nregularization enable the trained style prompter to handle data from unknown\ndomains effectively. Extensive experiments verify the effectiveness of our\nmethod and demonstrate state-of-the-art performances on multiple public\ndatasets. Codes will be available after the acceptance of this paper.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}