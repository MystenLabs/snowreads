{"id":"2407.01491","title":"Expressive and Generalizable Low-rank Adaptation for Large Models via\n  Slow Cascaded Learning","authors":"Siwei Li, Yifan Yang, Yifei Shen, Fangyun Wei, Zongqing Lu, Lili Qiu,\n  Yuqing Yang","authorsParsed":[["Li","Siwei",""],["Yang","Yifan",""],["Shen","Yifei",""],["Wei","Fangyun",""],["Lu","Zongqing",""],["Qiu","Lili",""],["Yang","Yuqing",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:28:59 GMT"}],"updateDate":"2024-07-02","timestamp":1719854939000,"abstract":"  Efficient fine-tuning plays a fundamental role in modern large models, with\nlow-rank adaptation emerging as a particularly promising approach. However, the\nexisting variants of LoRA are hampered by limited expressiveness, a tendency to\noverfit, and sensitivity to hyperparameter settings. This paper presents LoRA\nSlow Cascade Learning (LoRASC), an innovative technique designed to enhance\nLoRA's expressiveness and generalization capabilities while preserving its\ntraining efficiency. Our approach augments expressiveness through a cascaded\nlearning strategy that enables a mixture-of-low-rank adaptation, thereby\nincreasing the model's ability to capture complex patterns. Additionally, we\nintroduce a slow-fast update mechanism and cascading noisy tuning to bolster\ngeneralization. The extensive experiments on various language and vision\ndatasets, as well as robustness benchmarks, demonstrate that the proposed\nmethod not only significantly outperforms existing baselines, but also\nmitigates overfitting, enhances model stability, and improves OOD robustness.\nCode will be release in https://github.com/microsoft/LoRASC very soon.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0XYnJ0MQ0rzYGPXOMVFnj-_8S7A2d3ecSYSCyP1Ju2o","pdfSize":"557952"}
