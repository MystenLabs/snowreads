{"id":"2408.07254","title":"Learning Multi-Index Models with Neural Networks via Mean-Field Langevin\n  Dynamics","authors":"Alireza Mousavi-Hosseini and Denny Wu and Murat A. Erdogdu","authorsParsed":[["Mousavi-Hosseini","Alireza",""],["Wu","Denny",""],["Erdogdu","Murat A.",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 02:13:35 GMT"}],"updateDate":"2024-08-15","timestamp":1723601615000,"abstract":"  We study the problem of learning multi-index models in high-dimensions using\na two-layer neural network trained with the mean-field Langevin algorithm.\nUnder mild distributional assumptions on the data, we characterize the\neffective dimension $d_{\\mathrm{eff}}$ that controls both sample and\ncomputational complexity by utilizing the adaptivity of neural networks to\nlatent low-dimensional structures. When the data exhibit such a structure,\n$d_{\\mathrm{eff}}$ can be significantly smaller than the ambient dimension. We\nprove that the sample complexity grows almost linearly with $d_{\\mathrm{eff}}$,\nbypassing the limitations of the information and generative exponents that\nappeared in recent analyses of gradient-based feature learning. On the other\nhand, the computational complexity may inevitably grow exponentially with\n$d_{\\mathrm{eff}}$ in the worst-case scenario. Motivated by improving\ncomputational complexity, we take the first steps towards polynomial time\nconvergence of the mean-field Langevin algorithm by investigating a setting\nwhere the weights are constrained to be on a compact manifold with positive\nRicci curvature, such as the hypersphere. There, we study assumptions under\nwhich polynomial time convergence is achievable, whereas similar assumptions in\nthe Euclidean setting lead to exponential time complexity.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}