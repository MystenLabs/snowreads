{"id":"2407.02943","title":"PII-Compass: Guiding LLM training data extraction prompts towards the\n  target PII via grounding","authors":"Krishna Kanth Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, Xuebing\n  Zhou","authorsParsed":[["Nakka","Krishna Kanth",""],["Frikha","Ahmed",""],["Mendes","Ricardo",""],["Jiang","Xue",""],["Zhou","Xuebing",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 09:20:04 GMT"}],"updateDate":"2024-07-04","timestamp":1719998404000,"abstract":"  The latest and most impactful advances in large models stem from their\nincreased size. Unfortunately, this translates into an improved memorization\ncapacity, raising data privacy concerns. Specifically, it has been shown that\nmodels can output personal identifiable information (PII) contained in their\ntraining data. However, reported PIII extraction performance varies widely, and\nthere is no consensus on the optimal methodology to evaluate this risk,\nresulting in underestimating realistic adversaries. In this work, we\nempirically demonstrate that it is possible to improve the extractability of\nPII by over ten-fold by grounding the prefix of the manually constructed\nextraction prompt with in-domain data. Our approach, PII-Compass, achieves\nphone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308\nqueries, respectively, i.e., the phone number of 1 person in 15 is extractable.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}