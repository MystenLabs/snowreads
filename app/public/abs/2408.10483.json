{"id":"2408.10483","title":"PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series\n  Forecasting","authors":"Yongbo Yu, Weizhong Yu, Feiping Nie, Xuelong Li","authorsParsed":[["Yu","Yongbo",""],["Yu","Weizhong",""],["Nie","Feiping",""],["Li","Xuelong",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 01:56:07 GMT"}],"updateDate":"2024-08-21","timestamp":1724118967000,"abstract":"  The self-attention mechanism in Transformer architecture, invariant to\nsequence order, necessitates positional embeddings to encode temporal order in\ntime series prediction. We argue that this reliance on positional embeddings\nrestricts the Transformer's ability to effectively represent temporal\nsequences, particularly when employing longer lookback windows. To address\nthis, we introduce an innovative approach that combines Pyramid RNN\nembeddings(PRE) for univariate time series with the Transformer's capability to\nmodel multivariate dependencies. PRE, utilizing pyramidal one-dimensional\nconvolutional layers, constructs multiscale convolutional features that\npreserve temporal order. Additionally, RNNs, layered atop these features, learn\nmultiscale time series representations sensitive to sequence order. This\nintegration into Transformer models with attention mechanisms results in\nsignificant performance enhancements. We present the PRformer, a model\nintegrating PRE with a standard Transformer encoder, demonstrating\nstate-of-the-art performance on various real-world datasets. This performance\nhighlights the effectiveness of our approach in leveraging longer lookback\nwindows and underscores the critical role of robust temporal representations in\nmaximizing Transformer's potential for prediction tasks. Code is available at\nthis repository: \\url{https://github.com/usualheart/PRformer}.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}