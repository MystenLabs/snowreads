{"id":"2407.14086","title":"Temporal Correlation Meets Embedding: Towards a 2nd Generation of\n  JDE-based Real-Time Multi-Object Tracking","authors":"Yunfei Zhang, Chao Liang, Jin Gao, Zhipeng Zhang, Weiming Hu, Stephen\n  Maybank, Xue Zhou, Liang Li","authorsParsed":[["Zhang","Yunfei",""],["Liang","Chao",""],["Gao","Jin",""],["Zhang","Zhipeng",""],["Hu","Weiming",""],["Maybank","Stephen",""],["Zhou","Xue",""],["Li","Liang",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 07:48:45 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 09:56:36 GMT"}],"updateDate":"2024-08-07","timestamp":1721375325000,"abstract":"  Joint Detection and Embedding (JDE) trackers have demonstrated excellent\nperformance in Multi-Object Tracking (MOT) tasks by incorporating the\nextraction of appearance features as auxiliary tasks through embedding\nRe-Identification task (ReID) into the detector, achieving a balance between\ninference speed and tracking performance. However, solving the competition\nbetween the detector and the feature extractor has always been a challenge.\nMeanwhile, the issue of directly embedding the ReID task into MOT has remained\nunresolved. The lack of high discriminability in appearance features results in\ntheir limited utility. In this paper, a new learning approach using\ncross-correlation to capture temporal information of objects is proposed. The\nfeature extraction network is no longer trained solely on appearance features\nfrom each frame but learns richer motion features by utilizing feature heatmaps\nfrom consecutive frames, which addresses the challenge of inter-class feature\nsimilarity. Furthermore, our learning approach is applied to a more lightweight\nfeature extraction network, and treat the feature matching scores as strong\ncues rather than auxiliary cues, with an appropriate weight calculation to\nreflect the compatibility between our obtained features and the MOT task. Our\ntracker, named TCBTrack, achieves state-of-the-art performance on multiple\npublic benchmarks, i.e., MOT17, MOT20, and DanceTrack datasets. Specifically,\non the DanceTrack test set, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA,\nmaking it the best online tracker capable of achieving real-time performance.\nComparative evaluations with other trackers prove that our tracker achieves the\nbest balance between speed, robustness and accuracy. Code is available at\nhttps://github.com/yfzhang1214/TCBTrack.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}