{"id":"2407.20947","title":"An Asynchronous Multi-core Accelerator for SNN inference","authors":"Zhuo Chen, De Ma, Xiaofei Jin, Qinghui Xing, Ouwen Jin, Xin Du,\n  Shuibing He and Gang Pan","authorsParsed":[["Chen","Zhuo",""],["Ma","De",""],["Jin","Xiaofei",""],["Xing","Qinghui",""],["Jin","Ouwen",""],["Du","Xin",""],["He","Shuibing",""],["Pan","Gang",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 16:25:38 GMT"}],"updateDate":"2024-07-31","timestamp":1722356738000,"abstract":"  Spiking Neural Networks (SNNs) are extensively utilized in brain-inspired\ncomputing and neuroscience research. To enhance the speed and energy efficiency\nof SNNs, several many-core accelerators have been developed. However,\nmaintaining the accuracy of SNNs often necessitates frequent explicit\nsynchronization among all cores, which presents a challenge to overall\nefficiency. In this paper, we propose an asynchronous architecture for Spiking\nNeural Networks (SNNs) that eliminates the need for inter-core synchronization,\nthus enhancing speed and energy efficiency. This approach leverages the\npre-determined dependencies of neuromorphic cores established during\ncompilation. Each core is equipped with a scheduler that monitors the status of\nits dependencies, allowing it to safely advance to the next timestep without\nwaiting for other cores. This eliminates the necessity for global\nsynchronization and minimizes core waiting time despite inherent workload\nimbalances. Comprehensive evaluations using five different SNN workloads show\nthat our architecture achieves a 1.86x speedup and a 1.55x increase in energy\nefficiency compared to state-of-the-art synchronization architectures.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}