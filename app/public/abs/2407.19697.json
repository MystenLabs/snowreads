{"id":"2407.19697","title":"Multiscale Representation Enhanced Temporal Flow Fusion Model for\n  Long-Term Workload Forecasting","authors":"Shiyu Wang, Zhixuan Chu, Yinbo Sun, Yu Liu, Yuliang Guo, Yang Chen,\n  Huiyang Jian, Lintao Ma, Xingyu Lu, Jun Zhou","authorsParsed":[["Wang","Shiyu",""],["Chu","Zhixuan",""],["Sun","Yinbo",""],["Liu","Yu",""],["Guo","Yuliang",""],["Chen","Yang",""],["Jian","Huiyang",""],["Ma","Lintao",""],["Lu","Xingyu",""],["Zhou","Jun",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 04:42:18 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 02:13:57 GMT"}],"updateDate":"2024-08-20","timestamp":1722228138000,"abstract":"  Accurate workload forecasting is critical for efficient resource management\nin cloud computing systems, enabling effective scheduling and autoscaling.\nDespite recent advances with transformer-based forecasting models, challenges\nremain due to the non-stationary, nonlinear characteristics of workload time\nseries and the long-term dependencies. In particular, inconsistent performance\nbetween long-term history and near-term forecasts hinders long-range\npredictions. This paper proposes a novel framework leveraging self-supervised\nmultiscale representation learning to capture both long-term and near-term\nworkload patterns. The long-term history is encoded through multiscale\nrepresentations while the near-term observations are modeled via temporal flow\nfusion. These representations of different scales are fused using an attention\nmechanism and characterized with normalizing flows to handle\nnon-Gaussian/non-linear distributions of time series. Extensive experiments on\n9 benchmarks demonstrate superiority over existing methods.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}