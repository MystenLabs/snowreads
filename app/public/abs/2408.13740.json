{"id":"2408.13740","title":"PIE: Parkour with Implicit-Explicit Learning Framework for Legged Robots","authors":"Shixin Luo, Songbo Li, Ruiqi Yu, Zhicheng Wang, Jun Wu, Qiuguo Zhu","authorsParsed":[["Luo","Shixin",""],["Li","Songbo",""],["Yu","Ruiqi",""],["Wang","Zhicheng",""],["Wu","Jun",""],["Zhu","Qiuguo",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 07:01:37 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 08:05:45 GMT"},{"version":"v3","created":"Tue, 3 Sep 2024 07:34:49 GMT"}],"updateDate":"2024-09-04","timestamp":1724569297000,"abstract":"  Parkour presents a highly challenging task for legged robots, requiring them\nto traverse various terrains with agile and smooth locomotion. This\nnecessitates comprehensive understanding of both the robot's own state and the\nsurrounding terrain, despite the inherent unreliability of robot perception and\nactuation. Current state-of-the-art methods either rely on complex pre-trained\nhigh-level terrain reconstruction modules or limit the maximum potential of\nrobot parkour to avoid failure due to inaccurate perception. In this paper, we\npropose a one-stage end-to-end learning-based parkour framework: Parkour with\nImplicit-Explicit learning framework for legged robots (PIE) that leverages\ndual-level implicit-explicit estimation. With this mechanism, even a low-cost\nquadruped robot equipped with an unreliable egocentric depth camera can achieve\nexceptional performance on challenging parkour terrains using a relatively\nsimple training process and reward function. While the training process is\nconducted entirely in simulation, our real-world validation demonstrates\nsuccessful zero-shot deployment of our framework, showcasing superior parkour\nperformance on harsh terrains.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}