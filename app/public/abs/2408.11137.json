{"id":"2408.11137","title":"Exploring the use of Generative AI to Support Automated Just-in-Time\n  Programming for Visual Scene Displays","authors":"Cynthia Zastudil, Christine Holyfield, Christine Kapp, Xandria\n  Crosland, Elizabeth Lorah, Tara Zimmerman, Stephen MacNeil","authorsParsed":[["Zastudil","Cynthia",""],["Holyfield","Christine",""],["Kapp","Christine",""],["Crosland","Xandria",""],["Lorah","Elizabeth",""],["Zimmerman","Tara",""],["MacNeil","Stephen",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 18:41:05 GMT"}],"updateDate":"2024-08-22","timestamp":1724179265000,"abstract":"  Millions of people worldwide rely on alternative and augmentative\ncommunication devices to communicate. Visual scene displays (VSDs) can enhance\ncommunication for these individuals by embedding communication options within\ncontextualized images. However, existing VSDs often present default images that\nmay lack relevance or require manual configuration, placing a significant\nburden on communication partners. In this study, we assess the feasibility of\nleveraging large multimodal models (LMM), such as GPT-4V, to automatically\ncreate communication options for VSDs. Communication options were sourced from\na LMM and speech-language pathologists (SLPs) and AAC researchers (N=13) for\nevaluation through an expert assessment conducted by the SLPs and AAC\nresearchers. We present the study's findings, supplemented by insights from\nsemi-structured interviews (N=5) about SLP's and AAC researchers' opinions on\nthe use of generative AI in augmentative and alternative communication devices.\nOur results indicate that the communication options generated by the LMM were\ncontextually relevant and often resembled those created by humans. However,\nvital questions remain that must be addressed before LMMs can be confidently\nimplemented in AAC devices.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}