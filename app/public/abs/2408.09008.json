{"id":"2408.09008","title":"Approximations to worst-case data dropping: unmasking failure modes","authors":"Jenny Y. Huang, David R. Burt, Tin D. Nguyen, Yunyi Shen, and Tamara\n  Broderick","authorsParsed":[["Huang","Jenny Y.",""],["Burt","David R.",""],["Nguyen","Tin D.",""],["Shen","Yunyi",""],["Broderick","Tamara",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 20:21:59 GMT"}],"updateDate":"2024-08-20","timestamp":1723839719000,"abstract":"  A data analyst might worry about generalization if dropping a very small\nfraction of data points from a study could change its substantive conclusions.\nFinding the worst-case data subset to drop poses a combinatorial optimization\nproblem. To overcome this intractability, recent works propose using additive\napproximations, which treat the contribution of a collection of data points as\nthe sum of their individual contributions, and greedy approximations, which\niteratively select the point with the highest impact to drop and re-run the\ndata analysis without that point [Broderick et al., 2020, Kuschnig et al.,\n2021]. We identify that, even in a setting as simple as OLS linear regression,\nmany of these approximations can break down in realistic data arrangements.\nSeveral of our examples reflect masking, where one outlier may hide or conceal\nthe effect of another outlier. Based on the failures we identify, we provide\nrecommendations for users and suggest directions for future improvements.\n","subjects":["Statistics/Methodology","Statistics/Computation"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}