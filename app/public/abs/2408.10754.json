{"id":"2408.10754","title":"Ghost Echoes Revealed: Benchmarking Maintainability Metrics and Machine\n  Learning Predictions Against Human Assessments","authors":"Markus Borg, Marwa Ezzouhri, Adam Tornhill","authorsParsed":[["Borg","Markus",""],["Ezzouhri","Marwa",""],["Tornhill","Adam",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 11:37:30 GMT"}],"updateDate":"2024-08-21","timestamp":1724153850000,"abstract":"  As generative AI is expected to increase global code volumes, the importance\nof maintainability from a human perspective will become even greater. Various\nmethods have been developed to identify the most important maintainability\nissues, including aggregated metrics and advanced Machine Learning (ML) models.\nThis study benchmarks several maintainability prediction approaches, including\nState-of-the-Art (SotA) ML, SonarQube's Maintainability Rating, CodeScene's\nCode Health, and Microsoft's Maintainability Index. Our results indicate that\nCodeScene matches the accuracy of SotA ML and outperforms the average human\nexpert. Importantly, unlike SotA ML, CodeScene also provides end users with\nactionable code smell details to remedy identified issues. Finally, caution is\nadvised with SonarQube due to its tendency to generate many false positives.\nUnfortunately, our findings call into question the validity of previous studies\nthat solely relied on SonarQube output for establishing ground truth labels. To\nimprove reliability in future maintainability and technical debt studies, we\nrecommend employing more accurate metrics. Moreover, reevaluating previous\nfindings with Code Health would mitigate this revealed validity threat.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}