{"id":"2407.17280","title":"Enhanced Feature Learning via Regularisation: Integrating Neural\n  Networks and Kernel Methods","authors":"Bertille Follain and Francis Bach","authorsParsed":[["Follain","Bertille",""],["Bach","Francis",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 13:46:50 GMT"}],"updateDate":"2024-07-25","timestamp":1721828810000,"abstract":"  We propose a new method for feature learning and function estimation in\nsupervised learning via regularised empirical risk minimisation. Our approach\nconsiders functions as expectations of Sobolev functions over all possible\none-dimensional projections of the data. This framework is similar to kernel\nridge regression, where the kernel is $\\mathbb{E}_w ( k^{(B)}(w^\\top x,w^\\top\nx^\\prime))$, with $k^{(B)}(a,b) := \\min(|a|, |b|)1_{ab>0}$ the Brownian kernel,\nand the distribution of the projections $w$ is learnt. This can also be viewed\nas an infinite-width one-hidden layer neural network, optimising the first\nlayer's weights through gradient descent and explicitly adjusting the\nnon-linearity and weights of the second layer. We introduce an efficient\ncomputation method for the estimator, called Brownian Kernel Neural Network\n(BKerNN), using particles to approximate the expectation. The optimisation is\nprincipled due to the positive homogeneity of the Brownian kernel. Using\nRademacher complexity, we show that BKerNN's expected risk converges to the\nminimal risk with explicit high-probability rates of $O( \\min((d/n)^{1/2},\nn^{-1/6}))$ (up to logarithmic factors). Numerical experiments confirm our\noptimisation intuitions, and BKerNN outperforms kernel ridge regression, and\nfavourably compares to a one-hidden layer neural network with ReLU activations\nin various settings and real data sets.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}