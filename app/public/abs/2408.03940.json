{"id":"2408.03940","title":"How Well Can Vision Language Models See Image Details?","authors":"Chenhui Gou, Abdulwahab Felemban, Faizan Farooq Khan, Deyao Zhu,\n  Jianfei Cai, Hamid Rezatofighi, Mohamed Elhoseiny","authorsParsed":[["Gou","Chenhui",""],["Felemban","Abdulwahab",""],["Khan","Faizan Farooq",""],["Zhu","Deyao",""],["Cai","Jianfei",""],["Rezatofighi","Hamid",""],["Elhoseiny","Mohamed",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 17:59:40 GMT"}],"updateDate":"2024-08-08","timestamp":1723053580000,"abstract":"  Large Language Model-based Vision-Language Models (LLM-based VLMs) have\ndemonstrated impressive results in various vision-language understanding tasks.\nHowever, how well these VLMs can see image detail beyond the semantic level\nremains unclear. In our study, we introduce a pixel value prediction task (PVP)\nto explore \"How Well Can Vision Language Models See Image Details?\" and to\nassist VLMs in perceiving more details. Typically, these models comprise a\nfrozen CLIP visual encoder, a large language model, and a connecting module.\nAfter fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to\npredict precise pixel values by only fine-tuning the connection module and LLM;\nand 2) prediction precision is significantly improved when the vision encoder\nis also adapted. Additionally, our research reveals that incorporating pixel\nvalue prediction as one of the VLM pre-training tasks and vision encoder\nadaptation markedly boosts VLM performance on downstream image-language\nunderstanding tasks requiring detailed image perception, such as referring\nimage segmentation (with an average +10.19 cIoU improvement) and video game\ndecision making (with average score improvements of +80.34 and +70.54 on two\ngames, respectively).\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}