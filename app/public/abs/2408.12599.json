{"id":"2408.12599","title":"Controllable Text Generation for Large Language Models: A Survey","authors":"Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang,\n  Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, Zhiyu Li","authorsParsed":[["Liang","Xun",""],["Wang","Hanyu",""],["Wang","Yezhaohui",""],["Song","Shichao",""],["Yang","Jiawei",""],["Niu","Simin",""],["Hu","Jie",""],["Liu","Dan",""],["Yao","Shunyu",""],["Xiong","Feiyu",""],["Li","Zhiyu",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 17:59:04 GMT"}],"updateDate":"2024-08-23","timestamp":1724349544000,"abstract":"  In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}