{"id":"2408.11046","title":"Inside the Black Box: Detecting Data Leakage in Pre-trained Language\n  Encoders","authors":"Yuan Xin, Zheng Li, Ning Yu, Dingfan Chen, Mario Fritz, Michael Backes\n  and Yang Zhang","authorsParsed":[["Xin","Yuan",""],["Li","Zheng",""],["Yu","Ning",""],["Chen","Dingfan",""],["Fritz","Mario",""],["Backes","Michael",""],["Zhang","Yang",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 17:55:15 GMT"}],"updateDate":"2024-08-21","timestamp":1724176515000,"abstract":"  Despite being prevalent in the general field of Natural Language Processing\n(NLP), pre-trained language models inherently carry privacy and copyright\nconcerns due to their nature of training on large-scale web-scraped data. In\nthis paper, we pioneer a systematic exploration of such risks associated with\npre-trained language encoders, specifically focusing on the membership leakage\nof pre-training data exposed through downstream models adapted from pre-trained\nlanguage encoders-an aspect largely overlooked in existing literature. Our\nstudy encompasses comprehensive experiments across four types of pre-trained\nencoder architectures, three representative downstream tasks, and five\nbenchmark datasets. Intriguingly, our evaluations reveal, for the first time,\nthe existence of membership leakage even when only the black-box output of the\ndownstream model is exposed, highlighting a privacy risk far greater than\npreviously assumed. Alongside, we present in-depth analysis and insights toward\nguiding future researchers and practitioners in addressing the privacy\nconsiderations in developing pre-trained language models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}