{"id":"2408.16827","title":"Fluent and Accurate Image Captioning with a Self-Trained Reward Model","authors":"Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara","authorsParsed":[["Moratelli","Nicholas",""],["Cornia","Marcella",""],["Baraldi","Lorenzo",""],["Cucchiara","Rita",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 18:00:03 GMT"}],"updateDate":"2024-09-02","timestamp":1724954403000,"abstract":"  Fine-tuning image captioning models with hand-crafted rewards like the CIDEr\nmetric has been a classical strategy for promoting caption quality at the\nsequence level. This approach, however, is known to limit descriptiveness and\nsemantic richness and tends to drive the model towards the style of\nground-truth sentences, thus losing detail and specificity. On the contrary,\nrecent attempts to employ image-text models like CLIP as reward have led to\ngrammatically incorrect and repetitive captions. In this paper, we propose\nSelf-Cap, a captioning approach that relies on a learnable reward model based\non self-generated negatives that can discriminate captions based on their\nconsistency with the image. Specifically, our discriminator is a fine-tuned\ncontrastive image-text model trained to promote caption correctness while\navoiding the aberrations that typically happen when training with a CLIP-based\nreward. To this end, our discriminator directly incorporates negative samples\nfrom a frozen captioner, which significantly improves the quality and richness\nof the generated captions but also reduces the fine-tuning time in comparison\nto using the CIDEr score as the sole metric for optimization. Experimental\nresults demonstrate the effectiveness of our training strategy on both standard\nand zero-shot image captioning datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}