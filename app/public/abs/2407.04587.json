{"id":"2407.04587","title":"Multimodal Classification via Modal-Aware Interactive Enhancement","authors":"Qing-Yuan Jiang, Zhouyang Chi and Yang Yang","authorsParsed":[["Jiang","Qing-Yuan",""],["Chi","Zhouyang",""],["Yang","Yang",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 15:32:07 GMT"}],"updateDate":"2024-07-08","timestamp":1720193527000,"abstract":"  Due to the notorious modality imbalance problem, multimodal learning (MML)\nleads to the phenomenon of optimization imbalance, thus struggling to achieve\nsatisfactory performance. Recently, some representative methods have been\nproposed to boost the performance, mainly focusing on adaptive adjusting the\noptimization of each modality to rebalance the learning speed of dominant and\nnon-dominant modalities. To better facilitate the interaction of model\ninformation in multimodal learning, in this paper, we propose a novel\nmultimodal learning method, called modal-aware interactive enhancement (MIE).\nSpecifically, we first utilize an optimization strategy based on sharpness\naware minimization (SAM) to smooth the learning objective during the forward\nphase. Then, with the help of the geometry property of SAM, we propose a\ngradient modification strategy to impose the influence between different\nmodalities during the backward phase. Therefore, we can improve the\ngeneralization ability and alleviate the modality forgetting phenomenon\nsimultaneously for multimodal learning. Extensive experiments on widely used\ndatasets demonstrate that our proposed method can outperform various\nstate-of-the-art baselines to achieve the best performance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}