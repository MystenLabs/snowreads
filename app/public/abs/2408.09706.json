{"id":"2408.09706","title":"MePT: Multi-Representation Guided Prompt Tuning for Vision-Language\n  Model","authors":"Xinyang Wang, Yi Yang, Minfeng Zhu, Kecheng Zheng, Shi Liu, Wei Chen","authorsParsed":[["Wang","Xinyang",""],["Yang","Yi",""],["Zhu","Minfeng",""],["Zheng","Kecheng",""],["Liu","Shi",""],["Chen","Wei",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 05:42:00 GMT"}],"updateDate":"2024-08-20","timestamp":1724046120000,"abstract":"  Recent advancements in pre-trained Vision-Language Models (VLMs) have\nhighlighted the significant potential of prompt tuning for adapting these\nmodels to a wide range of downstream tasks. However, existing prompt tuning\nmethods typically map an image to a single representation, limiting the model's\nability to capture the diverse ways an image can be described. To address this\nlimitation, we investigate the impact of visual prompts on the model's\ngeneralization capability and introduce a novel method termed\nMulti-Representation Guided Prompt Tuning (MePT). Specifically, MePT employs a\nthree-branch framework that focuses on diverse salient regions, uncovering the\ninherent knowledge within images which is crucial for robust generalization.\nFurther, we employ efficient self-ensemble techniques to integrate these\nversatile image representations, allowing MePT to learn all conditional,\nmarginal, and fine-grained distributions effectively. We validate the\neffectiveness of MePT through extensive experiments, demonstrating significant\nimprovements on both base-to-novel class prediction and domain generalization\ntasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}