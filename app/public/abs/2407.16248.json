{"id":"2407.16248","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","authors":"Xiaowan Hu, Yiyi Chen, Yan Li, Minquan Wang, Haoqian Wang, Quan Chen,\n  Han Li, Peng Jiang","authorsParsed":[["Hu","Xiaowan",""],["Chen","Yiyi",""],["Li","Yan",""],["Wang","Minquan",""],["Wang","Haoqian",""],["Chen","Quan",""],["Li","Han",""],["Jiang","Peng",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 07:36:54 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 05:56:55 GMT"},{"version":"v3","created":"Mon, 5 Aug 2024 09:05:59 GMT"}],"updateDate":"2024-08-06","timestamp":1721720214000,"abstract":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\nhttps://github.com/Huxiaowan/SGMN.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}