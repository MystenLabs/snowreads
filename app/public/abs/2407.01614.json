{"id":"2407.01614","title":"Enhancing Stability for Large Models Training in Constrained Bandwidth\n  Networks","authors":"Yun Dai, Tejas Dharamsi, Byron Hsu, Tao Song, Hamed Firooz","authorsParsed":[["Dai","Yun",""],["Dharamsi","Tejas",""],["Hsu","Byron",""],["Song","Tao",""],["Firooz","Hamed",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 01:46:10 GMT"},{"version":"v2","created":"Thu, 1 Aug 2024 02:56:58 GMT"}],"updateDate":"2024-08-02","timestamp":1719539170000,"abstract":"  Training extremely large language models with billions of parameters is a\ncomputationally intensive task that pushes the limits of current data parallel\ntraining systems. While techniques like ZeRO++ have enabled efficient\ndistributed training of such giant models on inexpensive low-bandwidth\nclusters, they can suffer from convergence issues due to potential race\nconditions in the hierarchical partitioning (hpZ) scheme employed to reduce\ncross-machine communication. In this work, we first show how these race\nconditions cause instability when training models with billions of parameters.\nWe then propose a modification to the partitioning algorithm that addresses\nthese convergence challenges while maintaining competitive training efficiency.\nEmpirical evaluation on training the multi-billion parameters Falcon Models and\nLlama-2 models demonstrates the updated algorithm's ability to achieve reliable\nconvergence on these massive models, where stock ZeRO++ hpZ fails to converge.\nThe updated algorithm enables robust training of larger models with 98\\%\nthroughput and model training speed improvement without sacrificing the quality\nof convergence.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}