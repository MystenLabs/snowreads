{"id":"2408.16978","title":"Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer","authors":"Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, Aamir\n  Shafi, Hari Subramoni, Dhabaleswar K. Panda","authorsParsed":[["Yao","Jinghan",""],["Jacobs","Sam Ade",""],["Tanaka","Masahiro",""],["Ruwase","Olatunji",""],["Shafi","Aamir",""],["Subramoni","Hari",""],["Panda","Dhabaleswar K.",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 02:44:26 GMT"}],"updateDate":"2024-09-02","timestamp":1724985866000,"abstract":"  Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WRXMiyX1BqG7LX6rOGNsvq31Ivv4QqCVWZ3eviYGr5w","pdfSize":"5240747"}
