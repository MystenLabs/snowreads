{"id":"2408.04278","title":"LaDiMo: Layer-wise Distillation Inspired MoEfier","authors":"Sungyoon Kim, Youngjun Kim, Kihyo Moon, Minsung Jang","authorsParsed":[["Kim","Sungyoon",""],["Kim","Youngjun",""],["Moon","Kihyo",""],["Jang","Minsung",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 07:37:26 GMT"}],"updateDate":"2024-08-09","timestamp":1723102646000,"abstract":"  The advent of large language models has revolutionized natural language\nprocessing, but their increasing complexity has led to substantial training\ncosts, resource demands, and environmental impacts. In response, sparse\nMixture-of-Experts (MoE) models have emerged as a promising alternative to\ndense models. Since training MoE models from scratch can be prohibitively\nexpensive, recent studies have explored leveraging knowledge from pre-trained\nnon-MoE models. However, existing approaches have limitations, such as\nrequiring significant hardware resources and data. We propose a novel\nalgorithm, LaDiMo, which efficiently converts a Transformer-based non-MoE model\ninto a MoE model with minimal additional training cost. LaDiMo consists of two\nstages: layer-wise expert construction and routing policy decision. By\nharnessing the concept of Knowledge Distillation, we compress the model and\nrapidly recover its performance. Furthermore, we develop an adaptive router\nthat optimizes inference efficiency by profiling the distribution of routing\nweights and determining a layer-wise policy that balances accuracy and latency.\nWe demonstrate the effectiveness of our method by converting the LLaMA2-7B\nmodel to a MoE model using only 100K tokens, reducing activated parameters by\nover 20% while keeping accuracy. Our approach offers a flexible and efficient\nsolution for building and deploying MoE models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"WCfHKpiuujmeWO6nGOvzlBG44MR2cCebPPkPKFR5mnY","pdfSize":"1129570"}
