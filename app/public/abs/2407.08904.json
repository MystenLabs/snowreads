{"id":"2407.08904","title":"Improving the communication in decentralized manifold optimization\n  through single-step consensus and compression","authors":"Jiang Hu, Kangkang Deng","authorsParsed":[["Hu","Jiang",""],["Deng","Kangkang",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 00:42:35 GMT"}],"updateDate":"2024-07-15","timestamp":1720744955000,"abstract":"  We are concerned with decentralized optimization over a compact submanifold,\nwhere the loss functions of local datasets are defined by their respective\nlocal datasets. A key challenge in decentralized optimization is mitigating the\ncommunication bottleneck, which primarily involves two strategies: achieving\nconsensus and applying communication compression. Existing\nprojection/retraction-type algorithms rely on multi-step consensus to attain\nboth consensus and optimality. Due to the nonconvex nature of the manifold\nconstraint, it remains an open question whether the requirement for multi-step\nconsensus can be reduced to single-step consensus. We address this question by\ncarefully elaborating on the smoothness structure and the asymptotic\n1-Lipschitz continuity associated with the manifold constraint. Furthermore, we\nintegrate these insights with a communication compression strategy to propose a\ncommunication-efficient gradient algorithm for decentralized manifold\noptimization problems, significantly reducing per-iteration communication\ncosts. Additionally, we establish an iteration complexity of\n$\\mathcal{O}(\\epsilon^{-1})$ to find an $\\epsilon$-stationary point, which\nmatches the complexity in the Euclidean setting. Numerical experiments\ndemonstrate the efficiency of the proposed method in comparison to\nstate-of-the-art approaches.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}