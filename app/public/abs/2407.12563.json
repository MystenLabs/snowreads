{"id":"2407.12563","title":"Audio Conditioning for Music Generation via Discrete Bottleneck Features","authors":"Simon Rouard, Yossi Adi, Jade Copet, Axel Roebel, Alexandre D\\'efossez","authorsParsed":[["Rouard","Simon",""],["Adi","Yossi",""],["Copet","Jade",""],["Roebel","Axel",""],["DÃ©fossez","Alexandre",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 13:47:17 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 09:44:09 GMT"}],"updateDate":"2024-07-31","timestamp":1721224037000,"abstract":"  While most music generation models use textual or parametric conditioning\n(e.g. tempo, harmony, musical genre), we propose to condition a language model\nbased music generation system with audio input. Our exploration involves two\ndistinct strategies. The first strategy, termed textual inversion, leverages a\npre-trained text-to-music model to map audio input to corresponding\n\"pseudowords\" in the textual embedding space. For the second model we train a\nmusic language model from scratch jointly with a text conditioner and a\nquantized audio feature extractor. At inference time, we can mix textual and\naudio conditioning and balance them thanks to a novel double classifier free\nguidance method. We conduct automatic and human studies that validates our\napproach. We will release the code and we provide music samples on\nhttps://musicgenstyle.github.io in order to show the quality of our model.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}