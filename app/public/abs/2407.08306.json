{"id":"2407.08306","title":"Adversarial-MidiBERT: Symbolic Music Understanding Model Based on Unbias\n  Pre-training and Mask Fine-tuning","authors":"Zijian Zhao","authorsParsed":[["Zhao","Zijian",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 08:54:38 GMT"}],"updateDate":"2024-07-12","timestamp":1720688078000,"abstract":"  As an important part of Music Information Retrieval (MIR), Symbolic Music\nUnderstanding (SMU) has gained substantial attention, as it can assist\nmusicians and amateurs in learning and creating music. Recently, pre-trained\nlanguage models have been widely adopted in SMU because the symbolic music\nshares a huge similarity with natural language, and the pre-trained manner also\nhelps make full use of limited music data. However, the issue of bias, such as\nsexism, ageism, and racism, has been observed in pre-trained language models,\nwhich is attributed to the imbalanced distribution of training data. It also\nhas a significant influence on the performance of downstream tasks, which also\nhappens in SMU. To address this challenge, we propose Adversarial-MidiBERT, a\nsymbolic music understanding model based on Bidirectional Encoder\nRepresentations from Transformers (BERT). We introduce an unbiased pre-training\nmethod based on adversarial learning to minimize the participation of tokens\nthat lead to biases during training. Furthermore, we propose a mask fine-tuning\nmethod to narrow the data gap between pre-training and fine-tuning, which can\nhelp the model converge faster and perform better. We evaluate our method on\nfour music understanding tasks, and our approach demonstrates excellent\nperformance in all of them. The code for our model is publicly available at\nhttps://github.com/RS2002/Adversarial-MidiBERT.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}