{"id":"2408.15833","title":"Network transferability of adversarial patches in real-time object\n  detection","authors":"Jens Bayer and Stefan Becker and David M\\\"unch and Michael Arens","authorsParsed":[["Bayer","Jens",""],["Becker","Stefan",""],["MÃ¼nch","David",""],["Arens","Michael",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 14:47:34 GMT"}],"updateDate":"2024-08-29","timestamp":1724856454000,"abstract":"  Adversarial patches in computer vision can be used, to fool deep neural\nnetworks and manipulate their decision-making process. One of the most\nprominent examples of adversarial patches are evasion attacks for object\ndetectors. By covering parts of objects of interest, these patches suppress the\ndetections and thus make the target object 'invisible' to the object detector.\nSince these patches are usually optimized on a specific network with a specific\ntrain dataset, the transferability across multiple networks and datasets is not\ngiven. This paper addresses these issues and investigates the transferability\nacross numerous object detector architectures. Our extensive evaluation across\nvarious models on two distinct datasets indicates that patches optimized with\nlarger models provide better network transferability than patches that are\noptimized with smaller models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}