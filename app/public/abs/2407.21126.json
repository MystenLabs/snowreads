{"id":"2407.21126","title":"Self-supervised Multi-future Occupancy Forecasting for Autonomous\n  Driving","authors":"Bernard Lange, Masha Itkina, Jiachen Li, Mykel J. Kochenderfer","authorsParsed":[["Lange","Bernard",""],["Itkina","Masha",""],["Li","Jiachen",""],["Kochenderfer","Mykel J.",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 18:37:59 GMT"}],"updateDate":"2024-08-01","timestamp":1722364679000,"abstract":"  Environment prediction frameworks are critical for the safe navigation of\nautonomous vehicles (AVs) in dynamic settings. LiDAR-generated occupancy grid\nmaps (L-OGMs) offer a robust bird's-eye view for the scene representation,\nenabling self-supervised joint scene predictions while exhibiting resilience to\npartial observability and perception detection failures. Prior approaches have\nfocused on deterministic L-OGM prediction architectures within the grid cell\nspace. While these methods have seen some success, they frequently produce\nunrealistic predictions and fail to capture the stochastic nature of the\nenvironment. Additionally, they do not effectively integrate additional sensor\nmodalities present in AVs. Our proposed framework performs stochastic L-OGM\nprediction in the latent space of a generative architecture and allows for\nconditioning on RGB cameras, maps, and planned trajectories. We decode\npredictions using either a single-step decoder, which provides high-quality\npredictions in real-time, or a diffusion-based batch decoder, which can further\nrefine the decoded frames to address temporal consistency issues and reduce\ncompression losses. Our experiments on the nuScenes and Waymo Open datasets\nshow that all variants of our approach qualitatively and quantitatively\noutperform prior approaches.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}