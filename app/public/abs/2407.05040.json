{"id":"2407.05040","title":"Code Less, Align More: Efficient LLM Fine-tuning for Code Generation\n  with Data Pruning","authors":"Yun-Da Tsai, Mingjie Liu, Haoxing Ren","authorsParsed":[["Tsai","Yun-Da",""],["Liu","Mingjie",""],["Ren","Haoxing",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 10:30:43 GMT"}],"updateDate":"2024-07-09","timestamp":1720261843000,"abstract":"  Recent work targeting large language models (LLMs) for code generation\ndemonstrated that increasing the amount of training data through synthetic code\ngeneration often leads to exceptional performance. In this paper we explore\ndata pruning methods aimed at enhancing the efficiency of model training\nspecifically for code LLMs. We present techniques that integrate various\nclustering and pruning metrics to selectively reduce training data without\ncompromising the accuracy and functionality of the generated code. We observe\nsignificant redundancies in synthetic training data generation, where our\nexperiments demonstrate that benchmark performance can be largely preserved by\ntraining on only 10% of the data. Moreover, we observe consistent improvements\nin benchmark results through moderate pruning of the training data. Our\nexperiments show that these pruning strategies not only reduce the\ncomputational resources needed but also enhance the overall quality code\ngeneration.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}