{"id":"2407.15050","title":"Arondight: Red Teaming Large Vision Language Models with Auto-generated\n  Multi-modal Jailbreak Prompts","authors":"Yi Liu, Chengjun Cai, Xiaoli Zhang, Xingliang Yuan, Cong Wang","authorsParsed":[["Liu","Yi",""],["Cai","Chengjun",""],["Zhang","Xiaoli",""],["Yuan","Xingliang",""],["Wang","Cong",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 04:37:11 GMT"}],"updateDate":"2024-07-23","timestamp":1721536631000,"abstract":"  Large Vision Language Models (VLMs) extend and enhance the perceptual\nabilities of Large Language Models (LLMs). Despite offering new possibilities\nfor LLM applications, these advancements raise significant security and ethical\nconcerns, particularly regarding the generation of harmful content. While LLMs\nhave undergone extensive security evaluations with the aid of red teaming\nframeworks, VLMs currently lack a well-developed one. To fill this gap, we\nintroduce Arondight, a standardized red team framework tailored specifically\nfor VLMs. Arondight is dedicated to resolving issues related to the absence of\nvisual modality and inadequate diversity encountered when transitioning\nexisting red teaming methodologies from LLMs to VLMs. Our framework features an\nautomated multi-modal jailbreak attack, wherein visual jailbreak prompts are\nproduced by a red team VLM, and textual prompts are generated by a red team LLM\nguided by a reinforcement learning agent. To enhance the comprehensiveness of\nVLM security evaluation, we integrate entropy bonuses and novelty reward\nmetrics. These elements incentivize the RL agent to guide the red team LLM in\ncreating a wider array of diverse and previously unseen test cases. Our\nevaluation of ten cutting-edge VLMs exposes significant security\nvulnerabilities, particularly in generating toxic images and aligning\nmulti-modal prompts. In particular, our Arondight achieves an average attack\nsuccess rate of 84.5\\% on GPT-4 in all fourteen prohibited scenarios defined by\nOpenAI in terms of generating toxic text. For a clearer comparison, we also\ncategorize existing VLMs based on their safety levels and provide corresponding\nreinforcement recommendations. Our multimodal prompt dataset and red team code\nwill be released after ethics committee approval. CONTENT WARNING: THIS PAPER\nCONTAINS HARMFUL MODEL RESPONSES.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}