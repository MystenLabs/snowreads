{"id":"2407.00371","title":"Axiomatization of Gradient Smoothing in Neural Networks","authors":"Linjiang Zhou, Xiaochuan Shi, Chao Ma, Zepeng Wang","authorsParsed":[["Zhou","Linjiang",""],["Shi","Xiaochuan",""],["Ma","Chao",""],["Wang","Zepeng",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 08:43:38 GMT"}],"updateDate":"2024-07-02","timestamp":1719650618000,"abstract":"  Gradients play a pivotal role in neural networks explanation. The inherent\nhigh dimensionality and structural complexity of neural networks result in the\noriginal gradients containing a significant amount of noise. While several\napproaches were proposed to reduce noise with smoothing, there is little\ndiscussion of the rationale behind smoothing gradients in neural networks. In\nthis work, we proposed a gradient smooth theoretical framework for neural\nnetworks based on the function mollification and Monte Carlo integration. The\nframework intrinsically axiomatized gradient smoothing and reveals the\nrationale of existing methods. Furthermore, we provided an approach to design\nnew smooth methods derived from the framework. By experimental measurement of\nseveral newly designed smooth methods, we demonstrated the research potential\nof our framework.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_qqsYCGb2zVWGxbSuqpT_IZDGAvzI-kjcoJVZUPt-6I","pdfSize":"1872097"}
