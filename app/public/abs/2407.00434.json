{"id":"2407.00434","title":"Brevity is the soul of wit: Pruning long files for code generation","authors":"Aaditya K. Singh, Yu Yang, Kushal Tirumala, Mostafa Elhoushi, Ari S.\n  Morcos","authorsParsed":[["Singh","Aaditya K.",""],["Yang","Yu",""],["Tirumala","Kushal",""],["Elhoushi","Mostafa",""],["Morcos","Ari S.",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 13:08:24 GMT"}],"updateDate":"2024-07-02","timestamp":1719666504000,"abstract":"  Data curation is commonly considered a \"secret-sauce\" for LLM training, with\nhigher quality data usually leading to better LLM performance. Given the scale\nof internet-scraped corpora, data pruning has become a larger and larger focus.\nSpecifically, many have shown that de-duplicating data, or sub-selecting higher\nquality data, can lead to efficiency or performance improvements. Generally,\nthree types of methods are used to filter internet-scale corpora:\nembedding-based, heuristic-based, and classifier-based. In this work, we\ncontrast the former two in the domain of finetuning LLMs for code generation.\nWe find that embedding-based methods are often confounded by length, and that a\nsimple heuristic--pruning long files--outperforms other methods in\ncompute-limited regimes. Our method can yield up to a 2x efficiency benefit in\ntraining (while matching performance) or a 3.5% absolute performance\nimprovement on HumanEval (while matching compute). However, we find that\nperplexity on held-out long files can increase, begging the question of whether\noptimizing data mixtures for common coding benchmarks (HumanEval, MBPP)\nactually best serves downstream use cases. Overall, we hope our work builds\nuseful intuitions about code data (specifically, the low quality of extremely\nlong code files) provides a compelling heuristic-based method for data pruning,\nand brings to light questions in how we evaluate code generation models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7Z86sDqFTjGkakfp8japCvBgCpf8F7mgT2WwtlU3REk","pdfSize":"601798"}
