{"id":"2407.19714","title":"Rethinking RGB-D Fusion for Semantic Segmentation in Surgical Datasets","authors":"Muhammad Abdullah Jamal, Omid Mohareri","authorsParsed":[["Jamal","Muhammad Abdullah",""],["Mohareri","Omid",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 05:35:51 GMT"}],"updateDate":"2024-07-30","timestamp":1722231351000,"abstract":"  Surgical scene understanding is a key technical component for enabling\nintelligent and context aware systems that can transform various aspects of\nsurgical interventions. In this work, we focus on the semantic segmentation\ntask, propose a simple yet effective multi-modal (RGB and depth) training\nframework called SurgDepth, and show state-of-the-art (SOTA) results on all\npublicly available datasets applicable for this task. Unlike previous\napproaches, which either fine-tune SOTA segmentation models trained on natural\nimages, or encode RGB or RGB-D information using RGB only pre-trained\nbackbones, SurgDepth, which is built on top of Vision Transformers (ViTs), is\ndesigned to encode both RGB and depth information through a simple fusion\nmechanism. We conduct extensive experiments on benchmark datasets including\nEndoVis2022, AutoLapro, LapI2I and EndoVis2017 to verify the efficacy of\nSurgDepth. Specifically, SurgDepth achieves a new SOTA IoU of 0.86 on EndoVis\n2022 SAR-RARP50 challenge and outperforms the current best method by at least\n4%, using a shallow and compute efficient decoder consisting of ConvNeXt\nblocks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"gDUKI2AIEJO3THU3YKedMq3zBpqmRGUx9eBzy39LEE8","pdfSize":"17065295"}
