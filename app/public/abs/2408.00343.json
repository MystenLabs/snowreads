{"id":"2408.00343","title":"IN-Sight: Interactive Navigation through Sight","authors":"Philipp Schoch, Fan Yang, Yuntao Ma, Stefan Leutenegger, Marco Hutter\n  and Quentin Leboutet","authorsParsed":[["Schoch","Philipp",""],["Yang","Fan",""],["Ma","Yuntao",""],["Leutenegger","Stefan",""],["Hutter","Marco",""],["Leboutet","Quentin",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 07:27:54 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 10:19:08 GMT"}],"updateDate":"2024-08-13","timestamp":1722497274000,"abstract":"  Current visual navigation systems often treat the environment as static,\nlacking the ability to adaptively interact with obstacles. This limitation\nleads to navigation failure when encountering unavoidable obstructions. In\nresponse, we introduce IN-Sight, a novel approach to self-supervised path\nplanning, enabling more effective navigation strategies through interaction\nwith obstacles. Utilizing RGB-D observations, IN-Sight calculates\ntraversability scores and incorporates them into a semantic map, facilitating\nlong-range path planning in complex, maze-like environments. To precisely\nnavigate around obstacles, IN-Sight employs a local planner, trained\nimperatively on a differentiable costmap using representation learning\ntechniques. The entire framework undergoes end-to-end training within the\nstate-of-the-art photorealistic Intel SPEAR Simulator. We validate the\neffectiveness of IN-Sight through extensive benchmarking in a variety of\nsimulated scenarios and ablation studies. Moreover, we demonstrate the system's\nreal-world applicability with zero-shot sim-to-real transfer, deploying our\nplanner on the legged robot platform ANYmal, showcasing its practical potential\nfor interactive navigation in real environments.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}