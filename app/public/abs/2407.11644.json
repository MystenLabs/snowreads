{"id":"2407.11644","title":"Perception Helps Planning: Facilitating Multi-Stage Lane-Level\n  Integration via Double-Edge Structures","authors":"Guoliang You, Xiaomeng Chu, Yifan Duan, Wenyu Zhang, Xingchen Li, Sha\n  Zhang, Yao Li, Jianmin Ji, Yanyong Zhang","authorsParsed":[["You","Guoliang",""],["Chu","Xiaomeng",""],["Duan","Yifan",""],["Zhang","Wenyu",""],["Li","Xingchen",""],["Zhang","Sha",""],["Li","Yao",""],["Ji","Jianmin",""],["Zhang","Yanyong",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 12:12:06 GMT"}],"updateDate":"2024-07-17","timestamp":1721131926000,"abstract":"  When planning for autonomous driving, it is crucial to consider essential\ntraffic elements such as lanes, intersections, traffic regulations, and dynamic\nagents. However, they are often overlooked by the traditional end-to-end\nplanning methods, likely leading to inefficiencies and non-compliance with\ntraffic regulations. In this work, we endeavor to integrate the perception of\nthese elements into the planning task. To this end, we propose Perception Helps\nPlanning (PHP), a novel framework that reconciles lane-level planning with\nperception. This integration ensures that planning is inherently aligned with\ntraffic constraints, thus facilitating safe and efficient driving.\nSpecifically, PHP focuses on both edges of a lane for planning and perception\npurposes, taking into consideration the 3D positions of both lane edges and\nattributes for lane intersections, lane directions, lane occupancy, and\nplanning. In the algorithmic design, the process begins with the transformer\nencoding multi-camera images to extract the above features and predicting\nlane-level perception results. Next, the hierarchical feature early fusion\nmodule refines the features for predicting planning attributes. Finally, the\ndouble-edge interpreter utilizes a late-fusion process specifically designed to\nintegrate lane-level perception and planning information, culminating in the\ngeneration of vehicle control signals. Experiments on three Carla benchmarks\nshow significant improvements in driving score of 27.20%, 33.47%, and 15.54%\nover existing algorithms, respectively, achieving the state-of-the-art\nperformance, with the system operating up to 22.57 FPS.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}