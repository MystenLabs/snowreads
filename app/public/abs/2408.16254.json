{"id":"2408.16254","title":"EvLight++: Low-Light Video Enhancement with an Event Camera: A\n  Large-Scale Real-World Dataset, Novel Method, and More","authors":"Kanghao Chen, Guoqiang Liang, Hangyu Li, Yunfan Lu and Lin Wang","authorsParsed":[["Chen","Kanghao",""],["Liang","Guoqiang",""],["Li","Hangyu",""],["Lu","Yunfan",""],["Wang","Lin",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 04:30:31 GMT"}],"updateDate":"2024-08-30","timestamp":1724905831000,"abstract":"  Event cameras offer significant advantages for low-light video enhancement,\nprimarily due to their high dynamic range. Current research, however, is\nseverely limited by the absence of large-scale, real-world, and\nspatio-temporally aligned event-video datasets. To address this, we introduce a\nlarge-scale dataset with over 30,000 pairs of frames and events captured under\nvarying illumination. This dataset was curated using a robotic arm that traces\na consistent non-linear trajectory, achieving spatial alignment precision under\n0.03mm and temporal alignment with errors under 0.01s for 90% of the dataset.\nBased on the dataset, we propose \\textbf{EvLight++}, a novel event-guided\nlow-light video enhancement approach designed for robust performance in\nreal-world scenarios. Firstly, we design a multi-scale holistic fusion branch\nto integrate structural and textural information from both images and events.\nTo counteract variations in regional illumination and noise, we introduce\nSignal-to-Noise Ratio (SNR)-guided regional feature selection, enhancing\nfeatures from high SNR regions and augmenting those from low SNR regions by\nextracting structural information from events. To incorporate temporal\ninformation and ensure temporal coherence, we further introduce a recurrent\nmodule and temporal loss in the whole pipeline. Extensive experiments on our\nand the synthetic SDSD dataset demonstrate that EvLight++ significantly\noutperforms both single image- and video-based methods by 1.37 dB and 3.71 dB,\nrespectively. To further explore its potential in downstream tasks like\nsemantic segmentation and monocular depth estimation, we extend our datasets by\nadding pseudo segmentation and depth labels via meticulous annotation efforts\nwith foundation models. Experiments under diverse low-light scenes show that\nthe enhanced results achieve a 15.97% improvement in mIoU for semantic\nsegmentation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}