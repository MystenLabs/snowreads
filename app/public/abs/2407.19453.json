{"id":"2407.19453","title":"FIND: Fine-tuning Initial Noise Distribution with Policy Optimization\n  for Diffusion Models","authors":"Changgu Chen, Libing Yang, Xiaoyan Yang, Lianggangxu Chen, Gaoqi He,\n  CHangbo Wang, Yang Li","authorsParsed":[["Chen","Changgu",""],["Yang","Libing",""],["Yang","Xiaoyan",""],["Chen","Lianggangxu",""],["He","Gaoqi",""],["Wang","CHangbo",""],["Li","Yang",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 10:07:55 GMT"}],"updateDate":"2024-07-30","timestamp":1722161275000,"abstract":"  In recent years, large-scale pre-trained diffusion models have demonstrated\ntheir outstanding capabilities in image and video generation tasks. However,\nexisting models tend to produce visual objects commonly found in the training\ndataset, which diverges from user input prompts. The underlying reason behind\nthe inaccurate generated results lies in the model's difficulty in sampling\nfrom specific intervals of the initial noise distribution corresponding to the\nprompt. Moreover, it is challenging to directly optimize the initial\ndistribution, given that the diffusion process involves multiple denoising\nsteps. In this paper, we introduce a Fine-tuning Initial Noise Distribution\n(FIND) framework with policy optimization, which unleashes the powerful\npotential of pre-trained diffusion networks by directly optimizing the initial\ndistribution to align the generated contents with user-input prompts. To this\nend, we first reformulate the diffusion denoising procedure as a one-step\nMarkov decision process and employ policy optimization to directly optimize the\ninitial distribution. In addition, a dynamic reward calibration module is\nproposed to ensure training stability during optimization. Furthermore, we\nintroduce a ratio clipping algorithm to utilize historical data for network\ntraining and prevent the optimized distribution from deviating too far from the\noriginal policy to restrain excessive optimization magnitudes. Extensive\nexperiments demonstrate the effectiveness of our method in both text-to-image\nand text-to-video tasks, surpassing SOTA methods in achieving consistency\nbetween prompts and the generated content. Our method achieves 10 times faster\nthan the SOTA approach. Our homepage is available at\n\\url{https://github.com/vpx-ecnu/FIND-website}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}