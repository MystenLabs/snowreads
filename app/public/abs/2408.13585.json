{"id":"2408.13585","title":"FLEURS-ASL: Including American Sign Language in Massively Multilingual\n  Multitask Evaluation","authors":"Garrett Tanzer","authorsParsed":[["Tanzer","Garrett",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 13:59:41 GMT"}],"updateDate":"2024-08-27","timestamp":1724507981000,"abstract":"  Sign language translation has historically been peripheral to mainstream\nmachine translation research. In order to help converge the fields, we\nintroduce FLEURS-ASL, an extension of the multiway parallel benchmarks FLORES\n(for text) and FLEURS (for speech) to support their first sign language (as\nvideo), American Sign Language, translated by 5 Certified Deaf Interpreters.\nFLEURS-ASL can be used to evaluate a variety of tasks -- primarily sentence-\nand discourse-level translation -- between ASL and 200 other languages as text,\nor 102 languages as speech. We provide baselines for tasks from ASL to English\ntext using a unified modeling approach that incorporates timestamp tokens and\nprevious text tokens in a 34-second context window, trained on random video\nclips from YouTube-ASL. This model meets or exceeds the performance of\nphrase-level baselines while supporting a multitude of new tasks. We also use\nFLEURS-ASL to show that multimodal frontier models have virtually no\nunderstanding of ASL, underscoring the importance of including sign languages\nin standard evaluation suites.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}