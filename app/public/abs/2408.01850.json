{"id":"2408.01850","title":"MotionTrace: IMU-based Field of View Prediction for Smartphone AR\n  Interactions","authors":"Rahul Islam, Vasco Xu, Karan Ahuja","authorsParsed":[["Islam","Rahul",""],["Xu","Vasco",""],["Ahuja","Karan",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 19:31:02 GMT"}],"updateDate":"2024-08-06","timestamp":1722713462000,"abstract":"  For handheld smartphone AR interactions, bandwidth is a critical constraint.\nStreaming techniques have been developed to provide a seamless and high-quality\nuser experience despite these challenges. To optimize streaming performance in\nsmartphone-based AR, accurate prediction of the user's field of view is\nessential. This prediction allows the system to prioritize loading digital\ncontent that the user is likely to engage with, enhancing the overall\ninteractivity and immersion of the AR experience. In this paper, we present\nMotionTrace, a method for predicting the user's field of view using a\nsmartphone's inertial sensor. This method continuously estimates the user's\nhand position in 3D-space to localize the phone position. We evaluated\nMotionTrace over future hand positions at 50, 100, 200, 400, and 800ms time\nhorizons using the large motion capture (AMASS) and smartphone-based full-body\npose estimation (Pose-on-the-Go) datasets. We found that our method can\nestimate the future phone position of the user with an average MSE between 0.11\n- 143.62 mm across different time horizons.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dGnCjVZObO36ZStGjAJOs8mJj3WbVCcFcjARiq_KIBQ","pdfSize":"1427341","txDigest":"4KDCof1vCCYvrt5mS6uexsSj8NQ7j2LFAMcBQi3aDmXX","endEpoch":"1","status":"CERTIFIED"}
