{"id":"2407.12847","title":"Aligning Model Evaluations with Human Preferences: Mitigating Token\n  Count Bias in Language Model Assessments","authors":"Roland Daynauth, Jason Mars","authorsParsed":[["Daynauth","Roland",""],["Mars","Jason",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 09:26:40 GMT"}],"updateDate":"2024-07-19","timestamp":1720171600000,"abstract":"  The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a\nviable and cost-effective alternative to API-based Large Language Models\n(LLMs), such as OpenAI's GPT-4, offering comparable performance and stability.\nHowever, SLAM also identified discrepancies between human preferences and\ntraditional auto-evaluators. This follow-up paper explores methods to align LLM\nevaluator preferences with human evaluations by addressing biases, particularly\ntoward higher token counts. We employed Bayesian statistics and a t-test to\nquantify this bias and developed a recalibration procedure to adjust the\nGPTScorer. Our findings significantly improve aligning the recalibrated LLM\nevaluator with human evaluations across multiple use cases. For instance,\nspearman's ranking correlation score in the Recommendation use case improved\nfrom -27.27 to 44.55. These results highlight the importance of accounting for\nbiases in automated evaluations to ensure fair and accurate model assessments.\nThe recalibration process enhances the reliability of automated evaluators,\nleading to better AI models that align with human values and expectations. This\nstudy provides a robust methodology for future research into bias correction\nand emphasizes the feasibility and benefits of developing human-aligned AI\nevaluation systems.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/"}