{"id":"2408.06010","title":"DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D\n  Face Animation","authors":"Jisoo Kim, Jungbin Cho, Joonho Park, Soonmin Hwang, Da Eun Kim, Geon\n  Kim, Youngjae Yu","authorsParsed":[["Kim","Jisoo",""],["Cho","Jungbin",""],["Park","Joonho",""],["Hwang","Soonmin",""],["Kim","Da Eun",""],["Kim","Geon",""],["Yu","Youngjae",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 08:56:49 GMT"}],"updateDate":"2024-08-13","timestamp":1723453009000,"abstract":"  Speech-driven 3D facial animation has garnered lots of attention thanks to\nits broad range of applications. Despite recent advancements in achieving\nrealistic lip motion, current methods fail to capture the nuanced emotional\nundertones conveyed through speech and produce monotonous facial motion. These\nlimitations result in blunt and repetitive facial animations, reducing user\nengagement and hindering their applicability. To address these challenges, we\nintroduce DEEPTalk, a novel approach that generates diverse and emotionally\nrich 3D facial expressions directly from speech inputs. To achieve this, we\nfirst train DEE (Dynamic Emotion Embedding), which employs probabilistic\ncontrastive learning to forge a joint emotion embedding space for both speech\nand facial motion. This probabilistic framework captures the uncertainty in\ninterpreting emotions from speech and facial motion, enabling the derivation of\nemotion vectors from its multifaceted space. Moreover, to generate dynamic\nfacial motion, we design TH-VQVAE (Temporally Hierarchical VQ-VAE) as an\nexpressive and robust motion prior overcoming limitations of VAEs and VQ-VAEs.\nUtilizing these strong priors, we develop DEEPTalk, A talking head generator\nthat non-autoregressively predicts codebook indices to create dynamic facial\nmotion, incorporating a novel emotion consistency loss. Extensive experiments\non various datasets demonstrate the effectiveness of our approach in creating\ndiverse, emotionally expressive talking faces that maintain accurate lip-sync.\nSource code will be made publicly available soon.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kYKg9ze0zJ2FvFUVdpTNZCS_E1cm30NStP8XVDQNRPY","pdfSize":"3978165"}
