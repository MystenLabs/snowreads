{"id":"2408.16986","title":"AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene\n  Understanding","authors":"Yonghui Wang, Wengang Zhou, Hao Feng, Houqiang Li","authorsParsed":[["Wang","Yonghui",""],["Zhou","Wengang",""],["Feng","Hao",""],["Li","Houqiang",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 03:16:49 GMT"}],"updateDate":"2024-09-02","timestamp":1724987809000,"abstract":"  Over the past few years, the advancement of Multimodal Large Language Models\n(MLLMs) has captured the wide interest of researchers, leading to numerous\ninnovations to enhance MLLMs' comprehension. In this paper, we present\nAdaptVision, a multimodal large language model specifically designed to\ndynamically process input images at varying resolutions. We hypothesize that\nthe requisite number of visual tokens for the model is contingent upon both the\nresolution and content of the input image. Generally, natural images with a\nlower information density can be effectively interpreted by the model using\nfewer visual tokens at reduced resolutions. In contrast, images containing\ntextual content, such as documents with rich text, necessitate a higher number\nof visual tokens for accurate text interpretation due to their higher\ninformation density. Building on this insight, we devise a dynamic image\npartitioning module that adjusts the number of visual tokens according to the\nsize and aspect ratio of images. This method mitigates distortion effects that\narise from resizing images to a uniform resolution and dynamically optimizing\nthe visual tokens input to the LLMs. Our model is capable of processing images\nwith resolutions up to $1008\\times 1008$. Extensive experiments across various\ndatasets demonstrate that our method achieves impressive performance in\nhandling vision-language tasks in both natural and text-related scenes. The\nsource code and dataset are now publicly available at\n\\url{https://github.com/harrytea/AdaptVision}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}