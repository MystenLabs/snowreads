{"id":"2407.19683","title":"Revisiting the robustness of post-hoc interpretability methods","authors":"Jiawen Wei, Hugues Turb\\'e, Gianmarco Mengaldo","authorsParsed":[["Wei","Jiawen",""],["Turb√©","Hugues",""],["Mengaldo","Gianmarco",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 03:55:52 GMT"}],"updateDate":"2024-07-30","timestamp":1722225352000,"abstract":"  Post-hoc interpretability methods play a critical role in explainable\nartificial intelligence (XAI), as they pinpoint portions of data that a trained\ndeep learning model deemed important to make a decision. However, different\npost-hoc interpretability methods often provide different results, casting\ndoubts on their accuracy. For this reason, several evaluation strategies have\nbeen proposed to understand the accuracy of post-hoc interpretability. Many of\nthese evaluation strategies provide a coarse-grained assessment -- i.e., they\nevaluate how the performance of the model degrades on average by corrupting\ndifferent data points across multiple samples. While these strategies are\neffective in selecting the post-hoc interpretability method that is most\nreliable on average, they fail to provide a sample-level, also referred to as\nfine-grained, assessment. In other words, they do not measure the robustness of\npost-hoc interpretability methods. We propose an approach and two new metrics\nto provide a fine-grained assessment of post-hoc interpretability methods. We\nshow that the robustness is generally linked to its coarse-grained performance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"E3CwXeONM_2k9WbYai54c17YYAlgTMTF927tx-2elg8","pdfSize":"6338340","objectId":"0x47469843cf913089b9cdfa2613aa75cf203c1d2b717843db6fb26d1408f0ba77","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
