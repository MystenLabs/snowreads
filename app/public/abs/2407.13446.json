{"id":"2407.13446","title":"Subsampled One-Step Estimation for Fast Statistical Inference","authors":"Miaomiao Su and Ruoyu Wang","authorsParsed":[["Su","Miaomiao",""],["Wang","Ruoyu",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 12:14:34 GMT"}],"updateDate":"2024-07-19","timestamp":1721304874000,"abstract":"  Subsampling is an effective approach to alleviate the computational burden\nassociated with large-scale datasets. Nevertheless, existing subsampling\nestimators incur a substantial loss in estimation efficiency compared to\nestimators based on the full dataset. Specifically, the convergence rate of\nexisting subsampling estimators is typically $n^{-1/2}$ rather than $N^{-1/2}$,\nwhere $n$ and $N$ denote the subsample and full data sizes, respectively. This\npaper proposes a subsampled one-step (SOS) method to mitigate the estimation\nefficiency loss utilizing the asymptotic expansions of the subsampling and\nfull-data estimators. The resulting SOS estimator is computationally efficient\nand achieves a fast convergence rate of $\\max\\{n^{-1}, N^{-1/2}\\}$ rather than\n$n^{-1/2}$. We establish the asymptotic distribution of the SOS estimator,\nwhich can be non-normal in general, and construct confidence intervals on top\nof the asymptotic distribution. Furthermore, we prove that the SOS estimator is\nasymptotically normal and equivalent to the full data-based estimator when $n /\n\\sqrt{N} \\to \\infty$.Simulation studies and real data analyses were conducted\nto demonstrate the finite sample performance of the SOS estimator. Numerical\nresults suggest that the SOS estimator is almost as computationally efficient\nas the uniform subsampling estimator while achieving similar estimation\nefficiency to the full data-based estimator.\n","subjects":["Statistics/Methodology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}