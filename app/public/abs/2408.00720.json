{"id":"2408.00720","title":"Nonasymptotic Analysis of Accelerated Methods With Inexact Oracle Under\n  Absolute Error Bound","authors":"Yin Liu and Sam Davanloo Tajbakhsh","authorsParsed":[["Liu","Yin",""],["Tajbakhsh","Sam Davanloo",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:06:38 GMT"}],"updateDate":"2024-08-02","timestamp":1722531998000,"abstract":"  Performance analysis of first-order algorithms with inexact oracle has gained\nrecent attention due to various emerging applications in which obtaining exact\ngradients is impossible or computationally expensive. Previous research has\ndemonstrated that the performance of accelerated first-order methods is more\nsensitive to gradient errors compared with non-accelerated ones. This paper\ninvestigates the nonasymptotic convergence bound of two accelerated methods\nwith inexact gradients to solve deterministic smooth convex problems.\nPerformance Estimation Problem (PEP) is used as the primary tool to analyze the\nconvergence bounds of the underlying algorithms. By finding an analytical\nsolution to PEP, we derive novel convergence bounds for Inexact Optimized\nGradient Method (OGM) and Inexact Fast Gradient Method (FGM) with variable\ninexactness along iterations. Under the absolute error assumption, we derive\nbounds in which the accumulated errors are independent of the initial\nconditions and the trajectory of the sequences generated by the algorithms.\nFurthermore, we analyze the tradeoff between the convergence rate and\naccumulated error that guides finding the optimal stepsize. Finally, we\ndetermine the optimal strategy to set the gradient inexactness along iterations\n(if possible), ensuring that the accumulated error remains subordinate to the\nconvergence rate.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}