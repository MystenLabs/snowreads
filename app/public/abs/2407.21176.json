{"id":"2407.21176","title":"DKL-KAN: Scalable Deep Kernel Learning using Kolmogorov-Arnold Networks","authors":"Shrenik Zinage, Sudeepta Mondal, Soumalya Sarkar","authorsParsed":[["Zinage","Shrenik",""],["Mondal","Sudeepta",""],["Sarkar","Soumalya",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 20:30:44 GMT"}],"updateDate":"2024-08-01","timestamp":1722371444000,"abstract":"  The need for scalable and expressive models in machine learning is paramount,\nparticularly in applications requiring both structural depth and flexibility.\nTraditional deep learning methods, such as multilayer perceptrons (MLP), offer\ndepth but lack ability to integrate structural characteristics of deep learning\narchitectures with non-parametric flexibility of kernel methods. To address\nthis, deep kernel learning (DKL) was introduced, where inputs to a base kernel\nare transformed using a deep learning architecture. These kernels can replace\nstandard kernels, allowing both expressive power and scalability. The advent of\nKolmogorov-Arnold Networks (KAN) has generated considerable attention and\ndiscussion among researchers in scientific domain. In this paper, we introduce\na scalable deep kernel using KAN (DKL-KAN) as an effective alternative to DKL\nusing MLP (DKL-MLP). Our approach involves simultaneously optimizing these\nkernel attributes using marginal likelihood within a Gaussian process\nframework. We analyze two variants of DKL-KAN for a fair comparison with\nDKL-MLP: one with same number of neurons and layers as DKL-MLP, and another\nwith approximately same number of trainable parameters. To handle large\ndatasets, we use kernel interpolation for scalable structured Gaussian\nprocesses (KISS-GP) for low-dimensional inputs and KISS-GP with product kernels\nfor high-dimensional inputs. The efficacy of DKL-KAN is evaluated in terms of\ncomputational training time and test prediction accuracy across a wide range of\napplications. Additionally, the effectiveness of DKL-KAN is also examined in\nmodeling discontinuities and accurately estimating prediction uncertainty. The\nresults indicate that DKL-KAN outperforms DKL-MLP on datasets with a low number\nof observations. Conversely, DKL-MLP exhibits better scalability and higher\ntest prediction accuracy on datasets with large number of observations.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}