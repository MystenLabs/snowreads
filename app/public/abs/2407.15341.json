{"id":"2407.15341","title":"ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis\n  with Coarse-to-Fine In-context Learning","authors":"Senbin Zhu, Hanjie Zhao, Xingren Wang, Shanhong Liu, Yuxiang Jia,\n  Hongying Zan","authorsParsed":[["Zhu","Senbin",""],["Zhao","Hanjie",""],["Wang","Xingren",""],["Liu","Shanhong",""],["Jia","Yuxiang",""],["Zan","Hongying",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 02:54:46 GMT"}],"updateDate":"2024-07-23","timestamp":1721616886000,"abstract":"  The DimABSA task requires fine-grained sentiment intensity prediction for\nrestaurant reviews, including scores for Valence and Arousal dimensions for\neach Aspect Term. In this study, we propose a Coarse-to-Fine In-context\nLearning(CFICL) method based on the Baichuan2-7B model for the DimABSA task in\nthe SIGHAN 2024 workshop. Our method improves prediction accuracy through a\ntwo-stage optimization process. In the first stage, we use fixed in-context\nexamples and prompt templates to enhance the model's sentiment recognition\ncapability and provide initial predictions for the test data. In the second\nstage, we encode the Opinion field using BERT and select the most similar\ntraining data as new in-context examples based on similarity. These examples\ninclude the Opinion field and its scores, as well as related opinion words and\ntheir average scores. By filtering for sentiment polarity, we ensure that the\nexamples are consistent with the test data. Our method significantly improves\nprediction accuracy and consistency by effectively utilizing training data and\noptimizing in-context examples, as validated by experimental results.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}