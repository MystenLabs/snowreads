{"id":"2408.12936","title":"Smooth InfoMax -- Towards easier Post-Hoc interpretability","authors":"Fabian Denoodt, Bart de Boer and Jos\\'e Oramas","authorsParsed":[["Denoodt","Fabian",""],["de Boer","Bart",""],["Oramas","Jos√©",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 09:36:09 GMT"}],"updateDate":"2024-08-26","timestamp":1724405769000,"abstract":"  We introduce Smooth InfoMax (SIM), a novel method for self-supervised\nrepresentation learning that incorporates an interpretability constraint into\nthe learned representations at various depths of the neural network. SIM's\narchitecture is split up into probabilistic modules, each locally optimized\nusing the InfoNCE bound. Inspired by VAEs, the representations from these\nmodules are designed to be samples from Gaussian distributions and are further\nconstrained to be close to the standard normal distribution. This results in a\nsmooth and predictable space, enabling traversal of the latent space through a\ndecoder for easier post-hoc analysis of the learned representations. We\nevaluate SIM's performance on sequential speech data, showing that it performs\ncompetitively with its less interpretable counterpart, Greedy InfoMax (GIM).\nMoreover, we provide insights into SIM's internal representations,\ndemonstrating that the contained information is less entangled throughout the\nrepresentation and more concentrated in a smaller subset of the dimensions.\nThis further highlights the improved interpretability of SIM.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}