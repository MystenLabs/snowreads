{"id":"2407.12331","title":"I2AM: Interpreting Image-to-Image Latent Diffusion Models via\n  Attribution Maps","authors":"Junseo Park and Hyeryung Jang","authorsParsed":[["Park","Junseo",""],["Jang","Hyeryung",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 06:15:05 GMT"}],"updateDate":"2024-07-18","timestamp":1721196905000,"abstract":"  Large-scale diffusion models have made significant advancements in the field\nof image generation, especially through the use of cross-attention mechanisms\nthat guide image formation based on textual descriptions. While the analysis of\ntext-guided cross-attention in diffusion models has been extensively studied in\nrecent years, its application in image-to-image diffusion models remains\nunderexplored. This paper introduces the Image-to-Image Attribution Maps I2AM\nmethod, which aggregates patch-level cross-attention scores to enhance the\ninterpretability of latent diffusion models across time steps, heads, and\nattention layers. I2AM facilitates detailed image-to-image attribution\nanalysis, enabling observation of how diffusion models prioritize key features\nover time and head during the image generation process from reference images.\nThrough extensive experiments, we first visualize the attribution maps of both\ngenerated and reference images, verifying that critical information from the\nreference image is effectively incorporated into the generated image, and vice\nversa. To further assess our understanding, we introduce a new evaluation\nmetric tailored for reference-based image inpainting tasks. This metric,\nmeasuring the consistency between the attribution maps of generated and\nreference images, shows a strong correlation with established performance\nmetrics for inpainting tasks, validating the potential use of I2AM in future\nresearch endeavors.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}