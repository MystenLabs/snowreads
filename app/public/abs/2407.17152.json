{"id":"2407.17152","title":"XMeCap: Meme Caption Generation with Sub-Image Adaptability","authors":"Yuyan Chen, Songzhou Yan, Zhihong Zhu, Zhixu Li, Yanghua Xiao","authorsParsed":[["Chen","Yuyan",""],["Yan","Songzhou",""],["Zhu","Zhihong",""],["Li","Zhixu",""],["Xiao","Yanghua",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 10:51:46 GMT"},{"version":"v2","created":"Wed, 31 Jul 2024 12:56:22 GMT"}],"updateDate":"2024-08-01","timestamp":1721818306000,"abstract":"  Humor, deeply rooted in societal meanings and cultural details, poses a\nunique challenge for machines. While advances have been made in natural\nlanguage processing, real-world humor often thrives in a multi-modal context,\nencapsulated distinctively by memes. This paper poses a particular emphasis on\nthe impact of multi-images on meme captioning. After that, we introduce the\n\\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning\nand reinforcement learning based on an innovative reward model, which factors\nin both global and local similarities between visuals and text. Our results,\nbenchmarked against contemporary models, manifest a marked improvement in\ncaption generation for both single-image and multi-image memes, as well as\ndifferent meme categories. \\textsc{XMeCap} achieves an average evaluation score\nof 75.85 for single-image memes and 66.32 for multi-image memes, outperforming\nthe best baseline by 3.71\\% and 4.82\\%, respectively. This research not only\nestablishes a new frontier in meme-related studies but also underscores the\npotential of machines in understanding and generating humor in a multi-modal\nsetting.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}