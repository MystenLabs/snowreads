{"id":"2408.05831","title":"Robust Domain Generalization for Multi-modal Object Recognition","authors":"Yuxin Qiao, Keqin Li, Junhong Lin, Rong Wei, Chufeng Jiang, Yang Luo,\n  Haoyu Yang","authorsParsed":[["Qiao","Yuxin",""],["Li","Keqin",""],["Lin","Junhong",""],["Wei","Rong",""],["Jiang","Chufeng",""],["Luo","Yang",""],["Yang","Haoyu",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 17:13:21 GMT"}],"updateDate":"2024-08-13","timestamp":1723396401000,"abstract":"  In multi-label classification, machine learning encounters the challenge of\ndomain generalization when handling tasks with distributions differing from the\ntraining data. Existing approaches primarily focus on vision object recognition\nand neglect the integration of natural language. Recent advancements in\nvision-language pre-training leverage supervision from extensive\nvisual-language pairs, enabling learning across diverse domains and enhancing\nrecognition in multi-modal scenarios. However, these approaches face\nlimitations in loss function utilization, generality across backbones, and\nclass-aware visual fusion. This paper proposes solutions to these limitations\nby inferring the actual loss, broadening evaluations to larger vision-language\nbackbones, and introducing Mixup-CLIPood, which incorporates a novel mix-up\nloss for enhanced class-aware visual fusion. Our method demonstrates superior\nperformance in domain generalization across multiple datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}