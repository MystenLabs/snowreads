{"id":"2407.16637","title":"Course-Correction: Safety Alignment Using Synthetic Preferences","authors":"Rongwu Xu, Yishuo Cai, Zhenhong Zhou, Renjie Gu, Haiqin Weng, Yan Liu,\n  Tianwei Zhang, Wei Xu, Han Qiu","authorsParsed":[["Xu","Rongwu",""],["Cai","Yishuo",""],["Zhou","Zhenhong",""],["Gu","Renjie",""],["Weng","Haiqin",""],["Liu","Yan",""],["Zhang","Tianwei",""],["Xu","Wei",""],["Qiu","Han",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 16:54:28 GMT"}],"updateDate":"2024-07-24","timestamp":1721753668000,"abstract":"  The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}