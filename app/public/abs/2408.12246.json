{"id":"2408.12246","title":"OVA-DETR: Open Vocabulary Aerial Object Detection Using Image-Text\n  Alignment and Fusion","authors":"Guoting Wei, Xia Yuan, Yu Liu, Zhenhao Shang, Kelu Yao, Chao Li,\n  Qingsen Yan, Chunxia Zhao, Haokui Zhang, Rong Xiao","authorsParsed":[["Wei","Guoting",""],["Yuan","Xia",""],["Liu","Yu",""],["Shang","Zhenhao",""],["Yao","Kelu",""],["Li","Chao",""],["Yan","Qingsen",""],["Zhao","Chunxia",""],["Zhang","Haokui",""],["Xiao","Rong",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 09:33:25 GMT"}],"updateDate":"2024-08-23","timestamp":1724319205000,"abstract":"  Aerial object detection has been a hot topic for many years due to its wide\napplication requirements. However, most existing approaches can only handle\npredefined categories, which limits their applicability for the open scenarios\nin real-world. In this paper, we extend aerial object detection to open\nscenarios by exploiting the relationship between image and text, and propose\nOVA-DETR, a high-efficiency open-vocabulary detector for aerial images.\nSpecifically, based on the idea of image-text alignment, we propose region-text\ncontrastive loss to replace the category regression loss in the traditional\ndetection framework, which breaks the category limitation. Then, we propose\nBidirectional Vision-Language Fusion (Bi-VLF), which includes a dual-attention\nfusion encoder and a multi-level text-guided Fusion Decoder. The dual-attention\nfusion encoder enhances the feature extraction process in the encoder part. The\nmulti-level text-guided Fusion Decoder is designed to improve the detection\nability for small objects, which frequently appear in aerial object detection\nscenarios. Experimental results on three widely used benchmark datasets show\nthat our proposed method significantly improves the mAP and recall, while\nenjoying faster inference speed. For instance, in zero shot detection\nexperiments on DIOR, the proposed OVA-DETR outperforms DescReg and YOLO-World\nby 37.4% and 33.1%, respectively, while achieving 87 FPS inference speed, which\nis 7.9x faster than DescReg and 3x faster than YOLO-world. The code is\navailable at https://github.com/GT-Wei/OVA-DETR.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}