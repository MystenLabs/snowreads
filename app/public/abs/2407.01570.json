{"id":"2407.01570","title":"Ego-Foresight: Agent Visuomotor Prediction as Regularization for RL","authors":"Manuel S. Nunes and Atabak Dehban and Yiannis Demiris and Jos\\'e\n  Santos-Victor","authorsParsed":[["Nunes","Manuel S.",""],["Dehban","Atabak",""],["Demiris","Yiannis",""],["Santos-Victor","Jos√©",""]],"versions":[{"version":"v1","created":"Mon, 27 May 2024 13:32:43 GMT"}],"updateDate":"2024-07-03","timestamp":1716816763000,"abstract":"  Despite the significant advancements in Deep Reinforcement Learning (RL)\nobserved in the last decade, the amount of training experience necessary to\nlearn effective policies remains one of the primary concerns both in simulated\nand real environments. Looking to solve this issue, previous work has shown\nthat improved training efficiency can be achieved by separately modeling agent\nand environment, but usually requiring a supervisory agent mask. In contrast to\nRL, humans can perfect a new skill from a very small number of trials and in\nmost cases do so without a supervisory signal, making neuroscientific studies\nof human development a valuable source of inspiration for RL. In particular, we\nexplore the idea of motor prediction, which states that humans develop an\ninternal model of themselves and of the consequences that their motor commands\nhave on the immediate sensory inputs. Our insight is that the movement of the\nagent provides a cue that allows the duality between agent and environment to\nbe learned. To instantiate this idea, we present Ego-Foresight, a\nself-supervised method for disentangling agent and environment based on motion\nand prediction. Our main finding is that visuomotor prediction of the agent\nprovides regularization to the RL algorithm, by encouraging the actions to stay\nwithin predictable bounds. To test our approach, we first study the ability of\nour model to visually predict agent movement irrespective of the environment,\nin real-world robotic interactions. Then, we integrate Ego-Foresight with a\nmodel-free RL algorithm to solve simulated robotic manipulation tasks, showing\nan average improvement of 23% in efficiency and 8% in performance.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Jg35I8FAsxsrMpZ3uPGlFNiUsKoNjtGlQxhFwFtQ42M","pdfSize":"18772451","objectId":"0x66e616e606833417ea53104d74b901fa2e8108493f22788a9c30af5ba346cdd2","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
