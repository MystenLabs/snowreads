{"id":"2407.16173","title":"Integrating Meshes and 3D Gaussians for Indoor Scene Reconstruction with\n  SAM Mask Guidance","authors":"Jiyeop Kim, Jongwoo Lim","authorsParsed":[["Kim","Jiyeop",""],["Lim","Jongwoo",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 04:39:04 GMT"}],"updateDate":"2024-07-24","timestamp":1721709544000,"abstract":"  We present a novel approach for 3D indoor scene reconstruction that combines\n3D Gaussian Splatting (3DGS) with mesh representations. We use meshes for the\nroom layout of the indoor scene, such as walls, ceilings, and floors, while\nemploying 3D Gaussians for other objects. This hybrid approach leverages the\nstrengths of both representations, offering enhanced flexibility and ease of\nediting. However, joint training of meshes and 3D Gaussians is challenging\nbecause it is not clear which primitive should affect which part of the\nrendered image. Objects close to the room layout often struggle during\ntraining, particularly when the room layout is textureless, which can lead to\nincorrect optimizations and unnecessary 3D Gaussians. To overcome these\nchallenges, we employ Segment Anything Model (SAM) to guide the selection of\nprimitives. The SAM mask loss enforces each instance to be represented by\neither Gaussians or meshes, ensuring clear separation and stable training.\nFurthermore, we introduce an additional densification stage without resetting\nthe opacity after the standard densification. This stage mitigates the\ndegradation of image quality caused by a limited number of 3D Gaussians after\nthe standard densification.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}