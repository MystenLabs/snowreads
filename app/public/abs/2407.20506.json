{"id":"2407.20506","title":"Boosting Efficiency in Task-Agnostic Exploration through Causal\n  Knowledge","authors":"Yupei Yang, Biwei Huang, Shikui Tu, Lei Xu","authorsParsed":[["Yang","Yupei",""],["Huang","Biwei",""],["Tu","Shikui",""],["Xu","Lei",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 02:51:21 GMT"}],"updateDate":"2024-07-31","timestamp":1722307881000,"abstract":"  The effectiveness of model training heavily relies on the quality of\navailable training resources. However, budget constraints often impose\nlimitations on data collection efforts. To tackle this challenge, we introduce\ncausal exploration in this paper, a strategy that leverages the underlying\ncausal knowledge for both data collection and model training. We, in\nparticular, focus on enhancing the sample efficiency and reliability of the\nworld model learning within the domain of task-agnostic reinforcement learning.\nDuring the exploration phase, the agent actively selects actions expected to\nyield causal insights most beneficial for world model training. Concurrently,\nthe causal knowledge is acquired and incrementally refined with the ongoing\ncollection of data. We demonstrate that causal exploration aids in learning\naccurate world models using fewer data and provide theoretical guarantees for\nits convergence. Empirical experiments, on both synthetic data and real-world\napplications, further validate the benefits of causal exploration.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}