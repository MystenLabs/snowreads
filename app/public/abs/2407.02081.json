{"id":"2407.02081","title":"On the Performance and Memory Footprint of Distributed Training: An\n  Empirical Study on Transformers","authors":"Zhengxian Lu, Fangyu Wang, Zhiwei Xu, Fei Yang, Tao Li","authorsParsed":[["Lu","Zhengxian",""],["Wang","Fangyu",""],["Xu","Zhiwei",""],["Yang","Fei",""],["Li","Tao",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 09:17:19 GMT"}],"updateDate":"2024-07-03","timestamp":1719911839000,"abstract":"  Transformer models have emerged as potent solutions to a wide array of\nmultidisciplinary challenges. The deployment of Transformer architectures is\nsignificantly hindered by their extensive computational and memory\nrequirements, necessitating the reliance on advanced efficient distributed\ntraining methodologies. Prior research has delved into the performance\nbottlenecks associated with distributed training, aiming to unravel these\nbottlenecks and suggest optimization directions. However, such analyses often\noverlook three aspects unique to Transformer models: the specialized\narchitecture, the dependency on various distributed strategies, and the\nrequirement to balance computational and memory overhead.\n  This paper aims to bridge this gap by offering a comprehensive examination of\nthe performance bottlenecks inherent in distributed training of Transformer\nmodels, leveraging both theoretical analysis and empirical investigation. We\npropose an analytical framework tailored to these unique aspects of\nTransformers, facilitating a holistic evaluation of model architectures,\ndistributed strategies, and resource consumption. Based on this analytical\nframework, we conduct a comparative analysis of theoretical performances and\nfurther systematically explore how various distributed training strategies fare\nin real-world scenarios. Most of the experimental results can be well explained\nby the analytical outcomes derived from the analytical framework. Notably, our\nfindings suggest an advantage of pipeline parallelism over data parallelism for\nTransformer models. Moreover, we shed light on some unexpected outcomes, such\nas the potential for increased total memory overhead due to suboptimal model\npartitioning within pipeline parallelism. Additionally, we underscore the\nsignificance of communication block size and waiting time to further enhance\nperformance.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}