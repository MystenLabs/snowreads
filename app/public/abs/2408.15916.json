{"id":"2408.15916","title":"Multi-modal Adversarial Training for Zero-Shot Voice Cloning","authors":"John Janiczek, Dading Chong, Dongyang Dai, Arlo Faria, Chao Wang, Tao\n  Wang, Yuzong Liu","authorsParsed":[["Janiczek","John",""],["Chong","Dading",""],["Dai","Dongyang",""],["Faria","Arlo",""],["Wang","Chao",""],["Wang","Tao",""],["Liu","Yuzong",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 16:30:41 GMT"}],"updateDate":"2024-08-29","timestamp":1724862641000,"abstract":"  A text-to-speech (TTS) model trained to reconstruct speech given text tends\ntowards predictions that are close to the average characteristics of a dataset,\nfailing to model the variations that make human speech sound natural. This\nproblem is magnified for zero-shot voice cloning, a task that requires training\ndata with high variance in speaking styles. We build off of recent works which\nhave used Generative Advsarial Networks (GAN) by proposing a Transformer\nencoder-decoder architecture to conditionally discriminates between real and\ngenerated speech features. The discriminator is used in a training pipeline\nthat improves both the acoustic and prosodic features of a TTS model. We\nintroduce our novel adversarial training technique by applying it to a\nFastSpeech2 acoustic model and training on Libriheavy, a large multi-speaker\ndataset, for the task of zero-shot voice cloning. Our model achieves\nimprovements over the baseline in terms of speech quality and speaker\nsimilarity. Audio examples from our system are available online.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Machine Learning","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}