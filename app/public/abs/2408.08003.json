{"id":"2408.08003","title":"Leveraging Web-Crawled Data for High-Quality Fine-Tuning","authors":"Jing Zhou, Chenglin Jiang, Wei Shen, Xiao Zhou, Xiaonan He","authorsParsed":[["Zhou","Jing",""],["Jiang","Chenglin",""],["Shen","Wei",""],["Zhou","Xiao",""],["He","Xiaonan",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 08:12:52 GMT"}],"updateDate":"2024-08-16","timestamp":1723709572000,"abstract":"  Most large language models are fine-tuned using either expensive\nhuman-annotated data or GPT-4 generated data which cannot guarantee performance\nin certain domains. We argue that although the web-crawled data often has\nformatting errors causing semantic inaccuracies, it can still serve as a\nvaluable source for high-quality supervised fine-tuning in specific domains\nwithout relying on advanced models like GPT-4. To this end, we create a paired\ntraining dataset automatically by aligning web-crawled data with a smaller set\nof high-quality data. By training a language model on this dataset, we can\nconvert web data with irregular formats into high-quality ones. Our experiments\nshow that training with the model-transformed data yields better results,\nsurpassing training with only high-quality data by an average score of 9.4% in\nChinese math problems. Additionally, our 7B model outperforms several\nopen-source models larger than 32B and surpasses well-known closed-source\nmodels such as GPT-3.5, highlighting the efficacy of our approach.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}