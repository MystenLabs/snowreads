{"id":"2408.02009","title":"Joint Learning of Emotions in Music and Generalized Sounds","authors":"Federico Simonetta, Francesca Certo, Stavros Ntalampiras","authorsParsed":[["Simonetta","Federico",""],["Certo","Francesca",""],["Ntalampiras","Stavros",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 12:19:03 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 09:28:16 GMT"}],"updateDate":"2024-08-15","timestamp":1722773943000,"abstract":"  In this study, we aim to determine if generalized sounds and music can share\na common emotional space, improving predictions of emotion in terms of arousal\nand valence. We propose the use of multiple datasets as a multi-domain learning\ntechnique. Our approach involves creating a common space encompassing features\nthat characterize both generalized sounds and music, as they can evoke emotions\nin a similar manner. To achieve this, we utilized two publicly available\ndatasets, namely IADS-E and PMEmo, following a standardized experimental\nprotocol. We employed a wide variety of features that capture diverse aspects\nof the audio structure including key parameters of spectrum, energy, and\nvoicing. Subsequently, we performed joint learning on the common feature space,\nleveraging heterogeneous model architectures. Interestingly, this synergistic\nscheme outperforms the state-of-the-art in both sound and music emotion\nprediction. The code enabling full replication of the presented experimental\npipeline is available at https://github.com/LIMUNIMI/MusicSoundEmotions.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"B8ZtzN5Mg_w6i41-UpdiiSQrW2nu-UeMwB15DoV4cBU","pdfSize":"793904"}
