{"id":"2407.15160","title":"When Can Transformers Count to n?","authors":"Gilad Yehudai, Haim Kaplan, Asma Ghandeharioun, Mor Geva, Amir\n  Globerson","authorsParsed":[["Yehudai","Gilad",""],["Kaplan","Haim",""],["Ghandeharioun","Asma",""],["Geva","Mor",""],["Globerson","Amir",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 13:31:02 GMT"}],"updateDate":"2024-07-23","timestamp":1721568662000,"abstract":"  Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}