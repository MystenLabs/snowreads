{"id":"2408.14342","title":"Dual-Domain CLIP-Assisted Residual Optimization Perception Model for\n  Metal Artifact Reduction","authors":"Xinrui Zhang and Ailong Cai and Shaoyu Wang and Linyuan Wang and\n  Zhizhong Zheng and Lei Li and Bin Yan","authorsParsed":[["Zhang","Xinrui",""],["Cai","Ailong",""],["Wang","Shaoyu",""],["Wang","Linyuan",""],["Zheng","Zhizhong",""],["Li","Lei",""],["Yan","Bin",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 02:37:26 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 09:11:13 GMT"}],"updateDate":"2024-08-30","timestamp":1723603046000,"abstract":"  Metal artifacts in computed tomography (CT) imaging pose significant\nchallenges to accurate clinical diagnosis. The presence of high-density\nmetallic implants results in artifacts that deteriorate image quality,\nmanifesting in the forms of streaking, blurring, or beam hardening effects,\netc. Nowadays, various deep learning-based approaches, particularly generative\nmodels, have been proposed for metal artifact reduction (MAR). However, these\nmethods have limited perception ability in the diverse morphologies of\ndifferent metal implants with artifacts, which may generate spurious anatomical\nstructures and exhibit inferior generalization capability. To address the\nissues, we leverage visual-language model (VLM) to identify these morphological\nfeatures and introduce them into a dual-domain CLIP-assisted residual\noptimization perception model (DuDoCROP) for MAR. Specifically, a dual-domain\nCLIP (DuDoCLIP) is fine-tuned on the image domain and sinogram domain using\ncontrastive learning to extract semantic descriptions from anatomical\nstructures and metal artifacts. Subsequently, a diffusion model is guided by\nthe embeddings of DuDoCLIP, thereby enabling the dual-domain prior generation.\nAdditionally, we design prompt engineering for more precise image-text\ndescriptions that can enhance the model's perception capability. Then, a\ndownstream task is devised for the one-step residual optimization and\nintegration of dual-domain priors, while incorporating raw data fidelity.\nUltimately, a new perceptual indicator is proposed to validate the model's\nperception and generation performance. With the assistance of DuDoCLIP, our\nDuDoCROP exhibits at least 63.7% higher generalization capability compared to\nthe baseline model. Numerical experiments demonstrate that the proposed method\ncan generate more realistic image structures and outperform other SOTA\napproaches both qualitatively and quantitatively.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Physics/Medical Physics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}