{"id":"2408.11500","title":"Slicing Input Features to Accelerate Deep Learning: A Case Study with\n  Graph Neural Networks","authors":"Zhengjia Xu, Dingyang Lyu, Jinghui Zhang","authorsParsed":[["Xu","Zhengjia",""],["Lyu","Dingyang",""],["Zhang","Jinghui",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 10:18:41 GMT"}],"updateDate":"2024-08-22","timestamp":1724235521000,"abstract":"  As graphs grow larger, full-batch GNN training becomes hard for single GPU\nmemory. Therefore, to enhance the scalability of GNN training, some studies\nhave proposed sampling-based mini-batch training and distributed graph\nlearning. However, these methods still have drawbacks, such as performance\ndegradation and heavy communication. This paper introduces SliceGCN, a\nfeature-sliced distributed large-scale graph learning method. SliceGCN slices\nthe node features, with each computing device, i.e., GPU, handling partial\nfeatures. After each GPU processes its share, partial representations are\nobtained and concatenated to form complete representations, enabling a single\nGPU's memory to handle the entire graph structure. This aims to avoid the\naccuracy loss typically associated with mini-batch training (due to incomplete\ngraph structures) and to reduce inter-GPU communication during message passing\n(the forward propagation process of GNNs). To study and mitigate potential\naccuracy reductions due to slicing features, this paper proposes feature fusion\nand slice encoding. Experiments were conducted on six node classification\ndatasets, yielding some interesting analytical results. These results indicate\nthat while SliceGCN does not enhance efficiency on smaller datasets, it does\nimprove efficiency on larger datasets. Additionally, we found that SliceGCN and\nits variants have better convergence, feature fusion and slice encoding can\nmake training more stable, reduce accuracy fluctuations, and this study also\ndiscovered that the design of SliceGCN has a potentially parameter-efficient\nnature.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}