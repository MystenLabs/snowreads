{"id":"2408.02788","title":"GazeXplain: Learning to Predict Natural Language Explanations of Visual\n  Scanpaths","authors":"Xianyu Chen and Ming Jiang and Qi Zhao","authorsParsed":[["Chen","Xianyu",""],["Jiang","Ming",""],["Zhao","Qi",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 19:11:46 GMT"}],"updateDate":"2024-08-07","timestamp":1722885106000,"abstract":"  While exploring visual scenes, humans' scanpaths are driven by their\nunderlying attention processes. Understanding visual scanpaths is essential for\nvarious applications. Traditional scanpath models predict the where and when of\ngaze shifts without providing explanations, creating a gap in understanding the\nrationale behind fixations. To bridge this gap, we introduce GazeXplain, a\nnovel study of visual scanpath prediction and explanation. This involves\nannotating natural-language explanations for fixations across eye-tracking\ndatasets and proposing a general model with an attention-language decoder that\njointly predicts scanpaths and generates explanations. It integrates a unique\nsemantic alignment mechanism to enhance the consistency between fixations and\nexplanations, alongside a cross-dataset co-training approach for\ngeneralization. These novelties present a comprehensive and adaptable solution\nfor explainable human visual scanpath prediction. Extensive experiments on\ndiverse eye-tracking datasets demonstrate the effectiveness of GazeXplain in\nboth scanpath prediction and explanation, offering valuable insights into human\nvisual attention and cognitive processes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}