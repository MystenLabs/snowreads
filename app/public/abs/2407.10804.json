{"id":"2407.10804","title":"Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning\n  and Format Alignment","authors":"Jinhao Jiang, Junyi Li, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong\n  Wen","authorsParsed":[["Jiang","Jinhao",""],["Li","Junyi",""],["Zhao","Wayne Xin",""],["Song","Yang",""],["Zhang","Tao",""],["Wen","Ji-Rong",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:20:13 GMT"}],"updateDate":"2024-07-16","timestamp":1721056813000,"abstract":"  Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}