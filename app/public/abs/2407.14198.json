{"id":"2407.14198","title":"Double-Shot 3D Shape Measurement with a Dual-Branch Network","authors":"Mingyang Lei, Jingfan Fan, Long Shao, Hong Song, Deqiang Xiao, Danni\n  Ai, Tianyu Fu, Ying Gu, and Jian Yang","authorsParsed":[["Lei","Mingyang",""],["Fan","Jingfan",""],["Shao","Long",""],["Song","Hong",""],["Xiao","Deqiang",""],["Ai","Danni",""],["Fu","Tianyu",""],["Gu","Ying",""],["Yang","Jian",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 10:49:26 GMT"}],"updateDate":"2024-07-22","timestamp":1721386166000,"abstract":"  The structured light (SL)-based 3D measurement techniques with deep learning\nhave been widely studied, among which speckle projection profilometry (SPP) and\nfringe projection profilometry (FPP) are two popular methods. However, they\ngenerally use a single projection pattern for reconstruction, resulting in\nfringe order ambiguity or poor reconstruction accuracy. To alleviate these\nproblems, we propose a parallel dual-branch Convolutional Neural Network\n(CNN)-Transformer network (PDCNet), to take advantage of convolutional\noperations and self-attention mechanisms for processing different SL\nmodalities. Within PDCNet, a Transformer branch is used to capture global\nperception in the fringe images, while a CNN branch is designed to collect\nlocal details in the speckle images. To fully integrate complementary features,\nwe design a double-stream attention aggregation module (DAAM) that consist of a\nparallel attention subnetwork for aggregating multi-scale spatial structure\ninformation. This module can dynamically retain local and global\nrepresentations to the maximum extent. Moreover, an adaptive mixture density\nhead with bimodal Gaussian distribution is proposed for learning a\nrepresentation that is precise near discontinuities. Compared to the standard\ndisparity regression strategy, this adaptive mixture head can effectively\nimproves performance at object boundaries. Extensive experiments demonstrate\nthat our method can reduce fringe order ambiguity while producing high-accuracy\nresults on a self-made dataset. We also show that the proposed architecture\nreveals the potential in infrared-visible image fusion task.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}