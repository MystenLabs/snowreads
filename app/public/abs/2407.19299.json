{"id":"2407.19299","title":"The Impact of LoRA Adapters for LLMs on Clinical NLP Classification\n  Under Data Limitations","authors":"Thanh-Dung Le, Ti Ti Nguyen, Vu Nguyen Ha","authorsParsed":[["Le","Thanh-Dung",""],["Nguyen","Ti Ti",""],["Ha","Vu Nguyen",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 16:48:03 GMT"}],"updateDate":"2024-07-30","timestamp":1722098883000,"abstract":"  Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to the domain gap and limited\ndata availability. This study investigates the effectiveness of various adapter\ntechniques, equivalent to Low-Rank Adaptation (LoRA), for fine-tuning LLMs in a\nresource-constrained hospital environment. We experimented with four\nstructures-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN)-as final layers for clinical notes classification. We fine-tuned\nbiomedical pre-trained models, including CamemBERT-bio, AliBERT, and DrBERT,\nalongside two Transformer-based models. Our extensive experimental results\nindicate that i) employing adapter structures does not yield significant\nimprovements in fine-tuning biomedical pre-trained LLMs, and ii) simpler\nTransformer-based models, trained from scratch, perform better under resource\nconstraints. Among the adapter structures, GRN demonstrated superior\nperformance with accuracy, precision, recall, and an F1 score of 0.88.\nMoreover, the total training time for LLMs exceeded 1000 hours, compared to\nunder 6 hours for simpler transformer-based models, highlighting that LLMs are\nmore suitable for environments with extensive computational resources and\nlarger datasets. Consequently, this study demonstrates that simpler\nTransformer-based models can be effectively trained from scratch, providing a\nviable solution for clinical NLP tasks in low-resource environments with\nlimited data availability. By identifying the GRN as the most effective adapter\nstructure, we offer a practical approach to enhance clinical note\nclassification without requiring extensive computational resources.\n","subjects":["Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}