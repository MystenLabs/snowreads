{"id":"2407.14106","title":"TorchGT: A Holistic System for Large-scale Graph Transformer Training","authors":"Meng Zhang, Jie Sun, Qinghao Hu, Peng Sun, Zeke Wang, Yonggang Wen,\n  Tianwei Zhang","authorsParsed":[["Zhang","Meng",""],["Sun","Jie",""],["Hu","Qinghao",""],["Sun","Peng",""],["Wang","Zeke",""],["Wen","Yonggang",""],["Zhang","Tianwei",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 08:21:42 GMT"}],"updateDate":"2024-07-22","timestamp":1721377302000,"abstract":"  Graph Transformer is a new architecture that surpasses GNNs in graph\nlearning. While there emerge inspiring algorithm advancements, their practical\nadoption is still limited, particularly on real-world graphs involving up to\nmillions of nodes. We observe existing graph transformers fail on large-scale\ngraphs mainly due to heavy computation, limited scalability and inferior model\nquality. Motivated by these observations, we propose TorchGT, the first\nefficient, scalable, and accurate graph transformer training system. TorchGT\noptimizes training at different levels. At algorithm level, by harnessing the\ngraph sparsity, TorchGT introduces a Dual-interleaved Attention which is\ncomputation-efficient and accuracy-maintained. At runtime level, TorchGT scales\ntraining across workers with a communication-light Cluster-aware Graph\nParallelism. At kernel level, an Elastic Computation Reformation further\noptimizes the computation by reducing memory access latency in a dynamic way.\nExtensive experiments demonstrate that TorchGT boosts training by up to 62.7x\nand supports graph sequence lengths of up to 1M.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}