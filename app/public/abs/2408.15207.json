{"id":"2408.15207","title":"Investigating Coverage Criteria in Large Language Models: An In-Depth\n  Study Through Jailbreak Attacks","authors":"Shide Zhou and Tianlin Li and Kailong Wang and Yihao Huang and Ling\n  Shi and Yang Liu and Haoyu Wang","authorsParsed":[["Zhou","Shide",""],["Li","Tianlin",""],["Wang","Kailong",""],["Huang","Yihao",""],["Shi","Ling",""],["Liu","Yang",""],["Wang","Haoyu",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 17:14:21 GMT"}],"updateDate":"2024-08-28","timestamp":1724778861000,"abstract":"  The swift advancement of large language models (LLMs) has profoundly shaped\nthe landscape of artificial intelligence; however, their deployment in\nsensitive domains raises grave concerns, particularly due to their\nsusceptibility to malicious exploitation. This situation underscores the\ninsufficiencies in pre-deployment testing, highlighting the urgent need for\nmore rigorous and comprehensive evaluation methods. This study presents a\ncomprehensive empirical analysis assessing the efficacy of conventional\ncoverage criteria in identifying these vulnerabilities, with a particular\nemphasis on the pressing issue of jailbreak attacks. Our investigation begins\nwith a clustering analysis of the hidden states in LLMs, demonstrating that\nintrinsic characteristics of these states can distinctly differentiate between\nvarious types of queries. Subsequently, we assess the performance of these\ncriteria across three critical dimensions: criterion level, layer level, and\ntoken level. Our findings uncover significant disparities in neuron activation\npatterns between the processing of normal and jailbreak queries, thereby\ncorroborating the clustering results. Leveraging these findings, we propose an\ninnovative approach for the real-time detection of jailbreak attacks by\nutilizing neural activation features. Our classifier demonstrates remarkable\naccuracy, averaging 96.33% in identifying jailbreak queries, including those\nthat could lead to adversarial attacks. The importance of our research lies in\nits comprehensive approach to addressing the intricate challenges of LLM\nsecurity. By enabling instantaneous detection from the model's first token\noutput, our method holds promise for future systems integrating LLMs, offering\nrobust real-time detection capabilities. This study advances our understanding\nof LLM security testing, and lays a critical foundation for the development of\nmore resilient AI systems.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}