{"id":"2408.09974","title":"The Exploration-Exploitation Dilemma Revisited: An Entropy Perspective","authors":"Renye Yan, Yaozhong Gan, You Wu, Ling Liang, Junliang Xing, Yimao Cai,\n  Ru Huang","authorsParsed":[["Yan","Renye",""],["Gan","Yaozhong",""],["Wu","You",""],["Liang","Ling",""],["Xing","Junliang",""],["Cai","Yimao",""],["Huang","Ru",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 13:21:46 GMT"}],"updateDate":"2024-08-20","timestamp":1724073706000,"abstract":"  The imbalance of exploration and exploitation has long been a significant\nchallenge in reinforcement learning. In policy optimization, excessive reliance\non exploration reduces learning efficiency, while over-dependence on\nexploitation might trap agents in local optima. This paper revisits the\nexploration-exploitation dilemma from the perspective of entropy by revealing\nthe relationship between entropy and the dynamic adaptive process of\nexploration and exploitation. Based on this theoretical insight, we establish\nan end-to-end adaptive framework called AdaZero, which automatically determines\nwhether to explore or to exploit as well as their balance of strength.\nExperiments show that AdaZero significantly outperforms baseline models across\nvarious Atari and MuJoCo environments with only a single setting. Especially in\nthe challenging environment of Montezuma, AdaZero boosts the final returns by\nup to fifteen times. Moreover, we conduct a series of visualization analyses to\nreveal the dynamics of our self-adaptive mechanism, demonstrating how entropy\nreflects and changes with respect to the agent's performance and adaptive\nprocess.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}