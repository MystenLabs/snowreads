{"id":"2408.08073","title":"Extracting Sentence Embeddings from Pretrained Transformer Models","authors":"Lukas Stankevi\\v{c}ius and Mantas Luko\\v{s}evi\\v{c}ius","authorsParsed":[["Stankevičius","Lukas",""],["Lukoševičius","Mantas",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 10:54:55 GMT"}],"updateDate":"2024-08-16","timestamp":1723719295000,"abstract":"  Background/introduction: Pre-trained transformer models shine in many natural\nlanguage processing tasks and therefore are expected to bear the representation\nof the input sentence or text meaning. These sentence-level embeddings are also\nimportant in retrieval-augmented generation. But do commonly used plain\naveraging or prompt templates surface it enough?\n  Methods: Given 110M parameters BERT's hidden representations from multiple\nlayers and multiple tokens we tried various ways to extract optimal sentence\nrepresentations. We tested various token aggregation and representation\npost-processing techniques. We also tested multiple ways of using a general\nWikitext dataset to complement BERTs sentence representations. All methods were\ntested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12\nclassification tasks. We also evaluated our representation-shaping techniques\non other static models, including random token representations.\n  Results: Proposed representation extraction methods improved the performance\non STS and clustering tasks for all models considered. Very high improvements\nfor static token-based models, especially random embeddings for STS tasks\nalmost reach the performance of BERT-derived representations.\n  Conclusions: Our work shows that for multiple tasks simple baselines with\nrepresentation shaping techniques reach or even outperform more complex\nBERT-based models or are able to contribute to their performance.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}