{"id":"2407.10488","title":"How and where does CLIP process negation?","authors":"Vincent Quantmeyer and Pablo Mosteiro and Albert Gatt","authorsParsed":[["Quantmeyer","Vincent",""],["Mosteiro","Pablo",""],["Gatt","Albert",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 07:20:06 GMT"}],"updateDate":"2024-07-16","timestamp":1721028006000,"abstract":"  Various benchmarks have been proposed to test linguistic understanding in\npre-trained vision \\& language (VL) models. Here we build on the existence task\nfrom the VALSE benchmark (Parcalabescu et al, 2022) which we use to test\nmodels' understanding of negation, a particularly interesting issue for\nmultimodal models. However, while such VL benchmarks are useful for measuring\nmodel performance, they do not reveal anything about the internal processes\nthrough which these models arrive at their outputs in such visio-linguistic\ntasks. We take inspiration from the growing literature on model\ninterpretability to explain the behaviour of VL models on the understanding of\nnegation. Specifically, we approach these questions through an in-depth\nanalysis of the text encoder in CLIP (Radford et al, 2021), a highly\ninfluential VL model. We localise parts of the encoder that process negation\nand analyse the role of attention heads in this task. Our contributions are\nthreefold. We demonstrate how methods from the language model interpretability\nliterature (such as causal tracing) can be translated to multimodal models and\ntasks; we provide concrete insights into how CLIP processes negation on the\nVALSE existence task; and we highlight inherent limitations in the VALSE\ndataset as a benchmark for linguistic understanding.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}