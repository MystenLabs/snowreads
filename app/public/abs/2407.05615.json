{"id":"2407.05615","title":"OSN: Infinite Representations of Dynamic 3D Scenes from Monocular Videos","authors":"Ziyang Song, Jinxi Li, Bo Yang","authorsParsed":[["Song","Ziyang",""],["Li","Jinxi",""],["Yang","Bo",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 05:03:46 GMT"}],"updateDate":"2024-07-09","timestamp":1720415026000,"abstract":"  It has long been challenging to recover the underlying dynamic 3D scene\nrepresentations from a monocular RGB video. Existing works formulate this\nproblem into finding a single most plausible solution by adding various\nconstraints such as depth priors and strong geometry constraints, ignoring the\nfact that there could be infinitely many 3D scene representations corresponding\nto a single dynamic video. In this paper, we aim to learn all plausible 3D\nscene configurations that match the input video, instead of just inferring a\nspecific one. To achieve this ambitious goal, we introduce a new framework,\ncalled OSN. The key to our approach is a simple yet innovative object scale\nnetwork together with a joint optimization module to learn an accurate scale\nrange for every dynamic 3D object. This allows us to sample as many faithful 3D\nscene configurations as possible. Extensive experiments show that our method\nsurpasses all baselines and achieves superior accuracy in dynamic novel view\nsynthesis on multiple synthetic and real-world datasets. Most notably, our\nmethod demonstrates a clear advantage in learning fine-grained 3D scene\ngeometry. Our code and data are available at https://github.com/vLAR-group/OSN\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics","Computing Research Repository/Machine Learning","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}