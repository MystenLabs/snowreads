{"id":"2408.17065","title":"Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level\n  Blending and Spatiotemporal Adapter Tuning","authors":"Zhiyuan Yan, Yandan Zhao, Shen Chen, Xinghe Fu, Taiping Yao, Shouhong\n  Ding, Li Yuan","authorsParsed":[["Yan","Zhiyuan",""],["Zhao","Yandan",""],["Chen","Shen",""],["Fu","Xinghe",""],["Yao","Taiping",""],["Ding","Shouhong",""],["Yuan","Li",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 07:49:57 GMT"}],"updateDate":"2024-09-02","timestamp":1725004197000,"abstract":"  Three key challenges hinder the development of current deepfake video\ndetection: (1) Temporal features can be complex and diverse: how can we\nidentify general temporal artifacts to enhance model generalization? (2)\nSpatiotemporal models often lean heavily on one type of artifact and ignore the\nother: how can we ensure balanced learning from both? (3) Videos are naturally\nresource-intensive: how can we tackle efficiency without compromising accuracy?\n  This paper attempts to tackle the three challenges jointly. First, inspired\nby the notable generality of using image-level blending data for image forgery\ndetection, we investigate whether and how video-level blending can be effective\nin video. We then perform a thorough analysis and identify a previously\nunderexplored temporal forgery artifact: Facial Feature Drift (FFD), which\ncommonly exists across different forgeries. To reproduce FFD, we then propose a\nnovel Video-level Blending data (VB), where VB is implemented by blending the\noriginal image and its warped version frame-by-frame, serving as a hard\nnegative sample to mine more general artifacts. Second, we carefully design a\nlightweight Spatiotemporal Adapter (StA) to equip a pretrained image model\n(both ViTs and CNNs) with the ability to capture both spatial and temporal\nfeatures jointly and efficiently. StA is designed with two-stream 3D-Conv with\nvarying kernel sizes, allowing it to process spatial and temporal features\nseparately. Extensive experiments validate the effectiveness of the proposed\nmethods; and show our approach can generalize well to previously unseen forgery\nvideos, even the just-released (in 2024) SoTAs. We release our code and\npretrained weights at \\url{https://github.com/YZY-stack/StA4Deepfake}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/publicdomain/zero/1.0/"}