{"id":"2408.11049","title":"MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding","authors":"Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan\n  Shi, Ian En-Hsu Yen, Beidi Chen","authorsParsed":[["Chen","Jian",""],["Tiwari","Vashisth",""],["Sadhukhan","Ranajoy",""],["Chen","Zhuoming",""],["Shi","Jinyuan",""],["Yen","Ian En-Hsu",""],["Chen","Beidi",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 17:57:31 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 17:55:29 GMT"},{"version":"v3","created":"Fri, 23 Aug 2024 17:54:34 GMT"}],"updateDate":"2024-08-26","timestamp":1724176651000,"abstract":"  Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}