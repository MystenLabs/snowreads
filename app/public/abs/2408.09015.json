{"id":"2408.09015","title":"AdaRank: Disagreement Based Module Rank Prediction for Low-rank\n  Adaptation","authors":"Yihe Dong","authorsParsed":[["Dong","Yihe",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 20:48:41 GMT"}],"updateDate":"2024-08-20","timestamp":1723841321000,"abstract":"  With the rise of language and multimodal models of ever-increasing size,\npretraining a general-purpose foundational model and adapting it to downstream\ntasks has become common practice. To this end, adaptation efficiency can be a\ncritical bottleneck given the large model sizes, hence efficient finetuning\nmethods such as LoRA have become prevalent. However, LoRA is typically applied\nwith the same rank across all model layers, despite mounting evidence from\ntransfer learning literature that during finetuning, later layers diverge more\nfrom pretrained weights. Inspired by the theory and observations around feature\nlearning and module criticality, we develop a simple model disagreement based\ntechnique to predict the rank of a given module relative to the other modules.\nEmpirically, AdaRank generalizes notably better on unseen data than using\nuniform ranks with the same number of parameters. Compared to prior work,\nAdaRank has the unique advantage of leaving the pretraining and adaptation\nstages completely intact: no need for any additional objectives or\nregularizers, which can hinder adaptation accuracy and performance. Our code is\npublicly available at\nhttps://github.com/google-research/google-research/tree/master/adaptive_low_rank.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"LEMK6lIpP6CGHH-gMTQgt3vH4mqDqoj7TWVpWtvSyN0","pdfSize":"435374"}
