{"id":"2408.10536","title":"Synergistic Approach for Simultaneous Optimization of Monolingual,\n  Cross-lingual, and Multilingual Information Retrieval","authors":"Adel Elmahdy, Sheng-Chieh Lin, Amin Ahmad","authorsParsed":[["Elmahdy","Adel",""],["Lin","Sheng-Chieh",""],["Ahmad","Amin",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 04:30:26 GMT"}],"updateDate":"2024-08-21","timestamp":1724128226000,"abstract":"  Information retrieval across different languages is an increasingly important\nchallenge in natural language processing. Recent approaches based on\nmultilingual pre-trained language models have achieved remarkable success, yet\nthey often optimize for either monolingual, cross-lingual, or multilingual\nretrieval performance at the expense of others. This paper proposes a novel\nhybrid batch training strategy to simultaneously improve zero-shot retrieval\nperformance across monolingual, cross-lingual, and multilingual settings while\nmitigating language bias. The approach fine-tunes multilingual language models\nusing a mix of monolingual and cross-lingual question-answer pair batches\nsampled based on dataset size. Experiments on XQuAD-R, MLQA-R, and MIRACL\nbenchmark datasets show that the proposed method consistently achieves\ncomparable or superior results in zero-shot retrieval across various languages\nand retrieval tasks compared to monolingual-only or cross-lingual-only\ntraining. Hybrid batch training also substantially reduces language bias in\nmultilingual retrieval compared to monolingual training. These results\ndemonstrate the effectiveness of the proposed approach for learning\nlanguage-agnostic representations that enable strong zero-shot retrieval\nperformance across diverse languages.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}