{"id":"2408.02354","title":"RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders","authors":"Danil Gusak, Gleb Mezentsev, Ivan Oseledets, Evgeny Frolov","authorsParsed":[["Gusak","Danil",""],["Mezentsev","Gleb",""],["Oseledets","Ivan",""],["Frolov","Evgeny",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 10:02:29 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 10:11:28 GMT"},{"version":"v3","created":"Wed, 14 Aug 2024 15:19:41 GMT"}],"updateDate":"2024-08-15","timestamp":1722852149000,"abstract":"  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}