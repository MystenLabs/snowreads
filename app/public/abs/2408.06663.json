{"id":"2408.06663","title":"Amuro & Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models","authors":"Kaiser Sun, Mark Dredze","authorsParsed":[["Sun","Kaiser",""],["Dredze","Mark",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 06:28:43 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 15:23:38 GMT"}],"updateDate":"2024-08-15","timestamp":1723530523000,"abstract":"  The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}