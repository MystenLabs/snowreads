{"id":"2407.04181","title":"Orchestrating LLMs with Different Personalizations","authors":"Jin Peng Zhou, Katie Z Luo, Jingwen Gu, Jason Yuan, Kilian Q.\n  Weinberger, Wen Sun","authorsParsed":[["Zhou","Jin Peng",""],["Luo","Katie Z",""],["Gu","Jingwen",""],["Yuan","Jason",""],["Weinberger","Kilian Q.",""],["Sun","Wen",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 22:55:02 GMT"}],"updateDate":"2024-07-08","timestamp":1720133702000,"abstract":"  This paper presents a novel approach to aligning large language models (LLMs)\nwith individual human preferences, sometimes referred to as Reinforcement\nLearning from \\textit{Personalized} Human Feedback (RLPHF). Given stated\npreferences along multiple dimensions, such as helpfulness, conciseness, or\nhumor, the goal is to create an LLM without re-training that best adheres to\nthis specification. Starting from specialized expert LLMs, each trained for one\nsuch particular preference dimension, we propose a black-box method that merges\ntheir outputs on a per-token level. We train a lightweight Preference Control\nModel (PCM) that dynamically translates the preference description and current\ncontext into next-token prediction weights. By combining the expert models'\noutputs at the token level, our approach dynamically generates text that\noptimizes the given preference. Empirical tests show that our method matches or\nsurpasses existing preference merging techniques, providing a scalable,\nefficient alternative to fine-tuning LLMs for individual personalization.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}