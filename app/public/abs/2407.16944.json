{"id":"2407.16944","title":"Adaptive Gradient Regularization: A Faster and Generalizable\n  Optimization Technique for Deep Neural Networks","authors":"Huixiu Jiang, Ling Yang, Yu Bao, Rutong Si, Sikun Yang","authorsParsed":[["Jiang","Huixiu",""],["Yang","Ling",""],["Bao","Yu",""],["Si","Rutong",""],["Yang","Sikun",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 02:23:18 GMT"},{"version":"v2","created":"Wed, 31 Jul 2024 00:37:20 GMT"},{"version":"v3","created":"Fri, 2 Aug 2024 06:05:10 GMT"},{"version":"v4","created":"Tue, 20 Aug 2024 01:21:38 GMT"}],"updateDate":"2024-08-21","timestamp":1721787798000,"abstract":"  Stochastic optimization plays a crucial role in the advancement of deep\nlearning technologies. Over the decades, significant effort has been dedicated\nto improving the training efficiency and robustness of deep neural networks,\nvia various strategies including gradient normalization (GN) and gradient\ncentralization (GC). Nevertheless, to the best of our knowledge, no one has\nconsidered to capture the optimal gradient descent trajectory, by adaptively\ncontrolling gradient descent direction. To address this concern, this paper is\nthe first attempt to study a new optimization technique for deep neural\nnetworks, using the sum normalization of a gradient vector as coefficients, to\ndynamically regularize gradients and thus to effectively control optimization\ndirection. The proposed technique is hence named as the adaptive gradient\nregularization (AGR). It can be viewed as an adaptive gradient clipping method.\nThe theoretical analysis reveals that the AGR can effectively smooth the loss\nlandscape, and hence can significantly improve the training efficiency and\nmodel generalization performance. We note that AGR can greatly improve the\ntraining efficiency of vanilla optimizers' including Adan and AdamW, by adding\nonly three lines of code. The final experiments conducted on image generation,\nimage classification, and language representation, demonstrate that the AGR\nmethod can not only improve the training efficiency but also enhance the model\ngeneralization performance.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}