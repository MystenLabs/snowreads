{"id":"2407.11817","title":"A gradient flow on control space with rough initial condition","authors":"Paul Gassiat and Florin Suciu","authorsParsed":[["Gassiat","Paul",""],["Suciu","Florin",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:07:19 GMT"}],"updateDate":"2024-07-17","timestamp":1721142439000,"abstract":"  We consider the (sub-Riemannian type) control problem of finding a path going\nfrom an initial point $x$ to a target point $y$, by only moving in certain\nadmissible directions. We assume that the corresponding vector fields satisfy\nthe bracket-generating (H\\\"ormander) condition, so that the classical\nChow-Rashevskii theorem guarantees the existence of such a path. One natural\nway to try to solve this problem is via a gradient flow on control space.\nHowever, since the corresponding dynamics may have saddle points, any\nconvergence result must rely on suitable (e.g. random) initialisation. We\nconsider the case when this initialisation is irregular, which is conveniently\nformulated via Lyons' rough path theory. We show that one advantage of this\ninitialisation is that the saddle points are moved to infinity, while minima\nremain at a finite distance from the starting point. In the step $2$-nilpotent\ncase, we further manage to prove that the gradient flow converges to a\nsolution, if the initial condition is the path of a Brownian motion (or\nrougher). The proof is based on combining ideas from Malliavin calculus with\n{\\L}ojasiewicz inequalities. A possible motivation for our study comes from the\ntraining of deep Residual Neural Nets, in the regime when the number of\ntrainable parameters per layer is smaller than the dimension of the data\nvector.\n","subjects":["Mathematics/Optimization and Control","Mathematics/Probability"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}