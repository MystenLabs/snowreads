{"id":"2408.05457","title":"Investigating Instruction Tuning Large Language Models on Graphs","authors":"Kerui Zhu, Bo-Wei Huang, Bowen Jin, Yizhu Jiao, Ming Zhong, Kevin\n  Chang, Shou-De Lin, Jiawei Han","authorsParsed":[["Zhu","Kerui",""],["Huang","Bo-Wei",""],["Jin","Bowen",""],["Jiao","Yizhu",""],["Zhong","Ming",""],["Chang","Kevin",""],["Lin","Shou-De",""],["Han","Jiawei",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 06:54:35 GMT"}],"updateDate":"2024-08-13","timestamp":1723272875000,"abstract":"  Inspired by the recent advancements of Large Language Models (LLMs) in NLP\ntasks, there's growing interest in applying LLMs to graph-related tasks. This\nstudy delves into the capabilities of instruction-following LLMs for engaging\nwith real-world graphs, aiming to offer empirical insights into how LLMs can\neffectively interact with graphs and generalize across graph tasks. We begin by\nconstructing a dataset designed for instruction tuning, which comprises a\ndiverse collection of 79 graph-related tasks from academic and e-commerce\ndomains, featuring 44,240 training instances and 18,960 test samples. Utilizing\nthis benchmark, our initial investigation focuses on identifying the optimal\ngraph representation that serves as a conduit for LLMs to understand complex\ngraph structures. Our findings indicate that JSON format for graph\nrepresentation consistently outperforms natural language and code formats\nacross various LLMs and graph types. Furthermore, we examine the key factors\nthat influence the generalization abilities of instruction-tuned LLMs by\nevaluating their performance on both in-domain and out-of-domain graph tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}