{"id":"2408.07472","title":"Unsupervised Blind Joint Dereverberation and Room Acoustics Estimation\n  with Diffusion Models","authors":"Jean-Marie Lemercier, Eloi Moliner, Simon Welker, Vesa V\\\"alim\\\"aki,\n  Timo Gerkmann","authorsParsed":[["Lemercier","Jean-Marie",""],["Moliner","Eloi",""],["Welker","Simon",""],["Välimäki","Vesa",""],["Gerkmann","Timo",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 11:31:32 GMT"}],"updateDate":"2024-08-15","timestamp":1723635092000,"abstract":"  This paper presents an unsupervised method for single-channel blind\ndereverberation and room impulse response (RIR) estimation, called BUDDy. The\nalgorithm is rooted in Bayesian posterior sampling: it combines a likelihood\nmodel enforcing fidelity to the reverberant measurement, and an anechoic speech\nprior implemented by an unconditional diffusion model. We design a parametric\nfilter representing the RIR, with exponential decay for each frequency subband.\nRoom acoustics estimation and speech dereverberation are jointly carried out,\nas the filter parameters are iteratively estimated and the speech utterance\nrefined along the reverse diffusion trajectory. In a blind scenario where the\nroom impulse response is unknown, BUDDy successfully performs speech\ndereverberation in various acoustic scenarios, significantly outperforming\nother blind unsupervised baselines. Unlike supervised methods, which often\nstruggle to generalize, BUDDy seamlessly adapts to different acoustic\nconditions. This paper extends our previous work by offering new experimental\nresults and insights into the algorithm's performance and versatility. We first\ninvestigate the robustness of informed dereverberation methods to RIR\nestimation errors, to motivate the joint acoustic estimation and\ndereverberation paradigm. Then, we demonstrate the adaptability of our method\nto high-resolution singing voice dereverberation, study its performance in RIR\nestimation, and conduct subjective evaluation experiments to validate the\nperceptual quality of the results, among other contributions. Audio samples and\ncode can be found online.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Machine Learning","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}