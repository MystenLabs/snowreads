{"id":"2408.09112","title":"Training Verifiably Robust Agents Using Set-Based Reinforcement Learning","authors":"Manuel Wendl, Lukas Koller, Tobias Ladner, Matthias Althoff","authorsParsed":[["Wendl","Manuel",""],["Koller","Lukas",""],["Ladner","Tobias",""],["Althoff","Matthias",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 06:26:17 GMT"}],"updateDate":"2024-08-20","timestamp":1723875977000,"abstract":"  Reinforcement learning often uses neural networks to solve complex control\ntasks. However, neural networks are sensitive to input perturbations, which\nmakes their deployment in safety-critical environments challenging. This work\nlifts recent results from formally verifying neural networks against such\ndisturbances to reinforcement learning in continuous state and action spaces\nusing reachability analysis. While previous work mainly focuses on adversarial\nattacks for robust reinforcement learning, we train neural networks utilizing\nentire sets of perturbed inputs and maximize the worst-case reward. The\nobtained agents are verifiably more robust than agents obtained by related\nwork, making them more applicable in safety-critical environments. This is\ndemonstrated with an extensive empirical evaluation of four different\nbenchmarks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Robotics","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}