{"id":"2407.08720","title":"UNRealNet: Learning Uncertainty-Aware Navigation Features from\n  High-Fidelity Scans of Real Environments","authors":"Samuel Triest, David D. Fan, Sebastian Scherer, and Ali-Akbar\n  Agha-Mohammadi","authorsParsed":[["Triest","Samuel",""],["Fan","David D.",""],["Scherer","Sebastian",""],["Agha-Mohammadi","Ali-Akbar",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:54:44 GMT"}],"updateDate":"2024-07-12","timestamp":1720720484000,"abstract":"  Traversability estimation in rugged, unstructured environments remains a\nchallenging problem in field robotics. Often, the need for precise, accurate\ntraversability estimation is in direct opposition to the limited sensing and\ncompute capability present on affordable, small-scale mobile robots. To address\nthis issue, we present a novel method to learn [u]ncertainty-aware [n]avigation\nfeatures from high-fidelity scans of [real]-world environments (UNRealNet).\nThis network can be deployed on-robot to predict these high-fidelity features\nusing input from lower-quality sensors. UNRealNet predicts dense, metric-space\nfeatures directly from single-frame lidar scans, thus reducing the effects of\nocclusion and odometry error. Our approach is label-free, and is able to\nproduce traversability estimates that are robot-agnostic. Additionally, we can\nleverage UNRealNet's predictive uncertainty to both produce risk-aware\ntraversability estimates, and refine our feature predictions over time. We find\nthat our method outperforms traditional local mapping and inpainting baselines\nby up to 40%, and demonstrate its efficacy on multiple legged platforms.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RcIcuum3pk35UuWCnbKpWHtLfXIy2dh8FnkP1-DR4vk","pdfSize":"5867500"}
