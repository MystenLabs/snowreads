{"id":"2407.07464","title":"Video-to-Audio Generation with Hidden Alignment","authors":"Manjie Xu, Chenxing Li, Yong Ren, Rilin Chen, Yu Gu, Wei Liang, Dong\n  Yu","authorsParsed":[["Xu","Manjie",""],["Li","Chenxing",""],["Ren","Yong",""],["Chen","Rilin",""],["Gu","Yu",""],["Liang","Wei",""],["Yu","Dong",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 08:40:39 GMT"}],"updateDate":"2024-07-11","timestamp":1720600839000,"abstract":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model VTA-LDM built\non a simple yet surprisingly effective intuition, we explore various vision\nencoders and auxiliary embeddings through ablation studies. Employing a\ncomprehensive evaluation pipeline that emphasizes generation quality and\nvideo-audio synchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}