{"id":"2407.07672","title":"StoryDiffusion: How to Support UX Storyboarding With Generative-AI","authors":"Zhaohui Liang, Xiaoyu Zhang, Kevin Ma, Zhao Liu, Xipei Ren, Kosa\n  Goucher-Lambert, Can Liu","authorsParsed":[["Liang","Zhaohui",""],["Zhang","Xiaoyu",""],["Ma","Kevin",""],["Liu","Zhao",""],["Ren","Xipei",""],["Goucher-Lambert","Kosa",""],["Liu","Can",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:59:37 GMT"}],"updateDate":"2024-07-11","timestamp":1720619977000,"abstract":"  Storyboarding is an established method for designing user experiences.\nGenerative AI can support this process by helping designers quickly create\nvisual narratives. However, existing tools only focus on accurate text-to-image\ngeneration. Currently, it is not clear how to effectively support the entire\ncreative process of storyboarding and how to develop AI-powered tools to\nsupport designers' individual workflows. In this work, we iteratively developed\nand implemented StoryDiffusion, a system that integrates text-to-text and\ntext-to-image models, to support the generation of narratives and images in a\nsingle pipeline. With a user study, we observed 12 UX designers using the\nsystem for both concept ideation and illustration tasks. Our findings\nidentified AI-directed vs. user-directed creative strategies in both tasks and\nrevealed the importance of supporting the interchange between narrative\niteration and image generation. We also found effects of the design tasks on\ntheir strategies and preferences, providing insights for future development.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}