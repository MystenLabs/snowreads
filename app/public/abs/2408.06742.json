{"id":"2408.06742","title":"Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to\n  Tail","authors":"Yina He, Lei Peng, Yongcun Zhang, Juanjuan Weng, Zhiming Luo, Shaozi\n  Li","authorsParsed":[["He","Yina",""],["Peng","Lei",""],["Zhang","Yongcun",""],["Weng","Juanjuan",""],["Luo","Zhiming",""],["Li","Shaozi",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 09:03:00 GMT"},{"version":"v2","created":"Sat, 24 Aug 2024 13:41:04 GMT"}],"updateDate":"2024-08-27","timestamp":1723539780000,"abstract":"  Current out-of-distribution (OOD) detection methods typically assume balanced\nin-distribution (ID) data, while most real-world data follow a long-tailed\ndistribution. Previous approaches to long-tailed OOD detection often involve\nbalancing the ID data by reducing the semantics of head classes. However, this\nreduction can severely affect the classification accuracy of ID data. The main\nchallenge of this task lies in the severe lack of features for tail classes,\nleading to confusion with OOD data. To tackle this issue, we introduce a novel\nPrioritizing Attention to Tail (PATT) method using augmentation instead of\nreduction. Our main intuition involves using a mixture of von Mises-Fisher\n(vMF) distributions to model the ID data and a temperature scaling module to\nboost the confidence of ID data. This enables us to generate infinite\ncontrastive pairs, implicitly enhancing the semantics of ID classes while\npromoting differentiation between ID and OOD data. To further strengthen the\ndetection of OOD data without compromising the classification performance of ID\ndata, we propose feature calibration during the inference phase. By extracting\nan attention weight from the training set that prioritizes the tail classes and\nreduces the confidence in OOD data, we improve the OOD detection capability.\nExtensive experiments verified that our method outperforms the current\nstate-of-the-art methods on various benchmarks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}