{"id":"2407.03622","title":"MSfusion: A Dynamic Model Splitting Approach for Resource-Constrained\n  Machines to Collaboratively Train Larger Models","authors":"Jin Xie, Songze Li","authorsParsed":[["Xie","Jin",""],["Li","Songze",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 04:06:24 GMT"}],"updateDate":"2024-07-08","timestamp":1720065984000,"abstract":"  Training large models requires a large amount of data, as well as abundant\ncomputation resources. While collaborative learning (e.g., federated learning)\nprovides a promising paradigm to harness collective data from many\nparticipants, training large models remains a major challenge for participants\nwith limited resources like mobile devices. We introduce MSfusion, an effective\nand efficient collaborative learning framework, tailored for training larger\nmodels on resourceconstraint machines through model splitting. Specifically, a\ndouble shifting model splitting scheme is designed such that in each training\nround, each participant is assigned a subset of model parameters to train over\nlocal data, and aggregates with sub-models of other peers on common parameters.\nWhile model splitting significantly reduces the computation and communication\ncosts of individual participants, additional novel designs on adaptive model\noverlapping and contrastive loss functions help MSfusion to maintain training\neffectiveness, against model shift across participants. Extensive experiments\non image and NLP tasks illustrate significant advantages of MSfusion in\nperformance and efficiency for training large models, and its strong\nscalability: computation cost of each participant reduces significantly as the\nnumber of participants increases.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}