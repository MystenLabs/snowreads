{"id":"2408.06834","title":"GLGait: A Global-Local Temporal Receptive Field Network for Gait\n  Recognition in the Wild","authors":"Guozhen Peng, Yunhong Wang, Yuwei Zhao, Shaoxiong Zhang, Annan Li","authorsParsed":[["Peng","Guozhen",""],["Wang","Yunhong",""],["Zhao","Yuwei",""],["Zhang","Shaoxiong",""],["Li","Annan",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 11:48:28 GMT"}],"updateDate":"2024-08-14","timestamp":1723549708000,"abstract":"  Gait recognition has attracted increasing attention from academia and\nindustry as a human recognition technology from a distance in non-intrusive\nways without requiring cooperation. Although advanced methods have achieved\nimpressive success in lab scenarios, most of them perform poorly in the wild.\nRecently, some Convolution Neural Networks (ConvNets) based methods have been\nproposed to address the issue of gait recognition in the wild. However, the\ntemporal receptive field obtained by convolution operations is limited for long\ngait sequences. If directly replacing convolution blocks with visual\ntransformer blocks, the model may not enhance a local temporal receptive field,\nwhich is important for covering a complete gait cycle. To address this issue,\nwe design a Global-Local Temporal Receptive Field Network (GLGait). GLGait\nemploys a Global-Local Temporal Module (GLTM) to establish a global-local\ntemporal receptive field, which mainly consists of a Pseudo Global Temporal\nSelf-Attention (PGTA) and a temporal convolution operation. Specifically, PGTA\nis used to obtain a pseudo global temporal receptive field with less memory and\ncomputation complexity compared with a multi-head self-attention (MHSA). The\ntemporal convolution operation is used to enhance the local temporal receptive\nfield. Besides, it can also aggregate pseudo global temporal receptive field to\na true holistic temporal receptive field. Furthermore, we also propose a\nCenter-Augmented Triplet Loss (CTL) in GLGait to reduce the intra-class\ndistance and expand the positive samples in the training stage. Extensive\nexperiments show that our method obtains state-of-the-art results on\nin-the-wild datasets, $i.e.$, Gait3D and GREW. The code is available at\nhttps://github.com/bgdpgz/GLGait.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}