{"id":"2408.04905","title":"GlitchProber: Advancing Effective Detection and Mitigation of Glitch\n  Tokens in Large Language Models","authors":"Zhibo Zhang, Wuxia Bai, Yuxi Li, Mark Huasong Meng, Kailong Wang, Ling\n  Shi, Li Li, Jun Wang, Haoyu Wang","authorsParsed":[["Zhang","Zhibo",""],["Bai","Wuxia",""],["Li","Yuxi",""],["Meng","Mark Huasong",""],["Wang","Kailong",""],["Shi","Ling",""],["Li","Li",""],["Wang","Jun",""],["Wang","Haoyu",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 07:19:53 GMT"}],"updateDate":"2024-08-12","timestamp":1723187993000,"abstract":"  Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}