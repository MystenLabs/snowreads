{"id":"2408.13953","title":"InterTrack: Tracking Human Object Interaction without Object Templates","authors":"Xianghui Xie and Jan Eric Lenssen and Gerard Pons-Moll","authorsParsed":[["Xie","Xianghui",""],["Lenssen","Jan Eric",""],["Pons-Moll","Gerard",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 22:26:46 GMT"}],"updateDate":"2024-08-27","timestamp":1724624806000,"abstract":"  Tracking human object interaction from videos is important to understand\nhuman behavior from the rapidly growing stream of video data. Previous\nvideo-based methods require predefined object templates while\nsingle-image-based methods are template-free but lack temporal consistency. In\nthis paper, we present a method to track human object interaction without any\nobject shape templates. We decompose the 4D tracking problem into per-frame\npose tracking and canonical shape optimization. We first apply a single-view\nreconstruction method to obtain temporally-inconsistent per-frame interaction\nreconstructions. Then, for the human, we propose an efficient autoencoder to\npredict SMPL vertices directly from the per-frame reconstructions, introducing\ntemporally consistent correspondence. For the object, we introduce a pose\nestimator that leverages temporal information to predict smooth object\nrotations under occlusions. To train our model, we propose a method to generate\nsynthetic interaction videos and synthesize in total 10 hour videos of 8.5k\nsequences with full 3D ground truth. Experiments on BEHAVE and InterCap show\nthat our method significantly outperforms previous template-based video\ntracking and single-frame reconstruction methods. Our proposed synthetic video\ndataset also allows training video-based methods that generalize to real-world\nvideos. Our code and dataset will be publicly released.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}