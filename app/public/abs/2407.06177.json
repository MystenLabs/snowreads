{"id":"2407.06177","title":"Vision-Language Models under Cultural and Inclusive Considerations","authors":"Antonia Karamolegkou, Phillip Rust, Yong Cao, Ruixiang Cui, Anders\n  S{\\o}gaard, Daniel Hershcovich","authorsParsed":[["Karamolegkou","Antonia",""],["Rust","Phillip",""],["Cao","Yong",""],["Cui","Ruixiang",""],["SÃ¸gaard","Anders",""],["Hershcovich","Daniel",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:50:00 GMT"}],"updateDate":"2024-07-09","timestamp":1720461000000,"abstract":"  Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/"}