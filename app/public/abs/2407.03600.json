{"id":"2407.03600","title":"Chain-of-Thought Augmentation with Logit Contrast for Enhanced Reasoning\n  in Language Models","authors":"Jay Shim, Grant Kruttschnitt, Alyssa Ma, Daniel Kim, Benjamin Chek,\n  Athul Anand, Kevin Zhu, Sean O'Brien","authorsParsed":[["Shim","Jay",""],["Kruttschnitt","Grant",""],["Ma","Alyssa",""],["Kim","Daniel",""],["Chek","Benjamin",""],["Anand","Athul",""],["Zhu","Kevin",""],["O'Brien","Sean",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 03:20:31 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 08:00:03 GMT"}],"updateDate":"2024-08-28","timestamp":1720063231000,"abstract":"  Rapidly increasing model scales coupled with steering methods such as\nchain-of-thought prompting have led to drastic improvements in language model\nreasoning. At the same time, models struggle with compositional generalization\nand are far from human performance on many reasoning-based benchmarks.\nLeveraging the success of chain-of-thought prompting, and also taking\ninspiration from context-aware decoding (CAD), we explore input-based\ncontrasting methods to further encourage the type of reasoning induced by\nchain-of-thought prompting. While work remains to stabilize these results\nacross datasets and models, the improvements we find warrant further\ninvestigation into input-based steering methods for context-aware reasoning.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}