{"id":"2408.08328","title":"Unleash The Power of Pre-Trained Language Models for Irregularly Sampled\n  Time Series","authors":"Weijia Zhang, Chenlong Yin, Hao Liu, Hui Xiong","authorsParsed":[["Zhang","Weijia",""],["Yin","Chenlong",""],["Liu","Hao",""],["Xiong","Hui",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 14:22:14 GMT"}],"updateDate":"2024-08-19","timestamp":1723472534000,"abstract":"  Pre-trained Language Models (PLMs), such as ChatGPT, have significantly\nadvanced the field of natural language processing. This progress has inspired a\nseries of innovative studies that explore the adaptation of PLMs to time series\nanalysis, intending to create a unified foundation model that addresses various\ntime series analytical tasks. However, these efforts predominantly focus on\nRegularly Sampled Time Series (RSTS), neglecting the unique challenges posed by\nIrregularly Sampled Time Series (ISTS), which are characterized by non-uniform\nsampling intervals and prevalent missing data. To bridge this gap, this work\nexplores the potential of PLMs for ISTS analysis. We begin by investigating the\neffect of various methods for representing ISTS, aiming to maximize the\nefficacy of PLMs in this under-explored area. Furthermore, we present a unified\nPLM-based framework, ISTS-PLM, which integrates time-aware and variable-aware\nPLMs tailored for comprehensive intra and inter-time series modeling and\nincludes a learnable input embedding layer and a task-specific output layer to\ntackle diverse ISTS analytical tasks. Extensive experiments on a comprehensive\nbenchmark demonstrate that the ISTS-PLM, utilizing a simple yet effective\nseries-based representation for ISTS, consistently achieves state-of-the-art\nperformance across various analytical tasks, such as classification,\ninterpolation, and extrapolation, as well as few-shot and zero-shot learning\nscenarios, spanning scientific domains like healthcare and biomechanics.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Statistics/Applications"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}