{"id":"2408.16160","title":"CLPNets: Coupled Lie-Poisson Neural Networks for Multi-Part Hamiltonian\n  Systems with Symmetries","authors":"Christopher Eldred and Fran\\c{c}ois Gay-Balmaz and Vakhtang Putkaradze","authorsParsed":[["Eldred","Christopher",""],["Gay-Balmaz","Fran√ßois",""],["Putkaradze","Vakhtang",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 22:45:15 GMT"}],"updateDate":"2024-08-30","timestamp":1724885115000,"abstract":"  To accurately compute data-based prediction of Hamiltonian systems,\nespecially the long-term evolution of such systems, it is essential to utilize\nmethods that preserve the structure of the equations over time. We consider a\ncase that is particularly challenging for data-based methods: systems with\ninteracting parts that do not reduce to pure momentum evolution. Such systems\nare essential in scientific computations. For example, any discretization of a\ncontinuum elastic rod can be viewed as interacting elements that can move and\nrotate in space, with each discrete element moving on the group of rotations\nand translations $SE(3)$.\n  We develop a novel method of data-based computation and complete phase space\nlearning of such systems. We follow the original framework of \\emph{SympNets}\n(Jin et al, 2020) building the neural network from canonical phase space\nmappings, and transformations that preserve the Lie-Poisson structure\n(\\emph{LPNets}) as in (Eldred et al, 2024). We derive a novel system of\nmappings that are built into neural networks for coupled systems. We call such\nnetworks Coupled Lie-Poisson Neural Networks, or \\emph{CLPNets}. We consider\nincreasingly complex examples for the applications of CLPNets: rotation of two\nrigid bodies about a common axis, the free rotation of two rigid bodies, and\nfinally the evolution of two connected and interacting $SE(3)$ components. Our\nmethod preserves all Casimir invariants of each system to machine precision,\nirrespective of the quality of the training data, and preserves energy to high\naccuracy. Our method also shows good resistance to the curse of dimensionality,\nrequiring only a few thousand data points for all cases studied, with the\neffective dimension varying from three to eighteen. Additionally, the method is\nhighly economical in memory requirements, requiring only about 200 parameters\nfor the most complex case considered.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}