{"id":"2407.08639","title":"$\\beta$-DPO: Direct Preference Optimization with Dynamic $\\beta$","authors":"Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin\n  Ding, Xiang Wang, Xiangnan He","authorsParsed":[["Wu","Junkang",""],["Xie","Yuexiang",""],["Yang","Zhengyi",""],["Wu","Jiancan",""],["Gao","Jinyang",""],["Ding","Bolin",""],["Wang","Xiang",""],["He","Xiangnan",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 16:21:18 GMT"}],"updateDate":"2024-07-12","timestamp":1720714878000,"abstract":"  Direct Preference Optimization (DPO) has emerged as a compelling approach for\ntraining Large Language Models (LLMs) to adhere to human preferences. However,\nthe performance of DPO is sensitive to the fine-tuning of its trade-off\nparameter $\\beta$, as well as to the quality of the preference data. We analyze\nthe impact of $\\beta$ and data quality on DPO, uncovering that optimal $\\beta$\nvalues vary with the informativeness of pairwise data. Addressing the\nlimitations of static $\\beta$ values, we introduce a novel framework that\ndynamically calibrates $\\beta$ at the batch level, informed by data quality\nconsiderations. Additionally, our method incorporates $\\beta$-guided data\nfiltering to safeguard against the influence of outliers. Through empirical\nevaluation, we demonstrate that our dynamic $\\beta$ adjustment technique\nsignificantly improves DPO's performance across a range of models and datasets,\noffering a more robust and adaptable training paradigm for aligning LLMs with\nhuman feedback. The code is available at\n\\url{https://github.com/junkangwu/beta-DPO}.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}