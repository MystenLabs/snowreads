{"id":"2408.03855","title":"Why transformers are obviously good models of language","authors":"Felix Hill","authorsParsed":[["Hill","Felix",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 15:52:46 GMT"}],"updateDate":"2024-08-08","timestamp":1723045966000,"abstract":"  Nobody knows how language works, but many theories abound. Transformers are a\nclass of neural networks that process language automatically with more success\nthan alternatives, both those based on neural computations and those that rely\non other (e.g. more symbolic) mechanisms. Here, I highlight direct connections\nbetween the transformer architecture and certain theoretical perspectives on\nlanguage. The empirical success of transformers relative to alternative models\nprovides circumstantial evidence that the linguistic approaches that\ntransformers embody should be, at least, evaluated with greater scrutiny by the\nlinguistics community and, at best, considered to be the currently best\navailable theories.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}