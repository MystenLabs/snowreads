{"id":"2408.15158","title":"Delay as Payoff in MAB","authors":"Ofir Schlisselberg, Ido Cohen, Tal Lancewicki, Yishay Mansour","authorsParsed":[["Schlisselberg","Ofir",""],["Cohen","Ido",""],["Lancewicki","Tal",""],["Mansour","Yishay",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 15:52:52 GMT"}],"updateDate":"2024-08-28","timestamp":1724773972000,"abstract":"  In this paper, we investigate a variant of the classical stochastic\nMulti-armed Bandit (MAB) problem, where the payoff received by an agent (either\ncost or reward) is both delayed, and directly corresponds to the magnitude of\nthe delay. This setting models faithfully many real world scenarios such as the\ntime it takes for a data packet to traverse a network given a choice of route\n(where delay serves as the agent's cost); or a user's time spent on a web page\ngiven a choice of content (where delay serves as the agent's reward).\n  Our main contributions are tight upper and lower bounds for both the cost and\nreward settings. For the case that delays serve as costs, which we are the\nfirst to consider, we prove optimal regret that scales as $\\sum_{i:\\Delta_i >\n0}\\frac{\\log T}{\\Delta_i} + d^*$, where $T$ is the maximal number of steps,\n$\\Delta_i$ are the sub-optimality gaps and $d^*$ is the minimal expected delay\namongst arms. For the case that delays serves as rewards, we show optimal\nregret of $\\sum_{i:\\Delta_i > 0}\\frac{\\log T}{\\Delta_i} + \\bar{d}$, where $\\bar\nd$ is the second maximal expected delay. These improve over the regret in the\ngeneral delay-dependent payoff setting, which scales as $\\sum_{i:\\Delta_i >\n0}\\frac{\\log T}{\\Delta_i} + D$, where $D$ is the maximum possible delay. Our\nregret bounds highlight the difference between the cost and reward scenarios,\nshowing that the improvement in the cost scenario is more significant than for\nthe reward. Finally, we accompany our theoretical results with an empirical\nevaluation.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}