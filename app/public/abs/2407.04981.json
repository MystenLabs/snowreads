{"id":"2407.04981","title":"TRACE: TRansformer-based Attribution using Contrastive Embeddings in\n  LLMs","authors":"Cheng Wang, Xinyang Lu, See-Kiong Ng, Bryan Kian Hsiang Low","authorsParsed":[["Wang","Cheng",""],["Lu","Xinyang",""],["Ng","See-Kiong",""],["Low","Bryan Kian Hsiang",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 07:19:30 GMT"}],"updateDate":"2024-07-09","timestamp":1720250370000,"abstract":"  The rapid evolution of large language models (LLMs) represents a substantial\nleap forward in natural language understanding and generation. However,\nalongside these advancements come significant challenges related to the\naccountability and transparency of LLM responses. Reliable source attribution\nis essential to adhering to stringent legal and regulatory standards, including\nthose set forth by the General Data Protection Regulation. Despite the\nwell-established methods in source attribution within the computer vision\ndomain, the application of robust attribution frameworks to natural language\nprocessing remains underexplored. To bridge this gap, we propose a novel and\nversatile TRansformer-based Attribution framework using Contrastive Embeddings\ncalled TRACE that, in particular, exploits contrastive learning for source\nattribution. We perform an extensive empirical evaluation to demonstrate the\nperformance and efficiency of TRACE in various settings and show that TRACE\nsignificantly improves the ability to attribute sources accurately, making it a\nvaluable tool for enhancing the reliability and trustworthiness of LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}