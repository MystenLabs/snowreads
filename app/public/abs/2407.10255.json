{"id":"2407.10255","title":"CUSIDE-T: Chunking, Simulating Future and Decoding for Transducer based\n  Streaming ASR","authors":"Wenbo Zhao, Ziwei Li, Chuan Yu, Zhijian Ou","authorsParsed":[["Zhao","Wenbo",""],["Li","Ziwei",""],["Yu","Chuan",""],["Ou","Zhijian",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 15:43:48 GMT"},{"version":"v2","created":"Mon, 16 Sep 2024 01:23:54 GMT"}],"updateDate":"2024-09-17","timestamp":1720971828000,"abstract":"  Streaming automatic speech recognition (ASR) is very important for many\nreal-world ASR applications. However, a notable challenge for streaming ASR\nsystems lies in balancing operational performance against latency constraint.\nRecently, a method of chunking, simulating future context and decoding, called\nCUSIDE, has been proposed for connectionist temporal classification (CTC) based\nstreaming ASR, which obtains a good balance between reduced latency and high\nrecognition accuracy. In this paper, we present CUSIDE-T, which successfully\nadapts the CUSIDE method over the recurrent neural network transducer (RNN-T)\nASR architecture, instead of being based on the CTC architecture. We also\nincorporate language model rescoring in CUSIDE-T to further enhance accuracy,\nwhile only bringing a small additional latency. Extensive experiments are\nconducted over the AISHELL-1, WenetSpeech and SpeechIO datasets, comparing\nCUSIDE-T and U2++ (both based on RNN-T). U2++ is an existing counterpart of\nchunk based streaming ASR method. It is shown that CUSIDE-T achieves superior\naccuracy performance for streaming ASR, with equal settings of latency.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}