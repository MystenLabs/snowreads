{"id":"2407.07476","title":"A Transverse-Read-assisted Valid-Bit Collection to Accelerate Stochastic\n  Computing MAC for Energy-Efficient in-RTM DNNs","authors":"Jihe Wang, Zhiying Zhang, Xingwu Dong, Danghui Wang","authorsParsed":[["Wang","Jihe",""],["Zhang","Zhiying",""],["Dong","Xingwu",""],["Wang","Danghui",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 09:02:35 GMT"},{"version":"v2","created":"Sat, 20 Jul 2024 09:32:37 GMT"}],"updateDate":"2024-07-23","timestamp":1720602155000,"abstract":"  It looks attractive to coordinate racetrack-memory(RM) and\nstochastic-computing (SC) jointly to build an ultra-low power\nneuron-architecture. However, the above combination has always been questioned\nin a fatal weakness that the narrow bit-view of the RM-MTJ structure, a.k.a.\nshift-and-access pattern, cannot physically match the great throughput of\ndirect-stored stochastic sequences. Fortunately, a recently developed\nTransverse-Read(TR) provides a wider segment-view to RM via detecting the\nresistance of domain-walls between a couple of MTJs on single nanowire,\ntherefore RM can be enhanced with a faster access to the sequences without any\nsubstantial domain-shift. To utilize TR for a power-efficient SC-DNNs, we\npropose a segment-based compression to leverage one-cycle TR to only read those\nkernel segments of stochastic sequences, meanwhile, remove a large number of\nredundant segments for ultra-high storage density. In decompression stage,\nlow-discrepancy stochastic sequences can be quickly reassembled by a\nselect-and-output loop using kernel segments rather than slowly regenerated by\ncostly SNGs. Since TR can provide an ideal in-memory acceleration in\none-counting, counter-free SC-MACs are designed and deployed near RMs to form a\npower-efficient neuron-architecture, in which, the binary results of TR are\nactivated straightforward without sluggish APCs. The results show that under\nthe TR aided RM model, the power efficiency, speed, and stochastic accuracy of\nSeed-based Fast Stochastic Computing significantly enhance the performance of\nDNNs. The speed of computation is 2.88x faster in Lenet-5 and 4.40x faster in\nVGG-19 compared to the CORUSCANT. The integration of TR with RTM is deployed\nnear the memory to create a power-efficient neuron architecture, eliminating\nthe need for slow Accumulative Parallel Counters (APCs) and improving access\nspeed to stochastic sequences.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}