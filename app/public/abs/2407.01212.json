{"id":"2407.01212","title":"EconNLI: Evaluating Large Language Models on Economics Reasoning","authors":"Yue Guo, Yi Yang","authorsParsed":[["Guo","Yue",""],["Yang","Yi",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 11:58:24 GMT"}],"updateDate":"2024-07-02","timestamp":1719835104000,"abstract":"  Large Language Models (LLMs) are widely used for writing economic analysis\nreports or providing financial advice, but their ability to understand economic\nknowledge and reason about potential results of specific economic events lacks\nsystematic evaluation. To address this gap, we propose a new dataset, natural\nlanguage inference on economic events (EconNLI), to evaluate LLMs' knowledge\nand reasoning abilities in the economic domain. We evaluate LLMs on (1) their\nability to correctly classify whether a premise event will cause a hypothesis\nevent and (2) their ability to generate reasonable events resulting from a\ngiven premise. Our experiments reveal that LLMs are not sophisticated in\neconomic reasoning and may generate wrong or hallucinated answers. Our study\nraises awareness of the limitations of using LLMs for critical decision-making\ninvolving economic reasoning and analysis. The dataset and codes are available\nat https://github.com/Irenehere/EconNLI.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}