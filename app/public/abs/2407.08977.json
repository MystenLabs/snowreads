{"id":"2407.08977","title":"CURE: Privacy-Preserving Split Learning Done Right","authors":"Halil Ibrahim Kanpak, Aqsa Shabbir, Esra Gen\\c{c}, Alptekin\n  K\\\"up\\c{c}\\\"u, Sinem Sav","authorsParsed":[["Kanpak","Halil Ibrahim",""],["Shabbir","Aqsa",""],["Genç","Esra",""],["Küpçü","Alptekin",""],["Sav","Sinem",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 04:10:19 GMT"}],"updateDate":"2024-07-15","timestamp":1720757419000,"abstract":"  Training deep neural networks often requires large-scale datasets,\nnecessitating storage and processing on cloud servers due to computational\nconstraints. The procedures must follow strict privacy regulations in domains\nlike healthcare. Split Learning (SL), a framework that divides model layers\nbetween client(s) and server(s), is widely adopted for distributed model\ntraining. While Split Learning reduces privacy risks by limiting server access\nto the full parameter set, previous research has identified that intermediate\noutputs exchanged between server and client can compromise client's data\nprivacy. Homomorphic encryption (HE)-based solutions exist for this scenario\nbut often impose prohibitive computational burdens.\n  To address these challenges, we propose CURE, a novel system based on HE,\nthat encrypts only the server side of the model and optionally the data. CURE\nenables secure SL while substantially improving communication and\nparallelization through advanced packing techniques. We propose two packing\nschemes that consume one HE level for one-layer networks and generalize our\nsolutions to n-layer neural networks. We demonstrate that CURE can achieve\nsimilar accuracy to plaintext SL while being 16x more efficient in terms of the\nruntime compared to the state-of-the-art privacy-preserving alternatives.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}