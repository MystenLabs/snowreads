{"id":"2408.09948","title":"Caption-Driven Explorations: Aligning Image and Text Embeddings through\n  Human-Inspired Foveated Vision","authors":"Dario Zanca, Andrea Zugarini, Simon Dietz, Thomas R. Altstidl, Mark A.\n  Turban Ndjeuha, Leo Schwinn, Bjoern Eskofier","authorsParsed":[["Zanca","Dario",""],["Zugarini","Andrea",""],["Dietz","Simon",""],["Altstidl","Thomas R.",""],["Ndjeuha","Mark A. Turban",""],["Schwinn","Leo",""],["Eskofier","Bjoern",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 12:41:46 GMT"}],"updateDate":"2024-08-20","timestamp":1724071306000,"abstract":"  Understanding human attention is crucial for vision science and AI. While\nmany models exist for free-viewing, less is known about task-driven image\nexploration. To address this, we introduce CapMIT1003, a dataset with captions\nand click-contingent image explorations, to study human attention during the\ncaptioning task. We also present NevaClip, a zero-shot method for predicting\nvisual scanpaths by combining CLIP models with NeVA algorithms. NevaClip\ngenerates fixations to align the representations of foveated visual stimuli and\ncaptions. The simulated scanpaths outperform existing human attention models in\nplausibility for captioning and free-viewing tasks. This research enhances the\nunderstanding of human attention and advances scanpath prediction models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}