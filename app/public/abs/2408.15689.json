{"id":"2408.15689","title":"TempoFormer: A Transformer for Temporally-aware Representations in\n  Change Detection","authors":"Talia Tseriotou, Adam Tsakalidis, Maria Liakata","authorsParsed":[["Tseriotou","Talia",""],["Tsakalidis","Adam",""],["Liakata","Maria",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 10:25:53 GMT"}],"updateDate":"2024-08-29","timestamp":1724840753000,"abstract":"  Dynamic representation learning plays a pivotal role in understanding the\nevolution of linguistic content over time. On this front both context and time\ndynamics as well as their interplay are of prime importance. Current approaches\nmodel context via pre-trained representations, which are typically temporally\nagnostic. Previous work on modeling context and temporal dynamics has used\nrecurrent methods, which are slow and prone to overfitting. Here we introduce\nTempoFormer, the fist task-agnostic transformer-based and temporally-aware\nmodel for dynamic representation learning. Our approach is jointly trained on\ninter and intra context dynamics and introduces a novel temporal variation of\nrotary positional embeddings. The architecture is flexible and can be used as\nthe temporal representation foundation of other models or applied to different\ntransformer-based architectures. We show new SOTA performance on three\ndifferent real-time change detection tasks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}