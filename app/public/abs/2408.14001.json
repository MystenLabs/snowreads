{"id":"2408.14001","title":"Decentralized Federated Learning with Model Caching on Mobile Agents","authors":"Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu","authorsParsed":[["Wang","Xiaoyu",""],["Xiong","Guojun",""],["Cao","Houwei",""],["Li","Jian",""],["Liu","Yong",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 03:58:20 GMT"}],"updateDate":"2024-08-27","timestamp":1724644700000,"abstract":"  Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}