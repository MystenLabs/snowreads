{"id":"2408.07215","title":"Can Large Language Models Reason? A Characterization via 3-SAT","authors":"Rishi Hazra, Gabriele Venturato, Pedro Zuidberg Dos Martires, Luc De\n  Raedt","authorsParsed":[["Hazra","Rishi",""],["Venturato","Gabriele",""],["Martires","Pedro Zuidberg Dos",""],["De Raedt","Luc",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 21:54:10 GMT"}],"updateDate":"2024-08-15","timestamp":1723586050000,"abstract":"  Large Language Models (LLMs) are said to possess advanced reasoning\nabilities. However, some skepticism exists as recent works show how LLMs often\nbypass true reasoning using shortcuts. Current methods for assessing the\nreasoning abilities of LLMs typically rely on open-source benchmarks that may\nbe overrepresented in LLM training data, potentially skewing performance. We\ninstead provide a computational theory perspective of reasoning, using 3-SAT --\nthe prototypical NP-complete problem that lies at the core of logical reasoning\nand constraint satisfaction tasks. By examining the phase transitions in 3-SAT,\nwe empirically characterize the reasoning abilities of LLMs and show how they\nvary with the inherent hardness of the problems. Our experimental evidence\nshows that LLMs cannot perform true reasoning, as is required for solving 3-SAT\nproblems.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"299cn2O0AaR1KsQpTh5L7HDJ5oRyAJ9IJk6gKFQBKgs","pdfSize":"10826991"}
