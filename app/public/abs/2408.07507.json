{"id":"2408.07507","title":"Decoder ensembling for learned latent geometries","authors":"Stas Syrota, Pablo Moreno-Mu\\~noz, S{\\o}ren Hauberg","authorsParsed":[["Syrota","Stas",""],["Moreno-Muñoz","Pablo",""],["Hauberg","Søren",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 12:35:41 GMT"}],"updateDate":"2024-08-15","timestamp":1723638941000,"abstract":"  Latent space geometry provides a rigorous and empirically valuable framework\nfor interacting with the latent variables of deep generative models. This\napproach reinterprets Euclidean latent spaces as Riemannian through a pull-back\nmetric, allowing for a standard differential geometric analysis of the latent\nspace. Unfortunately, data manifolds are generally compact and easily\ndisconnected or filled with holes, suggesting a topological mismatch to the\nEuclidean latent space. The most established solution to this mismatch is to\nlet uncertainty be a proxy for topology, but in neural network models, this is\noften realized through crude heuristics that lack principle and generally do\nnot scale to high-dimensional representations. We propose using ensembles of\ndecoders to capture model uncertainty and show how to easily compute geodesics\non the associated expected manifold. Empirically, we find this simple and\nreliable, thereby coming one step closer to easy-to-use latent geometries.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}