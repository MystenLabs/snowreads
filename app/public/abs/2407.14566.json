{"id":"2407.14566","title":"Generalization Error Analysis of Deep Backward Dynamic Programming for\n  Solving Nonlinear PDEs","authors":"Du Ouyang, Jichang Xiao and Xiaoqun Wang","authorsParsed":[["Ouyang","Du",""],["Xiao","Jichang",""],["Wang","Xiaoqun",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 04:31:32 GMT"}],"updateDate":"2024-07-23","timestamp":1721363492000,"abstract":"  We explore the application of the quasi-Monte Carlo (QMC) method in deep\nbackward dynamic programming (DBDP) (Hure et al. 2020) for numerically solving\nhigh-dimensional nonlinear partial differential equations (PDEs). Our study\nfocuses on examining the generalization error as a component of the total error\nin the DBDP framework, discovering that the rate of convergence for the\ngeneralization error is influenced by the choice of sampling methods.\nSpecifically, for a given batch size $m$, the generalization error under QMC\nmethods exhibits a convergence rate of $O(m^{-1+\\varepsilon})$, where\n$\\varepsilon$ can be made arbitrarily small. This rate is notably more\nfavorable than that of the traditional Monte Carlo (MC) methods, which is\n$O(m^{-1/2+\\varepsilon})$. Our theoretical analysis shows that the\ngeneralization error under QMC methods achieves a higher order of convergence\nthan their MC counterparts. Numerical experiments demonstrate that QMC indeed\nsurpasses MC in delivering solutions that are both more precise and stable.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zFi99uvNpKG2KW3hRY7EEtuXM_tl4OOvuJ5mSdCSyCs","pdfSize":"687503"}
