{"id":"2408.16967","title":"MemLong: Memory-Augmented Retrieval for Long Text Modeling","authors":"Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, Min Zhang","authorsParsed":[["Liu","Weijie",""],["Tang","Zecheng",""],["Li","Juntao",""],["Chen","Kehai",""],["Zhang","Min",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 02:01:56 GMT"}],"updateDate":"2024-09-02","timestamp":1724983316000,"abstract":"  Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}