{"id":"2408.16180","title":"Benchmarking Japanese Speech Recognition on ASR-LLM Setups with\n  Multi-Pass Augmented Generative Error Correction","authors":"Yuka Ko, Sheng Li, Chao-Han Huck Yang, Tatsuya Kawahara","authorsParsed":[["Ko","Yuka",""],["Li","Sheng",""],["Yang","Chao-Han Huck",""],["Kawahara","Tatsuya",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 00:18:12 GMT"}],"updateDate":"2024-08-30","timestamp":1724890692000,"abstract":"  With the strong representational power of large language models (LLMs),\ngenerative error correction (GER) for automatic speech recognition (ASR) aims\nto provide semantic and phonetic refinements to address ASR errors. This work\nexplores how LLM-based GER can enhance and expand the capabilities of Japanese\nlanguage processing, presenting the first GER benchmark for Japanese ASR with\n0.9-2.6k text utterances. We also introduce a new multi-pass augmented\ngenerative error correction (MPA GER) by integrating multiple system hypotheses\non the input side with corrections from multiple LLMs on the output side and\nthen merging them. To the best of our knowledge, this is the first\ninvestigation of the use of LLMs for Japanese GER, which involves second-pass\nlanguage modeling on the output transcriptions generated by the ASR system\n(e.g., N-best hypotheses). Our experiments demonstrated performance improvement\nin the proposed methods of ASR quality and generalization both in SPREDS-U1-ja\nand CSJ data.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by/4.0/"}