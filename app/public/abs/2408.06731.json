{"id":"2408.06731","title":"Large language models can consistently generate high-quality content for\n  election disinformation operations","authors":"Angus R. Williams, Liam Burke-Moore, Ryan Sze-Yin Chan, Florence E.\n  Enock, Federico Nanni, Tvesha Sippy, Yi-Ling Chung, Evelina Gabasova, Kobi\n  Hackenburg, Jonathan Bright","authorsParsed":[["Williams","Angus R.",""],["Burke-Moore","Liam",""],["Chan","Ryan Sze-Yin",""],["Enock","Florence E.",""],["Nanni","Federico",""],["Sippy","Tvesha",""],["Chung","Yi-Ling",""],["Gabasova","Evelina",""],["Hackenburg","Kobi",""],["Bright","Jonathan",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 08:45:34 GMT"}],"updateDate":"2024-08-14","timestamp":1723538734000,"abstract":"  Advances in large language models have raised concerns about their potential\nuse in generating compelling election disinformation at scale. This study\npresents a two-part investigation into the capabilities of LLMs to automate\nstages of an election disinformation operation. First, we introduce DisElect, a\nnovel evaluation dataset designed to measure LLM compliance with instructions\nto generate content for an election disinformation operation in localised UK\ncontext, containing 2,200 malicious prompts and 50 benign prompts. Using\nDisElect, we test 13 LLMs and find that most models broadly comply with these\nrequests; we also find that the few models which refuse malicious prompts also\nrefuse benign election-related prompts, and are more likely to refuse to\ngenerate content from a right-wing perspective. Secondly, we conduct a series\nof experiments (N=2,340) to assess the \"humanness\" of LLMs: the extent to which\ndisinformation operation content generated by an LLM is able to pass as\nhuman-written. Our experiments suggest that almost all LLMs tested released\nsince 2022 produce election disinformation operation content indiscernible by\nhuman evaluators over 50% of the time. Notably, we observe that multiple models\nachieve above-human levels of humanness. Taken together, these findings suggest\nthat current LLMs can be used to generate high-quality content for election\ndisinformation operations, even in hyperlocalised scenarios, at far lower costs\nthan traditional methods, and offer researchers and policymakers an empirical\nbenchmark for the measurement and evaluation of these capabilities in current\nand future models.\n","subjects":["Computing Research Repository/Computers and Society","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}