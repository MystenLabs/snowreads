{"id":"2408.09723","title":"sTransformer: A Modular Approach for Extracting Inter-Sequential and\n  Temporal Information for Time-Series Forecasting","authors":"Jiaheng Yin, Zhengxin Shi, Jianshen Zhang, Xiaomin Lin, Yulin Huang,\n  Yongzhi Qi, Wei Qi","authorsParsed":[["Yin","Jiaheng",""],["Shi","Zhengxin",""],["Zhang","Jianshen",""],["Lin","Xiaomin",""],["Huang","Yulin",""],["Qi","Yongzhi",""],["Qi","Wei",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 06:23:41 GMT"}],"updateDate":"2024-08-20","timestamp":1724048621000,"abstract":"  In recent years, numerous Transformer-based models have been applied to\nlong-term time-series forecasting (LTSF) tasks. However, recent studies with\nlinear models have questioned their effectiveness, demonstrating that simple\nlinear layers can outperform sophisticated Transformer-based models. In this\nwork, we review and categorize existing Transformer-based models into two main\ntypes: (1) modifications to the model structure and (2) modifications to the\ninput data. The former offers scalability but falls short in capturing\ninter-sequential information, while the latter preprocesses time-series data\nbut is challenging to use as a scalable module. We propose\n$\\textbf{sTransformer}$, which introduces the Sequence and Temporal\nConvolutional Network (STCN) to fully capture both sequential and temporal\ninformation. Additionally, we introduce a Sequence-guided Mask Attention\nmechanism to capture global feature information. Our approach ensures the\ncapture of inter-sequential information while maintaining module scalability.\nWe compare our model with linear models and existing forecasting models on\nlong-term time-series forecasting, achieving new state-of-the-art results. We\nalso conducted experiments on other time-series tasks, achieving strong\nperformance. These demonstrate that Transformer-based structures remain\neffective and our model can serve as a viable baseline for time-series tasks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}