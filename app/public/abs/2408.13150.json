{"id":"2408.13150","title":"Adaptive Backtracking For Faster Optimization","authors":"Joao V. Cavalcanti and Laurent Lessard and Ashia C. Wilson","authorsParsed":[["Cavalcanti","Joao V.",""],["Lessard","Laurent",""],["Wilson","Ashia C.",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 15:16:57 GMT"}],"updateDate":"2024-08-26","timestamp":1724426217000,"abstract":"  Backtracking line search is foundational in numerical optimization. The basic\nidea is to adjust the step size of an algorithm by a constant factor until some\nchosen criterion (e.g. Armijo, Goldstein, Descent Lemma) is satisfied. We\npropose a new way for adjusting step sizes, replacing the constant factor used\nin regular backtracking with one that takes into account the degree to which\nthe chosen criterion is violated, without additional computational burden. For\nconvex problems, we prove adaptive backtracking requires fewer adjustments to\nproduce a feasible step size than regular backtracking does for two popular\nline search criteria: the Armijo condition and the descent lemma. For nonconvex\nsmooth problems, we additionally prove adaptive backtracking enjoys the same\nguarantees of regular backtracking. Finally, we perform a variety of\nexperiments on over fifteen real world datasets, all of which confirm that\nadaptive backtracking often leads to significantly faster optimization.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}