{"id":"2408.00588","title":"Closing the gap between open-source and commercial large language models\n  for medical evidence summarization","authors":"Gongbo Zhang, Qiao Jin, Yiliang Zhou, Song Wang, Betina R. Idnay,\n  Yiming Luo, Elizabeth Park, Jordan G. Nestor, Matthew E. Spotnitz, Ali\n  Soroush, Thomas Campion, Zhiyong Lu, Chunhua Weng, Yifan Peng","authorsParsed":[["Zhang","Gongbo",""],["Jin","Qiao",""],["Zhou","Yiliang",""],["Wang","Song",""],["Idnay","Betina R.",""],["Luo","Yiming",""],["Park","Elizabeth",""],["Nestor","Jordan G.",""],["Spotnitz","Matthew E.",""],["Soroush","Ali",""],["Campion","Thomas",""],["Lu","Zhiyong",""],["Weng","Chunhua",""],["Peng","Yifan",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 05:03:01 GMT"}],"updateDate":"2024-08-02","timestamp":1721883781000,"abstract":"  Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}