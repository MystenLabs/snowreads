{"id":"2408.02298","title":"Backward Compatibility in Attributive Explanation and Enhanced Model\n  Training Method","authors":"Ryuta Matsuno","authorsParsed":[["Matsuno","Ryuta",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 08:14:32 GMT"}],"updateDate":"2024-08-06","timestamp":1722845672000,"abstract":"  Model update is a crucial process in the operation of ML/AI systems. While\nupdating a model generally enhances the average prediction performance, it also\nsignificantly impacts the explanations of predictions. In real-world\napplications, even minor changes in explanations can have detrimental\nconsequences. To tackle this issue, this paper introduces BCX, a quantitative\nmetric that evaluates the backward compatibility of feature attribution\nexplanations between pre- and post-update models. BCX utilizes practical\nagreement metrics to calculate the average agreement between the explanations\nof pre- and post-update models, specifically among samples on which both models\naccurately predict. In addition, we propose BCXR, a BCX-aware model training\nmethod by designing surrogate losses which theoretically lower bounds agreement\nscores. Furthermore, we present a universal variant of BCXR that improves all\nagreement metrics, utilizing L2 distance among the explanations of the models.\nTo validate our approach, we conducted experiments on eight real-world\ndatasets, demonstrating that BCXR achieves superior trade-offs between\npredictive performances and BCX scores, showcasing the effectiveness of our\nBCXR methods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"Vq6xR8hrDMH-xdKnGiOYrcKRlxZRYEb7iFklAcrfEno","pdfSize":"1300381","txDigest":"B7HnjtdSno3hYAvgSzb1gGHCJioNvscsDnqu2dXAgJqH","endEpoch":"1","status":"CERTIFIED"}
