{"id":"2407.15346","title":"Knowledge Acquisition Disentanglement for Knowledge-based Visual\n  Question Answering with Large Language Models","authors":"Wenbin An, Feng Tian, Jiahao Nie, Wenkai Shi, Haonan Lin, Yan Chen,\n  QianYing Wang, Yaqiang Wu, Guang Dai, Ping Chen","authorsParsed":[["An","Wenbin",""],["Tian","Feng",""],["Nie","Jiahao",""],["Shi","Wenkai",""],["Lin","Haonan",""],["Chen","Yan",""],["Wang","QianYing",""],["Wu","Yaqiang",""],["Dai","Guang",""],["Chen","Ping",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 03:05:32 GMT"}],"updateDate":"2024-07-23","timestamp":1721617532000,"abstract":"  Knowledge-based Visual Question Answering (KVQA) requires both image and\nworld knowledge to answer questions. Current methods first retrieve knowledge\nfrom the image and external knowledge base with the original complex question,\nthen generate answers with Large Language Models (LLMs). However, since the\noriginal question contains complex elements that require knowledge from\ndifferent sources, acquiring different kinds of knowledge in a coupled manner\nmay confuse models and hinder them from retrieving precise knowledge.\nFurthermore, the ``forward-only'' answering process fails to explicitly capture\nthe knowledge needs of LLMs, which can further hurt answering quality. To cope\nwith the above limitations, we propose DKA: Disentangled Knowledge Acquisition\nfrom LLM feedback, a training-free framework that disentangles knowledge\nacquisition to avoid confusion and uses LLM's feedback to specify the required\nknowledge. Specifically, DKA requires LLMs to specify what knowledge they need\nto answer the question and decompose the original complex question into two\nsimple sub-questions: Image-based sub-question and Knowledge-based\nsub-question. Then we use the two sub-questions to retrieve knowledge from the\nimage and knowledge base, respectively. In this way, two knowledge acquisition\nmodels can focus on the content that corresponds to them and avoid disturbance\nof irrelevant elements in the original complex question, which can help to\nprovide more precise knowledge and better align the knowledge needs of LLMs to\nyield correct answers. Experiments on benchmark datasets show that DKA\nsignificantly outperforms SOTA models. To facilitate future research, our data\nand code are available at \\url{https://github.com/Lackel/DKA}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}