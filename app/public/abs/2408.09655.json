{"id":"2408.09655","title":"Contextual Bandits for Unbounded Context Distributions","authors":"Puning Zhao, Jiafei Wu, Zhe Liu, Huiwen Wu","authorsParsed":[["Zhao","Puning",""],["Wu","Jiafei",""],["Liu","Zhe",""],["Wu","Huiwen",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 02:30:37 GMT"}],"updateDate":"2024-08-20","timestamp":1724034637000,"abstract":"  Nonparametric contextual bandit is an important model of sequential decision\nmaking problems. Under $\\alpha$-Tsybakov margin condition, existing research\nhas established a regret bound of\n$\\tilde{O}\\left(T^{1-\\frac{\\alpha+1}{d+2}}\\right)$ for bounded supports.\nHowever, the optimal regret with unbounded contexts has not been analyzed. The\nchallenge of solving contextual bandit problems with unbounded support is to\nachieve both exploration-exploitation tradeoff and bias-variance tradeoff\nsimultaneously. In this paper, we solve the nonparametric contextual bandit\nproblem with unbounded contexts. We propose two nearest neighbor methods\ncombined with UCB exploration. The first method uses a fixed $k$. Our analysis\nshows that this method achieves minimax optimal regret under a weak margin\ncondition and relatively light-tailed context distributions. The second method\nuses adaptive $k$. By a proper data-driven selection of $k$, this method\nachieves an expected regret of\n$\\tilde{O}\\left(T^{1-\\frac{(\\alpha+1)\\beta}{\\alpha+(d+2)\\beta}}+T^{1-\\beta}\\right)$,\nin which $\\beta$ is a parameter describing the tail strength. This bound\nmatches the minimax lower bound up to logarithm factors, indicating that the\nsecond method is approximately optimal.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}