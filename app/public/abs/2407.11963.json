{"id":"2407.11963","title":"NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context\n  Window?","authors":"Mo Li, Songyang Zhang, Yunxin Liu, Kai Chen","authorsParsed":[["Li","Mo",""],["Zhang","Songyang",""],["Liu","Yunxin",""],["Chen","Kai",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:59:06 GMT"}],"updateDate":"2024-07-17","timestamp":1721152746000,"abstract":"  In evaluating the long-context capabilities of large language models (LLMs),\nidentifying content relevant to a user's query from original long documents is\na crucial prerequisite for any LLM to answer questions based on long text. We\npresent NeedleBench, a framework consisting of a series of progressively more\nchallenging tasks for assessing bilingual long-context capabilities, spanning\nmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and\ndifferent depth ranges, allowing the strategic insertion of critical data\npoints in different text depth zones to rigorously test the retrieval and\nreasoning capabilities of models in diverse contexts. We use the NeedleBench\nframework to assess how well the leading open-source models can identify key\ninformation relevant to the question and apply that information to reasoning in\nbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge\n(ATC) to mimic the complexity of logical reasoning challenges that are likely\nto be present in real-world long-context tasks, providing a simple method for\nevaluating LLMs in dealing with complex long-context situations. Our results\nsuggest that current LLMs have significant room for improvement in practical\nlong-context applications, as they struggle with the complexity of logical\nreasoning challenges that are likely to be present in real-world long-context\ntasks. All codes and resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}