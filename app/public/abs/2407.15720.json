{"id":"2407.15720","title":"Do Large Language Models Have Compositional Ability? An Investigation\n  into Limitations and Scalability","authors":"Zhuoyan Xu, Zhenmei Shi, Yingyu Liang","authorsParsed":[["Xu","Zhuoyan",""],["Shi","Zhenmei",""],["Liang","Yingyu",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 15:22:34 GMT"},{"version":"v2","created":"Sun, 11 Aug 2024 04:39:16 GMT"}],"updateDate":"2024-08-13","timestamp":1721661754000,"abstract":"  Large language models (LLMs) have emerged as powerful tools for many AI\nproblems and exhibit remarkable in-context learning (ICL) capabilities.\nCompositional ability, solving unseen complex tasks that combine two or more\nsimple tasks, is an essential reasoning ability for Artificial General\nIntelligence. Despite the tremendous success of LLMs, how they approach\ncomposite tasks, especially those not encountered during the pretraining phase,\nremains an open and largely underexplored question. In this study, we delve\ninto the ICL capabilities of LLMs on composite tasks, with only simple tasks as\nin-context examples. We develop a test suite of composite tasks including\nlinguistic and logical challenges and perform empirical studies across\ndifferent LLM families. We observe that models exhibit divergent behaviors: (1)\nFor simpler composite tasks that apply distinct mapping mechanisms to different\ninput segments, the models demonstrate decent compositional ability, while\nscaling up the model enhances this ability; (2) for more complex composite\ntasks involving reasoning multiple steps, where each step represents one task,\nmodels typically underperform, and scaling up generally provides no\nimprovements. We offer theoretical analysis in a simplified setting, explaining\nthat models exhibit compositional capability when the task handles different\ninput parts separately. We believe our work sheds new light on the capabilities\nof LLMs in solving composite tasks regarding the nature of the tasks and model\nscale. Our dataset and code are available at\n{\\url{https://github.com/OliverXUZY/LLM_Compose}}.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}