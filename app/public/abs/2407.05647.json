{"id":"2407.05647","title":"Learning to Adapt Category Consistent Meta-Feature of CLIP for Few-Shot\n  Classification","authors":"Jiaying Shi and Xuetong Xue and Shenghui Xu","authorsParsed":[["Shi","Jiaying",""],["Xue","Xuetong",""],["Xu","Shenghui",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 06:18:04 GMT"}],"updateDate":"2024-07-09","timestamp":1720419484000,"abstract":"  The recent CLIP-based methods have shown promising zero-shot and few-shot\nperformance on image classification tasks. Existing approaches such as CoOp and\nTip-Adapter only focus on high-level visual features that are fully aligned\nwith textual features representing the ``Summary\" of the image. However, the\ngoal of few-shot learning is to classify unseen images of the same category\nwith few labeled samples. Especially, in contrast to high-level\nrepresentations, local representations (LRs) at low-level are more consistent\nbetween seen and unseen samples. Based on this point, we propose the\nMeta-Feature Adaption method (MF-Adapter) that combines the complementary\nstrengths of both LRs and high-level semantic representations. Specifically, we\nintroduce the Meta-Feature Unit (MF-Unit), which is a simple yet effective\nlocal similarity metric to measure category-consistent local context in an\ninductive manner. Then we train an MF-Adapter to map image features to MF-Unit\nfor adequately generalizing the intra-class knowledge between unseen images and\nthe support set. Extensive experiments show that our proposed method is\nsuperior to the state-of-the-art CLIP downstream few-shot classification\nmethods, even showing stronger performance on a set of challenging visual\nclassification tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}