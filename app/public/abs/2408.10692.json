{"id":"2408.10692","title":"Unconditional Truthfulness: Learning Conditional Dependency for\n  Uncertainty Quantification of Large Language Models","authors":"Artem Vazhentsev, Ekaterina Fadeeva, Rui Xing, Alexander Panchenko,\n  Preslav Nakov, Timothy Baldwin, Maxim Panov, Artem Shelmanov","authorsParsed":[["Vazhentsev","Artem",""],["Fadeeva","Ekaterina",""],["Xing","Rui",""],["Panchenko","Alexander",""],["Nakov","Preslav",""],["Baldwin","Timothy",""],["Panov","Maxim",""],["Shelmanov","Artem",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 09:42:26 GMT"}],"updateDate":"2024-08-21","timestamp":1724146946000,"abstract":"  Uncertainty quantification (UQ) is a perspective approach to detecting Large\nLanguage Model (LLM) hallucinations and low quality output. In this work, we\naddress one of the challenges of UQ in generation tasks that arises from the\nconditional dependency between the generation steps of an LLM. We propose to\nlearn this dependency from data. We train a regression model, which target\nvariable is the gap between the conditional and the unconditional generation\nconfidence. During LLM inference, we use this learned conditional dependency\nmodel to modulate the uncertainty of the current generation step based on the\nuncertainty of the previous step. Our experimental evaluation on nine datasets\nand three LLMs shows that the proposed method is highly effective for\nuncertainty quantification, achieving substantial improvements over rivaling\napproaches.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}