{"id":"2408.11773","title":"Deviations from the Nash equilibrium and emergence of tacit collusion in\n  a two-player optimal execution game with reinforcement learning","authors":"Fabrizio Lillo, Andrea Macr\\`i","authorsParsed":[["Lillo","Fabrizio",""],["Macr√¨","Andrea",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 16:54:53 GMT"}],"updateDate":"2024-08-22","timestamp":1724259293000,"abstract":"  The use of reinforcement learning algorithms in financial trading is becoming\nincreasingly prevalent. However, the autonomous nature of these algorithms can\nlead to unexpected outcomes that deviate from traditional game-theoretical\npredictions and may even destabilize markets. In this study, we examine a\nscenario in which two autonomous agents, modeled with Double Deep Q-Learning,\nlearn to liquidate the same asset optimally in the presence of market impact,\nusing the Almgren-Chriss (2000) framework. Our results show that the strategies\nlearned by the agents deviate significantly from the Nash equilibrium of the\ncorresponding market impact game. Notably, the learned strategies exhibit tacit\ncollusion, closely aligning with the Pareto-optimal solution. We further\nexplore how different levels of market volatility influence the agents'\nperformance and the equilibria they discover, including scenarios where\nvolatility differs between the training and testing phases.\n","subjects":["Quantitative Finance/Trading and Market Microstructure","Economics/General Economics","Quantitative Finance/Computational Finance","Quantitative Finance/Economics","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}