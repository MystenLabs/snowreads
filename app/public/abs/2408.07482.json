{"id":"2408.07482","title":"Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems","authors":"Ning Lu, Qian Xie, Hao Zhang, Wenyi Fang, Yang Zheng, Zheng Hu,\n  Jiantao Ma","authorsParsed":[["Lu","Ning",""],["Xie","Qian",""],["Zhang","Hao",""],["Fang","Wenyi",""],["Zheng","Yang",""],["Hu","Zheng",""],["Ma","Jiantao",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 11:55:28 GMT"},{"version":"v2","created":"Fri, 6 Sep 2024 02:38:40 GMT"}],"updateDate":"2024-09-09","timestamp":1723636528000,"abstract":"  Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"8BID0wCV9jo14qO2VpIw41IygGSj9jrLcTkxDR6t2Qk","pdfSize":"250809"}
