{"id":"2407.16233","title":"Algebraic Adversarial Attacks on Integrated Gradients","authors":"Lachlan Simpson, Federico Costanza, Kyle Millar, Adriel Cheng,\n  Cheng-Chew Lim, Hong Gunn Chew","authorsParsed":[["Simpson","Lachlan",""],["Costanza","Federico",""],["Millar","Kyle",""],["Cheng","Adriel",""],["Lim","Cheng-Chew",""],["Chew","Hong Gunn",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 07:17:45 GMT"}],"updateDate":"2024-07-24","timestamp":1721719065000,"abstract":"  Adversarial attacks on explainability models have drastic consequences when\nexplanations are used to understand the reasoning of neural networks in safety\ncritical systems. Path methods are one such class of attribution methods\nsusceptible to adversarial attacks. Adversarial learning is typically phrased\nas a constrained optimisation problem. In this work, we propose algebraic\nadversarial examples and study the conditions under which one can generate\nadversarial examples for integrated gradients. Algebraic adversarial examples\nprovide a mathematically tractable approach to adversarial examples.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Group Theory"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"3KZ8CzuDBICgRianJeJMG3hQ7u9B009N3KkL8_WJjdA","pdfSize":"135029"}
