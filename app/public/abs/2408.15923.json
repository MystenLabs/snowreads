{"id":"2408.15923","title":"Generalized Naive Bayes","authors":"Edith Alice Kov\\'acs, Anna Orsz\\'ag, D\\'aniel Pfeifer, Andr\\'as\n  Bencz\\'ur","authorsParsed":[["Kovács","Edith Alice",""],["Ország","Anna",""],["Pfeifer","Dániel",""],["Benczúr","András",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 16:36:18 GMT"}],"updateDate":"2024-08-29","timestamp":1724862978000,"abstract":"  In this paper we introduce the so-called Generalized Naive Bayes structure as\nan extension of the Naive Bayes structure. We give a new greedy algorithm that\nfinds a good fitting Generalized Naive Bayes (GNB) probability distribution. We\nprove that this fits the data at least as well as the probability distribution\ndetermined by the classical Naive Bayes (NB). Then, under a not very\nrestrictive condition, we give a second algorithm for which we can prove that\nit finds the optimal GNB probability distribution, i.e. best fitting structure\nin the sense of KL divergence. Both algorithms are constructed to maximize the\ninformation content and aim to minimize redundancy. Based on these algorithms,\nnew methods for feature selection are introduced. We discuss the similarities\nand differences to other related algorithms in terms of structure, methodology,\nand complexity. Experimental results show, that the algorithms introduced\noutperform the related algorithms in many cases.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"ebvftu5XgFp8NjRtIJI_RiwlVzdQFojnywGMbcDptq8","pdfSize":"2328466"}
