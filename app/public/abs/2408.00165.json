{"id":"2408.00165","title":"Non-convolutional Graph Neural Networks","authors":"Yuanqing Wang, Kyunghyun Cho","authorsParsed":[["Wang","Yuanqing",""],["Cho","Kyunghyun",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 21:29:26 GMT"},{"version":"v2","created":"Sun, 4 Aug 2024 02:04:50 GMT"}],"updateDate":"2024-08-06","timestamp":1722461366000,"abstract":"  Rethink convolution-based graph neural networks (GNN) -- they\ncharacteristically suffer from limited expressiveness, over-smoothing, and\nover-squashing, and require specialized sparse kernels for efficient\ncomputation. Here, we design a simple graph learning module entirely free of\nconvolution operators, coined random walk with unifying memory (RUM) neural\nnetwork, where an RNN merges the topological and semantic graph features along\nthe random walks terminating at each node. Relating the rich literature on RNN\nbehavior and graph topology, we theoretically show and experimentally verify\nthat RUM attenuates the aforementioned symptoms and is more expressive than the\nWeisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level\nclassification and regression tasks, RUM not only achieves competitive\nperformance, but is also robust, memory-efficient, scalable, and faster than\nthe simplest convolutional GNNs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZIyAk9Nra3e6l1R4vByNmXKJ_78vmNTlsT_bSi5sniI","pdfSize":"872560","txDigest":"9EKnFx6QKFok4JP54QN8fxddbXhTkcAkoi7NBSNyWZ7m","endEpoch":"1","status":"CERTIFIED"}
