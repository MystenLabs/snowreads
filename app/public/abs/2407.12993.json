{"id":"2407.12993","title":"Improving SAM Requires Rethinking its Optimization Formulation","authors":"Wanyun Xie, Fabian Latorre, Kimon Antonakopoulos, Thomas Pethick,\n  Volkan Cevher","authorsParsed":[["Xie","Wanyun",""],["Latorre","Fabian",""],["Antonakopoulos","Kimon",""],["Pethick","Thomas",""],["Cevher","Volkan",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 20:22:33 GMT"}],"updateDate":"2024-07-19","timestamp":1721247753000,"abstract":"  This paper rethinks Sharpness-Aware Minimization (SAM), which is originally\nformulated as a zero-sum game where the weights of a network and a bounded\nperturbation try to minimize/maximize, respectively, the same differentiable\nloss. To fundamentally improve this design, we argue that SAM should instead be\nreformulated using the 0-1 loss. As a continuous relaxation, we follow the\nsimple conventional approach where the minimizing (maximizing) player uses an\nupper bound (lower bound) surrogate to the 0-1 loss. This leads to a novel\nformulation of SAM as a bilevel optimization problem, dubbed as BiSAM. BiSAM\nwith newly designed lower-bound surrogate loss indeed constructs stronger\nperturbation. Through numerical evidence, we show that BiSAM consistently\nresults in improved performance when compared to the original SAM and variants,\nwhile enjoying similar computational complexity. Our code is available at\nhttps://github.com/LIONS-EPFL/BiSAM.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}