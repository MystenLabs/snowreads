{"id":"2408.08072","title":"I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm","authors":"Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun\n  Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, Wenhao Huang, Jiajun\n  Zhang","authorsParsed":[["Liang","Yiming",""],["Zhang","Ge",""],["Qu","Xingwei",""],["Zheng","Tianyu",""],["Guo","Jiawei",""],["Du","Xinrun",""],["Yang","Zhenzhu",""],["Liu","Jiaheng",""],["Lin","Chenghua",""],["Ma","Lei",""],["Huang","Wenhao",""],["Zhang","Jiajun",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 10:44:38 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 04:50:12 GMT"}],"updateDate":"2024-08-28","timestamp":1723718678000,"abstract":"  Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_52SDGQ115GG3vPQbWPGGzMG0RVr4Hk7xGP3CEUg_IA","pdfSize":"2033851"}
