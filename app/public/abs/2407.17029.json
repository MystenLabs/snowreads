{"id":"2407.17029","title":"Accurate and Efficient Fine-Tuning of Quantized Large Language Models\n  Through Optimal Balance","authors":"Ao Shen, Qiang Wang, Zhiquan Lai, Xionglve Li, Dongsheng Li","authorsParsed":[["Shen","Ao",""],["Wang","Qiang",""],["Lai","Zhiquan",""],["Li","Xionglve",""],["Li","Dongsheng",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 06:16:37 GMT"}],"updateDate":"2024-07-25","timestamp":1721801797000,"abstract":"  Large Language Models (LLMs) have demonstrated impressive performance across\nvarious domains. However, the enormous number of model parameters makes\nfine-tuning challenging, significantly limiting their application and\ndeployment. Existing solutions combine parameter quantization with Low-Rank\nAdaptation (LoRA), greatly reducing memory usage but resulting in noticeable\nperformance degradation. In this paper, we identify an imbalance in fine-tuning\nquantized pre-trained models: overly complex adapter inputs and outputs versus\nlow effective trainability of the adaptation. We propose Quantized LLMs with\nBalanced-rank Adaptation (Q-BaRA), which simplifies the adapter inputs and\noutputs while increasing the adapter's rank to achieve a more suitable balance\nfor fine-tuning quantized LLMs. Additionally, for scenarios where fine-tuned\nLLMs need to be deployed as low-precision inference models, we introduce\nQuantization-Aware Fine-tuning with Higher Rank Adaptation (QA-HiRA), which\nsimplifies the adapter inputs and outputs to align with the pre-trained model's\nblock-wise quantization while employing a single matrix to achieve a higher\nrank. Both Q-BaRA and QA-HiRA are easily implemented and offer the following\noptimizations: (i) Q-BaRA consistently achieves the highest accuracy compared\nto baselines and other variants, requiring the same number of trainable\nparameters and computational effort; (ii) QA-HiRA naturally merges adapter\nparameters into the block-wise quantized model after fine-tuning, achieving the\nhighest accuracy compared to other methods. We apply our Q-BaRA and QA-HiRA to\nthe LLaMA and LLaMA2 model families and validate their effectiveness across\ndifferent fine-tuning datasets and downstream scenarios.\n  Code will be made available at\n\\href{https://github.com/xiaocaigou/qbaraqahira}{https://github.com/xiaocaigou/qbaraqahira}\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}