{"id":"2407.10957","title":"Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes","authors":"Yaoting Wang, Peiwen Sun, Dongzhan Zhou, Guangyao Li, Honggang Zhang,\n  Di Hu","authorsParsed":[["Wang","Yaoting",""],["Sun","Peiwen",""],["Zhou","Dongzhan",""],["Li","Guangyao",""],["Zhang","Honggang",""],["Hu","Di",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 17:54:45 GMT"}],"updateDate":"2024-07-16","timestamp":1721066085000,"abstract":"  Traditional reference segmentation tasks have predominantly focused on silent\nvisual scenes, neglecting the integral role of multimodal perception and\ninteraction in human experiences. In this work, we introduce a novel task\ncalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment\nobjects within the visual domain based on expressions containing multimodal\ncues. Such expressions are articulated in natural language forms but are\nenriched with multimodal cues, including audio and visual descriptions. To\nfacilitate this research, we construct the first Ref-AVS benchmark, which\nprovides pixel-level annotations for objects described in corresponding\nmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method\nthat adequately utilizes multimodal cues to offer precise segmentation\nguidance. Finally, we conduct quantitative and qualitative experiments on three\ntest subsets to compare our approach with existing methods from related tasks.\nThe results demonstrate the effectiveness of our method, highlighting its\ncapability to precisely segment objects using multimodal-cue expressions.\nDataset is available at\n\\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7r2LqvpT2rQroJDkR0Z3QKtQJqO5zwduocQmUDcgAt4","pdfSize":"18508340"}
