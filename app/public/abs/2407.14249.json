{"id":"2407.14249","title":"An Attention-based Representation Distillation Baseline for Multi-Label\n  Continual Learning","authors":"Martin Menabue, Emanuele Frascaroli, Matteo Boschini, Lorenzo\n  Bonicelli, Angelo Porrello, Simone Calderara","authorsParsed":[["Menabue","Martin",""],["Frascaroli","Emanuele",""],["Boschini","Matteo",""],["Bonicelli","Lorenzo",""],["Porrello","Angelo",""],["Calderara","Simone",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 12:30:03 GMT"}],"updateDate":"2024-07-22","timestamp":1721392203000,"abstract":"  The field of Continual Learning (CL) has inspired numerous researchers over\nthe years, leading to increasingly advanced countermeasures to the issue of\ncatastrophic forgetting. Most studies have focused on the single-class\nscenario, where each example comes with a single label. The recent literature\nhas successfully tackled such a setting, with impressive results. Differently,\nwe shift our attention to the multi-label scenario, as we feel it to be more\nrepresentative of real-world open problems. In our work, we show that existing\nstate-of-the-art CL methods fail to achieve satisfactory performance, thus\nquestioning the real advance claimed in recent years. Therefore, we assess both\nold-style and novel strategies and propose, on top of them, an approach called\nSelective Class Attention Distillation (SCAD). It relies on a knowledge\ntransfer technique that seeks to align the representations of the student\nnetwork -- which trains continuously and is subject to forgetting -- with the\nteacher ones, which is pretrained and kept frozen. Importantly, our method is\nable to selectively transfer the relevant information from the teacher to the\nstudent, thereby preventing irrelevant information from harming the student's\nperformance during online training. To demonstrate the merits of our approach,\nwe conduct experiments on two different multi-label datasets, showing that our\nmethod outperforms the current state-of-the-art Continual Learning methods. Our\nfindings highlight the importance of addressing the unique challenges posed by\nmulti-label environments in the field of Continual Learning. The code of SCAD\nis available at https://github.com/aimagelab/SCAD-LOD-2024.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}