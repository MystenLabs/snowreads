{"id":"2407.02251","title":"White-Box 3D-OMP-Transformer for ISAC","authors":"Bowen Zhang and Geoffrey Ye Li","authorsParsed":[["Zhang","Bowen",""],["Li","Geoffrey Ye",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 13:17:00 GMT"}],"updateDate":"2024-07-03","timestamp":1719926220000,"abstract":"  Transformers have found broad applications for their great ability to capture\nlong-range dependency among the inputs using attention mechanisms. The recent\nsuccess of transformers increases the need for mathematical interpretation of\ntheir underlying working mechanisms, leading to the development of a family of\nwhite-box transformer-like deep network architectures. However, designing\nwhite-box transformers with efficient three-dimensional (3D) attention is still\nan open challenge. In this work, we revisit the 3D-orthogonal matching pursuit\n(OMP) algorithm and demonstrate that the operation of 3D-OMP is analogous to a\nspecific kind of transformer with 3D attention. Therefore, we build a white-box\n3D-OMP-transformer by introducing additional learnable parameters to 3D-OMP. As\na transformer, its 3D-attention can be mathematically interpreted from 3D-OMP;\nwhile as a variant of OMP, it can learn to improve the matching pursuit process\nfrom data. Besides, a transformer's performance can be improved by stacking\nmore transformer blocks. To simulate this process, we design a cascaded\n3D-OMP-Transformer with dynamic small-scale dictionaries, which can improve the\nperformance of the 3D-OMP-Transformer with low costs. We evaluate the designed\n3D-OMP-transformer in the multi-target detection task of integrated sensing and\ncommunications (ISAC). Experimental results show that the designed\n3D-OMP-Transformer can outperform current baselines.\n","subjects":["Electrical Engineering and Systems Science/Signal Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}