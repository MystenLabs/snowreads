{"id":"2408.05894","title":"GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models","authors":"Zixuan Wu, Yoolim Kim, and Carolyn Jane Anderson","authorsParsed":[["Wu","Zixuan",""],["Kim","Yoolim",""],["Anderson","Carolyn Jane",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 02:16:47 GMT"}],"updateDate":"2024-08-13","timestamp":1723429007000,"abstract":"  Vision-Language Models (VLMs) building upon the foundation of powerful large\nlanguage models have made rapid progress in reasoning across visual and textual\ndata. While VLMs perform well on vision tasks that they are trained on, our\nresults highlight key challenges in abstract pattern recognition. We present\nGlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of\nvisual patterns from 40 writing systems with three visual presentation styles.\n  GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models\nto understand and judge natural language descriptions of visual patterns.\nGlyphPattern patterns are drawn from a large-scale cognitive science\ninvestigation of human writing systems; as a result, they are rich in spatial\nreference and compositionality. Our experiments show that GlyphPattern is\nchallenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with\nmarginal gains from few-shot prompting. Our detailed error analysis reveals\nchallenges at multiple levels, including visual processing, natural language\nunderstanding, and pattern generalization.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}