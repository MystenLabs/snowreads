{"id":"2407.14142","title":"Early Preparation Pays Off: New Classifier Pre-tuning for Class\n  Incremental Semantic Segmentation","authors":"Zhengyuan Xie, Haiquan Lu, Jia-wen Xiao, Enguang Wang, Le Zhang,\n  Xialei Liu","authorsParsed":[["Xie","Zhengyuan",""],["Lu","Haiquan",""],["Xiao","Jia-wen",""],["Wang","Enguang",""],["Zhang","Le",""],["Liu","Xialei",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 09:19:29 GMT"}],"updateDate":"2024-07-22","timestamp":1721380769000,"abstract":"  Class incremental semantic segmentation aims to preserve old knowledge while\nlearning new tasks, however, it is impeded by catastrophic forgetting and\nbackground shift issues. Prior works indicate the pivotal importance of\ninitializing new classifiers and mainly focus on transferring knowledge from\nthe background classifier or preparing classifiers for future classes,\nneglecting the flexibility and variance of new classifiers. In this paper, we\npropose a new classifier pre-tuning~(NeST) method applied before the formal\ntraining process, learning a transformation from old classifiers to generate\nnew classifiers for initialization rather than directly tuning the parameters\nof new classifiers. Our method can make new classifiers align with the backbone\nand adapt to the new data, preventing drastic changes in the feature extractor\nwhen learning new classes. Besides, we design a strategy considering the\ncross-task class similarity to initialize matrices used in the transformation,\nhelping achieve the stability-plasticity trade-off. Experiments on Pascal VOC\n2012 and ADE20K datasets show that the proposed strategy can significantly\nimprove the performance of previous methods. The code is available at\n\\url{https://github.com/zhengyuan-xie/ECCV24_NeST}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}