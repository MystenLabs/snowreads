{"id":"2408.06099","title":"Approximating Discrimination Within Models When Faced With Several\n  Non-Binary Sensitive Attributes","authors":"Yijun Bian, Yujie Luo, Ping Xu","authorsParsed":[["Bian","Yijun",""],["Luo","Yujie",""],["Xu","Ping",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 12:30:48 GMT"}],"updateDate":"2024-08-13","timestamp":1723465848000,"abstract":"  Discrimination mitigation with machine learning (ML) models could be\ncomplicated because multiple factors may interweave with each other including\nhierarchically and historically. Yet few existing fairness measures are able to\ncapture the discrimination level within ML models in the face of multiple\nsensitive attributes. To bridge this gap, we propose a fairness measure based\non distances between sets from a manifold perspective, named as 'harmonic\nfairness measure via manifolds (HFM)' with two optional versions, which can\ndeal with a fine-grained discrimination evaluation for several sensitive\nattributes of multiple values. To accelerate the computation of distances of\nsets, we further propose two approximation algorithms named 'Approximation of\ndistance between sets for one sensitive attribute with multiple values\n(ApproxDist)' and 'Approximation of extended distance between sets for several\nsensitive attributes with multiple values (ExtendDist)' to respectively resolve\nbias evaluation of one single sensitive attribute with multiple values and that\nof several sensitive attributes with multiple values. Moreover, we provide an\nalgorithmic effectiveness analysis for ApproxDist under certain assumptions to\nexplain how well it could work. The empirical results demonstrate that our\nproposed fairness measure HFM is valid and approximation algorithms (i.e.,\nApproxDist and ExtendDist) are effective and efficient.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computers and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}