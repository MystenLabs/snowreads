{"id":"2408.16656","title":"A Two Stepsize SQP Method for Nonlinear Equality Constrained Stochastic\n  Optimization","authors":"Michael J. O'Neill","authorsParsed":[["O'Neill","Michael J.",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 16:00:48 GMT"}],"updateDate":"2024-08-30","timestamp":1724947248000,"abstract":"  We develop a Sequential Quadratic Optimization (SQP) algorithm for minimizing\na stochastic objective function subject to deterministic equality constraints.\nThe method utilizes two different stepsizes, one which exclusively scales the\ncomponent of the step corrupted by the variance of the stochastic gradient\nestimates and a second which scales the entire step. We prove that this\nstepsize splitting scheme has a worst-case complexity result which improves\nover the best known result for this class of problems. In terms of\napproximately satisfying the constraint violation, this complexity result\nmatches that of deterministic SQP methods, up to constant factors, while\nmatching the known optimal rate for stochastic SQP methods to approximately\nminimize the norm of the gradient of the Lagrangian. We also propose and\nanalyze multiple variants of our algorithm. One of these variants is based upon\npopular adaptive gradient methods for unconstrained stochastic optimization\nwhile another incorporates a safeguarded line search along the constraint\nviolation. Preliminary numerical experiments show competitive performance\nagainst a state of the art stochastic SQP method. In addition, in these\nexperiments, we observe an improved rate of convergence in terms of the\nconstraint violation, as predicted by the theoretical results.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6xgFW20-22X1aGXhYR52IjnPkj7LlQtAb1xjFORPIAw","pdfSize":"674038"}
