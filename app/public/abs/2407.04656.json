{"id":"2407.04656","title":"Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models\n  with Adaptive Expert Placement","authors":"Yongji Wu, Wenjie Qu, Tianyang Tao, Zhuang Wang, Wei Bai, Zhuohao Li,\n  Yuan Tian, Jiaheng Zhang, Matthew Lentz, Danyang Zhuo","authorsParsed":[["Wu","Yongji",""],["Qu","Wenjie",""],["Tao","Tianyang",""],["Wang","Zhuang",""],["Bai","Wei",""],["Li","Zhuohao",""],["Tian","Yuan",""],["Zhang","Jiaheng",""],["Lentz","Matthew",""],["Zhuo","Danyang",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 17:13:41 GMT"}],"updateDate":"2024-07-08","timestamp":1720199621000,"abstract":"  Sparsely-activated Mixture-of-Experts (MoE) architecture has increasingly\nbeen adopted to further scale large language models (LLMs) due to its\nsub-linear scaling for computation costs. However, frequent failures still pose\nsignificant challenges as training scales. The cost of even a single failure is\nsignificant, as all GPUs need to wait idle until the failure is resolved,\npotentially losing considerable training progress as training has to restart\nfrom checkpoints. Existing solutions for efficient fault-tolerant training\neither lack elasticity or rely on building resiliency into pipeline\nparallelism, which cannot be applied to MoE models due to the expert\nparallelism strategy adopted by the MoE architecture.\n  We present Lazarus, a system for resilient and elastic training of MoE\nmodels. Lazarus adaptively allocates expert replicas to address the inherent\nimbalance in expert workload and speeds-up training, while a provably optimal\nexpert placement algorithm is developed to maximize the probability of recovery\nupon failures. Through adaptive expert placement and a flexible token\ndispatcher, Lazarus can also fully utilize all available nodes after failures,\nleaving no GPU idle. Our evaluation shows that Lazarus outperforms existing MoE\ntraining systems by up to 5.7x under frequent node failures and 3.4x on a real\nspot instance trace.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}