{"id":"2408.13152","title":"Long-term Pre-training for Temporal Action Detection with Transformers","authors":"Jihwan Kim, Miso Lee, Jae-Pil Heo","authorsParsed":[["Kim","Jihwan",""],["Lee","Miso",""],["Heo","Jae-Pil",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 15:20:53 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 16:16:19 GMT"}],"updateDate":"2024-09-10","timestamp":1724426453000,"abstract":"  Temporal action detection (TAD) is challenging, yet fundamental for\nreal-world video applications. Recently, DETR-based models for TAD have been\nprevailing thanks to their unique benefits. However, transformers demand a huge\ndataset, and unfortunately data scarcity in TAD causes a severe degeneration.\nIn this paper, we identify two crucial problems from data scarcity: attention\ncollapse and imbalanced performance. To this end, we propose a new pre-training\nstrategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two\nmain components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly,\nwe synthesize long-form video features by merging video snippets of a target\nclass and non-target classes. They are analogous to untrimmed data used in TAD,\ndespite being created from trimmed data. In addition, we devise two types of\nlong-term pretext tasks to learn long-term dependency. They impose long-term\nconditions such as finding second-to-fourth or short-duration actions. Our\nextensive experiments show state-of-the-art performances in DETR-based methods\non ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate\nthat LTP significantly relieves the data scarcity issues in TAD.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"z5pXu8JJObC1iUw56j02cj8iRXmEXPBE4SOp98VO-K4","pdfSize":"1915385"}
