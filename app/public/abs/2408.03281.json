{"id":"2408.03281","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","authors":"Boxi Cao, Mengjie Ren, Hongyu Lin, Xianpei Han, Feng Zhang, Junfeng\n  Zhan, Le Sun","authorsParsed":[["Cao","Boxi",""],["Ren","Mengjie",""],["Lin","Hongyu",""],["Han","Xianpei",""],["Zhang","Feng",""],["Zhan","Junfeng",""],["Sun","Le",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 16:28:30 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 01:00:55 GMT"}],"updateDate":"2024-08-08","timestamp":1722961710000,"abstract":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}