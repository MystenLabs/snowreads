{"id":"2408.06604","title":"MV-DETR: Multi-modality indoor object detection by Multi-View DEtecton\n  TRansformers","authors":"Zichao Dong, Yilin Zhang, Xufeng Huang, Hang Ji, Zhan Shi, Xin Zhan,\n  Junbo Chen","authorsParsed":[["Dong","Zichao",""],["Zhang","Yilin",""],["Huang","Xufeng",""],["Ji","Hang",""],["Shi","Zhan",""],["Zhan","Xin",""],["Chen","Junbo",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 03:37:13 GMT"}],"updateDate":"2024-08-14","timestamp":1723520233000,"abstract":"  We introduce a novel MV-DETR pipeline which is effective while efficient\ntransformer based detection method. Given input RGBD data, we notice that there\nare super strong pretraining weights for RGB data while less effective works\nfor depth related data. First and foremost , we argue that geometry and texture\ncues are both of vital importance while could be encoded separately. Secondly,\nwe find that visual texture feature is relatively hard to extract compared with\ngeometry feature in 3d space. Unfortunately, single RGBD dataset with thousands\nof data is not enough for training an discriminating filter for visual texture\nfeature extraction. Last but certainly not the least, we designed a lightweight\nVG module consists of a visual textual encoder, a geometry encoder and a VG\nconnector. Compared with previous state of the art works like V-DETR, gains\nfrom pretrained visual encoder could be seen. Extensive experiments on\nScanNetV2 dataset shows the effectiveness of our method. It is worth mentioned\nthat our method achieve 78\\% AP which create new state of the art on ScanNetv2\nbenchmark.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}