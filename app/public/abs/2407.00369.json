{"id":"2407.00369","title":"How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open\n  Models","authors":"Jaeyoung Lee, Ximing Lu, Jack Hessel, Faeze Brahman, Youngjae Yu,\n  Yonatan Bisk, Yejin Choi, Saadia Gabriel","authorsParsed":[["Lee","Jaeyoung",""],["Lu","Ximing",""],["Hessel","Jack",""],["Brahman","Faeze",""],["Yu","Youngjae",""],["Bisk","Yonatan",""],["Choi","Yejin",""],["Gabriel","Saadia",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 08:39:07 GMT"}],"updateDate":"2024-07-02","timestamp":1719650347000,"abstract":"  Given the growing influx of misinformation across news and social media,\nthere is a critical need for systems that can provide effective real-time\nverification of news claims. Large language or multimodal model based\nverification has been proposed to scale up online policing mechanisms for\nmitigating spread of false and harmful content. While these can potentially\nreduce burden on human fact-checkers, such efforts may be hampered by\nfoundation model training data becoming outdated. In this work, we test the\nlimits of improving foundation model performance without continual updating\nthrough an initial study of knowledge transfer using either existing intra- and\ninter- domain benchmarks or explanations generated from large language models\n(LLMs). We evaluate on 12 public benchmarks for fact-checking and\nmisinformation detection as well as two other tasks relevant to content\nmoderation -- toxicity and stance detection. Our results on two recent\nmulti-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that\nknowledge transfer strategies can improve Fakeddit performance over the\nstate-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GMFEgWvElPgu34r4pZ8uaEsdwQrPS6qc-uYllk7xhKg","pdfSize":"2875046","objectId":"0x03a036a35f69c6ca8ecb8b3e57764e2ae6a66307c772e3234188ae66c9d03f43","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
