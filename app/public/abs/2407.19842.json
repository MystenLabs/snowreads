{"id":"2407.19842","title":"Detecting and Understanding Vulnerabilities in Language Models via\n  Mechanistic Interpretability","authors":"Jorge Garc\\'ia-Carrasco, Alejandro Mat\\'e, Juan Trujillo","authorsParsed":[["García-Carrasco","Jorge",""],["Maté","Alejandro",""],["Trujillo","Juan",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 09:55:34 GMT"}],"updateDate":"2024-07-30","timestamp":1722246934000,"abstract":"  Large Language Models (LLMs), characterized by being trained on broad amounts\nof data in a self-supervised manner, have shown impressive performance across a\nwide range of tasks. Indeed, their generative abilities have aroused interest\non the application of LLMs across a wide range of contexts. However, neural\nnetworks in general, and LLMs in particular, are known to be vulnerable to\nadversarial attacks, where an imperceptible change to the input can mislead the\noutput of the model. This is a serious concern that impedes the use of LLMs on\nhigh-stakes applications, such as healthcare, where a wrong prediction can\nimply serious consequences. Even though there are many efforts on making LLMs\nmore robust to adversarial attacks, there are almost no works that study\n\\emph{how} and \\emph{where} these vulnerabilities that make LLMs prone to\nadversarial attacks happen. Motivated by these facts, we explore how to\nlocalize and understand vulnerabilities, and propose a method, based on\nMechanistic Interpretability (MI) techniques, to guide this process.\nSpecifically, this method enables us to detect vulnerabilities related to a\nconcrete task by (i) obtaining the subset of the model that is responsible for\nthat task, (ii) generating adversarial samples for that task, and (iii) using\nMI techniques together with the previous samples to discover and understand the\npossible vulnerabilities. We showcase our method on a pretrained GPT-2 Small\nmodel carrying out the task of predicting 3-letter acronyms to demonstrate its\neffectiveness on locating and understanding concrete vulnerabilities of the\nmodel.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}