{"id":"2407.12784","title":"AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge\n  Bases","authors":"Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, Bo Li","authorsParsed":[["Chen","Zhaorun",""],["Xiang","Zhen",""],["Xiao","Chaowei",""],["Song","Dawn",""],["Li","Bo",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 17:59:47 GMT"}],"updateDate":"2024-07-18","timestamp":1721239187000,"abstract":"  LLM agents have demonstrated remarkable performance across various\napplications, primarily due to their advanced capabilities in reasoning,\nutilizing external knowledge and tools, calling APIs, and executing actions to\ninteract with environments. Current agents typically utilize a memory module or\na retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and\ninstances with similar embeddings from knowledge bases to inform task planning\nand execution. However, the reliance on unverified knowledge bases raises\nsignificant concerns about their safety and trustworthiness. To uncover such\nvulnerabilities, we propose a novel red teaming approach AgentPoison, the first\nbackdoor attack targeting generic and RAG-based LLM agents by poisoning their\nlong-term memory or RAG knowledge base. In particular, we form the trigger\ngeneration process as a constrained optimization to optimize backdoor triggers\nby mapping the triggered instances to a unique embedding space, so as to ensure\nthat whenever a user instruction contains the optimized backdoor trigger, the\nmalicious demonstrations are retrieved from the poisoned memory or knowledge\nbase with high probability. In the meantime, benign instructions without the\ntrigger will still maintain normal performance. Unlike conventional backdoor\nattacks, AgentPoison requires no additional model training or fine-tuning, and\nthe optimized backdoor trigger exhibits superior transferability, in-context\ncoherence, and stealthiness. Extensive experiments demonstrate AgentPoison's\neffectiveness in attacking three types of real-world LLM agents: RAG-based\nautonomous driving agent, knowledge-intensive QA agent, and healthcare\nEHRAgent. On each agent, AgentPoison achieves an average attack success rate\nhigher than 80% with minimal impact on benign performance (less than 1%) with a\npoison rate less than 0.1%.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}