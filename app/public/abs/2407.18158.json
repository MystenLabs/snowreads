{"id":"2407.18158","title":"Unlocking Tokens as Data Points for Generalization Bounds on Larger\n  Language Models","authors":"Sanae Lotfi, Yilun Kuang, Brandon Amos, Micah Goldblum, Marc Finzi,\n  Andrew Gordon Wilson","authorsParsed":[["Lotfi","Sanae",""],["Kuang","Yilun",""],["Amos","Brandon",""],["Goldblum","Micah",""],["Finzi","Marc",""],["Wilson","Andrew Gordon",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 16:13:58 GMT"}],"updateDate":"2024-07-26","timestamp":1721924038000,"abstract":"  Large language models (LLMs) with billions of parameters excel at predicting\nthe next token in a sequence. Recent work computes non-vacuous\ncompression-based generalization bounds for LLMs, but these bounds are vacuous\nfor large models at the billion-parameter scale. Moreover, these bounds are\nobtained through restrictive compression techniques, bounding compressed models\nthat generate low-quality text. Additionally, the tightness of these existing\nbounds depends on the number of IID documents in a training set rather than the\nmuch larger number of non-IID constituent tokens, leaving untapped potential\nfor tighter bounds. In this work, we instead use properties of martingales to\nderive generalization bounds that benefit from the vast number of tokens in LLM\ntraining sets. Since a dataset contains far more tokens than documents, our\ngeneralization bounds not only tolerate but actually benefit from far less\nrestrictive compression schemes. With Monarch matrices, Kronecker\nfactorizations, and post-training quantization, we achieve non-vacuous\ngeneralization bounds for LLMs as large as LLaMA2-70B. Unlike previous\napproaches, our work achieves the first non-vacuous bounds for models that are\ndeployed in practice and generate high-quality text.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}