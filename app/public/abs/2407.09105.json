{"id":"2407.09105","title":"Enhancing Training Efficiency Using Packing with Flash Attention","authors":"Achintya Kundu, Rhui Dih Lee, Laura Wynter, Raghu Kiran Ganti, Mayank\n  Mishra","authorsParsed":[["Kundu","Achintya",""],["Lee","Rhui Dih",""],["Wynter","Laura",""],["Ganti","Raghu Kiran",""],["Mishra","Mayank",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 09:10:37 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 08:09:32 GMT"},{"version":"v3","created":"Mon, 29 Jul 2024 07:58:53 GMT"},{"version":"v4","created":"Tue, 30 Jul 2024 02:06:17 GMT"},{"version":"v5","created":"Fri, 23 Aug 2024 14:11:05 GMT"},{"version":"v6","created":"Sun, 1 Sep 2024 00:26:46 GMT"}],"updateDate":"2024-09-04","timestamp":1720775437000,"abstract":"  Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"Ld7T9XfQ8B9jPkUOmu2N4L92q-l0Z_U-DxItPHR--Hc","pdfSize":"343305"}
