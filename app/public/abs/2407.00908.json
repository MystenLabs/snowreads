{"id":"2407.00908","title":"FineSurE: Fine-grained Summarization Evaluation using LLMs","authors":"Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, Saab Mansour","authorsParsed":[["Song","Hwanjun",""],["Su","Hang",""],["Shalyminov","Igor",""],["Cai","Jason",""],["Mansour","Saab",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 02:20:28 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 04:32:44 GMT"},{"version":"v3","created":"Mon, 22 Jul 2024 04:45:11 GMT"}],"updateDate":"2024-07-23","timestamp":1719800428000,"abstract":"  Automated evaluation is crucial for streamlining text summarization\nbenchmarking and model development, given the costly and time-consuming nature\nof human evaluation. Traditional methods like ROUGE do not correlate well with\nhuman judgment, while recently proposed LLM-based metrics provide only\nsummary-level assessment using Likert-scale scores. This limits deeper model\nanalysis, e.g., we can only assign one hallucination score at the summary\nlevel, while at the sentence level, we can count sentences containing\nhallucinations. To remedy those limitations, we propose FineSurE, a\nfine-grained evaluator specifically tailored for the summarization task using\nlarge language models (LLMs). It also employs completeness and conciseness\ncriteria, in addition to faithfulness, enabling multi-dimensional assessment.\nWe compare various open-source and proprietary LLMs as backbones for FineSurE.\nIn addition, we conduct extensive benchmarking of FineSurE against SOTA methods\nincluding NLI-, QA-, and LLM-based methods, showing improved performance\nespecially on the completeness and conciseness dimensions. The code is\navailable at https://github.com/DISL-Lab/FineSurE-ACL24.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}