{"id":"2407.18752","title":"Knowledge Graph Structure as Prompt: Improving Small Language Models\n  Capabilities for Knowledge-based Causal Discovery","authors":"Yuni Susanti and Michael F\\\"arber","authorsParsed":[["Susanti","Yuni",""],["FÃ¤rber","Michael",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 14:07:00 GMT"},{"version":"v2","created":"Mon, 29 Jul 2024 08:27:33 GMT"},{"version":"v3","created":"Tue, 30 Jul 2024 12:05:11 GMT"}],"updateDate":"2024-07-31","timestamp":1722002820000,"abstract":"  Causal discovery aims to estimate causal structures among variables based on\nobservational data. Large Language Models (LLMs) offer a fresh perspective to\ntackle the causal discovery problem by reasoning on the metadata associated\nwith variables rather than their actual data values, an approach referred to as\nknowledge-based causal discovery. In this paper, we investigate the\ncapabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1\nbillion parameters) with prompt-based learning for knowledge-based causal\ndiscovery. Specifically, we present KG Structure as Prompt, a novel approach\nfor integrating structural information from a knowledge graph, such as common\nneighbor nodes and metapaths, into prompt-based learning to enhance the\ncapabilities of SLMs. Experimental results on three types of biomedical and\nopen-domain datasets under few-shot settings demonstrate the effectiveness of\nour approach, surpassing most baselines and even conventional fine-tuning\napproaches trained on full datasets. Our findings further highlight the strong\ncapabilities of SLMs: in combination with knowledge graphs and prompt-based\nlearning, SLMs demonstrate the potential to surpass LLMs with larger number of\nparameters. Our code and datasets are available on GitHub.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}