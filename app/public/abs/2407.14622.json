{"id":"2407.14622","title":"BOND: Aligning LLMs with Best-of-N Distillation","authors":"Pier Giuseppe Sessa, Robert Dadashi, L\\'eonard Hussenot, Johan Ferret,\n  Nino Vieillard, Alexandre Ram\\'e, Bobak Shariari, Sarah Perrin, Abe Friesen,\n  Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila\n  Sinopalnikov, Sabela Ramos, Am\\'elie H\\'eliou, Aliaksei Severyn, Matt\n  Hoffman, Nikola Momchev, Olivier Bachem","authorsParsed":[["Sessa","Pier Giuseppe",""],["Dadashi","Robert",""],["Hussenot","Léonard",""],["Ferret","Johan",""],["Vieillard","Nino",""],["Ramé","Alexandre",""],["Shariari","Bobak",""],["Perrin","Sarah",""],["Friesen","Abe",""],["Cideron","Geoffrey",""],["Girgin","Sertan",""],["Stanczyk","Piotr",""],["Michi","Andrea",""],["Sinopalnikov","Danila",""],["Ramos","Sabela",""],["Héliou","Amélie",""],["Severyn","Aliaksei",""],["Hoffman","Matt",""],["Momchev","Nikola",""],["Bachem","Olivier",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 18:38:25 GMT"}],"updateDate":"2024-07-23","timestamp":1721414305000,"abstract":"  Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9MltLYT4jlWfNIWSX3UsphztL_9ZtxSOw8cf2Fj6Blc","pdfSize":"2031580"}
