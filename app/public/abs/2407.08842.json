{"id":"2407.08842","title":"Evaluating Nuanced Bias in Large Language Model Free Response Answers","authors":"Jennifer Healey, Laurie Byrum, Md Nadeem Akhtar and Moumita Sinha","authorsParsed":[["Healey","Jennifer",""],["Byrum","Laurie",""],["Akhtar","Md Nadeem",""],["Sinha","Moumita",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 19:58:13 GMT"}],"updateDate":"2024-07-15","timestamp":1720727893000,"abstract":"  Pre-trained large language models (LLMs) can now be easily adapted for\nspecific business purposes using custom prompts or fine tuning. These\ncustomizations are often iteratively re-engineered to improve some aspect of\nperformance, but after each change businesses want to ensure that there has\nbeen no negative impact on the system's behavior around such critical issues as\nbias. Prior methods of benchmarking bias use techniques such as word masking\nand multiple choice questions to assess bias at scale, but these do not capture\nall of the nuanced types of bias that can occur in free response answers, the\ntypes of answers typically generated by LLM systems. In this paper, we identify\nseveral kinds of nuanced bias in free text that cannot be similarly identified\nby multiple choice tests. We describe these as: confidence bias, implied bias,\ninclusion bias and erasure bias. We present a semi-automated pipeline for\ndetecting these types of bias by first eliminating answers that can be\nautomatically classified as unbiased and then co-evaluating name reversed pairs\nusing crowd workers. We believe that the nuanced classifications our method\ngenerates can be used to give better feedback to LLMs, especially as LLM\nreasoning capabilities become more advanced.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}