{"id":"2407.06018","title":"Leveraging Transformers for Weakly Supervised Object Localization in\n  Unconstrained Videos","authors":"Shakeeb Murtaza, Marco Pedersoli, Aydin Sarraf, Eric Granger","authorsParsed":[["Murtaza","Shakeeb",""],["Pedersoli","Marco",""],["Sarraf","Aydin",""],["Granger","Eric",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 15:08:41 GMT"}],"updateDate":"2024-07-09","timestamp":1720451321000,"abstract":"  Weakly-Supervised Video Object Localization (WSVOL) involves localizing an\nobject in videos using only video-level labels, also referred to as tags.\nState-of-the-art WSVOL methods like Temporal CAM (TCAM) rely on class\nactivation mapping (CAM) and typically require a pre-trained CNN classifier.\nHowever, their localization accuracy is affected by their tendency to minimize\nthe mutual information between different instances of a class and exploit\ntemporal information during training for downstream tasks, e.g., detection and\ntracking. In the absence of bounding box annotation, it is challenging to\nexploit precise information about objects from temporal cues because the model\nstruggles to locate objects over time. To address these issues, a novel method\ncalled transformer based CAM for videos (TrCAM-V), is proposed for WSVOL. It\nconsists of a DeiT backbone with two heads for classification and localization.\nThe classification head is trained using standard classification loss (CL),\nwhile the localization head is trained using pseudo-labels that are extracted\nusing a pre-trained CLIP model. From these pseudo-labels, the high and low\nactivation values are considered to be foreground and background regions,\nrespectively. Our TrCAM-V method allows training a localization network by\nsampling pseudo-pixels on the fly from these regions. Additionally, a\nconditional random field (CRF) loss is employed to align the object boundaries\nwith the foreground map. During inference, the model can process individual\nframes for real-time localization applications. Extensive experiments on\nchallenging YouTube-Objects unconstrained video datasets show that our TrCAM-V\nmethod achieves new state-of-the-art performance in terms of classification and\nlocalization accuracy.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}