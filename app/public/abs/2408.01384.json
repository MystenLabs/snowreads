{"id":"2408.01384","title":"NOLO: Navigate Only Look Once","authors":"Bohan Zhou, Jiangxing Wang, and Zongqing Lu","authorsParsed":[["Zhou","Bohan",""],["Wang","Jiangxing",""],["Lu","Zongqing",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 16:41:34 GMT"}],"updateDate":"2024-08-05","timestamp":1722616894000,"abstract":"  The in-context learning ability of Transformer models has brought new\npossibilities to visual navigation. In this paper, we focus on the video\nnavigation setting, where an in-context navigation policy needs to be learned\npurely from videos in an offline manner, without access to the actual\nenvironment. For this setting, we propose Navigate Only Look Once (NOLO), a\nmethod for learning a navigation policy that possesses the in-context ability\nand adapts to new scenes by taking corresponding context videos as input\nwithout finetuning or re-training. To enable learning from videos, we first\npropose a pseudo action labeling procedure using optical flow to recover the\naction label from egocentric videos. Then, offline reinforcement learning is\napplied to learn the navigation policy. Through extensive experiments on\ndifferent scenes, we show that our algorithm outperforms baselines by a large\nmargin, which demonstrates the in-context learning ability of the learned\npolicy.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}