{"id":"2407.19520","title":"Ego-VPA: Egocentric Video Understanding with Parameter-efficient\n  Adaptation","authors":"Tz-Ying Wu, Kyle Min, Subarna Tripathi, Nuno Vasconcelos","authorsParsed":[["Wu","Tz-Ying",""],["Min","Kyle",""],["Tripathi","Subarna",""],["Vasconcelos","Nuno",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 16:01:32 GMT"}],"updateDate":"2024-07-30","timestamp":1722182492000,"abstract":"  Video understanding typically requires fine-tuning the large backbone when\nadapting to new domains. In this paper, we leverage the egocentric video\nfoundation models (Ego-VFMs) based on video-language pre-training and propose a\nparameter-efficient adaptation for egocentric video tasks, namely Ego-VPA. It\nemploys a local sparse approximation for each video frame/text feature using\nthe basis prompts, and the selected basis prompts are used to synthesize\nvideo/text prompts. Since the basis prompts are shared across frames and\nmodalities, it models context fusion and cross-modal transfer in an efficient\nfashion. Experiments show that Ego-VPA excels in lightweight adaptation (with\nonly 0.84% learnable parameters), largely improving over baselines and reaching\nthe performance of full fine-tuning.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}