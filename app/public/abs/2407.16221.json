{"id":"2407.16221","title":"Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models","authors":"Nishanth Madhusudhan, Sathwik Tejaswi Madhusudhan, Vikas Yadav, Masoud\n  Hashemi","authorsParsed":[["Madhusudhan","Nishanth",""],["Madhusudhan","Sathwik Tejaswi",""],["Yadav","Vikas",""],["Hashemi","Masoud",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 06:56:54 GMT"}],"updateDate":"2024-07-24","timestamp":1721717814000,"abstract":"  As Large Language Models (LLMs) achieve remarkable performance across various\nNLP tasks, their reliability becomes essential for widespread adoption. This\npaper focuses on Abstention Ability (AA), a critical yet under explored aspect\nof reliability - the ability of LLMs to refrain from answering questions when\nthey are uncertain or when definitive answer is not possible, while maintaining\nquestion-answering (QA) task performance. While previous works have focused on\nunderstanding the recollection abilities of LLMs or their ability to identify\nimponderable/unanswerable questions, we believe there is a need for an\neffective AA evaluation method. Therefore, we propose a black-box evaluation\nmethodology to examine and understand the AA of LLMs across a variety of\nmultiple-choice QA tasks. We measure AA by rewarding models for abstaining from\nanswering when their predictions are incorrect or when the questions are\ninherently unanswerable. We investigate three strategies, Strict Prompting,\nVerbal Confidence Thresholding, and Chain-of-Thought (CoT), to understand their\nimpact on abstention across different LLMs. Our findings reveal that while even\nstate-of-the-art LLMs like GPT-4 struggle with abstention, strategic prompting\nsuch as CoT, can significantly enhance this ability. Furthermore, we\ndemonstrate that improving AA also leads to better overall QA task performance,\nunderscoring the importance of evaluating AA in LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}