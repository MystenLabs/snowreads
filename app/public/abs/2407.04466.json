{"id":"2407.04466","title":"Using LLMs to label medical papers according to the CIViC evidence model","authors":"Markus Hisch, Xing David Wang","authorsParsed":[["Hisch","Markus",""],["Wang","Xing David",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 12:30:01 GMT"}],"updateDate":"2024-07-08","timestamp":1720182601000,"abstract":"  We introduce the sequence classification problem CIViC Evidence to the field\nof medical NLP. CIViC Evidence denotes the multi-label classification problem\nof assigning labels of clinical evidence to abstracts of scientific papers\nwhich have examined various combinations of genomic variants, cancer types, and\ntreatment approaches. We approach CIViC Evidence using different language\nmodels: We fine-tune pretrained checkpoints of BERT and RoBERTa on the CIViC\nEvidence dataset and challenge their performance with models of the same\narchitecture which have been pretrained on domain-specific text. In this\ncontext, we find that BiomedBERT and BioLinkBERT can outperform BERT on CIViC\nEvidence (+0.8% and +0.9% absolute improvement in class-support weighted F1\nscore). All transformer-based models show a clear performance edge when\ncompared to a logistic regression trained on bigram tf-idf scores (+1.5 - 2.7%\nimproved F1 score). We compare the aforementioned BERT-like models to OpenAI's\nGPT-4 in a few-shot setting (on a small subset of our original test dataset),\ndemonstrating that, without additional prompt-engineering or fine-tuning, GPT-4\nperforms worse on CIViC Evidence than our six fine-tuned models (66.1% weighted\nF1 score compared to 71.8% for the best fine-tuned model). However, performance\ngets reasonably close to the benchmark of a logistic regression model trained\non bigram tf-idf scores (67.7% weighted F1 score).\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"V1woFu9vLP668V24zRslx984uQXMq0DE2eYbtS9Veh8","pdfSize":"2173478"}
