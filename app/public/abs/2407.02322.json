{"id":"2407.02322","title":"Stochastic Differential Equations models for Least-Squares Stochastic\n  Gradient Descent","authors":"Adrien Schertzer and Loucas Pillaud-Vivien","authorsParsed":[["Schertzer","Adrien",""],["Pillaud-Vivien","Loucas",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 14:52:21 GMT"}],"updateDate":"2024-07-03","timestamp":1719931941000,"abstract":"  We study the dynamics of a continuous-time model of the Stochastic Gradient\nDescent (SGD) for the least-square problem. Indeed, pursuing the work of Li et\nal. (2019), we analyze Stochastic Differential Equations (SDEs) that model SGD\neither in the case of the training loss (finite samples) or the population one\n(online setting). A key qualitative feature of the dynamics is the existence of\na perfect interpolator of the data, irrespective of the sample size. In both\nscenarios, we provide precise, non-asymptotic rates of convergence to the\n(possibly degenerate) stationary distribution. Additionally, we describe this\nasymptotic distribution, offering estimates of its mean, deviations from it,\nand a proof of the emergence of heavy-tails related to the step-size magnitude.\nNumerical simulations supporting our findings are also presented.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Probability"],"license":"http://creativecommons.org/licenses/by/4.0/"}