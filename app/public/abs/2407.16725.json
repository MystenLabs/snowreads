{"id":"2407.16725","title":"Category-Extensible Out-of-Distribution Detection via Hierarchical\n  Context Descriptions","authors":"Kai Liu, Zhihang Fu, Chao Chen, Sheng Jin, Ze Chen, Mingyuan Tao,\n  Rongxin Jiang, Jieping Ye","authorsParsed":[["Liu","Kai",""],["Fu","Zhihang",""],["Chen","Chao",""],["Jin","Sheng",""],["Chen","Ze",""],["Tao","Mingyuan",""],["Jiang","Rongxin",""],["Ye","Jieping",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 12:53:38 GMT"}],"updateDate":"2024-07-25","timestamp":1721739218000,"abstract":"  The key to OOD detection has two aspects: generalized feature representation\nand precise category description. Recently, vision-language models such as CLIP\nprovide significant advances in both two issues, but constructing precise\ncategory descriptions is still in its infancy due to the absence of unseen\ncategories. This work introduces two hierarchical contexts, namely perceptual\ncontext and spurious context, to carefully describe the precise category\nboundary through automatic prompt tuning. Specifically, perceptual contexts\nperceive the inter-category difference (e.g., cats vs apples) for current\nclassification tasks, while spurious contexts further identify spurious\n(similar but exactly not) OOD samples for every single category (e.g., cats vs\npanthers, apples vs peaches). The two contexts hierarchically construct the\nprecise description for a certain category, which is, first roughly classifying\na sample to the predicted category and then delicately identifying whether it\nis truly an ID sample or actually OOD. Moreover, the precise descriptions for\nthose categories within the vision-language framework present a novel\napplication: CATegory-EXtensible OOD detection (CATEX). One can efficiently\nextend the set of recognizable categories by simply merging the hierarchical\ncontexts learned under different sub-task settings. And extensive experiments\nare conducted to demonstrate CATEX's effectiveness, robustness, and\ncategory-extensibility. For instance, CATEX consistently surpasses the rivals\nby a large margin with several protocols on the challenging ImageNet-1K\ndataset. In addition, we offer new insights on how to efficiently scale up the\nprompt engineering in vision-language models to recognize thousands of object\ncategories, as well as how to incorporate large language models (like GPT-3) to\nboost zero-shot applications. Code will be made public soon.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Bsfd7Hfyk_l6qJR33QCxaog7U7LaKm4BZs3agyNiGl8","pdfSize":"2909820"}
