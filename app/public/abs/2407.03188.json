{"id":"2407.03188","title":"MuDiT & MuSiT: Alignment with Colloquial Expression in\n  Description-to-Song Generation","authors":"Zihao Wang, Haoxuan Liu, Jiaxing Yu, Tao Zhang, Yan Liu, Kejun Zhang","authorsParsed":[["Wang","Zihao",""],["Liu","Haoxuan",""],["Yu","Jiaxing",""],["Zhang","Tao",""],["Liu","Yan",""],["Zhang","Kejun",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 15:12:36 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 03:32:44 GMT"}],"updateDate":"2024-07-12","timestamp":1720019556000,"abstract":"  Amid the rising intersection of generative AI and human artistic processes,\nthis study probes the critical yet less-explored terrain of alignment in\nhuman-centric automatic song composition. We propose a novel task of Colloquial\nDescription-to-Song Generation, which focuses on aligning the generated content\nwith colloquial human expressions. This task is aimed at bridging the gap\nbetween colloquial language understanding and auditory expression within an AI\nmodel, with the ultimate goal of creating songs that accurately satisfy human\nauditory expectations and structurally align with musical norms. Current\ndatasets are limited due to their narrow descriptive scope, semantic gaps and\ninaccuracies. To overcome data scarcity in this domain, we present the Caichong\nMusic Dataset (CaiMD). CaiMD is manually annotated by both professional\nmusicians and amateurs, offering diverse perspectives and a comprehensive\nunderstanding of colloquial descriptions. Unlike existing datasets pre-set with\nexpert annotations or auto-generated ones with inherent biases, CaiMD caters\nmore sufficiently to our purpose of aligning AI-generated music with widespread\nuser-desired results. Moreover, we propose an innovative single-stage framework\ncalled MuDiT/MuSiT for enabling effective human-machine alignment in song\ncreation. This framework not only achieves cross-modal comprehension between\ncolloquial language and auditory music perceptions but also ensures generated\nsongs align with user-desired results. MuDiT/MuSiT employs one DiT/SiT model\nfor end-to-end generation of musical components like melody, harmony, rhythm,\nvocals, and instrumentation. The approach ensures harmonious sonic cohesiveness\namongst all generated musical components, facilitating better resonance with\nhuman auditory expectations.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}