{"id":"2408.06070","title":"ControlNeXt: Powerful and Efficient Control for Image and Video\n  Generation","authors":"Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang and\n  Jiaya Jia","authorsParsed":[["Peng","Bohao",""],["Wang","Jian",""],["Zhang","Yuechen",""],["Li","Wenbo",""],["Yang","Ming-Chang",""],["Jia","Jiaya",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 11:41:18 GMT"},{"version":"v2","created":"Thu, 15 Aug 2024 02:08:08 GMT"}],"updateDate":"2024-08-16","timestamp":1723462878000,"abstract":"  Diffusion models have demonstrated remarkable and robust abilities in both\nimage and video generation. To achieve greater control over generated results,\nresearchers introduce additional architectures, such as ControlNet, Adapters\nand ReferenceNet, to integrate conditioning controls. However, current\ncontrollable generation methods often require substantial additional\ncomputational resources, especially for video generation, and face challenges\nin training or exhibit weak control. In this paper, we propose ControlNeXt: a\npowerful and efficient method for controllable image and video generation. We\nfirst design a more straightforward and efficient architecture, replacing heavy\nadditional branches with minimal additional cost compared to the base model.\nSuch a concise structure also allows our method to seamlessly integrate with\nother LoRA weights, enabling style alteration without the need for additional\ntraining. As for training, we reduce up to 90% of learnable parameters compared\nto the alternatives. Furthermore, we propose another method called Cross\nNormalization (CN) as a replacement for Zero-Convolution' to achieve fast and\nstable training convergence. We have conducted various experiments with\ndifferent base models across images and videos, demonstrating the robustness of\nour method.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}