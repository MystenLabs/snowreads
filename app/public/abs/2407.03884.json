{"id":"2407.03884","title":"Planning with Large Language Models for Conversational Agents","authors":"Zhigen Li, Jianxiang Peng, Yanmeng Wang, Tianhao Shen, Minghui Zhang,\n  Linxi Su, Shang Wu, Yihang Wu, Yuqian Wang, Ye Wang, Wei Hu, Jianfeng Li,\n  Shaojun Wang, Jing Xiao and Deyi Xiong","authorsParsed":[["Li","Zhigen",""],["Peng","Jianxiang",""],["Wang","Yanmeng",""],["Shen","Tianhao",""],["Zhang","Minghui",""],["Su","Linxi",""],["Wu","Shang",""],["Wu","Yihang",""],["Wang","Yuqian",""],["Wang","Ye",""],["Hu","Wei",""],["Li","Jianfeng",""],["Wang","Shaojun",""],["Xiao","Jing",""],["Xiong","Deyi",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 12:23:02 GMT"}],"updateDate":"2024-07-08","timestamp":1720095782000,"abstract":"  Controllability and proactivity are crucial properties of autonomous\nconversational agents (CAs). Controllability requires the CAs to follow the\nstandard operating procedures (SOPs), such as verifying identity before\nactivating credit cards. Proactivity requires the CAs to guide the conversation\ntowards the goal during user uncooperation, such as persuasive dialogue.\nExisting research cannot be unified with controllability, proactivity, and low\nmanual annotation. To bridge this gap, we propose a new framework for\nplanning-based conversational agents (PCA) powered by large language models\n(LLMs), which only requires humans to define tasks and goals for the LLMs.\nBefore conversation, LLM plans the core and necessary SOP for dialogue offline.\nDuring the conversation, LLM plans the best action path online referring to the\nSOP, and generates responses to achieve process controllability. Subsequently,\nwe propose a semi-automatic dialogue data creation framework and curate a\nhigh-quality dialogue dataset (PCA-D). Meanwhile, we develop multiple variants\nand evaluation metrics for PCA, e.g., planning with Monte Carlo Tree Search\n(PCA-M), which searches for the optimal dialogue action while satisfying SOP\nconstraints and achieving the proactive of the dialogue. Experiment results\nshow that LLMs finetuned on PCA-D can significantly improve the performance and\ngeneralize to unseen domains. PCA-M outperforms other CoT and ToT baselines in\nterms of conversation controllability, proactivity, task success rate, and\noverall logical coherence, and is applicable in industry dialogue scenarios.\nThe dataset and codes are available at XXXX.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}