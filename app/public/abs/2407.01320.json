{"id":"2407.01320","title":"Increasing Model Capacity for Free: A Simple Strategy for Parameter\n  Efficient Fine-tuning","authors":"Haobo Song, Hao Zhao, Soumajit Majumder, Tao Lin","authorsParsed":[["Song","Haobo",""],["Zhao","Hao",""],["Majumder","Soumajit",""],["Lin","Tao",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 14:26:48 GMT"}],"updateDate":"2024-07-02","timestamp":1719844008000,"abstract":"  Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, has\nattracted more attention for downstream tasks recently. While\nparameter-efficient fine-tuning methods have been proposed and proven effective\nwithout retraining all model parameters, their performance is limited by the\ncapacity of incremental modules, especially under constrained parameter\nbudgets. \\\\ To overcome this challenge, we propose CapaBoost, a simple yet\neffective strategy that enhances model capacity by leveraging low-rank updates\nthrough parallel weight modules in target layers. By applying static random\nmasks to the shared weight matrix, CapaBoost constructs a diverse set of weight\nmatrices, effectively increasing the rank of incremental weights without adding\nparameters. Notably, our approach can be seamlessly integrated into various\nexisting parameter-efficient fine-tuning methods. We extensively validate the\nefficacy of CapaBoost through experiments on diverse downstream tasks,\nincluding natural language understanding, question answering, and image\nclassification. Our results demonstrate significant improvements over\nbaselines, without incurring additional computation or storage costs. Our code\nis available at \\url{https://github.com/LINs-lab/CapaBoost}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"lYP3ARPukfQeCFHfgSa1W9eUMdm3KSDBw_xDmCMxwBk","pdfSize":"1510485"}
