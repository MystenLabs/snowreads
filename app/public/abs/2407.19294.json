{"id":"2407.19294","title":"Rethinking Attention Module Design for Point Cloud Analysis","authors":"Chengzhi Wu, Kaige Wang, Zeyun Zhong, Hao Fu, Junwei Zheng, Jiaming\n  Zhang, Julius Pfrommer, and J\\\"urgen Beyerer","authorsParsed":[["Wu","Chengzhi",""],["Wang","Kaige",""],["Zhong","Zeyun",""],["Fu","Hao",""],["Zheng","Junwei",""],["Zhang","Jiaming",""],["Pfrommer","Julius",""],["Beyerer","JÃ¼rgen",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 16:24:53 GMT"}],"updateDate":"2024-07-30","timestamp":1722097493000,"abstract":"  In recent years, there have been significant advancements in applying\nattention mechanisms to point cloud analysis. However, attention module\nvariants featured in various research papers often operate under diverse\nsettings and tasks, incorporating potential training strategies. This\nheterogeneity poses challenges in establishing a fair comparison among these\nattention module variants. In this paper, we address this issue by rethinking\nand exploring attention module design within a consistent base framework and\nsettings. Both global-based and local-based attention methods are studied, with\na focus on the selection basis and scales of neighbors for local-based\nattention. Different combinations of aggregated local features and computation\nmethods for attention scores are evaluated, ranging from the initial\naddition/concatenation-based approach to the widely adopted dot product-based\nmethod and the recently proposed vector attention technique. Various position\nencoding methods are also investigated. Our extensive experimental analysis\nreveals that there is no universally optimal design across diverse point cloud\ntasks. Instead, drawing from best practices, we propose tailored attention\nmodules for specific tasks, leading to superior performance on point cloud\nclassification and segmentation benchmarks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}