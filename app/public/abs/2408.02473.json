{"id":"2408.02473","title":"Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture\n  and Automated Deployment Flow","authors":"Philip Wiese, Gamze \\.Islamo\\u{g}lu, Moritz Scherer, Luka Macan,\n  Victor J.B. Jung, Alessio Burrello, Francesco Conti, Luca Benini","authorsParsed":[["Wiese","Philip",""],["İslamoğlu","Gamze",""],["Scherer","Moritz",""],["Macan","Luka",""],["Jung","Victor J. B.",""],["Burrello","Alessio",""],["Conti","Francesco",""],["Benini","Luca",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 13:57:32 GMT"}],"updateDate":"2024-08-06","timestamp":1722866252000,"abstract":"  One of the challenges for Tiny Machine Learning (tinyML) is keeping up with\nthe evolution of Machine Learning models from Convolutional Neural Networks to\nTransformers. We address this by leveraging a heterogeneous architectural\ntemplate coupling RISC-V processors with hardwired accelerators supported by an\nautomated deployment flow. We demonstrate an Attention-based model in a tinyML\npower envelope with an octa-core cluster coupled with an accelerator for\nquantized Attention. Our deployment flow enables an end-to-end 8-bit\nMobileBERT, achieving leading-edge energy efficiency and throughput of 2960\nGOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI\ntechnology).\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1DoQrfqz1mL8c-0XZmbfJ1o-bikwj26yE2ZpCtrpKUk","pdfSize":"418380","txDigest":"FWUSjpzk6h3sAiguAJi4nsJdJWPVsFPzDwTHxp8TvD7D","endEpoch":"1","status":"CERTIFIED"}
