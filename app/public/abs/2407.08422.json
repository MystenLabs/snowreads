{"id":"2407.08422","title":"On the (In)Security of LLM App Stores","authors":"Xinyi Hou, Yanjie Zhao, and Haoyu Wang","authorsParsed":[["Hou","Xinyi",""],["Zhao","Yanjie",""],["Wang","Haoyu",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 12:03:32 GMT"},{"version":"v2","created":"Mon, 29 Jul 2024 11:18:57 GMT"}],"updateDate":"2024-07-30","timestamp":1720699412000,"abstract":"  LLM app stores have seen rapid growth, leading to the proliferation of\nnumerous custom LLM apps. However, this expansion raises security concerns. In\nthis study, we propose a three-layer concern framework to identify the\npotential security risks of LLM apps, i.e., LLM apps with abusive potential,\nLLM apps with malicious intent, and LLM apps with exploitable vulnerabilities.\nOver five months, we collected 786,036 LLM apps from six major app stores: GPT\nStore, FlowGPT, Poe, Coze, Cici, and Character.AI. Our research integrates\nstatic and dynamic analysis, the development of a large-scale toxic word\ndictionary (i.e., ToxicDict) comprising over 31,783 entries, and automated\nmonitoring tools to identify and mitigate threats. We uncovered that 15,146\napps had misleading descriptions, 1,366 collected sensitive personal\ninformation against their privacy policies, and 15,996 generated harmful\ncontent such as hate speech, self-harm, extremism, etc. Additionally, we\nevaluated the potential for LLM apps to facilitate malicious activities,\nfinding that 616 apps could be used for malware generation, phishing, etc. Our\nfindings highlight the urgent need for robust regulatory frameworks and\nenhanced enforcement mechanisms.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}