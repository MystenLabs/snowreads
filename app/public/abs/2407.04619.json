{"id":"2407.04619","title":"CountGD: Multi-Modal Open-World Counting","authors":"Niki Amini-Naieni, Tengda Han, Andrew Zisserman","authorsParsed":[["Amini-Naieni","Niki",""],["Han","Tengda",""],["Zisserman","Andrew",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 16:20:48 GMT"}],"updateDate":"2024-07-08","timestamp":1720196448000,"abstract":"  The goal of this paper is to improve the generality and accuracy of\nopen-vocabulary object counting in images. To improve the generality, we\nrepurpose an open-vocabulary detection foundation model (GroundingDINO) for the\ncounting task, and also extend its capabilities by introducing modules to\nenable specifying the target object to count by visual exemplars. In turn,\nthese new capabilities - being able to specify the target object by\nmulti-modalites (text and exemplars) - lead to an improvement in counting\naccuracy.\n  We make three contributions: First, we introduce the first open-world\ncounting model, CountGD, where the prompt can be specified by a text\ndescription or visual exemplars or both; Second, we show that the performance\nof the model significantly improves the state of the art on multiple counting\nbenchmarks - when using text only, CountGD is comparable to or outperforms all\nprevious text-only works, and when using both text and visual exemplars, we\noutperform all previous models; Third, we carry out a preliminary study into\ndifferent interactions between the text and visual exemplar prompts, including\nthe cases where they reinforce each other and where one restricts the other.\nThe code and an app to test the model are available at\nhttps://www.robots.ox.ac.uk/~vgg/research/countgd/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}