{"id":"2407.15660","title":"MuTT: A Multimodal Trajectory Transformer for Robot Skills","authors":"Claudius Kienle, Benjamin Alt, Onur Celik, Philipp Becker, Darko\n  Katic, Rainer J\\\"akel and Gerhard Neumann","authorsParsed":[["Kienle","Claudius",""],["Alt","Benjamin",""],["Celik","Onur",""],["Becker","Philipp",""],["Katic","Darko",""],["JÃ¤kel","Rainer",""],["Neumann","Gerhard",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 14:18:52 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 09:12:19 GMT"}],"updateDate":"2024-08-23","timestamp":1721657932000,"abstract":"  High-level robot skills represent an increasingly popular paradigm in robot\nprogramming. However, configuring the skills' parameters for a specific task\nremains a manual and time-consuming endeavor. Existing approaches for learning\nor optimizing these parameters often require numerous real-world executions or\ndo not work in dynamic environments. To address these challenges, we propose\nMuTT, a novel encoder-decoder transformer architecture designed to predict\nenvironment-aware executions of robot skills by integrating vision, trajectory,\nand robot skill parameters. Notably, we pioneer the fusion of vision and\ntrajectory, introducing a novel trajectory projection. Furthermore, we\nillustrate MuTT's efficacy as a predictor when combined with a model-based\nrobot skill optimizer. This approach facilitates the optimization of robot\nskill parameters for the current environment, without the need for real-world\nexecutions during optimization. Designed for compatibility with any\nrepresentation of robot skills, MuTT demonstrates its versatility across three\ncomprehensive experiments, showcasing superior performance across two different\nskill representations.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}