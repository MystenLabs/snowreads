{"id":"2408.05686","title":"The Bandit Whisperer: Communication Learning for Restless Bandits","authors":"Yunfan Zhao, Tonghan Wang, Dheeraj Nagaraj, Aparna Taneja, Milind\n  Tambe","authorsParsed":[["Zhao","Yunfan",""],["Wang","Tonghan",""],["Nagaraj","Dheeraj",""],["Taneja","Aparna",""],["Tambe","Milind",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 03:39:46 GMT"}],"updateDate":"2024-08-13","timestamp":1723347586000,"abstract":"  Applying Reinforcement Learning (RL) to Restless Multi-Arm Bandits (RMABs)\noffers a promising avenue for addressing allocation problems with resource\nconstraints and temporal dynamics. However, classic RMAB models largely\noverlook the challenges of (systematic) data errors - a common occurrence in\nreal-world scenarios due to factors like varying data collection protocols and\nintentional noise for differential privacy. We demonstrate that conventional RL\nalgorithms used to train RMABs can struggle to perform well in such settings.\nTo solve this problem, we propose the first communication learning approach in\nRMABs, where we study which arms, when involved in communication, are most\neffective in mitigating the influence of such systematic data errors. In our\nsetup, the arms receive Q-function parameters from similar arms as messages to\nguide behavioral policies, steering Q-function updates. We learn communication\nstrategies by considering the joint utility of messages across all pairs of\narms and using a Q-network architecture that decomposes the joint utility. Both\ntheoretical and empirical evidence validate the effectiveness of our method in\nsignificantly improving RMAB performance across diverse problems.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Multiagent Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}