{"id":"2407.13920","title":"DuoFormer: Leveraging Hierarchical Visual Representations by Local and\n  Global Attention","authors":"Xiaoya Tang, Bodong Zhang, Beatrice S. Knudsen, Tolga Tasdizen","authorsParsed":[["Tang","Xiaoya",""],["Zhang","Bodong",""],["Knudsen","Beatrice S.",""],["Tasdizen","Tolga",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 22:15:35 GMT"}],"updateDate":"2024-07-22","timestamp":1721340935000,"abstract":"  We here propose a novel hierarchical transformer model that adeptly\nintegrates the feature extraction capabilities of Convolutional Neural Networks\n(CNNs) with the advanced representational potential of Vision Transformers\n(ViTs). Addressing the lack of inductive biases and dependence on extensive\ntraining datasets in ViTs, our model employs a CNN backbone to generate\nhierarchical visual representations. These representations are then adapted for\ntransformer input through an innovative patch tokenization. We also introduce a\n'scale attention' mechanism that captures cross-scale dependencies,\ncomplementing patch attention to enhance spatial understanding and preserve\nglobal perception. Our approach significantly outperforms baseline models on\nsmall and medium-sized medical datasets, demonstrating its efficiency and\ngeneralizability. The components are designed as plug-and-play for different\nCNN architectures and can be adapted for multiple applications. The code is\navailable at https://github.com/xiaoyatang/DuoFormer.git.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OmRZCV8Lnn5jr_0xGG1kRZ9fHe2tnYSScuXVSENR4P0","pdfSize":"1046938"}
