{"id":"2408.15813","title":"DQFormer: Towards Unified LiDAR Panoptic Segmentation with Decoupled\n  Queries","authors":"Yu Yang, Jianbiao Mei, Liang Liu, Siliang Du, Yilin Xiao, Jongwon Ra,\n  Yong Liu, Xiao Xu, Huifeng Wu","authorsParsed":[["Yang","Yu",""],["Mei","Jianbiao",""],["Liu","Liang",""],["Du","Siliang",""],["Xiao","Yilin",""],["Ra","Jongwon",""],["Liu","Yong",""],["Xu","Xiao",""],["Wu","Huifeng",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 14:14:33 GMT"}],"updateDate":"2024-08-29","timestamp":1724854473000,"abstract":"  LiDAR panoptic segmentation, which jointly performs instance and semantic\nsegmentation for things and stuff classes, plays a fundamental role in LiDAR\nperception tasks. While most existing methods explicitly separate these two\nsegmentation tasks and utilize different branches (i.e., semantic and instance\nbranches), some recent methods have embraced the query-based paradigm to unify\nLiDAR panoptic segmentation. However, the distinct spatial distribution and\ninherent characteristics of objects(things) and their surroundings(stuff) in 3D\nscenes lead to challenges, including the mutual competition of things/stuff and\nthe ambiguity of classification/segmentation. In this paper, we propose\ndecoupling things/stuff queries according to their intrinsic properties for\nindividual decoding and disentangling classification/segmentation to mitigate\nambiguity. To this end, we propose a novel framework dubbed DQFormer to\nimplement semantic and instance segmentation in a unified workflow.\nSpecifically, we design a decoupled query generator to propose informative\nqueries with semantics by localizing things/stuff positions and fusing\nmulti-level BEV embeddings. Moreover, a query-oriented mask decoder is\nintroduced to decode corresponding segmentation masks by performing masked\ncross-attention between queries and mask embeddings. Finally, the decoded masks\nare combined with the semantics of the queries to produce panoptic results.\nExtensive experiments on nuScenes and SemanticKITTI datasets demonstrate the\nsuperiority of our DQFormer framework.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}