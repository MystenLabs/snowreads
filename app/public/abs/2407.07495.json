{"id":"2407.07495","title":"Bucket Pre-training is All You Need","authors":"Hongtao Liu, Qiyao Peng, Qing Yang, Kai Liu, Hongyan Xu","authorsParsed":[["Liu","Hongtao",""],["Peng","Qiyao",""],["Yang","Qing",""],["Liu","Kai",""],["Xu","Hongyan",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 09:27:23 GMT"}],"updateDate":"2024-07-11","timestamp":1720603643000,"abstract":"  Large language models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, the conventional\nfixed-length data composition strategy for pretraining, which involves\nconcatenating and splitting documents, can introduce noise and limit the\nmodel's ability to capture long-range dependencies. To address this, we first\nintroduce three metrics for evaluating data composition quality: padding ratio,\ntruncation ratio, and concatenation ratio. We further propose a multi-bucket\ndata composition method that moves beyond the fixed-length paradigm, offering a\nmore flexible and efficient approach to pretraining. Extensive experiments\ndemonstrate that our proposed method could significantly improving both the\nefficiency and efficacy of LLMs pretraining. Our approach not only reduces\nnoise and preserves context but also accelerates training, making it a\npromising solution for LLMs pretraining.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}