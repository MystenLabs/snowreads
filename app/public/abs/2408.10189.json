{"id":"2408.10189","title":"Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic\n  Models","authors":"Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter and Albert Gu","authorsParsed":[["Bick","Aviv",""],["Li","Kevin Y.",""],["Xing","Eric P.",""],["Kolter","J. Zico",""],["Gu","Albert",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 17:48:11 GMT"}],"updateDate":"2024-08-20","timestamp":1724089691000,"abstract":"  Transformer architectures have become a dominant paradigm for domains like\nlanguage modeling but suffer in many inference settings due to their\nquadratic-time self-attention. Recently proposed subquadratic architectures,\nsuch as Mamba, have shown promise, but have been pretrained with substantially\nless computational resources than the strongest Transformer models. In this\nwork, we present a method that is able to distill a pretrained Transformer\narchitecture into alternative architectures such as state space models (SSMs).\nThe key idea to our approach is that we can view both Transformers and SSMs as\napplying different forms of mixing matrices over the token sequences. We can\nthus progressively distill the Transformer architecture by matching different\ndegrees of granularity in the SSM: first matching the mixing matrices\nthemselves, then the hidden units at each block, and finally the end-to-end\npredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant\nbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid\nversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the\ntraining data typically used to train models from scratch, Phi-Mamba boasts\nsubstantially stronger performance compared to all past open-source\nnon-Transformer models. MOHAWK allows models like SSMs to leverage\ncomputational resources invested in training Transformer-based architectures,\nhighlighting a new avenue for building such models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1YxKmqtAp_-e_1NEV2XtEUIciRVJzEvX7DqoYibWE1o","pdfSize":"807207"}
