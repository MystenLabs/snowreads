{"id":"2408.07116","title":"Generative Photomontage","authors":"Sean J. Liu, Nupur Kumari, Ariel Shamir, Jun-Yan Zhu","authorsParsed":[["Liu","Sean J.",""],["Kumari","Nupur",""],["Shamir","Ariel",""],["Zhu","Jun-Yan",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 17:59:51 GMT"},{"version":"v2","created":"Fri, 16 Aug 2024 17:59:51 GMT"}],"updateDate":"2024-08-20","timestamp":1723571991000,"abstract":"  Text-to-image models are powerful tools for image creation. However, the\ngeneration process is akin to a dice roll and makes it difficult to achieve a\nsingle image that captures everything a user wants. In this paper, we propose a\nframework for creating the desired image by compositing it from various parts\nof generated images, in essence forming a Generative Photomontage. Given a\nstack of images generated by ControlNet using the same input condition and\ndifferent seeds, we let users select desired parts from the generated results\nusing a brush stroke interface. We introduce a novel technique that takes in\nthe user's brush strokes, segments the generated images using a graph-based\noptimization in diffusion feature space, and then composites the segmented\nregions via a new feature-space blending method. Our method faithfully\npreserves the user-selected regions while compositing them harmoniously. We\ndemonstrate that our flexible framework can be used for many applications,\nincluding generating new appearance combinations, fixing incorrect shapes and\nartifacts, and improving prompt alignment. We show compelling results for each\napplication and demonstrate that our method outperforms existing image blending\nmethods and various baselines.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}