{"id":"2407.05233","title":"Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of\n  Gemma-2b-it and Phi2 Models","authors":"Jianlong Chen, Wei Xu, Zhicheng Ding, Jinxin Xu, Hao Yan and Xinyu\n  Zhang","authorsParsed":[["Chen","Jianlong",""],["Xu","Wei",""],["Ding","Zhicheng",""],["Xu","Jinxin",""],["Yan","Hao",""],["Zhang","Xinyu",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 02:15:26 GMT"}],"updateDate":"2024-07-09","timestamp":1720318526000,"abstract":"  Prompt recovery, a crucial task in natural language processing, entails the\nreconstruction of prompts or instructions that language models use to convert\ninput text into a specific output. Although pivotal, the design and\neffectiveness of prompts represent a challenging and relatively untapped field\nwithin NLP research. This paper delves into an exhaustive investigation of\nprompt recovery methodologies, employing a spectrum of pre-trained language\nmodels and strategies. Our study is a comparative analysis aimed at gauging the\nefficacy of various models on a benchmark dataset, with the goal of pinpointing\nthe most proficient approach for prompt recovery. Through meticulous\nexperimentation and detailed analysis, we elucidate the outstanding performance\nof the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its\ncounterparts, showcasing its exceptional capability in accurately\nreconstructing prompts for text transformation tasks. Our findings offer a\nsignificant contribution to the existing knowledge on prompt recovery, shedding\nlight on the intricacies of prompt design and offering insightful perspectives\nfor future innovations in text rewriting and the broader field of natural\nlanguage processing.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}