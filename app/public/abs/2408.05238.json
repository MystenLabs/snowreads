{"id":"2408.05238","title":"Solving Large Rank-Deficient Linear Least-Squares Problems on\n  Shared-Memory CPU Architectures and GPU Architectures","authors":"M\\'onica Chillar\\'on, Gregorio Quintana-Ort\\'i, Vicente Vidal,\n  Per-Gunnar Martinsson","authorsParsed":[["Chillarón","Mónica",""],["Quintana-Ortí","Gregorio",""],["Vidal","Vicente",""],["Martinsson","Per-Gunnar",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 16:14:07 GMT"}],"updateDate":"2024-08-13","timestamp":1722874447000,"abstract":"  Solving very large linear systems of equations is a key computational task in\nscience and technology. In many cases, the coefficient matrix of the linear\nsystem is rank-deficient, leading to systems that may be underdetermined,\ninconsistent, or both. In such cases, one generally seeks to compute the least\nsquares solution that minimizes the residual of the problem, which can be\nfurther defined as the solution with smallest norm in cases where the\ncoefficient matrix has a nontrivial nullspace. This work presents several new\ntechniques for solving least squares problems involving coefficient matrices\nthat are so large that they do not fit in main memory. The implementations\ninclude both CPU and GPU variants. All techniques rely on complete orthogonal\ndecompositions that guarantee that both conditions of a least squares solution\nare met, regardless of the rank properties of the matrix. Specifically, they\nrely on the recently proposed \"randUTV\" algorithm that is particularly\neffective in strongly communication-constrained environments. A detailed\nprecision and performance study reveals that the new methods, that operate on\ndata stored on disk, are competitive with state-of-the-art methods that store\nall data in main memory.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Performance"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}