{"id":"2408.11656","title":"Macformer: Transformer with Random Maclaurin Feature Attention","authors":"Yuhan Guo, Lizhong Ding, Ye Yuan, Guoren Wang","authorsParsed":[["Guo","Yuhan",""],["Ding","Lizhong",""],["Yuan","Ye",""],["Wang","Guoren",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 14:27:36 GMT"}],"updateDate":"2024-08-22","timestamp":1724250456000,"abstract":"  Random feature attention (RFA) adopts random fourier feature (RFF) methods to\napproximate the softmax function, resulting in a linear time and space\nattention mechanism that enables the construction of an efficient Transformer.\nInspired by RFA, we propose Macformer, a Transformer architecture that employs\nrandom Maclaurin features (RMF) to approximate various dot-product kernels,\nthereby accelerating attention computations for long sequence. Macformer\nconsists of Random Maclaurin Feature Attention (RMFA) and pre-post Scaling\nBatch Normalization (ppSBN), the former is an unbiased approximation for\ndot-product kernelized attention and the later is a two-stage regularization\nmechanism guaranteeing the error of RMFA. We conducted toy experiments to\ndemonstrate the efficiency of RMFA and ppSBN, and experiments on long range\narena (LRA) benchmark to validate the acceleration and accuracy of Macformer\nwith different dot-product kernels. Experiment results of Macformer are\nconsistent with our theoretical analysis.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}