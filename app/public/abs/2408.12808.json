{"id":"2408.12808","title":"VALE: A Multimodal Visual and Language Explanation Framework for Image\n  Classifiers using eXplainable AI and Language Models","authors":"Purushothaman Natarajan and Athira Nambiar","authorsParsed":[["Natarajan","Purushothaman",""],["Nambiar","Athira",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 03:02:11 GMT"}],"updateDate":"2024-08-26","timestamp":1724382131000,"abstract":"  Deep Neural Networks (DNNs) have revolutionized various fields by enabling\ntask automation and reducing human error. However, their internal workings and\ndecision-making processes remain obscure due to their black box nature.\nConsequently, the lack of interpretability limits the application of these\nmodels in high-risk scenarios. To address this issue, the emerging field of\neXplainable Artificial Intelligence (XAI) aims to explain and interpret the\ninner workings of DNNs. Despite advancements, XAI faces challenges such as the\nsemantic gap between machine and human understanding, the trade-off between\ninterpretability and performance, and the need for context-specific\nexplanations. To overcome these limitations, we propose a novel multimodal\nframework named VALE Visual and Language Explanation. VALE integrates\nexplainable AI techniques with advanced language models to provide\ncomprehensive explanations. This framework utilizes visual explanations from\nXAI tools, an advanced zero-shot image segmentation model, and a visual\nlanguage model to generate corresponding textual explanations. By combining\nvisual and textual explanations, VALE bridges the semantic gap between machine\noutputs and human interpretation, delivering results that are more\ncomprehensible to users. In this paper, we conduct a pilot study of the VALE\nframework for image classification tasks. Specifically, Shapley Additive\nExplanations (SHAP) are used to identify the most influential regions in\nclassified images. The object of interest is then extracted using the Segment\nAnything Model (SAM), and explanations are generated using state-of-the-art\npre-trained Vision-Language Models (VLMs). Extensive experimental studies are\nperformed on two datasets: the ImageNet dataset and a custom underwater SONAR\nimage dataset, demonstrating VALEs real-world applicability in underwater image\nclassification.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}