{"id":"2407.08488","title":"Lynx: An Open Source Hallucination Evaluation Model","authors":"Selvan Sunitha Ravi, Bartosz Mielczarek, Anand Kannappan, Douwe Kiela,\n  Rebecca Qian","authorsParsed":[["Ravi","Selvan Sunitha",""],["Mielczarek","Bartosz",""],["Kannappan","Anand",""],["Kiela","Douwe",""],["Qian","Rebecca",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:22:17 GMT"},{"version":"v2","created":"Mon, 22 Jul 2024 18:41:53 GMT"}],"updateDate":"2024-07-24","timestamp":1720704137000,"abstract":"  Retrieval Augmented Generation (RAG) techniques aim to mitigate\nhallucinations in Large Language Models (LLMs). However, LLMs can still produce\ninformation that is unsupported or contradictory to the retrieved contexts. We\nintroduce LYNX, a SOTA hallucination detection LLM that is capable of advanced\nreasoning on challenging real-world hallucination scenarios. To evaluate LYNX,\nwe present HaluBench, a comprehensive hallucination evaluation benchmark,\nconsisting of 15k samples sourced from various real-world domains. Our\nexperiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and\nclosed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,\nHaluBench and our evaluation code for public access.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"UJc_jvZt-E5kLiYkjdwC7BAU_VmbAwRr7HdfM1Vfl3M","pdfSize":"368412"}
