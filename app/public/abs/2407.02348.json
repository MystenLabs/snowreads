{"id":"2407.02348","title":"Revisiting Cascaded Ensembles for Efficient Inference","authors":"Steven Kolawole, Don Dennis, Ameet Talwalkar, and Virginia Smith","authorsParsed":[["Kolawole","Steven",""],["Dennis","Don",""],["Talwalkar","Ameet",""],["Smith","Virginia",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 15:14:12 GMT"}],"updateDate":"2024-07-03","timestamp":1719933252000,"abstract":"  A common approach to make machine learning inference more efficient is to use\nexample-specific adaptive schemes, which route or select models for each\nexample at inference time. In this work we study a simple scheme for adaptive\ninference. We build a cascade of ensembles (CoE), beginning with\nresource-efficient models and growing to larger, more expressive models, where\nensemble agreement serves as a data-dependent routing criterion. This scheme is\neasy to incorporate into existing inference pipelines, requires no additional\ntraining, and can be used to place models across multiple resource tiers--for\ninstance, serving efficient models at the edge and invoking larger models in\nthe cloud only when necessary. In cases where parallel inference is feasible,\nwe show that CoE can improve accuracy relative to the single best model while\nreducing the average cost of inference by up to 7x, and provides\nPareto-dominate solutions in accuracy and efficiency relative to existing\nadaptive inference baselines. These savings translate to an over 3x-reduction\nin total monetary cost when performing inference using a heterogeneous cluster\nof GPUs. Finally, for edge inference scenarios where portions of the cascade\nreside at the edge vs. in the cloud, CoE can provide a 14x reduction in\ncommunication cost and inference latency without sacrificing accuracy.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Efn-1Nlze4a-ykkaPfx3gHxiZnxF7h_Xj6Qf6ELZWBQ","pdfSize":"855609"}
