{"id":"2408.09817","title":"Contextual Dual Learning Algorithm with Listwise Distillation for\n  Unbiased Learning to Rank","authors":"Lulu Yu, Keping Bi, Shiyu Ni, Jiafeng Guo","authorsParsed":[["Yu","Lulu",""],["Bi","Keping",""],["Ni","Shiyu",""],["Guo","Jiafeng",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 09:13:52 GMT"}],"updateDate":"2024-08-20","timestamp":1724058832000,"abstract":"  Unbiased Learning to Rank (ULTR) aims to leverage biased implicit user\nfeedback (e.g., click) to optimize an unbiased ranking model. The effectiveness\nof the existing ULTR methods has primarily been validated on synthetic\ndatasets. However, their performance on real-world click data remains unclear.\nRecently, Baidu released a large publicly available dataset of their web search\nlogs. Subsequently, the NTCIR-17 ULTRE-2 task released a subset dataset\nextracted from it. We conduct experiments on commonly used or effective ULTR\nmethods on this subset to determine whether they maintain their effectiveness.\nIn this paper, we propose a Contextual Dual Learning Algorithm with Listwise\nDistillation (CDLA-LD) to simultaneously address both position bias and\ncontextual bias. We utilize a listwise-input ranking model to obtain\nreconstructed feature vectors incorporating local contextual information and\nemploy the Dual Learning Algorithm (DLA) method to jointly train this ranking\nmodel and a propensity model to address position bias. As this ranking model\nlearns the interaction information within the documents list of the training\nset, to enhance the ranking model's generalization ability, we additionally\ntrain a pointwise-input ranking model to learn the listwise-input ranking\nmodel's capability for relevance judgment in a listwise manner. Extensive\nexperiments and analysis confirm the effectiveness of our approach.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}