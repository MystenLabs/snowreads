{"id":"2408.08158","title":"EmBARDiment: an Embodied AI Agent for Productivity in XR","authors":"Riccardo Bovo, Steven Abreu, Karan Ahuja, Eric J Gonzalez, Li-Te\n  Cheng, Mar Gonzalez-Franco","authorsParsed":[["Bovo","Riccardo",""],["Abreu","Steven",""],["Ahuja","Karan",""],["Gonzalez","Eric J",""],["Cheng","Li-Te",""],["Gonzalez-Franco","Mar",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 13:48:44 GMT"}],"updateDate":"2024-08-16","timestamp":1723729724000,"abstract":"  XR devices running chat-bots powered by Large Language Models (LLMs) have\ntremendous potential as always-on agents that can enable much better\nproductivity scenarios. However, screen based chat-bots do not take advantage\nof the the full-suite of natural inputs available in XR, including inward\nfacing sensor data, instead they over-rely on explicit voice or text prompts,\nsometimes paired with multi-modal data dropped as part of the query. We propose\na solution that leverages an attention framework that derives context\nimplicitly from user actions, eye-gaze, and contextual memory within the XR\nenvironment. This minimizes the need for engineered explicit prompts, fostering\ngrounded and intuitive interactions that glean user insights for the chat-bot.\nOur user studies demonstrate the imminent feasibility and transformative\npotential of our approach to streamline user interaction in XR with chat-bots,\nwhile offering insights for the design of future XR-embodied LLM agents.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pmKf5WJQnDU-cM9cTj_zNc8wHPQk6gL4gZKHEm5lqZM","pdfSize":"5604545"}
