{"id":"2407.10048","title":"Whisper-SV: Adapting Whisper for Low-data-resource Speaker Verification","authors":"Li Zhang, Ning Jiang, Qing Wang, Yue Li, Quan Lu, and Lei Xie","authorsParsed":[["Zhang","Li",""],["Jiang","Ning",""],["Wang","Qing",""],["Li","Yue",""],["Lu","Quan",""],["Xie","Lei",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 02:21:52 GMT"}],"updateDate":"2024-07-16","timestamp":1720923712000,"abstract":"  Trained on 680,000 hours of massive speech data, Whisper is a multitasking,\nmultilingual speech foundation model demonstrating superior performance in\nautomatic speech recognition, translation, and language identification.\nHowever, its applicability in speaker verification (SV) tasks remains\nunexplored, particularly in low-data-resource scenarios where labeled speaker\ndata in specific domains are limited. To fill this gap, we propose a\nlightweight adaptor framework to boost SV with Whisper, namely Whisper-SV.\nGiven that Whisper is not specifically optimized for SV tasks, we introduce a\nrepresentation selection module to quantify the speaker-specific\ncharacteristics contained in each layer of Whisper and select the top-k layers\nwith prominent discriminative speaker features. To aggregate pivotal\nspeaker-related features while diminishing non-speaker redundancies across the\nselected top-k distinct layers of Whisper, we design a multi-layer aggregation\nmodule in Whisper-SV to integrate multi-layer representations into a singular,\ncompacted representation for SV. In the multi-layer aggregation module, we\nemploy convolutional layers with shortcut connections among different layers to\nrefine speaker characteristics derived from multi-layer representations from\nWhisper. In addition, an attention aggregation layer is used to reduce\nnon-speaker interference and amplify speaker-specific cues for SV tasks.\nFinally, a simple classification module is used for speaker classification.\nExperiments on VoxCeleb1, FFSVC, and IMSV datasets demonstrate that Whisper-SV\nachieves EER/minDCF of 2.22%/0.307, 6.14%/0.488, and 7.50%/0.582, respectively,\nshowing superior performance in low-data-resource SV scenarios.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}