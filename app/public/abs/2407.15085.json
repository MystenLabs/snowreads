{"id":"2407.15085","title":"Learn to Preserve and Diversify: Parameter-Efficient Group with\n  Orthogonal Regularization for Domain Generalization","authors":"Jiajun Hu, Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao","authorsParsed":[["Hu","Jiajun",""],["Zhang","Jian",""],["Qi","Lei",""],["Shi","Yinghuan",""],["Gao","Yang",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 07:50:49 GMT"}],"updateDate":"2024-07-23","timestamp":1721548249000,"abstract":"  Domain generalization (DG) aims to avoid the performance degradation of the\nmodel when the distribution shift between the limited training data and unseen\ntest data occurs. Recently, foundation models with enormous parameters have\nbeen pre-trained with huge datasets, demonstrating strong generalization\nability and showing promising direction for solving the DG problem. However,\nfully Fine-Tuning (FT) the foundation models results in unsatisfactory\nout-of-distribution accuracy due to the destroyed pre-trained generalized\nfeatures. Recently, Parameter-Efficient Fine-Tuning (PEFT) alleviates the above\nproblem by fine-tuning a small portion of the model parameters while keeping\nthe rest frozen, which achieves better generalization performance compared to\nFT. Nevertheless, PEFT still suffers from the issue of overfitting to the\ntraining domains. To address the above issue, we propose Parameter-Efficient\nGroup with Orthogonal regularization (PEGO) for vision transformers, which\neffectively preserves the generalization ability of the pre-trained network and\nlearns more diverse knowledge compared with conventional PEFT. Specifically, we\ninject a group of trainable Low-Rank Adaptation (LoRA) modules into the\npre-trained model and propose an orthogonal regularization loss to enhance the\ngeneralization ability of the model. Our framework achieves SOTA performance on\nfive DG benchmarks, while only requiring training a small number of parameters\nwithout adding additional testing cost.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}