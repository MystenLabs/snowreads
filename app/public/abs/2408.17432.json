{"id":"2408.17432","title":"SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame\n  Selection","authors":"Ismail Rasim Ulgen, Shreeram Suresh Chandra, Junchen Lu, Berrak Sisman","authorsParsed":[["Ulgen","Ismail Rasim",""],["Chandra","Shreeram Suresh",""],["Lu","Junchen",""],["Sisman","Berrak",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 17:34:46 GMT"}],"updateDate":"2024-09-06","timestamp":1725039286000,"abstract":"  Synthesizing the voices of unseen speakers is a persisting challenge in\nmulti-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on\nmodeling speaker characteristics through speaker conditioning during training.\nModeling unseen speaker attributes through this approach has necessitated an\nincrease in model complexity, which makes it challenging to reproduce results\nand improve upon them. We design a simple alternative to this. We propose\nSelectTTS, a novel method to select the appropriate frames from the target\nspeaker and decode using frame-level self-supervised learning (SSL) features.\nWe show that this approach can effectively capture speaker characteristics for\nunseen speakers, and achieves comparable results to other multi-speaker TTS\nframeworks in both objective and subjective metrics. With SelectTTS, we show\nthat frame selection from the target speaker's speech is a direct way to\nachieve generalization in unseen speakers with low model complexity. We achieve\nbetter speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E\nwith over an 8x reduction in model parameters and a 270x reduction in training\ndata.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XXMoKhnjeIs98V8EBAGKIR5PZopKyswqBdf-nDZtykY","pdfSize":"648387"}
