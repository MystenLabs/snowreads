{"id":"2408.14227","title":"TC-PDM: Temporally Consistent Patch Diffusion Models for\n  Infrared-to-Visible Video Translation","authors":"Anh-Dzung Doan and Vu Minh Hieu Phan and Surabhi Gupta and Markus\n  Wagner and Tat-Jun Chin and Ian Reid","authorsParsed":[["Doan","Anh-Dzung",""],["Phan","Vu Minh Hieu",""],["Gupta","Surabhi",""],["Wagner","Markus",""],["Chin","Tat-Jun",""],["Reid","Ian",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 12:43:48 GMT"}],"updateDate":"2024-08-27","timestamp":1724676228000,"abstract":"  Infrared imaging offers resilience against changing lighting conditions by\ncapturing object temperatures. Yet, in few scenarios, its lack of visual\ndetails compared to daytime visible images, poses a significant challenge for\nhuman and machine interpretation. This paper proposes a novel diffusion method,\ndubbed Temporally Consistent Patch Diffusion Models (TC-DPM), for\ninfrared-to-visible video translation. Our method, extending the Patch\nDiffusion Model, consists of two key components. Firstly, we propose a\nsemantic-guided denoising, leveraging the strong representations of\nfoundational models. As such, our method faithfully preserves the semantic\nstructure of generated visible images. Secondly, we propose a novel temporal\nblending module to guide the denoising trajectory, ensuring the temporal\nconsistency between consecutive frames. Experiment shows that TC-PDM\noutperforms state-of-the-art methods by 35.3% in FVD for infrared-to-visible\nvideo translation and by 6.1% in AP50 for day-to-night object detection. Our\ncode is publicly available at https://github.com/dzungdoan6/tc-pdm\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}