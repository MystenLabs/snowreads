{"id":"2407.08889","title":"Diff-MST: Differentiable Mixing Style Transfer","authors":"Soumya Sai Vanka, Christian Steinmetz, Jean-Baptiste Rolland, Joshua\n  Reiss, George Fazekas","authorsParsed":[["Vanka","Soumya Sai",""],["Steinmetz","Christian",""],["Rolland","Jean-Baptiste",""],["Reiss","Joshua",""],["Fazekas","George",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 23:06:47 GMT"}],"updateDate":"2024-07-15","timestamp":1720739207000,"abstract":"  Mixing style transfer automates the generation of a multitrack mix for a\ngiven set of tracks by inferring production attributes from a reference song.\nHowever, existing systems for mixing style transfer are limited in that they\noften operate only on a fixed number of tracks, introduce artifacts, and\nproduce mixes in an end-to-end fashion, without grounding in traditional audio\neffects, prohibiting interpretability and controllability. To overcome these\nchallenges, we introduce Diff-MST, a framework comprising a differentiable\nmixing console, a transformer controller, and an audio production style loss\nfunction. By inputting raw tracks and a reference song, our model estimates\ncontrol parameters for audio effects within a differentiable mixing console,\nproducing high-quality mixes and enabling post-hoc adjustments. Moreover, our\narchitecture supports an arbitrary number of input tracks without source\nlabelling, enabling real-world applications. We evaluate our model's\nperformance against robust baselines and showcase the effectiveness of our\napproach, architectural design, tailored audio production style loss, and\ninnovative training methodology for the given task.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gdX5ggOJX3ZTG0Grvj8L5xIRypEFISL_Cc4H98--sSY","pdfSize":"435629"}
