{"id":"2408.03516","title":"Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in\n  Autonomous Driving","authors":"Amirhosein Chahe, Lifeng Zhou","authorsParsed":[["Chahe","Amirhosein",""],["Zhou","Lifeng",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 02:54:43 GMT"}],"updateDate":"2024-08-08","timestamp":1722999283000,"abstract":"  This paper introduces a novel method for open-vocabulary 3D scene\nunderstanding in autonomous driving by combining Language Embedded 3D Gaussians\nwith Large Language Models (LLMs) for enhanced inference. We propose utilizing\nLLMs to generate contextually relevant canonical phrases for segmentation and\nscene interpretation. Our method leverages the contextual and semantic\ncapabilities of LLMs to produce a set of canonical phrases, which are then\ncompared with the language features embedded in the 3D Gaussians. This\nLLM-guided approach significantly improves zero-shot scene understanding and\ndetection of objects of interest, even in the most challenging or unfamiliar\nenvironments. Experimental results on the WayveScenes101 dataset demonstrate\nthat our approach surpasses state-of-the-art methods in terms of accuracy and\nflexibility for open-vocabulary object detection and segmentation. This work\nrepresents a significant advancement towards more intelligent, context-aware\nautonomous driving systems, effectively bridging 3D scene representation with\nhigh-level semantic understanding.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}