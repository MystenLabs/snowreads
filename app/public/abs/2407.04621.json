{"id":"2407.04621","title":"OneRestore: A Universal Restoration Framework for Composite Degradation","authors":"Yu Guo, Yuan Gao, Yuxu Lu, Huilin Zhu, Ryan Wen Liu, Shengfeng He","authorsParsed":[["Guo","Yu",""],["Gao","Yuan",""],["Lu","Yuxu",""],["Zhu","Huilin",""],["Liu","Ryan Wen",""],["He","Shengfeng",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 16:27:00 GMT"},{"version":"v2","created":"Mon, 8 Jul 2024 04:48:19 GMT"},{"version":"v3","created":"Tue, 9 Jul 2024 16:34:10 GMT"},{"version":"v4","created":"Wed, 10 Jul 2024 05:35:48 GMT"}],"updateDate":"2024-07-11","timestamp":1720196820000,"abstract":"  In real-world scenarios, image impairments often manifest as composite\ndegradations, presenting a complex interplay of elements such as low light,\nhaze, rain, and snow. Despite this reality, existing restoration methods\ntypically target isolated degradation types, thereby falling short in\nenvironments where multiple degrading factors coexist. To bridge this gap, our\nstudy proposes a versatile imaging model that consolidates four physical\ncorruption paradigms to accurately represent complex, composite degradation\nscenarios. In this context, we propose OneRestore, a novel transformer-based\nframework designed for adaptive, controllable scene restoration. The proposed\nframework leverages a unique cross-attention mechanism, merging degraded scene\ndescriptors with image features, allowing for nuanced restoration. Our model\nallows versatile input scene descriptors, ranging from manual text embeddings\nto automatic extractions based on visual attributes. Our methodology is further\nenhanced through a composite degradation restoration loss, using extra degraded\nimages as negative samples to fortify model constraints. Comparative results on\nsynthetic and real-world datasets demonstrate OneRestore as a superior\nsolution, significantly advancing the state-of-the-art in addressing complex,\ncomposite degradations.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}