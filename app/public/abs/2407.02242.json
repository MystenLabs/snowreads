{"id":"2407.02242","title":"Towards optimal hierarchical training of neural networks","authors":"Michael Feischl, Alexander Rieder, Fabian Zehetgruber","authorsParsed":[["Feischl","Michael",""],["Rieder","Alexander",""],["Zehetgruber","Fabian",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 13:03:46 GMT"}],"updateDate":"2024-07-03","timestamp":1719925426000,"abstract":"  We propose a hierarchical training algorithm for standard feed-forward neural\nnetworks that adaptively extends the network architecture as soon as the\noptimization reaches a stationary point. By solving small (low-dimensional)\noptimization problems, the extended network provably escapes any local minimum\nor stationary point. Under some assumptions on the approximability of the data\nwith stable neural networks, we show that the algorithm achieves an optimal\nconvergence rate s in the sense that loss is bounded by the number of\nparameters to the -s. As a byproduct, we obtain computable indicators which\njudge the optimality of the training state of a given network and derive a new\nnotion of generalization error.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}