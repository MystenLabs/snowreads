{"id":"2408.06622","title":"ActPrompt: In-Domain Feature Adaptation via Action Cues for Video\n  Temporal Grounding","authors":"Yubin Wang, Xinyang Jiang, De Cheng, Dongsheng Li, Cairong Zhao","authorsParsed":[["Wang","Yubin",""],["Jiang","Xinyang",""],["Cheng","De",""],["Li","Dongsheng",""],["Zhao","Cairong",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 04:18:32 GMT"}],"updateDate":"2024-08-14","timestamp":1723522712000,"abstract":"  Video temporal grounding is an emerging topic aiming to identify specific\nclips within videos. In addition to pre-trained video models, contemporary\nmethods utilize pre-trained vision-language models (VLM) to capture detailed\ncharacteristics of diverse scenes and objects from video frames. However, as\npre-trained on images, VLM may struggle to distinguish action-sensitive\npatterns from static objects, making it necessary to adapt them to specific\ndata domains for effective feature representation over temporal grounding. We\naddress two primary challenges to achieve this goal. Specifically, to mitigate\nhigh adaptation costs, we propose an efficient preliminary in-domain\nfine-tuning paradigm for feature adaptation, where downstream-adaptive features\nare learned through several pretext tasks. Furthermore, to integrate\naction-sensitive information into VLM, we introduce Action-Cue-Injected\nTemporal Prompt Learning (ActPrompt), which injects action cues into the image\nencoder of VLM for better discovering action-sensitive patterns. Extensive\nexperiments demonstrate that ActPrompt is an off-the-shelf training framework\nthat can be effectively applied to various SOTA methods, resulting in notable\nimprovements. The complete code used in this study is provided in the\nsupplementary materials.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}