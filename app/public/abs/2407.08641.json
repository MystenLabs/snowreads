{"id":"2407.08641","title":"How more data can hurt: Instability and regularization in\n  next-generation reservoir computing","authors":"Yuanzhao Zhang and Sean P. Cornelius","authorsParsed":[["Zhang","Yuanzhao",""],["Cornelius","Sean P.",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 16:22:13 GMT"}],"updateDate":"2024-07-12","timestamp":1720714933000,"abstract":"  It has been found recently that more data can, counter-intuitively, hurt the\nperformance of deep neural networks. Here, we show that a more extreme version\nof the phenomenon occurs in data-driven models of dynamical systems. To\nelucidate the underlying mechanism, we focus on next-generation reservoir\ncomputing (NGRC) -- a popular framework for learning dynamics from data. We\nfind that, despite learning a better representation of the flow map with more\ntraining data, NGRC can adopt an ill-conditioned ``integrator'' and lose\nstability. We link this data-induced instability to the auxiliary dimensions\ncreated by the delayed states in NGRC. Based on these findings, we propose\nsimple strategies to mitigate the instability, either by increasing\nregularization strength in tandem with data size, or by carefully introducing\nnoise during training. Our results highlight the importance of proper\nregularization in data-driven modeling of dynamical systems.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing","Mathematics/Dynamical Systems","Nonlinear Sciences/Adaptation and Self-Organizing Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}