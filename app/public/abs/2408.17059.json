{"id":"2408.17059","title":"A Survey of the Self Supervised Learning Mechanisms for Vision\n  Transformers","authors":"Asifullah Khan, Anabia Sohail, Mustansar Fiaz, Mehdi Hassan, Tariq\n  Habib Afridi, Sibghat Ullah Marwat, Farzeen Munir, Safdar Ali, Hannan Naseem,\n  Muhammad Zaigham Zaheer, Kamran Ali, Tangina Sultana, Ziaurrehman Tanoli,\n  Naeem Akhter","authorsParsed":[["Khan","Asifullah",""],["Sohail","Anabia",""],["Fiaz","Mustansar",""],["Hassan","Mehdi",""],["Afridi","Tariq Habib",""],["Marwat","Sibghat Ullah",""],["Munir","Farzeen",""],["Ali","Safdar",""],["Naseem","Hannan",""],["Zaheer","Muhammad Zaigham",""],["Ali","Kamran",""],["Sultana","Tangina",""],["Tanoli","Ziaurrehman",""],["Akhter","Naeem",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 07:38:28 GMT"}],"updateDate":"2024-09-02","timestamp":1725003508000,"abstract":"  Deep supervised learning models require high volume of labeled data to attain\nsufficiently good results. Although, the practice of gathering and annotating\nsuch big data is costly and laborious. Recently, the application of self\nsupervised learning (SSL) in vision tasks has gained significant attention. The\nintuition behind SSL is to exploit the synchronous relationships within the\ndata as a form of self-supervision, which can be versatile. In the current big\ndata era, most of the data is unlabeled, and the success of SSL thus relies in\nfinding ways to improve this vast amount of unlabeled data available. Thus its\nbetter for deep learning algorithms to reduce reliance on human supervision and\ninstead focus on self-supervision based on the inherent relationships within\nthe data. With the advent of ViTs, which have achieved remarkable results in\ncomputer vision, it is crucial to explore and understand the various SSL\nmechanisms employed for training these models specifically in scenarios where\nthere is less label data available. In this survey we thus develop a\ncomprehensive taxonomy of systematically classifying the SSL techniques based\nupon their representations and pre-training tasks being applied. Additionally,\nwe discuss the motivations behind SSL, review popular pre-training tasks, and\nhighlight the challenges and advancements in this field. Furthermore, we\npresent a comparative analysis of different SSL methods, evaluate their\nstrengths and limitations, and identify potential avenues for future research.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}