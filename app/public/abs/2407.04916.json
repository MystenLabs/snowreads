{"id":"2407.04916","title":"Completed Feature Disentanglement Learning for Multimodal MRIs Analysis","authors":"Tianling Liu and Hongying Liu and Fanhua Shang and Lequan Yu and Tong\n  Han and Liang Wan","authorsParsed":[["Liu","Tianling",""],["Liu","Hongying",""],["Shang","Fanhua",""],["Yu","Lequan",""],["Han","Tong",""],["Wan","Liang",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 01:49:38 GMT"}],"updateDate":"2024-07-09","timestamp":1720230578000,"abstract":"  Multimodal MRIs play a crucial role in clinical diagnosis and treatment.\nFeature disentanglement (FD)-based methods, aiming at learning superior feature\nrepresentations for multimodal data analysis, have achieved significant success\nin multimodal learning (MML). Typically, existing FD-based methods separate\nmultimodal data into modality-shared and modality-specific features, and employ\nconcatenation or attention mechanisms to integrate these features. However, our\npreliminary experiments indicate that these methods could lead to a loss of\nshared information among subsets of modalities when the inputs contain more\nthan two modalities, and such information is critical for prediction accuracy.\nFurthermore, these methods do not adequately interpret the relationships\nbetween the decoupled features at the fusion stage. To address these\nlimitations, we propose a novel Complete Feature Disentanglement (CFD) strategy\nthat recovers the lost information during feature decoupling. Specifically, the\nCFD strategy not only identifies modality-shared and modality-specific\nfeatures, but also decouples shared features among subsets of multimodal\ninputs, termed as modality-partial-shared features. We further introduce a new\nDynamic Mixture-of-Experts Fusion (DMF) module that dynamically integrates\nthese decoupled features, by explicitly learning the local-global relationships\namong the features. The effectiveness of our approach is validated through\nclassification tasks on three multimodal MRI datasets. Extensive experimental\nresults demonstrate that our approach outperforms other state-of-the-art MML\nmethods with obvious margins, showcasing its superior performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}