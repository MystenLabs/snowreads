{"id":"2407.02857","title":"AudioTime: A Temporally-aligned Audio-text Benchmark Dataset","authors":"Zeyu Xie, Xuenan Xu, Zhizheng Wu, and Mengyue Wu","authorsParsed":[["Xie","Zeyu",""],["Xu","Xuenan",""],["Wu","Zhizheng",""],["Wu","Mengyue",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 07:15:04 GMT"}],"updateDate":"2024-07-04","timestamp":1719990904000,"abstract":"  Recent advancements in audio generation have enabled the creation of\nhigh-fidelity audio clips from free-form textual descriptions. However,\ntemporal relationships, a critical feature for audio content, are currently\nunderrepresented in mainstream models, resulting in an imprecise temporal\ncontrollability. Specifically, users cannot accurately control the timestamps\nof sound events using free-form text. We acknowledge that a significant factor\nis the absence of high-quality, temporally-aligned audio-text datasets, which\nare essential for training models with temporal control. The more\ntemporally-aligned the annotations, the better the models can understand the\nprecise relationship between audio outputs and temporal textual prompts.\nTherefore, we present a strongly aligned audio-text dataset, AudioTime. It\nprovides text annotations rich in temporal information such as timestamps,\nduration, frequency, and ordering, covering almost all aspects of temporal\ncontrol. Additionally, we offer a comprehensive test set and evaluation metric\nto assess the temporal control performance of various models. Examples are\navailable on the https://zeyuxie29.github.io/AudioTime/\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}