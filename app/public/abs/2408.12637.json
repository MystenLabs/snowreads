{"id":"2408.12637","title":"Building and better understanding vision-language models: insights and\n  future directions","authors":"Hugo Lauren\\c{c}on, Andr\\'es Marafioti, Victor Sanh, L\\'eo Tronchon","authorsParsed":[["Laurençon","Hugo",""],["Marafioti","Andrés",""],["Sanh","Victor",""],["Tronchon","Léo",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 17:47:24 GMT"}],"updateDate":"2024-08-26","timestamp":1724348844000,"abstract":"  The field of vision-language models (VLMs), which take images and texts as\ninputs and output texts, is rapidly evolving and has yet to reach consensus on\nseveral key aspects of the development pipeline, including data, architecture,\nand training methods. This paper can be seen as a tutorial for building a VLM.\nWe begin by providing a comprehensive overview of the current state-of-the-art\napproaches, highlighting the strengths and weaknesses of each, addressing the\nmajor challenges in the field, and suggesting promising research directions for\nunderexplored areas. We then walk through the practical steps to build\nIdefics3-8B, a powerful VLM that significantly outperforms its predecessor\nIdefics2-8B, while being trained efficiently, exclusively on open datasets, and\nusing a straightforward pipeline. These steps include the creation of Docmatix,\na dataset for improving document understanding capabilities, which is 240 times\nlarger than previously available datasets. We release the model along with the\ndatasets created for its training.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_JL6tZh5VfX-HCc4T6zQ-0j00WkM5tc4de1ZCCAAPlI","pdfSize":"2956219"}
