{"id":"2407.03086","title":"Effective Heterogeneous Federated Learning via Efficient\n  Hypernetwork-based Weight Generation","authors":"Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim,\n  JeongGil Ko","authorsParsed":[["Shin","Yujin",""],["Lee","Kichang",""],["Lee","Sungmin",""],["Choi","You Rim",""],["Kim","Hyung-Sin",""],["Ko","JeongGil",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 13:15:12 GMT"}],"updateDate":"2024-07-04","timestamp":1720012512000,"abstract":"  While federated learning leverages distributed client resources, it faces\nchallenges due to heterogeneous client capabilities. This necessitates\nallocating models suited to clients' resources and careful parameter\naggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel\nfederated learning framework for supporting client heterogeneity by combining a\nmulti-exit network architecture with hypernetwork-based model weight\ngeneration. This approach aligns the feature spaces of heterogeneous model\nlayers and resolves per-layer information disparity during weight aggregation.\nTo practically realize HypeMeFed, we also propose a low-rank factorization\napproach to minimize computation and memory overhead associated with\nhypernetworks. Our evaluations on a real-world heterogeneous device testbed\nindicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the\nhypernetwork memory requirements by 98.22%, and accelerates its operations by\n1.86 times compared to a naive hypernetwork approach. These results demonstrate\nHypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for\nfederated learning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}