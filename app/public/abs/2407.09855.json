{"id":"2407.09855","title":"Building pre-train LLM Dataset for the INDIC Languages: a case study on\n  Hindi","authors":"Shantipriya Parida and Shakshi Panwar and Kusum Lata and Sanskruti\n  Mishra and Sambit Sekhar","authorsParsed":[["Parida","Shantipriya",""],["Panwar","Shakshi",""],["Lata","Kusum",""],["Mishra","Sanskruti",""],["Sekhar","Sambit",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 11:29:20 GMT"}],"updateDate":"2024-07-16","timestamp":1720870160000,"abstract":"  Large language models (LLMs) demonstrated transformative capabilities in many\napplications that require automatically generating responses based on human\ninstruction. However, the major challenge for building LLMs, particularly in\nIndic languages, is the availability of high-quality data for building\nfoundation LLMs. In this paper, we are proposing a large pre-train dataset in\nHindi useful for the Indic language Hindi. We have collected the data span\nacross several domains including major dialects in Hindi. The dataset contains\n1.28 billion Hindi tokens. We have explained our pipeline including data\ncollection, pre-processing, and availability for LLM pre-training. The proposed\napproach can be easily extended to other Indic and low-resource languages and\nwill be available freely for LLM pre-training and LLM research purposes.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"q6muJvunek0-37phlpugWxLm5mgjL_GsS0FsjcqIDrw","pdfSize":"390640"}
