{"id":"2407.08699","title":"Mitigating Catastrophic Forgetting in Language Transfer via Model\n  Merging","authors":"Anton Alexandrov, Veselin Raychev, Mark Niklas M\\\"uller, Ce Zhang,\n  Martin Vechev, Kristina Toutanova","authorsParsed":[["Alexandrov","Anton",""],["Raychev","Veselin",""],["MÃ¼ller","Mark Niklas",""],["Zhang","Ce",""],["Vechev","Martin",""],["Toutanova","Kristina",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:32:40 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 07:48:06 GMT"}],"updateDate":"2024-07-17","timestamp":1720719160000,"abstract":"  As open-weight large language models (LLMs) achieve ever more impressive\nperformances across a wide range of tasks in English, practitioners aim to\nadapt these models to different languages. However, such language adaptation is\noften accompanied by catastrophic forgetting of the base model's capabilities,\nseverely limiting the usefulness of the resulting model. We address this issue\nby proposing Branch-and-Merge (BaM), a new adaptation method based on\niteratively merging multiple models, fine-tuned on a subset of the available\ntraining data. BaM is based on the insight that this yields lower magnitude but\nhigher quality weight changes, reducing forgetting of the source domain while\nmaintaining learning on the target domain. We demonstrate in an extensive\nempirical study on Bulgarian and German that BaM can significantly reduce\nforgetting while matching or even improving target domain performance compared\nto both standard continued pretraining and instruction finetuning across\ndifferent model architectures.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}