{"id":"2408.01584","title":"GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS","authors":"Saman Kazemkhani, Aarav Pandya, Daphne Cornelisse, Brennan Shacklett,\n  Eugene Vinitsky","authorsParsed":[["Kazemkhani","Saman",""],["Pandya","Aarav",""],["Cornelisse","Daphne",""],["Shacklett","Brennan",""],["Vinitsky","Eugene",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 21:37:46 GMT"}],"updateDate":"2024-08-06","timestamp":1722634666000,"abstract":"  Multi-agent learning algorithms have been successful at generating superhuman\nplanning in a wide variety of games but have had little impact on the design of\ndeployed multi-agent planners. A key bottleneck in applying these techniques to\nmulti-agent planning is that they require billions of steps of experience. To\nenable the study of multi-agent planning at this scale, we present GPUDrive, a\nGPU-accelerated, multi-agent simulator built on top of the Madrona Game Engine\nthat can generate over a million steps of experience per second. Observation,\nreward, and dynamics functions are written directly in C++, allowing users to\ndefine complex, heterogeneous agent behaviors that are lowered to\nhigh-performance CUDA. We show that using GPUDrive we are able to effectively\ntrain reinforcement learning agents over many scenes in the Waymo Motion\ndataset, yielding highly effective goal-reaching agents in minutes for\nindividual scenes and generally capable agents in a few hours. We ship these\ntrained agents as part of the code base at\nhttps://github.com/Emerge-Lab/gpudrive.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Hardware Architecture","Computing Research Repository/Graphics","Computing Research Repository/Performance"],"license":"http://creativecommons.org/licenses/by/4.0/"}