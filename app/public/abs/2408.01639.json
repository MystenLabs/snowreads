{"id":"2408.01639","title":"Coordinating Planning and Tracking in Layered Control Policies via\n  Actor-Critic Learning","authors":"Fengjun Yang and Nikolai Matni","authorsParsed":[["Yang","Fengjun",""],["Matni","Nikolai",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 02:53:24 GMT"}],"updateDate":"2024-08-06","timestamp":1722653604000,"abstract":"  We propose a reinforcement learning (RL)-based algorithm to jointly train (1)\na trajectory planner and (2) a tracking controller in a layered control\narchitecture. Our algorithm arises naturally from a rewrite of the underlying\noptimal control problem that lends itself to an actor-critic learning approach.\nBy explicitly learning a \\textit{dual} network to coordinate the interaction\nbetween the planning and tracking layers, we demonstrate the ability to achieve\nan effective consensus between the two components, leading to an interpretable\npolicy. We theoretically prove that our algorithm converges to the optimal dual\nnetwork in the Linear Quadratic Regulator (LQR) setting and empirically\nvalidate its applicability to nonlinear systems through simulation experiments\non a unicycle model.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Machine Learning","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/"}