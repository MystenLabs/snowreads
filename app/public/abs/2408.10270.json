{"id":"2408.10270","title":"SEAL: Systematic Error Analysis for Value ALignment","authors":"Manon Revel, Matteo Cargnelutti, Tyna Eloundou, Greg Leppert","authorsParsed":[["Revel","Manon",""],["Cargnelutti","Matteo",""],["Eloundou","Tyna",""],["Leppert","Greg",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 18:48:30 GMT"}],"updateDate":"2024-08-21","timestamp":1723834110000,"abstract":"  Reinforcement Learning from Human Feedback (RLHF) aims to align language\nmodels (LMs) with human values by training reward models (RMs) on binary\npreferences and using these RMs to fine-tune the base LMs. Despite its\nimportance, the internal mechanisms of RLHF remain poorly understood. This\npaper introduces new metrics to evaluate the effectiveness of modeling and\naligning human values, namely feature imprint, alignment resistance and\nalignment robustness. We categorize alignment datasets into target features\n(desired values) and spoiler features (undesired concepts). By regressing RM\nscores against these features, we quantify the extent to which RMs reward them\n- a metric we term feature imprint. We define alignment resistance as the\nproportion of the preference dataset where RMs fail to match human preferences,\nand we assess alignment robustness by analyzing RM responses to perturbed\ninputs. Our experiments, utilizing open-source components like the\nAnthropic/hh-rlhf preference dataset and OpenAssistant RMs, reveal significant\nimprints of target features and a notable sensitivity to spoiler features. We\nobserved a 26% incidence of alignment resistance in portions of the dataset\nwhere LM-labelers disagreed with human preferences. Furthermore, we find that\nmisalignment often arises from ambiguous entries within the alignment dataset.\nThese findings underscore the importance of scrutinizing both RMs and alignment\ndatasets for a deeper understanding of value alignment.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}