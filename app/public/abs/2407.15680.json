{"id":"2407.15680","title":"HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal\n  Reasoning","authors":"Zhecan Wang, Garrett Bingham, Adams Yu, Quoc Le, Thang Luong, Golnaz\n  Ghiasi","authorsParsed":[["Wang","Zhecan",""],["Bingham","Garrett",""],["Yu","Adams",""],["Le","Quoc",""],["Luong","Thang",""],["Ghiasi","Golnaz",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 14:49:51 GMT"}],"updateDate":"2024-07-23","timestamp":1721659791000,"abstract":"  Hallucination has been a major problem for large language models and remains\na critical challenge when it comes to multimodality in which vision-language\nmodels (VLMs) have to deal with not just textual but also visual inputs.\nDespite rapid progress in VLMs, resources for evaluating and addressing\nmultimodal hallucination are limited and mostly focused on evaluation. This\nwork introduces HaloQuest, a novel visual question answering dataset that\ncaptures various aspects of multimodal hallucination such as false premises,\ninsufficient contexts, and visual challenges. A novel idea from HaloQuest is to\nleverage synthetic images, apart from real ones, to enable dataset creation at\nscale. With over 7.7K examples spanning across a wide variety of categories,\nHaloQuest was designed to be both a challenging benchmark for VLMs and a\nfine-tuning dataset for advancing multimodal reasoning. Our experiments reveal\nthat current models struggle with HaloQuest, with all open-source VLMs\nachieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest\nsignificantly reduces hallucination rates while preserving performance on\nstandard reasoning tasks. Our results discover that benchmarking with generated\nimages is highly correlated (r=0.97) with real images. Last but not least, we\npropose a novel Auto-Eval mechanism that is highly correlated with human raters\n(r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards\nunderstanding, evaluating, and mitigating hallucination in VLMs, serving as an\nimportant step towards more reliable multimodal AI systems in the future.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TDTAhEmXaXIESm6falHu-ur-DB0BeEuyCPCKAqFmJg8","pdfSize":"4446442"}
