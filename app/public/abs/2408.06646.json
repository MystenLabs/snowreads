{"id":"2408.06646","title":"Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion\n  Models","authors":"Chenqian Yan, Songwei Liu, Hongjian Liu, Xurui Peng, Xiaojian Wang,\n  Fangming Chen, Lean Fu, Xing Mei","authorsParsed":[["Yan","Chenqian",""],["Liu","Songwei",""],["Liu","Hongjian",""],["Peng","Xurui",""],["Wang","Xiaojian",""],["Chen","Fangming",""],["Fu","Lean",""],["Mei","Xing",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 05:30:41 GMT"}],"updateDate":"2024-08-14","timestamp":1723527041000,"abstract":"  Stable Diffusion Models (SDMs) have shown remarkable proficiency in image\nsynthesis. However, their broad application is impeded by their large model\nsizes and intensive computational requirements, which typically require\nexpensive cloud servers for deployment. On the flip side, while there are many\ncompact models tailored for edge devices that can reduce these demands, they\noften compromise on semantic integrity and visual quality when compared to\nfull-sized SDMs. To bridge this gap, we introduce Hybrid SD, an innovative,\ntraining-free SDMs inference framework designed for edge-cloud collaborative\ninference. Hybrid SD distributes the early steps of the diffusion process to\nthe large models deployed on cloud servers, enhancing semantic planning.\nFurthermore, small efficient models deployed on edge devices can be integrated\nfor refining visual details in the later stages. Acknowledging the diversity of\nedge devices with differing computational and storage capacities, we employ\nstructural pruning to the SDMs U-Net and train a lightweight VAE. Empirical\nevaluations demonstrate that our compressed models achieve state-of-the-art\nparameter efficiency (225.8M) on edge devices with competitive image quality.\nAdditionally, Hybrid SD reduces the cloud cost by 66% with edge-cloud\ncollaborative inference.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}