{"id":"2408.00741","title":"DynamoLLM: Designing LLM Inference Clusters for Performance and Energy\n  Efficiency","authors":"Jovan Stojkovic and Chaojie Zhang and \\'I\\~nigo Goiri and Josep\n  Torrellas and Esha Choukse","authorsParsed":[["Stojkovic","Jovan",""],["Zhang","Chaojie",""],["Goiri","Íñigo",""],["Torrellas","Josep",""],["Choukse","Esha",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:40:45 GMT"}],"updateDate":"2024-08-02","timestamp":1722534045000,"abstract":"  The rapid evolution and widespread adoption of generative large language\nmodels (LLMs) have made them a pivotal workload in various applications. Today,\nLLM inference clusters receive a large number of queries with strict Service\nLevel Objectives (SLOs). To achieve the desired performance, these models\nexecute on power-hungry GPUs causing the inference clusters to consume large\namount of energy and, consequently, result in excessive carbon emissions.\nFortunately, we find that there is a great opportunity to exploit the\nheterogeneity in inference compute properties and fluctuations in inference\nworkloads, to significantly improve energy-efficiency. However, such a diverse\nand dynamic environment creates a large search-space where different system\nconfigurations (e.g., number of instances, model parallelism, and GPU\nfrequency) translate into different energy-performance trade-offs. To address\nthese challenges, we propose DynamoLLM, the first energy-management framework\nfor LLM inference environments. DynamoLLM automatically and dynamically\nreconfigures the inference cluster to optimize for energy and cost of LLM\nserving under the service's performance SLOs. We show that at a service-level,\nDynamoLLM conserves 53% energy and 38% operational carbon emissions, and\nreduces 61% cost to the customer, while meeting the latency SLOs.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Hardware Architecture","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"C_tpL2y0s1xO6kVzrD6BZ4Cxc8fju4vkb7-Z3iKLGyQ","pdfSize":"900173","txDigest":"3AJDYiujfESMmvMgcJPF2N6wB3kwoCoVbQydyDckVFEg","endEpoch":"1","status":"CERTIFIED"}
