{"id":"2408.01417","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","authors":"Yilun Hua and Yoav Artzi","authorsParsed":[["Hua","Yilun",""],["Artzi","Yoav",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 17:51:57 GMT"}],"updateDate":"2024-08-05","timestamp":1722621117000,"abstract":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}