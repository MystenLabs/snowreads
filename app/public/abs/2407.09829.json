{"id":"2407.09829","title":"VLMPC: Vision-Language Model Predictive Control for Robotic Manipulation","authors":"Wentao Zhao, Jiaming Chen, Ziyu Meng, Donghui Mao, Ran Song, Wei Zhang","authorsParsed":[["Zhao","Wentao",""],["Chen","Jiaming",""],["Meng","Ziyu",""],["Mao","Donghui",""],["Song","Ran",""],["Zhang","Wei",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 09:42:02 GMT"}],"updateDate":"2024-07-16","timestamp":1720863722000,"abstract":"  Although Model Predictive Control (MPC) can effectively predict the future\nstates of a system and thus is widely used in robotic manipulation tasks, it\ndoes not have the capability of environmental perception, leading to the\nfailure in some complex scenarios. To address this issue, we introduce\nVision-Language Model Predictive Control (VLMPC), a robotic manipulation\nframework which takes advantage of the powerful perception capability of vision\nlanguage model (VLM) and integrates it with MPC. Specifically, we propose a\nconditional action sampling module which takes as input a goal image or a\nlanguage instruction and leverages VLM to sample a set of candidate action\nsequences. Then, a lightweight action-conditioned video prediction model is\ndesigned to generate a set of future frames conditioned on the candidate action\nsequences. VLMPC produces the optimal action sequence with the assistance of\nVLM through a hierarchical cost function that formulates both pixel-level and\nknowledge-level consistence between the current observation and the goal image.\nWe demonstrate that VLMPC outperforms the state-of-the-art methods on public\nbenchmarks. More importantly, our method showcases excellent performance in\nvarious real-world tasks of robotic manipulation. Code is available\nat~\\url{https://github.com/PPjmchen/VLMPC}.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}