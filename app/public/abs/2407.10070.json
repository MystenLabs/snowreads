{"id":"2407.10070","title":"Have ASkotch: Fast Methods for Large-scale, Memory-constrained Kernel\n  Ridge Regression","authors":"Pratik Rathore, Zachary Frangella, Madeleine Udell","authorsParsed":[["Rathore","Pratik",""],["Frangella","Zachary",""],["Udell","Madeleine",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 04:11:10 GMT"}],"updateDate":"2024-07-16","timestamp":1720930270000,"abstract":"  Kernel ridge regression (KRR) is a fundamental computational tool, appearing\nin problems that range from computational chemistry to health analytics, with a\nparticular interest due to its starring role in Gaussian process regression.\nHowever, it is challenging to scale KRR solvers to large datasets: with $n$\ntraining points, a direct solver (i.e., Cholesky decomposition) uses $O(n^2)$\nstorage and $O(n^3)$ flops. Iterative methods for KRR, such as preconditioned\nconjugate gradient (PCG), avoid the cubic scaling of direct solvers and often\nuse low-rank preconditioners; a rank $r$ preconditioner uses $O(rn)$ storage\nand each iteration requires $O(n^2)$ flops. To reduce the storage and iteration\ncomplexity of iterative solvers for KRR, we propose ASkotch\n($\\textbf{A}$ccelerated $\\textbf{s}$calable $\\textbf{k}$ernel\n$\\textbf{o}$p$\\textbf{t}$imization using block $\\textbf{c}$oordinate descent\nwith $\\textbf{H}$essian preconditioning). For a given block size $|b| << n$,\neach iteration of ASkotch uses $O(r|b| + n)$ storage and $O(n|b|)$ flops, so\nASkotch scales better than Cholesky decomposition and PCG. We prove that\nASkotch obtains linear convergence to the optimum, with the convergence rate\ndepending on the square roots of the $\\textit{preconditioned}$ block condition\nnumbers. Furthermore, we solve KRR problems that were considered to be\nimpossibly large while using limited computational resources: we show that\nASkotch outperforms PCG methods with respect to generalization error on\nlarge-scale KRR (up to $n = 10^8$) and KRR classification tasks (up to $n =\n10^7$) while running each of our experiments on $\\textit{a single 12 GB Titan V\nGPU}$. Our work opens up the possibility of as-yet-unimagined applications of\nKRR across a wide range of disciplines.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}