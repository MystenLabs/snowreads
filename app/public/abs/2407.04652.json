{"id":"2407.04652","title":"Pretraining End-to-End Keyword Search with Automatically Discovered\n  Acoustic Units","authors":"Bolaji Yusuf, Jan \"Honza\" \\v{C}ernock\\'y, Murat Sara\\c{c}lar","authorsParsed":[["Yusuf","Bolaji",""],["Černocký","Jan \"Honza\"",""],["Saraçlar","Murat",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 17:07:58 GMT"}],"updateDate":"2024-07-08","timestamp":1720199278000,"abstract":"  End-to-end (E2E) keyword search (KWS) has emerged as an alternative and\ncomplimentary approach to conventional keyword search which depends on the\noutput of automatic speech recognition (ASR) systems. While E2E methods greatly\nsimplify the KWS pipeline, they generally have worse performance than their\nASR-based counterparts, which can benefit from pretraining with untranscribed\ndata. In this work, we propose a method for pretraining E2E KWS systems with\nuntranscribed data, which involves using acoustic unit discovery (AUD) to\nobtain discrete units for untranscribed data and then learning to locate\nsequences of such units in the speech. We conduct experiments across languages\nand AUD systems: we show that finetuning such a model significantly outperforms\na model trained from scratch, and the performance improvements are generally\ncorrelated with the quality of the AUD system used for pretraining.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}