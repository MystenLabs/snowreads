{"id":"2408.13233","title":"Multi-Layer Transformers Gradient Can be Approximated in Almost Linear\n  Time","authors":"Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Yufa Zhou","authorsParsed":[["Liang","Yingyu",""],["Sha","Zhizhou",""],["Shi","Zhenmei",""],["Song","Zhao",""],["Zhou","Yufa",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 17:16:43 GMT"}],"updateDate":"2024-08-26","timestamp":1724433403000,"abstract":"  The quadratic computational complexity in the self-attention mechanism of\npopular transformer architectures poses significant challenges for training and\ninference, particularly in terms of efficiency and memory requirements. Towards\naddressing these challenges, this paper introduces a novel fast computation\nmethod for gradient calculation in multi-layer transformer models. Our approach\nenables the computation of gradients for the entire multi-layer transformer\nmodel in almost linear time $n^{1+o(1)}$, where $n$ is the input sequence\nlength. This breakthrough significantly reduces the computational bottleneck\nassociated with the traditional quadratic time complexity. Our theory holds for\nany loss function and maintains a bounded approximation error across the entire\nmodel. Furthermore, our analysis can hold when the multi-layer transformer\nmodel contains many practical sub-modules, such as residual connection, casual\nmask, and multi-head attention. By improving the efficiency of gradient\ncomputation in large language models, we hope that our work will facilitate the\nmore effective training and deployment of long-context language models based on\nour theoretical results.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AyNpf8NNpsPweY0pj02B2MBKpsPX4iNFr5JnsiCFEVs","pdfSize":"793068"}
