{"id":"2407.01646","title":"ESALE: Enhancing Code-Summary Alignment Learning for Source Code\n  Summarization","authors":"Chunrong Fang and Weisong Sun and Yuchen Chen and Xiao Chen and Zhao\n  Wei and Quanjun Zhang and Yudu You and Bin Luo and Yang Liu and Zhenyu Chen","authorsParsed":[["Fang","Chunrong",""],["Sun","Weisong",""],["Chen","Yuchen",""],["Chen","Xiao",""],["Wei","Zhao",""],["Zhang","Quanjun",""],["You","Yudu",""],["Luo","Bin",""],["Liu","Yang",""],["Chen","Zhenyu",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 03:06:51 GMT"}],"updateDate":"2024-07-03","timestamp":1719803211000,"abstract":"  (Source) code summarization aims to automatically generate succinct natural\nlanguage summaries for given code snippets. Such summaries play a significant\nrole in promoting developers to understand and maintain code. Inspired by\nneural machine translation, deep learning-based code summarization techniques\nwidely adopt an encoder-decoder framework, where the encoder transforms given\ncode snippets into context vectors, and the decoder decodes context vectors\ninto summaries. Recently, large-scale pre-trained models for source code are\nequipped with encoders capable of producing general context vectors and have\nachieved substantial improvements on code summarization. However, although they\nare usually trained mainly on code-focused tasks and can capture general code\nfeatures, they still fall short in capturing specific features that need to be\nsummarized.\n  This paper proposes a novel approach to improve code summarization based on\nsummary-focused tasks. Specifically, we exploit a multi-task learning paradigm\nto train the encoder on three summary-focused tasks to enhance its ability to\nlearn code-summary alignment, including unidirectional language modeling (ULM),\nmasked language modeling (MLM), and action word prediction (AWP). Unlike\npre-trained models that mainly predict masked tokens in code snippets, we\ndesign ULM and MLM to predict masked words in summaries. Intuitively,\npredicting words based on given code snippets would help learn the code-summary\nalignment. Additionally, we introduce the domain-specific task AWP to enhance\nthe ability of the encoder to learn the alignment between action words and code\nsnippets. The extensive experiments on four datasets demonstrate that our\napproach, called ESALE significantly outperforms baselines in all three widely\nused metrics, including BLEU, METEOR, and ROUGE-L.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}