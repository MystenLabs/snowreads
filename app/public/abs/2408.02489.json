{"id":"2408.02489","title":"Full error analysis of policy gradient learning algorithms for\n  exploratory linear quadratic mean-field control problem in continuous time\n  with common noise","authors":"Noufel Frikha (CES), Huy\\^en Pham (LPSM (UMR\\_8001)), Xuanye Song\n  (LPSM (UMR\\_8001))","authorsParsed":[["Frikha","Noufel","","CES"],["Pham","HuyÃªn","","LPSM"],["Song","Xuanye","","LPSM"]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 14:11:51 GMT"}],"updateDate":"2024-08-06","timestamp":1722867111000,"abstract":"  We consider reinforcement learning (RL) methods for finding optimal policies\nin linear quadratic (LQ) mean field control (MFC) problems over an infinite\nhorizon in continuous time, with common noise and entropy regularization. We\nstudy policy gradient (PG) learning and first demonstrate convergence in a\nmodel-based setting by establishing a suitable gradient domination\ncondition.Next, our main contribution is a comprehensive error analysis, where\nwe prove the global linear convergence and sample complexity of the PG\nalgorithm with two-point gradient estimates in a model-free setting with\nunknown parameters. In this setting, the parameterized optimal policies are\nlearned from samples of the states and population distribution.Finally, we\nprovide numerical evidence supporting the convergence of our implemented\nalgorithms.\n","subjects":["Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}