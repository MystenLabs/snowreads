{"id":"2408.10658","title":"Learning Instruction-Guided Manipulation Affordance via Large Models for\n  Embodied Robotic Tasks","authors":"Dayou Li, Chenkun Zhao, Shuo Yang, Lin Ma, Yibin Li, and Wei Zhang","authorsParsed":[["Li","Dayou",""],["Zhao","Chenkun",""],["Yang","Shuo",""],["Ma","Lin",""],["Li","Yibin",""],["Zhang","Wei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 08:54:34 GMT"}],"updateDate":"2024-08-27","timestamp":1724144074000,"abstract":"  We study the task of language instruction-guided robotic manipulation, in\nwhich an embodied robot is supposed to manipulate the target objects based on\nthe language instructions. In previous studies, the predicted manipulation\nregions of the target object typically do not change with specification from\nthe language instructions, which means that the language perception and\nmanipulation prediction are separate. However, in human behavioral patterns,\nthe manipulation regions of the same object will change for different language\ninstructions. In this paper, we propose Instruction-Guided Affordance Net\n(IGANet) for predicting affordance maps of instruction-guided robotic\nmanipulation tasks by utilizing powerful priors from vision and language\nencoders pre-trained on large-scale datasets. We develop a\nVison-Language-Models(VLMs)-based data augmentation pipeline, which can\ngenerate a large amount of data automatically for model training. Besides, with\nthe help of Large-Language-Models(LLMs), actions can be effectively executed to\nfinish the tasks defined by instructions. A series of real-world experiments\nrevealed that our method can achieve better performance with generated data.\nMoreover, our model can generalize better to scenarios with unseen objects and\nlanguage instructions.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}