{"id":"2408.16756","title":"How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of\n  Large Language Models","authors":"Jiyue Jiang, Liheng Chen, Pengan Chen, Sheng Wang, Qinghang Bao,\n  Lingpeng Kong, Yu Li, Chuan Wu","authorsParsed":[["Jiang","Jiyue",""],["Chen","Liheng",""],["Chen","Pengan",""],["Wang","Sheng",""],["Bao","Qinghang",""],["Kong","Lingpeng",""],["Li","Yu",""],["Wu","Chuan",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 17:54:14 GMT"}],"updateDate":"2024-08-30","timestamp":1724954054000,"abstract":"  The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}