{"id":"2408.03626","title":"On the choice of the non-trainable internal weights in random feature\n  maps","authors":"Pinak Mandal, Georg A. Gottwald","authorsParsed":[["Mandal","Pinak",""],["Gottwald","Georg A.",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 08:37:23 GMT"}],"updateDate":"2024-08-08","timestamp":1723019843000,"abstract":"  The computationally cheap machine learning architecture of random feature\nmaps can be viewed as a single-layer feedforward network in which the weights\nof the hidden layer are random but fixed and only the outer weights are learned\nvia linear regression. The internal weights are typically chosen from a\nprescribed distribution. The choice of the internal weights significantly\nimpacts the accuracy of random feature maps. We address here the task of how to\nbest select the internal weights. In particular, we consider the forecasting\nproblem whereby random feature maps are used to learn a one-step propagator map\nfor a dynamical system. We provide a computationally cheap hit-and-run\nalgorithm to select good internal weights which lead to good forecasting skill.\nWe show that the number of good features is the main factor controlling the\nforecasting skill of random feature maps and acts as an effective feature\ndimension. Lastly, we compare random feature maps with single-layer feedforward\nneural networks in which the internal weights are now learned using gradient\ndescent. We find that random feature maps have superior forecasting\ncapabilities whilst having several orders of magnitude lower computational\ncost.\n","subjects":["Computing Research Repository/Machine Learning","Physics/Data Analysis, Statistics and Probability","Statistics/Methodology","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}