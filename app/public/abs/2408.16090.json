{"id":"2408.16090","title":"EPO: Hierarchical LLM Agents with Environment Preference Optimization","authors":"Qi Zhao, Haotian Fu, Chen Sun, George Konidaris","authorsParsed":[["Zhao","Qi",""],["Fu","Haotian",""],["Sun","Chen",""],["Konidaris","George",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 18:44:02 GMT"}],"updateDate":"2024-08-30","timestamp":1724870642000,"abstract":"  Long-horizon decision-making tasks present significant challenges for\nLLM-based agents due to the need for extensive planning over multiple steps. In\nthis paper, we propose a hierarchical framework that decomposes complex tasks\ninto manageable subgoals, utilizing separate LLMs for subgoal prediction and\nlow-level action generation. To address the challenge of creating training\nsignals for unannotated datasets, we develop a reward model that leverages\nmultimodal environment feedback to automatically generate reward signals. We\nintroduce Environment Preference Optimization (EPO), a novel method that\ngenerates preference signals from the environment's feedback and uses them to\ntrain LLM-based agents. Extensive experiments on ALFRED demonstrate the\nstate-of-the-art performance of our framework, achieving first place on the\nALFRED public leaderboard and showcasing its potential to improve long-horizon\ndecision-making in diverse environments.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}