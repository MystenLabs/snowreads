{"id":"2407.04873","title":"Evaluating Language Models for Generating and Judging Programming\n  Feedback","authors":"Charles Koutcheme, Nicola Dainese, Arto Hellas, Sami Sarsa, Juho\n  Leinonen, Syed Ashraf, Paul Denny","authorsParsed":[["Koutcheme","Charles",""],["Dainese","Nicola",""],["Hellas","Arto",""],["Sarsa","Sami",""],["Leinonen","Juho",""],["Ashraf","Syed",""],["Denny","Paul",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 21:44:11 GMT"}],"updateDate":"2024-07-09","timestamp":1720215851000,"abstract":"  The emergence of large language models (LLMs) has transformed research and\npractice in a wide range of domains. Within the computing education research\n(CER) domain, LLMs have received plenty of attention especially in the context\nof learning programming. Much of the work on LLMs in CER has however focused on\napplying and evaluating proprietary models. In this article, we evaluate the\nefficiency of open-source LLMs in generating high-quality feedback for\nprogramming assignments, and in judging the quality of the programming\nfeedback, contrasting the results against proprietary models. Our evaluations\non a dataset of students' submissions to Python introductory programming\nexercises suggest that the state-of-the-art open-source LLMs (Meta's Llama3)\nare almost on-par with proprietary models (GPT-4o) in both the generation and\nassessment of programming feedback. We further demonstrate the efficiency of\nsmaller LLMs in the tasks, and highlight that there are a wide range of LLMs\nthat are accessible even for free for educators and practitioners.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/"}