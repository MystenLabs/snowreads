{"id":"2408.08454","title":"Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention","authors":"Zohaib Khan, Muhammad Khaquan, Omer Tafveez, Burhanuddin Samiwala,\n  Agha Ali Raza","authorsParsed":[["Khan","Zohaib",""],["Khaquan","Muhammad",""],["Tafveez","Omer",""],["Samiwala","Burhanuddin",""],["Raza","Agha Ali",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 23:34:04 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 08:31:28 GMT"}],"updateDate":"2024-08-29","timestamp":1723764844000,"abstract":"  The Transformer architecture has revolutionized deep learning through its\nSelf-Attention mechanism, which effectively captures contextual information.\nHowever, the memory footprint of Self-Attention presents significant challenges\nfor long-sequence tasks. Grouped Query Attention (GQA) addresses this issue by\ngrouping queries and mean-pooling the corresponding key-value heads - reducing\nthe number of overall parameters and memory requirements in a flexible manner\nwithout adversely compromising model accuracy. In this work, we introduce\nenhancements to GQA, focusing on two novel approaches that deviate from the\nstatic nature of grouping: Key-Distributed GQA (KDGQA) and Dynamic\nKey-Distributed GQA (DGQA), which leverage information from the norms of the\nkey heads to inform query allocation. Specifically, KDGQA looks at the ratios\nof the norms of the key heads during each forward pass, while DGQA examines the\nratios of the norms as they evolve through training. Additionally, we present\nPerturbed GQA (PGQA) as a case-study, which introduces variability in (static)\ngroup formation via subtracting noise from the attention maps. Our experiments\nwith up-trained Vision Transformers, for Image Classification on datasets such\nas CIFAR-10, CIFAR-100, Food101, and Tiny ImageNet, demonstrate the promise of\nthese variants in improving upon the original GQA through more informed and\nadaptive grouping mechanisms: specifically ViT-L experiences accuracy gains of\nup to 8% when utilizing DGQA in comparison to GQA and other variants. We\nfurther analyze the impact of the number of Key-Value Heads on performance,\nunderscoring the importance of utilizing query-key affinities. Code is\navailable on GitHub.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"x1k03_52pIvAKaysXnaM73ePwHus_pjR4RnqoMCEO-0","pdfSize":"1063809"}
