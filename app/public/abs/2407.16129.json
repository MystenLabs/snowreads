{"id":"2407.16129","title":"FoRA: Low-Rank Adaptation Model beyond Multimodal Siamese Network","authors":"Weiying Xie and Yusi Zhang and Tianlin Hui and Jiaqing Zhang and Jie\n  Lei and Yunsong Li","authorsParsed":[["Xie","Weiying",""],["Zhang","Yusi",""],["Hui","Tianlin",""],["Zhang","Jiaqing",""],["Lei","Jie",""],["Li","Yunsong",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 02:27:52 GMT"}],"updateDate":"2024-07-24","timestamp":1721701672000,"abstract":"  Multimodal object detection offers a promising prospect to facilitate robust\ndetection in various visual conditions. However, existing two-stream backbone\nnetworks are challenged by complex fusion and substantial parameter increments.\nThis is primarily due to large data distribution biases of multimodal\nhomogeneous information. In this paper, we propose a novel multimodal object\ndetector, named Low-rank Modal Adaptors (LMA) with a shared backbone. The\nshared parameters enhance the consistency of homogeneous information, while\nlightweight modal adaptors focus on modality unique features. Furthermore, we\ndesign an adaptive rank allocation strategy to adapt to the varying\nheterogeneity at different feature levels. When applied to two multimodal\nobject detection datasets, experiments validate the effectiveness of our\nmethod. Notably, on DroneVehicle, LMA attains a 10.4% accuracy improvement over\nthe state-of-the-art method with a 149M-parameters reduction. The code is\navailable at https://github.com/zyszxhy/FoRA.\n  Our work was submitted to ACM MM in April 2024, but was rejected. We will\ncontinue to refine our work and paper writing next, mainly including proof of\ntheory and multi-task applications of FoRA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}