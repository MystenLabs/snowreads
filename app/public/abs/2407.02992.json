{"id":"2407.02992","title":"Scientific Text Analysis with Robots applied to observatory proposals","authors":"T. Jerabkova, H.M.J. Boffin, F. Patat, D. Dorigo, F. Sogni, and F.\n  Primas","authorsParsed":[["Jerabkova","T.",""],["Boffin","H. M. J.",""],["Patat","F.",""],["Dorigo","D.",""],["Sogni","F.",""],["Primas","F.",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 10:44:15 GMT"}],"updateDate":"2024-07-04","timestamp":1720003455000,"abstract":"  To test the potential disruptive effect of Artificial Intelligence (AI)\ntransformers (e.g., ChatGPT) and their associated Large Language Models on the\ntime allocation process, both in proposal reviewing and grading, an experiment\nhas been set-up at ESO for the P112 Call for Proposals. The experiment aims at\nraising awareness in the ESO community and build valuable knowledge by\nidentifying what future steps ESO and other observatories might need to take to\nstay up to date with current technologies. We present here the results of the\nexperiment, which may further be used to inform decision-makers regarding the\nuse of AI in the proposal review process. We find that the ChatGPT-adjusted\nproposals tend to receive lower grades compared to the original proposals.\nMoreover, ChatGPT 3.5 can generally not be trusted in providing correct\nscientific references, while the most recent version makes a better, but far\nfrom perfect, job. We also studied how ChatGPT deals with assessing proposals.\nIt does an apparent remarkable job at providing a summary of ESO proposals,\nalthough it doesn't do so good to identify weaknesses. When looking at how it\nevaluates proposals, however, it appears that ChatGPT systematically gives a\nhigher mark than humans, and tends to prefer proposals written by itself.\n","subjects":["Astrophysics/Instrumentation and Methods for Astrophysics","Physics/Physics and Society"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}