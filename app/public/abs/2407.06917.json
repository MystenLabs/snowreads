{"id":"2407.06917","title":"Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models","authors":"Zara Siddique, Liam D. Turner, Luis Espinosa-Anke","authorsParsed":[["Siddique","Zara",""],["Turner","Liam D.",""],["Espinosa-Anke","Luis",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 14:52:52 GMT"}],"updateDate":"2024-07-10","timestamp":1720536772000,"abstract":"  Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/"}