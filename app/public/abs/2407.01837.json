{"id":"2407.01837","title":"To Switch or Not to Switch? Balanced Policy Switching in Offline\n  Reinforcement Learning","authors":"Tao Ma, Xuzhi Yang, Zoltan Szabo","authorsParsed":[["Ma","Tao",""],["Yang","Xuzhi",""],["Szabo","Zoltan",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 22:24:31 GMT"}],"updateDate":"2024-07-03","timestamp":1719872671000,"abstract":"  Reinforcement learning (RL) -- finding the optimal behaviour (also referred\nto as policy) maximizing the collected long-term cumulative reward -- is among\nthe most influential approaches in machine learning with a large number of\nsuccessful applications. In several decision problems, however, one faces the\npossibility of policy switching -- changing from the current policy to a new\none -- which incurs a non-negligible cost (examples include the shifting of the\ncurrently applied educational technology, modernization of a computing cluster,\nand the introduction of a new webpage design), and in the decision one is\nlimited to using historical data without the availability for further online\ninteraction. Despite the inevitable importance of this offline learning\nscenario, to our best knowledge, very little effort has been made to tackle the\nkey problem of balancing between the gain and the cost of switching in a\nflexible and principled way. Leveraging ideas from the area of optimal\ntransport, we initialize the systematic study of policy switching in offline\nRL. We establish fundamental properties and design a Net Actor-Critic algorithm\nfor the proposed novel switching formulation. Numerical experiments demonstrate\nthe efficiency of our approach on multiple benchmarks of the Gymnasium.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Information Theory","Computing Research Repository/Machine Learning","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}