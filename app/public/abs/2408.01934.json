{"id":"2408.01934","title":"A Survey and Evaluation of Adversarial Attacks for Object Detection","authors":"Khoi Nguyen Tiet Nguyen, Wenyu Zhang, Kangkang Lu, Yuhuan Wu, Xingjian\n  Zheng, Hui Li Tan, Liangli Zhen","authorsParsed":[["Nguyen","Khoi Nguyen Tiet",""],["Zhang","Wenyu",""],["Lu","Kangkang",""],["Wu","Yuhuan",""],["Zheng","Xingjian",""],["Tan","Hui Li",""],["Zhen","Liangli",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 05:22:08 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 02:39:46 GMT"}],"updateDate":"2024-08-07","timestamp":1722748928000,"abstract":"  Deep learning models excel in various computer vision tasks but are\nsusceptible to adversarial examples-subtle perturbations in input data that\nlead to incorrect predictions. This vulnerability poses significant risks in\nsafety-critical applications such as autonomous vehicles, security\nsurveillance, and aircraft health monitoring. While numerous surveys focus on\nadversarial attacks in image classification, the literature on such attacks in\nobject detection is limited. This paper offers a comprehensive taxonomy of\nadversarial attacks specific to object detection, reviews existing adversarial\nrobustness evaluation metrics, and systematically assesses open-source attack\nmethods and model robustness. Key observations are provided to enhance the\nunderstanding of attack effectiveness and corresponding countermeasures.\nAdditionally, we identify crucial research challenges to guide future efforts\nin securing automated object detection systems.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}