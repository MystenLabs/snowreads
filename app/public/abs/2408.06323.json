{"id":"2408.06323","title":"Infer-and-widen versus split-and-condition: two tales of selective\n  inference","authors":"Ronan Perry, Zichun Xu, Olivia McGough, Daniela Witten","authorsParsed":[["Perry","Ronan",""],["Xu","Zichun",""],["McGough","Olivia",""],["Witten","Daniela",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 17:43:10 GMT"}],"updateDate":"2024-08-13","timestamp":1723484590000,"abstract":"  Recent attention has focused on the development of methods for post-selection\ninference. However, the connections between these methods, and the extent to\nwhich one might be preferred to another, remain unclear. In this paper, we\nclassify existing methods for post-selection inference into one of two\nframeworks: infer-and-widen or split-and-condition. The infer-and-widen\nframework produces confidence intervals whose midpoints are biased due to\nselection, and must be wide enough to account for this bias. By contrast,\nsplit-and-condition directly adjusts the intervals' midpoints to account for\nselection. We compare the two frameworks in three vignettes: the winner's\ncurse, maximal contrasts, and inference after the lasso. Our results are\nstriking: in each of these examples, a split-and-condition strategy leads to\nconfidence intervals that are much narrower than the state-of-the-art\ninfer-and-widen proposal, when methods are tuned to yield identical selection\nevents. Furthermore, even an ``oracle\" infer-and-widen confidence interval --\nthe narrowest possible interval that could be theoretically attained via\ninfer-and-widen -- is not necessarily narrower than a feasible\nsplit-and-condition method. Taken together, these results point to\nsplit-and-condition as the most promising framework for post-selection\ninference in real-world settings.\n","subjects":["Statistics/Methodology"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ofebkMWr5NTsz4sJ4wfFkCV362QgzwmsujY958cv-xM","pdfSize":"1125679"}
