{"id":"2408.16429","title":"Gradient-free variational learning with conditional mixture networks","authors":"Conor Heins, Hao Wu, Dimitrije Markovic, Alexander Tschantz, Jeff\n  Beck, Christopher Buckley","authorsParsed":[["Heins","Conor",""],["Wu","Hao",""],["Markovic","Dimitrije",""],["Tschantz","Alexander",""],["Beck","Jeff",""],["Buckley","Christopher",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 10:43:55 GMT"}],"updateDate":"2024-08-30","timestamp":1724928235000,"abstract":"  Balancing computational efficiency with robust predictive performance is\ncrucial in supervised learning, especially for critical applications. Standard\ndeep learning models, while accurate and scalable, often lack probabilistic\nfeatures like calibrated predictions and uncertainty quantification. Bayesian\nmethods address these issues but can be computationally expensive as model and\ndata complexity increase. Previous work shows that fast variational methods can\nreduce the compute requirements of Bayesian methods by eliminating the need for\ngradient computation or sampling, but are often limited to simple models. We\ndemonstrate that conditional mixture networks (CMNs), a probabilistic variant\nof the mixture-of-experts (MoE) model, are suitable for fast, gradient-free\ninference and can solve complex classification tasks. CMNs employ linear\nexperts and a softmax gating network. By exploiting conditional conjugacy and\nP\\'olya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of\nboth the linear experts and the gating network. This enables efficient\nvariational updates using coordinate ascent variational inference (CAVI),\navoiding traditional gradient-based optimization. We validate this approach by\ntraining two-layer CMNs on standard benchmarks from the UCI repository. Our\nmethod, CAVI-CMN, achieves competitive and often superior predictive accuracy\ncompared to maximum likelihood estimation (MLE) with backpropagation, while\nmaintaining competitive runtime and full posterior distributions over all model\nparameters. Moreover, as input size or the number of experts increases,\ncomputation time scales competitively with MLE and other gradient-based\nsolutions like black-box variational inference (BBVI), making CAVI-CMN a\npromising tool for deep, fast, and gradient-free Bayesian networks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}