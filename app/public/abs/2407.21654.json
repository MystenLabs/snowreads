{"id":"2407.21654","title":"MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment","authors":"Anurag Das, Xinting Hu, Li Jiang, Bernt Schiele","authorsParsed":[["Das","Anurag",""],["Hu","Xinting",""],["Jiang","Li",""],["Schiele","Bernt",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 14:56:42 GMT"}],"updateDate":"2024-08-01","timestamp":1722437802000,"abstract":"  Recent approaches have shown that large-scale vision-language models such as\nCLIP can improve semantic segmentation performance. These methods typically aim\nfor pixel-level vision-language alignment, but often rely on low resolution\nimage features from CLIP, resulting in class ambiguities along boundaries.\nMoreover, the global scene representations in CLIP text embeddings do not\ndirectly correlate with the local and detailed pixel-level features, making\nmeaningful alignment more difficult. To address these limitations, we introduce\nMTA-CLIP, a novel framework employing mask-level vision-language alignment.\nSpecifically, we first propose Mask-Text Decoder that enhances the mask\nrepresentations using rich textual data with the CLIP language model.\nSubsequently, it aligns mask representations with text embeddings using\nMask-to-Text Contrastive Learning. Furthermore, we introduce MaskText Prompt\nLearning, utilizing multiple context-specific prompts for text embeddings to\ncapture diverse class representations across masks. Overall, MTA-CLIP achieves\nstate-of-the-art, surpassing prior works by an average of 2.8% and 1.3% on on\nstandard benchmark datasets, ADE20k and Cityscapes, respectively.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}