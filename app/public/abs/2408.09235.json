{"id":"2408.09235","title":"Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of\n  Free-Form Text","authors":"Sher Badshah, Hassan Sajjad","authorsParsed":[["Badshah","Sher",""],["Sajjad","Hassan",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 16:01:45 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 15:12:08 GMT"}],"updateDate":"2024-08-21","timestamp":1723910505000,"abstract":"  The emergence of Large Language Models (LLMs) as chat assistants capable of\ngenerating human-like conversations has amplified the need for robust\nevaluation methods, particularly for open-ended tasks. Conventional metrics\nlike BLEU and ROUGE, while useful, are increasingly inadequate for capturing\nthe subtle semantics and contextual richness of such generative outputs. We\npropose a reference-guided verdict method that automates the evaluation process\nby leveraging multiple LLMs-as-judges. Through experiments on three open-ended\nquestion-answering tasks, we demonstrate that combining multiple LLMs-as-judges\nsignificantly improves the reliability and accuracy of evaluations,\nparticularly in complex tasks where a single model might struggle. Our findings\nreveal a strong correlation with human evaluations, establishing our method as\na viable and effective alternative to traditional metrics and human judgments,\nparticularly in the context of LLM-based chat assistants where the complexity\nand diversity of responses challenge existing benchmarks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}