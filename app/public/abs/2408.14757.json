{"id":"2408.14757","title":"Learning effective pruning at initialization from iterative pruning","authors":"Shengkai Liu, Yaofeng Cheng, Fusheng Zha, Wei Guo, Lining Sun,\n  Zhenshan Bing, Chenguang Yang","authorsParsed":[["Liu","Shengkai",""],["Cheng","Yaofeng",""],["Zha","Fusheng",""],["Guo","Wei",""],["Sun","Lining",""],["Bing","Zhenshan",""],["Yang","Chenguang",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 03:17:52 GMT"}],"updateDate":"2024-08-28","timestamp":1724728672000,"abstract":"  Pruning at initialization (PaI) reduces training costs by removing weights\nbefore training, which becomes increasingly crucial with the growing network\nsize. However, current PaI methods still have a large accuracy gap with\niterative pruning, especially at high sparsity levels. This raises an\nintriguing question: can we get inspiration from iterative pruning to improve\nthe PaI performance? In the lottery ticket hypothesis, the iterative rewind\npruning (IRP) finds subnetworks retroactively by rewinding the parameter to the\noriginal initialization in every pruning iteration, which means all the\nsubnetworks are based on the initial state. Here, we hypothesise the surviving\nsubnetworks are more important and bridge the initial feature and their\nsurviving score as the PaI criterion. We employ an end-to-end neural network\n(\\textbf{AutoS}parse) to learn this correlation, input the model's initial\nfeatures, output their score and then prune the lowest score parameters before\ntraining. To validate the accuracy and generalization of our method, we\nperformed PaI across various models. Results show that our approach outperforms\nexisting methods in high-sparsity settings. Notably, as the underlying logic of\nmodel pruning is consistent in different models, only one-time IRP on one model\nis needed (e.g., once IRP on ResNet-18/CIFAR-10, AutoS can be generalized to\nVGG-16/CIFAR-10, ResNet-18/TinyImageNet, et al.). As the first neural\nnetwork-based PaI method, we conduct extensive experiments to validate the\nfactors influencing this approach. These results reveal the learning tendencies\nof neural networks and provide new insights into our understanding and research\nof PaI from a practical perspective. Our code is available at:\nhttps://github.com/ChengYaofeng/AutoSparse.git.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"z-1CE3EKBs5koaxTxQDaI851TnIgvz3ytVXz28srQhY","pdfSize":"1251054"}
