{"id":"2408.02336","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","authors":"Hyogun Lee, Soyeon Hong, Mujeen Sung and Jinwoo Choi","authorsParsed":[["Lee","Hyogun",""],["Hong","Soyeon",""],["Sung","Mujeen",""],["Choi","Jinwoo",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 09:19:52 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 04:04:23 GMT"}],"updateDate":"2024-08-07","timestamp":1722849592000,"abstract":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}