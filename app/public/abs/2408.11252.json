{"id":"2408.11252","title":"Counterfactuals As a Means for Evaluating Faithfulness of Attribution\n  Methods in Autoregressive Language Models","authors":"Sepehr Kamahi, Yadollah Yaghoobzadeh","authorsParsed":[["Kamahi","Sepehr",""],["Yaghoobzadeh","Yadollah",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 00:17:59 GMT"}],"updateDate":"2024-08-22","timestamp":1724199479000,"abstract":"  Despite the widespread adoption of autoregressive language models,\nexplainability evaluation research has predominantly focused on span infilling\nand masked language models (MLMs). Evaluating the faithfulness of an\nexplanation method -- how accurately the method explains the inner workings and\ndecision-making of the model -- is very challenging because it is very hard to\nseparate the model from its explanation. Most faithfulness evaluation\ntechniques corrupt or remove some input tokens considered important according\nto a particular attribution (feature importance) method and observe the change\nin the model's output. This approach creates out-of-distribution inputs for\ncausal language models (CLMs) due to their training objective of next token\nprediction. In this study, we propose a technique that leverages counterfactual\ngeneration to evaluate the faithfulness of attribution methods for\nautoregressive language modeling scenarios. Our technique creates fluent and\nin-distribution counterfactuals that makes evaluation protocol more reliable.\nCode is available at https://github.com/Sepehr-Kamahi/faith\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}