{"id":"2407.15794","title":"Disentangling spatio-temporal knowledge for weakly supervised object\n  detection and segmentation in surgical video","authors":"Guiqiu Liao, Matjaz Jogan, Sai Koushik, Eric Eaton, Daniel A.\n  Hashimoto","authorsParsed":[["Liao","Guiqiu",""],["Jogan","Matjaz",""],["Koushik","Sai",""],["Eaton","Eric",""],["Hashimoto","Daniel A.",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 16:52:32 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 14:57:14 GMT"},{"version":"v3","created":"Thu, 12 Sep 2024 23:35:41 GMT"}],"updateDate":"2024-09-16","timestamp":1721667152000,"abstract":"  Weakly supervised video object segmentation (WSVOS) enables the\nidentification of segmentation maps without requiring an extensive training\ndataset of object masks, relying instead on coarse video labels indicating\nobject presence. Current state-of-the-art methods either require multiple\nindependent stages of processing that employ motion cues or, in the case of\nend-to-end trainable networks, lack in segmentation accuracy, in part due to\nthe difficulty of learning segmentation maps from videos with transient object\npresence. This limits the application of WSVOS for semantic annotation of\nsurgical videos where multiple surgical tools frequently move in and out of the\nfield of view, a problem that is more difficult than typically encountered in\nWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks\n(VDST-Net), a framework to disentangle spatiotemporal information using\nsemi-decoupled knowledge distillation to predict high-quality class activation\nmaps (CAMs). A teacher network designed to resolve temporal conflicts when\nspecifics about object location and timing in the video are not provided works\nwith a student network that integrates information over time by leveraging\ntemporal dependencies. We demonstrate the efficacy of our framework on a public\nreference dataset and on a more challenging surgical video dataset where\nobjects are, on average, present in less than 60\\% of annotated frames. Our\nmethod outperforms state-of-the-art techniques and generates superior\nsegmentation masks under video-level weak supervision.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}