{"id":"2407.04185","title":"HAF-RM: A Hybrid Alignment Framework for Reward Model Training","authors":"Shujun Liu, Xiaoyu Shen, Yuhang Lai, Siyuan Wang, Shengbin Yue,\n  Zengfeng Huang, Xuanjing Huang, Zhongyu Wei","authorsParsed":[["Liu","Shujun",""],["Shen","Xiaoyu",""],["Lai","Yuhang",""],["Wang","Siyuan",""],["Yue","Shengbin",""],["Huang","Zengfeng",""],["Huang","Xuanjing",""],["Wei","Zhongyu",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 23:26:56 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 07:35:06 GMT"}],"updateDate":"2024-07-12","timestamp":1720135616000,"abstract":"  The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}