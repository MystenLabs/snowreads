{"id":"2408.00444","title":"Ontological Relations from Word Embeddings","authors":"Mathieu d'Aquin and Emmanuel Nauer","authorsParsed":[["d'Aquin","Mathieu",""],["Nauer","Emmanuel",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 10:31:32 GMT"}],"updateDate":"2024-08-02","timestamp":1722508292000,"abstract":"  It has been reliably shown that the similarity of word embeddings obtained\nfrom popular neural models such as BERT approximates effectively a form of\nsemantic similarity of the meaning of those words. It is therefore natural to\nwonder if those embeddings contain enough information to be able to connect\nthose meanings through ontological relationships such as the one of\nsubsumption. If so, large knowledge models could be built that are capable of\nsemantically relating terms based on the information encapsulated in word\nembeddings produced by pre-trained models, with implications not only for\nontologies (ontology matching, ontology evolution, etc.) but also on the\nability to integrate ontological knowledge in neural models. In this paper, we\ntest how embeddings produced by several pre-trained models can be used to\npredict relations existing between classes and properties of popular\nupper-level and general ontologies. We show that even a simple feed-forward\narchitecture on top of those embeddings can achieve promising accuracies, with\nvarying generalisation abilities depending on the input data. To achieve that,\nwe produce a dataset that can be used to further enhance those models, opening\nnew possibilities for applications integrating knowledge from web ontologies.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"P2i4EnRmiWz3KctKph614U1rn8-O0DBftQBvqFak84w","pdfSize":"479131"}
