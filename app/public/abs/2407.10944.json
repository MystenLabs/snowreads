{"id":"2407.10944","title":"Learning from Naturally Occurring Feedback","authors":"Shachar Don-Yehiya, Leshem Choshen, Omri Abend","authorsParsed":[["Don-Yehiya","Shachar",""],["Choshen","Leshem",""],["Abend","Omri",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 17:41:34 GMT"}],"updateDate":"2024-07-16","timestamp":1721065294000,"abstract":"  Human feedback data is a critical component in developing language models.\nHowever, collecting this feedback is costly and ultimately not scalable. We\npropose a scalable method for extracting feedback that users naturally include\nwhen interacting with chat models, and leveraging it for model training. We are\nfurther motivated by previous work that showed there are also qualitative\nadvantages to using naturalistic (rather than auto-generated) feedback, such as\nless hallucinations and biases. We manually annotated conversation data to\nconfirm the presence of naturally occurring feedback in a standard corpus,\nfinding that as much as 30% of the chats include explicit feedback. We apply\nour method to over 1M conversations to obtain hundreds of thousands of feedback\nsamples. Training with the extracted feedback shows significant performance\nimprovements over baseline models, demonstrating the efficacy of our approach\nin enhancing model alignment to human preferences.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}