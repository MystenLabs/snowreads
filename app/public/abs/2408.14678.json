{"id":"2408.14678","title":"Bridging the Gap: Unpacking the Hidden Challenges in Knowledge\n  Distillation for Online Ranking Systems","authors":"Nikhil Khani, Shuo Yang, Aniruddh Nath, Yang Liu, Pendo Abbo, Li Wei,\n  Shawn Andrews, Maciej Kula, Jarrod Kahn, Zhe Zhao, Lichan Hong, Ed Chi","authorsParsed":[["Khani","Nikhil",""],["Yang","Shuo",""],["Nath","Aniruddh",""],["Liu","Yang",""],["Abbo","Pendo",""],["Wei","Li",""],["Andrews","Shawn",""],["Kula","Maciej",""],["Kahn","Jarrod",""],["Zhao","Zhe",""],["Hong","Lichan",""],["Chi","Ed",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 23:01:48 GMT"}],"updateDate":"2024-08-28","timestamp":1724713308000,"abstract":"  Knowledge Distillation (KD) is a powerful approach for compressing a large\nmodel into a smaller, more efficient model, particularly beneficial for\nlatency-sensitive applications like recommender systems. However, current KD\nresearch predominantly focuses on Computer Vision (CV) and NLP tasks,\noverlooking unique data characteristics and challenges inherent to recommender\nsystems. This paper addresses these overlooked challenges, specifically: (1)\nmitigating data distribution shifts between teacher and student models, (2)\nefficiently identifying optimal teacher configurations within time and\nbudgetary constraints, and (3) enabling computationally efficient and rapid\nsharing of teacher labels to support multiple students. We present a robust KD\nsystem developed and rigorously evaluated on multiple large-scale personalized\nvideo recommendation systems within Google. Our live experiment results\ndemonstrate significant improvements in student model performance while\nensuring consistent and reliable generation of high quality teacher labels from\na continuous data stream of data.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}