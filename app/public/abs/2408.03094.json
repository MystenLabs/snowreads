{"id":"2408.03094","title":"500xCompressor: Generalized Prompt Compression for Large Language Models","authors":"Zongqian Li, Yixuan Su, Nigel Collier","authorsParsed":[["Li","Zongqian",""],["Su","Yixuan",""],["Collier","Nigel",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 10:51:47 GMT"}],"updateDate":"2024-08-07","timestamp":1722941507000,"abstract":"  Prompt compression is crucial for enhancing inference speed, reducing costs,\nand improving user experience. However, current methods face challenges such as\nlow compression ratios and potential data leakage during evaluation. To address\nthese issues, we propose 500xCompressor, a method that compresses extensive\nnatural language contexts into a minimum of one single special token. The\n500xCompressor introduces approximately 0.3% additional parameters and achieves\ncompression ratios ranging from 6x to 480x. It is designed to compress any\ntext, answer various types of questions, and could be utilized by the original\nlarge language model (LLM) without requiring fine-tuning. Initially,\n500xCompressor was pretrained on the Arxiv Corpus, followed by fine-tuning on\nthe ArxivQA dataset, and subsequently evaluated on strictly unseen and\nclassical question answering (QA) datasets. The results demonstrate that the\nLLM retained 62.26-72.89% of its capabilities compared to using non-compressed\nprompts. This study also shows that not all the compressed tokens are equally\nutilized and that K V values have significant advantages over embeddings in\npreserving information at high compression ratios. The highly compressive\nnature of natural language prompts, even for fine-grained complex information,\nsuggests promising potential for future applications and further research into\ndeveloping a new LLM language.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"vR930FRUky-wOn-LA71d0WxGTivvcYPmYAEg-KAoD4Y","pdfSize":"1583622"}
