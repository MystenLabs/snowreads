{"id":"2408.11746","title":"Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for\n  Transformer Pretraining","authors":"Pihe Hu, Shaolong Li, Longbo Huang","authorsParsed":[["Hu","Pihe",""],["Li","Shaolong",""],["Huang","Longbo",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 16:13:16 GMT"}],"updateDate":"2024-08-22","timestamp":1724256796000,"abstract":"  Large language models (LLMs) have made significant strides in complex tasks,\nyet their widespread adoption is impeded by substantial computational demands.\nWith hundreds of billion parameters, transformer-based LLMs necessitate months\nof pretraining across a high-end GPU cluster. However, this paper reveals a\ncompelling finding: transformers exhibit considerable redundancy in pretraining\ncomputations, which motivates our proposed solution, Mixed Sparsity Training\n(MST), an efficient pretraining method that can reduce about $75\\%$ of Floating\nPoint Operations (FLOPs) while maintaining performance. MST integrates dynamic\nsparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention\n(HSA) during pretraining, involving three distinct phases: warm-up,\nultra-sparsification, and restoration. The warm-up phase transforms the dense\nmodel into a sparse one, and the restoration phase reinstates connections.\nThroughout these phases, the model is trained with a dynamically evolving\nsparse topology and an HSA mechanism to maintain performance and minimize\ntraining FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reduction\nof $4\\times$ without compromising performance.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}