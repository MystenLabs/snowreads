{"id":"2407.10499","title":"CIBench: Evaluating Your LLMs with a Code Interpreter Plugin","authors":"Songyang Zhang, Chuyu Zhang, Yingfan Hu, Haowen Shen, Kuikun Liu,\n  Zerun Ma, Fengzhe Zhou, Wenwei Zhang, Xuming He, Dahua Lin, Kai Chen","authorsParsed":[["Zhang","Songyang",""],["Zhang","Chuyu",""],["Hu","Yingfan",""],["Shen","Haowen",""],["Liu","Kuikun",""],["Ma","Zerun",""],["Zhou","Fengzhe",""],["Zhang","Wenwei",""],["He","Xuming",""],["Lin","Dahua",""],["Chen","Kai",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 07:43:55 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 04:44:54 GMT"}],"updateDate":"2024-07-26","timestamp":1721029435000,"abstract":"  While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZtHgrqxsAUT5HpW1sSzJ2Lz7trf-JXKKQTL2WjZ8O-0","pdfSize":"1368062"}
