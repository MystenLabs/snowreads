{"id":"2408.03506","title":"1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your\n  Language Model Thrives on Quality Data","authors":"Calvin Tan, Jerome Wang","authorsParsed":[["Tan","Calvin",""],["Wang","Jerome",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 02:14:52 GMT"}],"updateDate":"2024-08-08","timestamp":1722996892000,"abstract":"  This paper presents a compute-efficient approach to pre-training a Language\nModel-the \"1.5-Pints\"-in only 9 days, while outperforming state-of-the-art\nmodels as an instruction-following assistant.Based on MT-Bench (a benchmark\nthat emulates human judgments), 1.5-Pints outperforms Apple's OpenELM and\nMicrosoft's Phi.This is achieved by a carefully curated pre-training dataset of\n57 billion tokens, using a mix of automated workflows and manual human review.\nThe selection of the dataset prioritizes content that is considered expository\nand \"textbook-like\" to aid the model in reasoning and logical deduction,\nculminating in its overall ability as a strong and versatile AI model. In terms\nof the model architecture, we employed a modified Mistral tokenizer, alongside\na Llama-2 architecture for wider compatibility. For training, we adopted the\nmethodologies used by StableLM, TinyLlama, and Huggingface Zephyr. 1.5-Pints\ndemonstrates that by focusing on data quality over quantity in LLM training, we\ncan significantly reduce training time and resources required. We believe this\napproach will not only make pre-training more accessible but also reduce our\ncarbon footprint. Our findings and resources from this research are\nopen-sourced, aiming to facilitate further advancements in the field. The\n1.5-Pints model is available in two versions: 2K and 16K context windows.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}