{"id":"2407.13709","title":"Understanding Reference Policies in Direct Preference Optimization","authors":"Yixin Liu, Pengfei Liu, Arman Cohan","authorsParsed":[["Liu","Yixin",""],["Liu","Pengfei",""],["Cohan","Arman",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:08:10 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 17:56:15 GMT"}],"updateDate":"2024-08-23","timestamp":1721322490000,"abstract":"  Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of the KL-constraint from the reference policies in DPO by providing\nboth theoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority in this controlled setting.\nAdditionally, we investigate whether DPO benefits from stronger reference\npolicies, finding that a stronger reference policy can lead to improved\nperformance, but only when it is similar to the model being fine-tuned. Our\nfindings highlight the confounding role of reference policies in DPO and offer\ninsights for best practices, while also identifying open research questions for\nfuture studies.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}