{"id":"2408.03728","title":"A Convex-optimization-based Layer-wise Post-training Pruner for Large\n  Language Models","authors":"Pengxiang Zhao, Hanyu Hu, Ping Li, Yi Zheng, Zhefeng Wang, Xiaoming\n  Yuan","authorsParsed":[["Zhao","Pengxiang",""],["Hu","Hanyu",""],["Li","Ping",""],["Zheng","Yi",""],["Wang","Zhefeng",""],["Yuan","Xiaoming",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 12:33:46 GMT"}],"updateDate":"2024-08-08","timestamp":1723034026000,"abstract":"  Pruning is a critical strategy for compressing trained large language models\n(LLMs), aiming at substantial memory conservation and computational\nacceleration without compromising performance. However, existing pruning\nmethods often necessitate inefficient retraining for billion-scale LLMs or rely\non heuristic methods such as the optimal brain surgeon framework, which degrade\nperformance. In this paper, we introduce FISTAPruner, the first post-training\npruner based on convex optimization models and algorithms. Specifically, we\npropose a convex optimization model incorporating $\\ell_1$ norm to induce\nsparsity and utilize the FISTA solver for optimization. FISTAPruner\nincorporates an intra-layer cumulative error correction mechanism and supports\nparallel pruning. We comprehensively evaluate FISTAPruner on models such as\nOPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructured\nand 2:4 semi-structured sparsity, demonstrating superior performance over\nexisting state-of-the-art methods across various language benchmarks.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"z3Jz3NwA1t-2n-HsJsDnPhUC2V4zprQEWcJUrcR19r4","pdfSize":"646110"}
