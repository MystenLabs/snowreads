{"id":"2407.20417","title":"Optimizing Variational Physics-Informed Neural Networks Using Least\n  Squares","authors":"Carlos Uriarte, Manuela Bastidas, David Pardo, Jamie M. Taylor, Sergio\n  Rojas","authorsParsed":[["Uriarte","Carlos",""],["Bastidas","Manuela",""],["Pardo","David",""],["Taylor","Jamie M.",""],["Rojas","Sergio",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 21:02:21 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 17:21:59 GMT"}],"updateDate":"2024-08-30","timestamp":1722286941000,"abstract":"  Variational Physics-Informed Neural Networks often suffer from poor\nconvergence when using stochastic gradient-descent-based optimizers. By\nintroducing a Least Squares solver for the weights of the last layer of the\nneural network, we improve the convergence of the loss during training in most\npractical scenarios. This work analyzes the computational cost of the resulting\nhybrid Least-Squares/Gradient-Descent optimizer and explains how to implement\nit efficiently. In particular, we show that a traditional implementation based\non backward-mode automatic differentiation leads to a prohibitively expensive\nalgorithm. To remedy this, we propose using either forward-mode automatic\ndifferentiation or an ultraweak-type scheme that avoids the differentiation of\ntrial functions in the discrete weak formulation. The proposed alternatives are\nup to one hundred times faster than the traditional one, recovering a\ncomputational cost-per-iteration similar to that of a conventional\ngradient-descent-based optimizer alone. To support our analysis, we derive\ncomputational estimates and conduct numerical experiments in one- and\ntwo-dimensional problems.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"cSl5cE8jzsdtwucTaORCKkh6za1bJYXxfzgaDSCBAjo","pdfSize":"9485922"}
