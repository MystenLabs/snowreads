{"id":"2407.12442","title":"ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language\n  Inference","authors":"Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng,\n  Wayne Zhang","authorsParsed":[["Lan","Mengcheng",""],["Chen","Chaofeng",""],["Ke","Yiping",""],["Wang","Xinjiang",""],["Feng","Litong",""],["Zhang","Wayne",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 09:52:20 GMT"}],"updateDate":"2024-07-18","timestamp":1721209940000,"abstract":"  Despite the success of large-scale pretrained Vision-Language Models (VLMs)\nespecially CLIP in various open-vocabulary tasks, their application to semantic\nsegmentation remains challenging, producing noisy segmentation maps with\nmis-segmented regions. In this paper, we carefully re-investigate the\narchitecture of CLIP, and identify residual connections as the primary source\nof noise that degrades segmentation quality. With a comparative analysis of\nstatistical properties in the residual connection and the attention output\nacross different pretrained models, we discover that CLIP's image-text\ncontrastive training paradigm emphasizes global features at the expense of\nlocal discriminability, leading to noisy segmentation results. In response, we\npropose ClearCLIP, a novel approach that decomposes CLIP's representations to\nenhance open-vocabulary semantic segmentation. We introduce three simple\nmodifications to the final layer: removing the residual connection,\nimplementing the self-self attention, and discarding the feed-forward network.\nClearCLIP consistently generates clearer and more accurate segmentation maps\nand outperforms existing approaches across multiple benchmarks, affirming the\nsignificance of our discoveries.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}