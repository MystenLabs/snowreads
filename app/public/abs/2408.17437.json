{"id":"2408.17437","title":"SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists","authors":"Raoyuan Zhao, Abdullatif K\\\"oksal, Yihong Liu, Leonie Weissweiler,\n  Anna Korhonen, Hinrich Sch\\\"utze","authorsParsed":[["Zhao","Raoyuan",""],["Köksal","Abdullatif",""],["Liu","Yihong",""],["Weissweiler","Leonie",""],["Korhonen","Anna",""],["Schütze","Hinrich",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 17:41:30 GMT"}],"updateDate":"2024-09-02","timestamp":1725039690000,"abstract":"  Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}