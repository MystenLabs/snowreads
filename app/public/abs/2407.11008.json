{"id":"2407.11008","title":"Figuring out Figures: Using Textual References to Caption Scientific\n  Figures","authors":"Stanley Cao and Kevin Liu","authorsParsed":[["Cao","Stanley",""],["Liu","Kevin",""]],"versions":[{"version":"v1","created":"Tue, 25 Jun 2024 21:49:21 GMT"}],"updateDate":"2024-07-17","timestamp":1719352161000,"abstract":"  Figures are essential channels for densely communicating complex ideas in\nscientific papers. Previous work in automatically generating figure captions\nhas been largely unsuccessful and has defaulted to using single-layer LSTMs,\nwhich no longer achieve state-of-the-art performance. In our work, we use the\nSciCap datasets curated by Hsu et al. and use a variant of a CLIP+GPT-2\nencoder-decoder model with cross-attention to generate captions conditioned on\nthe image. Furthermore, we augment our training pipeline by creating a new\ndataset MetaSciCap that incorporates textual metadata from the original paper\nrelevant to the figure, such as the title, abstract, and in-text references. We\nuse SciBERT to encode the textual metadata and use this encoding alongside the\nfigure embedding. In our experimentation with different models, we found that\nthe CLIP+GPT-2 model performs better when it receives all textual metadata from\nthe SciBERT encoder in addition to the figure, but employing a SciBERT+GPT2\nmodel that uses only the textual metadata achieved optimal performance.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WbMa9KnsTllDxiXSnhwxX0Ao1pk9CLcYvU5cOT7waio","pdfSize":"781231"}
