{"id":"2408.02666","title":"Self-Taught Evaluators","authors":"Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane\n  Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li","authorsParsed":[["Wang","Tianlu",""],["Kulikov","Ilia",""],["Golovneva","Olga",""],["Yu","Ping",""],["Yuan","Weizhe",""],["Dwivedi-Yu","Jane",""],["Pang","Richard Yuanzhe",""],["Fazel-Zarandi","Maryam",""],["Weston","Jason",""],["Li","Xian",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 17:57:02 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 17:09:58 GMT"}],"updateDate":"2024-08-09","timestamp":1722880622000,"abstract":"  Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"1m9eJkRsgbF2OhqNu4xrphIZYDC9rZ1d62wg2_oRqOg","pdfSize":"1674475","txDigest":"BpVyUA4R68mUH19ruWtAkGVgfxTufYancRCoYE2kvbri","endEpoch":"1","status":"CERTIFIED"}
