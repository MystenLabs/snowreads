{"id":"2408.13808","title":"Towards Reliable Medical Question Answering: Techniques and Challenges\n  in Mitigating Hallucinations in Language Models","authors":"Duy Khoa Pham, Bao Quoc Vo","authorsParsed":[["Pham","Duy Khoa",""],["Vo","Bao Quoc",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 11:09:15 GMT"}],"updateDate":"2024-08-27","timestamp":1724584155000,"abstract":"  The rapid advancement of large language models (LLMs) has significantly\nimpacted various domains, including healthcare and biomedicine. However, the\nphenomenon of hallucination, where LLMs generate outputs that deviate from\nfactual accuracy or context, poses a critical challenge, especially in\nhigh-stakes domains. This paper conducts a scoping study of existing techniques\nfor mitigating hallucinations in knowledge-based task in general and especially\nfor medical domains. Key methods covered in the paper include\nRetrieval-Augmented Generation (RAG)-based techniques, iterative feedback\nloops, supervised fine-tuning, and prompt engineering. These techniques, while\npromising in general contexts, require further adaptation and optimization for\nthe medical domain due to its unique demands for up-to-date, specialized\nknowledge and strict adherence to medical guidelines. Addressing these\nchallenges is crucial for developing trustworthy AI systems that enhance\nclinical decision-making and patient safety as well as accuracy of biomedical\nscientific research.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}