{"id":"2407.16604","title":"Shared Imagination: LLMs Hallucinate Alike","authors":"Yilun Zhou, Caiming Xiong, Silvio Savarese, Chien-Sheng Wu","authorsParsed":[["Zhou","Yilun",""],["Xiong","Caiming",""],["Savarese","Silvio",""],["Wu","Chien-Sheng",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 16:06:22 GMT"}],"updateDate":"2024-07-24","timestamp":1721750782000,"abstract":"  Despite the recent proliferation of large language models (LLMs), their\ntraining recipes -- model architecture, pre-training data and optimization\nalgorithm -- are often very similar. This naturally raises the question of the\nsimilarity among the resulting models. In this paper, we propose a novel\nsetting, imaginary question answering (IQA), to better understand model\nsimilarity. In IQA, we ask one model to generate purely imaginary questions\n(e.g., on completely made-up concepts in physics) and prompt another model to\nanswer. Surprisingly, despite the total fictionality of these questions, all\nmodels can answer each other's questions with remarkable success, suggesting a\n\"shared imagination space\" in which these models operate during such\nhallucinations. We conduct a series of investigations into this phenomenon and\ndiscuss implications on model homogeneity, hallucination, and computational\ncreativity.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZpHvZ31kS36o115BltLfkx7bxrx7LGHcnC2IqPAZdSA","pdfSize":"1529552"}
