{"id":"2407.19548","title":"Cycle3D: High-quality and Consistent Image-to-3D Generation via\n  Generation-Reconstruction Cycle","authors":"Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng,\n  Yatian Pang, Bin Lin, Li Yuan","authorsParsed":[["Tang","Zhenyu",""],["Zhang","Junwu",""],["Cheng","Xinhua",""],["Yu","Wangbo",""],["Feng","Chaoran",""],["Pang","Yatian",""],["Lin","Bin",""],["Yuan","Li",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 17:58:35 GMT"}],"updateDate":"2024-07-30","timestamp":1722189515000,"abstract":"  Recent 3D large reconstruction models typically employ a two-stage process,\nincluding first generate multi-view images by a multi-view diffusion model, and\nthen utilize a feed-forward model to reconstruct images to 3D content.However,\nmulti-view diffusion models often produce low-quality and inconsistent images,\nadversely affecting the quality of the final 3D reconstruction. To address this\nissue, we propose a unified 3D generation framework called Cycle3D, which\ncyclically utilizes a 2D diffusion-based generation module and a feed-forward\n3D reconstruction module during the multi-step diffusion process. Concretely,\n2D diffusion model is applied for generating high-quality texture, and the\nreconstruction model guarantees multi-view consistency.Moreover, 2D diffusion\nmodel can further control the generated content and inject reference-view\ninformation for unseen views, thereby enhancing the diversity and texture\nconsistency of 3D generation during the denoising process. Extensive\nexperiments demonstrate the superior ability of our method to create 3D content\nwith high-quality and consistency compared with state-of-the-art baselines.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}