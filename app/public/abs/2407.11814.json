{"id":"2407.11814","title":"Contrastive Sequential-Diffusion Learning: An approach to Multi-Scene\n  Instructional Video Synthesis","authors":"Vasco Ramos, Yonatan Bitton, Michal Yarom, Idan Szpektor, Joao\n  Magalhaes","authorsParsed":[["Ramos","Vasco",""],["Bitton","Yonatan",""],["Yarom","Michal",""],["Szpektor","Idan",""],["Magalhaes","Joao",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:03:05 GMT"}],"updateDate":"2024-07-17","timestamp":1721142185000,"abstract":"  Action-centric sequence descriptions like recipe instructions and\ndo-it-yourself projects include non-linear patterns in which the next step may\nrequire to be visually consistent not on the immediate previous step but on\nearlier steps. Current video synthesis approaches fail to generate consistent\nmulti-scene videos for such task descriptions. We propose a contrastive\nsequential video diffusion method that selects the most suitable previously\ngenerated scene to guide and condition the denoising process of the next scene.\nThe result is a multi-scene video that is grounded in the scene descriptions\nand coherent w.r.t the scenes that require consistent visualisation. Our\nexperiments with real-world data demonstrate the practicality and improved\nconsistency of our model compared to prior work.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}