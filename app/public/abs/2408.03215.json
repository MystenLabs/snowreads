{"id":"2408.03215","title":"FedBAT: Communication-Efficient Federated Learning via Learnable\n  Binarization","authors":"Shiwei Li and Wenchao Xu and Haozhao Wang and Xing Tang and Yining Qi\n  and Shijie Xu and Weihong Luo and Yuhua Li and Xiuqiang He and Ruixuan Li","authorsParsed":[["Li","Shiwei",""],["Xu","Wenchao",""],["Wang","Haozhao",""],["Tang","Xing",""],["Qi","Yining",""],["Xu","Shijie",""],["Luo","Weihong",""],["Li","Yuhua",""],["He","Xiuqiang",""],["Li","Ruixuan",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 14:19:06 GMT"}],"updateDate":"2024-08-07","timestamp":1722953946000,"abstract":"  Federated learning is a promising distributed machine learning paradigm that\ncan effectively exploit large-scale data without exposing users' privacy.\nHowever, it may incur significant communication overhead, thereby potentially\nimpairing the training efficiency. To address this challenge, numerous studies\nsuggest binarizing the model updates. Nonetheless, traditional methods usually\nbinarize model updates in a post-training manner, resulting in significant\napproximation errors and consequent degradation in model accuracy. To this end,\nwe propose Federated Binarization-Aware Training (FedBAT), a novel framework\nthat directly learns binary model updates during the local training process,\nthus inherently reducing the approximation errors. FedBAT incorporates an\ninnovative binarization operator, along with meticulously designed derivatives\nto facilitate efficient learning. In addition, we establish theoretical\nguarantees regarding the convergence of FedBAT. Extensive experiments are\nconducted on four popular datasets. The results show that FedBAT significantly\naccelerates the convergence and exceeds the accuracy of baselines by up to 9\\%,\neven surpassing that of FedAvg in some cases.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}