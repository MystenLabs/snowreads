{"id":"2407.14494","title":"InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic\n  Interpretability Techniques","authors":"Rohan Gupta, Iv\\'an Arcuschin, Thomas Kwa, Adri\\`a Garriga-Alonso","authorsParsed":[["Gupta","Rohan",""],["Arcuschin","Iván",""],["Kwa","Thomas",""],["Garriga-Alonso","Adrià",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:46:51 GMT"}],"updateDate":"2024-07-22","timestamp":1721411211000,"abstract":"  Mechanistic interpretability methods aim to identify the algorithm a neural\nnetwork implements, but it is difficult to validate such methods when the true\nalgorithm is unknown. This work presents InterpBench, a collection of\nsemi-synthetic yet realistic transformers with known circuits for evaluating\nthese techniques. We train these neural networks using a stricter version of\nInterchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like\nthe original, SIIT trains neural networks by aligning their internal\ncomputation with a desired high-level causal model, but it also prevents\nnon-circuit nodes from affecting the model's output. We evaluate SIIT on sparse\ntransformers produced by the Tracr tool and find that SIIT models maintain\nTracr's original circuit while being more realistic. SIIT can also train\ntransformers with larger circuits, like Indirect Object Identification (IOI).\nFinally, we use our benchmark to evaluate existing circuit discovery\ntechniques.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}