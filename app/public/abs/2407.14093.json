{"id":"2407.14093","title":"Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models","authors":"Qiong Wu, Zhaoxi Ke, Yiyi Zhou, Gen Luo, Xiaoshuai Sun, Rongrong Ji","authorsParsed":[["Wu","Qiong",""],["Ke","Zhaoxi",""],["Zhou","Yiyi",""],["Luo","Gen",""],["Sun","Xiaoshuai",""],["Ji","Rongrong",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 07:57:48 GMT"}],"updateDate":"2024-07-25","timestamp":1721375868000,"abstract":"  Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/"}