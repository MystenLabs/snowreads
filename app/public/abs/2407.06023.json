{"id":"2407.06023","title":"Distilling System 2 into System 1","authors":"Ping Yu, Jing Xu, Jason Weston, Ilia Kulikov","authorsParsed":[["Yu","Ping",""],["Xu","Jing",""],["Weston","Jason",""],["Kulikov","Ilia",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 15:17:46 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 16:29:11 GMT"},{"version":"v3","created":"Wed, 24 Jul 2024 18:40:36 GMT"}],"updateDate":"2024-07-26","timestamp":1720451866000,"abstract":"  Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}