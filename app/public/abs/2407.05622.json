{"id":"2407.05622","title":"On the Complexity of Learning Sparse Functions with Statistical and\n  Gradient Queries","authors":"Nirmit Joshi, Theodor Misiakiewicz, Nathan Srebro","authorsParsed":[["Joshi","Nirmit",""],["Misiakiewicz","Theodor",""],["Srebro","Nathan",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 05:30:34 GMT"}],"updateDate":"2024-07-09","timestamp":1720416634000,"abstract":"  The goal of this paper is to investigate the complexity of gradient\nalgorithms when learning sparse functions (juntas). We introduce a type of\nStatistical Queries ($\\mathsf{SQ}$), which we call Differentiable Learning\nQueries ($\\mathsf{DLQ}$), to model gradient queries on a specified loss with\nrespect to an arbitrary model. We provide a tight characterization of the query\ncomplexity of $\\mathsf{DLQ}$ for learning the support of a sparse function over\ngeneric product distributions. This complexity crucially depends on the loss\nfunction. For the squared loss, $\\mathsf{DLQ}$ matches the complexity of\nCorrelation Statistical Queries $(\\mathsf{CSQ})$--potentially much worse than\n$\\mathsf{SQ}$. But for other simple loss functions, including the $\\ell_1$\nloss, $\\mathsf{DLQ}$ always achieves the same complexity as $\\mathsf{SQ}$. We\nalso provide evidence that $\\mathsf{DLQ}$ can indeed capture learning with\n(stochastic) gradient descent by showing it correctly describes the complexity\nof learning with a two-layer neural network in the mean field regime and linear\nscaling.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Data Structures and Algorithms"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}