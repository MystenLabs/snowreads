{"id":"2408.01950","title":"Why Perturbing Symbolic Music is Necessary: Fitting the Distribution of\n  Never-used Notes through a Joint Probabilistic Diffusion Model","authors":"Shipei Liu and Xiaoya Fan and Guowei Wu","authorsParsed":[["Liu","Shipei",""],["Fan","Xiaoya",""],["Wu","Guowei",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 07:38:38 GMT"}],"updateDate":"2024-08-06","timestamp":1722757118000,"abstract":"  Existing music generation models are mostly language-based, neglecting the\nfrequency continuity property of notes, resulting in inadequate fitting of rare\nor never-used notes and thus reducing the diversity of generated samples. We\nargue that the distribution of notes can be modeled by translational invariance\nand periodicity, especially using diffusion models to generalize notes by\ninjecting frequency-domain Gaussian noise. However, due to the low-density\nnature of music symbols, estimating the distribution of notes latent in the\nhigh-density solution space poses significant challenges. To address this\nproblem, we introduce the Music-Diff architecture, which fits a joint\ndistribution of notes and accompanying semantic information to generate\nsymbolic music conditionally. We first enhance the fragmentation module for\nextracting semantics by using event-based notations and the structural\nsimilarity index, thereby preventing boundary blurring. As a prerequisite for\nmultivariate perturbation, we introduce a joint pre-training method to\nconstruct the progressions between notes and musical semantics while avoiding\ndirect modeling of low-density notes. Finally, we recover the perturbed notes\nby a multi-branch denoiser that fits multiple noise objectives via Pareto\noptimization. Our experiments suggest that in contrast to language models,\njoint probability diffusion models perturbing at both note and semantic levels\ncan provide more sample diversity and compositional regularity. The case study\nhighlights the rhythmic advantages of our model over language- and DDPMs-based\nmodels by analyzing the hierarchical structure expressed in the self-similarity\nmetrics.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"-18tJUj3R2pCV8h5Qc40qyK4Q47ocK7FclK62WrSoCk","pdfSize":"5205291","txDigest":"5Rp71zH6N99512vo99SrMJZuVFzbAtaN5e7FTLmYMdVH","endEpoch":"1","status":"CERTIFIED"}
