{"id":"2407.20171","title":"Diffusion Feedback Helps CLIP See Better","authors":"Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, Xinlong Wang","authorsParsed":[["Wang","Wenxuan",""],["Sun","Quan",""],["Zhang","Fan",""],["Tang","Yepeng",""],["Liu","Jing",""],["Wang","Xinlong",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 17:00:09 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 08:42:47 GMT"},{"version":"v3","created":"Sun, 18 Aug 2024 15:15:36 GMT"},{"version":"v4","created":"Sat, 24 Aug 2024 03:55:36 GMT"}],"updateDate":"2024-08-27","timestamp":1722272409000,"abstract":"  Contrastive Language-Image Pre-training (CLIP), which excels at abstracting\nopen-world representations across domains and modalities, has become a\nfoundation for a variety of vision and multimodal tasks. However, recent\nstudies reveal that CLIP has severe visual shortcomings, such as which can\nhardly distinguish orientation, quantity, color, structure, etc. These visual\nshortcomings also limit the perception capabilities of multimodal large\nlanguage models (MLLMs) built on CLIP. The main reason could be that the\nimage-text pairs used to train CLIP are inherently biased, due to the lack of\nthe distinctiveness of the text and the diversity of images. In this work, we\npresent a simple post-training approach for CLIP models, which largely\novercomes its visual shortcomings via a self-supervised diffusion process. We\nintroduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP.\nSpecifically, DIVA leverages generative feedback from text-to-image diffusion\nmodels to optimize CLIP representations, with only images (without\ncorresponding text). We demonstrate that DIVA improves CLIP's performance on\nthe challenging MMVP-VLM benchmark which assesses fine-grained visual abilities\nto a large extent (e.g., 3-7%), and enhances the performance of MLLMs and\nvision models on multimodal understanding and segmentation tasks. Extensive\nevaluation on 29 image classification and retrieval benchmarks confirms that\nour framework preserves CLIP's strong zero-shot capabilities. The code is\navailable at https://github.com/baaivision/DIVA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}