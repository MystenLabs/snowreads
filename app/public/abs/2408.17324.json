{"id":"2408.17324","title":"Modularity in Transformers: Investigating Neuron Separability &\n  Specialization","authors":"Nicholas Pochinkov, Thomas Jones, Mohammed Rashidur Rahman","authorsParsed":[["Pochinkov","Nicholas",""],["Jones","Thomas",""],["Rahman","Mohammed Rashidur",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 14:35:01 GMT"}],"updateDate":"2024-09-02","timestamp":1725028501000,"abstract":"  Transformer models are increasingly prevalent in various applications, yet\nour understanding of their internal workings remains limited. This paper\ninvestigates the modularity and task specialization of neurons within\ntransformer architectures, focusing on both vision (ViT) and language (Mistral\n7B) models. Using a combination of selective pruning and MoEfication clustering\ntechniques, we analyze the overlap and specialization of neurons across\ndifferent tasks and data subsets. Our findings reveal evidence of task-specific\nneuron clusters, with varying degrees of overlap between related tasks. We\nobserve that neuron importance patterns persist to some extent even in randomly\ninitialized models, suggesting an inherent structure that training refines.\nAdditionally, we find that neuron clusters identified through MoEfication\ncorrespond more strongly to task-specific neurons in earlier and later layers\nof the models. This work contributes to a more nuanced understanding of\ntransformer internals and offers insights into potential avenues for improving\nmodel interpretability and efficiency.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}