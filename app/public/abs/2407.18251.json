{"id":"2407.18251","title":"Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal\n  Models: An Empirical Analysis","authors":"Cristian-Alexandru Botocan, Raphael Meier, Ljiljana Dolamic","authorsParsed":[["Botocan","Cristian-Alexandru",""],["Meier","Raphael",""],["Dolamic","Ljiljana",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 17:59:48 GMT"}],"updateDate":"2024-07-26","timestamp":1721930388000,"abstract":"  Assessing the robustness of multimodal models against adversarial examples is\nan important aspect for the safety of its users. We craft L0-norm perturbation\nattacks on the preprocessed input images. We launch them in a black-box setup\nagainst four multimodal models and two unimodal DNNs, considering both targeted\nand untargeted misclassification. Our attacks target less than 0.04% of\nperturbed image area and integrate different spatial positioning of perturbed\npixels: sparse positioning and pixels arranged in different contiguous shapes\n(row, column, diagonal, and patch). To the best of our knowledge, we are the\nfirst to assess the robustness of three state-of-the-art multimodal models\n(ALIGN, AltCLIP, GroupViT) against different sparse and contiguous pixel\ndistribution perturbations. The obtained results indicate that unimodal DNNs\nare more robust than multimodal models. Furthermore, models using CNN-based\nImage Encoder are more vulnerable than models with ViT - for untargeted\nattacks, we obtain a 99% success rate by perturbing less than 0.02% of the\nimage area.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}