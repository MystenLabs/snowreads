{"id":"2408.07889","title":"MambaVT: Spatio-Temporal Contextual Modeling for robust RGB-T Tracking","authors":"Simiao Lai, Chang Liu, Jiawen Zhu, Ben Kang, Yang Liu, Dong Wang,\n  Huchuan Lu","authorsParsed":[["Lai","Simiao",""],["Liu","Chang",""],["Zhu","Jiawen",""],["Kang","Ben",""],["Liu","Yang",""],["Wang","Dong",""],["Lu","Huchuan",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 02:29:00 GMT"}],"updateDate":"2024-08-16","timestamp":1723688940000,"abstract":"  Existing RGB-T tracking algorithms have made remarkable progress by\nleveraging the global interaction capability and extensive pre-trained models\nof the Transformer architecture. Nonetheless, these methods mainly adopt\nimagepair appearance matching and face challenges of the intrinsic high\nquadratic complexity of the attention mechanism, resulting in constrained\nexploitation of temporal information. Inspired by the recently emerged State\nSpace Model Mamba, renowned for its impressive long sequence modeling\ncapabilities and linear computational complexity, this work innovatively\nproposes a pure Mamba-based framework (MambaVT) to fully exploit\nspatio-temporal contextual modeling for robust visible-thermal tracking.\nSpecifically, we devise the long-range cross-frame integration component to\nglobally adapt to target appearance variations, and introduce short-term\nhistorical trajectory prompts to predict the subsequent target states based on\nlocal temporal location clues. Extensive experiments show the significant\npotential of vision Mamba for RGB-T tracking, with MambaVT achieving\nstate-of-the-art performance on four mainstream benchmarks while requiring\nlower computational costs. We aim for this work to serve as a simple yet strong\nbaseline, stimulating future research in this field. The code and pre-trained\nmodels will be made available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}