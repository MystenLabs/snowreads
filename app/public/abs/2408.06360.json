{"id":"2408.06360","title":"Modality-Balanced Learning for Multimedia Recommendation","authors":"Jinghao Zhang, Guofan Liu, Qiang Liu, Shu Wu, Liang Wang","authorsParsed":[["Zhang","Jinghao",""],["Liu","Guofan",""],["Liu","Qiang",""],["Wu","Shu",""],["Wang","Liang",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 07:53:01 GMT"}],"updateDate":"2024-08-14","timestamp":1721980381000,"abstract":"  Many recommender models have been proposed to investigate how to incorporate\nmultimodal content information into traditional collaborative filtering\nframework effectively. The use of multimodal information is expected to provide\nmore comprehensive information and lead to superior performance. However, the\nintegration of multiple modalities often encounters the modal imbalance\nproblem: since the information in different modalities is unbalanced,\noptimizing the same objective across all modalities leads to the\nunder-optimization problem of the weak modalities with a slower convergence\nrate or lower performance. Even worse, we find that in multimodal\nrecommendation models, all modalities suffer from the problem of insufficient\noptimization. To address these issues, we propose a Counterfactual Knowledge\nDistillation method that could solve the imbalance problem and make the best\nuse of all modalities. Through modality-specific knowledge distillation, it\ncould guide the multimodal model to learn modality-specific knowledge from\nuni-modal teachers. We also design a novel generic-and-specific distillation\nloss to guide the multimodal student to learn wider-and-deeper knowledge from\nteachers. Additionally, to adaptively recalibrate the focus of the multimodal\nmodel towards weaker modalities during training, we estimate the causal effect\nof each modality on the training objective using counterfactual inference\ntechniques, through which we could determine the weak modalities, quantify the\nimbalance degree and re-weight the distillation loss accordingly. Our method\ncould serve as a plug-and-play module for both late-fusion and early-fusion\nbackbones. Extensive experiments on six backbones show that our proposed method\ncan improve the performance by a large margin. The source code will be released\nat \\url{https://github.com/CRIPAC-DIG/Balanced-Multimodal-Rec}\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}