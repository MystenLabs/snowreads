{"id":"2407.12108","title":"Private prediction for large-scale synthetic text generation","authors":"Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia\n  Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii","authorsParsed":[["Amin","Kareem",""],["Bie","Alex",""],["Kong","Weiwei",""],["Kurakin","Alexey",""],["Ponomareva","Natalia",""],["Syed","Umar",""],["Terzis","Andreas",""],["Vassilvitskii","Sergei",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 18:28:40 GMT"}],"updateDate":"2024-07-18","timestamp":1721154520000,"abstract":"  We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}