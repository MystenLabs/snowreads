{"id":"2407.11790","title":"Characterizing and Understanding HGNN Training on GPUs","authors":"Dengke Han, Mingyu Yan, Xiaochun Ye, Dongrui Fan","authorsParsed":[["Han","Dengke",""],["Yan","Mingyu",""],["Ye","Xiaochun",""],["Fan","Dongrui",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:45:46 GMT"},{"version":"v2","created":"Thu, 18 Jul 2024 00:50:49 GMT"},{"version":"v3","created":"Fri, 16 Aug 2024 01:11:48 GMT"}],"updateDate":"2024-08-19","timestamp":1721141146000,"abstract":"  Owing to their remarkable representation capabilities for heterogeneous graph\ndata, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in\nmany critical real-world domains such as recommendation systems and medical\nanalysis. Prior to their practical application, identifying the optimal HGNN\nmodel parameters tailored to specific tasks through extensive training is a\ntime-consuming and costly process. To enhance the efficiency of HGNN training,\nit is essential to characterize and analyze the execution semantics and\npatterns within the training process to identify performance bottlenecks. In\nthis study, we conduct an in-depth quantification and analysis of two\nmainstream HGNN training scenarios, including single-GPU and multi-GPU\ndistributed training. Based on the characterization results, we disclose the\nperformance bottlenecks and their underlying causes in different HGNN training\nscenarios and provide optimization guidelines from both software and hardware\nperspectives.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Hardware Architecture","Computing Research Repository/Performance"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}