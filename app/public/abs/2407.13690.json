{"id":"2407.13690","title":"DART-Math: Difficulty-Aware Rejection Tuning for Mathematical\n  Problem-Solving","authors":"Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, Junxian He","authorsParsed":[["Tong","Yuxuan",""],["Zhang","Xiwen",""],["Wang","Rui",""],["Wu","Ruidong",""],["He","Junxian",""]],"versions":[{"version":"v1","created":"Tue, 18 Jun 2024 07:14:02 GMT"}],"updateDate":"2024-07-19","timestamp":1718694842000,"abstract":"  Solving mathematical problems requires advanced reasoning abilities and\npresents notable challenges for large language models. Previous works usually\nsynthesize data from proprietary models to augment existing datasets, followed\nby instruction tuning to achieve top-tier results. However, our analysis of\nthese datasets reveals severe biases towards easy queries, with frequent\nfailures to generate any correct response for the most challenging queries.\nHypothesizing that difficult queries are crucial to learn complex reasoning, we\npropose Difficulty-Aware Rejection Tuning (DART), a method that allocates\ndifficult queries more trials during the synthesis phase, enabling more\nextensive training on difficult samples. Utilizing DART, we have created new\ndatasets for mathematical problem-solving that focus more on difficult queries\nand are substantially smaller than previous ones. Remarkably, our synthesis\nprocess solely relies on a 7B-sized open-weight model, without reliance on the\ncommonly used proprietary GPT-4. We fine-tune various base models on our\ndatasets ranging from 7B to 70B in size, resulting in a series of strong models\ncalled DART-MATH. In comprehensive in-domain and out-of-domain evaluation on 6\nmathematical benchmarks, DART-MATH outperforms vanilla rejection tuning\nsignificantly, being superior or comparable to previous arts, despite using\nmuch smaller datasets and no proprietary models. Furthermore, our results\nposition our synthetic datasets as the most effective and cost-efficient\npublicly available resources for advancing mathematical problem-solving.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}