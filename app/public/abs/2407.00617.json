{"id":"2407.00617","title":"Iterative Nash Policy Optimization: Aligning LLMs with General\n  Preferences via No-Regret Learning","authors":"Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue\n  Huo, Nan Jiang, Haitao Mi, Dong Yu","authorsParsed":[["Zhang","Yuheng",""],["Yu","Dian",""],["Peng","Baolin",""],["Song","Linfeng",""],["Tian","Ye",""],["Huo","Mingyue",""],["Jiang","Nan",""],["Mi","Haitao",""],["Yu","Dong",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 08:00:34 GMT"},{"version":"v2","created":"Sun, 7 Jul 2024 09:51:26 GMT"}],"updateDate":"2024-07-09","timestamp":1719734434000,"abstract":"  Reinforcement Learning with Human Feedback (RLHF) has achieved great success\nin aligning large language models (LLMs) with human preferences. Prevalent RLHF\napproaches are reward-based, following the Bradley-Terry (BT) model assumption,\nwhich may not fully capture the complexity of human preferences. In this paper,\nwe explore RLHF under a general preference framework and approach it from a\ngame-theoretic perspective. Specifically, we formulate the problem as a\ntwo-player game and propose a novel algorithm, iterative Nash policy\noptimization (INPO). The key idea is to let the policy play against itself via\nno-regret learning, thereby approximating the Nash policy. Unlike previous\nmethods, INPO bypasses the need for estimating the expected win rate for\nindividual responses, which typically incurs high computational or annotation\ncosts. Instead, we introduce a new loss objective that is directly minimized\nover a preference dataset. We provide theoretical analysis for our approach and\ndemonstrate its effectiveness through experiments on various representative\nbenchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 41.5%\nlength-controlled win rate on AlpacaEval 2.0 and a 38.3% win rate on\nArena-Hard, showing substantial improvement over the state-of-the-art iterative\nalgorithm [Dong et al., 2024] under the BT model assumption. Additionally, our\nablation study highlights the benefits of incorporating KL regularization for\nresponse length control.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Science and Game Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}