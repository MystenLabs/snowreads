{"id":"2408.15379","title":"DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model\n  Transformer for Multimodal Aspect-based Sentiment Analysis","authors":"Adamu Lawan, Juhua Pu, Haruna Yunusa, Muhammad Lawan, Aliyu Umar,\n  Adamu Sani Yahya","authorsParsed":[["Lawan","Adamu",""],["Pu","Juhua",""],["Yunusa","Haruna",""],["Lawan","Muhammad",""],["Umar","Aliyu",""],["Yahya","Adamu Sani",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 19:33:15 GMT"},{"version":"v2","created":"Fri, 30 Aug 2024 16:30:39 GMT"}],"updateDate":"2024-09-02","timestamp":1724787195000,"abstract":"  Multimodal aspect-based sentiment analysis (MABSA) enhances sentiment\ndetection by combining text with other data types like images. However, despite\nsetting significant benchmarks, attention mechanisms exhibit limitations in\nefficiently modelling long-range dependencies between aspect and opinion\ntargets within the text. They also face challenges in capturing global-context\ndependencies for visual representations. To this end, we propose\nKolmogorov-Arnold Networks (KANs) and Selective State Space model (Mamba)\ntransformer (DualKanbaFormer), a novel architecture to address the above\nissues. We leverage the power of Mamba to capture global context dependencies,\nMulti-head Attention (MHA) to capture local context dependencies, and KANs to\ncapture non-linear modelling patterns for both textual representations (textual\nKanbaFormer) and visual representations (visual KanbaFormer). Furthermore, we\nfuse the textual KanbaFormer and visual KanbaFomer with a gated fusion layer to\ncapture the inter-modality dynamics. According to extensive experimental\nresults, our model outperforms some state-of-the-art (SOTA) studies on two\npublic datasets.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}