{"id":"2408.06721","title":"Response Wide Shut: Surprising Observations in Basic Vision Language\n  Model Capabilities","authors":"Shivam Chandhok, Wan-Cyuan Fan, Leonid Sigal","authorsParsed":[["Chandhok","Shivam",""],["Fan","Wan-Cyuan",""],["Sigal","Leonid",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 08:26:32 GMT"}],"updateDate":"2024-08-14","timestamp":1723537592000,"abstract":"  Vision-Language Models (VLMs) have emerged as general purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, also lacking some basic\nvisual understanding skills. In this paper, we set out to understand the\nlimitations of SoTA VLMs on fundamental visual tasks: object classification,\nunderstanding spatial arrangement, and ability to delineate individual object\ninstances (through counting), by constructing a series of tests that probe\nwhich components of design, specifically, maybe lacking. Importantly, we go\nsignificantly beyond the current benchmarks, that simply measure final\nperformance of VLM, by also comparing and contrasting it to performance of\nprobes trained directly on features obtained from visual encoder (image\nembeddings), as well as intermediate vision-language projection used to bridge\nimage-encoder and LLM-decoder ouput in many SoTA models (e.g., LLaVA, BLIP,\nInstructBLIP). In doing so, we uncover nascent shortcomings in VLMs response\nand make a number of important observations which could help train and develop\nmore effective VLM models in future.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}