{"id":"2407.02476","title":"Scalable Multi-Output Gaussian Processes with Stochastic Variational\n  Inference","authors":"Xiaoyu Jiang, Sokratia Georgaka, Magnus Rattray, Mauricio A. Alvarez","authorsParsed":[["Jiang","Xiaoyu",""],["Georgaka","Sokratia",""],["Rattray","Magnus",""],["Alvarez","Mauricio A.",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:53:56 GMT"}],"updateDate":"2024-07-03","timestamp":1719942836000,"abstract":"  The Multi-Output Gaussian Process is is a popular tool for modelling data\nfrom multiple sources. A typical choice to build a covariance function for a\nMOGP is the Linear Model of Coregionalization (LMC) which parametrically models\nthe covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalises\nthis idea by modelling the covariance between outputs using a kernel applied to\nlatent variables, one per output, leading to a flexible MOGP model that allows\nefficient generalization to new outputs with few data points. Computational\ncomplexity in LV-MOGP grows linearly with the number of outputs, which makes it\nunsuitable for problems with a large number of outputs. In this paper, we\npropose a stochastic variational inference approach for the LV-MOGP that allows\nmini-batches for both inputs and outputs, making computational complexity per\ntraining iteration independent of the number of outputs.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}