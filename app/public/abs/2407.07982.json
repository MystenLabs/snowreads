{"id":"2407.07982","title":"Automating Weak Label Generation for Data Programming with Clinicians in\n  the Loop","authors":"Jean Park, Sydney Pugh, Kaustubh Sridhar, Mengyu Liu, Navish Yarna,\n  Ramneet Kaur, Souradeep Dutta, Elena Bernardis, Oleg Sokolsky, and Insup Lee","authorsParsed":[["Park","Jean",""],["Pugh","Sydney",""],["Sridhar","Kaustubh",""],["Liu","Mengyu",""],["Yarna","Navish",""],["Kaur","Ramneet",""],["Dutta","Souradeep",""],["Bernardis","Elena",""],["Sokolsky","Oleg",""],["Lee","Insup",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 18:29:22 GMT"}],"updateDate":"2024-07-12","timestamp":1720636162000,"abstract":"  Large Deep Neural Networks (DNNs) are often data hungry and need high-quality\nlabeled data in copious amounts for learning to converge. This is a challenge\nin the field of medicine since high quality labeled data is often scarce. Data\nprogramming has been the ray of hope in this regard, since it allows us to\nlabel unlabeled data using multiple weak labeling functions. Such functions are\noften supplied by a domain expert. Data-programming can combine multiple weak\nlabeling functions and suggest labels better than simple majority voting over\nthe different functions. However, it is not straightforward to express such\nweak labeling functions, especially in high-dimensional settings such as images\nand time-series data. What we propose in this paper is a way to bypass this\nissue, using distance functions. In high-dimensional spaces, it is easier to\nfind meaningful distance metrics which can generalize across different labeling\ntasks. We propose an algorithm that queries an expert for labels of a few\nrepresentative samples of the dataset. These samples are carefully chosen by\nthe algorithm to capture the distribution of the dataset. The labels assigned\nby the expert on the representative subset induce a labeling on the full\ndataset, thereby generating weak labels to be used in the data programming\npipeline. In our medical time series case study, labeling a subset of 50 to 130\nout of 3,265 samples showed 17-28% improvement in accuracy and 13-28%\nimprovement in F1 over the baseline using clinician-defined labeling functions.\nIn our medical image case study, labeling a subset of about 50 to 120 images\nfrom 6,293 unlabeled medical images using our approach showed significant\nimprovement over the baseline method, Snuba, with an increase of approximately\n5-15% in accuracy and 12-19% in F1 score.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}