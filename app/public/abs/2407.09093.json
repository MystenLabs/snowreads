{"id":"2407.09093","title":"On Exact Bit-level Reversible Transformers Without Changing\n  Architectures","authors":"Guoqiang Zhang and J.P. Lewis and W. B. Kleijn","authorsParsed":[["Zhang","Guoqiang",""],["Lewis","J. P.",""],["Kleijn","W. B.",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 08:42:58 GMT"}],"updateDate":"2024-07-15","timestamp":1720773778000,"abstract":"  In the literature, various reversible deep neural networks (DNN) models have\nbeen proposed to reduce memory consumption or improve data-throughput in the\ntraining process. However, almost all existing reversible DNNs either are\nconstrained to have special structures or are constructed by modifying the\noriginal DNN architectures considerably to enable reversibility. In this work,\nwe propose exact bit-level reversible transformers without changing the\narchitectures in the inference procedure. The basic idea is to first treat each\ntransformer block as the Euler integration approximation for solving an\nordinary differential equation (ODE) and then incorporate the technique of\nbidirectional integration approximation (BDIA) (see [26]) for BDIA-based\ndiffusion inversion) into the neural architecture together with activation\nquantization to make it exactly bit-level reversible, referred to as\nBDIA-transformer. In the training process, we let a hyper-parameter $\\gamma$ in\nBDIA-transformer randomly take one of the two values $\\{0.5, -0.5\\}$ per\ntransformer block for averaging two consecutive integration approximations,\nwhich regularizes the models for improving the validation accuracy.\nLight-weight side information per transformer block is required to be stored in\nthe forward process to account for binary quantization loss to enable exact\nbit-level reversibility. In the inference procedure, the expectation\n$\\mathbb{E}(\\gamma)=0$ is taken to make the resulting architectures of\nBDIA-transformer be identical to transformers up to activation quantization.\nEmpirical study indicates that BDIA-transformers outperform their original\ncounterparts notably due to the regularization effect of the $\\gamma$\nparameter.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}