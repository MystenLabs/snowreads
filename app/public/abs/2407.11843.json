{"id":"2407.11843","title":"InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive\n  Evaluation and Human Feedback","authors":"Haishuo Fang, Xiaodan Zhu, Iryna Gurevych","authorsParsed":[["Fang","Haishuo",""],["Zhu","Xiaodan",""],["Gurevych","Iryna",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:24:44 GMT"}],"updateDate":"2024-07-17","timestamp":1721143484000,"abstract":"  A crucial requirement for deploying LLM-based agents in real-life\napplications is robustness against risky or irreversible mistakes. However,\nexisting research lacks a focus on the preemptive evaluation of reasoning\ntrajectories performed by LLM agents, leading to a gap in ensuring safe and\nreliable operations. To explore better solutions, this paper introduces\nInferAct, a novel approach that leverages the Theory-of-Mind capability of LLMs\nto proactively detect potential errors before critical actions are executed\n(e.g., \"buy-now\" in automatic online trading or web shopping). InferAct is also\ncapable of integrating human feedback to prevent irreversible risks and enhance\nthe actor agent's decision-making process. Experiments on three widely used\ntasks demonstrate the effectiveness of InferAct. The proposed solution presents\na novel approach and concrete contributions toward developing LLM agents that\ncan be safely deployed in different environments involving critical\ndecision-making.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yBdywA627p_mIkmaiKypo1oOM7P2UtcMSDCqv7edZ8c","pdfSize":"685665","objectId":"0x64ead93641ee997f5d08a1c871c60454241b9082547541bd5af41489bcd8e8e3","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
