{"id":"2408.05431","title":"Simple and Nearly-Optimal Sampling for Rank-1 Tensor Completion via\n  Gauss-Jordan","authors":"Alejandro Gomez-Leos, Oscar L\\'opez","authorsParsed":[["Gomez-Leos","Alejandro",""],["LÃ³pez","Oscar",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 04:26:19 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 22:30:20 GMT"}],"updateDate":"2024-08-21","timestamp":1723263979000,"abstract":"  We revisit the sample and computational complexity of completing a rank-1\ntensor in $\\otimes_{i=1}^{N} \\mathbb{R}^{d}$, given a uniformly sampled subset\nof its entries. We present a characterization of the problem (i.e. nonzero\nentries) which admits an algorithm amounting to Gauss-Jordan on a pair of\nrandom linear systems. For example, when $N = \\Theta(1)$, we prove it uses no\nmore than $m = O(d^2 \\log d)$ samples and runs in $O(md^2)$ time. Moreover, we\nshow any algorithm requires $\\Omega(d\\log d)$ samples.\n  By contrast, existing upper bounds on the sample complexity are at least as\nlarge as $d^{1.5} \\mu^{\\Omega(1)} \\log^{\\Omega(1)} d$, where $\\mu$ can be\n$\\Theta(d)$ in the worst case. Prior work obtained these looser guarantees in\nhigher rank versions of our problem, and tend to involve more complicated\nalgorithms.\n","subjects":["Computing Research Repository/Data Structures and Algorithms","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}