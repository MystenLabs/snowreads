{"id":"2408.14283","title":"Predictability and Causality in Spanish and English Natural Language\n  Generation","authors":"Andrea Busto-Casti\\~neira and Francisco J. Gonz\\'alez-Casta\\~no and\n  Silvia Garc\\'ia-M\\'endez and Francisco de Arriba-P\\'erez","authorsParsed":[["Busto-Castiñeira","Andrea",""],["González-Castaño","Francisco J.",""],["García-Méndez","Silvia",""],["de Arriba-Pérez","Francisco",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 14:09:28 GMT"}],"updateDate":"2024-08-27","timestamp":1724681368000,"abstract":"  In recent years, the field of Natural Language Generation (NLG) has been\nboosted by the recent advances in deep learning technologies. Nonetheless,\nthese new data-intensive methods introduce language-dependent disparities in\nNLG as the main training data sets are in English. Also, most neural NLG\nsystems use decoder-only (causal) transformer language models, which work well\nfor English, but were not designed with other languages in mind. In this work\nwe depart from the hypothesis that they may introduce generation bias in target\nlanguages with less rigid word ordering, subject omission, or different\nattachment preferences for relative clauses, so that for these target languages\nother language generation strategies may be more desirable. This paper first\ncompares causal and non-causal language modeling for English and Spanish, two\nlanguages with different grammatical structures and over 1.5 billion and 0.5\nbillion speakers, respectively. For this purpose, we define a novel metric of\naverage causal and non-causal context-conditioned entropy of the grammatical\ncategory distribution for both languages as an information-theoretic a priori\napproach. The evaluation of natural text sources (such as training data) in\nboth languages reveals lower average non-causal conditional entropy in Spanish\nand lower causal conditional entropy in English. According to this experiment,\nSpanish is more predictable than English given a non-causal context. Then, by\napplying a conditional relative entropy metric to text generation experiments,\nwe obtain as insights that the best performance is respectively achieved with\ncausal NLG in English, and with non-causal NLG in Spanish. These insights\nsupport further research in NLG in Spanish using bidirectional transformer\nlanguage models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}