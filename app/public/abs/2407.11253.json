{"id":"2407.11253","title":"Separable Operator Networks","authors":"Xinling Yu, Sean Hooten, Ziyue Liu, Yequan Zhao, Marco Fiorentino,\n  Thomas Van Vaerenbergh, Zheng Zhang","authorsParsed":[["Yu","Xinling",""],["Hooten","Sean",""],["Liu","Ziyue",""],["Zhao","Yequan",""],["Fiorentino","Marco",""],["Van Vaerenbergh","Thomas",""],["Zhang","Zheng",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 21:43:41 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 07:08:49 GMT"}],"updateDate":"2024-08-14","timestamp":1721079821000,"abstract":"  Operator learning has become a powerful tool in machine learning for modeling\ncomplex physical systems governed by partial differential equations (PDEs).\nAlthough Deep Operator Networks (DeepONet) show promise, they require extensive\ndata acquisition. Physics-informed DeepONets (PI-DeepONet) mitigate data\nscarcity but suffer from inefficient training processes. We introduce Separable\nOperator Networks (SepONet), a novel framework that significantly enhances the\nefficiency of physics-informed operator learning. SepONet uses independent\ntrunk networks to learn basis functions separately for different coordinate\naxes, enabling faster and more memory-efficient training via forward-mode\nautomatic differentiation. We provide a universal approximation theorem for\nSepONet proving that it generalizes to arbitrary operator learning problems,\nand then validate its performance through comprehensive benchmarking against\nPI-DeepONet. Our results demonstrate SepONet's superior performance across\nvarious nonlinear and inseparable PDEs, with SepONet's advantages increasing\nwith problem complexity, dimension, and scale. For 1D time-dependent PDEs,\nSepONet achieves up to $112\\times$ faster training and $82\\times$ reduction in\nGPU memory usage compared to PI-DeepONet, while maintaining comparable\naccuracy. For the 2D time-dependent nonlinear diffusion equation, SepONet\nefficiently handles the complexity, achieving a 6.44\\% mean relative $\\ell_{2}$\ntest error, while PI-DeepONet fails due to memory constraints. This work paves\nthe way for extreme-scale learning of continuous mappings between\ninfinite-dimensional function spaces. Open source code is available at\n\\url{https://github.com/HewlettPackard/separable-operator-networks}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computational Engineering, Finance, and Science"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"i4CWFv11EpGqm6atTY0kZXIZyo4uguxlismDxcVZaZM","pdfSize":"5632081"}
