{"id":"2408.04323","title":"Towards SLO-Optimized LLM Serving via Automatic Inference Engine Tuning","authors":"Ke Cheng, Zhi Wang, Wen Hu, Tiannuo Yang, Jianguo Li, Sheng Zhang","authorsParsed":[["Cheng","Ke",""],["Wang","Zhi",""],["Hu","Wen",""],["Yang","Tiannuo",""],["Li","Jianguo",""],["Zhang","Sheng",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 09:06:41 GMT"}],"updateDate":"2024-08-09","timestamp":1723108001000,"abstract":"  A service-level objective (SLO) is a target performance metric of service\nthat cloud vendors aim to ensure. Delivering optimized SLOs can enhance user\nsatisfaction and improve the competitiveness of cloud vendors. As large\nlanguage models (LLMs) are gaining increasing popularity across various fields,\nit is of great significance to optimize SLOs for LLM inference services. In\nthis paper, we observe that adjusting the parameters of LLM inference engines\ncan improve service performance, and the optimal parameter configurations of\ndifferent services are different. Therefore, we propose SCOOT, an automatic\nperformance tuning system to optimize SLOs for each LLM inference service by\ntuning the parameters of the inference engine. We first propose a generalized\nformulation of the tuning problem to handle various objectives and constraints\nbetween parameters, and SCOOT exploits the Bayesian optimization (BO) technique\nto resolve the problem via exploration and exploitation. Moreover, SCOOT adopts\na random forest to learn hidden constraints during the tuning process to\nmitigate invalid exploration. To improve the tuning efficiency, SCOOT utilizes\nthe parallel suggestion to accelerate the tuning process. Extensive experiments\ndemonstrate that SCOOT can significantly outperform existing tuning techniques\nin SLO optimization while greatly improving the tuning efficiency.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}