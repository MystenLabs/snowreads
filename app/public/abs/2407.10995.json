{"id":"2407.10995","title":"LionGuard: Building a Contextualized Moderation Classifier to Tackle\n  Localized Unsafe Content","authors":"Jessica Foo and Shaun Khoo","authorsParsed":[["Foo","Jessica",""],["Khoo","Shaun",""]],"versions":[{"version":"v1","created":"Mon, 24 Jun 2024 14:05:56 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 00:27:42 GMT"}],"updateDate":"2024-07-22","timestamp":1719237956000,"abstract":"  As large language models (LLMs) become increasingly prevalent in a wide\nvariety of applications, concerns about the safety of their outputs have become\nmore significant. Most efforts at safety-tuning or moderation today take on a\npredominantly Western-centric view of safety, especially for toxic, hateful, or\nviolent speech. In this paper, we describe LionGuard, a\nSingapore-contextualized moderation classifier that can serve as guardrails\nagainst unsafe LLM outputs. When assessed on Singlish data, LionGuard\noutperforms existing widely-used moderation APIs, which are not finetuned for\nthe Singapore context, by 14% (binary) and up to 51% (multi-label). Our work\nhighlights the benefits of localization for moderation classifiers and presents\na practical and scalable approach for low-resource languages.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}