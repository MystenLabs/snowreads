{"id":"2408.04631","title":"Puppet-Master: Scaling Interactive Video Generation as a Motion Prior\n  for Part-Level Dynamics","authors":"Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi","authorsParsed":[["Li","Ruining",""],["Zheng","Chuanxia",""],["Rupprecht","Christian",""],["Vedaldi","Andrea",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 17:59:38 GMT"}],"updateDate":"2024-08-09","timestamp":1723139978000,"abstract":"  We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}