{"id":"2407.00968","title":"How Does Overparameterization Affect Features?","authors":"Ahmet Cagri Duzgun, Samy Jelassi, Yuanzhi Li","authorsParsed":[["Duzgun","Ahmet Cagri",""],["Jelassi","Samy",""],["Li","Yuanzhi",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 05:01:03 GMT"}],"updateDate":"2024-07-02","timestamp":1719810063000,"abstract":"  Overparameterization, the condition where models have more parameters than\nnecessary to fit their training loss, is a crucial factor for the success of\ndeep learning. However, the characteristics of the features learned by\noverparameterized networks are not well understood. In this work, we explore\nthis question by comparing models with the same architecture but different\nwidths. We first examine the expressivity of the features of these models, and\nshow that the feature space of overparameterized networks cannot be spanned by\nconcatenating many underparameterized features, and vice versa. This reveals\nthat both overparameterized and underparameterized networks acquire some\ndistinctive features. We then evaluate the performance of these models, and\nfind that overparameterized networks outperform underparameterized networks,\neven when many of the latter are concatenated. We corroborate these findings\nusing a VGG-16 and ResNet18 on CIFAR-10 and a Transformer on the MNLI\nclassification dataset. Finally, we propose a toy setting to explain how\noverparameterized networks can learn some important features that the\nunderparamaterized networks cannot learn.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Mf8LwthymJf2w9UqLRVO5bKDoLqPm1NhwTuo-3IAp-Q","pdfSize":"1794576"}
