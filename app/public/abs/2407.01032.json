{"id":"2407.01032","title":"Overcoming Common Flaws in the Evaluation of Selective Classification\n  Systems","authors":"Jeremias Traub, Till J. Bungert, Carsten T. L\\\"uth, Michael\n  Baumgartner, Klaus H. Maier-Hein, Lena Maier-Hein, Paul F Jaeger","authorsParsed":[["Traub","Jeremias",""],["Bungert","Till J.",""],["LÃ¼th","Carsten T.",""],["Baumgartner","Michael",""],["Maier-Hein","Klaus H.",""],["Maier-Hein","Lena",""],["Jaeger","Paul F",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 07:32:58 GMT"}],"updateDate":"2024-07-02","timestamp":1719819178000,"abstract":"  Selective Classification, wherein models can reject low-confidence\npredictions, promises reliable translation of machine-learning based\nclassification systems to real-world scenarios such as clinical diagnostics.\nWhile current evaluation of these systems typically assumes fixed working\npoints based on pre-defined rejection thresholds, methodological progress\nrequires benchmarking the general performance of systems akin to the\n$\\mathrm{AUROC}$ in standard classification. In this work, we define 5\nrequirements for multi-threshold metrics in selective classification regarding\ntask alignment, interpretability, and flexibility, and show how current\napproaches fail to meet them. We propose the Area under the Generalized Risk\nCoverage curve ($\\mathrm{AUGRC}$), which meets all requirements and can be\ndirectly interpreted as the average risk of undetected failures. We empirically\ndemonstrate the relevance of $\\mathrm{AUGRC}$ on a comprehensive benchmark\nspanning 6 data sets and 13 confidence scoring functions. We find that the\nproposed metric substantially changes metric rankings on 5 out of the 6 data\nsets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition","Statistics/Methodology"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}