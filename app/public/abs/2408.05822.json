{"id":"2408.05822","title":"Sampling Foundational Transformer: A Theoretical Perspective","authors":"Viet Anh Nguyen, Minh Lenhat, Khoa Nguyen, Duong Duc Hieu, Dao Huu\n  Hung, Truong Son Hy","authorsParsed":[["Nguyen","Viet Anh",""],["Lenhat","Minh",""],["Nguyen","Khoa",""],["Hieu","Duong Duc",""],["Hung","Dao Huu",""],["Hy","Truong Son",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 16:53:09 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 22:33:06 GMT"}],"updateDate":"2024-08-20","timestamp":1723395189000,"abstract":"  The versatility of self-attention mechanism earned transformers great success\nin almost all data modalities, with limitations on the quadratic complexity and\ndifficulty of training. To apply transformers across different data modalities,\npractitioners have to make specific clever data-modality-dependent\nconstructions. In this paper, we propose Sampling Foundational Transformer\n(SFT) that can work on multiple data modalities (e.g., point cloud, graph, and\nsequence) and constraints (e.g., rotational-invariant). The existence of such\nmodel is important as contemporary foundational modeling requires operability\non multiple data sources. For efficiency on large number of tokens, our model\nrelies on our context aware sampling-without-replacement mechanism for both\nlinear asymptotic computational complexity and real inference time gain. For\nefficiency, we rely on our newly discovered pseudoconvex formulation of\ntransformer layer to increase model's convergence rate. As a model working on\nmultiple data modalities, SFT has achieved competitive results on many\nbenchmarks, while being faster in inference, compared to other very specialized\nmodels.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}