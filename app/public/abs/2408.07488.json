{"id":"2408.07488","title":"Towards Enhanced Context Awareness with Vision-based Multimodal\n  Interfaces","authors":"Yongquan Hu, Wen Hu, Aaron Quigley","authorsParsed":[["Hu","Yongquan",""],["Hu","Wen",""],["Quigley","Aaron",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 12:09:58 GMT"}],"updateDate":"2024-08-15","timestamp":1723637398000,"abstract":"  Vision-based Interfaces (VIs) are pivotal in advancing Human-Computer\nInteraction (HCI), particularly in enhancing context awareness. However, there\nare significant opportunities for these interfaces due to rapid advancements in\nmultimodal Artificial Intelligence (AI), which promise a future of tight\ncoupling between humans and intelligent systems. AI-driven VIs, when integrated\nwith other modalities, offer a robust solution for effectively capturing and\ninterpreting user intentions and complex environmental information, thereby\nfacilitating seamless and efficient interactions. This PhD study explores three\napplication cases of multimodal interfaces to augment context awareness,\nrespectively focusing on three dimensions of visual modality: scale, depth, and\ntime: a fine-grained analysis of physical surfaces via microscopic image,\nprecise projection of the real world using depth data, and rendering haptic\nfeedback from video background in virtual environments.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/"}