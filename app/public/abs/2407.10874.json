{"id":"2407.10874","title":"Random Channel Ablation for Robust Hand Gesture Classification with\n  Multimodal Biosignals","authors":"Keshav Bimbraw, Jing Liu, Ye Wang, and Toshiaki Koike-Akino","authorsParsed":[["Bimbraw","Keshav",""],["Liu","Jing",""],["Wang","Ye",""],["Koike-Akino","Toshiaki",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 16:23:53 GMT"}],"updateDate":"2024-07-16","timestamp":1721060633000,"abstract":"  Biosignal-based hand gesture classification is an important component of\neffective human-machine interaction. For multimodal biosignal sensing, the\nmodalities often face data loss due to missing channels in the data which can\nadversely affect the gesture classification performance. To make the\nclassifiers robust to missing channels in the data, this paper proposes using\nRandom Channel Ablation (RChA) during the training process. Ultrasound and\nforce myography (FMG) data were acquired from the forearm for 12 hand gestures\nover 2 subjects. The resulting multimodal data had 16 total channels, 8 for\neach modality. The proposed method was applied to convolutional neural network\narchitecture, and compared with baseline, imputation, and oracle methods. Using\n5-fold cross-validation for the two subjects, on average, 12.2% and 24.5%\nimprovement was observed for gesture classification with up to 4 and 8 missing\nchannels respectively compared to the baseline. Notably, the proposed method is\nalso robust to an increase in the number of missing channels compared to other\nmethods. These results show the efficacy of using random channel ablation to\nimprove classifier robustness for multimodal and multi-channel biosignal-based\nhand gesture classification.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}