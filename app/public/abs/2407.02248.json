{"id":"2407.02248","title":"EvolBA: Evolutionary Boundary Attack under Hard-label Black Box\n  condition","authors":"Ayane Tajima, Satoshi Ono","authorsParsed":[["Tajima","Ayane",""],["Ono","Satoshi",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 13:12:52 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 12:58:36 GMT"},{"version":"v3","created":"Tue, 9 Jul 2024 13:02:23 GMT"}],"updateDate":"2024-07-10","timestamp":1719925972000,"abstract":"  Research has shown that deep neural networks (DNNs) have vulnerabilities that\ncan lead to the misrecognition of Adversarial Examples (AEs) with specifically\ndesigned perturbations. Various adversarial attack methods have been proposed\nto detect vulnerabilities under hard-label black box (HL-BB) conditions in the\nabsence of loss gradients and confidence scores.However, these methods fall\ninto local solutions because they search only local regions of the search\nspace. Therefore, this study proposes an adversarial attack method named EvolBA\nto generate AEs using Covariance Matrix Adaptation Evolution Strategy (CMA-ES)\nunder the HL-BB condition, where only a class label predicted by the target DNN\nmodel is available. Inspired by formula-driven supervised learning, the\nproposed method introduces domain-independent operators for the initialization\nprocess and a jump that enhances search exploration. Experimental results\nconfirmed that the proposed method could determine AEs with smaller\nperturbations than previous methods in images where the previous methods have\ndifficulty.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}