{"id":"2408.11061","title":"StructuredRAG: JSON Response Formatting with Large Language Models","authors":"Connor Shorten, Charles Pierse, Thomas Benjamin Smith, Erika Cardenas,\n  Akanksha Sharma, John Trengrove, Bob van Luijt","authorsParsed":[["Shorten","Connor",""],["Pierse","Charles",""],["Smith","Thomas Benjamin",""],["Cardenas","Erika",""],["Sharma","Akanksha",""],["Trengrove","John",""],["van Luijt","Bob",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 19:32:59 GMT"}],"updateDate":"2024-08-22","timestamp":1723059179000,"abstract":"  The ability of Large Language Models (LLMs) to generate structured outputs,\nsuch as JSON, is crucial for their use in Compound AI Systems. However,\nevaluating and improving this capability remains challenging. In this work, we\nintroduce StructuredRAG, a benchmark of six tasks designed to assess LLMs'\nproficiency in following response format instructions. We evaluate two\nstate-of-the-art LLMs, Gemini 1.5 Pro and Llama 3 8B-instruct with 4-bit\nquantization using two distinct prompting strategies. We introduce these\nprompting strategies as f-String and Follow the Format (FF) prompting. Across\n24 experiments, we find an average success rate of 82.55%. We further find a\nhigh variance in performance across tasks, models, and prompting strategies\nwith success rates ranging from 0 to 100%. We find that Llama 3 8B-instruct\noften performs competitively with Gemini 1.5 Pro. We observe that task\ncomplexity significantly influences performance, with tasks involving lists or\ncomposite object outputs proving more challenging. Our findings highlight the\nneed for further research into improving the reliability and consistency of\nstructured output generation in LLMs. We have open-sourced our experimental\ncode and results at github.com/weaviate/structured-rag.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}