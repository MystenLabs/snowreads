{"id":"2408.09347","title":"S^3D-NeRF: Single-Shot Speech-Driven Neural Radiance Field for High\n  Fidelity Talking Head Synthesis","authors":"Dongze Li, Kang Zhao, Wei Wang, Yifeng Ma, Bo Peng, Yingya Zhang, Jing\n  Dong","authorsParsed":[["Li","Dongze",""],["Zhao","Kang",""],["Wang","Wei",""],["Ma","Yifeng",""],["Peng","Bo",""],["Zhang","Yingya",""],["Dong","Jing",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 03:59:57 GMT"}],"updateDate":"2024-08-20","timestamp":1723953597000,"abstract":"  Talking head synthesis is a practical technique with wide applications.\nCurrent Neural Radiance Field (NeRF) based approaches have shown their\nsuperiority on driving one-shot talking heads with videos or signals regressed\nfrom audio. However, most of them failed to take the audio as driven\ninformation directly, unable to enjoy the flexibility and availability of\nspeech. Since mapping audio signals to face deformation is non-trivial, we\ndesign a Single-Shot Speech-Driven Neural Radiance Field (S^3D-NeRF) method in\nthis paper to tackle the following three difficulties: learning a\nrepresentative appearance feature for each identity, modeling motion of\ndifferent face regions with audio, and keeping the temporal consistency of the\nlip area. To this end, we introduce a Hierarchical Facial Appearance Encoder to\nlearn multi-scale representations for catching the appearance of different\nspeakers, and elaborate a Cross-modal Facial Deformation Field to perform\nspeech animation according to the relationship between the audio signal and\ndifferent face regions. Moreover, to enhance the temporal consistency of the\nimportant lip area, we introduce a lip-sync discriminator to penalize the\nout-of-sync audio-visual sequences. Extensive experiments have shown that our\nS^3D-NeRF surpasses previous arts on both video fidelity and audio-lip\nsynchronization.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}