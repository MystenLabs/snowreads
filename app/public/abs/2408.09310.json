{"id":"2408.09310","title":"Narrowing the Focus: Learned Optimizers for Pretrained Models","authors":"Gus Kristiansen, Mark Sandler, Andrey Zhmoginov, Nolan Miller, Anirudh\n  Goyal, Jihwan Lee, Max Vladymyrov","authorsParsed":[["Kristiansen","Gus",""],["Sandler","Mark",""],["Zhmoginov","Andrey",""],["Miller","Nolan",""],["Goyal","Anirudh",""],["Lee","Jihwan",""],["Vladymyrov","Max",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 23:55:19 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 18:35:06 GMT"}],"updateDate":"2024-08-23","timestamp":1723938919000,"abstract":"  In modern deep learning, the models are learned by applying gradient updates\nusing an optimizer, which transforms the updates based on various statistics.\nOptimizers are often hand-designed and tuning their hyperparameters is a big\npart of the training process. Learned optimizers have shown some initial\npromise, but are generally unsuccessful as a general optimization mechanism\napplicable to every problem. In this work we explore a different direction:\ninstead of learning general optimizers, we instead specialize them to a\nspecific training environment. We propose a novel optimizer technique that\nlearns a layer-specific linear combination of update directions provided by a\nset of base optimizers, effectively adapting its strategy to the specific model\nand dataset. When evaluated on image classification tasks, this specialized\noptimizer significantly outperforms both traditional off-the-shelf methods such\nas Adam, as well as existing general learned optimizers. Moreover, it\ndemonstrates robust generalization with respect to model initialization,\nevaluating on unseen datasets, and training durations beyond its meta-training\nhorizon.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}