{"id":"2407.12259","title":"In-Context Probing Approximates Influence Function for Data Valuation","authors":"Cathy Jiao and Gary Gao and Chenyan Xiong","authorsParsed":[["Jiao","Cathy",""],["Gao","Gary",""],["Xiong","Chenyan",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 02:06:56 GMT"}],"updateDate":"2024-07-18","timestamp":1721182016000,"abstract":"  Data valuation quantifies the value of training data, and is used for data\nattribution (i.e., determining the contribution of training data towards model\npredictions), and data selection; both of which are important for curating\nhigh-quality datasets to train large language models. In our paper, we show\nthat data valuation through in-context probing (i.e., prompting a LLM)\napproximates influence functions for selecting training data. We provide a\ntheoretical sketch on this connection based on transformer models performing\n\"implicit\" gradient descent on its in-context inputs. Our empirical findings\nshow that in-context probing and gradient-based influence frameworks are\nsimilar in how they rank training data. Furthermore, fine-tuning experiments on\ndata selected by either method reveal similar model performance.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}