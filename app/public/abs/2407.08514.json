{"id":"2407.08514","title":"Rethinking the Threat and Accessibility of Adversarial Attacks against\n  Face Recognition Systems","authors":"Yuxin Cao, Yumeng Zhu, Derui Wang, Sheng Wen, Minhui Xue, Jin Lu, Hao\n  Ge","authorsParsed":[["Cao","Yuxin",""],["Zhu","Yumeng",""],["Wang","Derui",""],["Wen","Sheng",""],["Xue","Minhui",""],["Lu","Jin",""],["Ge","Hao",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:58:09 GMT"}],"updateDate":"2024-07-12","timestamp":1720706289000,"abstract":"  Face recognition pipelines have been widely deployed in various\nmission-critical systems in trust, equitable and responsible AI applications.\nHowever, the emergence of adversarial attacks has threatened the security of\nthe entire recognition pipeline. Despite the sheer number of attack methods\nproposed for crafting adversarial examples in both digital and physical forms,\nit is never an easy task to assess the real threat level of different attacks\nand obtain useful insight into the key risks confronted by face recognition\nsystems. Traditional attacks view imperceptibility as the most important\nmeasurement to keep perturbations stealthy, while we suspect that industry\nprofessionals may possess a different opinion. In this paper, we delve into\nmeasuring the threat brought about by adversarial attacks from the perspectives\nof the industry and the applications of face recognition. In contrast to widely\nstudied sophisticated attacks in the field, we propose an effective yet\neasy-to-launch physical adversarial attack, named AdvColor, against black-box\nface recognition pipelines in the physical world. AdvColor fools models in the\nrecognition pipeline via directly supplying printed photos of human faces to\nthe system under adversarial illuminations. Experimental results show that\nphysical AdvColor examples can achieve a fooling rate of more than 96% against\nthe anti-spoofing model and an overall attack success rate of 88% against the\nface recognition pipeline. We also conduct a survey on the threats of\nprevailing adversarial attacks, including AdvColor, to understand the gap\nbetween the machine-measured and human-assessed threat levels of different\nforms of adversarial attacks. The survey results surprisingly indicate that,\ncompared to deliberately launched imperceptible attacks, perceptible but\naccessible attacks pose more lethal threats to real-world commercial systems of\nface recognition.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}