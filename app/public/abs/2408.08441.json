{"id":"2408.08441","title":"D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning","authors":"Rafael Rafailov, Kyle Hatch, Anikait Singh, Laura Smith, Aviral Kumar,\n  Ilya Kostrikov, Philippe Hansen-Estruch, Victor Kolev, Philip Ball, Jiajun\n  Wu, Chelsea Finn, Sergey Levine","authorsParsed":[["Rafailov","Rafael",""],["Hatch","Kyle",""],["Singh","Anikait",""],["Smith","Laura",""],["Kumar","Aviral",""],["Kostrikov","Ilya",""],["Hansen-Estruch","Philippe",""],["Kolev","Victor",""],["Ball","Philip",""],["Wu","Jiajun",""],["Finn","Chelsea",""],["Levine","Sergey",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 22:27:00 GMT"}],"updateDate":"2024-08-19","timestamp":1723760820000,"abstract":"  Offline reinforcement learning algorithms hold the promise of enabling\ndata-driven RL methods that do not require costly or dangerous real-world\nexploration and benefit from large pre-collected datasets. This in turn can\nfacilitate real-world applications, as well as a more standardized approach to\nRL research. Furthermore, offline RL methods can provide effective\ninitializations for online finetuning to overcome challenges with exploration.\nHowever, evaluating progress on offline RL algorithms requires effective and\nchallenging benchmarks that capture properties of real-world tasks, provide a\nrange of task difficulties, and cover a range of challenges both in terms of\nthe parameters of the domain (e.g., length of the horizon, sparsity of rewards)\nand the parameters of the data (e.g., narrow demonstration data or broad\nexploratory data). While considerable progress in offline RL in recent years\nhas been enabled by simpler benchmark tasks, the most widely used datasets are\nincreasingly saturating in performance and may fail to reflect properties of\nrealistic tasks. We propose a new benchmark for offline RL that focuses on\nrealistic simulations of robotic manipulation and locomotion environments,\nbased on models of real-world robotic systems, and comprising a variety of data\nsources, including scripted data, play-style data collected by human\nteleoperators, and other data sources. Our proposed benchmark covers\nstate-based and image-based domains, and supports both offline RL and online\nfine-tuning evaluation, with some of the tasks specifically designed to require\nboth pre-training and fine-tuning. We hope that our proposed benchmark will\nfacilitate further progress on both offline RL and fine-tuning algorithms.\nWebsite with code, examples, tasks, and data is available at\n\\url{https://sites.google.com/view/d5rl/}\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}