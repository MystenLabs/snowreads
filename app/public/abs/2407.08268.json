{"id":"2407.08268","title":"Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic\n  Segmentation","authors":"Tong Shao, Zhuotao Tian, Hang Zhao, Jingyong Su","authorsParsed":[["Shao","Tong",""],["Tian","Zhuotao",""],["Zhao","Hang",""],["Su","Jingyong",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 08:12:16 GMT"}],"updateDate":"2024-07-12","timestamp":1720685536000,"abstract":"  CLIP, as a vision-language model, has significantly advanced Open-Vocabulary\nSemantic Segmentation (OVSS) with its zero-shot capabilities. Despite its\nsuccess, its application to OVSS faces challenges due to its initial\nimage-level alignment training, which affects its performance in tasks\nrequiring detailed local context. Our study delves into the impact of CLIP's\n[CLS] token on patch feature correlations, revealing a dominance of \"global\"\npatches that hinders local feature discrimination. To overcome this, we propose\nCLIPtrase, a novel training-free semantic segmentation strategy that enhances\nlocal feature awareness through recalibrated self-correlation among patches.\nThis approach demonstrates notable improvements in segmentation accuracy and\nthe ability to maintain semantic coherence across objects.Experiments show that\nwe are 22.3% ahead of CLIP on average on 9 segmentation benchmarks,\noutperforming existing state-of-the-art training-free methods.The code are made\npublicly available at: https://github.com/leaves162/CLIPtrase.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}