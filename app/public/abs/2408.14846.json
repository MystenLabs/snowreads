{"id":"2408.14846","title":"Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion","authors":"Guoqing Zhang, Jian Liu","authorsParsed":[["Zhang","Guoqing",""],["Liu","Jian",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 07:57:58 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 06:50:18 GMT"}],"updateDate":"2024-09-10","timestamp":1724745478000,"abstract":"  Point clouds are crucial for capturing three-dimensional data but often\nsuffer from incompleteness due to limitations such as resolution and occlusion.\nTraditional methods typically rely on point-based approaches within\ndiscriminative frameworks for point cloud completion. In this paper, we\nintroduce \\textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud\nCompletion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the\nfirst stage, the Coarse Density Voxel Prediction Network (CDNet) processes\npartial points to predict coarse density voxels, streamlining global feature\nextraction through voxel classification, as opposed to previous\nregression-based methods. In the second stage, we introduce the Occupancy\nGeneration Network (OccGen), a conditional occupancy diffusion model based on a\ntransformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This\nblock integrates coarse density voxels with partial points to leverage both\nglobal and local features for comprehensive completion. By thresholding the\noccupancy field, we convert it into a complete point cloud. Additionally, our\nmethod employs diverse training mixtures and efficient diffusion\nparameterization to enable effective one-step sampling during both training and\ninference. Experimental results demonstrate that Diffusion-Occ outperforms\nexisting discriminative and generative methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}