{"id":"2407.08231","title":"E2VIDiff: Perceptual Events-to-Video Reconstruction using Diffusion\n  Priors","authors":"Jinxiu Liang, Bohan Yu, Yixin Yang, Yiming Han, Boxin Shi","authorsParsed":[["Liang","Jinxiu",""],["Yu","Bohan",""],["Yang","Yixin",""],["Han","Yiming",""],["Shi","Boxin",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 07:10:58 GMT"}],"updateDate":"2024-07-12","timestamp":1720681858000,"abstract":"  Event cameras, mimicking the human retina, capture brightness changes with\nunparalleled temporal resolution and dynamic range. Integrating events into\nintensities poses a highly ill-posed challenge, marred by initial condition\nambiguities. Traditional regression-based deep learning methods fall short in\nperceptual quality, offering deterministic and often unrealistic\nreconstructions. In this paper, we introduce diffusion models to\nevents-to-video reconstruction, achieving colorful, realistic, and perceptually\nsuperior video generation from achromatic events. Powered by the image\ngeneration ability and knowledge of pretrained diffusion models, the proposed\nmethod can achieve a better trade-off between the perception and distortion of\nthe reconstructed frame compared to previous solutions. Extensive experiments\non benchmark datasets demonstrate that our approach can produce diverse,\nrealistic frames with faithfulness to the given events.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}