{"id":"2408.08926","title":"Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk\n  of Language Models","authors":"Andy K. Zhang, Neil Perry, Riya Dulepet, Eliot Jones, Justin W. Lin,\n  Joey Ji, Celeste Menders, Gashon Hussein, Samantha Liu, Donovan Jasper, Pura\n  Peetathawatchai, Ari Glenn, Vikram Sivashankar, Daniel Zamoshchin, Leo\n  Glikbarg, Derek Askaryar, Mike Yang, Teddy Zhang, Rishi Alluri, Nathan Tran,\n  Rinnara Sangpisit, Polycarpos Yiorkadjis, Kenny Osele, Gautham Raghupathi,\n  Dan Boneh, Daniel E. Ho, Percy Liang","authorsParsed":[["Zhang","Andy K.",""],["Perry","Neil",""],["Dulepet","Riya",""],["Jones","Eliot",""],["Lin","Justin W.",""],["Ji","Joey",""],["Menders","Celeste",""],["Hussein","Gashon",""],["Liu","Samantha",""],["Jasper","Donovan",""],["Peetathawatchai","Pura",""],["Glenn","Ari",""],["Sivashankar","Vikram",""],["Zamoshchin","Daniel",""],["Glikbarg","Leo",""],["Askaryar","Derek",""],["Yang","Mike",""],["Zhang","Teddy",""],["Alluri","Rishi",""],["Tran","Nathan",""],["Sangpisit","Rinnara",""],["Yiorkadjis","Polycarpos",""],["Osele","Kenny",""],["Raghupathi","Gautham",""],["Boneh","Dan",""],["Ho","Daniel E.",""],["Liang","Percy",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:23:10 GMT"}],"updateDate":"2024-08-20","timestamp":1723742590000,"abstract":"  Language Model (LM) agents for cybersecurity that are capable of autonomously\nidentifying vulnerabilities and executing exploits have the potential to cause\nreal-world impact. Policymakers, model providers, and other researchers in the\nAI and cybersecurity communities are interested in quantifying the capabilities\nof such agents to help mitigate cyberrisk and investigate opportunities for\npenetration testing. Toward that end, we introduce Cybench, a framework for\nspecifying cybersecurity tasks and evaluating agents on those tasks. We include\n40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF\ncompetitions, chosen to be recent, meaningful, and spanning a wide range of\ndifficulties. Each task includes its own description, starter files, and is\ninitialized in an environment where an agent can execute bash commands and\nobserve outputs. Since many tasks are beyond the capabilities of existing LM\nagents, we introduce subtasks, which break down a task into intermediary steps\nfor more gradated evaluation; we add subtasks for 17 of the 40 tasks. To\nevaluate agent capabilities, we construct a cybersecurity agent and evaluate 7\nmodels: GPT-4o, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct,\nGemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without\nguidance, we find that agents are able to solve only the easiest complete tasks\nthat took human teams up to 11 minutes to solve, with Claude 3.5 Sonnet and\nGPT-4o having the highest success rates. Finally, subtasks provide more signal\nfor measuring performance compared to unguided runs, with models achieving a\n3.2\\% higher success rate on complete tasks with subtask-guidance than without\nsubtask-guidance. All code and data are publicly available at\nhttps://cybench.github.io\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gp2AQcfUy6xVdh8C_CR5mp_I3t3eBTM-3L-SdXPewN0","pdfSize":"3107034"}
