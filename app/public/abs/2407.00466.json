{"id":"2407.00466","title":"BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for\n  Biomedical Science","authors":"Xinna Lin, Siqi Ma, Junjie Shan, Xiaojing Zhang, Shell Xu Hu, Tiannan\n  Guo, Stan Z. Li, Kaicheng Yu","authorsParsed":[["Lin","Xinna",""],["Ma","Siqi",""],["Shan","Junjie",""],["Zhang","Xiaojing",""],["Hu","Shell Xu",""],["Guo","Tiannan",""],["Li","Stan Z.",""],["Yu","Kaicheng",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 15:23:28 GMT"}],"updateDate":"2024-07-02","timestamp":1719674608000,"abstract":"  Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist,\ndraws increasing attention, where one common approach is to build a copilot\nagent driven by Large Language Models (LLMs). However, to evaluate such\nsystems, people either rely on direct Question-Answering (QA) to the LLM\nitself, or in a biomedical experimental manner. How to precisely benchmark\nbiomedical agents from an AI Scientist perspective remains largely unexplored.\nTo this end, we draw inspiration from one most important abilities of\nscientists, understanding the literature, and introduce BioKGBench. In contrast\nto traditional evaluation benchmark that only focuses on factual QA, where the\nLLMs are known to have hallucination issues, we first disentangle\n\"Understanding Literature\" into two atomic abilities, i) \"Understanding\" the\nunstructured text from research papers by performing scientific claim\nverification, and ii) Ability to interact with structured Knowledge-Graph\nQuestion-Answering (KGQA) as a form of \"Literature\" grounding. We then\nformulate a novel agent task, dubbed KGCheck, using KGQA and domain-based\nRetrieval-Augmented Generation (RAG) to identify the factual errors of existing\nlarge-scale knowledge graph databases. We collect over two thousand data for\ntwo atomic tasks and 225 high-quality annotated data for the agent task.\nSurprisingly, we discover that state-of-the-art agents, both daily scenarios\nand biomedical ones, have either failed or inferior performance on our\nbenchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.\nOn the widely used popular knowledge graph, we discover over 90 factual errors\nwhich provide scenarios for agents to make discoveries and demonstrate the\neffectiveness of our approach. The code and data are available at\nhttps://github.com/westlake-autolab/BioKGBench.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yUSbnb0F2RvE03aO0BaBGEWtu1rTiYlopvcYL7kX0ks","pdfSize":"8882572"}
