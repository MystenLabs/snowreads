{"id":"2407.14274","title":"Mixed-precision Neural Networks on RISC-V Cores: ISA extensions for\n  Multi-Pumped Soft SIMD Operations","authors":"Giorgos Armeniakos, Alexis Maras, Sotirios Xydis, Dimitrios Soudris","authorsParsed":[["Armeniakos","Giorgos",""],["Maras","Alexis",""],["Xydis","Sotirios",""],["Soudris","Dimitrios",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 12:54:04 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 11:40:53 GMT"}],"updateDate":"2024-08-14","timestamp":1721393644000,"abstract":"  Recent advancements in quantization and mixed-precision approaches offers\nsubstantial opportunities to improve the speed and energy efficiency of Neural\nNetworks (NN). Research has shown that individual parameters with varying low\nprecision, can attain accuracies comparable to full-precision counterparts.\nHowever, modern embedded microprocessors provide very limited support for\nmixed-precision NNs regarding both Instruction Set Architecture (ISA)\nextensions and their hardware design for efficient execution of mixed-precision\noperations, i.e., introducing several performance bottlenecks due to numerous\ninstructions for data packing and unpacking, arithmetic unit under-utilizations\netc. In this work, we bring together, for the first time, ISA extensions\ntailored to mixed-precision hardware optimizations, targeting energy-efficient\nDNN inference on leading RISC-V CPU architectures. To this end, we introduce a\nhardware-software co-design framework that enables cooperative hardware design,\nmixed-precision quantization, ISA extensions and inference in cycle-accurate\nemulations. At hardware level, we firstly expand the ALU unit within our\nproof-of-concept micro-architecture to support configurable fine grained\nmixed-precision arithmetic operations. Subsequently, we implement multi-pumping\nto minimize execution latency, with an additional soft SIMD optimization\napplied for 2-bit operations. At the ISA level, three distinct MAC instructions\nare encoded extending the RISC-V ISA, and exposed up to the compiler level,\neach corresponding to a different mixed-precision operational mode. Our\nextensive experimental evaluation over widely used DNNs and datasets, such as\nCIFAR10 and ImageNet, demonstrates that our framework can achieve, on average,\n15x energy reduction for less than 1% accuracy loss and outperforms the\nISA-agnostic state-of-the-art RISC-V cores.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}