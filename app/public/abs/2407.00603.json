{"id":"2407.00603","title":"Hierarchical Memory for Long Video QA","authors":"Yiqin Wang, Haoji Zhang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng\n  Dai, Xiaojie Jin","authorsParsed":[["Wang","Yiqin",""],["Zhang","Haoji",""],["Tang","Yansong",""],["Liu","Yong",""],["Feng","Jiashi",""],["Dai","Jifeng",""],["Jin","Xiaojie",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 06:08:12 GMT"}],"updateDate":"2024-07-02","timestamp":1719727692000,"abstract":"  This paper describes our champion solution to the LOVEU Challenge @ CVPR'24,\nTrack 1 (Long Video VQA). Processing long sequences of visual tokens is\ncomputationally expensive and memory-intensive, making long video\nquestion-answering a challenging task. The key is to compress visual tokens\neffectively, reducing memory footprint and decoding latency, while preserving\nthe essential information for accurate question-answering. We adopt a\nhierarchical memory mechanism named STAR Memory, proposed in Flash-VStream,\nthat is capable of processing long videos with limited GPU memory (VRAM). We\nfurther utilize the video and audio data of MovieChat-1K training set to\nfine-tune the pretrained weight released by Flash-VStream, achieving 1st place\nin the challenge. Code is available at project homepage\nhttps://invinciblewyq.github.io/vstream-page\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}