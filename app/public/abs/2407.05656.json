{"id":"2407.05656","title":"Multi-label Learning with Random Circular Vectors","authors":"Ken Nishida, Kojiro Machi, Kazuma Onishi, Katsuhiko Hayashi, Hidetaka\n  Kamigaito","authorsParsed":[["Nishida","Ken",""],["Machi","Kojiro",""],["Onishi","Kazuma",""],["Hayashi","Katsuhiko",""],["Kamigaito","Hidetaka",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 06:29:46 GMT"}],"updateDate":"2024-07-09","timestamp":1720420186000,"abstract":"  The extreme multi-label classification~(XMC) task involves learning a\nclassifier that can predict from a large label set the most relevant subset of\nlabels for a data instance. While deep neural networks~(DNNs) have demonstrated\nremarkable success in XMC problems, the task is still challenging because it\nmust deal with a large number of output labels, which make the DNN training\ncomputationally expensive. This paper addresses the issue by exploring the use\nof random circular vectors, where each vector component is represented as a\ncomplex amplitude. In our framework, we can develop an output layer and loss\nfunction of DNNs for XMC by representing the final output layer as a fully\nconnected layer that directly predicts a low-dimensional circular vector\nencoding a set of labels for a data instance. We conducted experiments on\nsynthetic datasets to verify that circular vectors have better label encoding\ncapacity and retrieval ability than normal real-valued vectors. Then, we\nconducted experiments on actual XMC datasets and found that these appealing\nproperties of circular vectors contribute to significant improvements in task\nperformance compared with a previous model using random real-valued vectors,\nwhile reducing the size of the output layers by up to 99%.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}