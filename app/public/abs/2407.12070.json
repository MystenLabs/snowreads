{"id":"2407.12070","title":"Co-Designing Binarized Transformer and Hardware Accelerator for\n  Efficient End-to-End Edge Deployment","authors":"Yuhao Ji, Chao Fang, Shaobo Ma, Haikuo Shao, Zhongfeng Wang","authorsParsed":[["Ji","Yuhao",""],["Fang","Chao",""],["Ma","Shaobo",""],["Shao","Haikuo",""],["Wang","Zhongfeng",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 12:36:10 GMT"}],"updateDate":"2024-07-18","timestamp":1721133370000,"abstract":"  Transformer models have revolutionized AI tasks, but their large size hinders\nreal-world deployment on resource-constrained and latency-critical edge\ndevices. While binarized Transformers offer a promising solution by\nsignificantly reducing model size, existing approaches suffer from\nalgorithm-hardware mismatches with limited co-design exploration, leading to\nsuboptimal performance on edge devices. Hence, we propose a co-design method\nfor efficient end-to-end edge deployment of Transformers from three aspects:\nalgorithm, hardware, and joint optimization. First, we propose BMT, a novel\nhardware-friendly binarized Transformer with optimized quantization methods and\ncomponents, and we further enhance its model accuracy by leveraging the\nweighted ternary weight splitting training technique. Second, we develop a\nstreaming processor mixed binarized Transformer accelerator, namely BAT, which\nis equipped with specialized units and scheduling pipelines for efficient\ninference of binarized Transformers. Finally, we co-optimize the algorithm and\nhardware through a design space exploration approach to achieve a global\ntrade-off between accuracy, latency, and robustness for real-world deployments.\nExperimental results show our co-design achieves up to 2.14-49.37x throughput\ngains and 3.72-88.53x better energy efficiency over state-of-the-art\nTransformer accelerators, enabling efficient end-to-end edge deployment.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}