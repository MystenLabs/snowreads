{"id":"2407.20299","title":"Dataset Distillation for Offline Reinforcement Learning","authors":"Jonathan Light, Yuanzhe Liu, Ziniu Hu","authorsParsed":[["Light","Jonathan",""],["Liu","Yuanzhe",""],["Hu","Ziniu",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 04:02:17 GMT"},{"version":"v2","created":"Thu, 1 Aug 2024 01:33:48 GMT"}],"updateDate":"2024-08-02","timestamp":1722225737000,"abstract":"  Offline reinforcement learning often requires a quality dataset that we can\ntrain a policy on. However, in many situations, it is not possible to get such\na dataset, nor is it easy to train a policy to perform well in the actual\nenvironment given the offline data. We propose using data distillation to train\nand distill a better dataset which can then be used for training a better\npolicy model. We show that our method is able to synthesize a dataset where a\nmodel trained on it achieves similar performance to a model trained on the full\ndataset or a model trained using percentile behavioral cloning. Our project\nsite is available at\n$\\href{https://datasetdistillation4rl.github.io}{\\text{here}}$. We also provide\nour implementation at $\\href{https://github.com/ggflow123/DDRL}{\\text{this\nGitHub repository}}$.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}