{"id":"2408.11997","title":"Floating-Point Multiply-Add with Approximate Normalization for Low-Cost\n  Matrix Engines","authors":"Kosmas Alexandridis, Christodoulos Peltekis, Dionysios Filippas,\n  Giorgos Dimitrakopoulos","authorsParsed":[["Alexandridis","Kosmas",""],["Peltekis","Christodoulos",""],["Filippas","Dionysios",""],["Dimitrakopoulos","Giorgos",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 21:18:05 GMT"}],"updateDate":"2024-08-23","timestamp":1724275085000,"abstract":"  The widespread adoption of machine learning algorithms necessitates hardware\nacceleration to ensure efficient performance. This acceleration relies on\ncustom matrix engines that operate on full or reduced-precision floating-point\narithmetic. However, conventional floating-point implementations can be power\nhungry. This paper proposes a method to improve the energy efficiency of the\nmatrix engines used in machine learning algorithm acceleration. Our approach\nleverages approximate normalization within the floating-point multiply-add\nunits as a means to reduce their hardware complexity, without sacrificing\noverall machine-learning model accuracy. Hardware synthesis results show that\nthis technique reduces area and power consumption roughly by 16% and 13% on\naverage for Bfloat16 format. Also, the error introduced in transformer model\naccuracy is 1% on average, for the most efficient configuration of the proposed\napproach.\n","subjects":["Computing Research Repository/Hardware Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}