{"id":"2407.03942","title":"Diverse and Fine-Grained Instruction-Following Ability Exploration with\n  Synthetic Data","authors":"Zihui Gu, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Cheng-Zhong Xu, Ju\n  Fan","authorsParsed":[["Gu","Zihui",""],["Sun","Xingwu",""],["Lian","Fengzong",""],["Kang","Zhanhui",""],["Xu","Cheng-Zhong",""],["Fan","Ju",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 13:54:41 GMT"}],"updateDate":"2024-07-08","timestamp":1720101281000,"abstract":"  Instruction-following is particularly crucial for large language models\n(LLMs) to support diverse user requests. While existing work has made progress\nin aligning LLMs with human preferences, evaluating their capabilities on\ninstruction following remains a challenge due to complexity and diversity of\nreal-world user instructions. While existing evaluation methods focus on\ngeneral skills, they suffer from two main shortcomings, i.e., lack of\nfine-grained task-level evaluation and reliance on singular instruction\nexpression. To address these problems, this paper introduces DINGO, a\nfine-grained and diverse instruction-following evaluation dataset that has two\nmain advantages: (1) DINGO is based on a manual annotated, fine-grained and\nmulti-level category tree with 130 nodes derived from real-world user requests;\n(2) DINGO includes diverse instructions, generated by both GPT-4 and human\nexperts. Through extensive experiments, we demonstrate that DINGO can not only\nprovide more challenging and comprehensive evaluation for LLMs, but also\nprovide task-level fine-grained directions to further improve LLMs.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}