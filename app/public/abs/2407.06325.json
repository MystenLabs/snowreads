{"id":"2407.06325","title":"CONGO: Compressive Online Gradient Optimization with Application to\n  Microservices Management","authors":"Jeremy Carleton, Prathik Vijaykumar, Divyanshu Saxena, Dheeraj\n  Narasimha, Srinivas Shakkottai, Aditya Akella","authorsParsed":[["Carleton","Jeremy",""],["Vijaykumar","Prathik",""],["Saxena","Divyanshu",""],["Narasimha","Dheeraj",""],["Shakkottai","Srinivas",""],["Akella","Aditya",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 18:42:50 GMT"}],"updateDate":"2024-07-10","timestamp":1720464170000,"abstract":"  We address the challenge of online convex optimization where the objective\nfunction's gradient exhibits sparsity, indicating that only a small number of\ndimensions possess non-zero gradients. Our aim is to leverage this sparsity to\nobtain useful estimates of the objective function's gradient even when the only\ninformation available is a limited number of function samples. Our motivation\nstems from distributed queueing systems like microservices-based applications,\ncharacterized by request-response workloads. Here, each request type proceeds\nthrough a sequence of microservices to produce a response, and the resource\nallocation across the collection of microservices is controlled to balance\nend-to-end latency with resource costs. While the number of microservices is\nsubstantial, the latency function primarily reacts to resource changes in a\nfew, rendering the gradient sparse. Our proposed method, CONGO (Compressive\nOnline Gradient Optimization), combines simultaneous perturbation with\ncompressive sensing to estimate gradients. We establish analytical bounds on\nthe requisite number of compressive sensing samples per iteration to maintain\nbounded bias of gradient estimates, ensuring sub-linear regret. By exploiting\nsparsity, we reduce the samples required per iteration to match the gradient's\nsparsity, rather than the problem's original dimensionality. Numerical\nexperiments and real-world microservices benchmarks demonstrate CONGO's\nsuperiority over multiple stochastic gradient descent approaches, as it quickly\nconverges to performance comparable to policies pre-trained with workload\nawareness.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}