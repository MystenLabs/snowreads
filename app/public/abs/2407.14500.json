{"id":"2407.14500","title":"ViLLa: Video Reasoning Segmentation with Large Language Model","authors":"Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, Hengshuang\n  Zhao","authorsParsed":[["Zheng","Rongkun",""],["Qi","Lu",""],["Chen","Xi",""],["Wang","Yi",""],["Wang","Kun",""],["Qiao","Yu",""],["Zhao","Hengshuang",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:59:17 GMT"},{"version":"v2","created":"Mon, 29 Jul 2024 13:32:14 GMT"}],"updateDate":"2024-07-30","timestamp":1721325557000,"abstract":"  Although video perception models have made remarkable advancements in recent\nyears, they still heavily rely on explicit text descriptions or pre-defined\ncategories to identify target instances before executing video perception\ntasks. These models, however, fail to proactively comprehend and reason the\nuser's intentions via textual input. Even though previous works attempt to\ninvestigate solutions to incorporate reasoning with image segmentation, they\nfail to reason with videos due to the video's complexity in object motion. To\nbridge the gap between image and video, in this work, we propose a new video\nsegmentation task - video reasoning segmentation. The task is designed to\noutput tracklets of segmentation masks given a complex input text query. What's\nmore, to promote research in this unexplored area, we construct a reasoning\nvideo segmentation benchmark. Finally, we present ViLLa: Video reasoning\nsegmentation with a Large Language Model, which incorporates the language\ngeneration capabilities of multimodal Large Language Models (LLMs) while\nretaining the capabilities of detecting, segmenting, and tracking multiple\ninstances. We use a temporal-aware context aggregation module to incorporate\ncontextual visual cues to text embeddings and propose a video-frame decoder to\nbuild temporal correlations across segmentation tokens. Remarkably, our ViLLa\ndemonstrates capability in handling complex reasoning and referring video\nsegmentation. Also, our model shows impressive ability in different temporal\nunderstanding benchmarks. Both quantitative and qualitative experiments show\nour method effectively unlocks new video reasoning segmentation capabilities\nfor multimodal LLMs. The code and dataset will be available at\nhttps://github.com/rkzheng99/ViLLa.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}