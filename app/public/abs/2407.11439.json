{"id":"2407.11439","title":"Repurformer: Transformers for Repurposing-Aware Molecule Generation","authors":"Changhun Lee, Gyumin Lee","authorsParsed":[["Lee","Changhun",""],["Lee","Gyumin",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 07:16:13 GMT"}],"updateDate":"2024-07-17","timestamp":1721114173000,"abstract":"  Generating as diverse molecules as possible with desired properties is\ncrucial for drug discovery research, which invokes many approaches based on\ndeep generative models today. Despite recent advancements in these models,\nparticularly in variational autoencoders (VAEs), generative adversarial\nnetworks (GANs), Transformers, and diffusion models, a significant challenge\nknown as \\textit{the sample bias problem} remains. This problem occurs when\ngenerated molecules targeting the same protein tend to be structurally similar,\nreducing the diversity of generation. To address this, we propose leveraging\nmulti-hop relationships among proteins and compounds. Our model, Repurformer,\nintegrates bi-directional pretraining with Fast Fourier Transform (FFT) and\nlow-pass filtering (LPF) to capture complex interactions and generate diverse\nmolecules. A series of experiments on BindingDB dataset confirm that\nRepurformer successfully creates substitutes for anchor compounds that resemble\npositive compounds, increasing diversity between the anchor and generated\ncompounds.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Quantitative Biology/Biomolecules"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}