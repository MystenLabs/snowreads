{"id":"2407.13122","title":"MO-EMT-NAS: Multi-Objective Continuous Transfer of Architectural\n  Knowledge Between Tasks from Different Datasets","authors":"Peng Liao, XiLu Wang, Yaochu Jin, WenLi Du","authorsParsed":[["Liao","Peng",""],["Wang","XiLu",""],["Jin","Yaochu",""],["Du","WenLi",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 03:12:35 GMT"}],"updateDate":"2024-07-19","timestamp":1721272355000,"abstract":"  Deploying models across diverse devices demands tradeoffs among multiple\nobjectives due to different resource constraints. Arguably, due to the small\nmodel trap problem in multi-objective neural architecture search (MO-NAS) based\non a supernet, existing approaches may fail to maintain large models. Moreover,\nmulti-tasking neural architecture search (MT-NAS) excels in handling multiple\ntasks simultaneously, but most existing efforts focus on tasks from the same\ndataset, limiting their practicality in real-world scenarios where multiple\ntasks may come from distinct datasets. To tackle the above challenges, we\npropose a Multi-Objective Evolutionary Multi-Tasking framework for NAS\n(MO-EMT-NAS) to achieve architectural knowledge transfer across tasks from\ndifferent datasets while finding Pareto optimal architectures for\nmulti-objectives, model accuracy and computational efficiency. To alleviate the\nsmall model trap issue, we introduce an auxiliary objective that helps maintain\nmultiple larger models of similar accuracy. Moreover, the computational\nefficiency is further enhanced by parallelizing the training and validation of\nthe weight-sharing-based supernet. Experimental results on seven datasets with\ntwo, three, and four task combinations show that MO-EMT-NAS achieves a better\nminimum classification error while being able to offer flexible trade-offs\nbetween model performance and complexity, compared to the state-of-the-art\nsingle-objective MT-NAS algorithms. The runtime of MO-EMT-NAS is reduced by\n59.7% to 77.7%, compared to the corresponding multi-objective single-task\napproaches.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}