{"id":"2407.15894","title":"Craft: Cross-modal Aligned Features Improve Robustness of Prompt Tuning","authors":"Jingchen Sun, Rohan Sharma, Vishnu Suresh Lokhande, Changyou Chen","authorsParsed":[["Sun","Jingchen",""],["Sharma","Rohan",""],["Lokhande","Vishnu Suresh",""],["Chen","Changyou",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 03:51:16 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 02:16:11 GMT"}],"updateDate":"2024-07-26","timestamp":1721620276000,"abstract":"  Prompt Tuning has emerged as a prominent research paradigm for adapting\nvision-language models to various downstream tasks. However, recent research\nindicates that prompt tuning methods often lead to overfitting due to limited\ntraining samples. In this paper, we propose a Cross-modal Aligned Feature\nTuning (Craft) method to address this issue. Cross-modal alignment is conducted\nby first selecting anchors from the alternative domain and deriving relative\nrepresentations of the embeddings for the selected anchors. Optimizing for a\nfeature alignment loss over anchor-aligned text and image modalities creates a\nmore unified text-image common space. Overfitting in prompt tuning also\ndeteriorates model performance on out-of-distribution samples. To further\nimprove the prompt model's robustness, we propose minimizing Maximum Mean\nDiscrepancy (MMD) over the anchor-aligned feature spaces to mitigate domain\nshift. The experiment on four different prompt tuning structures consistently\nshows the improvement of our method, with increases of up to $6.1\\%$ in the\nBase-to-Novel generalization task, $5.8\\%$ in the group robustness task, and\n$2.7\\%$ in the out-of-distribution tasks. The code will be available at\nhttps://github.com/Jingchensun/Craft\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}