{"id":"2408.08059","title":"Maximally Permissive Reward Machines","authors":"Giovanni Varricchione, Natasha Alechina, Mehdi Dastani, Brian Logan","authorsParsed":[["Varricchione","Giovanni",""],["Alechina","Natasha",""],["Dastani","Mehdi",""],["Logan","Brian",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 09:59:26 GMT"}],"updateDate":"2024-08-16","timestamp":1723715966000,"abstract":"  Reward machines allow the definition of rewards for temporally extended tasks\nand behaviors. Specifying \"informative\" reward machines can be challenging. One\nway to address this is to generate reward machines from a high-level abstract\ndescription of the learning environment, using techniques such as AI planning.\nHowever, previous planning-based approaches generate a reward machine based on\na single (sequential or partial-order) plan, and do not allow maximum\nflexibility to the learning agent. In this paper we propose a new approach to\nsynthesising reward machines which is based on the set of partial order plans\nfor a goal. We prove that learning using such \"maximally permissive\" reward\nmachines results in higher rewards than learning using RMs based on a single\nplan. We present experimental results which support our theoretical claims by\nshowing that our approach obtains higher rewards than the single-plan approach\nin practice.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Et2NxszZO48xqTuBGx2crqSh8bDtijP2GJGm_5nkIO8","pdfSize":"649510"}
