{"id":"2408.17225","title":"Adaptive Growing Randomized Neural Networks for Solving Partial\n  Differential Equations","authors":"Haoning Dang and Fei Wang","authorsParsed":[["Dang","Haoning",""],["Wang","Fei",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 12:07:09 GMT"}],"updateDate":"2024-09-02","timestamp":1725019629000,"abstract":"  Randomized neural network (RNN) methods have been proposed for solving\nvarious partial differential equations (PDEs), demonstrating high accuracy and\nefficiency. However, initializing the fixed parameters remains a challenging\nissue. Additionally, RNNs often struggle to solve PDEs with sharp or\ndiscontinuous solutions. In this paper, we propose a novel approach called\nAdaptive Growing Randomized Neural Network (AG-RNN) to address these\nchallenges. First, we establish a parameter initialization strategy based on\nfrequency information to construct the initial RNN. After obtaining a numerical\nsolution from this initial network, we use the residual as an error indicator.\nBased on the error indicator, we introduce growth strategies that expand the\nneural network, making it wider and deeper to improve the accuracy of the\nnumerical solution. A key feature of AG-RNN is its adaptive strategy for\ndetermining the weights and biases of newly added neurons, enabling the network\nto expand in both width and depth without requiring additional training.\nInstead, all weights and biases are generated constructively, significantly\nenhancing the network's approximation capabilities compared to conventional\nrandomized neural network methods. In addition, a domain splitting strategy is\nintroduced to handle the case of discontinuous solutions. Extensive numerical\nexperiments are conducted to demonstrate the efficiency and accuracy of this\ninnovative approach.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Numerical Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}