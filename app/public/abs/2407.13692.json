{"id":"2407.13692","title":"Prover-Verifier Games improve legibility of LLM outputs","authors":"Jan Hendrik Kirchner, Yining Chen, Harri Edwards, Jan Leike, Nat\n  McAleese, Yuri Burda","authorsParsed":[["Kirchner","Jan Hendrik",""],["Chen","Yining",""],["Edwards","Harri",""],["Leike","Jan",""],["McAleese","Nat",""],["Burda","Yuri",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 16:58:18 GMT"},{"version":"v2","created":"Thu, 1 Aug 2024 17:18:54 GMT"}],"updateDate":"2024-08-02","timestamp":1721321898000,"abstract":"  One way to increase confidence in the outputs of Large Language Models (LLMs)\nis to support them with reasoning that is clear and easy to check -- a property\nwe call legibility. We study legibility in the context of solving grade-school\nmath problems and show that optimizing chain-of-thought solutions only for\nanswer correctness can make them less legible. To mitigate the loss in\nlegibility, we propose a training algorithm inspired by Prover-Verifier Game\nfrom Anil et al. (2021). Our algorithm iteratively trains small verifiers to\npredict solution correctness, \"helpful\" provers to produce correct solutions\nthat the verifier accepts, and \"sneaky\" provers to produce incorrect solutions\nthat fool the verifier. We find that the helpful prover's accuracy and the\nverifier's robustness to adversarial attacks increase over the course of\ntraining. Furthermore, we show that legibility training transfers to\ntime-constrained humans tasked with verifying solution correctness. Over course\nof LLM training human accuracy increases when checking the helpful prover's\nsolutions, and decreases when checking the sneaky prover's solutions. Hence,\ntraining for checkability by small verifiers is a plausible technique for\nincreasing output legibility. Our results suggest legibility training against\nsmall verifiers as a practical avenue for increasing legibility of large LLMs\nto humans, and thus could help with alignment of superhuman models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}