{"id":"2407.14845","title":"Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models","authors":"Ze Yu Zhang, Arun Verma, Finale Doshi-Velez, Bryan Kian Hsiang Low","authorsParsed":[["Zhang","Ze Yu",""],["Verma","Arun",""],["Doshi-Velez","Finale",""],["Low","Bryan Kian Hsiang",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 11:19:58 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 02:23:12 GMT"}],"updateDate":"2024-08-23","timestamp":1721474398000,"abstract":"  Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real datasets validate our\nproposed model.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}