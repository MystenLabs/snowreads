{"id":"2408.16009","title":"Ranking evaluation metrics from a group-theoretic perspective","authors":"Chiara Balestra, Andreas Mayr, Emmanuel M\\\"uller","authorsParsed":[["Balestra","Chiara",""],["Mayr","Andreas",""],["MÃ¼ller","Emmanuel",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 09:06:58 GMT"}],"updateDate":"2024-08-30","timestamp":1723626418000,"abstract":"  Confronted with the challenge of identifying the most suitable metric to\nvalidate the merits of newly proposed models, the decision-making process is\nanything but straightforward. Given that comparing rankings introduces its own\nset of formidable challenges and the likely absence of a universal metric\napplicable to all scenarios, the scenario does not get any better. Furthermore,\nmetrics designed for specific contexts, such as for Recommender Systems,\nsometimes extend to other domains without a comprehensive grasp of their\nunderlying mechanisms, resulting in unforeseen outcomes and potential misuses.\nComplicating matters further, distinct metrics may emphasize different aspects\nof rankings, frequently leading to seemingly contradictory comparisons of model\nresults and hindering the trustworthiness of evaluations.\n  We unveil these aspects in the domain of ranking evaluation metrics. Firstly,\nwe show instances resulting in inconsistent evaluations, sources of potential\nmistrust in commonly used metrics; by quantifying the frequency of such\ndisagreements, we prove that these are common in rankings. Afterward, we\nconceptualize rankings using the mathematical formalism of symmetric groups\ndetaching from possible domains where the metrics have been created; through\nthis approach, we can rigorously and formally establish essential mathematical\nproperties for ranking evaluation metrics, essential for a deeper comprehension\nof the source of inconsistent evaluations. We conclude with a discussion,\nconnecting our theoretical analysis to the practical applications, highlighting\nwhich properties are important in each domain where rankings are commonly\nevaluated. In conclusion, our analysis sheds light on ranking evaluation\nmetrics, highlighting that inconsistent evaluations should not be seen as a\nsource of mistrust but as the need to carefully choose how to evaluate our\nmodels in the future.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}