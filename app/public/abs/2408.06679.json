{"id":"2408.06679","title":"Case-based Explainability for Random Forest: Prototypes, Critics,\n  Counter-factuals and Semi-factuals","authors":"Gregory Yampolsky, Dhruv Desai, Mingshu Li, Stefano Pasquali, Dhagash\n  Mehta","authorsParsed":[["Yampolsky","Gregory",""],["Desai","Dhruv",""],["Li","Mingshu",""],["Pasquali","Stefano",""],["Mehta","Dhagash",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 07:08:54 GMT"}],"updateDate":"2024-08-14","timestamp":1723532934000,"abstract":"  The explainability of black-box machine learning algorithms, commonly known\nas Explainable Artificial Intelligence (XAI), has become crucial for financial\nand other regulated industrial applications due to regulatory requirements and\nthe need for transparency in business practices. Among the various paradigms of\nXAI, Explainable Case-Based Reasoning (XCBR) stands out as a pragmatic approach\nthat elucidates the output of a model by referencing actual examples from the\ndata used to train or test the model. Despite its potential, XCBR has been\nrelatively underexplored for many algorithms such as tree-based models until\nrecently. We start by observing that most XCBR methods are defined based on the\ndistance metric learned by the algorithm. By utilizing a recently proposed\ntechnique to extract the distance metric learned by Random Forests (RFs), which\nis both geometry- and accuracy-preserving, we investigate various XCBR methods.\nThese methods amount to identify special points from the training datasets,\nsuch as prototypes, critics, counter-factuals, and semi-factuals, to explain\nthe predictions for a given query of the RF. We evaluate these special points\nusing various evaluation metrics to assess their explanatory power and\neffectiveness.\n","subjects":["Computing Research Repository/Machine Learning","Quantitative Finance/Statistical Finance","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}