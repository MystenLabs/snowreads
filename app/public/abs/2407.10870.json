{"id":"2407.10870","title":"GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via\n  VLM","authors":"Keshav Bimbraw, Ye Wang, Jing Liu, Toshiaki Koike-Akino","authorsParsed":[["Bimbraw","Keshav",""],["Wang","Ye",""],["Liu","Jing",""],["Koike-Akino","Toshiaki",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 16:18:06 GMT"}],"updateDate":"2024-07-16","timestamp":1721060286000,"abstract":"  Large vision-language models (LVLMs), such as the Generative Pre-trained\nTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models which\nhave great potential as powerful artificial-intelligence (AI) assistance tools\nfor a myriad of applications, including healthcare, industrial, and academic\nsectors. Although such foundation models perform well in a wide range of\ngeneral tasks, their capability without fine-tuning is often limited in\nspecialized tasks. However, full fine-tuning of large foundation models is\nchallenging due to enormous computation/memory/dataset requirements. We show\nthat GPT-4o can decode hand gestures from forearm ultrasound data even with no\nfine-tuning, and improves with few-shot, in-context learning.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}