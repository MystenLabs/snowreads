{"id":"2408.13836","title":"PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in\n  Multi-Modal Medical Images","authors":"Zifan Chen, Xinyu Nan, Jiazheng Li, Jie Zhao, Haifeng Li, Zilin Lin,\n  Haoshen Li, Heyun Chen, Yiting Liu, Bin Dong, Li Zhang, Lei Tang","authorsParsed":[["Chen","Zifan",""],["Nan","Xinyu",""],["Li","Jiazheng",""],["Zhao","Jie",""],["Li","Haifeng",""],["Lin","Zilin",""],["Li","Haoshen",""],["Chen","Heyun",""],["Liu","Yiting",""],["Dong","Bin",""],["Zhang","Li",""],["Tang","Lei",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 13:42:47 GMT"}],"updateDate":"2024-08-27","timestamp":1724593367000,"abstract":"  Volumetric segmentation is crucial for medical imaging but is often\nconstrained by labor-intensive manual annotations and the need for\nscenario-specific model training. Furthermore, existing general segmentation\nmodels are inefficient due to their design and inferential approaches.\nAddressing this clinical demand, we introduce PropSAM, a propagation-based\nsegmentation model that optimizes the use of 3D medical structure information.\nPropSAM integrates a CNN-based UNet for intra-slice processing with a\nTransformer-based module for inter-slice propagation, focusing on structural\nand semantic continuities to enhance segmentation across various modalities.\nDistinctively, PropSAM operates on a one-view prompt, such as a 2D bounding box\nor sketch mask, unlike conventional models that require two-view prompts. It\nhas demonstrated superior performance, significantly improving the Dice\nSimilarity Coefficient (DSC) across 44 medical datasets and various imaging\nmodalities, outperforming models like MedSAM and SegVol with an average DSC\nimprovement of 18.1%. PropSAM also maintains stable predictions despite prompt\ndeviations and varying propagation configurations, confirmed by one-way ANOVA\ntests with P>0.5985 and P>0.6131, respectively. Moreover, PropSAM's efficient\narchitecture enables faster inference speeds (Wilcoxon rank-sum test, P<0.001)\nand reduces user interaction time by 37.8% compared to two-view prompt models.\nIts ability to handle irregular and complex objects with robust performance\nfurther demonstrates its potential in clinical settings, facilitating more\nautomated and reliable medical imaging analyses with minimal retraining.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}