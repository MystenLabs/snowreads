{"id":"2407.09704","title":"What an Elegant Bridge: Multilingual LLMs are Biased Similarly in\n  Different Languages","authors":"Viktor Mihaylov, Aleksandar Shtedritski","authorsParsed":[["Mihaylov","Viktor",""],["Shtedritski","Aleksandar",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 22:10:16 GMT"}],"updateDate":"2024-07-16","timestamp":1720822216000,"abstract":"  This paper investigates biases of Large Language Models (LLMs) through the\nlens of grammatical gender. Drawing inspiration from seminal works in\npsycholinguistics, particularly the study of gender's influence on language\nperception, we leverage multilingual LLMs to revisit and expand upon the\nfoundational experiments of Boroditsky (2003). Employing LLMs as a novel method\nfor examining psycholinguistic biases related to grammatical gender, we prompt\na model to describe nouns with adjectives in various languages, focusing\nspecifically on languages with grammatical gender. In particular, we look at\nadjective co-occurrences across gender and languages, and train a binary\nclassifier to predict grammatical gender given adjectives an LLM uses to\ndescribe a noun. Surprisingly, we find that a simple classifier can not only\npredict noun gender above chance but also exhibit cross-language\ntransferability. We show that while LLMs may describe words differently in\ndifferent languages, they are biased similarly.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}