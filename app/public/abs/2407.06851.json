{"id":"2407.06851","title":"Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders","authors":"Jinseok Kim, Jaewon Jung, Sangyeop Kim, Sohyung Park, Sungzoon Cho","authorsParsed":[["Kim","Jinseok",""],["Jung","Jaewon",""],["Kim","Sangyeop",""],["Park","Sohyung",""],["Cho","Sungzoon",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 13:35:54 GMT"}],"updateDate":"2024-07-10","timestamp":1720532154000,"abstract":"  Despite the impressive capabilities of Large Language Models (LLMs) in\nvarious tasks, their vulnerability to unsafe prompts remains a critical issue.\nThese prompts can lead LLMs to generate responses on illegal or sensitive\ntopics, posing a significant threat to their safe and ethical use. Existing\napproaches attempt to address this issue using classification models, but they\nhave several drawbacks. With the increasing complexity of unsafe prompts,\nsimilarity search-based techniques that identify specific features of unsafe\nprompts provide a more robust and effective solution to this evolving problem.\nThis paper investigates the potential of sentence encoders to distinguish safe\nfrom unsafe prompts, and the ability to classify various unsafe prompts\naccording to a safety taxonomy. We introduce new pairwise datasets and the\nCategorical Purity (CP) metric to measure this capability. Our findings reveal\nboth the effectiveness and limitations of existing sentence encoders, proposing\ndirections to improve sentence encoders to operate as more robust safety\ndetectors. Our code is available at https://github.com/JwdanielJung/Safe-Embed.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}