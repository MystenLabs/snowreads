{"id":"2407.13174","title":"Compressed models are NOT miniature versions of large models","authors":"Rohit Raj Rai, Rishant Pal, Amit Awekar","authorsParsed":[["Rai","Rohit Raj",""],["Pal","Rishant",""],["Awekar","Amit",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 05:28:50 GMT"}],"updateDate":"2024-07-19","timestamp":1721280530000,"abstract":"  Large neural models are often compressed before deployment. Model compression\nis necessary for many practical reasons, such as inference latency, memory\nfootprint, and energy consumption. Compressed models are assumed to be\nminiature versions of corresponding large neural models. However, we question\nthis belief in our work. We compare compressed models with corresponding large\nneural models using four model characteristics: prediction errors, data\nrepresentation, data distribution, and vulnerability to adversarial attack. We\nperform experiments using the BERT-large model and its five compressed\nversions. For all four model characteristics, compressed models significantly\ndiffer from the BERT-large model. Even among compressed models, they differ\nfrom each other on all four model characteristics. Apart from the expected loss\nin model performance, there are major side effects of using compressed models\nto replace large neural models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}