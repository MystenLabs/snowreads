{"id":"2407.07080","title":"Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary\n  and Instruction Capabilities","authors":"Shaltiel Shmidman, Avi Shmidman, Amir DN Cohen, Moshe Koppel","authorsParsed":[["Shmidman","Shaltiel",""],["Shmidman","Avi",""],["Cohen","Amir DN",""],["Koppel","Moshe",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 17:51:37 GMT"}],"updateDate":"2024-07-10","timestamp":1720547497000,"abstract":"  Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}