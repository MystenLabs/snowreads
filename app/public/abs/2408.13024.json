{"id":"2408.13024","title":"Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding","authors":"Xianqiang Gao, Pingrui Zhang, Delin Qu, Dong Wang, Zhigang Wang, Yan\n  Ding, Bin Zhao, Xuelong Li","authorsParsed":[["Gao","Xianqiang",""],["Zhang","Pingrui",""],["Qu","Delin",""],["Wang","Dong",""],["Wang","Zhigang",""],["Ding","Yan",""],["Zhao","Bin",""],["Li","Xuelong",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 12:27:33 GMT"}],"updateDate":"2024-08-26","timestamp":1724416053000,"abstract":"  3D Object Affordance Grounding aims to predict the functional regions on a 3D\nobject and has laid the foundation for a wide range of applications in\nrobotics. Recent advances tackle this problem via learning a mapping between 3D\nregions and a single human-object interaction image. However, the geometric\nstructure of the 3D object and the object in the human-object interaction image\nare not always consistent, leading to poor generalization. To address this\nissue, we propose to learn generalizable invariant affordance knowledge from\nmultiple human-object interaction images within the same affordance category.\nSpecifically, we introduce the \\textbf{M}ulti-\\textbf{I}mage Guided\nInvariant-\\textbf{F}eature-Aware 3D \\textbf{A}ffordance \\textbf{G}rounding\n(\\textbf{MIFAG}) framework. It grounds 3D object affordance regions by\nidentifying common interaction patterns across multiple human-object\ninteraction images. First, the Invariant Affordance Knowledge Extraction Module\n(\\textbf{IAM}) utilizes an iterative updating strategy to gradually extract\naligned affordance knowledge from multiple images and integrate it into an\naffordance dictionary. Then, the Affordance Dictionary Adaptive Fusion Module\n(\\textbf{ADM}) learns comprehensive point cloud representations that consider\nall affordance candidates in multiple images. Besides, the Multi-Image and\nPoint Affordance (\\textbf{MIPA}) benchmark is constructed and our method\noutperforms existing state-of-the-art methods on various experimental\ncomparisons. Project page: \\url{https://goxq.github.io/mifag}\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}