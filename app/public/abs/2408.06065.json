{"id":"2408.06065","title":"An Investigation Into Explainable Audio Hate Speech Detection","authors":"Jinmyeong An, Wonjun Lee, Yejin Jeon, Jungseul Ok, Yunsu Kim and Gary\n  Geunbae Lee","authorsParsed":[["An","Jinmyeong",""],["Lee","Wonjun",""],["Jeon","Yejin",""],["Ok","Jungseul",""],["Kim","Yunsu",""],["Lee","Gary Geunbae",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 11:32:34 GMT"}],"updateDate":"2024-08-13","timestamp":1723462354000,"abstract":"  Research on hate speech has predominantly revolved around detection and\ninterpretation from textual inputs, leaving verbal content largely unexplored.\nWhile there has been limited exploration into hate speech detection within\nverbal acoustic speech inputs, the aspect of interpretability has been\noverlooked. Therefore, we introduce a new task of explainable audio hate speech\ndetection. Specifically, we aim to identify the precise time intervals,\nreferred to as audio frame-level rationales, which serve as evidence for hate\nspeech classification. Towards this end, we propose two different approaches:\ncascading and End-to-End (E2E). The cascading approach initially converts audio\nto transcripts, identifies hate speech within these transcripts, and\nsubsequently locates the corresponding audio time frames. Conversely, the E2E\napproach processes audio utterances directly, which allows it to pinpoint hate\nspeech within specific time frames. Additionally, due to the lack of\nexplainable audio hate speech datasets that include audio frame-level\nrationales, we curated a synthetic audio dataset to train our models. We\nfurther validated these models on actual human speech utterances and found that\nthe E2E approach outperforms the cascading method in terms of the audio frame\nIntersection over Union (IoU) metric. Furthermore, we observed that including\nframe-level rationales significantly enhances hate speech detection accuracy\nfor the E2E approach.\n  \\textbf{Disclaimer} The reader may encounter content of an offensive or\nhateful nature. However, given the nature of the work, this cannot be avoided.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}