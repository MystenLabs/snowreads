{"id":"2408.15241","title":"GenRec: Unifying Video Generation and Recognition with Diffusion Models","authors":"Zejia Weng, Xitong Yang, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang","authorsParsed":[["Weng","Zejia",""],["Yang","Xitong",""],["Xing","Zhen",""],["Wu","Zuxuan",""],["Jiang","Yu-Gang",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 17:59:41 GMT"}],"updateDate":"2024-08-28","timestamp":1724781581000,"abstract":"  Video diffusion models are able to generate high-quality videos by learning\nstrong spatial-temporal priors on large-scale datasets. In this paper, we aim\nto investigate whether such priors derived from a generative process are\nsuitable for video recognition, and eventually joint optimization of generation\nand recognition. Building upon Stable Video Diffusion, we introduce GenRec, the\nfirst unified framework trained with a random-frame conditioning process so as\nto learn generalized spatial-temporal representations. The resulting framework\ncan naturally supports generation and recognition, and more importantly is\nrobust even when visual inputs contain limited information. Extensive\nexperiments demonstrate the efficacy of GenRec for both recognition and\ngeneration. In particular, GenRec achieves competitive recognition performance,\noffering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also\nperforms the best class-conditioned image-to-video generation results,\nachieving 46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore,\nGenRec demonstrates extraordinary robustness in scenarios that only limited\nframes can be observed.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}