{"id":"2408.08453","title":"CRQBench: A Benchmark of Code Reasoning Questions","authors":"Elizabeth Dinella, Satish Chandra, and Petros Maniatis","authorsParsed":[["Dinella","Elizabeth",""],["Chandra","Satish",""],["Maniatis","Petros",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 23:30:47 GMT"}],"updateDate":"2024-08-19","timestamp":1723764647000,"abstract":"  Large Language Models have demonstrated exceptional proficiency on coding\ntasks, but it is challenging to precisely evaluate their code reasoning\nability. Existing benchmarks are insufficient as they are unrealistic and\nconflate semantic reasoning ability with performance on software engineering\ntasks. We introduce CRQBench, a benchmark of 100 C++ code reasoning questions\nand answers derived from contextualized code review comments. To curate\nCRQBench, we use an LLM assistant alongside human inspection, reducing manual\neffort. We conduct an evaluation of GPT-4 on CRQBench and find that it produces\ncorrect responses grounded in the given context for 65 of the 100 questions.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Ezovekya2HhCfDG7O3IXa2pZLLdhHVQ9257xBCwQieg","pdfSize":"4410971"}
