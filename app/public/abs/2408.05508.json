{"id":"2408.05508","title":"PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer\n  Architecture","authors":"Qiang Zheng, Chao Zhang, Jian Sun","authorsParsed":[["Zheng","Qiang",""],["Zhang","Chao",""],["Sun","Jian",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 10:16:03 GMT"},{"version":"v2","created":"Mon, 16 Sep 2024 16:44:58 GMT"}],"updateDate":"2024-09-17","timestamp":1723284963000,"abstract":"  In recent years, point cloud analysis methods based on the Transformer\narchitecture have made significant progress, particularly in the context of\nmultimedia applications such as 3D modeling, virtual reality, and autonomous\nsystems. However, the high computational resource demands of the Transformer\narchitecture hinder its scalability, real-time processing capabilities, and\ndeployment on mobile devices and other platforms with limited computational\nresources. This limitation remains a significant obstacle to its practical\napplication in scenarios requiring on-device intelligence and multimedia\nprocessing. To address this challenge, we propose an efficient point cloud\nanalysis architecture, \\textbf{Point} \\textbf{M}LP-\\textbf{T}ransformer\n(PointMT). This study tackles the quadratic complexity of the self-attention\nmechanism by introducing a linear complexity local attention mechanism for\neffective feature aggregation. Additionally, to counter the Transformer's focus\non token differences while neglecting channel differences, we introduce a\nparameter-free channel temperature adaptation mechanism that adaptively adjusts\nthe attention weight distribution in each channel, enhancing the precision of\nfeature aggregation. To improve the Transformer's slow convergence speed due to\nthe limited scale of point cloud datasets, we propose an MLP-Transformer hybrid\nmodule, which significantly enhances the model's convergence speed.\nFurthermore, to boost the feature representation capability of point tokens, we\nrefine the classification head, enabling point tokens to directly participate\nin prediction. Experimental results on multiple evaluation benchmarks\ndemonstrate that PointMT achieves performance comparable to state-of-the-art\nmethods while maintaining an optimal balance between performance and accuracy.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}