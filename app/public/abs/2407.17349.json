{"id":"2407.17349","title":"Boosting Large Language Models with Socratic Method for Conversational\n  Mathematics Teaching","authors":"Yuyang Ding and Hanglei Hu and Jie Zhou and Qin Chen and Bo Jiang and\n  Liang He","authorsParsed":[["Ding","Yuyang",""],["Hu","Hanglei",""],["Zhou","Jie",""],["Chen","Qin",""],["Jiang","Bo",""],["He","Liang",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 15:18:17 GMT"}],"updateDate":"2024-07-25","timestamp":1721834297000,"abstract":"  With the introduction of large language models (LLMs), automatic math\nreasoning has seen tremendous success. However, current methods primarily focus\non providing solutions or using techniques like Chain-of-Thought to enhance\nproblem-solving accuracy. In this paper, we focus on improving the capability\nof mathematics teaching via a Socratic teaching-based LLM\n(\\texttt{SocraticLLM}), which guides learners toward profound thinking with\nclarity and self-discovery via conversation. We collect and release a\nhigh-quality mathematical teaching dataset, named \\texttt{SocraticMATH}, which\nprovides Socratic-style conversations of problems with extra knowledge. Also,\nwe propose a knowledge-enhanced LLM as a strong baseline to generate reliable\nresponses with review, guidance/heuristic, rectification, and summarization.\nExperimental results show the great advantages of \\texttt{SocraticLLM} by\ncomparing it with several strong generative models. The codes and datasets are\navailable on \\url{https://github.com/ECNU-ICALK/SocraticMath}.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}