{"id":"2407.20266","title":"Accelerating the Low-Rank Decomposed Models","authors":"Habib Hajimolahoseini, Walid Ahmed, Austin Wen, Yang Liu","authorsParsed":[["Hajimolahoseini","Habib",""],["Ahmed","Walid",""],["Wen","Austin",""],["Liu","Yang",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 20:26:58 GMT"}],"updateDate":"2024-07-31","timestamp":1721852818000,"abstract":"  Tensor decomposition is a mathematically supported technique for data\ncompression. It consists of applying some kind of a Low Rank Decomposition\ntechnique on the tensors or matrices in order to reduce the redundancy of the\ndata. However, it is not a popular technique for compressing the AI models duo\nto the high number of new layers added to the architecture after decomposition.\nAlthough the number of parameters could shrink significantly, it could result\nin the model be more than twice deeper which could add some latency to the\ntraining or inference. In this paper, we present a comprehensive study about\nhow to modify low rank decomposition technique in AI models so that we could\nbenefit from both high accuracy and low memory consumption as well as speeding\nup the training and inference\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"_MziYFd23taO2jB1kBnjRytx14ip5vm8rxRcQi2nGbk","pdfSize":"795423","objectId":"0x6793bba034a5ddc9c050db832e937a396d69b651a1d6292ccb6a4b9f91260800","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
