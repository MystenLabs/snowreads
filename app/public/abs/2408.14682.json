{"id":"2408.14682","title":"Detecting Interpretable Subgroup Drifts","authors":"Flavio Giobergia, Eliana Pastor, Luca de Alfaro, Elena Baralis","authorsParsed":[["Giobergia","Flavio",""],["Pastor","Eliana",""],["de Alfaro","Luca",""],["Baralis","Elena",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 23:13:38 GMT"}],"updateDate":"2024-08-28","timestamp":1724714018000,"abstract":"  The ability to detect and adapt to changes in data distributions is crucial\nto maintain the accuracy and reliability of machine learning models. Detection\nis generally approached by observing the drift of model performance from a\nglobal point of view. However, drifts occurring in (fine-grained) data\nsubgroups may go unnoticed when monitoring global drift. We take a different\nperspective, and introduce methods for observing drift at the finer granularity\nof subgroups. Relevant data subgroups are identified during training and\nmonitored efficiently throughout the model's life. Performance drifts in any\nsubgroup are detected, quantified and characterized so as to provide an\ninterpretable summary of the model behavior over time. Experimental results\nconfirm that our subgroup-level drift analysis identifies drifts that do not\nshow at the (coarser) global dataset level. The proposed approach provides a\nvaluable tool for monitoring model performance in dynamic real-world\napplications, offering insights into the evolving nature of data and ultimately\ncontributing to more robust and adaptive models.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}