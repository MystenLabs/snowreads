{"id":"2407.02119","title":"Cost-Effective Proxy Reward Model Construction with On-Policy and Active\n  Learning","authors":"Yifang Chen, Shuohang Wang, Ziyi Yang, Hiteshi Sharma, Nikos\n  Karampatziakis, Donghan Yu, Kevin Jamieson, Simon Shaolei Du, Yelong Shen","authorsParsed":[["Chen","Yifang",""],["Wang","Shuohang",""],["Yang","Ziyi",""],["Sharma","Hiteshi",""],["Karampatziakis","Nikos",""],["Yu","Donghan",""],["Jamieson","Kevin",""],["Du","Simon Shaolei",""],["Shen","Yelong",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 10:09:19 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 08:24:06 GMT"}],"updateDate":"2024-07-10","timestamp":1719914959000,"abstract":"  Reinforcement learning with human feedback (RLHF), as a widely adopted\napproach in current large language model pipelines, is \\textit{bottlenecked by\nthe size of human preference data}. While traditional methods rely on offline\npreference dataset constructions, recent approaches have shifted towards online\nsettings, where a learner uses a small amount of labeled seed data and a large\npool of unlabeled prompts to iteratively construct new preference data through\nself-generated responses and high-quality reward/preference feedback. However,\nmost current online algorithms still focus on preference labeling during policy\nmodel updating with given feedback oracles, which incurs significant expert\nquery costs. \\textit{We are the first to explore cost-effective proxy reward\noracles construction strategies for further labeling preferences or rewards\nwith extremely limited labeled data and expert query budgets}. Our approach\nintroduces two key innovations: (1) on-policy query to avoid OOD and imbalance\nissues in seed data, and (2) active learning to select the most informative\ndata for preference queries. Using these methods, we train a evaluation model\nwith minimal expert-labeled data, which then effectively labels nine times more\npreference pairs for further RLHF training. For instance, our model using\nDirect Preference Optimization (DPO) gains around over 1% average improvement\non AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our\nmethodology is orthogonal to other direct expert query-based strategies and\ntherefore might be integrated with them to further reduce query costs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}