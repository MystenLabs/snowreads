{"id":"2408.01394","title":"Improving Multilingual Neural Machine Translation by Utilizing Semantic\n  and Linguistic Features","authors":"Mengyu Bu, Shuhao Gu, Yang Feng","authorsParsed":[["Bu","Mengyu",""],["Gu","Shuhao",""],["Feng","Yang",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 17:10:12 GMT"}],"updateDate":"2024-08-05","timestamp":1722618612000,"abstract":"  The many-to-many multilingual neural machine translation can be regarded as\nthe process of integrating semantic features from the source sentences and\nlinguistic features from the target sentences. To enhance zero-shot\ntranslation, models need to share knowledge across languages, which can be\nachieved through auxiliary tasks for learning a universal representation or\ncross-lingual mapping. To this end, we propose to exploit both semantic and\nlinguistic features between multiple languages to enhance multilingual\ntranslation. On the encoder side, we introduce a disentangling learning task\nthat aligns encoder representations by disentangling semantic and linguistic\nfeatures, thus facilitating knowledge transfer while preserving complete\ninformation. On the decoder side, we leverage a linguistic encoder to integrate\nlow-level linguistic features to assist in the target language generation.\nExperimental results on multilingual datasets demonstrate significant\nimprovement in zero-shot translation compared to the baseline system, while\nmaintaining performance in supervised translation. Further analysis validates\nthe effectiveness of our method in leveraging both semantic and linguistic\nfeatures. The code is available at https://github.com/ictnlp/SemLing-MNMT.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"t2pZdQ9BzJzqTldC_MmLQIXuJX7dipICxCPmJUPfh9Q","pdfSize":"1260029","txDigest":"FTUdgHfs2dgz5VwJE9EeLAMGtnh8aDmdXy28bPuN9pp7","endEpoch":"1","status":"CERTIFIED"}
