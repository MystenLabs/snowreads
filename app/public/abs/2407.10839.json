{"id":"2407.10839","title":"Offline Reinforcement Learning with Imputed Rewards","authors":"Carlo Romeo and Andrew D. Bagdanov","authorsParsed":[["Romeo","Carlo",""],["Bagdanov","Andrew D.",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:53:13 GMT"}],"updateDate":"2024-07-16","timestamp":1721058793000,"abstract":"  Offline Reinforcement Learning (ORL) offers a robust solution to training\nagents in applications where interactions with the environment must be strictly\nlimited due to cost, safety, or lack of accurate simulation environments.\nDespite its potential to facilitate deployment of artificial agents in the real\nworld, Offline Reinforcement Learning typically requires very many\ndemonstrations annotated with ground-truth rewards. Consequently,\nstate-of-the-art ORL algorithms can be difficult or impossible to apply in\ndata-scarce scenarios. In this paper we propose a simple but effective Reward\nModel that can estimate the reward signal from a very limited sample of\nenvironment transitions annotated with rewards. Once the reward signal is\nmodeled, we use the Reward Model to impute rewards for a large sample of\nreward-free transitions, thus enabling the application of ORL techniques. We\ndemonstrate the potential of our approach on several D4RL continuous locomotion\ntasks. Our results show that, using only 1\\% of reward-labeled transitions from\nthe original datasets, our learned reward model is able to impute rewards for\nthe remaining 99\\% of the transitions, from which performant agents can be\nlearned using Offline Reinforcement Learning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}