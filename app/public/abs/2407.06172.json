{"id":"2407.06172","title":"On Speeding Up Language Model Evaluation","authors":"Jin Peng Zhou, Christian K. Belardi, Ruihan Wu, Travis Zhang, Carla P.\n  Gomes, Wen Sun, Kilian Q. Weinberger","authorsParsed":[["Zhou","Jin Peng",""],["Belardi","Christian K.",""],["Wu","Ruihan",""],["Zhang","Travis",""],["Gomes","Carla P.",""],["Sun","Wen",""],["Weinberger","Kilian Q.",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:48:42 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 22:31:35 GMT"}],"updateDate":"2024-08-16","timestamp":1720460922000,"abstract":"  Developing prompt-based methods with Large Language Models (LLMs) requires\nmaking numerous decisions, which give rise to a combinatorial search problem.\nFor example, selecting the right pre-trained LLM, prompt, and hyperparameters\nto attain the best performance for a task typically necessitates evaluating an\nexpoential number of candidates on large validation sets. This exhaustive\nevaluation can be time-consuming and costly, as both inference and evaluation\nof LLM-based approaches are resource-intensive. Worse, a lot of computation is\nwasted: Many hyper-parameter settings are non-competitive, and many samples\nfrom the validation set are highly correlated - providing little or no new\ninformation. So, if the goal is to identify the best method, it can be done far\nmore efficiently if the validation samples and methods are selected adaptively.\nIn this paper, we propose a novel method to address this challenge. We lean on\nlow-rank matrix factorization to fill in missing evaluations and on multi-armed\nbandits to sequentially identify the next (method, validation sample)-pair to\nevaluate. We carefully assess the efficacy of our approach on several\ncompetitive benchmark problems and show that it can identify the top-performing\nmethod using only 5-15% of the typically needed resources -- resulting in a\nstaggering 85-95% LLM cost savings.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}