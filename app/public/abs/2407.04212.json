{"id":"2407.04212","title":"Smart Vision-Language Reasoners","authors":"Denisa Roberts and Lucas Roberts","authorsParsed":[["Roberts","Denisa",""],["Roberts","Lucas",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 01:47:21 GMT"}],"updateDate":"2024-07-08","timestamp":1720144041000,"abstract":"  In this article, we investigate vision-language models (VLM) as reasoners.\nThe ability to form abstractions underlies mathematical reasoning,\nproblem-solving, and other Math AI tasks. Several formalisms have been given to\nthese underlying abstractions and skills utilized by humans and intelligent\nsystems for reasoning. Furthermore, human reasoning is inherently multimodal,\nand as such, we focus our investigations on multimodal AI. In this article, we\nemploy the abstractions given in the SMART task (Simple Multimodal Algorithmic\nReasoning Task) introduced in \\cite{cherian2022deep} as meta-reasoning and\nproblem-solving skills along eight axes: math, counting, path, measure, logic,\nspatial, and pattern. We investigate the ability of vision-language models to\nreason along these axes and seek avenues of improvement. Including composite\nrepresentations with vision-language cross-attention enabled learning\nmultimodal representations adaptively from fused frozen pretrained backbones\nfor better visual grounding. Furthermore, proper hyperparameter and other\ntraining choices led to strong improvements (up to $48\\%$ gain in accuracy) on\nthe SMART task, further underscoring the power of deep multimodal learning. The\nsmartest VLM, which includes a novel QF multimodal layer, improves upon the\nbest previous baselines in every one of the eight fundamental reasoning skills.\nEnd-to-end code is available at https://github.com/smarter-vlm/smarter.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}