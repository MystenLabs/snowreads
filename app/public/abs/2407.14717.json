{"id":"2407.14717","title":"Differential Privacy of Cross-Attention with Provable Guarantee","authors":"Jiuxiang Gu, Yingyu Liang, Zhenmei Shi, Zhao Song, Yufa Zhou","authorsParsed":[["Gu","Jiuxiang",""],["Liang","Yingyu",""],["Shi","Zhenmei",""],["Song","Zhao",""],["Zhou","Yufa",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 01:02:27 GMT"}],"updateDate":"2024-07-23","timestamp":1721437347000,"abstract":"  Cross-attention has become a fundamental module nowadays in many important\nartificial intelligence applications, e.g., retrieval-augmented generation\n(RAG), system prompt, guided stable diffusion, and many so on. Ensuring\ncross-attention privacy is crucial and urgently needed because its key and\nvalue matrices may contain sensitive information about companies and their\nusers, many of which profit solely from their system prompts or RAG data. In\nthis work, we design a novel differential privacy (DP) data structure to\naddress the privacy security of cross-attention with a theoretical guarantee.\nIn detail, let $n$ be the input token length of system prompt/RAG data, $d$ be\nthe feature dimension, $0 < \\alpha \\le 1$ be the relative error parameter, $R$\nbe the maximum value of the query and key matrices, $R_w$ be the maximum value\nof the value matrix, and $r,s,\\epsilon_s$ be parameters of polynomial kernel\nmethods. Then, our data structure requires $\\widetilde{O}(ndr^2)$ memory\nconsumption with $\\widetilde{O}(nr^2)$ initialization time complexity and\n$\\widetilde{O}(\\alpha^{-1} r^2)$ query time complexity for a single token\nquery. In addition, our data structure can guarantee that the user query is\n$(\\epsilon, \\delta)$-DP with $\\widetilde{O}(n^{-1} \\epsilon^{-1} \\alpha^{-1/2}\nR^{2s} R_w r^2)$ additive error and $n^{-1} (\\alpha + \\epsilon_s)$ relative\nerror between our output and the true answer. Furthermore, our result is robust\nto adaptive queries in which users can intentionally attack the cross-attention\nsystem. To our knowledge, this is the first work to provide DP for\ncross-attention. We believe it can inspire more privacy algorithm design in\nlarge generative models (LGMs).\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XwOZ7Btwgg-H8DNBhM90hWj_rRHI4Xc3TWJD_8IBV8I","pdfSize":"855965"}
