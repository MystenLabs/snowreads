{"id":"2407.03913","title":"MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices","authors":"Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, Jianping\n  Fan","authorsParsed":[["Zhang","Jiayi",""],["Zhao","Chuang",""],["Zhao","Yihan",""],["Yu","Zhaoyang",""],["He","Ming",""],["Fan","Jianping",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 13:12:19 GMT"}],"updateDate":"2024-07-08","timestamp":1720098739000,"abstract":"  The attainment of autonomous operations in mobile computing devices has\nconsistently been a goal of human pursuit. With the development of Large\nLanguage Models (LLMs) and Visual Language Models (VLMs), this aspiration is\nprogressively turning into reality. While contemporary research has explored\nautomation of simple tasks on mobile devices via VLMs, there remains\nsignificant room for improvement in handling complex tasks and reducing high\nreasoning costs. In this paper, we introduce MobileExperts, which for the first\ntime introduces tool formulation and multi-agent collaboration to address the\naforementioned challenges. More specifically, MobileExperts dynamically\nassembles teams based on the alignment of agent portraits with the human\nrequirements. Following this, each agent embarks on an independent exploration\nphase, formulating its tools to evolve into an expert. Lastly, we develop a\ndual-layer planning mechanism to establish coordinate collaboration among\nexperts. To validate our effectiveness, we design a new benchmark of\nhierarchical intelligence levels, offering insights into algorithm's capability\nto address tasks across a spectrum of complexity. Experimental results\ndemonstrate that MobileExperts performs better on all intelligence levels and\nachieves ~ 22% reduction in reasoning costs, thus verifying the superiority of\nour design.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}