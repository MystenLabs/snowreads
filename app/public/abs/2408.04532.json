{"id":"2408.04532","title":"How Transformers Utilize Multi-Head Attention in In-Context Learning? A\n  Case Study on Sparse Linear Regression","authors":"Xingwu Chen, Lei Zhao, Difan Zou","authorsParsed":[["Chen","Xingwu",""],["Zhao","Lei",""],["Zou","Difan",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 15:33:02 GMT"}],"updateDate":"2024-08-09","timestamp":1723131182000,"abstract":"  Despite the remarkable success of transformer-based models in various\nreal-world tasks, their underlying mechanisms remain poorly understood. Recent\nstudies have suggested that transformers can implement gradient descent as an\nin-context learner for linear regression problems and have developed various\ntheoretical analyses accordingly. However, these works mostly focus on the\nexpressive power of transformers by designing specific parameter constructions,\nlacking a comprehensive understanding of their inherent working mechanisms\npost-training. In this study, we consider a sparse linear regression problem\nand investigate how a trained multi-head transformer performs in-context\nlearning. We experimentally discover that the utilization of multi-heads\nexhibits different patterns across layers: multiple heads are utilized and\nessential in the first layer, while usually only a single head is sufficient\nfor subsequent layers. We provide a theoretical explanation for this\nobservation: the first layer preprocesses the context data, and the following\nlayers execute simple optimization steps based on the preprocessed context.\nMoreover, we demonstrate that such a preprocess-then-optimize algorithm can\nsignificantly outperform naive gradient descent and ridge regression\nalgorithms. Further experimental results support our explanations. Our findings\noffer insights into the benefits of multi-head attention and contribute to\nunderstanding the more intricate mechanisms hidden within trained transformers.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"s35EvHmzMZS8DdHpvDAY11T5_2_l2J4fUXjjQupDPDg","pdfSize":"1488883"}
