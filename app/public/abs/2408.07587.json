{"id":"2408.07587","title":"FedQUIT: On-Device Federated Unlearning via a Quasi-Competent Virtual\n  Teacher","authors":"Alessio Mora, Lorenzo Valerio, Paolo Bellavista, Andrea Passarella","authorsParsed":[["Mora","Alessio",""],["Valerio","Lorenzo",""],["Bellavista","Paolo",""],["Passarella","Andrea",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 14:36:28 GMT"}],"updateDate":"2024-08-15","timestamp":1723646188000,"abstract":"  Federated Learning (FL) promises better privacy guarantees for individuals'\ndata when machine learning models are collaboratively trained. When an FL\nparticipant exercises its right to be forgotten, i.e., to detach from the FL\nframework it has participated and to remove its past contributions to the\nglobal model, the FL solution should perform all the necessary steps to make it\npossible without sacrificing the overall performance of the global model, which\nis not supported in state-of-the-art related solutions nowadays. In this paper,\nwe propose FedQUIT, a novel algorithm that uses knowledge distillation to scrub\nthe contribution of the forgetting data from an FL global model while\npreserving its generalization ability. FedQUIT directly works on clients'\ndevices and does not require sharing additional information if compared with a\nregular FL process, nor does it assume the availability of publicly available\nproxy data. Our solution is efficient, effective, and applicable in both\ncentralized and federated settings. Our experimental results show that, on\naverage, FedQUIT requires less than 2.5% additional communication rounds to\nrecover generalization performances after unlearning, obtaining a sanitized\nglobal model whose predictions are comparable to those of a global model that\nhas never seen the data to be forgotten.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}