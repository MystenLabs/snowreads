{"id":"2407.06544","title":"Multiple Instance Verification","authors":"Xin Xu, Eibe Frank, Geoffrey Holmes","authorsParsed":[["Xu","Xin",""],["Frank","Eibe",""],["Holmes","Geoffrey",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 04:51:22 GMT"}],"updateDate":"2024-07-10","timestamp":1720500682000,"abstract":"  We explore multiple-instance verification, a problem setting where a query\ninstance is verified against a bag of target instances with heterogeneous,\nunknown relevancy. We show that naive adaptations of attention-based multiple\ninstance learning (MIL) methods and standard verification methods like Siamese\nneural networks are unsuitable for this setting: directly combining\nstate-of-the-art (SOTA) MIL methods and Siamese networks is shown to be no\nbetter, and sometimes significantly worse, than a simple baseline model.\nPostulating that this may be caused by the failure of the representation of the\ntarget bag to incorporate the query instance, we introduce a new pooling\napproach named ``cross-attention pooling'' (CAP). Under the CAP framework, we\npropose two novel attention functions to address the challenge of\ndistinguishing between highly similar instances in a target bag. Through\nempirical studies on three different verification tasks, we demonstrate that\nCAP outperforms adaptations of SOTA MIL methods and the baseline by substantial\nmargins, in terms of both classification accuracy and quality of the\nexplanations provided for the classifications. Ablation studies confirm the\nsuperior ability of the new attention functions to identify key instances.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}