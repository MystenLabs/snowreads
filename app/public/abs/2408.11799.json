{"id":"2408.11799","title":"Practical token pruning for foundation models in few-shot conversational\n  virtual assistant systems","authors":"Haode Qi, Cheng Qian, Jian Ni, Pratyush Singh, Reza Fazeli, Gengyu\n  Wang, Zhongzheng Shu, Eric Wayne, Juergen Bross","authorsParsed":[["Qi","Haode",""],["Qian","Cheng",""],["Ni","Jian",""],["Singh","Pratyush",""],["Fazeli","Reza",""],["Wang","Gengyu",""],["Shu","Zhongzheng",""],["Wayne","Eric",""],["Bross","Juergen",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:42:17 GMT"}],"updateDate":"2024-08-22","timestamp":1724262137000,"abstract":"  In an enterprise Virtual Assistant (VA) system, intent classification is the\ncrucial component that determines how a user input is handled based on what the\nuser wants. The VA system is expected to be a cost-efficient SaaS service with\nlow training and inference time while achieving high accuracy even with a small\nnumber of training samples. We pretrain a transformer-based sentence embedding\nmodel with a contrastive learning objective and leverage the embedding of the\nmodel as features when training intent classification models. Our approach\nachieves the state-of-the-art results for few-shot scenarios and performs\nbetter than other commercial solutions on popular intent classification\nbenchmarks. However, generating features via a transformer-based model\nincreases the inference time, especially for longer user inputs, due to the\nquadratic runtime of the transformer's attention mechanism. On top of model\ndistillation, we introduce a practical multi-task adaptation approach that\nconfigures dynamic token pruning without the need for task-specific training\nfor intent classification. We demonstrate that this approach improves the\ninference speed of popular sentence transformer models without affecting model\nperformance.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}