{"id":"2408.13484","title":"IntOPE: Off-Policy Evaluation in the Presence of Interference","authors":"Yuqi Bai, Ziyu Zhao, Minqin Zhu, Kun Kuang","authorsParsed":[["Bai","Yuqi",""],["Zhao","Ziyu",""],["Zhu","Minqin",""],["Kuang","Kun",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 06:07:25 GMT"}],"updateDate":"2024-08-28","timestamp":1724479645000,"abstract":"  Off-Policy Evaluation (OPE) is employed to assess the potential impact of a\nhypothetical policy using logged contextual bandit feedback, which is crucial\nin areas such as personalized medicine and recommender systems, where online\ninteractions are associated with significant risks and costs. Traditionally,\nOPE methods rely on the Stable Unit Treatment Value Assumption (SUTVA), which\nassumes that the reward for any given individual is unaffected by the actions\nof others. However, this assumption often fails in real-world scenarios due to\nthe presence of interference, where an individual's reward is affected not just\nby their own actions but also by the actions of their peers. This realization\nreveals significant limitations of existing OPE methods in real-world\napplications. To address this limitation, we propose IntIPW, an IPW-style\nestimator that extends the Inverse Probability Weighting (IPW) framework by\nintegrating marginalized importance weights to account for both individual\nactions and the influence of adjacent entities. Extensive experiments are\nconducted on both synthetic and real-world data to demonstrate the\neffectiveness of the proposed IntIPW method.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}