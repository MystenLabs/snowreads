{"id":"2407.21058","title":"Understanding the Interplay of Scale, Data, and Bias in Language Models:\n  A Case Study with BERT","authors":"Muhammad Ali, Swetasudha Panda, Qinlan Shen, Michael Wick, Ari Kobren","authorsParsed":[["Ali","Muhammad",""],["Panda","Swetasudha",""],["Shen","Qinlan",""],["Wick","Michael",""],["Kobren","Ari",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 23:09:33 GMT"}],"updateDate":"2024-08-01","timestamp":1721948973000,"abstract":"  In the current landscape of language model research, larger models, larger\ndatasets and more compute seems to be the only way to advance towards\nintelligence. While there have been extensive studies of scaling laws and\nmodels' scaling behaviors, the effect of scale on a model's social biases and\nstereotyping tendencies has received less attention. In this study, we explore\nthe influence of model scale and pre-training data on its learnt social biases.\nWe focus on BERT -- an extremely popular language model -- and investigate\nbiases as they show up during language modeling (upstream), as well as during\nclassification applications after fine-tuning (downstream). Our experiments on\nfour architecture sizes of BERT demonstrate that pre-training data\nsubstantially influences how upstream biases evolve with model scale. With\nincreasing scale, models pre-trained on large internet scrapes like Common\nCrawl exhibit higher toxicity, whereas models pre-trained on moderated data\nsources like Wikipedia show greater gender stereotypes. However, downstream\nbiases generally decrease with increasing model scale, irrespective of the\npre-training data. Our results highlight the qualitative role of pre-training\ndata in the biased behavior of language models, an often overlooked aspect in\nthe study of scale. Through a detailed case study of BERT, we shed light on the\ncomplex interplay of data and model scale, and investigate how it translates to\nconcrete biases.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}