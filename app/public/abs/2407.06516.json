{"id":"2407.06516","title":"VQA-Diff: Exploiting VQA and Diffusion for Zero-Shot Image-to-3D Vehicle\n  Asset Generation in Autonomous Driving","authors":"Yibo Liu, Zheyuan Yang, Guile Wu, Yuan Ren, Kejian Lin, Bingbing Liu,\n  Yang Liu, Jinjun Shan","authorsParsed":[["Liu","Yibo",""],["Yang","Zheyuan",""],["Wu","Guile",""],["Ren","Yuan",""],["Lin","Kejian",""],["Liu","Bingbing",""],["Liu","Yang",""],["Shan","Jinjun",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 03:09:55 GMT"},{"version":"v2","created":"Wed, 10 Jul 2024 18:51:40 GMT"}],"updateDate":"2024-07-12","timestamp":1720494595000,"abstract":"  Generating 3D vehicle assets from in-the-wild observations is crucial to\nautonomous driving. Existing image-to-3D methods cannot well address this\nproblem because they learn generation merely from image RGB information without\na deeper understanding of in-the-wild vehicles (such as car models,\nmanufacturers, etc.). This leads to their poor zero-shot prediction capability\nto handle real-world observations with occlusion or tricky viewing angles. To\nsolve this problem, in this work, we propose VQA-Diff, a novel framework that\nleverages in-the-wild vehicle images to create photorealistic 3D vehicle assets\nfor autonomous driving. VQA-Diff exploits the real-world knowledge inherited\nfrom the Large Language Model in the Visual Question Answering (VQA) model for\nrobust zero-shot prediction and the rich image prior knowledge in the Diffusion\nmodel for structure and appearance generation. In particular, we utilize a\nmulti-expert Diffusion Models strategy to generate the structure information\nand employ a subject-driven structure-controlled generation mechanism to model\nappearance information. As a result, without the necessity to learn from a\nlarge-scale image-to-3D vehicle dataset collected from the real world, VQA-Diff\nstill has a robust zero-shot image-to-novel-view generation ability. We conduct\nexperiments on various datasets, including Pascal 3D+, Waymo, and Objaverse, to\ndemonstrate that VQA-Diff outperforms existing state-of-the-art methods both\nqualitatively and quantitatively.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"S8WTEdXuXtF4Pxg1fzdldLgX4YXYwGJU2iGvXfhM2Ss","pdfSize":"14593830"}
