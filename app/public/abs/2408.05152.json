{"id":"2408.05152","title":"Sparsity-Preserving Encodings for Straggler-Optimal Distributed Matrix\n  Computations at the Edge","authors":"Anindya Bijoy Das, Aditya Ramamoorthy, David J. Love and Christopher\n  G. Brinton","authorsParsed":[["Das","Anindya Bijoy",""],["Ramamoorthy","Aditya",""],["Love","David J.",""],["Brinton","Christopher G.",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 16:16:53 GMT"}],"updateDate":"2024-08-12","timestamp":1723220213000,"abstract":"  Matrix computations are a fundamental building-block of edge computing\nsystems, with a major recent uptick in demand due to their use in AI/ML\ntraining and inference procedures. Existing approaches for distributing matrix\ncomputations involve allocating coded combinations of submatrices to worker\nnodes, to build resilience to slower nodes, called stragglers. In the edge\nlearning context, however, these approaches will compromise sparsity properties\nthat are often present in the original matrices found at the edge server. In\nthis study, we consider the challenge of augmenting such approaches to preserve\ninput sparsity when distributing the task across edge devices, thereby\nretaining the associated computational efficiency enhancements. First, we find\na lower bound on the weight of coding, i.e., the number of submatrices to be\ncombined to obtain coded submatrices, to provide the resilience to the maximum\npossible number of straggler devices (for given number of devices and their\nstorage constraints). Next we propose distributed matrix computation schemes\nwhich meet the exact lower bound on the weight of the coding. Numerical\nexperiments conducted in Amazon Web Services (AWS) validate our assertions\nregarding straggler mitigation and computation speed for sparse matrices.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}