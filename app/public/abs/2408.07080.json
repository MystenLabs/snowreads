{"id":"2408.07080","title":"DisCoM-KD: Cross-Modal Knowledge Distillation via Disentanglement\n  Representation and Adversarial Learning","authors":"Dino Ienco (EVERGREEN, UMR TETIS, INRAE), Cassio Fraga Dantas (UMR\n  TETIS, INRAE, EVERGREEN)","authorsParsed":[["Ienco","Dino","","EVERGREEN, UMR TETIS, INRAE"],["Dantas","Cassio Fraga","","UMR\n  TETIS, INRAE, EVERGREEN"]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 13:44:15 GMT"}],"updateDate":"2024-08-15","timestamp":1722865455000,"abstract":"  Cross-modal knowledge distillation (CMKD) refers to the scenario in which a\nlearning framework must handle training and test data that exhibit a modality\nmismatch, more precisely, training and test data do not cover the same set of\ndata modalities. Traditional approaches for CMKD are based on a teacher/student\nparadigm where a teacher is trained on multi-modal data with the aim to\nsuccessively distill knowledge from a multi-modal teacher to a single-modal\nstudent. Despite the widespread adoption of such paradigm, recent research has\nhighlighted its inherent limitations in the context of cross-modal knowledge\ntransfer.Taking a step beyond the teacher/student paradigm, here we introduce a\nnew framework for cross-modal knowledge distillation, named DisCoM-KD\n(Disentanglement-learning based Cross-Modal Knowledge Distillation), that\nexplicitly models different types of per-modality information with the aim to\ntransfer knowledge from multi-modal data to a single-modal classifier. To this\nend, DisCoM-KD effectively combines disentanglement representation learning\nwith adversarial domain adaptation to simultaneously extract, foreach modality,\ndomain-invariant, domain-informative and domain-irrelevant features according\nto a specific downstream task. Unlike the traditional teacher/student paradigm,\nour framework simultaneously learns all single-modal classifiers, eliminating\nthe need to learn each student model separately as well as the teacher\nclassifier. We evaluated DisCoM-KD on three standard multi-modal benchmarks and\ncompared its behaviourwith recent SOTA knowledge distillation frameworks. The\nfindings clearly demonstrate the effectiveness of DisCoM-KD over competitors\nconsidering mismatch scenarios involving both overlapping and non-overlapping\nmodalities. These results offer insights to reconsider the traditional paradigm\nfor distilling information from multi-modal data to single-modal neural\nnetworks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}