{"id":"2407.07279","title":"Towards a theory of learning dynamics in deep state space models","authors":"Jakub Sm\\'ekal, Jimmy T.H. Smith, Michael Kleinman, Dan Biderman,\n  Scott W. Linderman","authorsParsed":[["Sm√©kal","Jakub",""],["Smith","Jimmy T. H.",""],["Kleinman","Michael",""],["Biderman","Dan",""],["Linderman","Scott W.",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 00:01:56 GMT"}],"updateDate":"2024-07-11","timestamp":1720569716000,"abstract":"  State space models (SSMs) have shown remarkable empirical performance on many\nlong sequence modeling tasks, but a theoretical understanding of these models\nis still lacking. In this work, we study the learning dynamics of linear SSMs\nto understand how covariance structure in data, latent state size, and\ninitialization affect the evolution of parameters throughout learning with\ngradient descent. We show that focusing on the learning dynamics in the\nfrequency domain affords analytical solutions under mild assumptions, and we\nestablish a link between one-dimensional SSMs and the dynamics of deep linear\nfeed-forward networks. Finally, we analyze how latent state\nover-parameterization affects convergence time and describe future work in\nextending our results to the study of deep SSMs with nonlinear connections.\nThis work is a step toward a theory of learning dynamics in deep state space\nmodels.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}