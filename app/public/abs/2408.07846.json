{"id":"2408.07846","title":"A System for Automated Unit Test Generation Using Large Language Models\n  and Assessment of Generated Test Suites","authors":"Andrea Lops, Fedelucio Narducci, Azzurra Ragone, Michelantonio Trizio,\n  Claudio Bartolini","authorsParsed":[["Lops","Andrea",""],["Narducci","Fedelucio",""],["Ragone","Azzurra",""],["Trizio","Michelantonio",""],["Bartolini","Claudio",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 23:02:16 GMT"},{"version":"v2","created":"Fri, 16 Aug 2024 00:18:03 GMT"}],"updateDate":"2024-08-19","timestamp":1723676536000,"abstract":"  Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present AgoneTest: an automated system for generating test suites for\nJava projects and a comprehensive and principled methodology for evaluating the\ngenerated test suites. Starting from a state-of-the-art dataset (i.e.,\nMethods2Test), we built a new dataset for comparing human-written tests with\nthose generated by LLMs. Our key contributions include a scalable automated\nsoftware system, a new dataset, and a detailed methodology for evaluating test\nquality.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}