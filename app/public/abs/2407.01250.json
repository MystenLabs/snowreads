{"id":"2407.01250","title":"Metric-Entropy Limits on Nonlinear Dynamical System Learning","authors":"Yang Pan, Clemens Hutter, Helmut B\\\"olcskei","authorsParsed":[["Pan","Yang",""],["Hutter","Clemens",""],["BÃ¶lcskei","Helmut",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 12:57:03 GMT"}],"updateDate":"2024-07-02","timestamp":1719838623000,"abstract":"  This paper is concerned with the fundamental limits of nonlinear dynamical\nsystem learning from input-output traces. Specifically, we show that recurrent\nneural networks (RNNs) are capable of learning nonlinear systems that satisfy a\nLipschitz property and forget past inputs fast enough in a metric-entropy\noptimal manner. As the sets of sequence-to-sequence maps realized by the\ndynamical systems we consider are significantly more massive than function\nclasses generally considered in deep neural network approximation theory, a\nrefined metric-entropy characterization is needed, namely in terms of order,\ntype, and generalized dimension. We compute these quantities for the classes of\nexponentially-decaying and polynomially-decaying Lipschitz fading-memory\nsystems and show that RNNs can achieve them.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Information Theory","Mathematics/Dynamical Systems","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}