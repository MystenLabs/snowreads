{"id":"2407.11901","title":"Combining Wasserstein-1 and Wasserstein-2 proximals: robust manifold\n  learning via well-posed generative flows","authors":"Hyemin Gu, Markos A. Katsoulakis, Luc Rey-Bellet, Benjamin J. Zhang","authorsParsed":[["Gu","Hyemin",""],["Katsoulakis","Markos A.",""],["Rey-Bellet","Luc",""],["Zhang","Benjamin J.",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 16:34:31 GMT"}],"updateDate":"2024-07-17","timestamp":1721147671000,"abstract":"  We formulate well-posed continuous-time generative flows for learning\ndistributions that are supported on low-dimensional manifolds through\nWasserstein proximal regularizations of $f$-divergences. Wasserstein-1 proximal\noperators regularize $f$-divergences so that singular distributions can be\ncompared. Meanwhile, Wasserstein-2 proximal operators regularize the paths of\nthe generative flows by adding an optimal transport cost, i.e., a kinetic\nenergy penalization. Via mean-field game theory, we show that the combination\nof the two proximals is critical for formulating well-posed generative flows.\nGenerative flows can be analyzed through optimality conditions of a mean-field\ngame (MFG), a system of a backward Hamilton-Jacobi (HJ) and a forward\ncontinuity partial differential equations (PDEs) whose solution characterizes\nthe optimal generative flow. For learning distributions that are supported on\nlow-dimensional manifolds, the MFG theory shows that the Wasserstein-1\nproximal, which addresses the HJ terminal condition, and the Wasserstein-2\nproximal, which addresses the HJ dynamics, are both necessary for the\ncorresponding backward-forward PDE system to be well-defined and have a unique\nsolution with provably linear flow trajectories. This implies that the\ncorresponding generative flow is also unique and can therefore be learned in a\nrobust manner even for learning high-dimensional distributions supported on\nlow-dimensional manifolds. The generative flows are learned through adversarial\ntraining of continuous-time flows, which bypasses the need for reverse\nsimulation. We demonstrate the efficacy of our approach for generating\nhigh-dimensional images without the need to resort to autoencoders or\nspecialized architectures.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Statistics/Computation","Statistics/Methodology"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Zk7dy-jT9oo3aUS_bq_GIaif6AO9bbmIYvvfbv0cOSQ","pdfSize":"1055886"}
