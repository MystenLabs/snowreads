{"id":"2407.12532","title":"Towards Collaborative Intelligence: Propagating Intentions and Reasoning\n  for Multi-Agent Coordination with Large Language Models","authors":"Xihe Qiu, Haoyu Wang, Xiaoyu Tan, Chao Qu, Yujie Xiong, Yuan Cheng,\n  Yinghui Xu, Wei Chu, Yuan Qi","authorsParsed":[["Qiu","Xihe",""],["Wang","Haoyu",""],["Tan","Xiaoyu",""],["Qu","Chao",""],["Xiong","Yujie",""],["Cheng","Yuan",""],["Xu","Yinghui",""],["Chu","Wei",""],["Qi","Yuan",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 13:14:00 GMT"}],"updateDate":"2024-07-18","timestamp":1721222040000,"abstract":"  Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}