{"id":"2407.04697","title":"VCoME: Verbal Video Composition with Multimodal Editing Effects","authors":"Weibo Gong and Xiaojie Jin and Xin Li and Dongliang He and Xinglong Wu","authorsParsed":[["Gong","Weibo",""],["Jin","Xiaojie",""],["Li","Xin",""],["He","Dongliang",""],["Wu","Xinglong",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 17:59:02 GMT"}],"updateDate":"2024-07-08","timestamp":1720202342000,"abstract":"  Verbal videos, featuring voice-overs or text overlays, provide valuable\ncontent but present significant challenges in composition, especially when\nincorporating editing effects to enhance clarity and visual appeal. In this\npaper, we introduce the novel task of verbal video composition with editing\neffects. This task aims to generate coherent and visually appealing verbal\nvideos by integrating multimodal editing effects across textual, visual, and\naudio categories. To achieve this, we curate a large-scale dataset of video\neffects compositions from publicly available sources. We then formulate this\ntask as a generative problem, involving the identification of appropriate\npositions in the verbal content and the recommendation of editing effects for\nthese positions. To address this task, we propose VCoME, a general framework\nthat employs a large multimodal model to generate editing effects for video\ncomposition. Specifically, VCoME takes in the multimodal video context and\nautoregressively outputs where to apply effects within the verbal content and\nwhich effects are most appropriate for each position. VCoME also supports\nprompt-based control of composition density and style, providing substantial\nflexibility for diverse applications. Through extensive quantitative and\nqualitative evaluations, we clearly demonstrate the effectiveness of VCoME. A\ncomprehensive user study shows that our method produces videos of professional\nquality while being 85$\\times$ more efficient than professional editors.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kdvUP9HFATGUIEFOniCMuIaBF_bDdkN6DT_sjLE1QLE","pdfSize":"4292143"}
