{"id":"2408.13068","title":"On Class Separability Pitfalls In Audio-Text Contrastive Zero-Shot\n  Learning","authors":"Tiago Tavares, Fabio Ayres, Zhepei Wang, Paris Smaragdis","authorsParsed":[["Tavares","Tiago",""],["Ayres","Fabio",""],["Wang","Zhepei",""],["Smaragdis","Paris",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 13:52:56 GMT"}],"updateDate":"2024-08-26","timestamp":1724421176000,"abstract":"  Recent advances in audio-text cross-modal contrastive learning have shown its\npotential towards zero-shot learning. One possibility for this is by projecting\nitem embeddings from pre-trained backbone neural networks into a cross-modal\nspace in which item similarity can be calculated in either domain. This process\nrelies on a strong unimodal pre-training of the backbone networks, and on a\ndata-intensive training task for the projectors. These two processes can be\nbiased by unintentional data leakage, which can arise from using supervised\nlearning in pre-training or from inadvertently training the cross-modal\nprojection using labels from the zero-shot learning evaluation. In this study,\nwe show that a significant part of the measured zero-shot learning accuracy is\ndue to strengths inherited from the audio and text backbones, that is, they are\nnot learned in the cross-modal domain and are not transferred from one modality\nto another.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}