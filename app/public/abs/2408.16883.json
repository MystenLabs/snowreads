{"id":"2408.16883","title":"Revising Multimodal VAEs with Diffusion Decoders","authors":"Daniel Wesego, Amirmohammad Rooshenas","authorsParsed":[["Wesego","Daniel",""],["Rooshenas","Amirmohammad",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 20:12:01 GMT"}],"updateDate":"2024-09-02","timestamp":1724962321000,"abstract":"  Multimodal VAEs often struggle with generating high-quality outputs, a\nchallenge that extends beyond the inherent limitations of the VAE framework.\nThe core issue lies in the restricted joint representation of the latent space,\nparticularly when complex modalities like images are involved. Feedforward\ndecoders, commonly used for these intricate modalities, inadvertently constrain\nthe joint latent space, leading to a degradation in the quality of the other\nmodalities as well. Although recent studies have shown improvement by\nintroducing modality-specific representations, the issue remains significant.\nIn this work, we demonstrate that incorporating a flexible diffusion decoder\nspecifically for the image modality not only enhances the generation quality of\nthe images but also positively impacts the performance of the other modalities\nthat rely on feedforward decoders. This approach addresses the limitations\nimposed by conventional joint representations and opens up new possibilities\nfor improving multimodal generation tasks using the multimodal VAE framework.\nOur model provides state-of-the-art results compared to other multimodal VAEs\nin different datasets with higher coherence and superior quality in the\ngenerated modalities\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CTAM2BP4t_-j-7rfS47eXQ4QS2ZAHZn6l2gGrByUQCk","pdfSize":"7002513"}
