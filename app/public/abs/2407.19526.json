{"id":"2407.19526","title":"Impact of Decoding Methods on Human Alignment of Conversational LLMs","authors":"Shaz Furniturewala, Kokil Jaidka, Yashvardhan Sharma","authorsParsed":[["Furniturewala","Shaz",""],["Jaidka","Kokil",""],["Sharma","Yashvardhan",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 16:31:09 GMT"}],"updateDate":"2024-07-30","timestamp":1722184269000,"abstract":"  To be included into chatbot systems, Large language models (LLMs) must be\naligned with human conversational conventions. However, being trained mainly on\nweb-scraped data gives existing LLMs a voice closer to informational text than\nactual human speech. In this paper, we examine the effect of decoding methods\non the alignment between LLM-generated and human conversations, including Beam\nSearch, Top K Sampling, and Nucleus Sampling. We present new measures of\nalignment in substance, style, and psychometric orientation, and experiment\nwith two conversation datasets. Our results provide subtle insights: better\nalignment is attributed to fewer beams in Beam Search and lower values of P in\nNucleus Sampling. We also find that task-oriented and open-ended datasets\nperform differently in terms of alignment, indicating the significance of\ntaking into account the context of the interaction.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}