{"id":"2407.21616","title":"EZSR: Event-based Zero-Shot Recognition","authors":"Yan Yang and Liyuan Pan and Dongxu Li and Liu Liu","authorsParsed":[["Yang","Yan",""],["Pan","Liyuan",""],["Li","Dongxu",""],["Liu","Liu",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 14:06:06 GMT"}],"updateDate":"2024-08-01","timestamp":1722434766000,"abstract":"  This paper studies zero-shot object recognition using event camera data.\nGuided by CLIP, which is pre-trained on RGB images, existing approaches achieve\nzero-shot object recognition by maximizing embedding similarities between event\ndata encoded by an event encoder and RGB images encoded by the CLIP image\nencoder. Alternatively, several methods learn RGB frame reconstructions from\nevent data for the CLIP image encoder. However, these approaches often result\nin suboptimal zero-shot performance.\n  This study develops an event encoder without relying on additional\nreconstruction networks. We theoretically analyze the performance bottlenecks\nof previous approaches: global similarity-based objective (i.e., maximizing the\nembedding similarities) cause semantic misalignments between the learned event\nembedding space and the CLIP text embedding space due to the degree of freedom.\nTo mitigate the issue, we explore a scalar-wise regularization strategy.\nFurthermore, to scale up the number of events and RGB data pairs for training,\nwe also propose a pipeline for synthesizing event data from static RGB images.\n  Experimentally, our data synthesis strategy exhibits an attractive scaling\nproperty, and our method achieves superior zero-shot object recognition\nperformance on extensive standard benchmark datasets, even compared with past\nsupervised learning approaches. For example, we achieve 47.84% zero-shot\naccuracy on the N-ImageNet dataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}