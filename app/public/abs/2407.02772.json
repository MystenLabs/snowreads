{"id":"2407.02772","title":"Automatic gradient descent with generalized Newton's method","authors":"Zhiqi Bu, Shiyun Xu","authorsParsed":[["Bu","Zhiqi",""],["Xu","Shiyun",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 03:01:43 GMT"}],"updateDate":"2024-07-04","timestamp":1719975703000,"abstract":"  We propose the generalized Newton's method (GeN) -- a Hessian-informed\napproach that applies to any optimizer such as SGD and Adam, and covers the\nNewton-Raphson method as a sub-case. Our method automatically and dynamically\nselects the learning rate that accelerates the convergence, without the\nintensive tuning of the learning rate scheduler. In practice, out method is\neasily implementable, since it only requires additional forward passes with\nalmost zero computational overhead (in terms of training time and memory cost),\nif the overhead is amortized over many iterations. We present extensive\nexperiments on language and vision tasks (e.g. GPT and ResNet) to showcase that\nGeN optimizers match the state-of-the-art performance, which was achieved with\ncarefully tuned learning rate schedulers. Code to be released at\n\\url{https://github.com/ShiyunXu/AutoGeN}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}