{"id":"2408.08601","title":"Learning A Low-Level Vision Generalist via Visual Task Prompt","authors":"Xiangyu Chen, Yihao Liu, Yuandong Pu, Wenlong Zhang, Jiantao Zhou, Yu\n  Qiao and Chao Dong","authorsParsed":[["Chen","Xiangyu",""],["Liu","Yihao",""],["Pu","Yuandong",""],["Zhang","Wenlong",""],["Zhou","Jiantao",""],["Qiao","Yu",""],["Dong","Chao",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 08:37:56 GMT"}],"updateDate":"2024-08-19","timestamp":1723797476000,"abstract":"  Building a unified model for general low-level vision tasks holds significant\nresearch and practical value. Current methods encounter several critical\nissues. Multi-task restoration approaches can address multiple\ndegradation-to-clean restoration tasks, while their applicability to tasks with\ndifferent target domains (e.g., image stylization) is limited. Methods like\nPromptGIP can handle multiple input-target domains but rely on the Masked\nAutoencoder (MAE) paradigm. Consequently, they are tied to the ViT\narchitecture, resulting in suboptimal image reconstruction quality. In\naddition, these methods are sensitive to prompt image content and often\nstruggle with low-frequency information processing. In this paper, we propose a\nVisual task Prompt-based Image Processing (VPIP) framework to overcome these\nchallenges. VPIP employs visual task prompts to manage tasks with different\ninput-target domains and allows flexible selection of backbone network suitable\nfor general tasks. Besides, a new prompt cross-attention is introduced to\nfacilitate interaction between the input and prompt information. Based on the\nVPIP framework, we train a low-level vision generalist model, namely GenLV, on\n30 diverse tasks. Experimental results show that GenLV can successfully address\na variety of low-level tasks, significantly outperforming existing methods both\nquantitatively and qualitatively. Codes are available at\nhttps://github.com/chxy95/GenLV.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}