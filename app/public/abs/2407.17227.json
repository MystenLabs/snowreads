{"id":"2407.17227","title":"LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN\n  prover","authors":"Zijian Wu, Jiayu Wang, Dahua Lin, Kai Chen","authorsParsed":[["Wu","Zijian",""],["Wang","Jiayu",""],["Lin","Dahua",""],["Chen","Kai",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 12:28:03 GMT"}],"updateDate":"2024-07-25","timestamp":1721824083000,"abstract":"  Recently, large language models have presented promising results in aiding\nformal mathematical reasoning. However, their performance is restricted due to\nthe scarcity of formal theorem-proving data, which requires additional effort\nto be extracted from raw formal language corpora. Meanwhile, a significant\namount of human-written formal language corpora remains underutilized. To\naddress this issue, we propose LEAN-GitHub, a dataset consisting of large-scale\nformal data extracted from almost all Lean 4 repositories on GitHub. After\nfine-tuning InternLM-math-plus on this dataset, our model achieved accuracies\nof 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F\ntest, surpassing state-of-the-art method at 52%. And it also achieves\nstate-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting\ndifferent fields/levels of math. These results demonstrate that our proposed\ndataset is beneficial for formal reasoning on a wide range of math topics. We\nopen-source our model at https://GitHub. com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/ datasets/InternLM/Lean-GitHub\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}