{"id":"2408.10682","title":"Towards Robust Knowledge Unlearning: An Adversarial Framework for\n  Assessing and Improving Unlearning Robustness in Large Language Models","authors":"Hongbang Yuan, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao","authorsParsed":[["Yuan","Hongbang",""],["Jin","Zhuoran",""],["Cao","Pengfei",""],["Chen","Yubo",""],["Liu","Kang",""],["Zhao","Jun",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 09:36:04 GMT"}],"updateDate":"2024-08-21","timestamp":1724146564000,"abstract":"  LLM have achieved success in many fields but still troubled by problematic\ncontent in the training corpora. LLM unlearning aims at reducing their\ninfluence and avoid undesirable behaviours. However, existing unlearning\nmethods remain vulnerable to adversarial queries and the unlearned knowledge\nresurfaces after the manually designed attack queries. As part of a red-team\neffort to proactively assess the vulnerabilities of unlearned models, we design\nDynamic Unlearning Attack (DUA), a dynamic and automated framework to attack\nthese models and evaluate their robustness. It optimizes adversarial suffixes\nto reintroduce the unlearned knowledge in various scenarios. We find that\nunlearned knowledge can be recovered in $55.2\\%$ of the questions, even without\nrevealing the unlearned model's parameters. In response to this vulnerability,\nwe propose Latent Adversarial Unlearning (LAU), a universal framework that\neffectively enhances the robustness of the unlearned process. It formulates the\nunlearning process as a min-max optimization problem and resolves it through\ntwo stages: an attack stage, where perturbation vectors are trained and added\nto the latent space of LLMs to recover the unlearned knowledge, and a defense\nstage, where previously trained perturbation vectors are used to enhance\nunlearned model's robustness. With our LAU framework, we obtain two robust\nunlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across\nmultiple unlearning benchmarks and various models, and demonstrate that they\nimprove the unlearning effectiveness by over $53.5\\%$, cause only less than a\n$11.6\\%$ reduction in neighboring knowledge, and have almost no impact on the\nmodel's general capabilities.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}