{"id":"2407.03154","title":"Reinforcement Learning for Sequence Design Leveraging Protein Language\n  Models","authors":"Jithendaraa Subramanian, Shivakanth Sujit, Niloy Irtisam, Umong Sain,\n  Derek Nowrouzezahrai, Samira Ebrahimi Kahou, Riashat Islam","authorsParsed":[["Subramanian","Jithendaraa",""],["Sujit","Shivakanth",""],["Irtisam","Niloy",""],["Sain","Umong",""],["Nowrouzezahrai","Derek",""],["Kahou","Samira Ebrahimi",""],["Islam","Riashat",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:31:36 GMT"}],"updateDate":"2024-07-04","timestamp":1720017096000,"abstract":"  Protein sequence design, determined by amino acid sequences, are essential to\nprotein engineering problems in drug discovery. Prior approaches have resorted\nto evolutionary strategies or Monte-Carlo methods for protein design, but often\nfail to exploit the structure of the combinatorial search space, to generalize\nto unseen sequences. In the context of discrete black box optimization over\nlarge search spaces, learning a mutation policy to generate novel sequences\nwith reinforcement learning is appealing. Recent advances in protein language\nmodels (PLMs) trained on large corpora of protein sequences offer a potential\nsolution to this problem by scoring proteins according to their biological\nplausibility (such as the TM-score). In this work, we propose to use PLMs as a\nreward function to generate new sequences. Yet the PLM can be computationally\nexpensive to query due to its large size. To this end, we propose an\nalternative paradigm where optimization can be performed on scores from a\nsmaller proxy model that is periodically finetuned, jointly while learning the\nmutation policy. We perform extensive experiments on various sequence lengths\nto benchmark RL-based approaches, and provide comprehensive evaluations along\nbiological plausibility and diversity of the protein. Our experimental results\ninclude favorable evaluations of the proposed sequences, along with high\ndiversity scores, demonstrating that RL is a strong candidate for biological\nsequence design. Finally, we provide a modular open source implementation can\nbe easily integrated in most RL training loops, with support for replacing the\nreward model with other PLMs, to spur further research in this domain. The code\nfor all experiments is provided in the supplementary material.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Quantitative Biology/Biomolecules"],"license":"http://creativecommons.org/licenses/by/4.0/"}