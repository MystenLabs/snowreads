{"id":"2407.05704","title":"Narrowing the Gap between Adversarial and Stochastic MDPs via Policy\n  Optimization","authors":"Daniil Tiapkin (CMAP, LMO), Evgenii Chzhen (LMO, CELESTE), Gilles\n  Stoltz (LMO, CELESTE)","authorsParsed":[["Tiapkin","Daniil","","CMAP, LMO"],["Chzhen","Evgenii","","LMO, CELESTE"],["Stoltz","Gilles","","LMO, CELESTE"]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 08:06:45 GMT"}],"updateDate":"2024-07-09","timestamp":1720426005000,"abstract":"  In this paper, we consider the problem of learning in adversarial Markov\ndecision processes [MDPs] with an oblivious adversary in a full-information\nsetting. The agent interacts with an environment during $T$ episodes, each of\nwhich consists of $H$ stages, and each episode is evaluated with respect to a\nreward function that will be revealed only at the end of the episode. We\npropose an algorithm, called APO-MVP, that achieves a regret bound of order\n$\\tilde{\\mathcal{O}}(\\mathrm{poly}(H)\\sqrt{SAT})$, where $S$ and $A$ are sizes\nof the state and action spaces, respectively. This result improves upon the\nbest-known regret bound by a factor of $\\sqrt{S}$, bridging the gap between\nadversarial and stochastic MDPs, and matching the minimax lower bound\n$\\Omega(\\sqrt{H^3SAT})$ as far as the dependencies in $S,A,T$ are concerned.\nThe proposed algorithm and analysis completely avoid the typical tool given by\noccupancy measures; instead, it performs policy optimization based only on\ndynamic programming and on a black-box online linear optimization strategy run\nover estimated advantage functions, making it easy to implement. The analysis\nleverages two recent techniques: policy optimization based on online linear\noptimization strategies (Jonckheere et al., 2023) and a refined martingale\nanalysis of the impact on values of estimating transitions kernels (Zhang et\nal., 2023).\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}