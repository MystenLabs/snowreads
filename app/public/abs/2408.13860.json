{"id":"2408.13860","title":"Knowledge-Aware Reasoning over Multimodal Semi-structured Tables","authors":"Suyash Vardhan Mathur, Jainit Sushil Bafna, Kunal Kartik, Harshita\n  Khandelwal, Manish Shrivastava, Vivek Gupta, Mohit Bansal, Dan Roth","authorsParsed":[["Mathur","Suyash Vardhan",""],["Bafna","Jainit Sushil",""],["Kartik","Kunal",""],["Khandelwal","Harshita",""],["Shrivastava","Manish",""],["Gupta","Vivek",""],["Bansal","Mohit",""],["Roth","Dan",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 15:17:43 GMT"}],"updateDate":"2024-08-27","timestamp":1724599063000,"abstract":"  Existing datasets for tabular question answering typically focus exclusively\non text within cells. However, real-world data is inherently multimodal, often\nblending images such as symbols, faces, icons, patterns, and charts with\ntextual content in tables. With the evolution of AI models capable of\nmultimodal reasoning, it is pertinent to assess their efficacy in handling such\nstructured data. This study investigates whether current AI models can perform\nknowledge-aware reasoning on multimodal structured data. We explore their\nability to reason on tables that integrate both images and text, introducing\nMMTabQA, a new dataset designed for this purpose. Our experiments highlight\nsubstantial challenges for current AI models in effectively integrating and\ninterpreting multiple text and image inputs, understanding visual context, and\ncomparing visual content across images. These findings establish our dataset as\na robust benchmark for advancing AI's comprehension and capabilities in\nanalyzing multimodal structured data.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4OlTobOduLdgmg3jQSFWAO61zOSgVfHDeeDPpcZxMIY","pdfSize":"5766138"}
