{"id":"2408.11901","title":"A Unified Theory of Quantum Neural Network Loss Landscapes","authors":"Eric R. Anschuetz","authorsParsed":[["Anschuetz","Eric R.",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 18:00:08 GMT"}],"updateDate":"2024-08-23","timestamp":1724263208000,"abstract":"  Classical neural networks with random initialization famously behave as\nGaussian processes in the limit of many neurons, with the architecture of the\nnetwork determining the covariance of the associated process. This limit allows\none to completely characterize the training behavior of such networks and show\nthat, generally, classical neural networks train efficiently via gradient\ndescent. No such general understanding exists for quantum neural networks\n(QNNs), which -- outside of certain special cases -- are known to not behave as\nGaussian processes when randomly initialized.\n  We here prove that instead QNNs and their first two derivatives generally\nform what we call Wishart processes, where now certain algebraic properties of\nthe network determine the hyperparameters of the process. This Wishart process\ndescription allows us to, for the first time:\n  1. Give necessary and sufficient conditions for a QNN architecture to have a\nGaussian process limit.\n  2. Calculate the full gradient distribution, unifying previously known barren\nplateau results.\n  3. Calculate the local minima distribution of algebraically constrained QNNs.\n  The transition from trainability to untrainability in each of these contexts\nis governed by a single parameter we call the \"degrees of freedom\" of the\nnetwork architecture. We thus end by proposing a formal definition for the\n\"trainability\" of a given QNN architecture using this experimentally accessible\nquantity.\n","subjects":["Physics/Quantum Physics","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}