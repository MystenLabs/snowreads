{"id":"2408.11729","title":"LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification\n  Testsuites","authors":"Zachariah Sollenberger, Jay Patel, Christian Munley, Aaron Jarmusch,\n  Sunita Chandrasekaran","authorsParsed":[["Sollenberger","Zachariah",""],["Patel","Jay",""],["Munley","Christian",""],["Jarmusch","Aaron",""],["Chandrasekaran","Sunita",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 15:54:17 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 02:38:56 GMT"}],"updateDate":"2024-09-04","timestamp":1724255657000,"abstract":"  Large Language Models (LLM) are evolving and have significantly\nrevolutionized the landscape of software development. If used well, they can\nsignificantly accelerate the software development cycle. At the same time, the\ncommunity is very cautious of the models being trained on biased or sensitive\ndata, which can lead to biased outputs along with the inadvertent release of\nconfidential information. Additionally, the carbon footprints and the\nun-explainability of these black box models continue to raise questions about\nthe usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores\nthe idea of judging tests used to evaluate compiler implementations of\ndirective-based programming models as well as probe into the black box of LLMs.\nBased on our results, utilizing an agent-based prompting approach and setting\nup a validation pipeline structure drastically increased the quality of\nDeepSeek Coder, the LLM chosen for the evaluation purposes.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CiL1FW6MraIcBwtmRvydfJNAm1zetfmPf2KZnM0FNco","pdfSize":"354393"}
