{"id":"2407.16181","title":"Structural Optimization Ambiguity and Simplicity Bias in Unsupervised\n  Neural Grammar Induction","authors":"Jinwook Park, Kangil Kim","authorsParsed":[["Park","Jinwook",""],["Kim","Kangil",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 04:57:03 GMT"}],"updateDate":"2024-07-24","timestamp":1721710623000,"abstract":"  Neural parameterization has significantly advanced unsupervised grammar\ninduction. However, training these models with a traditional likelihood loss\nfor all possible parses exacerbates two issues: 1) $\\textit{structural\noptimization ambiguity}$ that arbitrarily selects one among structurally\nambiguous optimal grammars despite the specific preference of gold parses, and\n2) $\\textit{structural simplicity bias}$ that leads a model to underutilize\nrules to compose parse trees. These challenges subject unsupervised neural\ngrammar induction (UNGI) to inevitable prediction errors, high variance, and\nthe necessity for extensive grammars to achieve accurate predictions. This\npaper tackles these issues, offering a comprehensive analysis of their origins.\nAs a solution, we introduce $\\textit{sentence-wise parse-focusing}$ to reduce\nthe parse pool per sentence for loss evaluation, using the structural bias from\npre-trained parsers on the same dataset. In unsupervised parsing benchmark\ntests, our method significantly improves performance while effectively reducing\nvariance and bias toward overly simplistic parses. Our research promotes\nlearning more compact, accurate, and consistent explicit grammars, facilitating\nbetter interpretability.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}