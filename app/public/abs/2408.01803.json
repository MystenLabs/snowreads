{"id":"2408.01803","title":"STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs","authors":"Peijie Dong, Lujun Li, Dayou Du, Yuhan Chen, Zhenheng Tang, Qiang\n  Wang, Wei Xue, Wenhan Luo, Qifeng Liu, Yike Guo, Xiaowen Chu","authorsParsed":[["Dong","Peijie",""],["Li","Lujun",""],["Du","Dayou",""],["Chen","Yuhan",""],["Tang","Zhenheng",""],["Wang","Qiang",""],["Xue","Wei",""],["Luo","Wenhan",""],["Liu","Qifeng",""],["Guo","Yike",""],["Chu","Xiaowen",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 15:07:44 GMT"}],"updateDate":"2024-08-06","timestamp":1722697664000,"abstract":"  In this paper, we present STBLLM, the first structural binarization framework\nfor compressing Large Language Models (LLMs) to less than 1-bit precision. LLMs\nhave achieved remarkable performance, but their heavy memory requirements have\nhindered widespread adoption, particularly on resource-constrained devices.\nBinarization, which quantifies weights to a mere 1-bit, achieves a milestone in\nincreasing computational efficiency. However, we observe that some weights in\nbinarized LLMs can be randomly flipped without significant performance\ndegradation, indicating the potential for further compression. To exploit this,\nour STBLLM employs an N:M sparsity to perform structural binarization of the\nweights. First, we introduce a new Standardized Importance (SI) metric that\nconsiders weight magnitude and input feature norm to better evaluate weight\nsignificance. Then, we propose a layer-wise approach where different layers of\nthe LLM can be sparsified with varying N:M ratios, balancing compression and\naccuracy. Finally, we use residual approximation with double binarization to\npreserve information for salient weights. In addition, we utilize a\nfine-grained grouping strategy for less important weights that applies\ndifferent quantization schemes to sparse, intermediate, and dense regions. We\nconduct extensive experiments on various language models, including the\nLLaMA-1/2/3, OPT family, and Mistral, to evaluate the effectiveness of STBLLM.\nThe results demonstrate that our approach performs better than other compressed\nbinarization LLM methods while significantly reducing memory requirements.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gf7Wy26qZL0U2wJ-meuLF8nfVdp_cBCIBo5dmw9P6Ns","pdfSize":"1154849","txDigest":"2Yrcvux2cWBWXfKKGvxcZXfsURK4gr5HPABtB55s67mj","endEpoch":"1","status":"CERTIFIED"}
