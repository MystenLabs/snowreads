{"id":"2408.09640","title":"Acquiring Bidirectionality via Large and Small Language Models","authors":"Takumi Goto, Hiroyoshi Nagao, Yuta Koreeda","authorsParsed":[["Goto","Takumi",""],["Nagao","Hiroyoshi",""],["Koreeda","Yuta",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 01:54:37 GMT"}],"updateDate":"2024-08-20","timestamp":1724032477000,"abstract":"  Using token representation from bidirectional language models (LMs) such as\nBERT is still a widely used approach for token-classification tasks. Even\nthough there exist much larger unidirectional LMs such as Llama-2, they are\nrarely used to replace the token representation of bidirectional LMs. In this\nwork, we hypothesize that their lack of bidirectionality is keeping them\nbehind. To that end, we propose to newly train a small backward LM and\nconcatenate its representations to those of existing LM for downstream tasks.\nThrough experiments in named entity recognition, we demonstrate that\nintroducing backward model improves the benchmark performance more than 10\npoints. Furthermore, we show that the proposed method is especially effective\nfor rare domains and in few-shot learning settings.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}