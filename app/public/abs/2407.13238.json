{"id":"2407.13238","title":"Transformers with Stochastic Competition for Tabular Data Modelling","authors":"Andreas Voskou, Charalambos Christoforou, Sotirios Chatzis","authorsParsed":[["Voskou","Andreas",""],["Christoforou","Charalambos",""],["Chatzis","Sotirios",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 07:48:48 GMT"}],"updateDate":"2024-07-19","timestamp":1721288928000,"abstract":"  Despite the prevalence and significance of tabular data across numerous\nindustries and fields, it has been relatively underexplored in the realm of\ndeep learning. Even today, neural networks are often overshadowed by techniques\nsuch as gradient boosted decision trees (GBDT). However, recent models are\nbeginning to close this gap, outperforming GBDT in various setups and garnering\nincreased attention in the field. Inspired by this development, we introduce a\nnovel stochastic deep learning model specifically designed for tabular data.\nThe foundation of this model is a Transformer-based architecture, carefully\nadapted to cater to the unique properties of tabular data through strategic\narchitectural modifications and leveraging two forms of stochastic competition.\nFirst, we employ stochastic \"Local Winner Takes All\" units to promote\ngeneralization capacity through stochasticity and sparsity. Second, we\nintroduce a novel embedding layer that selects among alternative linear\nembedding layers through a mechanism of stochastic competition. The\neffectiveness of the model is validated on a variety of widely-used, publicly\navailable datasets. We demonstrate that, through the incorporation of these\nelements, our model yields high performance and marks a significant advancement\nin the application of deep learning to tabular data.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LOsA-srlMVxNYYZK_vdwbYB5kXAZgesqFMxSmKiX-Bo","pdfSize":"523196"}
