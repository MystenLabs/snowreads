{"id":"2408.05540","title":"Convergence Analysis for Deep Sparse Coding via Convolutional Neural\n  Networks","authors":"Jianfei Li and Han Feng and Ding-Xuan Zhou","authorsParsed":[["Li","Jianfei",""],["Feng","Han",""],["Zhou","Ding-Xuan",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 12:43:55 GMT"}],"updateDate":"2024-08-13","timestamp":1723293835000,"abstract":"  In this work, we explore the intersection of sparse coding theory and deep\nlearning to enhance our understanding of feature extraction capabilities in\nadvanced neural network architectures. We begin by introducing a novel class of\nDeep Sparse Coding (DSC) models and establish a thorough theoretical analysis\nof their uniqueness and stability properties. By applying iterative algorithms\nto these DSC models, we derive convergence rates for convolutional neural\nnetworks (CNNs) in their ability to extract sparse features. This provides a\nstrong theoretical foundation for the use of CNNs in sparse feature learning\ntasks. We additionally extend this convergence analysis to more general neural\nnetwork architectures, including those with diverse activation functions, as\nwell as self-attention and transformer-based models. This broadens the\napplicability of our findings to a wide range of deep learning methods for deep\nsparse feature extraction. Inspired by the strong connection between sparse\ncoding and CNNs, we also explore training strategies to encourage neural\nnetworks to learn more sparse features. Through numerical experiments, we\ndemonstrate the effectiveness of these approaches, providing valuable insights\nfor the design of efficient and interpretable deep learning models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Data Structures and Algorithms","Computing Research Repository/Information Theory","Computing Research Repository/Neural and Evolutionary Computing","Mathematics/Information Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}