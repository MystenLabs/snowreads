{"id":"2408.12970","title":"SUMO: Search-Based Uncertainty Estimation for Model-Based Offline\n  Reinforcement Learning","authors":"Zhongjian Qiao, Jiafei Lyu, Kechen Jiao, Qi Liu, Xiu Li","authorsParsed":[["Qiao","Zhongjian",""],["Lyu","Jiafei",""],["Jiao","Kechen",""],["Liu","Qi",""],["Li","Xiu",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 10:36:08 GMT"}],"updateDate":"2024-08-26","timestamp":1724409368000,"abstract":"  The performance of offline reinforcement learning (RL) suffers from the\nlimited size and quality of static datasets. Model-based offline RL addresses\nthis issue by generating synthetic samples through a dynamics model to enhance\noverall performance. To evaluate the reliability of the generated samples,\nuncertainty estimation methods are often employed. However, model ensemble, the\nmost commonly used uncertainty estimation method, is not always the best\nchoice. In this paper, we propose a \\textbf{S}earch-based \\textbf{U}ncertainty\nestimation method for \\textbf{M}odel-based \\textbf{O}ffline RL (SUMO) as an\nalternative. SUMO characterizes the uncertainty of synthetic samples by\nmeasuring their cross entropy against the in-distribution dataset samples, and\nuses an efficient search-based method for implementation. In this way, SUMO can\nachieve trustworthy uncertainty estimation. We integrate SUMO into several\nmodel-based offline RL algorithms including MOPO and Adapted MOReL (AMOReL),\nand provide theoretical analysis for them. Extensive experimental results on\nD4RL datasets demonstrate that SUMO can provide more accurate uncertainty\nestimation and boost the performance of base algorithms. These indicate that\nSUMO could be a better uncertainty estimator for model-based offline RL when\nused in either reward penalty or trajectory truncation. Our code is available\nand will be open-source for further research and development.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}