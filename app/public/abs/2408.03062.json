{"id":"2408.03062","title":"Analysis of Argument Structure Constructions in a Deep Recurrent\n  Language Model","authors":"Pegah Ramezani, Achim Schilling, Patrick Krauss","authorsParsed":[["Ramezani","Pegah",""],["Schilling","Achim",""],["Krauss","Patrick",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 09:27:41 GMT"}],"updateDate":"2024-08-07","timestamp":1722936461000,"abstract":"  Understanding how language and linguistic constructions are processed in the\nbrain is a fundamental question in cognitive computational neuroscience. In\nthis study, we explore the representation and processing of Argument Structure\nConstructions (ASCs) in a recurrent neural language model. We trained a Long\nShort-Term Memory (LSTM) network on a custom-made dataset consisting of 2000\nsentences, generated using GPT-4, representing four distinct ASCs: transitive,\nditransitive, caused-motion, and resultative constructions.\n  We analyzed the internal activations of the LSTM model's hidden layers using\nMultidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding\n(t-SNE) to visualize the sentence representations. The Generalized\nDiscrimination Value (GDV) was calculated to quantify the degree of clustering\nwithin these representations. Our results show that sentence representations\nform distinct clusters corresponding to the four ASCs across all hidden layers,\nwith the most pronounced clustering observed in the last hidden layer before\nthe output layer. This indicates that even a relatively simple,\nbrain-constrained recurrent neural network can effectively differentiate\nbetween various construction types.\n  These findings are consistent with previous studies demonstrating the\nemergence of word class and syntax rule representations in recurrent language\nmodels trained on next word prediction tasks. In future work, we aim to\nvalidate these results using larger language models and compare them with\nneuroimaging data obtained during continuous speech perception. This study\nhighlights the potential of recurrent neural language models to mirror\nlinguistic processing in the human brain, providing valuable insights into the\ncomputational and neural mechanisms underlying language understanding.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}