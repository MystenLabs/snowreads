{"id":"2408.09686","title":"Algorithmic Contract Design with Reinforcement Learning Agents","authors":"David Molina Concha, Kyeonghyeon Park, Hyun-Rok Lee, Taesik Lee,\n  Chi-Guhn Lee","authorsParsed":[["Concha","David Molina",""],["Park","Kyeonghyeon",""],["Lee","Hyun-Rok",""],["Lee","Taesik",""],["Lee","Chi-Guhn",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 03:48:38 GMT"}],"updateDate":"2024-08-20","timestamp":1724039318000,"abstract":"  We introduce a novel problem setting for algorithmic contract design, named\nthe principal-MARL contract design problem. This setting extends traditional\ncontract design to account for dynamic and stochastic environments using Markov\nGames and Multi-Agent Reinforcement Learning. To tackle this problem, we\npropose a Multi-Objective Bayesian Optimization (MOBO) framework named\nConstrained Pareto Maximum Entropy Search (cPMES). Our approach integrates MOBO\nand MARL to explore the highly constrained contract design space, identifying\npromising incentive and recruitment decisions. cPMES transforms the\nprincipal-MARL contract design problem into an unconstrained multi-objective\nproblem, leveraging the probability of feasibility as part of the objectives\nand ensuring promising designs predicted on the feasibility border are included\nin the Pareto front. By focusing the entropy prediction on designs within the\nPareto set, cPMES mitigates the risk of the search strategy being overwhelmed\nby entropy from constraints. We demonstrate the effectiveness of cPMES through\nextensive benchmark studies in synthetic and simulated environments, showing\nits ability to find feasible contract designs that maximize the principal's\nobjectives. Additionally, we provide theoretical support with a sub-linear\nregret bound concerning the number of iterations.\n","subjects":["Computing Research Repository/Multiagent Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}