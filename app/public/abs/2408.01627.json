{"id":"2408.01627","title":"JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid\n  Transformer-Mamba Model","authors":"Farzaneh Jafari, Stefano Berretti, Anup Basu","authorsParsed":[["Jafari","Farzaneh",""],["Berretti","Stefano",""],["Basu","Anup",""]],"versions":[{"version":"v1","created":"Sat, 3 Aug 2024 01:38:11 GMT"}],"updateDate":"2024-08-08","timestamp":1722649091000,"abstract":"  In recent years, talking head generation has become a focal point for\nresearchers. Considerable effort is being made to refine lip-sync motion,\ncapture expressive facial expressions, generate natural head poses, and achieve\nhigh video quality. However, no single model has yet achieved equivalence\nacross all these metrics. This paper aims to animate a 3D face using Jamba, a\nhybrid Transformers-Mamba model. Mamba, a pioneering Structured State Space\nModel (SSM) architecture, was designed to address the constraints of the\nconventional Transformer architecture. Nevertheless, it has several drawbacks.\nJamba merges the advantages of both Transformer and Mamba approaches, providing\na holistic solution. Based on the foundational Jamba block, we present\nJambaTalk to enhance motion variety and speed through multimodal integration.\nExtensive experiments reveal that our method achieves performance comparable or\nsuperior to state-of-the-art models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}