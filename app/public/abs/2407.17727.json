{"id":"2407.17727","title":"Distributed Memory Approximate Message Passing","authors":"Jun Lu, Lei Liu, Shunqi Huang, Ning Wei, Xiaoming Chen","authorsParsed":[["Lu","Jun",""],["Liu","Lei",""],["Huang","Shunqi",""],["Wei","Ning",""],["Chen","Xiaoming",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 02:58:35 GMT"}],"updateDate":"2024-07-26","timestamp":1721876315000,"abstract":"  Approximate message passing (AMP) algorithms are iterative methods for signal\nrecovery in noisy linear systems. In some scenarios, AMP algorithms need to\noperate within a distributed network. To address this challenge, the\ndistributed extensions of AMP (D-AMP, FD-AMP) and orthogonal/vector AMP\n(D-OAMP/D-VAMP) were proposed, but they still inherit the limitations of\ncentralized algorithms. In this letter, we propose distributed memory AMP\n(D-MAMP) to overcome the IID matrix limitation of D-AMP/FD-AMP, as well as the\nhigh complexity and heavy communication cost of D-OAMP/D-VAMP. We introduce a\nmatrix-by-vector variant of MAMP tailored for distributed computing. Leveraging\nthis variant, D-MAMP enables each node to execute computations utilizing\nlocally available observation vectors and transform matrices. Meanwhile, global\nsummations of locally updated results are conducted through message interaction\namong nodes. For acyclic graphs, D-MAMP converges to the same mean square error\nperformance as the centralized MAMP.\n","subjects":["Electrical Engineering and Systems Science/Signal Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}