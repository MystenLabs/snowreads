{"id":"2407.02914","title":"The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design\n  Trade-Offs in Ensemble Learning Systems","authors":"Rafiullah Omar, Justus Bogner, Henry Muccini, Patricia Lago, Silverio\n  Mart\\'inez-Fern\\'andez, Xavier Franch","authorsParsed":[["Omar","Rafiullah",""],["Bogner","Justus",""],["Muccini","Henry",""],["Lago","Patricia",""],["Martínez-Fernández","Silverio",""],["Franch","Xavier",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 08:45:17 GMT"}],"updateDate":"2024-07-04","timestamp":1719996317000,"abstract":"  Background: Machine learning (ML) model composition is a popular technique to\nmitigate shortcomings of a single ML model and to design more effective\nML-enabled systems. While ensemble learning, i.e., forwarding the same request\nto several models and fusing their predictions, has been studied extensively\nfor accuracy, we have insufficient knowledge about how to design\nenergy-efficient ensembles. Objective: We therefore analyzed three types of\ndesign decisions for ensemble learning regarding a potential trade-off between\naccuracy and energy consumption: a) ensemble size, i.e., the number of models\nin the ensemble, b) fusion methods (majority voting vs. a meta-model), and c)\npartitioning methods (whole-dataset vs. subset-based training). Methods: By\ncombining four popular ML algorithms for classification in different ensembles,\nwe conducted a full factorial experiment with 11 ensembles x 4 datasets x 2\nfusion methods x 2 partitioning methods (176 combinations). For each\ncombination, we measured accuracy (F1-score) and energy consumption in J (for\nboth training and inference). Results: While a larger ensemble size\nsignificantly increased energy consumption (size 2 ensembles consumed 37.49%\nless energy than size 3 ensembles, which in turn consumed 26.96% less energy\nthan the size 4 ensembles), it did not significantly increase accuracy.\nFurthermore, majority voting outperformed meta-model fusion both in terms of\naccuracy (Cohen's d of 0.38) and energy consumption (Cohen's d of 0.92).\nLastly, subset-based training led to significantly lower energy consumption\n(Cohen's d of 0.91), while training on the whole dataset did not increase\naccuracy significantly. Conclusions: From a Green AI perspective, we recommend\ndesigning ensembles of small size (2 or maximum 3 models), using subset-based\ntraining, majority voting, and energy-efficient ML algorithms like decision\ntrees, Naive Bayes, or KNN.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}