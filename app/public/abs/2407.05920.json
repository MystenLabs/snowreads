{"id":"2407.05920","title":"LPGD: A General Framework for Backpropagation through Embedded\n  Optimization Layers","authors":"Anselm Paulus and Georg Martius and V\\'it Musil","authorsParsed":[["Paulus","Anselm",""],["Martius","Georg",""],["Musil","VÃ­t",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 13:27:41 GMT"}],"updateDate":"2024-07-09","timestamp":1720445261000,"abstract":"  Embedding parameterized optimization problems as layers into machine learning\narchitectures serves as a powerful inductive bias. Training such architectures\nwith stochastic gradient descent requires care, as degenerate derivatives of\nthe embedded optimization problem often render the gradients uninformative. We\npropose Lagrangian Proximal Gradient Descent (LPGD) a flexible framework for\ntraining architectures with embedded optimization layers that seamlessly\nintegrates into automatic differentiation libraries. LPGD efficiently computes\nmeaningful replacements of the degenerate optimization layer derivatives by\nre-running the forward solver oracle on a perturbed input. LPGD captures\nvarious previously proposed methods as special cases, while fostering deep\nlinks to traditional optimization methods. We theoretically analyze our method\nand demonstrate on historical and synthetic data that LPGD converges faster\nthan gradient descent even in a differentiable setup.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}