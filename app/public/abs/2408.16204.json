{"id":"2408.16204","title":"Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient\n  Manipulation","authors":"Lun Wang","authorsParsed":[["Wang","Lun",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 01:50:13 GMT"}],"updateDate":"2024-08-30","timestamp":1724896213000,"abstract":"  Micro-batch clipping, a gradient clipping method, has recently shown\npotential in enhancing auto-speech recognition (ASR) model performance.\nHowever, the underlying mechanism behind this improvement remains mysterious,\nparticularly the observation that only certain micro-batch sizes are\nbeneficial. In this paper, we make the first attempt to explain this\nphenomenon. Inspired by recent data pruning research, we assume that specific\ntraining samples may impede model convergence during certain training phases.\nUnder this assumption, the convergence analysis shows that micro-batch clipping\ncan improve the convergence rate asymptotically at the cost of an additional\nconstant bias that does not diminish with more training iterations. The bias is\ndependent on a few factors and can be minimized at specific micro-batch size,\nthereby elucidating the existence of the sweet-spot micro-batch size observed\npreviously. We also verify the effectiveness of micro-batch clipping beyond\nspeech models on vision and language models, and show promising performance\ngains in these domains. An exploration of potential limitations shows that\nmicro-batch clipping is less effective when training data originates from\nmultiple distinct domains.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nv_TUd5knvLrWv3O-TQ1THqwvgZ6JQA4GtwhqhjJh-k","pdfSize":"338555"}
