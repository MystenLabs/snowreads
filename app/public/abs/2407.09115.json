{"id":"2407.09115","title":"Layer-Wise Relevance Propagation with Conservation Property for ResNet","authors":"Seitaro Otsuki, Tsumugi Iida, F\\'elix Doublet, Tsubasa Hirakawa,\n  Takayoshi Yamashita, Hironobu Fujiyoshi and Komei Sugiura","authorsParsed":[["Otsuki","Seitaro",""],["Iida","Tsumugi",""],["Doublet","FÃ©lix",""],["Hirakawa","Tsubasa",""],["Yamashita","Takayoshi",""],["Fujiyoshi","Hironobu",""],["Sugiura","Komei",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 09:30:09 GMT"}],"updateDate":"2024-07-15","timestamp":1720776609000,"abstract":"  The transparent formulation of explanation methods is essential for\nelucidating the predictions of neural networks, which are typically black-box\nmodels. Layer-wise Relevance Propagation (LRP) is a well-established method\nthat transparently traces the flow of a model's prediction backward through its\narchitecture by backpropagating relevance scores. However, the conventional LRP\ndoes not fully consider the existence of skip connections, and thus its\napplication to the widely used ResNet architecture has not been thoroughly\nexplored. In this study, we extend LRP to ResNet models by introducing\nRelevance Splitting at points where the output from a skip connection converges\nwith that from a residual block. Our formulation guarantees the conservation\nproperty throughout the process, thereby preserving the integrity of the\ngenerated explanations. To evaluate the effectiveness of our approach, we\nconduct experiments on ImageNet and the Caltech-UCSD Birds-200-2011 dataset.\nOur method achieves superior performance to that of baseline methods on\nstandard evaluation metrics such as the Insertion-Deletion score while\nmaintaining its conservation property. We will release our code for further\nresearch at https://5ei74r0.github.io/lrp-for-resnet.page/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}