{"id":"2407.18264","title":"Latency optimized Deep Neural Networks (DNNs): An Artificial\n  Intelligence approach at the Edge using Multiprocessor System on Chip (MPSoC)","authors":"Seyed Nima Omidsajedi, Rekha Reddy, Jianming Yi, Jan Herbst, Christoph\n  Lipps, Hans Dieter Schotten","authorsParsed":[["Omidsajedi","Seyed Nima",""],["Reddy","Rekha",""],["Yi","Jianming",""],["Herbst","Jan",""],["Lipps","Christoph",""],["Schotten","Hans Dieter",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 11:51:41 GMT"}],"updateDate":"2024-07-29","timestamp":1721130701000,"abstract":"  Almost in every heavily computation-dependent application, from 6G\ncommunication systems to autonomous driving platforms, a large portion of\ncomputing should be near to the client side. Edge computing (AI at Edge) in\nmobile devices is one of the optimized approaches for addressing this\nrequirement. Therefore, in this work, the possibilities and challenges of\nimplementing a low-latency and power-optimized smart mobile system are\nexamined. Utilizing Field Programmable Gate Array (FPGA) based solutions at the\nedge will lead to bandwidth-optimized designs and as a consequence can boost\nthe computational effectiveness at a system-level deadline. Moreover, various\nperformance aspects and implementation feasibilities of Neural Networks (NNs)\non both embedded FPGA edge devices (using Xilinx Multiprocessor System on Chip\n(MPSoC)) and Cloud are discussed throughout this research. The main goal of\nthis work is to demonstrate a hybrid system that uses the deep learning\nprogrammable engine developed by Xilinx Inc. as the main component of the\nhardware accelerator. Then based on this design, an efficient system for mobile\nedge computing is represented by utilizing an embedded solution.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}