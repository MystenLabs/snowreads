{"id":"2408.09807","title":"World Models Increase Autonomy in Reinforcement Learning","authors":"Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Edward S. Hu","authorsParsed":[["Yang","Zhao",""],["Moerland","Thomas M.",""],["Preuss","Mike",""],["Plaat","Aske",""],["Hu","Edward S.",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 08:56:00 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 09:23:34 GMT"}],"updateDate":"2024-08-21","timestamp":1724057760000,"abstract":"  Reinforcement learning (RL) is an appealing paradigm for training intelligent\nagents, enabling policy acquisition from the agent's own autonomously acquired\nexperience. However, the training process of RL is far from automatic,\nrequiring extensive human effort to reset the agent and environments. To tackle\nthe challenging reset-free setting, we first demonstrate the superiority of\nmodel-based (MB) RL methods in such setting, showing that a straightforward\nadaptation of MBRL can outperform all the prior state-of-the-art methods while\nrequiring less supervision. We then identify limitations inherent to this\ndirect extension and propose a solution called model-based reset-free\n(MoReFree) agent, which further enhances the performance. MoReFree adapts two\nkey mechanisms, exploration and policy learning, to handle reset-free tasks by\nprioritizing task-relevant states. It exhibits superior data-efficiency across\nvarious reset-free tasks without access to environmental reward or\ndemonstrations while significantly outperforming privileged baselines that\nrequire supervision. Our findings suggest model-based methods hold significant\npromise for reducing human effort in RL. Website:\nhttps://sites.google.com/view/morefree\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7OFpe0xPbRdXcUkycR4AvzNHdmF6O5ZJeB0GSYSfQG4","pdfSize":"18265922"}
