{"id":"2408.13040","title":"SpeechPrompt: Prompting Speech Language Models for Speech Processing\n  Tasks","authors":"Kai-Wei Chang, Haibin Wu, Yu-Kai Wang, Yuan-Kuei Wu, Hua Shen,\n  Wei-Cheng Tseng, Iu-thing Kang, Shang-Wen Li, Hung-yi Lee","authorsParsed":[["Chang","Kai-Wei",""],["Wu","Haibin",""],["Wang","Yu-Kai",""],["Wu","Yuan-Kuei",""],["Shen","Hua",""],["Tseng","Wei-Cheng",""],["Kang","Iu-thing",""],["Li","Shang-Wen",""],["Lee","Hung-yi",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 13:00:10 GMT"}],"updateDate":"2024-08-26","timestamp":1724418010000,"abstract":"  Prompting has become a practical method for utilizing pre-trained language\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\nto new tasks with minimal training and parameter updates, thus achieving\nefficiency in both storage and computation. Additionally, prompting modifies\nonly the LM's inputs and harnesses the generative capabilities of language\nmodels to address various downstream tasks in a unified manner. This\nsignificantly reduces the need for human labor in designing task-specific\nmodels. These advantages become even more evident as the number of tasks served\nby the LM scales up. Motivated by the strengths of prompting, we are the first\nto explore the potential of prompting speech LMs in the domain of speech\nprocessing. Recently, there has been a growing interest in converting speech\ninto discrete units for language modeling. Our pioneer research demonstrates\nthat these quantized speech units are highly versatile within our unified\nprompting framework. Not only can they serve as class labels, but they also\ncontain rich phonetic information that can be re-synthesized back into speech\nsignals for speech generation tasks. Specifically, we reformulate speech\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\nseamlessly integrate tasks such as speech classification, sequence generation,\nand speech generation within a single, unified prompting framework. The\nexperiment results show that the prompting method can achieve competitive\nperformance compared to the strong fine-tuning method based on self-supervised\nlearning models with a similar number of trainable parameters. The prompting\nmethod also shows promising results in the few-shot setting. Moreover, with the\nadvanced speech LMs coming into the stage, the proposed prompting framework\nattains great potential.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}