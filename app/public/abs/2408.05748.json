{"id":"2408.05748","title":"Low-Dimensional Federated Knowledge Graph Embedding via Knowledge\n  Distillation","authors":"Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, Zhiqi Shen","authorsParsed":[["Zhang","Xiaoxiong",""],["Zeng","Zhiwei",""],["Zhou","Xin",""],["Shen","Zhiqi",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 11:15:41 GMT"}],"updateDate":"2024-08-13","timestamp":1723374941000,"abstract":"  Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative\nlearning of entity and relation embeddings from distributed Knowledge Graphs\n(KGs) across multiple clients, while preserving data privacy. Training FKGE\nmodels with higher dimensions is typically favored due to their potential for\nachieving superior performance. However, high-dimensional embeddings present\nsignificant challenges in terms of storage resource and inference speed. Unlike\ntraditional KG embedding methods, FKGE involves multiple client-server\ncommunication rounds, where communication efficiency is critical. Existing\nembedding compression methods for traditional KGs may not be directly\napplicable to FKGE as they often require multiple model trainings which\npotentially incur substantial communication costs. In this paper, we propose a\nlight-weight component based on Knowledge Distillation (KD) which is titled\nFedKD and tailored specifically for FKGE methods. During client-side local\ntraining, FedKD facilitates the low-dimensional student model to mimic the\nscore distribution of triples from the high-dimensional teacher model using KL\ndivergence loss. Unlike traditional KD way, FedKD adaptively learns a\ntemperature to scale the score of positive triples and separately adjusts the\nscores of corresponding negative triples using a predefined temperature,\nthereby mitigating teacher over-confidence issue. Furthermore, we dynamically\nadjust the weight of KD loss to optimize the training process. Extensive\nexperiments on three datasets support the effectiveness of FedKD.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}