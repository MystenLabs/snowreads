{"id":"2407.08213","title":"PrefCLM: Enhancing Preference-based Reinforcement Learning with\n  Crowdsourced Large Language Models","authors":"Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Ike Obi, and Byung-Cheol Min","authorsParsed":[["Wang","Ruiqi",""],["Zhao","Dezhong",""],["Yuan","Ziqin",""],["Obi","Ike",""],["Min","Byung-Cheol",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 06:30:46 GMT"}],"updateDate":"2024-07-12","timestamp":1720679446000,"abstract":"  Preference-based reinforcement learning (PbRL) is emerging as a promising\napproach to teaching robots through human comparative feedback, sidestepping\nthe need for complex reward engineering. However, the substantial volume of\nfeedback required in existing PbRL methods often lead to reliance on synthetic\nfeedback generated by scripted teachers. This approach necessitates intricate\nreward engineering again and struggles to adapt to the nuanced preferences\nparticular to human-robot interaction (HRI) scenarios, where users may have\nunique expectations toward the same task. To address these challenges, we\nintroduce PrefCLM, a novel framework that utilizes crowdsourced large language\nmodels (LLMs) as simulated teachers in PbRL. We utilize Dempster-Shafer Theory\nto fuse individual preferences from multiple LLM agents at the score level,\nefficiently leveraging their diversity and collective intelligence. We also\nintroduce a human-in-the-loop pipeline that facilitates collective refinements\nbased on user interactive feedback. Experimental results across various general\nRL tasks show that PrefCLM achieves competitive performance compared to\ntraditional scripted teachers and excels in facilitating more more natural and\nefficient behaviors. A real-world user study (N=10) further demonstrates its\ncapability to tailor robot behaviors to individual user preferences,\nsignificantly enhancing user satisfaction in HRI scenarios.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}