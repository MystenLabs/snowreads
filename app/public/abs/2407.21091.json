{"id":"2407.21091","title":"The Stochastic Conjugate Subgradient Algorithm For Kernel Support Vector\n  Machines","authors":"Di Zhang and Suvrajeet Sen","authorsParsed":[["Zhang","Di",""],["Sen","Suvrajeet",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 17:03:19 GMT"}],"updateDate":"2024-08-01","timestamp":1722358999000,"abstract":"  Stochastic First-Order (SFO) methods have been a cornerstone in addressing a\nbroad spectrum of modern machine learning (ML) challenges. However, their\nefficacy is increasingly questioned, especially in large-scale applications\nwhere empirical evidence indicates potential performance limitations. In\nresponse, this paper proposes an innovative method specifically designed for\nkernel support vector machines (SVMs). This method not only achieves faster\nconvergence per iteration but also exhibits enhanced scalability when compared\nto conventional SFO techniques. Diverging from traditional sample average\napproximation strategies that typically frame kernel SVM as an 'all-in-one'\nQuadratic Program (QP), our approach adopts adaptive sampling. This strategy\nincrementally refines approximation accuracy on an 'as-needed' basis.\nCrucially, this approach also inspires a decomposition-based algorithm,\neffectively decomposing parameter selection from error estimation, with the\nlatter being independently determined for each data point. To exploit the\nquadratic nature of the kernel matrix, we introduce a stochastic conjugate\nsubgradient method. This method preserves many benefits of first-order\napproaches while adeptly handling both nonlinearity and non-smooth aspects of\nthe SVM problem. Thus, it extends beyond the capabilities of standard SFO\nalgorithms for non-smooth convex optimization. The convergence rate of this\nnovel method is thoroughly analyzed within this paper. Our experimental results\ndemonstrate that the proposed algorithm not only maintains but potentially\nexceeds the scalability of SFO methods. Moreover, it significantly enhances\nboth speed and accuracy of the optimization process.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}