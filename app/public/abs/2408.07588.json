{"id":"2408.07588","title":"\"How Big is Big Enough?\" Adjusting Model Size in Continual Gaussian\n  Processes","authors":"Guiomar Pescador-Barrios, Sarah Filippi, Mark van der Wilk","authorsParsed":[["Pescador-Barrios","Guiomar",""],["Filippi","Sarah",""],["van der Wilk","Mark",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 14:40:00 GMT"}],"updateDate":"2024-08-15","timestamp":1723646400000,"abstract":"  For many machine learning methods, creating a model requires setting a\nparameter that controls the model's capacity before training, e.g.~number of\nneurons in DNNs, or inducing points in GPs. Increasing capacity improves\nperformance until all the information from the dataset is captured. After this\npoint, computational cost keeps increasing, without improved performance. This\nleads to the question ``How big is big enough?'' We investigate this problem\nfor Gaussian processes (single-layer neural networks) in continual learning.\nHere, data becomes available incrementally, and the final dataset size will\ntherefore not be known before training, preventing the use of heuristics for\nsetting the model size. We provide a method that automatically adjusts this,\nwhile maintaining near-optimal performance, and show that a single\nhyperparameter setting for our method performs well across datasets with a wide\nrange of properties.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}