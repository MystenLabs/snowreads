{"id":"2408.00280","title":"Towards Scalable GPU-Accelerated SNN Training via Temporal Fusion","authors":"Yanchen Li, Jiachun Li, Kebin Sun, Luziwei Leng, Ran Cheng","authorsParsed":[["Li","Yanchen",""],["Li","Jiachun",""],["Sun","Kebin",""],["Leng","Luziwei",""],["Cheng","Ran",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 04:41:56 GMT"}],"updateDate":"2024-08-02","timestamp":1722487316000,"abstract":"  Drawing on the intricate structures of the brain, Spiking Neural Networks\n(SNNs) emerge as a transformative development in artificial intelligence,\nclosely emulating the complex dynamics of biological neural networks. While\nSNNs show promising efficiency on specialized sparse-computational hardware,\ntheir practical training often relies on conventional GPUs. This reliance\nfrequently leads to extended computation times when contrasted with traditional\nArtificial Neural Networks (ANNs), presenting significant hurdles for advancing\nSNN research. To navigate this challenge, we present a novel temporal fusion\nmethod, specifically designed to expedite the propagation dynamics of SNNs on\nGPU platforms, which serves as an enhancement to the current significant\napproaches for handling deep learning tasks with SNNs. This method underwent\nthorough validation through extensive experiments in both authentic training\nscenarios and idealized conditions, confirming its efficacy and adaptability\nfor single and multi-GPU systems. Benchmarked against various existing SNN\nlibraries/implementations, our method achieved accelerations ranging from\n$5\\times$ to $40\\times$ on NVIDIA A100 GPUs. Publicly available experimental\ncodes can be found at https://github.com/EMI-Group/snn-temporal-fusion.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}