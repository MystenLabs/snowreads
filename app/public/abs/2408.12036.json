{"id":"2408.12036","title":"Reasoning and Tools for Human-Level Forecasting","authors":"Elvis Hsieh, Preston Fu, Jonathan Chen","authorsParsed":[["Hsieh","Elvis",""],["Fu","Preston",""],["Chen","Jonathan",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 23:42:06 GMT"}],"updateDate":"2024-08-23","timestamp":1724283726000,"abstract":"  Language models (LMs) trained on web-scale datasets are largely successful\ndue to their ability to memorize large amounts of training data, even if only\npresent in a few examples. These capabilities are often desirable in evaluation\non tasks such as question answering but raise questions about whether these\nmodels can exhibit genuine reasoning or succeed only at mimicking patterns from\nthe training data. This distinction is particularly salient in forecasting\ntasks, where the answer is not present in the training data, and the model must\nreason to make logical deductions. We present Reasoning and Tools for\nForecasting (RTF), a framework of reasoning-and-acting (ReAct) agents that can\ndynamically retrieve updated information and run numerical simulation with\nequipped tools. We evaluate our model with questions from competitive\nforecasting platforms and demonstrate that our method is competitive with and\ncan outperform human predictions. This suggests that LMs, with the right tools,\ncan indeed think and adapt like humans, offering valuable insights for\nreal-world decision-making.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eDbyiSHlbFUvTxptMR1swSMPgIxwvYEOdSzDWo1j1SY","pdfSize":"724195"}
