{"id":"2407.09527","title":"BitNet b1.58 Reloaded: State-of-the-art Performance Also on Smaller\n  Networks","authors":"Jacob Nielsen and Peter Schneider-Kamp","authorsParsed":[["Nielsen","Jacob",""],["Schneider-Kamp","Peter",""]],"versions":[{"version":"v1","created":"Mon, 24 Jun 2024 20:55:36 GMT"}],"updateDate":"2024-07-16","timestamp":1719262536000,"abstract":"  Recently proposed methods for 1-bit and 1.58-bit quantization aware training\ninvestigate the performance and behavior of these methods in the context of\nlarge language models, finding state-of-the-art performance for models with\nmore than 3B parameters. In this work, we investigate 1.58-bit quantization for\nsmall language and vision models ranging from 100K to 48M parameters. We\nintroduce a variant of BitNet b1.58, which allows to rely on the median rather\nthan the mean in the quantization process.\n  Through extensive experiments we investigate the performance of 1.58-bit\nmodels obtained through quantization aware training. We further investigate the\nrobustness of 1.58-bit quantization-aware training to changes in the learning\nrate and regularization through weight decay, finding different patterns for\nsmall language and vision models than previously reported for large language\nmodels.\n  Our results showcase that 1.58-bit quantization-aware training provides\nstate-of-the-art performance for small language models when doubling hidden\nlayer sizes and reaches or even surpasses state-of-the-art performance for\nsmall vision models of identical size. Ultimately, we demonstrate that 1.58-bit\nquantization-aware training is a viable and promising approach also for\ntraining smaller deep learning networks, facilitating deployment of such models\nin low-resource use-cases and encouraging future research.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}