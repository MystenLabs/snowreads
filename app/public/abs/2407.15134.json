{"id":"2407.15134","title":"Proximal Policy Distillation","authors":"Giacomo Spigler","authorsParsed":[["Spigler","Giacomo",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 12:08:54 GMT"}],"updateDate":"2024-07-23","timestamp":1721563734000,"abstract":"  We introduce Proximal Policy Distillation (PPD), a novel policy distillation\nmethod that integrates student-driven distillation and Proximal Policy\nOptimization (PPO) to increase sample efficiency and to leverage the additional\nrewards that the student policy collects during distillation. To assess the\nefficacy of our method, we compare PPD with two common alternatives,\nstudent-distill and teacher-distill, over a wide range of reinforcement\nlearning environments that include discrete actions and continuous control\n(ATARI, Mujoco, and Procgen). For each environment and method, we perform\ndistillation to a set of target student neural networks that are smaller,\nidentical (self-distillation), or larger than the teacher network. Our findings\nindicate that PPD improves sample efficiency and produces better student\npolicies compared to typical policy distillation approaches. Moreover, PPD\ndemonstrates greater robustness than alternative methods when distilling\npolicies from imperfect demonstrations. The code for the paper is released as\npart of a new Python library built on top of stable-baselines3 to facilitate\npolicy distillation: `sb3-distill'.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}