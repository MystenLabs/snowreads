{"id":"2408.04116","title":"Combining Neural Architecture Search and Automatic Code Optimization: A\n  Survey","authors":"Inas Bachiri, Hadjer Benmeziane, Smail Niar, Riyadh Baghdadi, Hamza\n  Ouarnoughi, Abdelkrime Aries","authorsParsed":[["Bachiri","Inas",""],["Benmeziane","Hadjer",""],["Niar","Smail",""],["Baghdadi","Riyadh",""],["Ouarnoughi","Hamza",""],["Aries","Abdelkrime",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 22:40:05 GMT"}],"updateDate":"2024-08-09","timestamp":1723070405000,"abstract":"  Deep Learning models have experienced exponential growth in complexity and\nresource demands in recent years. Accelerating these models for efficient\nexecution on resource-constrained devices has become more crucial than ever.\nTwo notable techniques employed to achieve this goal are Hardware-aware Neural\nArchitecture Search (HW-NAS) and Automatic Code Optimization (ACO). HW-NAS\nautomatically designs accurate yet hardware-friendly neural networks, while ACO\ninvolves searching for the best compiler optimizations to apply on neural\nnetworks for efficient mapping and inference on the target hardware. This\nsurvey explores recent works that combine these two techniques within a single\nframework. We present the fundamental principles of both domains and\ndemonstrate their sub-optimality when performed independently. We then\ninvestigate their integration into a joint optimization process that we call\nHardware Aware-Neural Architecture and Compiler Optimizations co-Search\n(NACOS).\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Programming Languages"],"license":"http://creativecommons.org/licenses/by/4.0/"}