{"id":"2408.03733","title":"Bayes-optimal learning of an extensive-width neural network from\n  quadratically many samples","authors":"Antoine Maillard, Emanuele Troiani, Simon Martin, Florent Krzakala,\n  Lenka Zdeborov\\'a","authorsParsed":[["Maillard","Antoine",""],["Troiani","Emanuele",""],["Martin","Simon",""],["Krzakala","Florent",""],["Zdeborov√°","Lenka",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 12:41:56 GMT"}],"updateDate":"2024-08-08","timestamp":1723034516000,"abstract":"  We consider the problem of learning a target function corresponding to a\nsingle hidden layer neural network, with a quadratic activation function after\nthe first layer, and random weights. We consider the asymptotic limit where the\ninput dimension and the network width are proportionally large. Recent work\n[Cui & al '23] established that linear regression provides Bayes-optimal test\nerror to learn such a function when the number of available samples is only\nlinear in the dimension. That work stressed the open challenge of theoretically\nanalyzing the optimal test error in the more interesting regime where the\nnumber of samples is quadratic in the dimension. In this paper, we solve this\nchallenge for quadratic activations and derive a closed-form expression for the\nBayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,\nwhich combines approximate message passing with rotationally invariant matrix\ndenoising, and that asymptotically achieves the optimal performance.\nTechnically, our result is enabled by establishing a link with recent works on\noptimal denoising of extensive-rank matrices and on the ellipsoid fitting\nproblem. We further show empirically that, in the absence of noise,\nrandomly-initialized gradient descent seems to sample the space of weights,\nleading to zero training loss, and averaging over initialization leads to a\ntest error equal to the Bayes-optimal one.\n","subjects":["Statistics/Machine Learning","Condensed Matter/Disordered Systems and Neural Networks","Computing Research Repository/Information Theory","Computing Research Repository/Machine Learning","Mathematics/Information Theory","Mathematics/Probability"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}