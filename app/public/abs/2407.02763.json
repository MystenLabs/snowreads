{"id":"2407.02763","title":"ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization\n  for Vision Transformers","authors":"Yanfeng Jiang, Ning Sun, Xueshuo Xie, Fei Yang, Tao Li","authorsParsed":[["Jiang","Yanfeng",""],["Sun","Ning",""],["Xie","Xueshuo",""],["Yang","Fei",""],["Li","Tao",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 02:41:59 GMT"}],"updateDate":"2024-07-04","timestamp":1719974519000,"abstract":"  Vision Transformers (ViTs) have exhibited exceptional performance across\ndiverse computer vision tasks, while their substantial parameter size incurs\nsignificantly increased memory and computational demands, impeding effective\ninference on resource-constrained devices. Quantization has emerged as a\npromising solution to mitigate these challenges, yet existing methods still\nsuffer from significant accuracy loss at low-bit. We attribute this issue to\nthe distinctive distributions of post-LayerNorm and post-GELU activations\nwithin ViTs, rendering conventional hardware-friendly quantizers ineffective,\nparticularly in low-bit scenarios. To address this issue, we propose a novel\nframework called Activation-Distribution-Friendly post-training Quantization\nfor Vision Transformers, ADFQ-ViT. Concretely, we introduce the Per-Patch\nOutlier-aware Quantizer to tackle irregular outliers in post-LayerNorm\nactivations. This quantizer refines the granularity of the uniform quantizer to\na per-patch level while retaining a minimal subset of values exceeding a\nthreshold at full-precision. To handle the non-uniform distributions of\npost-GELU activations between positive and negative regions, we design the\nShift-Log2 Quantizer, which shifts all elements to the positive region and then\napplies log2 quantization. Moreover, we present the Attention-score enhanced\nModule-wise Optimization which adjusts the parameters of each quantizer by\nreconstructing errors to further mitigate quantization error. Extensive\nexperiments demonstrate ADFQ-ViT provides significant improvements over various\nbaselines in image classification, object detection, and instance segmentation\ntasks at 4-bit. Specifically, when quantizing the ViT-B model to 4-bit, we\nachieve a 10.23% improvement in Top-1 accuracy on the ImageNet dataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}