{"id":"2408.10193","title":"Area under the ROC Curve has the Most Consistent Evaluation for Binary\n  Classification","authors":"Jing Li","authorsParsed":[["Li","Jing",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 17:52:38 GMT"}],"updateDate":"2024-08-20","timestamp":1724089958000,"abstract":"  Evaluation Metrics is an important question for model evaluation and model\nselection in binary classification tasks. This study investigates how\nconsistent metrics are at evaluating different models under different data\nscenarios. Analyzing over 150 data scenarios and 18 model evaluation metrics\nusing statistical simulation, I find that for binary classification tasks,\nevaluation metrics that are less influenced by prevalence offer more consistent\nranking of a set of different models. In particular, Area Under the ROC Curve\n(AUC) has smallest variance in ranking of different models. Matthew's\ncorrelation coefficient as a more strict measure of model performance has the\nsecond smallest variance. These patterns holds across a rich set of data\nscenarios and five commonly used machine learning models as well as a naive\nrandom guess model. The results have significant implications for model\nevaluation and model selection in binary classification tasks.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}