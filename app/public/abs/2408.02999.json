{"id":"2408.02999","title":"LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning","authors":"Lekai Chen, Ashutosh Trivedi, Alvaro Velasquez","authorsParsed":[["Chen","Lekai",""],["Trivedi","Ashutosh",""],["Velasquez","Alvaro",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 07:12:09 GMT"}],"updateDate":"2024-08-07","timestamp":1722928329000,"abstract":"  The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.\n","subjects":["Computing Research Repository/Formal Languages and Automata Theory","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"rjXB1oDk0-_n4wscl6Iak4KqLORYLRLvXxlHy3mX34k","pdfSize":"897070"}
