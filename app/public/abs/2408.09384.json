{"id":"2408.09384","title":"FD2Talk: Towards Generalized Talking Head Generation with Facial\n  Decoupled Diffusion Model","authors":"Ziyu Yao, Xuxin Cheng, Zhiqi Huang","authorsParsed":[["Yao","Ziyu",""],["Cheng","Xuxin",""],["Huang","Zhiqi",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 07:03:53 GMT"}],"updateDate":"2024-08-20","timestamp":1723964633000,"abstract":"  Talking head generation is a significant research topic that still faces\nnumerous challenges. Previous works often adopt generative adversarial networks\nor regression models, which are plagued by generation quality and average\nfacial shape problem. Although diffusion models show impressive generative\nability, their exploration in talking head generation remains unsatisfactory.\nThis is because they either solely use the diffusion model to obtain an\nintermediate representation and then employ another pre-trained renderer, or\nthey overlook the feature decoupling of complex facial details, such as\nexpressions, head poses and appearance textures. Therefore, we propose a Facial\nDecoupled Diffusion model for Talking head generation called FD2Talk, which\nfully leverages the advantages of diffusion models and decouples the complex\nfacial details through multi-stages. Specifically, we separate facial details\ninto motion and appearance. In the initial phase, we design the Diffusion\nTransformer to accurately predict motion coefficients from raw audio. These\nmotions are highly decoupled from appearance, making them easier for the\nnetwork to learn compared to high-dimensional RGB images. Subsequently, in the\nsecond phase, we encode the reference image to capture appearance textures. The\npredicted facial and head motions and encoded appearance then serve as the\nconditions for the Diffusion UNet, guiding the frame generation. Benefiting\nfrom decoupling facial details and fully leveraging diffusion models, extensive\nexperiments substantiate that our approach excels in enhancing image quality\nand generating more accurate and diverse results compared to previous\nstate-of-the-art methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}