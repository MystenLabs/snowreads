{"id":"2407.05784","title":"Hecaton: Training and Finetuning Large Language Models with Scalable\n  Chiplet Systems","authors":"Zongle Huang, Shupei Fan, Chen Tang, Xinyuan Lin, Shuwen Deng, Yongpan\n  Liu","authorsParsed":[["Huang","Zongle",""],["Fan","Shupei",""],["Tang","Chen",""],["Lin","Xinyuan",""],["Deng","Shuwen",""],["Liu","Yongpan",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 09:47:35 GMT"}],"updateDate":"2024-07-09","timestamp":1720432055000,"abstract":"  Large Language Models (LLMs) have achieved remarkable success in various\nfields, but their training and finetuning require massive computation and\nmemory, necessitating parallelism which introduces heavy communication\noverheads. Driven by advances in packaging, the chiplet architecture emerges as\na potential solution, as it can integrate computing power, as well as utilize\non-package links with better signal integrity, higher bandwidth, and lower\nenergy consumption. However, most existing chiplet-related works focus on DNN\ninference. Directly porting them to LLM training introduces significantly large\nquantities of DRAM access and network-on-package (NoP) overheads which make\nstate-of-the-art chiplet designs fail, highlighting a research gap.\n  This work proposes Hecaton, a scalable and cost-effective chiplet system for\nLLM training and finetuning. We first provide a chiplet architecture with\ntailored scheduling that can largely reduce DRAM accesses. We further design an\nefficient distributed training method that reduces NoP communication complexity\nand relieves constraints on SRAM capacity and layout. Theoretical analysis\nshows that the entire system achieves weak scaling: as the workload and\nhardware resources grow proportionally, the computation-to-communication ratio\nremains nearly constant. Experiments with various workloads and hardware\nconfigurations verify the property, and Hecaton achieves $4.98\\times$\nperformance improvement and $2.35\\times$ energy reduction on Llama2-70B,\ncompared to the tensor parallelism in Megatron. To the best of our knowledge,\nwe propose the first chiplet architecture specifically used for LLM training or\nfinetuning, with guaranteed performance regardless of the problem scale.\n","subjects":["Computing Research Repository/Hardware Architecture"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}