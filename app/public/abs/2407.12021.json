{"id":"2407.12021","title":"Adaptive Draft-Verification for Efficient Large Language Model Decoding","authors":"Xukun Liu, Bowen Lei, Ruqi Zhang, Dongkuan Xu","authorsParsed":[["Liu","Xukun",""],["Lei","Bowen",""],["Zhang","Ruqi",""],["Xu","Dongkuan",""]],"versions":[{"version":"v1","created":"Thu, 27 Jun 2024 22:20:39 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 15:28:37 GMT"}],"updateDate":"2024-08-20","timestamp":1719526839000,"abstract":"  Large language model (LLM) decoding involves generating a sequence of tokens\nbased on a given context, where each token is predicted one at a time using the\nmodel's learned probabilities. The typical autoregressive decoding method\nrequires a separate forward pass through the model for each token generated,\nwhich is computationally inefficient and poses challenges for deploying LLMs in\nlatency-sensitive scenarios. The main limitations of current decoding methods\nstem from their inefficiencies and resource demands. Existing approaches either\nnecessitate fine-tuning smaller models, which is resource-intensive, or rely on\nfixed retrieval schemes to construct drafts for the next tokens, which lack\nadaptability and fail to generalize across different models and contexts. To\naddress these issues, we introduce a novel methodology called ADED, which\naccelerates LLM decoding without requiring fine-tuning. Our approach involves\nan adaptive draft-verification process that evolves over time to improve\nefficiency. We utilize a tri-gram matrix-based LLM representation to\ndynamically approximate the output distribution of the LLM, allowing the model\nto adjust to changing token probabilities during the decoding process.\nAdditionally, we implement a draft construction mechanism that effectively\nbalances exploration and exploitation, ensuring that the drafts generated are\nboth diverse and close to the true output distribution of the LLM. The\nimportance of this design lies in its ability to optimize the draft\ndistribution adaptively, leading to faster and more accurate decoding. Through\nextensive experiments on various benchmark datasets and LLM architectures, we\ndemonstrate that ADED significantly accelerates the decoding process while\nmaintaining high accuracy, making it suitable for deployment in a wide range of\npractical applications.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}