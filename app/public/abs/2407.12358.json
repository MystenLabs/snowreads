{"id":"2407.12358","title":"ProcTag: Process Tagging for Assessing the Efficacy of Document\n  Instruction Data","authors":"Yufan Shen, Chuwei Luo, Zhaoqing Zhu, Yang Chen, Qi Zheng, Zhi Yu,\n  Jiajun Bu, Cong Yao","authorsParsed":[["Shen","Yufan",""],["Luo","Chuwei",""],["Zhu","Zhaoqing",""],["Chen","Yang",""],["Zheng","Qi",""],["Yu","Zhi",""],["Bu","Jiajun",""],["Yao","Cong",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 07:29:59 GMT"}],"updateDate":"2024-07-18","timestamp":1721201399000,"abstract":"  Recently, large language models (LLMs) and multimodal large language models\n(MLLMs) have demonstrated promising results on document visual question\nanswering (VQA) task, particularly after training on document instruction\ndatasets. An effective evaluation method for document instruction data is\ncrucial in constructing instruction data with high efficacy, which, in turn,\nfacilitates the training of LLMs and MLLMs for document VQA. However, most\nexisting evaluation methods for instruction data are limited to the textual\ncontent of the instructions themselves, thereby hindering the effective\nassessment of document instruction datasets and constraining their\nconstruction. In this paper, we propose ProcTag, a data-oriented method that\nassesses the efficacy of document instruction data. ProcTag innovatively\nperforms tagging on the execution process of instructions rather than the\ninstruction text itself. By leveraging the diversity and complexity of these\ntags to assess the efficacy of the given dataset, ProcTag enables selective\nsampling or filtering of document instructions. Furthermore, DocLayPrompt, a\nnovel semi-structured layout-aware document prompting strategy, is proposed for\neffectively representing documents. Experiments demonstrate that sampling\nexisting open-sourced and generated document VQA/instruction datasets with\nProcTag significantly outperforms current methods for evaluating instruction\ndata. Impressively, with ProcTag-based sampling in the generated document\ndatasets, only 30.5\\% of the document instructions are required to achieve\n100\\% efficacy compared to the complete dataset. The code is publicly available\nat\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/ProcTag.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}