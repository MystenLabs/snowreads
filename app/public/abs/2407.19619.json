{"id":"2407.19619","title":"Enhancing Code Translation in Language Models with Few-Shot Learning via\n  Retrieval-Augmented Generation","authors":"Manish Bhattarai, Javier E. Santos, Shawn Jones, Ayan Biswas, Boian\n  Alexandrov, Daniel O'Malley","authorsParsed":[["Bhattarai","Manish",""],["Santos","Javier E.",""],["Jones","Shawn",""],["Biswas","Ayan",""],["Alexandrov","Boian",""],["O'Malley","Daniel",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 00:41:48 GMT"}],"updateDate":"2024-07-30","timestamp":1722213708000,"abstract":"  The advent of large language models (LLMs) has significantly advanced the\nfield of code translation, enabling automated translation between programming\nlanguages. However, these models often struggle with complex translation tasks\ndue to inadequate contextual understanding. This paper introduces a novel\napproach that enhances code translation through Few-Shot Learning, augmented\nwith retrieval-based techniques. By leveraging a repository of existing code\ntranslations, we dynamically retrieve the most relevant examples to guide the\nmodel in translating new code segments. Our method, based on\nRetrieval-Augmented Generation (RAG), substantially improves translation\nquality by providing contextual examples from which the model can learn in\nreal-time. We selected RAG over traditional fine-tuning methods due to its\nability to utilize existing codebases or a locally stored corpus of code, which\nallows for dynamic adaptation to diverse translation tasks without extensive\nretraining. Extensive experiments on diverse datasets with open LLM models such\nas Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code\nInstruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5\nTurbo and GPT-4o, demonstrate our approach's superiority over traditional\nzero-shot methods, especially in translating between Fortran and CPP. We also\nexplored varying numbers of shots i.e. examples provided during inference,\nspecifically 1, 2, and 3 shots and different embedding models for RAG,\nincluding Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and\neffectiveness of our approach.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}