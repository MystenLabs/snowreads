{"id":"2407.13781","title":"RDBE: Reasoning Distillation-Based Evaluation Enhances Automatic Essay\n  Scoring","authors":"Ali Ghiasvand Mohammadkhani","authorsParsed":[["Mohammadkhani","Ali Ghiasvand",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 05:49:01 GMT"}],"updateDate":"2024-07-22","timestamp":1719985741000,"abstract":"  Recently, various encoder-only and encoder-decoder pre-trained models like\nBERT and T5 have been applied to automatic essay scoring (AES) as small\nlanguage models. However, existing studies have primarily treated this task\nakin to a classification problem, focusing solely on outputting scores in the\ntarget text without offering interpretations for the generated scores.\nDeparting from the approaches, we introduce Reasoning Distillation-Based\nEvaluation (RDBE), which integrates interpretability to elucidate the rationale\nbehind model scores while enhancing performance through initial reasoning. This\ninterpretive capability is acquired during training by leveraging generated\nreasoning from a large language model (LLM) to distill a small language model\n(SLM). Our experimental results demonstrate the efficacy of RDBE across all\nscoring rubrics considered in the dataset. RDBE outperforms both zero-shot LLM\ngeneration and generation from a baseline fine-tuned model, establishing itself\nas state-of-the-art in the corresponding dataset. This highlights its practical\ninterpretative output and enhanced performance.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}