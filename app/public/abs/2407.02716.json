{"id":"2407.02716","title":"Light-weight Fine-tuning Method for Defending Adversarial Noise in\n  Pre-trained Medical Vision-Language Models","authors":"Xu Han, Linghao Jin, Xuezhe Ma, Xiaofeng Liu","authorsParsed":[["Han","Xu",""],["Jin","Linghao",""],["Ma","Xuezhe",""],["Liu","Xiaofeng",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 23:48:43 GMT"}],"updateDate":"2024-07-04","timestamp":1719964123000,"abstract":"  Fine-tuning pre-trained Vision-Language Models (VLMs) has shown remarkable\ncapabilities in medical image and textual depiction synergy. Nevertheless, many\npre-training datasets are restricted by patient privacy concerns, potentially\ncontaining noise that can adversely affect downstream performance. Moreover,\nthe growing reliance on multi-modal generation exacerbates this issue because\nof its susceptibility to adversarial attacks. To investigate how VLMs trained\non adversarial noisy data perform on downstream medical tasks, we first craft\nnoisy upstream datasets using multi-modal adversarial attacks. Through our\ncomprehensive analysis, we unveil that moderate noise enhances model robustness\nand transferability, but increasing noise levels negatively impact downstream\ntask performance. To mitigate this issue, we propose rectify adversarial noise\n(RAN) framework, a recipe designed to effectively defend adversarial attacks\nand rectify the influence of upstream noise during fine-tuning.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}