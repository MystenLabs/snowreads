{"id":"2408.08590","title":"A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive\n  Language Models","authors":"Geonhee Kim, Marco Valentino, Andr\\'e Freitas","authorsParsed":[["Kim","Geonhee",""],["Valentino","Marco",""],["Freitas","Andr√©",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 07:47:39 GMT"}],"updateDate":"2024-08-19","timestamp":1723794459000,"abstract":"  Recent studies on logical reasoning in auto-regressive Language Models (LMs)\nhave sparked a debate on whether such models can learn systematic reasoning\nprinciples during pre-training or merely exploit superficial patterns in the\ntraining data. This paper presents a mechanistic interpretation of syllogistic\nreasoning in LMs to further enhance our understanding of internal dynamics.\nSpecifically, we present a methodology for circuit discovery aimed at\ndisentangling content-independent reasoning mechanisms from world knowledge\nacquired during pre-training. Through two distinct intervention methods, we\nuncover a sufficient and necessary circuit involving middle-term suppression\nthat elucidates how LMs transfer information to derive valid conclusions from\npremises. Furthermore, we investigate how belief biases manifest in syllogistic\nreasoning, finding evidence of partial contamination from additional attention\nheads responsible for encoding commonsense and contextualized knowledge.\nFinally, we explore the generalization of the discovered mechanisms across\nvarious syllogistic schemes and model sizes, finding that the identified\ncircuit is sufficient and necessary for all the schemes on which the model\nachieves high downstream accuracy ($\\geq$ 60\\%). Overall, our findings suggest\nthat LMs indeed learn transferable content-independent reasoning mechanisms,\nbut that, at the same time, such mechanisms do not involve generalisable and\nabstract logical primitives, being susceptible to contamination by the same\nworld knowledge acquired during pre-training.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}