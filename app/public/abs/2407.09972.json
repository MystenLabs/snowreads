{"id":"2407.09972","title":"Harvesting Private Medical Images in Federated Learning Systems with\n  Crafted Models","authors":"Shanghao Shi, Md Shahedul Haque, Abhijeet Parida, Marius George\n  Linguraru, Y.Thomas Hou, Syed Muhammad Anwar, and Wenjing Lou","authorsParsed":[["Shi","Shanghao",""],["Haque","Md Shahedul",""],["Parida","Abhijeet",""],["Linguraru","Marius George",""],["Hou","Y. Thomas",""],["Anwar","Syed Muhammad",""],["Lou","Wenjing",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 18:31:35 GMT"}],"updateDate":"2024-07-16","timestamp":1720895495000,"abstract":"  Federated learning (FL) allows a set of clients to collaboratively train a\nmachine-learning model without exposing local training samples. In this\ncontext, it is considered to be privacy-preserving and hence has been adopted\nby medical centers to train machine-learning models over private data. However,\nin this paper, we propose a novel attack named MediLeak that enables a\nmalicious parameter server to recover high-fidelity patient images from the\nmodel updates uploaded by the clients. MediLeak requires the server to generate\nan adversarial model by adding a crafted module in front of the original model\narchitecture. It is published to the clients in the regular FL training process\nand each client conducts local training on it to generate corresponding model\nupdates. Then, based on the FL protocol, the model updates are sent back to the\nserver and our proposed analytical method recovers private data from the\nparameter updates of the crafted module. We provide a comprehensive analysis\nfor MediLeak and show that it can successfully break the state-of-the-art\ncryptographic secure aggregation protocols, designed to protect the FL systems\nfrom privacy inference attacks. We implement MediLeak on the MedMNIST and\nCOVIDx CXR-4 datasets. The results show that MediLeak can nearly perfectly\nrecover private images with high recovery rates and quantitative scores. We\nfurther perform downstream tasks such as disease classification with the\nrecovered data, where our results show no significant performance degradation\ncompared to using the original training samples.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}