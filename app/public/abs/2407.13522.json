{"id":"2407.13522","title":"INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question\n  Answering capability of LLMs for Indic Languages","authors":"Abhishek Kumar Singh, Rudra Murthy, Vishwajeet kumar, Jaydeep Sen,\n  Ganesh Ramakrishnan","authorsParsed":[["Singh","Abhishek Kumar",""],["Murthy","Rudra",""],["kumar","Vishwajeet",""],["Sen","Jaydeep",""],["Ramakrishnan","Ganesh",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 13:57:16 GMT"}],"updateDate":"2024-07-19","timestamp":1721311036000,"abstract":"  Large Language Models (LLMs) have demonstrated remarkable zero-shot and\nfew-shot capabilities in unseen tasks, including context-grounded question\nanswering (QA) in English. However, the evaluation of LLMs' capabilities in\nnon-English languages for context-based QA is limited by the scarcity of\nbenchmarks in non-English languages. To address this gap, we introduce\nIndic-QA, the largest publicly available context-grounded question-answering\ndataset for 11 major Indian languages from two language families. The dataset\ncomprises both extractive and abstractive question-answering tasks and includes\nexisting datasets as well as English QA datasets translated into Indian\nlanguages. Additionally, we generate a synthetic dataset using the Gemini model\nto create question-answer pairs given a passage, which is then manually\nverified for quality assurance. We evaluate various multilingual Large Language\nModels and their instruction-fine-tuned variants on the benchmark and observe\nthat their performance is subpar, particularly for low-resource languages. We\nhope that the release of this dataset will stimulate further research on the\nquestion-answering abilities of LLMs for low-resource languages.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"B7U_PIOW9y-fF6SyhlCnehS0rCXRUUzCyhYIa36odF0","pdfSize":"2618115"}
