{"id":"2408.04522","title":"Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large\n  Language Models","authors":"Fabio Pernisi, Dirk Hovy, Paul R\\\"ottger","authorsParsed":[["Pernisi","Fabio",""],["Hovy","Dirk",""],["RÃ¶ttger","Paul",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 15:24:03 GMT"}],"updateDate":"2024-08-09","timestamp":1723130643000,"abstract":"  As diverse linguistic communities and users adopt large language models\n(LLMs), assessing their safety across languages becomes critical. Despite\nongoing efforts to make LLMs safe, they can still be made to behave unsafely\nwith jailbreaking, a technique in which models are prompted to act outside\ntheir operational guidelines. Research on LLM safety and jailbreaking, however,\nhas so far mostly focused on English, limiting our understanding of LLM safety\nin other languages. We contribute towards closing this gap by investigating the\neffectiveness of many-shot jailbreaking, where models are prompted with unsafe\ndemonstrations to induce unsafe behaviour, in Italian. To enable our analysis,\nwe create a new dataset of unsafe Italian question-answer pairs. With this\ndataset, we identify clear safety vulnerabilities in four families of\nopen-weight LLMs. We find that the models exhibit unsafe behaviors even when\nprompted with few unsafe demonstrations, and -- more alarmingly -- that this\ntendency rapidly escalates with more demonstrations.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}