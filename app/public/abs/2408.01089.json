{"id":"2408.01089","title":"Prototypical Partial Optimal Transport for Universal Domain Adaptation","authors":"Yucheng Yang, Xiang Gu, Jian Sun","authorsParsed":[["Yang","Yucheng",""],["Gu","Xiang",""],["Sun","Jian",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 08:08:56 GMT"}],"updateDate":"2024-08-05","timestamp":1722586136000,"abstract":"  Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled\nsource domain to an unlabeled target domain without requiring the same label\nsets of both domains. The existence of domain and category shift makes the task\nchallenging and requires us to distinguish \"known\" samples (i.e., samples whose\nlabels exist in both domains) and \"unknown\" samples (i.e., samples whose labels\nexist in only one domain) in both domains before reducing the domain gap. In\nthis paper, we consider the problem from the point of view of distribution\nmatching which we only need to align two distributions partially. A novel\napproach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is\nproposed to conduct partial distribution alignment for UniDA. In training\nphase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT\nto reweight source prototypes and target samples, and design reweighted entropy\nloss and reweighted cross-entropy loss to distinguish \"known\" and \"unknown\"\nsamples. Experiments on four benchmarks show that our method outperforms the\nprevious state-of-the-art UniDA methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}