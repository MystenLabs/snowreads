{"id":"2408.11795","title":"EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large\n  Language Model","authors":"Feipeng Ma, Yizhou Zhou, Hebei Li, Zilong He, Siying Wu, Fengyun Rao,\n  Yueyi Zhang, Xiaoyan Sun","authorsParsed":[["Ma","Feipeng",""],["Zhou","Yizhou",""],["Li","Hebei",""],["He","Zilong",""],["Wu","Siying",""],["Rao","Fengyun",""],["Zhang","Yueyi",""],["Sun","Xiaoyan",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:36:37 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 18:57:01 GMT"}],"updateDate":"2024-09-11","timestamp":1724261797000,"abstract":"  In the realm of multimodal research, numerous studies leverage substantial\nimage-text pairs to conduct modal alignment learning, transforming Large\nLanguage Models (LLMs) into Multimodal LLMs and excelling in a variety of\nvisual-language tasks. The prevailing methodologies primarily fall into two\ncategories: self-attention-based and cross-attention-based methods. While\nself-attention-based methods offer superior data efficiency due to their simple\nMLP architecture, they often suffer from lower computational efficiency due to\nconcatenating visual and textual tokens as input for LLM. Conversely,\ncross-attention-based methods, although less data-efficient due to additional\nlearnable parameters, exhibit higher computational efficiency by avoiding long\nsequence input for LLM. To address these trade-offs, we introduce the\nData-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).\nWithout introducing additional modules or learnable parameters, EE-MLLM\nachieves both data and compute efficiency. Specifically, we modify the original\nself-attention mechanism in MLLM to a composite attention mechanism. This\nmechanism has two key characteristics: 1) Eliminating the computational\noverhead of self-attention within visual tokens to achieve compute efficiency,\nand 2) Reusing the weights on each layer of LLM to facilitate effective\nmodality alignment between vision and language for data efficiency.\nExperimental results demonstrate the effectiveness of EE-MLLM across a range of\nbenchmarks, including general-purpose datasets like MMBench and SeedBench, as\nwell as fine-grained tasks such as TextVQA and DocVQA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}