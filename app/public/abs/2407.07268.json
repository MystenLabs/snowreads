{"id":"2407.07268","title":"Dataset Quantization with Active Learning based Adaptive Sampling","authors":"Zhenghao Zhao, Yuzhang Shang, Junyi Wu, Yan Yan","authorsParsed":[["Zhao","Zhenghao",""],["Shang","Yuzhang",""],["Wu","Junyi",""],["Yan","Yan",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 23:09:18 GMT"}],"updateDate":"2024-07-11","timestamp":1720566558000,"abstract":"  Deep learning has made remarkable progress recently, largely due to the\navailability of large, well-labeled datasets. However, the training on such\ndatasets elevates costs and computational demands. To address this, various\ntechniques like coreset selection, dataset distillation, and dataset\nquantization have been explored in the literature. Unlike traditional\ntechniques that depend on uniform sample distributions across different\nclasses, our research demonstrates that maintaining performance is feasible\neven with uneven distributions. We find that for certain classes, the variation\nin sample quantity has a minimal impact on performance. Inspired by this\nobservation, an intuitive idea is to reduce the number of samples for stable\nclasses and increase the number of samples for sensitive classes to achieve a\nbetter performance with the same sampling ratio. Then the question arises: how\ncan we adaptively select samples from a dataset to achieve optimal performance?\nIn this paper, we propose a novel active learning based adaptive sampling\nstrategy, Dataset Quantization with Active Learning based Adaptive Sampling\n(DQAS), to optimize the sample selection. In addition, we introduce a novel\npipeline for dataset quantization, utilizing feature space from the final stage\nof dataset quantization to generate more precise dataset bins. Our\ncomprehensive evaluations on the multiple datasets show that our approach\noutperforms the state-of-the-art dataset compression methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}