{"id":"2408.04275","title":"DistTrain: Addressing Model and Data Heterogeneity with Disaggregated\n  Training for Multimodal Large Language Models","authors":"Zili Zhang, Yinmin Zhong, Ranchen Ming, Hanpeng Hu, Jianjian Sun,\n  Zheng Ge, Yibo Zhu, Xin Jin","authorsParsed":[["Zhang","Zili",""],["Zhong","Yinmin",""],["Ming","Ranchen",""],["Hu","Hanpeng",""],["Sun","Jianjian",""],["Ge","Zheng",""],["Zhu","Yibo",""],["Jin","Xin",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 07:20:42 GMT"},{"version":"v2","created":"Thu, 15 Aug 2024 15:20:53 GMT"}],"updateDate":"2024-08-16","timestamp":1723101642000,"abstract":"  Multimodal large language models (LLMs) have demonstrated significant\npotential in a wide range of AI applications. Yet, training multimodal LLMs\nsuffers from low efficiency and scalability, due to the inherent model\nheterogeneity and data heterogeneity across different modalities.\n  We present DistTrain, an efficient and adaptive framework to reform the\ntraining of multimodal large language models on large-scale clusters. The core\nof DistTrain is the disaggregated training technique that exploits the\ncharacteristics of multimodal LLM training to achieve high efficiency and\nscalability. Specifically, it leverages disaggregated model orchestration and\ndisaggregated data reordering to address model and data heterogeneity\nrespectively. We also tailor system optimization for multimodal LLM training to\noverlap GPU communication and computation. We evaluate DistTrain across\ndifferent sizes of multimodal LLMs on a large-scale production cluster with\nthousands of GPUs. The experimental results show that DistTrain achieves 54.7%\nModel FLOPs Utilization (MFU) when training a 72B multimodal LLM on 1172 GPUs\nand outperforms Megatron-LM by up to 2.2$\\times$ on throughput. The ablation\nstudy shows the main techniques of DistTrain are both effective and\nlightweight.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}