{"id":"2408.09834","title":"Minor DPO reject penalty to increase training robustness","authors":"Shiming Xie, Hong Chen, Fred Yu, Zeye Sun, Xiuyu Wu, Yingfan Hu","authorsParsed":[["Xie","Shiming",""],["Chen","Hong",""],["Yu","Fred",""],["Sun","Zeye",""],["Wu","Xiuyu",""],["Hu","Yingfan",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 09:29:31 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 11:49:15 GMT"},{"version":"v3","created":"Fri, 30 Aug 2024 13:54:36 GMT"}],"updateDate":"2024-09-02","timestamp":1724059771000,"abstract":"  Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"Pc71YCJk-tQIjg3ulqSPFHdbMNg3R0NWdgnsqWQhbpM","pdfSize":"991714"}
