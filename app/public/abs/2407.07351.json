{"id":"2407.07351","title":"Unity in Diversity: Multi-expert Knowledge Confrontation and\n  Collaboration for Generalizable Vehicle Re-identification","authors":"Zhenyu Kuang and Hongyang Zhang and Lidong Cheng and Yinhao Liu and\n  Yue Huang and Xinghao Ding","authorsParsed":[["Kuang","Zhenyu",""],["Zhang","Hongyang",""],["Cheng","Lidong",""],["Liu","Yinhao",""],["Huang","Yue",""],["Ding","Xinghao",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 04:06:39 GMT"}],"updateDate":"2024-07-11","timestamp":1720584399000,"abstract":"  Generalizable vehicle re-identification (ReID) aims to enable the\nwell-trained model in diverse source domains to broadly adapt to unknown target\ndomains without additional fine-tuning or retraining. However, it still faces\nthe challenges of domain shift problem and has difficulty accurately\ngeneralizing to unknown target domains. This limitation occurs because the\nmodel relies heavily on primary domain-invariant features in the training data\nand pays less attention to potentially valuable secondary features. To solve\nthis complex and common problem, this paper proposes the two-stage Multi-expert\nKnowledge Confrontation and Collaboration (MiKeCoCo) method, which incorporates\nmultiple experts with unique perspectives into Contrastive Language-Image\nPretraining (CLIP) and fully leverages high-level semantic knowledge for\ncomprehensive feature representation. Specifically, we propose to construct the\nlearnable prompt set of all specific-perspective experts by adversarial\nlearning in the latent space of visual features during the first stage of\ntraining. The learned prompt set with high-level semantics is then utilized to\nguide representation learning of the multi-level features for final knowledge\nfusion in the next stage. In this process of knowledge fusion, although\nmultiple experts employ different assessment ways to examine the same vehicle,\ntheir common goal is to confirm the vehicle's true identity. Their collective\ndecision can ensure the accuracy and consistency of the evaluation results.\nFurthermore, we design different image inputs for two-stage training, which\ninclude image component separation and diversity enhancement in order to\nextract the ID-related prompt representation and to obtain feature\nrepresentation highlighted by all experts, respectively. Extensive experimental\nresults demonstrate that our method achieves state-of-the-art recognition\nperformance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}