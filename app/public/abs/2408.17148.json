{"id":"2408.17148","title":"The Many Faces of Optimal Weak-to-Strong Learning","authors":"Mikael M{\\o}ller H{\\o}gsgaard, Kasper Green Larsen, Markus Engelund\n  Mathiasen","authorsParsed":[["Høgsgaard","Mikael Møller",""],["Larsen","Kasper Green",""],["Mathiasen","Markus Engelund",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 09:38:51 GMT"}],"updateDate":"2024-09-02","timestamp":1725010731000,"abstract":"  Boosting is an extremely successful idea, allowing one to combine multiple\nlow accuracy classifiers into a much more accurate voting classifier. In this\nwork, we present a new and surprisingly simple Boosting algorithm that obtains\na provably optimal sample complexity. Sample optimal Boosting algorithms have\nonly recently been developed, and our new algorithm has the fastest runtime\namong all such algorithms and is the simplest to describe: Partition your\ntraining data into 5 disjoint pieces of equal size, run AdaBoost on each, and\ncombine the resulting classifiers via a majority vote. In addition to this\ntheoretical contribution, we also perform the first empirical comparison of the\nproposed sample optimal Boosting algorithms. Our pilot empirical study suggests\nthat our new algorithm might outperform previous algorithms on large data sets.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Data Structures and Algorithms"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}