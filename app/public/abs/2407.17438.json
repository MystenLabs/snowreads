{"id":"2407.17438","title":"HumanVid: Demystifying Training Data for Camera-controllable Human Image\n  Animation","authors":"Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran\n  Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin","authorsParsed":[["Wang","Zhenzhi",""],["Li","Yixuan",""],["Zeng","Yanhong",""],["Fang","Youqing",""],["Guo","Yuwei",""],["Liu","Wenran",""],["Tan","Jing",""],["Chen","Kai",""],["Xue","Tianfan",""],["Dai","Bo",""],["Lin","Dahua",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:15:58 GMT"},{"version":"v2","created":"Sun, 28 Jul 2024 05:00:10 GMT"}],"updateDate":"2024-07-30","timestamp":1721841358000,"abstract":"  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation. To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at https://github.com/zhenzhiwang/HumanVid/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}