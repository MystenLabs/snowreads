{"id":"2407.09709","title":"GOFA: A Generative One-For-All Model for Joint Graph Language Modeling","authors":"Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang,\n  Yixin Chen, Muhan Zhang","authorsParsed":[["Kong","Lecheng",""],["Feng","Jiarui",""],["Liu","Hao",""],["Huang","Chengsong",""],["Huang","Jiaxin",""],["Chen","Yixin",""],["Zhang","Muhan",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 22:23:51 GMT"}],"updateDate":"2024-07-16","timestamp":1720823031000,"abstract":"  Foundation models, such as Large Language Models (LLMs) or Large Vision\nModels (LVMs), have emerged as one of the most powerful tools in the respective\nfields. However, unlike text and image data, graph data do not have a\ndefinitive structure, posing great challenges to developing a Graph Foundation\nModel (GFM). For example, current attempts at designing general graph models\neither transform graph data into a language format for LLM-based prediction or\nstill train a GNN model with LLM as an assistant. The former can handle\nunlimited tasks, while the latter captures graph structure much better -- yet,\nno existing work can achieve both simultaneously. In this paper, we identify\nthree key desirable properties of a GFM: self-supervised pretraining, fluidity\nin tasks, and graph awareness. To account for these properties, we extend the\nconventional language modeling to the graph domain and propose a novel\ngenerative graph language model GOFA to solve the problem. The model\ninterleaves randomly initialized GNN layers into a frozen pre-trained LLM so\nthat the semantic and structural modeling abilities are organically combined.\nGOFA is pre-trained on newly proposed graph-level next-word prediction,\nquestion-answering, and structural tasks to obtain the above GFM properties.\nThe pre-trained model is further fine-tuned on downstream tasks to obtain\ntask-solving ability. The fine-tuned model is evaluated on various downstream\ntasks, demonstrating a strong ability to solve structural and contextual\nproblems in zero-shot scenarios. The code is available at\nhttps://github.com/JiaruiFeng/GOFA.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}