{"id":"2408.03936","title":"SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic\n  Performance for Mercosur Common Nomenclature","authors":"Vin\\'icius Di Oliveira, Yuri Fa\\c{c}anha Bezerra, Li Weigang, Pedro\n  Carvalho Brom and Victor Rafael R. Celestino","authorsParsed":[["Di Oliveira","Vinícius",""],["Bezerra","Yuri Façanha",""],["Weigang","Li",""],["Brom","Pedro Carvalho",""],["Celestino","Victor Rafael R.",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 17:54:21 GMT"}],"updateDate":"2024-08-08","timestamp":1723053261000,"abstract":"  Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}