{"id":"2407.07662","title":"Mitigating Backdoor Attacks using Activation-Guided Model Editing","authors":"Felix Hsieh, Huy H. Nguyen, AprilPyone MaungMaung, Dmitrii Usynin,\n  Isao Echizen","authorsParsed":[["Hsieh","Felix",""],["Nguyen","Huy H.",""],["MaungMaung","AprilPyone",""],["Usynin","Dmitrii",""],["Echizen","Isao",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:43:47 GMT"}],"updateDate":"2024-07-11","timestamp":1720619027000,"abstract":"  Backdoor attacks compromise the integrity and reliability of machine learning\nmodels by embedding a hidden trigger during the training process, which can\nlater be activated to cause unintended misbehavior. We propose a novel backdoor\nmitigation approach via machine unlearning to counter such backdoor attacks.\nThe proposed method utilizes model activation of domain-equivalent unseen data\nto guide the editing of the model's weights. Unlike the previous\nunlearning-based mitigation methods, ours is computationally inexpensive and\nachieves state-of-the-art performance while only requiring a handful of unseen\nsamples for unlearning. In addition, we also point out that unlearning the\nbackdoor may cause the whole targeted class to be unlearned, thus introducing\nan additional repair step to preserve the model's utility after editing the\nmodel. Experiment results show that the proposed method is effective in\nunlearning the backdoor on different datasets and trigger patterns.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}