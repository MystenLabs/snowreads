{"id":"2408.14470","title":"Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models","authors":"Aradhye Agarwal, Suhas K Ramesh, Ayan Sengupta, Tanmoy Chakraborty","authorsParsed":[["Agarwal","Aradhye",""],["Ramesh","Suhas K",""],["Sengupta","Ayan",""],["Chakraborty","Tanmoy",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 17:58:53 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 03:56:11 GMT"}],"updateDate":"2024-08-28","timestamp":1724695133000,"abstract":"  Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}