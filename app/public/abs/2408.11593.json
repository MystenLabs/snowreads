{"id":"2408.11593","title":"MCDubber: Multimodal Context-Aware Expressive Video Dubbing","authors":"Yuan Zhao, Zhenqi Jia, Rui Liu, De Hu, Feilong Bao, and Guanglai Gao","authorsParsed":[["Zhao","Yuan",""],["Jia","Zhenqi",""],["Liu","Rui",""],["Hu","De",""],["Bao","Feilong",""],["Gao","Guanglai",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 12:59:42 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 09:04:51 GMT"},{"version":"v3","created":"Wed, 4 Sep 2024 01:25:55 GMT"}],"updateDate":"2024-09-05","timestamp":1724245182000,"abstract":"  Automatic Video Dubbing (AVD) aims to take the given script and generate\nspeech that aligns with lip motion and prosody expressiveness. Current AVD\nmodels mainly utilize visual information of the current sentence to enhance the\nprosody of synthesized speech. However, it is crucial to consider whether the\nprosody of the generated dubbing aligns with the multimodal context, as the\ndubbing will be combined with the original context in the final video. This\naspect has been overlooked in previous studies. To address this issue, we\npropose a Multimodal Context-aware video Dubbing model, termed\n\\textbf{MCDubber}, to convert the modeling object from a single sentence to a\nlonger sequence with context information to ensure the consistency of the\nglobal context prosody. MCDubber comprises three main components: (1) A context\nduration aligner aims to learn the context-aware alignment between the text and\nlip frames; (2) A context prosody predictor seeks to read the global context\nvisual sequence and predict the context-aware global energy and pitch; (3) A\ncontext acoustic decoder ultimately predicts the global context mel-spectrogram\nwith the assistance of adjacent ground-truth mel-spectrograms of the target\nsentence. Through this process, MCDubber fully considers the influence of\nmultimodal context on the prosody expressiveness of the current sentence when\ndubbing. The extracted mel-spectrogram belonging to the target sentence from\nthe output context mel-spectrograms is the final required dubbing audio.\nExtensive experiments on the Chem benchmark dataset demonstrate that our\nMCDubber significantly improves dubbing expressiveness compared to all advanced\nbaselines. The code and demos are available at\nhttps://github.com/XiaoYuanJun-zy/MCDubber.\n","subjects":["Computing Research Repository/Multimedia","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}