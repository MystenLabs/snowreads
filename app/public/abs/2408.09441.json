{"id":"2408.09441","title":"CLIP-CID: Efficient CLIP Distillation via Cluster-Instance\n  Discrimination","authors":"Kaicheng Yang, Tiancheng Gu, Xiang An, Haiqiang Jiang, Xiangzi Dai,\n  Ziyong Feng, Weidong Cai, Jiankang Deng","authorsParsed":[["Yang","Kaicheng",""],["Gu","Tiancheng",""],["An","Xiang",""],["Jiang","Haiqiang",""],["Dai","Xiangzi",""],["Feng","Ziyong",""],["Cai","Weidong",""],["Deng","Jiankang",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 11:23:21 GMT"}],"updateDate":"2024-08-20","timestamp":1723980201000,"abstract":"  Contrastive Language-Image Pre-training (CLIP) has achieved excellent\nperformance over a wide range of tasks. However, the effectiveness of CLIP\nheavily relies on a substantial corpus of pre-training data, resulting in\nnotable consumption of computational resources. Although knowledge distillation\nhas been widely applied in single modality models, how to efficiently expand\nknowledge distillation to vision-language foundation models with extensive data\nremains relatively unexplored. In this paper, we introduce CLIP-CID, a novel\ndistillation mechanism that effectively transfers knowledge from a large\nvision-language foundation model to a smaller model. We initially propose a\nsimple but efficient image semantic balance method to reduce transfer learning\nbias and improve distillation efficiency. This method filters out 43.7% of\nimage-text pairs from the LAION400M while maintaining superior performance.\nAfter that, we leverage cluster-instance discrimination to facilitate knowledge\ntransfer from the teacher model to the student model, thereby empowering the\nstudent model to acquire a holistic semantic comprehension of the pre-training\ndata. Experimental results demonstrate that CLIP-CID achieves state-of-the-art\nperformance on various downstream tasks including linear probe and zero-shot\nclassification.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8kCJ8bVkRVTWXS88nB_daoPRdNY_djWB83FznWYfhuw","pdfSize":"21511430"}
