{"id":"2408.12325","title":"Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators","authors":"Dingkang Yang, Dongling Xiao, Jinjie Wei, Mingcheng Li, Zhaoyu Chen,\n  Ke Li, Lihua Zhang","authorsParsed":[["Yang","Dingkang",""],["Xiao","Dongling",""],["Wei","Jinjie",""],["Li","Mingcheng",""],["Chen","Zhaoyu",""],["Li","Ke",""],["Zhang","Lihua",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 12:00:31 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 13:10:50 GMT"}],"updateDate":"2024-09-10","timestamp":1724328031000,"abstract":"  Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}