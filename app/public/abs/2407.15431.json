{"id":"2407.15431","title":"Pre-Training and Prompting for Few-Shot Node Classification on\n  Text-Attributed Graphs","authors":"Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang,\n  Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang","authorsParsed":[["Zhao","Huanjing",""],["Yang","Beining",""],["Cen","Yukuo",""],["Ren","Junyu",""],["Zhang","Chenhui",""],["Dong","Yuxiao",""],["Kharlamov","Evgeny",""],["Zhao","Shu",""],["Tang","Jie",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 07:24:21 GMT"}],"updateDate":"2024-07-23","timestamp":1721633061000,"abstract":"  The text-attributed graph (TAG) is one kind of important real-world\ngraph-structured data with each node associated with raw texts. For TAGs,\ntraditional few-shot node classification methods directly conduct training on\nthe pre-processed node features and do not consider the raw texts. The\nperformance is highly dependent on the choice of the feature pre-processing\nmethod. In this paper, we propose P2TAG, a framework designed for few-shot node\nclassification on TAGs with graph pre-training and prompting. P2TAG first\npre-trains the language model (LM) and graph neural network (GNN) on TAGs with\nself-supervised loss. To fully utilize the ability of language models, we adapt\nthe masked language modeling objective for our framework. The pre-trained model\nis then used for the few-shot node classification with a mixed prompt method,\nwhich simultaneously considers both text and graph information. We conduct\nexperiments on six real-world TAGs, including paper citation networks and\nproduct co-purchasing networks. Experimental results demonstrate that our\nproposed framework outperforms existing graph few-shot learning methods on\nthese datasets with +18.98% ~ +35.98% improvements.\n","subjects":["Computing Research Repository/Social and Information Networks","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}