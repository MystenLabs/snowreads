{"id":"2408.08553","title":"Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model\n  with Large Language Model's Experience","authors":"Xin Yin, Chao Ni, Xiaodan Xu, Xinrui Li, Xiaohu Yang","authorsParsed":[["Yin","Xin",""],["Ni","Chao",""],["Xu","Xiaodan",""],["Li","Xinrui",""],["Yang","Xiaohu",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 06:37:59 GMT"}],"updateDate":"2024-08-19","timestamp":1723790279000,"abstract":"  Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models aim to understand the patterns in source code and\nuse these patterns to predict code properties. However, fine-tuning LLMs is\ntime-consuming and costly for end users and small organizations. Furthermore,\nfine-tuning LMs heavily depends on the amount and quality of datasets\navailable. As a result, the current lack of data and the high cost of\ncollecting it in real-world scenarios further limit the applicability of LMs.\nIn this paper, we leverage the powerful generation capabilities of LLMs to\nenhance pre-trained LMs. Specifically, we use LLMs to generate domain-specific\ndata, thereby improving the performance of pre-trained LMs on the target tasks.\nWe conduct experiments by combining different LLMs in our generation phase and\nintroducing various LMs to learn from the LLM-generated data. Then, we compare\nthe performance of these LMs before and after learning the data. We find that\nLLM-generated data significantly enhances the performance of LMs. The\nimprovement can reach up to 58.36% for fault localization and up to 6.09% for\nclone detection. Our study highlights that using LLMs to generate data for LMs\ncan improve performance by a large margin.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}