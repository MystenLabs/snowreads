{"id":"2408.00523","title":"Jailbreaking Text-to-Image Models with LLM-Based Agents","authors":"Yingkai Dong, Zheng Li, Xiangtao Meng, Ning Yu, Shanqing Guo","authorsParsed":[["Dong","Yingkai",""],["Li","Zheng",""],["Meng","Xiangtao",""],["Yu","Ning",""],["Guo","Shanqing",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 12:54:46 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 08:09:14 GMT"}],"updateDate":"2024-09-10","timestamp":1722516886000,"abstract":"  Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving their potential for addressing generative AI safety tasks\nlargely unexplored. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework targeting generative AI models, specifically focusing on\njailbreak attacks against text-to-image (T2I) models with built-in safety\nfilters. Atlas consists of two agents, namely the mutation agent and the\nselection agent, each comprising four key modules: a vision-language model\n(VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses\nits VLM brain to determine whether a prompt triggers the T2I model's safety\nfilter. It then collaborates iteratively with the LLM brain of the selection\nagent to generate new candidate jailbreak prompts with the highest potential to\nbypass the filter. In addition to multi-agent communication, we leverage\nin-context learning (ICL) memory mechanisms and the chain-of-thought (COT)\napproach to learn from past successes and failures, thereby enhancing Atlas's\nperformance. Our evaluation demonstrates that Atlas successfully jailbreaks\nseveral state-of-the-art T2I models equipped with multi-modal safety filters in\na black-box setting. Additionally, Atlas outperforms existing methods in both\nquery efficiency and the quality of generated images. This work convincingly\ndemonstrates the successful application of LLM-based agents in studying the\nsafety vulnerabilities of popular text-to-image generation models. We urge the\ncommunity to consider advanced techniques like ours in response to the rapidly\nevolving text-to-image generation field.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}