{"id":"2407.00950","title":"Causal Bandits: The Pareto Optimal Frontier of Adaptivity, a Reduction\n  to Linear Bandits, and Limitations around Unknown Marginals","authors":"Ziyi Liu, Idan Attias, Daniel M. Roy","authorsParsed":[["Liu","Ziyi",""],["Attias","Idan",""],["Roy","Daniel M.",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 04:12:15 GMT"}],"updateDate":"2024-07-02","timestamp":1719807135000,"abstract":"  In this work, we investigate the problem of adapting to the presence or\nabsence of causal structure in multi-armed bandit problems. In addition to the\nusual reward signal, we assume the learner has access to additional variables,\nobserved in each round after acting. When these variables $d$-separate the\naction from the reward, existing work in causal bandits demonstrates that one\ncan achieve strictly better (minimax) rates of regret (Lu et al., 2020). Our\ngoal is to adapt to this favorable \"conditionally benign\" structure, if it is\npresent in the environment, while simultaneously recovering worst-case minimax\nregret, if it is not. Notably, the learner has no prior knowledge of whether\nthe favorable structure holds. In this paper, we establish the Pareto optimal\nfrontier of adaptive rates. We prove upper and matching lower bounds on the\npossible trade-offs in the performance of learning in conditionally benign and\narbitrary environments, resolving an open question raised by Bilodeau et al.\n(2022). Furthermore, we are the first to obtain instance-dependent bounds for\ncausal bandits, by reducing the problem to the linear bandit setting. Finally,\nwe examine the common assumption that the marginal distributions of the\npost-action contexts are known and show that a nontrivial estimate is necessary\nfor better-than-worst-case minimax rates.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}