{"id":"2408.08568","title":"Unsupervised Non-Rigid Point Cloud Matching through Large Vision Models","authors":"Zhangquan Chen, Puhua Jiang, Ruqi Huang","authorsParsed":[["Chen","Zhangquan",""],["Jiang","Puhua",""],["Huang","Ruqi",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 07:02:19 GMT"}],"updateDate":"2024-08-19","timestamp":1723791739000,"abstract":"  In this paper, we propose a novel learning-based framework for non-rigid\npoint cloud matching, which can be trained purely on point clouds without any\ncorrespondence annotation but also be extended naturally to partial-to-full\nmatching. Our key insight is to incorporate semantic features derived from\nlarge vision models (LVMs) to geometry-based shape feature learning. Our\nframework effectively leverages the structural information contained in the\nsemantic features to address ambiguities arise from self-similarities among\nlocal geometries. Furthermore, our framework also enjoys the strong\ngeneralizability and robustness regarding partial observations of LVMs, leading\nto improvements in the regarding point cloud matching tasks. In order to\nachieve the above, we propose a pixel-to-point feature aggregation module, a\nlocal and global attention network as well as a geometrical similarity loss\nfunction. Experimental results show that our method achieves state-of-the-art\nresults in matching non-rigid point clouds in both near-isometric and\nheterogeneous shape collection as well as more realistic partial and noisy\ndata.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}