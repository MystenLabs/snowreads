{"id":"2407.19361","title":"On universal inference in Gaussian mixture models","authors":"Hongjian Shi and Mathias Drton","authorsParsed":[["Shi","Hongjian",""],["Drton","Mathias",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 01:54:50 GMT"}],"updateDate":"2024-07-30","timestamp":1722131690000,"abstract":"  Recent work on game-theoretic statistics and safe anytime-valid inference\n(SAVI) provides new tools for statistical inference without assuming any\nregularity conditions. In particular, the framework of universal inference\nproposed by Wasserman, Ramdas, and Balakrishnan (2020) offers new solutions by\nmodifying the likelihood ratio test in a data-splitting scheme. In this paper,\nwe study the performance of the resulting split likelihood ratio test under\nGaussian mixture models, which are canonical examples for models in which\nclassical regularity conditions fail to hold. We first establish that under the\nnull hypothesis, the split likelihood ratio statistic is asymptotically normal\nwith increasing mean and variance. Moreover, contradicting the usual belief\nthat the flexibility of SAVI and universal methods comes at the price of a\nsignificant loss of power, we are able to prove that universal inference\nsurprisingly achieves the same detection rate $(n^{-1}\\log\\log n)^{1/2}$ as the\nclassical likelihood ratio test.\n","subjects":["Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}