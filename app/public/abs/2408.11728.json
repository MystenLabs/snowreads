{"id":"2408.11728","title":"AI-assisted Automated Short Answer Grading of Handwritten University\n  Level Mathematics Exams","authors":"Tianyi Liu, Julia Chatain, Laura Kobel-Keller, Gerd Kortemeyer, Thomas\n  Willwacher and Mrinmaya Sachan","authorsParsed":[["Liu","Tianyi",""],["Chatain","Julia",""],["Kobel-Keller","Laura",""],["Kortemeyer","Gerd",""],["Willwacher","Thomas",""],["Sachan","Mrinmaya",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 15:54:06 GMT"}],"updateDate":"2024-08-22","timestamp":1724255646000,"abstract":"  Effective and timely feedback in educational assessments is essential but\nlabor-intensive, especially for complex tasks. Recent developments in automated\nfeedback systems, ranging from deterministic response grading to the evaluation\nof semi-open and open-ended essays, have been facilitated by advances in\nmachine learning. The emergence of pre-trained Large Language Models, such as\nGPT-4, offers promising new opportunities for efficiently processing diverse\nresponse types with minimal customization. This study evaluates the\neffectiveness of a pre-trained GPT-4 model in grading semi-open handwritten\nresponses in a university-level mathematics exam. Our findings indicate that\nGPT-4 provides surprisingly reliable and cost-effective initial grading,\nsubject to subsequent human verification. Future research should focus on\nrefining grading rules and enhancing the extraction of handwritten responses to\nfurther leverage these technologies.\n","subjects":["Mathematics/History and Overview"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}