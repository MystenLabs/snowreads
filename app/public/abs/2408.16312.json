{"id":"2408.16312","title":"SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval","authors":"Hossein A. Rahmani, Xi Wang, Emine Yilmaz, Nick Craswell, Bhaskar\n  Mitra, Paul Thomas","authorsParsed":[["Rahmani","Hossein A.",""],["Wang","Xi",""],["Yilmaz","Emine",""],["Craswell","Nick",""],["Mitra","Bhaskar",""],["Thomas","Paul",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 07:20:56 GMT"},{"version":"v2","created":"Fri, 30 Aug 2024 11:48:40 GMT"}],"updateDate":"2024-09-02","timestamp":1724916056000,"abstract":"  Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}