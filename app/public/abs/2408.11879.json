{"id":"2408.11879","title":"Beyond Labels: Aligning Large Language Models with Human-like Reasoning","authors":"Muhammad Rafsan Kabir, Rafeed Mohammad Sultan, Ihsanul Haque Asif,\n  Jawad Ibn Ahad, Fuad Rahman, Mohammad Ruhul Amin, Nabeel Mohammed, Shafin\n  Rahman","authorsParsed":[["Kabir","Muhammad Rafsan",""],["Sultan","Rafeed Mohammad",""],["Asif","Ihsanul Haque",""],["Ahad","Jawad Ibn",""],["Rahman","Fuad",""],["Amin","Mohammad Ruhul",""],["Mohammed","Nabeel",""],["Rahman","Shafin",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 17:44:51 GMT"}],"updateDate":"2024-08-23","timestamp":1724175891000,"abstract":"  Aligning large language models (LLMs) with a human reasoning approach ensures\nthat LLMs produce morally correct and human-like decisions. Ethical concerns\nare raised because current models are prone to generating false positives and\nproviding malicious responses. To contribute to this issue, we have curated an\nethics dataset named Dataset for Aligning Reasons (DFAR), designed to aid in\naligning language models to generate human-like reasons. The dataset comprises\nstatements with ethical-unethical labels and their corresponding reasons. In\nthis study, we employed a unique and novel fine-tuning approach that utilizes\nethics labels and their corresponding reasons (L+R), in contrast to the\nexisting fine-tuning approach that only uses labels (L). The original\npre-trained versions, the existing fine-tuned versions, and our proposed\nfine-tuned versions of LLMs were then evaluated on an ethical-unethical\nclassification task and a reason-generation task. Our proposed fine-tuning\nstrategy notably outperforms the others in both tasks, achieving significantly\nhigher accuracy scores in the classification task and lower misalignment rates\nin the reason-generation task. The increase in classification accuracies and\ndecrease in misalignment rates indicate that the L+R fine-tuned models align\nmore with human ethics. Hence, this study illustrates that injecting reasons\nhas substantially improved the alignment of LLMs, resulting in more human-like\nresponses. We have made the DFAR dataset and corresponding codes publicly\navailable at https://github.com/apurba-nsu-rnd-lab/DFAR.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}