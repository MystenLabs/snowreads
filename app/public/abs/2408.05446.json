{"id":"2408.05446","title":"Ensemble everything everywhere: Multi-scale aggregation for adversarial\n  robustness","authors":"Stanislav Fort, Balaji Lakshminarayanan","authorsParsed":[["Fort","Stanislav",""],["Lakshminarayanan","Balaji",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 14:21:16 GMT"}],"updateDate":"2024-08-13","timestamp":1723126876000,"abstract":"  Adversarial examples pose a significant challenge to the robustness,\nreliability and alignment of deep neural networks. We propose a novel,\neasy-to-use approach to achieving high-quality representations that lead to\nadversarial robustness through the use of multi-resolution input\nrepresentations and dynamic self-ensembling of intermediate layer predictions.\nWe demonstrate that intermediate layer predictions exhibit inherent robustness\nto adversarial attacks crafted to fool the full classifier, and propose a\nrobust aggregation mechanism based on Vickrey auction that we call\n\\textit{CrossMax} to dynamically ensemble them. By combining multi-resolution\ninputs and robust ensembling, we achieve significant adversarial robustness on\nCIFAR-10 and CIFAR-100 datasets without any adversarial training or extra data,\nreaching an adversarial accuracy of $\\approx$72% (CIFAR-10) and $\\approx$48%\n(CIFAR-100) on the RobustBench AutoAttack suite ($L_\\infty=8/255)$ with a\nfinetuned ImageNet-pretrained ResNet152. This represents a result comparable\nwith the top three models on CIFAR-10 and a +5 % gain compared to the best\ncurrent dedicated approach on CIFAR-100. Adding simple adversarial training on\ntop, we get $\\approx$78% on CIFAR-10 and $\\approx$51% on CIFAR-100, improving\nSOTA by 5 % and 9 % respectively and seeing greater gains on the harder\ndataset. We validate our approach through extensive experiments and provide\ninsights into the interplay between adversarial robustness, and the\nhierarchical nature of deep representations. We show that simple gradient-based\nattacks against our model lead to human-interpretable images of the target\nclasses as well as interpretable image changes. As a byproduct, using our\nmulti-resolution prior, we turn pre-trained classifiers and CLIP models into\ncontrollable image generators and develop successful transferable attacks on\nlarge vision language models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JfaxMycmj5bdVUfRRVLggRW_dNwPye_6hV0wnap9lPs","pdfSize":"9035864"}
