{"id":"2407.12073","title":"Relational Representation Distillation","authors":"Nikolaos Giakoumoglou, Tania Stathaki","authorsParsed":[["Giakoumoglou","Nikolaos",""],["Stathaki","Tania",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:56:13 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 10:25:18 GMT"},{"version":"v3","created":"Mon, 9 Sep 2024 06:13:48 GMT"}],"updateDate":"2024-09-10","timestamp":1721141773000,"abstract":"  Knowledge distillation (KD) is an effective method for transferring knowledge\nfrom a large, well-trained teacher model to a smaller, more efficient student\nmodel. Despite its success, one of the main challenges in KD is ensuring the\nefficient transfer of complex knowledge while maintaining the student's\ncomputational efficiency. Unlike previous works that applied contrastive\nobjectives promoting explicit negative instances with little attention to the\nrelationships between them, we introduce Relational Representation Distillation\n(RRD). Our approach leverages pairwise similarities to explore and reinforce\nthe relationships between the teacher and student models. Inspired by\nself-supervised learning principles, it uses a relaxed contrastive loss that\nfocuses on similarity rather than exact replication. This method aligns the\noutput distributions of teacher samples in a large memory buffer, improving the\nrobustness and performance of the student model without the need for strict\nnegative instance differentiation. Our approach demonstrates superior\nperformance on CIFAR-100 and ImageNet ILSVRC-2012, outperforming traditional KD\nand sometimes even outperforms the teacher network when combined with KD. It\nalso transfers successfully to other datasets like Tiny ImageNet and STL-10.\nCode is available at https://github.com/giakoumoglou/distillers.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}