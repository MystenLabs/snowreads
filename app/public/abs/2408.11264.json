{"id":"2408.11264","title":"Correlation Analysis of Adversarial Attack in Time Series Classification","authors":"Zhengyang Li, Wenhao Liang, Chang Dong, Weitong Chen, Dong Huang","authorsParsed":[["Li","Zhengyang",""],["Liang","Wenhao",""],["Dong","Chang",""],["Chen","Weitong",""],["Huang","Dong",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 01:11:32 GMT"}],"updateDate":"2024-08-22","timestamp":1724202692000,"abstract":"  This study investigates the vulnerability of time series classification\nmodels to adversarial attacks, with a focus on how these models process local\nversus global information under such conditions. By leveraging the Normalized\nAuto Correlation Function (NACF), an exploration into the inclination of neural\nnetworks is conducted. It is demonstrated that regularization techniques,\nparticularly those employing Fast Fourier Transform (FFT) methods and targeting\nfrequency components of perturbations, markedly enhance the effectiveness of\nattacks. Meanwhile, the defense strategies, like noise introduction and\nGaussian filtering, are shown to significantly lower the Attack Success Rate\n(ASR), with approaches based on noise introducing notably effective in\ncountering high-frequency distortions. Furthermore, models designed to\nprioritize global information are revealed to possess greater resistance to\nadversarial manipulations. These results underline the importance of designing\nattack and defense mechanisms, informed by frequency domain analysis, as a\nmeans to considerably reinforce the resilience of neural network models against\nadversarial threats.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Y57ArGg015HGuP1xjJ3WegM39KMNzlk2A7FxZMQE9JA","pdfSize":"2777196"}
