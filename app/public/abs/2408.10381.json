{"id":"2408.10381","title":"Efficient Reinforcement Learning in Probabilistic Reward Machines","authors":"Xiaofeng Lin, Xuezhou Zhang","authorsParsed":[["Lin","Xiaofeng",""],["Zhang","Xuezhou",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 19:51:53 GMT"}],"updateDate":"2024-08-21","timestamp":1724097113000,"abstract":"  In this paper, we study reinforcement learning in Markov Decision Processes\nwith Probabilistic Reward Machines (PRMs), a form of non-Markovian reward\ncommonly found in robotics tasks. We design an algorithm for PRMs that achieves\na regret bound of $\\widetilde{O}(\\sqrt{HOAT} + H^2O^2A^{3/2} + H\\sqrt{T})$,\nwhere $H$ is the time horizon, $O$ is the number of observations, $A$ is the\nnumber of actions, and $T$ is the number of time-steps. This result improves\nover the best-known bound, $\\widetilde{O}(H\\sqrt{OAT})$ of\n\\citet{pmlr-v206-bourel23a} for MDPs with Deterministic Reward Machines (DRMs),\na special case of PRMs. When $T \\geq H^3O^3A^2$ and $OA \\geq H$, our regret\nbound leads to a regret of $\\widetilde{O}(\\sqrt{HOAT})$, which matches the\nestablished lower bound of $\\Omega(\\sqrt{HOAT})$ for MDPs with DRMs up to a\nlogarithmic factor. To the best of our knowledge, this is the first efficient\nalgorithm for PRMs. Additionally, we present a new simulation lemma for\nnon-Markovian rewards, which enables reward-free exploration for any\nnon-Markovian reward given access to an approximate planner. Complementing our\ntheoretical findings, we show through extensive experiment evaluations that our\nalgorithm indeed outperforms prior methods in various PRM environments.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}