{"id":"2408.17027","title":"ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features\n  from Multi-View Images","authors":"Xiaoshuai Zhang, Zhicheng Wang, Howard Zhou, Soham Ghosh, Danushen\n  Gnanapragasam, Varun Jampani, Hao Su, Leonidas Guibas","authorsParsed":[["Zhang","Xiaoshuai",""],["Wang","Zhicheng",""],["Zhou","Howard",""],["Ghosh","Soham",""],["Gnanapragasam","Danushen",""],["Jampani","Varun",""],["Su","Hao",""],["Guibas","Leonidas",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 05:57:01 GMT"}],"updateDate":"2024-09-02","timestamp":1724997421000,"abstract":"  To advance the state of the art in the creation of 3D foundation models, this\npaper introduces the ConDense framework for 3D pre-training utilizing existing\npre-trained 2D networks and large-scale multi-view datasets. We propose a novel\n2D-3D joint training scheme to extract co-embedded 2D and 3D features in an\nend-to-end pipeline, where 2D-3D feature consistency is enforced through a\nvolume rendering NeRF-like ray marching process. Using dense per pixel features\nwe are able to 1) directly distill the learned priors from 2D models to 3D\nmodels and create useful 3D backbones, 2) extract more consistent and less\nnoisy 2D features, 3) formulate a consistent embedding space where 2D, 3D, and\nother modalities of data (e.g., natural language prompts) can be jointly\nqueried. Furthermore, besides dense features, ConDense can be trained to\nextract sparse features (e.g., key points), also with 2D-3D consistency --\ncondensing 3D NeRF representations into compact sets of decorated key points.\nWe demonstrate that our pre-trained model provides good initialization for\nvarious 3D tasks including 3D classification and segmentation, outperforming\nother 3D pre-training methods by a significant margin. It also enables, by\nexploiting our sparse features, additional useful downstream tasks, such as\nmatching 2D images to 3D scenes, detecting duplicate 3D scenes, and querying a\nrepository of 3D scenes through natural language -- all quite efficiently and\nwithout any per-scene fine-tuning.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}