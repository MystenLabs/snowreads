{"id":"2408.04220","title":"Diffusion Guided Language Modeling","authors":"Justin Lovelace, Varsha Kishore, Yiwei Chen, Kilian Q. Weinberger","authorsParsed":[["Lovelace","Justin",""],["Kishore","Varsha",""],["Chen","Yiwei",""],["Weinberger","Kilian Q.",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 05:06:22 GMT"}],"updateDate":"2024-08-09","timestamp":1723093582000,"abstract":"  Current language models demonstrate remarkable proficiency in text\ngeneration. However, for many applications it is desirable to control\nattributes, such as sentiment, or toxicity, of the generated language --\nideally tailored towards each specific use case and target audience. For\nauto-regressive language models, existing guidance methods are prone to\ndecoding errors that cascade during generation and degrade performance. In\ncontrast, text diffusion models can easily be guided with, for example, a\nsimple linear sentiment classifier -- however they do suffer from significantly\nhigher perplexity than auto-regressive alternatives. In this paper we use a\nguided diffusion model to produce a latent proposal that steers an\nauto-regressive language model to generate text with desired properties. Our\nmodel inherits the unmatched fluency of the auto-regressive approach and the\nplug-and-play flexibility of diffusion. We show that it outperforms previous\nplug-and-play guidance methods across a wide range of benchmark data sets.\nFurther, controlling a new attribute in our framework is reduced to training a\nsingle logistic regression classifier.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"g5dMkYjyDoYvMTR9QowWfZcJkkuCL_bvRcTK0k6Zz90","pdfSize":"1117809"}
