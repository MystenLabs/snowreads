{"id":"2408.01923","title":"Scalable Signal Temporal Logic Guided Reinforcement Learning via Value\n  Function Space Optimization","authors":"Yiting He, Peiran Liu, Yiding Ji","authorsParsed":[["He","Yiting",""],["Liu","Peiran",""],["Ji","Yiding",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 04:34:29 GMT"}],"updateDate":"2024-08-06","timestamp":1722746069000,"abstract":"  The integration of reinforcement learning (RL) and formal methods has emerged\nas a promising framework for solving long-horizon planning problems.\nConventional approaches typically involve abstraction of the state and action\nspaces and manually created labeling functions or predicates. However, the\nefficiency of these approaches deteriorates as the tasks become increasingly\ncomplex, which results in exponential growth in the size of labeling functions\nor predicates. To address these issues, we propose a scalable model-based RL\nframework, called VFSTL, which schedules pre-trained skills to follow unseen\nSTL specifications without using hand-crafted predicates. Given a set of value\nfunctions obtained by goal-conditioned RL, we formulate an optimization problem\nto maximize the robustness value of Signal Temporal Logic (STL) defined\nspecifications, which is computed using value functions as predicates. To\nfurther reduce the computation burden, we abstract the environment state space\ninto the value function space (VFS). Then the optimization problem is solved by\nModel-Based Reinforcement Learning. Simulation results show that STL with value\nfunctions as predicates approximates the ground truth robustness and the\nplanning in VFS directly achieves unseen specifications using data from\nsensors.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TWC-XGFAeeaqbGrVTOU7lBDUk0xPgOfKpynKjRq4G4Y","pdfSize":"1251030","txDigest":"3LreXGj3fYhHrCnnt3EmqH5bmDuPP2dDBsdvs6kNBmSh","endEpoch":"1","status":"CERTIFIED"}
