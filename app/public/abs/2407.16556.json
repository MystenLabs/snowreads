{"id":"2407.16556","title":"DC is all you need: describing ReLU from a signal processing standpoint","authors":"Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza","authorsParsed":[["Kechris","Christodoulos",""],["Dan","Jonathan",""],["Miranda","Jose",""],["Atienza","David",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 15:09:40 GMT"}],"updateDate":"2024-07-24","timestamp":1721747380000,"abstract":"  Non-linear activation functions are crucial in Convolutional Neural Networks.\nHowever, until now they have not been well described in the frequency domain.\nIn this work, we study the spectral behavior of ReLU, a popular activation\nfunction. We use the ReLU's Taylor expansion to derive its frequency domain\nbehavior. We demonstrate that ReLU introduces higher frequency oscillations in\nthe signal and a constant DC component. Furthermore, we investigate the\nimportance of this DC component, where we demonstrate that it helps the model\nextract meaningful features related to the input frequency content. We\naccompany our theoretical derivations with experiments and real-world examples.\nFirst, we numerically validate our frequency response model. Then we observe\nReLU's spectral behavior on two example models and a real-world one. Finally,\nwe experimentally investigate the role of the DC component introduced by ReLU\nin the CNN's representations. Our results indicate that the DC helps to\nconverge to a weight configuration that is close to the initial random weights.\n","subjects":["Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}