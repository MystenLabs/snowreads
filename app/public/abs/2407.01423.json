{"id":"2407.01423","title":"FairLay-ML: Intuitive Debugging of Fairness in Data-Driven\n  Social-Critical Software","authors":"Normen Yu and Luciana Carreon and Gang Tan and Saeid Tizpaz-Niari","authorsParsed":[["Yu","Normen",""],["Carreon","Luciana",""],["Tan","Gang",""],["Tizpaz-Niari","Saeid",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 16:13:54 GMT"}],"updateDate":"2024-07-02","timestamp":1719850434000,"abstract":"  Data-driven software solutions have significantly been used in critical\ndomains with significant socio-economic, legal, and ethical implications. The\nrapid adoptions of data-driven solutions, however, pose major threats to the\ntrustworthiness of automated decision-support software. A diminished\nunderstanding of the solution by the developer and historical/current biases in\nthe data sets are primary challenges.\n  To aid data-driven software developers and end-users, we present \\toolname, a\ndebugging tool to test and explain the fairness implications of data-driven\nsolutions. \\toolname visualizes the logic of datasets, trained models, and\ndecisions for a given data point. In addition, it trains various models with\nvarying fairness-accuracy trade-offs. Crucially, \\toolname incorporates\ncounterfactual fairness testing that finds bugs beyond the development\ndatasets. We conducted two studies through \\toolname that allowed us to measure\nfalse positives/negatives in prevalent counterfactual testing and understand\nthe human perception of counterfactual test cases in a class survey. \\toolname\nand its benchmarks are publicly available\nat~\\url{https://github.com/Pennswood/FairLay-ML}. The live version of the tool\nis available at~\\url{https://fairlayml-v2.streamlit.app/}. We provide a video\ndemo of the tool at https://youtu.be/wNI9UWkywVU?t=127\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Computers and Society","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}