{"id":"2408.03885","title":"Global-Local Progressive Integration Network for Blind Image Quality\n  Assessment","authors":"Xiaoqi Wang, Yun Zhang","authorsParsed":[["Wang","Xiaoqi",""],["Zhang","Yun",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 16:34:32 GMT"}],"updateDate":"2024-08-08","timestamp":1723048472000,"abstract":"  Vision transformers (ViTs) excel in computer vision for modeling long-term\ndependencies, yet face two key challenges for image quality assessment (IQA):\ndiscarding fine details during patch embedding, and requiring extensive\ntraining data due to lack of inductive biases. In this study, we propose a\nGlobal-Local progressive INTegration network for IQA, called GlintIQA, to\naddress these issues through three key components: 1) Hybrid feature extraction\ncombines ViT-based global feature extractor (VGFE) and convolutional neural\nnetworks (CNNs)-based local feature extractor (CLFE) to capture global\ncoarse-grained features and local fine-grained features, respectively. The\nincorporation of CNNs mitigates the patch-level information loss and inductive\nbias constraints inherent to ViT architectures. 2) Progressive feature\nintegration leverages diverse kernel sizes in embedding to spatially align\ncoarse- and fine-grained features, and progressively aggregate these features\nby interactively stacking channel-wise attention and spatial enhancement\nmodules to build effective quality-aware representations. 3) Content\nsimilarity-based labeling approach is proposed that automatically assigns\nquality labels to images with diverse content based on subjective quality\nscores. This addresses the scarcity of labeled training data in synthetic\ndatasets and bolsters model generalization. The experimental results\ndemonstrate the efficacy of our approach, yielding 5.04% average SROCC gains on\ncross-authentic dataset evaluations. Moreover, our model and its counterpart\npre-trained on the proposed dataset respectively exhibited 5.40% and 13.23%\nimprovements on across-synthetic datasets evaluation. The codes and proposed\ndataset will be released at https://github.com/XiaoqiWang/GlintIQA.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}