{"id":"2407.15537","title":"Exterior Penalty Policy Optimization with Penalty Metric Network under\n  Constraints","authors":"Shiqing Gao, Jiaxin Ding, Luoyi Fu, Xinbing Wang, Chenghu Zhou","authorsParsed":[["Gao","Shiqing",""],["Ding","Jiaxin",""],["Fu","Luoyi",""],["Wang","Xinbing",""],["Zhou","Chenghu",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 10:57:32 GMT"}],"updateDate":"2024-07-23","timestamp":1721645852000,"abstract":"  In Constrained Reinforcement Learning (CRL), agents explore the environment\nto learn the optimal policy while satisfying constraints. The penalty function\nmethod has recently been studied as an effective approach for handling\nconstraints, which imposes constraints penalties on the objective to transform\nthe constrained problem into an unconstrained one. However, it is challenging\nto choose appropriate penalties that balance policy performance and constraint\nsatisfaction efficiently. In this paper, we propose a theoretically guaranteed\npenalty function method, Exterior Penalty Policy Optimization (EPO), with\nadaptive penalties generated by a Penalty Metric Network (PMN). PMN responds\nappropriately to varying degrees of constraint violations, enabling efficient\nconstraint satisfaction and safe exploration. We theoretically prove that EPO\nconsistently improves constraint satisfaction with a convergence guarantee. We\npropose a new surrogate function and provide worst-case constraint violation\nand approximation error. In practice, we propose an effective smooth penalty\nfunction, which can be easily implemented with a first-order optimizer.\nExtensive experiments are conducted, showing that EPO outperforms the baselines\nin terms of policy performance and constraint satisfaction with a stable\ntraining process, particularly on complex tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}