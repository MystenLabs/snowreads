{"id":"2407.03386","title":"Visual Robustness Benchmark for Visual Question Answering (VQA)","authors":"Md Farhan Ishmam, Ishmam Tashdeed, Talukder Asir Saadat, Md Hamjajul\n  Ashmafee, Abu Raihan Mostofa Kamal, Md. Azam Hossain","authorsParsed":[["Ishmam","Md Farhan",""],["Tashdeed","Ishmam",""],["Saadat","Talukder Asir",""],["Ashmafee","Md Hamjajul",""],["Kamal","Abu Raihan Mostofa",""],["Hossain","Md. Azam",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 08:35:03 GMT"},{"version":"v2","created":"Fri, 13 Sep 2024 17:57:20 GMT"}],"updateDate":"2024-09-17","timestamp":1719995703000,"abstract":"  Can Visual Question Answering (VQA) systems perform just as well when\ndeployed in the real world? Or are they susceptible to realistic corruption\neffects e.g. image blur, which can be detrimental in sensitive applications,\nsuch as medical VQA? While linguistic or textual robustness has been thoroughly\nexplored in the VQA literature, there has yet to be any significant work on the\nvisual robustness of VQA models. We propose the first large-scale benchmark\ncomprising 213,000 augmented images, challenging the visual robustness of\nmultiple VQA models and assessing the strength of realistic visual corruptions.\nAdditionally, we have designed several robustness evaluation metrics that can\nbe aggregated into a unified metric and tailored to fit a variety of use cases.\nOur experiments reveal several insights into the relationships between model\nsize, performance, and robustness with the visual corruptions. Our benchmark\nhighlights the need for a balanced approach in model development that considers\nmodel performance without compromising the robustness.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FYHJGJsYFlwsZYJ2HHWQ2sL9hxLcCWIwK6nJg7sl20E","pdfSize":"9524345"}
