{"id":"2407.11353","title":"Preconditioned Gradient Descent Finds Over-Parameterized Neural Networks\n  with Sharp Generalization for Nonparametric Regression","authors":"Yingzhen Yang","authorsParsed":[["Yang","Yingzhen",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 03:38:34 GMT"}],"updateDate":"2024-07-17","timestamp":1721101114000,"abstract":"  We consider nonparametric regression by an over-parameterized two-layer\nneural network trained by gradient descent (GD) or its variant in this paper.\nWe show that, if the neural network is trained with a novel Preconditioned\nGradient Descent (PGD) with early stopping and the target function has spectral\nbias widely studied in the deep learning literature, the trained network\nrenders a particularly sharp generalization bound with a minimax optimal rate\nof $\\cO({1}/{n^{4\\alpha/(4\\alpha+1)}})$, which is sharper the current standard\nrate of $\\cO({1}/{n^{2\\alpha/(2\\alpha+1)}})$ with $2\\alpha = d/(d-1)$ when the\ndata is distributed uniformly on the unit sphere in $\\RR^d$ and $n$ is the size\nof the training data. When the target function has no spectral bias, we prove\nthat neural network trained with regular GD with early stopping still enjoys\nminimax optimal rate, and in this case our results do not require\ndistributional assumptions in contrast with the current known results. Our\nresults are built upon two significant technical contributions. First, uniform\nconvergence to the NTK is established during the training process by PGD or GD,\nso that we can have a nice decomposition of the neural network function at any\nstep of GD or PGD into a function in the RKHS and an error function with a\nsmall $L^{\\infty}$-norm. Second, local Rademacher complexity is employed to\ntightly bound the Rademacher complexity of the function class comprising all\nthe possible neural network functions obtained by GD or PGD. Our results also\nindicate that PGD can be another way of avoiding the usual linear regime of NTK\nand obtaining sharper generalization bound, because PGD induces a different\nkernel with lower kernel complexity during the training than the regular NTK\ninduced by the network architecture trained by regular GD.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}