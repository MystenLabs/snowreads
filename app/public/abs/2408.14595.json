{"id":"2408.14595","title":"Surprisingly Fragile: Assessing and Addressing Prompt Instability in\n  Multimodal Foundation Models","authors":"Ian Stewart, Sameera Horawalavithana, Brendan Kennedy, Sai Munikoti,\n  Karl Pazdernik","authorsParsed":[["Stewart","Ian",""],["Horawalavithana","Sameera",""],["Kennedy","Brendan",""],["Munikoti","Sai",""],["Pazdernik","Karl",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 19:26:55 GMT"}],"updateDate":"2024-08-28","timestamp":1724700415000,"abstract":"  Multimodal foundation models (MFMs) such as OFASys show the potential to\nunlock analysis of complex data such as images, videos, and audio data via text\nprompts alone. However, their performance may suffer in the face of text input\nthat differs even slightly from their training distribution, which is\nsurprising considering the use of modality-specific data to \"ground\" the text\ninput. This study demonstrates that prompt instability is a major concern for\nMFMs, leading to a consistent drop in performance across all modalities, but\nthat instability can be mitigated with additional training with augmented data.\nWe evaluate several methods for grounded prompt perturbation, where we generate\nperturbations and filter based on similarity to text and/or modality data.\nAfter re-training the models on the augmented data, we find improved accuracy\nand more stable performance on the perturbed test data regardless of\nperturbation condition, suggesting that the data augmentation strategy helps\nthe models handle domain shifts more effectively. In error analysis, we find\nconsistent patterns of performance improvement across domains, suggesting that\nretraining on prompt perturbations tends to help general reasoning capabilities\nin MFMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"lNRIjQOpxWFp1gWBW63aDPL3qPntt2UiqGzdy-tkwpI","pdfSize":"896642"}
