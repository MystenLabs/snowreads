{"id":"2408.09632","title":"MoDeGPT: Modular Decomposition for Large Language Model Compression","authors":"Chi-Heng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel,\n  Shikhar Tuli, Yilin Shen, Hongxia Jin, Yen-Chang Hsu","authorsParsed":[["Lin","Chi-Heng",""],["Gao","Shangqian",""],["Smith","James Seale",""],["Patel","Abhishek",""],["Tuli","Shikhar",""],["Shen","Yilin",""],["Jin","Hongxia",""],["Hsu","Yen-Chang",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 01:30:14 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 05:28:27 GMT"},{"version":"v3","created":"Fri, 13 Sep 2024 05:34:14 GMT"}],"updateDate":"2024-09-16","timestamp":1724031014000,"abstract":"  Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}