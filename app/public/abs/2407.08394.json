{"id":"2407.08394","title":"Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers","authors":"Zhengbo Zhang, Li Xu, Duo Peng, Hossein Rahmani, Jun Liu","authorsParsed":[["Zhang","Zhengbo",""],["Xu","Li",""],["Peng","Duo",""],["Rahmani","Hossein",""],["Liu","Jun",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 10:57:33 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 07:34:53 GMT"}],"updateDate":"2024-07-17","timestamp":1720695453000,"abstract":"  We introduce Diff-Tracker, a novel approach for the challenging unsupervised\nvisual tracking task leveraging the pre-trained text-to-image diffusion model.\nOur main idea is to leverage the rich knowledge encapsulated within the\npre-trained diffusion model, such as the understanding of image semantics and\nstructural information, to address unsupervised visual tracking. To this end,\nwe design an initial prompt learner to enable the diffusion model to recognize\nthe tracking target by learning a prompt representing the target. Furthermore,\nto facilitate dynamic adaptation of the prompt to the target's movements, we\npropose an online prompt updater. Extensive experiments on five benchmark\ndatasets demonstrate the effectiveness of our proposed method, which also\nachieves state-of-the-art performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}