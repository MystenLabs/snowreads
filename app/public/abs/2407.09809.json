{"id":"2407.09809","title":"Preserving the Privacy of Reward Functions in MDPs through Deception","authors":"Shashank Reddy Chirra, Pradeep Varakantham and Praveen Paruchuri","authorsParsed":[["Chirra","Shashank Reddy",""],["Varakantham","Pradeep",""],["Paruchuri","Praveen",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 09:03:22 GMT"}],"updateDate":"2024-07-16","timestamp":1720861402000,"abstract":"  Preserving the privacy of preferences (or rewards) of a sequential\ndecision-making agent when decisions are observable is crucial in many physical\nand cybersecurity domains. For instance, in wildlife monitoring, agents must\nallocate patrolling resources without revealing animal locations to poachers.\nThis paper addresses privacy preservation in planning over a sequence of\nactions in MDPs, where the reward function represents the preference structure\nto be protected. Observers can use Inverse RL (IRL) to learn these preferences,\nmaking this a challenging task.\n  Current research on differential privacy in reward functions fails to ensure\nguarantee on the minimum expected reward and offers theoretical guarantees that\nare inadequate against IRL-based observers. To bridge this gap, we propose a\nnovel approach rooted in the theory of deception. Deception includes two\nmodels: dissimulation (hiding the truth) and simulation (showing the wrong).\nOur first contribution theoretically demonstrates significant privacy leaks in\nexisting dissimulation-based methods. Our second contribution is a novel\nRL-based planning algorithm that uses simulation to effectively address these\nprivacy concerns while ensuring a guarantee on the expected reward. Experiments\non multiple benchmark problems show that our approach outperforms previous\nmethods in preserving reward function privacy.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0j6zOKiR_V7EO0wsa7jDygNqz8xnqpS4H2U8zumsiLY","pdfSize":"8239574"}
