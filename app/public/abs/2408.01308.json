{"id":"2408.01308","title":"Reconsidering Token Embeddings with the Definitions for Pre-trained\n  Language Models","authors":"Ying Zhang, Dongyuan Li, and Manabu Okumura","authorsParsed":[["Zhang","Ying",""],["Li","Dongyuan",""],["Okumura","Manabu",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 15:00:05 GMT"}],"updateDate":"2024-08-05","timestamp":1722610805000,"abstract":"  Learning token embeddings based on token co-occurrence statistics has proven\neffective for both pre-training and fine-tuning in natural language processing.\nHowever, recent studies have pointed out the distribution of learned embeddings\ndegenerates into anisotropy, and even pre-trained language models (PLMs) suffer\nfrom a loss of semantics-related information in embeddings for low-frequency\ntokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,\nand demonstrates its robustness against degeneration. On the basis of this\nfinding, we propose DefinitionEMB, a method that utilizes definitions to\nconstruct isotropically distributed and semantics-related token embeddings for\nPLMs while maintaining original robustness during fine-tuning. Our experiments\ndemonstrate the effectiveness of leveraging definitions from Wiktionary to\nconstruct such embeddings for RoBERTa-base and BART-large. Furthermore, the\nconstructed embeddings for low-frequency tokens improve the performance of\nthese models across various GLUE and four text summarization datasets.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}