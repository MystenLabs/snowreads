{"id":"2407.07523","title":"SHERL: Synthesizing High Accuracy and Efficient Memory for\n  Resource-Limited Transfer Learning","authors":"Haiwen Diao, Bo Wan, Xu Jia, Yunzhi Zhuge, Ying Zhang, Huchuan Lu,\n  Long Chen","authorsParsed":[["Diao","Haiwen",""],["Wan","Bo",""],["Jia","Xu",""],["Zhuge","Yunzhi",""],["Zhang","Ying",""],["Lu","Huchuan",""],["Chen","Long",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 10:22:35 GMT"}],"updateDate":"2024-07-11","timestamp":1720606955000,"abstract":"  Parameter-efficient transfer learning (PETL) has emerged as a flourishing\nresearch field for adapting large pre-trained models to downstream tasks,\ngreatly reducing trainable parameters while grappling with memory challenges\nduring fine-tuning. To address it, memory-efficient series (METL) avoid\nbackpropagating gradients through the large backbone. However, they compromise\nby exclusively relying on frozen intermediate outputs and limiting the\nexhaustive exploration of prior knowledge from pre-trained models. Moreover,\nthe dependency and redundancy between cross-layer features are frequently\noverlooked, thereby submerging more discriminative representations and causing\nan inherent performance gap (vs. conventional PETL methods). Hence, we propose\nan innovative METL strategy called SHERL for resource-limited scenarios to\ndecouple the entire adaptation into two successive and complementary processes.\nIn the early route, intermediate outputs are consolidated via an\nanti-redundancy operation, enhancing their compatibility for subsequent\ninteractions; thereby in the late route, utilizing minimal late pre-trained\nlayers could alleviate the peak demand on memory overhead and regulate these\nfairly flexible features into more adaptive and powerful representations for\nnew domains. Extensive ablations on vision-and-language and language-only tasks\nshow that SHERL combines the strengths of both parameter and memory-efficient\ntechniques, performing on-par or better across diverse architectures with lower\nmemory during fine-tuning. Our code is publicly available at:\nhttps://github.com/Paranioar/SHERL.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}