{"id":"2408.10718","title":"CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?","authors":"Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan\n  Li, Jing Ma","authorsParsed":[["Zhao","Yuwei",""],["Luo","Ziyang",""],["Tian","Yuchen",""],["Lin","Hongzhan",""],["Yan","Weixiang",""],["Li","Annan",""],["Ma","Jing",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 10:40:35 GMT"},{"version":"v2","created":"Fri, 13 Sep 2024 08:09:50 GMT"}],"updateDate":"2024-09-16","timestamp":1724150435000,"abstract":"  Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our codes and benchmark are available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}