{"id":"2408.14585","title":"Global-Local Distillation Network-Based Audio-Visual Speaker Tracking\n  with Incomplete Modalities","authors":"Yidi Li, Yihan Li, Yixin Guo, Bin Ren, Zhenhuan Xu, Hao Guo, Hong Liu,\n  Nicu Sebe","authorsParsed":[["Li","Yidi",""],["Li","Yihan",""],["Guo","Yixin",""],["Ren","Bin",""],["Xu","Zhenhuan",""],["Guo","Hao",""],["Liu","Hong",""],["Sebe","Nicu",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 19:09:21 GMT"}],"updateDate":"2024-08-28","timestamp":1724699361000,"abstract":"  In speaker tracking research, integrating and complementing multi-modal data\nis a crucial strategy for improving the accuracy and robustness of tracking\nsystems. However, tracking with incomplete modalities remains a challenging\nissue due to noisy observations caused by occlusion, acoustic noise, and sensor\nfailures. Especially when there is missing data in multiple modalities, the\nperformance of existing multi-modal fusion methods tends to decrease. To this\nend, we propose a Global-Local Distillation-based Tracker (GLDTracker) for\nrobust audio-visual speaker tracking. GLDTracker is driven by a teacher-student\ndistillation model, enabling the flexible fusion of incomplete information from\neach modality. The teacher network processes global signals captured by camera\nand microphone arrays, and the student network handles local information\nsubject to visual occlusion and missing audio channels. By transferring\nknowledge from teacher to student, the student network can better adapt to\ncomplex dynamic scenes with incomplete observations. In the student network, a\nglobal feature reconstruction module based on the generative adversarial\nnetwork is constructed to reconstruct global features from feature embedding\nwith missing local information. Furthermore, a multi-modal multi-level fusion\nattention is introduced to integrate the incomplete feature and the\nreconstructed feature, leveraging the complementarity and consistency of\naudio-visual and global-local features. Experimental results on the AV16.3\ndataset demonstrate that the proposed GLDTracker outperforms existing\nstate-of-the-art audio-visual trackers and achieves leading performance on both\nstandard and incomplete modalities datasets, highlighting its superiority and\nrobustness in complex conditions. The code and models will be available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}