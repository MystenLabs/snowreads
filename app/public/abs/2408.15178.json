{"id":"2408.15178","title":"A Review of Transformer-Based Models for Computer Vision Tasks:\n  Capturing Global Context and Spatial Relationships","authors":"Gracile Astlin Pereira and Muhammad Hussain","authorsParsed":[["Pereira","Gracile Astlin",""],["Hussain","Muhammad",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 16:22:18 GMT"}],"updateDate":"2024-08-28","timestamp":1724775738000,"abstract":"  Transformer-based models have transformed the landscape of natural language\nprocessing (NLP) and are increasingly applied to computer vision tasks with\nremarkable success. These models, renowned for their ability to capture\nlong-range dependencies and contextual information, offer a promising\nalternative to traditional convolutional neural networks (CNNs) in computer\nvision. In this review paper, we provide an extensive overview of various\ntransformer architectures adapted for computer vision tasks. We delve into how\nthese models capture global context and spatial relationships in images,\nempowering them to excel in tasks such as image classification, object\ndetection, and segmentation. Analyzing the key components, training\nmethodologies, and performance metrics of transformer-based models, we\nhighlight their strengths, limitations, and recent advancements. Additionally,\nwe discuss potential research directions and applications of transformer-based\nmodels in computer vision, offering insights into their implications for future\nadvancements in the field.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}