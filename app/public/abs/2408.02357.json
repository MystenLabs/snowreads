{"id":"2408.02357","title":"On the consistent reasoning paradox of intelligence and optimal trust in\n  AI: The power of 'I don't know'","authors":"Alexander Bastounis, Paolo Campodonico, Mihaela van der Schaar, Ben\n  Adcock and Anders C. Hansen","authorsParsed":[["Bastounis","Alexander",""],["Campodonico","Paolo",""],["van der Schaar","Mihaela",""],["Adcock","Ben",""],["Hansen","Anders C.",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 10:06:53 GMT"}],"updateDate":"2024-08-06","timestamp":1722852413000,"abstract":"  We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning,\nwhich lies at the core of human intelligence, is the ability to handle tasks\nthat are equivalent, yet described by different sentences ('Tell me the time!'\nand 'What is the time?'). The CRP asserts that consistent reasoning implies\nfallibility -- in particular, human-like intelligence in AI necessarily comes\nwith human-like fallibility. Specifically, it states that there are problems,\ne.g. in basic arithmetic, where any AI that always answers and strives to mimic\nhuman intelligence by reasoning consistently will hallucinate (produce wrong,\nyet plausible answers) infinitely often. The paradox is that there exists a\nnon-consistently reasoning AI (which therefore cannot be on the level of human\nintelligence) that will be correct on the same set of problems. The CRP also\nshows that detecting these hallucinations, even in a probabilistic sense, is\nstrictly harder than solving the original problems, and that there are problems\nthat an AI may answer correctly, but it cannot provide a correct logical\nexplanation for how it arrived at the answer. Therefore, the CRP implies that\nany trustworthy AI (i.e., an AI that never answers incorrectly) that also\nreasons consistently must be able to say 'I don't know'. Moreover, this can\nonly be done by implicitly computing a new concept that we introduce, termed\nthe 'I don't know' function -- something currently lacking in modern AI. In\nview of these insights, the CRP also provides a glimpse into the behaviour of\nArtificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can\nit always explain itself, and therefore to be trustworthy it must be able to\nsay 'I don't know'.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Mathematics/Probability"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}