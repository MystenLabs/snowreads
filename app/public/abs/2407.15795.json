{"id":"2407.15795","title":"AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot\n  Anomaly Detection","authors":"Yunkang Cao, Jiangning Zhang, Luca Frittoli, Yuqi Cheng, Weiming Shen,\n  Giacomo Boracchi","authorsParsed":[["Cao","Yunkang",""],["Zhang","Jiangning",""],["Frittoli","Luca",""],["Cheng","Yuqi",""],["Shen","Weiming",""],["Boracchi","Giacomo",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 16:52:37 GMT"}],"updateDate":"2024-07-23","timestamp":1721667157000,"abstract":"  Zero-shot anomaly detection (ZSAD) targets the identification of anomalies\nwithin images from arbitrary novel categories. This study introduces AdaCLIP\nfor the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP.\nAdaCLIP incorporates learnable prompts into CLIP and optimizes them through\ntraining on auxiliary annotated anomaly detection data. Two types of learnable\nprompts are proposed: static and dynamic. Static prompts are shared across all\nimages, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic\nprompts are generated for each test image, providing CLIP with dynamic\nadaptation capabilities. The combination of static and dynamic prompts is\nreferred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive\nexperiments conducted across 14 real-world anomaly detection datasets from\nindustrial and medical domains indicate that AdaCLIP outperforms other ZSAD\nmethods and can generalize better to different categories and even domains.\nFinally, our analysis highlights the importance of diverse auxiliary data and\noptimized prompts for enhanced generalization capacity. Code is available at\nhttps://github.com/caoyunkang/AdaCLIP.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}