{"id":"2407.04797","title":"Revealing the Utilized Rank of Subspaces of Learning in Neural Networks","authors":"Isha Garg, Christian Koguchi, Eshan Verma, Daniel Ulbricht","authorsParsed":[["Garg","Isha",""],["Koguchi","Christian",""],["Verma","Eshan",""],["Ulbricht","Daniel",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 18:14:39 GMT"}],"updateDate":"2024-07-09","timestamp":1720203279000,"abstract":"  In this work, we study how well the learned weights of a neural network\nutilize the space available to them. This notion is related to capacity, but\nadditionally incorporates the interaction of the network architecture with the\ndataset. Most learned weights appear to be full rank, and are therefore not\namenable to low rank decomposition. This deceptively implies that the weights\nare utilizing the entire space available to them. We propose a simple\ndata-driven transformation that projects the weights onto the subspace where\nthe data and the weight interact. This preserves the functional mapping of the\nlayer and reveals its low rank structure. In our findings, we conclude that\nmost models utilize a fraction of the available space. For instance, for\nViTB-16 and ViTL-16 trained on ImageNet, the mean layer utilization is 35% and\n20% respectively. Our transformation results in reducing the parameters to 50%\nand 25% respectively, while resulting in less than 0.2% accuracy drop after\nfine-tuning. We also show that self-supervised pre-training drives this\nutilization up to 70%, justifying its suitability for downstream tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}