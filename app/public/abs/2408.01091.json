{"id":"2408.01091","title":"Dissecting Dissonance: Benchmarking Large Multimodal Models Against\n  Self-Contradictory Instructions","authors":"Jin Gao, Lei Gan, Yuankai Li, Yixin Ye, Dequan Wang","authorsParsed":[["Gao","Jin",""],["Gan","Lei",""],["Li","Yuankai",""],["Ye","Yixin",""],["Wang","Dequan",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 08:11:11 GMT"},{"version":"v2","created":"Mon, 5 Aug 2024 06:56:44 GMT"}],"updateDate":"2024-08-06","timestamp":1722586271000,"abstract":"  Large multimodal models (LMMs) excel in adhering to human instructions.\nHowever, self-contradictory instructions may arise due to the increasing trend\nof multimodal interaction and context length, which is challenging for language\nbeginners and vulnerable populations. We introduce the Self-Contradictory\nInstructions benchmark to evaluate the capability of LMMs in recognizing\nconflicting commands. It comprises 20,000 conflicts, evenly distributed between\nlanguage and vision paradigms. It is constructed by a novel automatic dataset\ncreation framework, which expedites the process and enables us to encompass a\nwide range of instruction forms. Our comprehensive evaluation reveals current\nLMMs consistently struggle to identify multimodal instruction discordance due\nto a lack of self-awareness. Hence, we propose the Cognitive Awakening\nPrompting to inject cognition from external, largely enhancing dissonance\ndetection. The dataset and code are here: https://selfcontradiction.github.io/.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}