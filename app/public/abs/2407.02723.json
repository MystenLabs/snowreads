{"id":"2407.02723","title":"e-Health CSIRO at \"Discharge Me!\" 2024: Generating Discharge Summary\n  Sections with Fine-tuned Language Models","authors":"Jinghui Liu, Aaron Nicolson, Jason Dowling, Bevan Koopman, Anthony\n  Nguyen","authorsParsed":[["Liu","Jinghui",""],["Nicolson","Aaron",""],["Dowling","Jason",""],["Koopman","Bevan",""],["Nguyen","Anthony",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 00:32:28 GMT"}],"updateDate":"2024-07-04","timestamp":1719966748000,"abstract":"  Clinical documentation is an important aspect of clinicians' daily work and\noften demands a significant amount of time. The BioNLP 2024 Shared Task on\nStreamlining Discharge Documentation (Discharge Me!) aims to alleviate this\ndocumentation burden by automatically generating discharge summary sections,\nincluding brief hospital course and discharge instruction, which are often\ntime-consuming to synthesize and write manually. We approach the generation\ntask by fine-tuning multiple open-sourced language models (LMs), including both\ndecoder-only and encoder-decoder LMs, with various configurations on input\ncontext. We also examine different setups for decoding algorithms, model\nensembling or merging, and model specialization. Our results show that\nconditioning on the content of discharge summary prior to the target sections\nis effective for the generation task. Furthermore, we find that smaller\nencoder-decoder LMs can work as well or even slightly better than larger\ndecoder based LMs fine-tuned through LoRA. The model checkpoints from our team\n(aehrc) are openly available.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ddvJ2WUyCeIBcdgwzKfVelgL1q-GTEBhJr9XI54NQ74","pdfSize":"543798","objectId":"0xbff5c89059a811ca6552b9af3f945cf3bb4bfdab6f3ae974012ac4e665df080d","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
