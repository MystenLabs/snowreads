{"id":"2408.04190","title":"Listwise Reward Estimation for Offline Preference-based Reinforcement\n  Learning","authors":"Heewoong Choi, Sangwon Jung, Hongjoon Ahn, Taesup Moon","authorsParsed":[["Choi","Heewoong",""],["Jung","Sangwon",""],["Ahn","Hongjoon",""],["Moon","Taesup",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 03:18:42 GMT"}],"updateDate":"2024-08-09","timestamp":1723087122000,"abstract":"  In Reinforcement Learning (RL), designing precise reward functions remains to\nbe a challenge, particularly when aligning with human intent. Preference-based\nRL (PbRL) was introduced to address this problem by learning reward models from\nhuman feedback. However, existing PbRL methods have limitations as they often\noverlook the second-order preference that indicates the relative strength of\npreference. In this paper, we propose Listwise Reward Estimation (LiRE), a\nnovel approach for offline PbRL that leverages second-order preference\ninformation by constructing a Ranked List of Trajectories (RLT), which can be\nefficiently built by using the same ternary feedback type as traditional\nmethods. To validate the effectiveness of LiRE, we propose a new offline PbRL\ndataset that objectively reflects the effect of the estimated rewards. Our\nextensive experiments on the dataset demonstrate the superiority of LiRE, i.e.,\noutperforming state-of-the-art baselines even with modest feedback budgets and\nenjoying robustness with respect to the number of feedbacks and feedback noise.\nOur code is available at https://github.com/chwoong/LiRE\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}