{"id":"2408.17041","title":"Generative Modeling Perspective for Control and Reasoning in Robotics","authors":"Takuma Yoneda","authorsParsed":[["Yoneda","Takuma",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 06:42:10 GMT"}],"updateDate":"2024-09-02","timestamp":1725000130000,"abstract":"  Heralded by the initial success in speech recognition and image\nclassification, learning-based approaches with neural networks, commonly\nreferred to as deep learning, have spread across various fields. A primitive\nform of a neural network functions as a deterministic mapping from one vector\nto another, parameterized by trainable weights. This is well suited for point\nestimation in which the model learns a one-to-one mapping (e.g., mapping a\nfront camera view to a steering angle) that is required to solve the task of\ninterest. Although learning such a deterministic, one-to-one mapping is\neffective, there are scenarios where modeling \\emph{multimodal} data\ndistributions, namely learning one-to-many relationships, is helpful or even\nnecessary.\n  In this thesis, we adopt a generative modeling perspective on robotics\nproblems. Generative models learn and produce samples from multimodal\ndistributions, rather than performing point estimation. We will explore the\nadvantages this perspective offers for three topics in robotics.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/"}