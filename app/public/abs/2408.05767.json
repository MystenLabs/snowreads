{"id":"2408.05767","title":"Reference-free Hallucination Detection for Large Vision-Language Models","authors":"Qing Li, Chenyang Lyu, Jiahui Geng, Derui Zhu, Maxim Panov, Fakhri\n  Karray","authorsParsed":[["Li","Qing",""],["Lyu","Chenyang",""],["Geng","Jiahui",""],["Zhu","Derui",""],["Panov","Maxim",""],["Karray","Fakhri",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 13:17:14 GMT"}],"updateDate":"2024-08-13","timestamp":1723382234000,"abstract":"  Large vision-language models (LVLMs) have made significant progress in recent\nyears. While LVLMs exhibit excellent ability in language understanding,\nquestion answering, and conversations of visual inputs, they are prone to\nproducing hallucinations. While several methods are proposed to evaluate the\nhallucinations in LVLMs, most are reference-based and depend on external tools,\nwhich complicates their practical application. To assess the viability of\nalternative methods, it is critical to understand whether the reference-free\napproaches, which do not rely on any external tools, can efficiently detect\nhallucinations. Therefore, we initiate an exploratory study to demonstrate the\neffectiveness of different reference-free solutions in detecting hallucinations\nin LVLMs. In particular, we conduct an extensive study on three kinds of\ntechniques: uncertainty-based, consistency-based, and supervised uncertainty\nquantification methods on four representative LVLMs across two different tasks.\nThe empirical results show that the reference-free approaches are capable of\neffectively detecting non-factual responses in LVLMs, with the supervised\nuncertainty quantification method outperforming the others, achieving the best\nperformance across different settings.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"F8Ax_w5PEkaSL2vYCZ1IZ9l28aYtCe7UjiBXozNcmMg","pdfSize":"6282174"}
