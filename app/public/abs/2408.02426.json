{"id":"2408.02426","title":"FPT+: A Parameter and Memory Efficient Transfer Learning Method for\n  High-resolution Medical Image Classification","authors":"Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang","authorsParsed":[["Huang","Yijin",""],["Cheng","Pujin",""],["Tam","Roger",""],["Tang","Xiaoying",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 12:33:07 GMT"}],"updateDate":"2024-08-06","timestamp":1722861187000,"abstract":"  The success of large-scale pre-trained models has established fine-tuning as\na standard method for achieving significant improvements in downstream tasks.\nHowever, fine-tuning the entire parameter set of a pre-trained model is costly.\nParameter-efficient transfer learning (PETL) has recently emerged as a\ncost-effective alternative for adapting pre-trained models to downstream tasks.\nDespite its advantages, the increasing model size and input resolution present\nchallenges for PETL, as the training memory consumption is not reduced as\neffectively as the parameter usage. In this paper, we introduce Fine-grained\nPrompt Tuning plus (FPT+), a PETL method designed for high-resolution medical\nimage classification, which significantly reduces memory consumption compared\nto other PETL methods. FPT+ performs transfer learning by training a\nlightweight side network and accessing pre-trained knowledge from a large\npre-trained model (LPM) through fine-grained prompts and fusion modules.\nSpecifically, we freeze the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM processes high-resolution images to extract\nfine-grained features, while the side network employs the corresponding\ndown-sampled low-resolution images to minimize the memory usage. To enable the\nside network to leverage pre-trained knowledge, we propose fine-grained prompts\nand fusion modules, which collaborate to summarize information through the\nLPM's intermediate activations. We evaluate FPT+ on eight medical image\ndatasets of varying sizes, modalities, and complexities. Experimental results\ndemonstrate that FPT+ outperforms other PETL methods, using only 1.03% of the\nlearnable parameters and 3.18% of the memory required for fine-tuning an entire\nViT-B model. Our code is available at https://github.com/YijinHuang/FPT.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AznLdYoMACkRN71b3ynz89CmjQjVfxaQ0Y_mzqXZfbw","pdfSize":"870086","txDigest":"8bqsoGPvreyYK5jjyhDaAdd3tzf5pughoeBM81scr9Sp","endEpoch":"1","status":"CERTIFIED"}
