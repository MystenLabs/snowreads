{"id":"2408.01331","title":"UnifiedNN: Efficient Neural Network Training on the Cloud","authors":"Sifat Ut Taki, Arthi Padmanabhan, Spyridon Mastorakis","authorsParsed":[["Taki","Sifat Ut",""],["Padmanabhan","Arthi",""],["Mastorakis","Spyridon",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 15:29:39 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 01:10:34 GMT"}],"updateDate":"2024-08-07","timestamp":1722612579000,"abstract":"  Nowadays, cloud-based services are widely favored over the traditional\napproach of locally training a Neural Network (NN) model. Oftentimes, a cloud\nservice processes multiple requests from users--thus training multiple NN\nmodels concurrently. However, training NN models concurrently is a challenging\nprocess, which typically requires significant amounts of available computing\nresources and takes a long time to complete. In this paper, we present\nUnifiedNN to effectively train multiple NN models concurrently on the cloud.\nUnifiedNN effectively \"combines\" multiple NN models and features several memory\nand time conservation mechanisms to train multiple NN models simultaneously\nwithout impacting the accuracy of the training process. Specifically, UnifiedNN\nmerges multiple NN models and creates a large singular unified model in order\nto efficiently train all models at once. We have implemented a prototype of\nUnifiedNN in PyTorch and we have compared its performance with relevant\nstate-of-the-art frameworks. Our experimental results demonstrate that\nUnifiedNN can reduce memory consumption by up to 53% and training time by up to\n81% when compared with vanilla PyTorch without impacting the model training and\ntesting accuracy. Finally, our results indicate that UnifiedNN can reduce\nmemory consumption by up to 52% and training time by up to 41% when compared to\nstate-of-the-art frameworks when training multiple models concurrently.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}