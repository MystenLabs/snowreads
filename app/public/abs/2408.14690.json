{"id":"2408.14690","title":"Training-Free Activation Sparsity in Large Language Models","authors":"James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, Ben\n  Athiwaratkun","authorsParsed":[["Liu","James",""],["Ponnusamy","Pragaash",""],["Cai","Tianle",""],["Guo","Han",""],["Kim","Yoon",""],["Athiwaratkun","Ben",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 23:30:15 GMT"}],"updateDate":"2024-08-28","timestamp":1724715015000,"abstract":"  Activation sparsity can enable practical inference speedups in large language\nmodels (LLMs) by reducing the compute and memory-movement required for matrix\nmultiplications during the forward pass. However, existing methods face\nlimitations that inhibit widespread adoption. Some approaches are tailored\ntowards older models with ReLU-based sparsity, while others require extensive\ncontinued pre-training on up to hundreds of billions of tokens. This paper\ndescribes TEAL, a simple training-free method that applies magnitude-based\nactivation sparsity to hidden states throughout the entire model. TEAL achieves\n40-50% model-wide sparsity with minimal performance degradation across Llama-2,\nLlama-3, and Mistral families, with sizes varying from 7B to 70B. We improve\nexisting sparse kernels and demonstrate wall-clock decoding speed-ups of up to\n1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is\ncompatible with weight quantization, enabling further efficiency gains.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}