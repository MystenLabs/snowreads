{"id":"2407.14371","title":"Open Artificial Knowledge","authors":"Vadim Borisov, Richard H. Schreiber","authorsParsed":[["Borisov","Vadim",""],["Schreiber","Richard H.",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 15:01:24 GMT"}],"updateDate":"2024-07-22","timestamp":1721401284000,"abstract":"  The tremendous success of chat-based AI systems like ChatGPT, Claude, and\nGemini stems from Large Language Models (LLMs) trained on vast amount of\ndatasets. However, acquiring high-quality, diverse, and ethically sourced\ntraining data remains a significant challenge. We introduce the Open Artificial\nKnowledge (OAK) dataset, a large-scale resource of over 500 million tokens (at\nthe moment of writing) designed to address this issue. OAK leverages an\nensemble of state-of-the-art LLMs, including GPT4o, LLaMa3-70B, LLaMa3-8B,\nMixtral-8x7B, Gemma-7B, and Gemma-2-9B , to generate high-quality text across\ndiverse domains, guided by Wikipedia's main categories. Our methodology ensures\nbroad knowledge coverage while maintaining coherence and factual accuracy. The\nOAK dataset aims to foster the development of more capable and aligned language\nmodels while addressing critical issues of data scarcity and privacy in LLM\ntraining, and it is freely available on www.oakdataset.org.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1hoMZ-0MwsL9822QD6s3UHJvefHoxGS6Y0EvkScI80w","pdfSize":"2995861","objectId":"0x718c58b567dc04e11bfd2609903badd0d05eb5c9095e66e7c9c5a718dbcbe001","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
