{"id":"2408.09438","title":"Enhancing Modal Fusion by Alignment and Label Matching for Multimodal\n  Emotion Recognition","authors":"Qifei Li, Yingming Gao, Yuhua Wen, Cong Wang, Ya Li","authorsParsed":[["Li","Qifei",""],["Gao","Yingming",""],["Wen","Yuhua",""],["Wang","Cong",""],["Li","Ya",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 11:05:21 GMT"}],"updateDate":"2024-08-20","timestamp":1723979121000,"abstract":"  To address the limitation in multimodal emotion recognition (MER) performance\narising from inter-modal information fusion, we propose a novel MER framework\nbased on multitask learning where fusion occurs after alignment, called\nFoal-Net. The framework is designed to enhance the effectiveness of modality\nfusion and includes two auxiliary tasks: audio-video emotion alignment (AVEL)\nand cross-modal emotion label matching (MEM). First, AVEL achieves alignment of\nemotional information in audio-video representations through contrastive\nlearning. Then, a modal fusion network integrates the aligned features.\nMeanwhile, MEM assesses whether the emotions of the current sample pair are the\nsame, providing assistance for modal information fusion and guiding the model\nto focus more on emotional information. The experimental results conducted on\nIEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and\nemotion alignment is necessary before modal fusion.\n","subjects":["Computing Research Repository/Multimedia","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}