{"id":"2407.01402","title":"Superconstant Inapproximability of Decision Tree Learning","authors":"Caleb Koch and Carmen Strassle and Li-Yang Tan","authorsParsed":[["Koch","Caleb",""],["Strassle","Carmen",""],["Tan","Li-Yang",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 15:53:03 GMT"}],"updateDate":"2024-07-02","timestamp":1719849183000,"abstract":"  We consider the task of properly PAC learning decision trees with queries.\nRecent work of Koch, Strassle, and Tan showed that the strictest version of\nthis task, where the hypothesis tree $T$ is required to be optimally small, is\nNP-hard. Their work leaves open the question of whether the task remains\nintractable if $T$ is only required to be close to optimal, say within a factor\nof 2, rather than exactly optimal.\n  We answer this affirmatively and show that the task indeed remains NP-hard\neven if $T$ is allowed to be within any constant factor of optimal. More\ngenerally, our result allows for a smooth tradeoff between the hardness\nassumption and the inapproximability factor. As Koch et al.'s techniques do not\nappear to be amenable to such a strengthening, we first recover their result\nwith a new and simpler proof, which we couple with a new XOR lemma for decision\ntrees. While there is a large body of work on XOR lemmas for decision trees,\nour setting necessitates parameters that are extremely sharp, and are not known\nto be attainable by existing XOR lemmas. Our work also carries new implications\nfor the related problem of Decision Tree Minimization.\n","subjects":["Computing Research Repository/Computational Complexity","Computing Research Repository/Data Structures and Algorithms","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"S2Rb4KwuksFqVkxxeXGF4NVKs-wIIcDzrpITg8BEJ9I","pdfSize":"364143"}
