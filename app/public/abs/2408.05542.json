{"id":"2408.05542","title":"You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic\n  Code Search","authors":"Yanlin Wang, Lianghong Guo, Ensheng Shi, Wenqing Chen, Jiachi Chen,\n  Wanjun Zhong, Menghan Wang, Hui Li, Hongyu Zhang, Ziyu Lyu, Zibin Zheng","authorsParsed":[["Wang","Yanlin",""],["Guo","Lianghong",""],["Shi","Ensheng",""],["Chen","Wenqing",""],["Chen","Jiachi",""],["Zhong","Wanjun",""],["Wang","Menghan",""],["Li","Hui",""],["Zhang","Hongyu",""],["Lyu","Ziyu",""],["Zheng","Zibin",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 12:51:21 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 11:18:16 GMT"}],"updateDate":"2024-08-20","timestamp":1723294281000,"abstract":"  Code search plays a crucial role in software development, enabling developers\nto retrieve and reuse code using natural language queries. While the\nperformance of code search models improves with an increase in high-quality\ndata, obtaining such data can be challenging and expensive. Recently, large\nlanguage models (LLMs) such as ChatGPT have made remarkable progress in both\nnatural and programming language understanding and generation, offering\nuser-friendly interaction via simple prompts. Inspired by these advancements,\nwe propose a novel approach ChatDANCE, which utilizes high-quality and diverse\naugmented data generated by a large language model and leverages a filtering\nmechanism to eliminate low-quality augmentations. Specifically, we first\npropose a set of ChatGPT prompting rules that are specifically designed for\nsource code and queries. Then, we leverage ChatGPT to rewrite code and queries\nbased on the according prompts and then propose a filtering mechanism which\ntrains a cross-encoder from the backbone model UniXcoder to filter out code and\nquery pairs with low matching scores. Finally, we re-train the backbone model\nusing the obtained high-quality augmented data. Experimental results show that\nChatDANCE achieves state-of-the-art performance, improving the best baseline by\n13.2% (R@1) and 7% (MRR). Surprisingly, we find that this\naugment-filter-retrain strategy enables the backbone model (UniXcoder) to\nself-grow. Moreover, extensive experiments show the effectiveness of each\ncomponent and ChatDANCE has stable performance under different hyperparameter\nsettings. In addition, we conduct qualitative and quantitative analyses to\ninvestigate why ChatDANCE works well and find that it learns a more uniform\ndistribution of representations and effectively aligns the code and query\nspaces.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}