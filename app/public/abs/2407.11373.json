{"id":"2407.11373","title":"Reliable Reasoning Beyond Natural Language","authors":"Nasim Borazjanizadeh, Steven T. Piantadosi","authorsParsed":[["Borazjanizadeh","Nasim",""],["Piantadosi","Steven T.",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 04:34:18 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 18:54:02 GMT"}],"updateDate":"2024-07-23","timestamp":1721104458000,"abstract":"  Despite their linguistic competence, Large Language models (LLMs) often\nexhibit limitations in their ability to reason reliably and flexibly. To\naddress this, we propose a neurosymbolic approach that prompts LLMs to extract\nand encode all relevant information from a problem statement as logical code\nstatements, and then use a logic programming language (Prolog) to conduct the\niterative computations of explicit deductive reasoning. Our approach\nsignificantly enhances the performance of LLMs on the standard mathematical\nreasoning benchmark, GSM8k, and the Navigate dataset from the BIG-bench\ndataset. Additionally, we introduce a novel dataset, the Non-Linear Reasoning\n(NLR) dataset, consisting of 55 unique word problems that target the\nshortcomings of the next token prediction paradigm of LLMs and require complex\nnon-linear reasoning but only basic arithmetic skills to solve. Our findings\ndemonstrate that the integration of Prolog enables LLMs to achieve high\nperformance on the NLR dataset, which even the most advanced language models\n(including GPT4) fail to solve using text only.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}