{"id":"2408.09757","title":"Strategic Demonstration Selection for Improved Fairness in LLM\n  In-Context Learning","authors":"Jingyu Hu, Weiru Liu, Mengnan Du","authorsParsed":[["Hu","Jingyu",""],["Liu","Weiru",""],["Du","Mengnan",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 07:34:43 GMT"}],"updateDate":"2024-08-20","timestamp":1724052883000,"abstract":"  Recent studies highlight the effectiveness of using in-context learning (ICL)\nto steer large language models (LLMs) in processing tabular data, a challenging\ntask given the structured nature of such data. Despite advancements in\nperformance, the fairness implications of these methods are less understood.\nThis study investigates how varying demonstrations within ICL prompts influence\nthe fairness outcomes of LLMs. Our findings reveal that deliberately including\nminority group samples in prompts significantly boosts fairness without\nsacrificing predictive accuracy. Further experiments demonstrate that the\nproportion of minority to majority samples in demonstrations affects the\ntrade-off between fairness and prediction accuracy. Based on these insights, we\nintroduce a mitigation technique that employs clustering and evolutionary\nstrategies to curate a diverse and representative sample set from the training\ndata. This approach aims to enhance both predictive performance and fairness in\nICL applications. Experimental results validate that our proposed method\ndramatically improves fairness across various metrics, showing its efficacy in\nreal-world scenarios.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"P-_7cCQz8TMk1I5GwgEhVuVhTRSlLtvdn3la-M5Q-Hg","pdfSize":"6559304"}
