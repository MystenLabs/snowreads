{"id":"2408.04619","title":"Transformer Explainer: Interactive Learning of Text-Generative Models","authors":"Aeree Cho, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J.\n  Wang, Seongmin Lee, Benjamin Hoover, Duen Horng Chau","authorsParsed":[["Cho","Aeree",""],["Kim","Grace C.",""],["Karpekov","Alexander",""],["Helbling","Alec",""],["Wang","Zijie J.",""],["Lee","Seongmin",""],["Hoover","Benjamin",""],["Chau","Duen Horng",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 17:49:07 GMT"}],"updateDate":"2024-08-09","timestamp":1723139347000,"abstract":"  Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}