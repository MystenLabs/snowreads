{"id":"2408.11832","title":"OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs","authors":"Hasan Iqbal, Yuxia Wang, Minghan Wang, Georgi Georgiev, Jiahui Geng,\n  Iryna Gurevych, Preslav Nakov","authorsParsed":[["Iqbal","Hasan",""],["Wang","Yuxia",""],["Wang","Minghan",""],["Georgiev","Georgi",""],["Geng","Jiahui",""],["Gurevych","Iryna",""],["Nakov","Preslav",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 15:49:58 GMT"}],"updateDate":"2024-08-23","timestamp":1722959398000,"abstract":"  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/hasaniqbal777/openfactcheck) and publicly released as a\nPython library (https://pypi.org/project/openfactcheck/) and also as a web\nservice (https://huggingface.co/spaces/hasaniqbal777/OpenFactCheck). A video\ndescribing the system is available at https://youtu.be/-i9VKL0HleI.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"BHm25iZtFnR6iOrbq6mQpPlSDa0g8qgRk4CwPADWEWE","pdfSize":"2024775"}
