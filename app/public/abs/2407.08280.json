{"id":"2407.08280","title":"WayveScenes101: A Dataset and Benchmark for Novel View Synthesis in\n  Autonomous Driving","authors":"Jannik Z\\\"urn, Paul Gladkov, Sof\\'ia Dudas, Fergal Cotter, Sofi\n  Toteva, Jamie Shotton, Vasiliki Simaiaki, Nikhil Mohan","authorsParsed":[["Zürn","Jannik",""],["Gladkov","Paul",""],["Dudas","Sofía",""],["Cotter","Fergal",""],["Toteva","Sofi",""],["Shotton","Jamie",""],["Simaiaki","Vasiliki",""],["Mohan","Nikhil",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 08:29:45 GMT"}],"updateDate":"2024-07-12","timestamp":1720686585000,"abstract":"  We present WayveScenes101, a dataset designed to help the community advance\nthe state of the art in novel view synthesis that focuses on challenging\ndriving scenes containing many dynamic and deformable elements with changing\ngeometry and texture. The dataset comprises 101 driving scenes across a wide\nrange of environmental conditions and driving scenarios. The dataset is\ndesigned for benchmarking reconstructions on in-the-wild driving scenes, with\nmany inherent challenges for scene reconstruction methods including image\nglare, rapid exposure changes, and highly dynamic scenes with significant\nocclusion. Along with the raw images, we include COLMAP-derived camera poses in\nstandard data formats. We propose an evaluation protocol for evaluating models\non held-out camera views that are off-axis from the training views,\nspecifically testing the generalisation capabilities of methods. Finally, we\nprovide detailed metadata for all scenes, including weather, time of day, and\ntraffic conditions, to allow for a detailed model performance breakdown across\nscene characteristics. Dataset and code are available at\nhttps://github.com/wayveai/wayve_scenes.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}