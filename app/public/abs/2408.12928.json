{"id":"2408.12928","title":"ParGo: Bridging Vision-Language with Partial and Global Views","authors":"An-Lan Wang, Bin Shan, Wei Shi, Kun-Yu Lin, Xiang Fei, Guozhi Tang,\n  Lei Liao, Jingqun Tang, Can Huang, Wei-Shi Zheng","authorsParsed":[["Wang","An-Lan",""],["Shan","Bin",""],["Shi","Wei",""],["Lin","Kun-Yu",""],["Fei","Xiang",""],["Tang","Guozhi",""],["Liao","Lei",""],["Tang","Jingqun",""],["Huang","Can",""],["Zheng","Wei-Shi",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 09:14:58 GMT"}],"updateDate":"2024-08-26","timestamp":1724404498000,"abstract":"  This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}