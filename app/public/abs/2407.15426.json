{"id":"2407.15426","title":"Resource-Efficient Federated Multimodal Learning via Layer-wise and\n  Progressive Training","authors":"Ye Lin Tun, Chu Myaet Thwal, Minh N. H. Nguyen, Choong Seon Hong","authorsParsed":[["Tun","Ye Lin",""],["Thwal","Chu Myaet",""],["Nguyen","Minh N. H.",""],["Hong","Choong Seon",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 07:06:17 GMT"}],"updateDate":"2024-07-23","timestamp":1721631977000,"abstract":"  Combining different data modalities enables deep neural networks to tackle\ncomplex tasks more effectively, making multimodal learning increasingly\npopular. To harness multimodal data closer to end users, it is essential to\nintegrate multimodal learning with privacy-preserving training approaches such\nas federated learning (FL). However, compared to conventional unimodal\nlearning, multimodal setting requires dedicated encoders for each modality,\nresulting in larger and more complex models that demand significant resources.\nThis presents a substantial challenge for FL clients operating with limited\ncomputational resources and communication bandwidth. To address these\nchallenges, we introduce LW-FedMML, a layer-wise federated multimodal learning\napproach, which decomposes the training process into multiple steps. Each step\nfocuses on training only a portion of the model, thereby significantly reducing\nthe memory and computational requirements. Moreover, FL clients only need to\nexchange the trained model portion with the central server, lowering the\nresulting communication cost. We conduct extensive experiments across various\nFL scenarios and multimodal learning setups to validate the effectiveness of\nour proposed method. The results demonstrate that LW-FedMML can compete with\nconventional end-to-end federated multimodal learning (FedMML) while\nsignificantly reducing the resource burden on FL clients. Specifically,\nLW-FedMML reduces memory usage by up to $2.7\\times$, computational operations\n(FLOPs) by $2.4\\times$, and total communication cost by $2.3\\times$. We also\nintroduce a progressive training approach called Prog-FedMML. While it offers\nlesser resource efficiency than LW-FedMML, Prog-FedMML has the potential to\nsurpass the performance of end-to-end FedMML, making it a viable option for\nscenarios with fewer resource constraints.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}