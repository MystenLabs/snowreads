{"id":"2407.11650","title":"Statistics-aware Audio-visual Deepfake Detector","authors":"Marcella Astrid, Enjie Ghorbel, Djamila Aouada","authorsParsed":[["Astrid","Marcella",""],["Ghorbel","Enjie",""],["Aouada","Djamila",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 12:15:41 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 11:41:59 GMT"}],"updateDate":"2024-07-18","timestamp":1721132141000,"abstract":"  In this paper, we propose an enhanced audio-visual deep detection method.\nRecent methods in audio-visual deepfake detection mostly assess the\nsynchronization between audio and visual features. Although they have shown\npromising results, they are based on the maximization/minimization of isolated\nfeature distances without considering feature statistics. Moreover, they rely\non cumbersome deep learning architectures and are heavily dependent on\nempirically fixed hyperparameters. Herein, to overcome these limitations, we\npropose: (1) a statistical feature loss to enhance the discrimination\ncapability of the model, instead of relying solely on feature distances; (2)\nusing the waveform for describing the audio as a replacement of frequency-based\nrepresentations; (3) a post-processing normalization of the fakeness score; (4)\nthe use of shallower network for reducing the computational complexity.\nExperiments on the DFDC and FakeAVCeleb datasets demonstrate the relevance of\nthe proposed method.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}