{"id":"2408.02993","title":"DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model","authors":"Yiming Zhong, Xiaolin Zhang, Yao Zhao, and Yunchao Wei","authorsParsed":[["Zhong","Yiming",""],["Zhang","Xiaolin",""],["Zhao","Yao",""],["Wei","Yunchao",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 06:59:15 GMT"},{"version":"v2","created":"Fri, 9 Aug 2024 14:12:49 GMT"}],"updateDate":"2024-08-12","timestamp":1722927555000,"abstract":"  Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents. In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models. In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}