{"id":"2408.13714","title":"TalkLoRA: Low-Rank Adaptation for Speech-Driven Animation","authors":"Jack Saunders, Vinay Namboodiri","authorsParsed":[["Saunders","Jack",""],["Namboodiri","Vinay",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 03:30:05 GMT"}],"updateDate":"2024-08-27","timestamp":1724556605000,"abstract":"  Speech-driven facial animation is important for many applications including\nTV, film, video games, telecommunication and AR/VR. Recently, transformers have\nbeen shown to be extremely effective for this task. However, we identify two\nissues with the existing transformer-based models. Firstly, they are difficult\nto adapt to new personalised speaking styles and secondly, they are slow to run\nfor long sentences due to the quadratic complexity of the transformer. We\npropose TalkLoRA to address both of these issues. TalkLoRA uses Low-Rank\nAdaptation to effectively and efficiently adapt to new speaking styles, even\nwith limited data. It does this by training an adaptor with a small number of\nparameters for each subject. We also utilise a chunking strategy to reduce the\ncomplexity of the underlying transformer, allowing for long sentences at\ninference time. TalkLoRA can be applied to any transformer-based speech-driven\nanimation method. We perform extensive experiments to show that TalkLoRA\narchives state-of-the-art style adaptation and that it allows for an\norder-of-complexity reduction in inference times without sacrificing quality.\nWe also investigate and provide insights into the hyperparameter selection for\nLoRA fine-tuning of speech-driven facial animation models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"qVVZZrUVcEkAZQqh3yX10m4fn6Ri5xV3Bm6p6c315KY","pdfSize":"698956"}
