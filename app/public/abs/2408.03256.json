{"id":"2408.03256","title":"Synthesizing Text-to-SQL Data from Weak and Strong LLMs","authors":"Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, Chang Zhou","authorsParsed":[["Yang","Jiaxi",""],["Hui","Binyuan",""],["Yang","Min",""],["Yang","Jian",""],["Lin","Junyang",""],["Zhou","Chang",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 15:40:32 GMT"}],"updateDate":"2024-08-07","timestamp":1722958832000,"abstract":"  The capability gap between open-source and closed-source large language\nmodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we\nintroduce a synthetic data approach that combines data produced by larger, more\npowerful models (strong models) with error information data generated by\nsmaller, not well-aligned models (weak models). The method not only enhances\nthe domain generalization of text-to-SQL models but also explores the potential\nof error data supervision through preference learning. Furthermore, we employ\nthe synthetic data approach for instruction tuning on open-source LLMs,\nresulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is\ndemonstrated through state-of-the-art results on the SPIDER and BIRD\nbenchmarks, bridging the performance gap between open-source models and methods\nprompted by closed-source models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}