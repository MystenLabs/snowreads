{"id":"2407.13622","title":"Misspecified $Q$-Learning with Sparse Linear Function Approximation:\n  Tight Bounds on Approximation Error","authors":"Ally Yalei Du, Lin F. Yang, Ruosong Wang","authorsParsed":[["Du","Ally Yalei",""],["Yang","Lin F.",""],["Wang","Ruosong",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 15:58:04 GMT"}],"updateDate":"2024-07-19","timestamp":1721318284000,"abstract":"  The recent work by Dong & Yang (2023) showed for misspecified sparse linear\nbandits, one can obtain an $O\\left(\\epsilon\\right)$-optimal policy using a\npolynomial number of samples when the sparsity is a constant, where $\\epsilon$\nis the misspecification error. This result is in sharp contrast to misspecified\nlinear bandits without sparsity, which require an exponential number of samples\nto get the same guarantee. In order to study whether the analog result is\npossible in the reinforcement learning setting, we consider the following\nproblem: assuming the optimal $Q$-function is a $d$-dimensional linear function\nwith sparsity $k$ and misspecification error $\\epsilon$, whether we can obtain\nan $O\\left(\\epsilon\\right)$-optimal policy using number of samples polynomially\nin the feature dimension $d$. We first demonstrate why the standard approach\nbased on Bellman backup or the existing optimistic value function elimination\napproach such as OLIVE (Jiang et al., 2017) achieves suboptimal guarantees for\nthis problem. We then design a novel elimination-based algorithm to show one\ncan obtain an $O\\left(H\\epsilon\\right)$-optimal policy with sample complexity\npolynomially in the feature dimension $d$ and planning horizon $H$. Lastly, we\ncomplement our upper bound with an $\\widetilde{\\Omega}\\left(H\\epsilon\\right)$\nsuboptimality lower bound, giving a complete picture of this problem.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}