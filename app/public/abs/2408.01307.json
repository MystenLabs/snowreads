{"id":"2408.01307","title":"Decentralized Smoothing ADMM for Quantile Regression with Non-Convex\n  Sparse Penalties","authors":"Reza Mirzaeifard, Diyako Ghaderyan, Stefan Werner","authorsParsed":[["Mirzaeifard","Reza",""],["Ghaderyan","Diyako",""],["Werner","Stefan",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 15:00:04 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 19:16:35 GMT"}],"updateDate":"2024-08-12","timestamp":1722610804000,"abstract":"  In the rapidly evolving internet-of-things (IoT) ecosystem, effective data\nanalysis techniques are crucial for handling distributed data generated by\nsensors. Addressing the limitations of existing methods, such as the\nsub-gradient approach, which fails to distinguish between active and non-active\ncoefficients effectively, this paper introduces the decentralized smoothing\nalternating direction method of multipliers (DSAD) for penalized quantile\nregression. Our method leverages non-convex sparse penalties like the minimax\nconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), improving\nthe identification and retention of significant predictors. DSAD incorporates a\ntotal variation norm within a smoothing ADMM framework, achieving consensus\namong distributed nodes and ensuring uniform model performance across disparate\ndata sources. This approach overcomes traditional convergence challenges\nassociated with non-convex penalties in decentralized settings. We present\ntheoretical proofs and extensive simulation results to validate the\neffectiveness of the DSAD, demonstrating its superiority in achieving reliable\nconvergence and enhancing estimation accuracy compared with prior methods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}