{"id":"2408.13987","title":"Focused Large Language Models are Stable Many-Shot Learners","authors":"Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang,\n  Chuyi Tan, Boyuan Pan, Heda Wang, Yao Hu, Kan Li","authorsParsed":[["Yuan","Peiwen",""],["Feng","Shaoxiong",""],["Li","Yiwei",""],["Wang","Xinglin",""],["Zhang","Yueqi",""],["Tan","Chuyi",""],["Pan","Boyuan",""],["Wang","Heda",""],["Hu","Yao",""],["Li","Kan",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 02:53:24 GMT"}],"updateDate":"2024-08-27","timestamp":1724640804000,"abstract":"  In-Context Learning (ICL) enables large language models (LLMs) to achieve\nrapid task adaptation by learning from demonstrations. With the increase in\navailable context length of LLMs, recent experiments have shown that the\nperformance of ICL does not necessarily scale well in many-shot (demonstration)\nsettings. We theoretically and experimentally confirm that the reason lies in\nmore demonstrations dispersing the model attention from the query, hindering\nits understanding of key content. Inspired by how humans learn from examples,\nwe propose a training-free method FocusICL, which conducts triviality filtering\nto avoid attention being diverted by unimportant contents at token-level and\noperates hierarchical attention to further ensure sufficient attention towards\ncurrent query at demonstration-level. We also design an efficient\nhyperparameter searching strategy for FocusICL based on model perplexity of\ndemonstrations. Comprehensive experiments validate that FocusICL achieves an\naverage performance improvement of 5.2% over vanilla ICL and scales well with\nmany-shot demonstrations.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jpA-zpNsJWSpelFMkWdofb6X-ONe0pJ1DbrQwHY2cFs","pdfSize":"1112118"}
