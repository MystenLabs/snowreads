{"id":"2407.18200","title":"Sparse Incremental Aggregation in Multi-Hop Federated Learning","authors":"Sourav Mukherjee, Nasrin Razmi, Armin Dekorsy, Petar Popovski, Bho\n  Matthiesen","authorsParsed":[["Mukherjee","Sourav",""],["Razmi","Nasrin",""],["Dekorsy","Armin",""],["Popovski","Petar",""],["Matthiesen","Bho",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 17:09:22 GMT"}],"updateDate":"2024-07-26","timestamp":1721927362000,"abstract":"  This paper investigates federated learning (FL) in a multi-hop communication\nsetup, such as in constellations with inter-satellite links. In this setup,\npart of the FL clients are responsible for forwarding other client's results to\nthe parameter server. Instead of using conventional routing, the communication\nefficiency can be improved significantly by using in-network model aggregation\nat each intermediate hop, known as incremental aggregation (IA). Prior works\n[1] have indicated diminishing gains for IA under gradient sparsification. Here\nwe study this issue and propose several novel correlated sparsification methods\nfor IA. Numerical results show that, for some of these algorithms, the full\npotential of IA is still available under sparsification without impairing\nconvergence. We demonstrate a 15x improvement in communication efficiency over\nconventional routing and a 11x improvement over state-of-the-art (SoA) sparse\nIA.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rjh5S8vTnDEPmF4XqvJwBbm2iSkoEDZZoKQJrt_VxDs","pdfSize":"190281"}
