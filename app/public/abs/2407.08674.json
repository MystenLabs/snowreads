{"id":"2407.08674","title":"Still-Moving: Customized Video Generation without Customized Video Data","authors":"Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael\n  Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, Inbar Mosseri","authorsParsed":[["Chefer","Hila",""],["Zada","Shiran",""],["Paiss","Roni",""],["Ephrat","Ariel",""],["Tov","Omer",""],["Rubinstein","Michael",""],["Wolf","Lior",""],["Dekel","Tali",""],["Michaeli","Tomer",""],["Mosseri","Inbar",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:06:53 GMT"}],"updateDate":"2024-07-12","timestamp":1720717613000,"abstract":"  Customizing text-to-image (T2I) models has seen tremendous progress recently,\nparticularly in areas such as personalization, stylization, and conditional\ngeneration. However, expanding this progress to video generation is still in\nits infancy, primarily due to the lack of customized video data. In this work,\nwe introduce Still-Moving, a novel generic framework for customizing a\ntext-to-video (T2V) model, without requiring any customized video data. The\nframework applies to the prominent T2V design where the video model is built\nover a text-to-image (T2I) model (e.g., via inflation). We assume access to a\ncustomized version of the T2I model, trained only on still image data (e.g.,\nusing DreamBooth or StyleDrop). Naively plugging in the weights of the\ncustomized T2I model into the T2V model often leads to significant artifacts or\ninsufficient adherence to the customization data. To overcome this issue, we\ntrain lightweight $\\textit{Spatial Adapters}$ that adjust the features produced\nby the injected T2I layers. Importantly, our adapters are trained on\n$\\textit{\"frozen videos\"}$ (i.e., repeated images), constructed from image\nsamples generated by the customized T2I model. This training is facilitated by\na novel $\\textit{Motion Adapter}$ module, which allows us to train on such\nstatic videos while preserving the motion prior of the video model. At test\ntime, we remove the Motion Adapter modules and leave in only the trained\nSpatial Adapters. This restores the motion prior of the T2V model while\nadhering to the spatial prior of the customized T2I model. We demonstrate the\neffectiveness of our approach on diverse tasks including personalized,\nstylized, and conditional generation. In all evaluated scenarios, our method\nseamlessly integrates the spatial prior of the customized T2I model with a\nmotion prior supplied by the T2V model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}