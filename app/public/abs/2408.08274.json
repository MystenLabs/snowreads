{"id":"2408.08274","title":"BAM! Just Like That: Simple and Efficient Parameter Upcycling for\n  Mixture of Experts","authors":"Qizhen Zhang, Nikolas Gritsch, Dwaraknath Gnaneshwar, Simon Guo, David\n  Cairuz, Bharat Venkitesh, Jakob Foerster, Phil Blunsom, Sebastian Ruder,\n  Ahmet Ustun, Acyr Locatelli","authorsParsed":[["Zhang","Qizhen",""],["Gritsch","Nikolas",""],["Gnaneshwar","Dwaraknath",""],["Guo","Simon",""],["Cairuz","David",""],["Venkitesh","Bharat",""],["Foerster","Jakob",""],["Blunsom","Phil",""],["Ruder","Sebastian",""],["Ustun","Ahmet",""],["Locatelli","Acyr",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:19:12 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 01:33:30 GMT"}],"updateDate":"2024-08-20","timestamp":1723742352000,"abstract":"  The Mixture of Experts (MoE) framework has become a popular architecture for\nlarge language models due to its superior performance over dense models.\nHowever, training MoEs from scratch in a large-scale regime is prohibitively\nexpensive. Existing methods mitigate this by pre-training multiple dense expert\nmodels independently and using them to initialize an MoE. This is done by using\nexperts' feed-forward network (FFN) to initialize the MoE's experts while\nmerging other parameters. However, this method limits the reuse of dense model\nparameters to only the FFN layers, thereby constraining the advantages when\n\"upcycling\" these models into MoEs. We propose BAM (Branch-Attend-Mix), a\nsimple yet effective method that addresses this shortcoming. BAM makes full use\nof specialized dense models by not only using their FFN to initialize the MoE\nlayers but also leveraging experts' attention parameters fully by initializing\nthem into a soft-variant of Mixture of Attention (MoA) layers. We explore two\nmethods for upcycling attention parameters: 1) initializing separate attention\nexperts from dense models including all attention parameters for the best model\nperformance; and 2) sharing key and value parameters across all experts to\nfacilitate for better inference efficiency. To further improve efficiency, we\nadopt a parallel attention transformer architecture to MoEs, which allows the\nattention experts and FFN experts to be computed concurrently. Our experiments\non seed models ranging from 590 million to 2 billion parameters demonstrate\nthat BAM surpasses baselines in both perplexity and downstream task\nperformance, within the same computational and data constraints.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}