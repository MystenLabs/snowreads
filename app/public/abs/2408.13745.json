{"id":"2408.13745","title":"DOCE: Finding the Sweet Spot for Execution-Based Code Generation","authors":"Haau-Sing Li, Patrick Fernandes, Iryna Gurevych, Andr\\'e F.T. Martins","authorsParsed":[["Li","Haau-Sing",""],["Fernandes","Patrick",""],["Gurevych","Iryna",""],["Martins","Andr√© F. T.",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 07:10:36 GMT"},{"version":"v2","created":"Sat, 7 Sep 2024 10:09:31 GMT"},{"version":"v3","created":"Fri, 13 Sep 2024 05:50:11 GMT"}],"updateDate":"2024-09-16","timestamp":1724569836000,"abstract":"  Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Programming Languages"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AY4mzoH4aLHw91a4XkuKfzp4Vj8cJWt1ASEkXcCugPg","pdfSize":"7750824"}
