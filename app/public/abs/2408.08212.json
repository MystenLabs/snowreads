{"id":"2408.08212","title":"Covert Bias: The Severity of Social Views' Unalignment in Language\n  Models Towards Implicit and Explicit Opinion","authors":"Abeer Aldayel, Areej Alokaili, Rehab Alahmadi","authorsParsed":[["Aldayel","Abeer",""],["Alokaili","Areej",""],["Alahmadi","Rehab",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 15:23:00 GMT"},{"version":"v2","created":"Fri, 16 Aug 2024 11:57:53 GMT"}],"updateDate":"2024-08-19","timestamp":1723735380000,"abstract":"  While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models. To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}