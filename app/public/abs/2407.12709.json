{"id":"2407.12709","title":"MoME: Mixture of Multimodal Experts for Generalist Multimodal Large\n  Language Models","authors":"Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, Liqiang Nie","authorsParsed":[["Shen","Leyang",""],["Chen","Gongwei",""],["Shao","Rui",""],["Guan","Weili",""],["Nie","Liqiang",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 16:31:38 GMT"}],"updateDate":"2024-07-18","timestamp":1721233898000,"abstract":"  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities across various vision-language tasks. However, a generalist MLLM\ntypically underperforms compared with a specialist MLLM on most VL tasks, which\ncan be attributed to task interference. In this paper, we propose a mixture of\nmultimodal experts (MoME) to mitigate task interference and obtain a generalist\nMLLM. Our MoME is composed of two key components, a mixture of vision experts\n(MoVE) and a mixture of language experts (MoLE). MoVE can adaptively modulate\nthe features transformed from various vision encoders, and has a strong\ncompatibility in transformation architecture. MoLE incorporates sparsely gated\nexperts into LLMs to achieve painless improvements with roughly unchanged\ninference costs. In response to task interference, our MoME specializes in both\nvision and language modality to adapt to task discrepancies. Extensive\nexperiments show that MoME significantly improves the performance of generalist\nMLLMs across various VL tasks. The source code is released at\nhttps://github.com/JiuTian-VL/MoME\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}