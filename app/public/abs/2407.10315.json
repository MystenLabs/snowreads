{"id":"2407.10315","title":"Order parameters and phase transitions of continual learning in deep\n  neural networks","authors":"Haozhe Shan, Qianyi Li, Haim Sompolinsky","authorsParsed":[["Shan","Haozhe",""],["Li","Qianyi",""],["Sompolinsky","Haim",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 20:22:36 GMT"}],"updateDate":"2024-07-16","timestamp":1720988556000,"abstract":"  Continual learning (CL) enables animals to learn new tasks without erasing\nprior knowledge. CL in artificial neural networks (NNs) is challenging due to\ncatastrophic forgetting, where new learning degrades performance on older\ntasks. While various techniques exist to mitigate forgetting, theoretical\ninsights into when and why CL fails in NNs are lacking. Here, we present a\nstatistical-mechanics theory of CL in deep, wide NNs, which characterizes the\nnetwork's input-output mapping as it learns a sequence of tasks. It gives rise\nto order parameters (OPs) that capture how task relations and network\narchitecture influence forgetting and knowledge transfer, as verified by\nnumerical evaluations. We found that the input and rule similarity between\ntasks have different effects on CL performance. In addition, the theory\npredicts that increasing the network depth can effectively reduce overlap\nbetween tasks, thereby lowering forgetting. For networks with task-specific\nreadouts, the theory identifies a phase transition where CL performance shifts\ndramatically as tasks become less similar, as measured by the OPs. Sufficiently\nlow similarity leads to catastrophic anterograde interference, where the\nnetwork retains old tasks perfectly but completely fails to generalize new\nlearning. Our results delineate important factors affecting CL performance and\nsuggest strategies for mitigating forgetting.\n","subjects":["Computing Research Repository/Machine Learning","Physics/Applied Physics","Quantitative Biology/Neurons and Cognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"E_-onlk10YOK44qdTUnBSFPdRaSnUJc3OtYCv9mxhkk","pdfSize":"25948287"}
