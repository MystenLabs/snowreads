{"id":"2407.20505","title":"Interpreting and Mitigating Hallucination in MLLMs through Multi-agent\n  Debate","authors":"Zheng Lin and Zhenxing Niu and Zhibin Wang and Yinghui Xu","authorsParsed":[["Lin","Zheng",""],["Niu","Zhenxing",""],["Wang","Zhibin",""],["Xu","Yinghui",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 02:41:32 GMT"}],"updateDate":"2024-07-31","timestamp":1722307292000,"abstract":"  MLLMs often generate outputs that are inconsistent with the visual content, a\nchallenge known as hallucination. Previous methods focus on determining whether\na generated output is hallucinated, without identifying which image region\nleads to the hallucination or interpreting why such hallucinations occur. In\nthis paper, we argue that hallucination in MLLMs is partially due to a lack of\nslow-thinking and divergent-thinking in these models. To address this, we\npropose adopting a self-reflection scheme to promote slow-thinking.\nFurthermore, we consider eliminating hallucination as a complex reasoning task\nand propose a multi-agent debate approach to encourage divergent-thinking.\nConsequently, our approach can not only mitigate hallucinations but also\ninterpret why they occur and detail the specifics of hallucination. In\naddition, we propose to distinguish creativity from hallucination in the\ncontext of MLLMs, and illustrate how to evaluate MLLMs' creativity capability.\nExtensive experiments on various benchmarks demonstrate that our approach\nexhibits generalized hallucinations-mitigating performance across several\nMLLMs.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}