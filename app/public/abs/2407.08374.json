{"id":"2407.08374","title":"Enhancing Robustness of Vision-Language Models through Orthogonality\n  Learning and Cross-Regularization","authors":"Jinlong Li, Zequn Jie, Elisa Ricci, Lin Ma, Nicu Sebe","authorsParsed":[["Li","Jinlong",""],["Jie","Zequn",""],["Ricci","Elisa",""],["Ma","Lin",""],["Sebe","Nicu",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 10:35:53 GMT"},{"version":"v2","created":"Mon, 15 Jul 2024 08:38:40 GMT"}],"updateDate":"2024-07-16","timestamp":1720694153000,"abstract":"  Efficient finetuning of vision-language models (VLMs) like CLIP for specific\ndownstream tasks is gaining significant attention. Previous works primarily\nfocus on prompt learning to adapt the CLIP into a variety of downstream tasks,\nhowever, suffering from task overfitting when finetuned on a small data set. In\nthis paper, we introduce an orthogonal finetuning method for efficiently\nupdating pretrained weights which enhances robustness and generalization, while\na cross-regularization strategy is further exploited to maintain the stability\nin terms of zero-shot generalization of VLMs, dubbed \\textbf{\\textit{OrthCR}}.\nSpecifically, trainable orthogonal matrices are injected seamlessly into the\ntransformer architecture and enforced with orthogonality constraint using\nCayley parameterization, benefiting from the norm-preserving property and thus\nleading to stable and faster convergence. To alleviate deviation from\northogonal constraint during training, a cross-regularization strategy is\nfurther employed with initial pretrained weights within a bypass manner. In\naddition, to enrich the sample diversity for downstream tasks, we first explore\nCutout data augmentation to boost the efficient finetuning and comprehend how\nour approach improves the specific downstream performance and maintains the\ngeneralizability in the perspective of Orthogonality Learning. Beyond existing\nprompt learning techniques, we conduct extensive experiments to demonstrate\nthat our method explicitly steers pretrained weight space to represent the\ntask-specific knowledge and presents competitive generalizability under\nbase-to-base/base-to-new, cross-dataset transfer and domain generalization\nevaluations.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}