{"id":"2408.02697","title":"Why Rectified Power Unit Networks Fail and How to Improve It: An\n  Effective Theory Perspective","authors":"Taeyoung Kim, Myungjoo Kang","authorsParsed":[["Kim","Taeyoung",""],["Kang","Myungjoo",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 13:05:05 GMT"}],"updateDate":"2024-08-07","timestamp":1722776705000,"abstract":"  The Rectified Power Unit (RePU) activation functions, unlike the Rectified\nLinear Unit (ReLU), have the advantage of being a differentiable function when\nconstructing neural networks. However, it can be experimentally observed when\ndeep layers are stacked, neural networks constructed with RePU encounter\ncritical issues. These issues include the values exploding or vanishing and\nfailure of training. And these happen regardless of the hyperparameter\ninitialization. From the perspective of effective theory, we aim to identify\nthe causes of this phenomenon and propose a new activation function that\nretains the advantages of RePU while overcoming its drawbacks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}