{"id":"2407.12703","title":"Subgraph-Aware Training of Text-based Methods for Knowledge Graph\n  Completion","authors":"Youmin Ko, Hyemin Yang, Taeuk Kim, and Hyunjoon Kim","authorsParsed":[["Ko","Youmin",""],["Yang","Hyemin",""],["Kim","Taeuk",""],["Kim","Hyunjoon",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 16:25:37 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 00:34:13 GMT"},{"version":"v3","created":"Tue, 23 Jul 2024 06:26:30 GMT"}],"updateDate":"2024-07-24","timestamp":1721233537000,"abstract":"  Fine-tuning pre-trained language models (PLMs) has recently shown a potential\nto improve knowledge graph completion (KGC). However, most PLM-based methods\nencode only textual information, neglecting various topological structures of\nknowledge graphs (KGs). In this paper, we empirically validate the significant\nrelations between the structural properties of KGs and the performance of the\nPLM-based methods. To leverage the structural knowledge, we propose a\nSubgraph-Aware Training framework for KGC (SATKGC) that combines (i)\nsubgraph-aware mini-batching to encourage hard negative sampling, and (ii) a\nnew contrastive learning method to focus more on harder entities and harder\nnegative triples in terms of the structural properties. To the best of our\nknowledge, this is the first study to comprehensively incorporate the\nstructural inductive bias of the subgraphs into fine-tuning PLMs. Extensive\nexperiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our\ncode is available.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}