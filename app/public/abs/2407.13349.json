{"id":"2407.13349","title":"DCNv3: Towards Next Generation Deep Cross Network for CTR Prediction","authors":"Honghao Li, Yiwen Zhang, Yi Zhang, Hanwei Li, Lei Sang, and Jieming\n  Zhu","authorsParsed":[["Li","Honghao",""],["Zhang","Yiwen",""],["Zhang","Yi",""],["Li","Hanwei",""],["Sang","Lei",""],["Zhu","Jieming",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 09:49:13 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 03:23:01 GMT"},{"version":"v3","created":"Mon, 29 Jul 2024 16:30:42 GMT"},{"version":"v4","created":"Wed, 31 Jul 2024 15:59:46 GMT"},{"version":"v5","created":"Tue, 6 Aug 2024 14:10:16 GMT"},{"version":"v6","created":"Fri, 9 Aug 2024 06:31:56 GMT"}],"updateDate":"2024-08-14","timestamp":1721296153000,"abstract":"  Deep & Cross Network and its derivative models have become an important\nparadigm for click-through rate (CTR) prediction due to their effective balance\nbetween computational cost and performance. However, these models face four\nmajor limitations: (1) the performance of existing explicit feature interaction\nmethods is often weaker than that of implicit deep neural network (DNN),\nundermining their necessity; (2) many models fail to adaptively filter noise\nwhile increasing the order of feature interactions; (3) the fusion methods of\nmost models cannot provide suitable supervision signals for their different\nsub-networks; (4) while most models claim to capture high-order feature\ninteractions, they often do so implicitly and non-interpretably through DNN,\nwhich limits the trustworthiness of the model's predictions.\n  To address the identified limitations, this paper proposes the next\ngeneration deep cross network: Deep Cross Network v3 (DCNv3), along with its\ntwo sub-networks: Linear Cross Network (LCN) and Exponential Cross Network\n(ECN) for CTR prediction. DCNv3 ensures interpretability in feature interaction\nmodeling while linearly and exponentially increasing the order of feature\ninteractions to achieve genuine Deep Crossing rather than just Deep & Cross.\nAdditionally, we employ a Self-Mask operation to filter noise and reduce the\nnumber of parameters in the Cross Network by half. In the fusion layer, we use\na simple yet effective multi-loss trade-off and calculation method, called\nTri-BCE, to provide appropriate supervision signals. Comprehensive experiments\non six datasets demonstrate the effectiveness, efficiency, and interpretability\nof DCNv3. The code, running logs, and detailed hyperparameter configurations\nare available at: https://github.com/salmon1802/DCNv3.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}