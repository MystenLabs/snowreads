{"id":"2408.05334","title":"Revisiting Multi-Modal LLM Evaluation","authors":"Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha, Manoj\n  Acharya, Kushal Kafle, Christopher Kanan","authorsParsed":[["Lu","Jian",""],["Srivastava","Shikhar",""],["Chen","Junyu",""],["Shrestha","Robik",""],["Acharya","Manoj",""],["Kafle","Kushal",""],["Kanan","Christopher",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 20:55:46 GMT"}],"updateDate":"2024-08-13","timestamp":1723236946000,"abstract":"  With the advent of multi-modal large language models (MLLMs), datasets used\nfor visual question answering (VQA) and referring expression comprehension have\nseen a resurgence. However, the most popular datasets used to evaluate MLLMs\nare some of the earliest ones created, and they have many known problems,\nincluding extreme bias, spurious correlations, and an inability to permit\nfine-grained analysis. In this paper, we pioneer evaluating recent MLLMs (LLaVA\n1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o) on datasets designed\nto address weaknesses in earlier ones. We assess three VQA datasets: 1) TDIUC,\nwhich permits fine-grained analysis on 12 question types; 2) TallyQA, which has\nsimple and complex counting questions; and 3) DVQA, which requires optical\ncharacter recognition for chart understanding. We also study VQDv1, a dataset\nthat requires identifying all image regions that satisfy a given query. Our\nexperiments reveal the weaknesses of many MLLMs that have not previously been\nreported. Our code is integrated into the widely used LAVIS framework for MLLM\nevaluation, enabling the rapid assessment of future MLLMs. Project webpage:\nhttps://kevinlujian.github.io/MLLM_Evaluations/\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}