{"id":"2407.00671","title":"Establishing Deep InfoMax as an effective self-supervised learning\n  methodology in materials informatics","authors":"Michael Moran, Vladimir V. Gusev, Michael W. Gaultois, Dmytro Antypov,\n  Matthew J. Rosseinsky","authorsParsed":[["Moran","Michael",""],["Gusev","Vladimir V.",""],["Gaultois","Michael W.",""],["Antypov","Dmytro",""],["Rosseinsky","Matthew J.",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 11:33:49 GMT"}],"updateDate":"2024-07-02","timestamp":1719747229000,"abstract":"  The scarcity of property labels remains a key challenge in materials\ninformatics, whereas materials data without property labels are abundant in\ncomparison. By pretraining supervised property prediction models on\nself-supervised tasks that depend only on the \"intrinsic information\" available\nin any Crystallographic Information File (CIF), there is potential to leverage\nthe large amount of crystal data without property labels to improve property\nprediction results on small datasets. We apply Deep InfoMax as a\nself-supervised machine learning framework for materials informatics that\nexplicitly maximises the mutual information between a point set (or graph)\nrepresentation of a crystal and a vector representation suitable for downstream\nlearning. This allows the pretraining of supervised models on large materials\ndatasets without the need for property labels and without requiring the model\nto reconstruct the crystal from a representation vector. We investigate the\nbenefits of Deep InfoMax pretraining implemented on the Site-Net architecture\nto improve the performance of downstream property prediction models with small\namounts (<10^3) of data, a situation relevant to experimentally measured\nmaterials property databases. Using a property label masking methodology, where\nwe perform self-supervised learning on larger supervised datasets and then\ntrain supervised models on a small subset of the labels, we isolate Deep\nInfoMax pretraining from the effects of distributional shift. We demonstrate\nperformance improvements in the contexts of representation learning and\ntransfer learning on the tasks of band gap and formation energy prediction.\nHaving established the effectiveness of Deep InfoMax pretraining in a\ncontrolled environment, our findings provide a foundation for extending the\napproach to address practical challenges in materials informatics.\n","subjects":["Computing Research Repository/Machine Learning","Condensed Matter/Materials Science"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}