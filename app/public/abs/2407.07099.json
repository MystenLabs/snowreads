{"id":"2407.07099","title":"Nash CoT: Multi-Path Inference with Preference Equilibrium","authors":"Ziqi Zhang, Cunxiang Wang, Xiong Xiao, Yue Zhang, Donglin Wang","authorsParsed":[["Zhang","Ziqi",""],["Wang","Cunxiang",""],["Xiao","Xiong",""],["Zhang","Yue",""],["Wang","Donglin",""]],"versions":[{"version":"v1","created":"Tue, 18 Jun 2024 07:46:13 GMT"}],"updateDate":"2024-07-11","timestamp":1718696773000,"abstract":"  Chain-of-thought (CoT) prompting has emerged as a powerful technique for\nenhancing the reasoning capabilities of Large Language Models (LLMs) on complex\nproblems. Among CoT-related studies, self-consistency (Multi-path inference\nwith answer filtering through voting) involves generating multiple reasoning\npaths using the CoT framework and then selecting the most frequently produced\noutputs standing out as a concise yet competitive approach. While\nself-consistency has indeed led to the improvements in LLM inference, the use\nof multi-path inference also escalates deployment costs. Therefore, maintaining\nthe performance benefits of self-consistency inherited from multi-path\ninference while reducing the inference costs holds significant value. In this\nresearch, we conceptualize language decoding as a preference consensus game,\nconstructing a bi-player gaming system within each local path, and introduce\nNash Chain-of-Thought (Nash CoT). Specifically, for a given question, we\nleverage LLM to autonomously select the contextually relevant template and\ngenerate outputs guided by this template, aiming to reach Nash Equilibrium\nalongside normal generation in each path. This approach allows us to achieve\ncomparable or improved performance compared to self-consistency while using\nfewer inference paths on various inference tasks, including Arabic reasoning,\nCommonsense Question answering, and Symbolic inference.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Science and Game Theory","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}