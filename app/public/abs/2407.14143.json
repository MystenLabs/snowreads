{"id":"2407.14143","title":"Class-Incremental Learning with CLIP: Adaptive Representation Adjustment\n  and Parameter Fusion","authors":"Linlan Huang, Xusheng Cao, Haori Lu, Xialei Liu","authorsParsed":[["Huang","Linlan",""],["Cao","Xusheng",""],["Lu","Haori",""],["Liu","Xialei",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 09:20:33 GMT"}],"updateDate":"2024-07-22","timestamp":1721380833000,"abstract":"  Class-incremental learning is a challenging problem, where the goal is to\ntrain a model that can classify data from an increasing number of classes over\ntime. With the advancement of vision-language pre-trained models such as CLIP,\nthey demonstrate good generalization ability that allows them to excel in\nclass-incremental learning with completely frozen parameters. However, further\nadaptation to downstream tasks by simply fine-tuning the model leads to severe\nforgetting. Most existing works with pre-trained models assume that the\nforgetting of old classes is uniform when the model acquires new knowledge. In\nthis paper, we propose a method named Adaptive Representation Adjustment and\nParameter Fusion (RAPF). During training for new data, we measure the influence\nof new classes on old ones and adjust the representations, using textual\nfeatures. After training, we employ a decomposed parameter fusion to further\nmitigate forgetting during adapter module fine-tuning. Experiments on several\nconventional benchmarks show that our method achieves state-of-the-art results.\nOur code is available at \\url{https://github.com/linlany/RAPF}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}