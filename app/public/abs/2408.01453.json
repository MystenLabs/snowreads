{"id":"2408.01453","title":"Reporting and Analysing the Environmental Impact of Language Models on\n  the Example of Commonsense Question Answering with External Knowledge","authors":"Aida Usmanova, Junbo Huang, Debayan Banerjee and Ricardo Usbeck","authorsParsed":[["Usmanova","Aida",""],["Huang","Junbo",""],["Banerjee","Debayan",""],["Usbeck","Ricardo",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 16:16:16 GMT"}],"updateDate":"2024-08-06","timestamp":1721837776000,"abstract":"  Human-produced emissions are growing at an alarming rate, causing already\nobservable changes in the climate and environment in general. Each year global\ncarbon dioxide emissions hit a new record, and it is reported that 0.5% of\ntotal US greenhouse gas emissions are attributed to data centres as of 2021.\nThe release of ChatGPT in late 2022 sparked social interest in Large Language\nModels (LLMs), the new generation of Language Models with a large number of\nparameters and trained on massive amounts of data. Currently, numerous\ncompanies are releasing products featuring various LLMs, with many more models\nin development and awaiting release. Deep Learning research is a competitive\nfield, with only models that reach top performance attracting attention and\nbeing utilized. Hence, achieving better accuracy and results is often the first\npriority, while the model's efficiency and the environmental impact of the\nstudy are neglected. However, LLMs demand substantial computational resources\nand are very costly to train, both financially and environmentally. It becomes\nessential to raise awareness and promote conscious decisions about algorithmic\nand hardware choices. Providing information on training time, the approximate\ncarbon dioxide emissions and power consumption would assist future studies in\nmaking necessary adjustments and determining the compatibility of available\ncomputational resources with model requirements. In this study, we infused T5\nLLM with external knowledge and fine-tuned the model for Question-Answering\ntask. Furthermore, we calculated and reported the approximate environmental\nimpact for both steps. The findings demonstrate that the smaller models may not\nalways be sustainable options, and increased training does not always imply\nbetter performance. The most optimal outcome is achieved by carefully\nconsidering both performance and efficiency factors.\n","subjects":["Computing Research Repository/Computers and Society","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}