{"id":"2407.06730","title":"LVLM-empowered Multi-modal Representation Learning for Visual Place\n  Recognition","authors":"Teng Wang, Lingquan Meng, Lei Cheng, Changyin Sun","authorsParsed":[["Wang","Teng",""],["Meng","Lingquan",""],["Cheng","Lei",""],["Sun","Changyin",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 10:15:31 GMT"}],"updateDate":"2024-07-10","timestamp":1720520131000,"abstract":"  Visual place recognition (VPR) remains challenging due to significant\nviewpoint changes and appearance variations. Mainstream works tackle these\nchallenges by developing various feature aggregation methods to transform deep\nfeatures into robust and compact global representations. Unfortunately,\nsatisfactory results cannot be achieved under challenging conditions. We start\nfrom a new perspective and attempt to build a discriminative global\nrepresentations by fusing image data and text descriptions of the the visual\nscene. The motivation is twofold: (1) Current Large Vision-Language Models\n(LVLMs) demonstrate extraordinary emergent capability in visual instruction\nfollowing, and thus provide an efficient and flexible manner in generating text\ndescriptions of images; (2) The text descriptions, which provide high-level\nscene understanding, show strong robustness against environment variations.\nAlthough promising, leveraging LVLMs to build multi-modal VPR solutions remains\nchallenging in efficient multi-modal fusion. Furthermore, LVLMs will inevitably\nproduces some inaccurate descriptions, making it even harder. To tackle these\nchallenges, we propose a novel multi-modal VPR solution. It first adapts\npre-trained visual and language foundation models to VPR for extracting image\nand text features, which are then fed into the feature combiner to enhance each\nother. As the main component, the feature combiner first propose a token-wise\nattention block to adaptively recalibrate text tokens according to their\nrelevance to the image data, and then develop an efficient cross-attention\nfusion module to propagate information across different modalities. The\nenhanced multi-modal features are compressed into the feature descriptor for\nperforming retrieval. Experimental results show that our method outperforms\nstate-of-the-art methods by a large margin with significantly smaller image\ndescriptor dimension.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}