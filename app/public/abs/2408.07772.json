{"id":"2408.07772","title":"Out-of-Distribution Learning with Human Feedback","authors":"Haoyue Bai, Xuefeng Du, Katie Rainey, Shibin Parameswaran, Yixuan Li","authorsParsed":[["Bai","Haoyue",""],["Du","Xuefeng",""],["Rainey","Katie",""],["Parameswaran","Shibin",""],["Li","Yixuan",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 18:49:27 GMT"}],"updateDate":"2024-08-16","timestamp":1723661367000,"abstract":"  Out-of-distribution (OOD) learning often relies heavily on statistical\napproaches or predefined assumptions about OOD data distributions, hindering\ntheir efficacy in addressing multifaceted challenges of OOD generalization and\nOOD detection in real-world deployment environments. This paper presents a\nnovel framework for OOD learning with human feedback, which can provide\ninvaluable insights into the nature of OOD shifts and guide effective model\nadaptation. Our framework capitalizes on the freely available unlabeled data in\nthe wild that captures the environmental test-time OOD distributions under both\ncovariate and semantic shifts. To harness such data, our key idea is to\nselectively provide human feedback and label a small number of informative\nsamples from the wild data distribution, which are then used to train a\nmulti-class classifier and an OOD detector. By exploiting human feedback, we\nenhance the robustness and reliability of machine learning models, equipping\nthem with the capability to handle OOD scenarios with greater precision. We\nprovide theoretical insights on the generalization error bounds to justify our\nalgorithm. Extensive experiments show the superiority of our method,\noutperforming the current state-of-the-art by a significant margin.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fEK6FB-iFYG6Yf4MC1fmH1yYHQD0AjDcVWdSEyAnHZ8","pdfSize":"1198511"}
