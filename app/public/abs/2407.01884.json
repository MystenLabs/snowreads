{"id":"2407.01884","title":"EIT-1M: One Million EEG-Image-Text Pairs for Human Visual-textual\n  Recognition and More","authors":"Xu Zheng, Ling Wang, Kanghao Chen, Yuanhuiyi Lyu, Jiazhou Zhou, Lin\n  Wang","authorsParsed":[["Zheng","Xu",""],["Wang","Ling",""],["Chen","Kanghao",""],["Lyu","Yuanhuiyi",""],["Zhou","Jiazhou",""],["Wang","Lin",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 02:11:15 GMT"}],"updateDate":"2024-07-03","timestamp":1719886275000,"abstract":"  Recently, electroencephalography (EEG) signals have been actively\nincorporated to decode brain activity to visual or textual stimuli and achieve\nobject recognition in multi-modal AI. Accordingly, endeavors have been focused\non building EEG-based datasets from visual or textual single-modal stimuli.\nHowever, these datasets offer limited EEG epochs per category, and the complex\nsemantics of stimuli presented to participants compromise their quality and\nfidelity in capturing precise brain activity. The study in neuroscience unveils\nthat the relationship between visual and textual stimulus in EEG recordings\nprovides valuable insights into the brain's ability to process and integrate\nmulti-modal information simultaneously. Inspired by this, we propose a novel\nlarge-scale multi-modal dataset, named EIT-1M, with over 1 million\nEEG-image-text pairs. Our dataset is superior in its capacity of reflecting\nbrain activities in simultaneously processing multi-modal information. To\nachieve this, we collected data pairs while participants viewed alternating\nsequences of visual-textual stimuli from 60K natural images and\ncategory-specific texts. Common semantic categories are also included to elicit\nbetter reactions from participants' brains. Meanwhile, response-based stimulus\ntiming and repetition across blocks and sessions are included to ensure data\ndiversity. To verify the effectiveness of EIT-1M, we provide an in-depth\nanalysis of EEG data captured from multi-modal stimuli across different\ncategories and participants, along with data quality scores for transparency.\nWe demonstrate its validity on two tasks: 1) EEG recognition from visual or\ntextual stimuli or both and 2) EEG-to-visual generation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}