{"id":"2408.09385","title":"Offline RLHF Methods Need More Accurate Supervision Signals","authors":"Shiqi Wang, Zhengze Zhang, Rui Zhao, Fei Tan, Cam Tu Nguyen","authorsParsed":[["Wang","Shiqi",""],["Zhang","Zhengze",""],["Zhao","Rui",""],["Tan","Fei",""],["Nguyen","Cam Tu",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 07:04:16 GMT"}],"updateDate":"2024-08-20","timestamp":1723964656000,"abstract":"  With the rapid advances in Large Language Models (LLMs), aligning LLMs with\nhuman preferences become increasingly important. Although Reinforcement\nLearning with Human Feedback (RLHF) proves effective, it is complicated and\nhighly resource-intensive. As such, offline RLHF has been introduced as an\nalternative solution, which directly optimizes LLMs with ranking losses on a\nfixed preference dataset. Current offline RLHF only captures the ``ordinal\nrelationship'' between responses, overlooking the crucial aspect of ``how\nmuch'' one is preferred over the others. To address this issue, we propose a\nsimple yet effective solution called \\textbf{R}eward \\textbf{D}ifference\n\\textbf{O}ptimization, shorted as \\textbf{RDO}. Specifically, we introduce {\\it\nreward difference coefficients} to reweigh sample pairs in offline RLHF. We\nthen develop a {\\it difference model} involving rich interactions between a\npair of responses for predicting these difference coefficients. Experiments\nwith 7B LLMs on the HH and TL;DR datasets substantiate the effectiveness of our\nmethod in both automatic metrics and human evaluation, thereby highlighting its\npotential for aligning LLMs with human intent and values.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}