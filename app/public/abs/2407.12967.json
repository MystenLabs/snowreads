{"id":"2407.12967","title":"R\\'enyi-infinity constrained sampling with $d^3$ membership queries","authors":"Yunbum Kook, Matthew S. Zhang","authorsParsed":[["Kook","Yunbum",""],["Zhang","Matthew S.",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 19:20:08 GMT"}],"updateDate":"2024-07-19","timestamp":1721244008000,"abstract":"  Uniform sampling over a convex body is a fundamental algorithmic problem, yet\nthe convergence in KL or R\\'enyi divergence of most samplers remains poorly\nunderstood. In this work, we propose a constrained proximal sampler, a\nprincipled and simple algorithm that possesses elegant convergence guarantees.\nLeveraging the uniform ergodicity of this sampler, we show that it converges in\nthe R\\'enyi-infinity divergence ($\\mathcal R_\\infty$) with no query complexity\noverhead when starting from a warm start. This is the strongest of commonly\nconsidered performance metrics, implying rates in $\\{\\mathcal R_q,\n\\mathsf{KL}\\}$ convergence as special cases.\n  By applying this sampler within an annealing scheme, we propose an algorithm\nwhich can approximately sample $\\varepsilon$-close to the uniform distribution\non convex bodies in $\\mathcal R_\\infty$-divergence with\n$\\widetilde{\\mathcal{O}}(d^3\\, \\text{polylog} \\frac{1}{\\varepsilon})$ query\ncomplexity. This improves on all prior results in $\\{\\mathcal R_q,\n\\mathsf{KL}\\}$-divergences, without resorting to any algorithmic modifications\nor post-processing of the sample. It also matches the prior best known\ncomplexity in total variation distance.\n","subjects":["Computing Research Repository/Data Structures and Algorithms","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CgpG8UXo3YhMCHU8bgLpw3PLdGXgbRS2Q8kv04Aa6B8","pdfSize":"495188"}
