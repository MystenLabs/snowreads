{"id":"2408.04029","title":"Human Speech Perception in Noise: Can Large Language Models Paraphrase\n  to Improve It?","authors":"Anupama Chingacham, Miaoran Zhang, Vera Demberg, Dietrich Klakow","authorsParsed":[["Chingacham","Anupama",""],["Zhang","Miaoran",""],["Demberg","Vera",""],["Klakow","Dietrich",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 18:24:23 GMT"}],"updateDate":"2024-08-09","timestamp":1723055063000,"abstract":"  Large Language Models (LLMs) can generate text by transferring style\nattributes like formality resulting in formal or informal text. However,\ninstructing LLMs to generate text that when spoken, is more intelligible in an\nacoustically difficult environment, is an under-explored topic. We conduct the\nfirst study to evaluate LLMs on a novel task of generating acoustically\nintelligible paraphrases for better human speech perception in noise. Our\nexperiments in English demonstrated that with standard prompting, LLMs struggle\nto control the non-textual attribute, i.e., acoustic intelligibility, while\nefficiently capturing the desired textual attributes like semantic equivalence.\nTo remedy this issue, we propose a simple prompting approach,\nprompt-and-select, which generates paraphrases by decoupling the desired\ntextual and non-textual attributes in the text generation pipeline. Our\napproach resulted in a 40% relative improvement in human speech perception, by\nparaphrasing utterances that are highly distorted in a listening condition with\nbabble noise at a signal-to-noise ratio (SNR) -5 dB. This study reveals the\nlimitation of LLMs in capturing non-textual attributes, and our proposed method\nshowcases the potential of using LLMs for better human speech perception in\nnoise.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"lNshds2zt4ESZOF1i_xvFNTyDJX4LJOEdr0WtogRmv8","pdfSize":"563552"}
