{"id":"2407.09077","title":"Mapping Large Memory-constrained Workflows onto Heterogeneous Platforms","authors":"Svetlana Kulagina, Henning Meyerhenke, Anne Benoit","authorsParsed":[["Kulagina","Svetlana",""],["Meyerhenke","Henning",""],["Benoit","Anne",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 08:03:24 GMT"}],"updateDate":"2024-07-15","timestamp":1720771404000,"abstract":"  Scientific workflows are often represented as directed acyclic graphs (DAGs),\nwhere vertices correspond to tasks and edges represent the dependencies between\nthem. Since these graphs are often large in both the number of tasks and their\nresource requirements, it is important to schedule them efficiently on parallel\nor distributed compute systems. Typically, each task requires a certain amount\nof memory to be executed and needs to communicate data to its successor tasks.\nThe goal is thus to execute the workflow as fast as possible (i.e., to minimize\nits makespan) while satisfying the memory constraints. Hence, we investigate\nthe partitioning and mapping of DAG-shaped workflows onto heterogeneous\nplatforms where each processor can have a different speed and a different\nmemory size. We first propose a baseline algorithm in the absence of existing\nmemory-aware solutions. As our main contribution, we then present a four-step\nheuristic. Its first step is to partition the input DAG into smaller blocks\nwith an existing DAG partitioner. The next two steps adapt the resulting blocks\nof the DAG to fit the processor memories and optimize for the overall makespan\nby further splitting and merging these blocks. Finally, we use local search via\nblock swaps to further improve the makespan. Our experimental evaluation on\nreal-world and simulated workflows with up to 30,000 tasks shows that\nexploiting the heterogeneity with the four-step heuristic reduces the makespan\nby a factor of 2.44 on average (even more on large workflows), compared to the\nbaseline that ignores heterogeneity.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}