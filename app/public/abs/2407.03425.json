{"id":"2407.03425","title":"Lift, Splat, Map: Lifting Foundation Masks for Label-Free Semantic Scene\n  Completion","authors":"Arthur Zhang, Rainier Heijne, Joydeep Biswas","authorsParsed":[["Zhang","Arthur",""],["Heijne","Rainier",""],["Biswas","Joydeep",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 18:08:05 GMT"}],"updateDate":"2024-07-08","timestamp":1720030085000,"abstract":"  Autonomous mobile robots deployed in urban environments must be\ncontext-aware, i.e., able to distinguish between different semantic entities,\nand robust to occlusions. Current approaches like semantic scene completion\n(SSC) require pre-enumerating the set of classes and costly human annotations,\nwhile representation learning methods relax these assumptions but are not\nrobust to occlusions and learn representations tailored towards auxiliary\ntasks. To address these limitations, we propose LSMap, a method that lifts\nmasks from visual foundation models to predict a continuous, open-set semantic\nand elevation-aware representation in bird's eye view (BEV) for the entire\nscene, including regions underneath dynamic entities and in occluded areas. Our\nmodel only requires a single RGBD image, does not require human labels, and\noperates in real time. We quantitatively demonstrate our approach outperforms\nexisting models trained from scratch on semantic and elevation scene completion\ntasks with finetuning. Furthermore, we show that our pre-trained representation\noutperforms existing visual foundation models at unsupervised semantic scene\ncompletion. We evaluate our approach using CODa, a large-scale, real-world\nurban robot dataset. Supplementary visualizations, code, data, and pre-trained\nmodels, will be publicly available soon.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}