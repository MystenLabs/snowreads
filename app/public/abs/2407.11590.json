{"id":"2407.11590","title":"Rethinking Learned Image Compression: Context is All You Need","authors":"Jixiang Luo","authorsParsed":[["Luo","Jixiang",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 10:50:10 GMT"},{"version":"v2","created":"Mon, 29 Jul 2024 09:02:11 GMT"},{"version":"v3","created":"Fri, 2 Aug 2024 01:36:59 GMT"}],"updateDate":"2024-08-05","timestamp":1721127010000,"abstract":"  Since LIC has made rapid progress recently compared to traditional methods,\nthis paper attempts to discuss the question about 'Where is the boundary of\nLearned Image Compression(LIC)?'. Thus this paper splits the above problem into\ntwo sub-problems:1)Where is the boundary of rate-distortion performance of\nPSNR? 2)How to further improve the compression gain and achieve the boundary?\nTherefore this paper analyzes the effectiveness of scaling parameters for\nencoder, decoder and context model, which are the three components of LIC. Then\nwe conclude that scaling for LIC is to scale for context model and decoder\nwithin LIC. Extensive experiments demonstrate that overfitting can actually\nserve as an effective context. By optimizing the context, this paper further\nimproves PSNR and achieves state-of-the-art performance, showing a performance\ngain of 14.39% with BD-RATE over VVC.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}