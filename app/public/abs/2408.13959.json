{"id":"2408.13959","title":"Bidirectional Awareness Induction in Autoregressive Seq2Seq Models","authors":"Jia Cheng Hu, Roberto Cavicchioli, Alessandro Capotondi","authorsParsed":[["Hu","Jia Cheng",""],["Cavicchioli","Roberto",""],["Capotondi","Alessandro",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 23:46:35 GMT"}],"updateDate":"2024-08-27","timestamp":1724629595000,"abstract":"  Autoregressive Sequence-To-Sequence models are the foundation of many Deep\nLearning achievements in major research fields such as Vision and Natural\nLanguage Processing. Despite that, they still present significant limitations.\nFor instance, when errors occur in the early steps of the prediction, the whole\noutput is severely affected. Such reliance on previously predicted tokens and\nthe inherent computational unfriendliness of sequential algorithms, motivated\nresearchers to explore different architectures and methods in the search for\nbidirectional approaches. In this work, we introduce the Bidirectional\nAwareness Induction (BAI), a training method that leverages a subset of\nelements in the network, the Pivots, to perform bidirectional learning without\nbreaking the autoregressive constraints. To showcase its flexibility, we apply\nthe method to three architectures, the Transformer, ExpansionNet v2 and GPT,\nthen perform experiments over three tasks. Experimental results showcase BAI's\neffectiveness on all selected tasks and architectures. In particular, we\nobserved an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in\nNeural Machine Translation, and 1.16 ROUGE in Text Summarization compared to\nthe respective baselines. Notably, BAI not only has a positive impact on models\ntrained from scratch but on pre-trained models as well. Such an aspect,\ncombined with the absence of architectural requirements synergizes well with\nthe current trend of LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}