{"id":"2408.09815","title":"A Population-to-individual Tuning Framework for Adapting Pretrained LM\n  to On-device User Intent Prediction","authors":"Jiahui Gong, Jingtao Ding, Fanjin Meng, Guilong Chen, Hong Chen, Shen\n  Zhao, Haisheng Lu, Yong Li","authorsParsed":[["Gong","Jiahui",""],["Ding","Jingtao",""],["Meng","Fanjin",""],["Chen","Guilong",""],["Chen","Hong",""],["Zhao","Shen",""],["Lu","Haisheng",""],["Li","Yong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 09:07:33 GMT"}],"updateDate":"2024-08-20","timestamp":1724058453000,"abstract":"  Mobile devices, especially smartphones, can support rich functions and have\ndeveloped into indispensable tools in daily life. With the rise of generative\nAI services, smartphones can potentially transform into personalized\nassistants, anticipating user needs and scheduling services accordingly.\nPredicting user intents on smartphones, and reflecting anticipated activities\nbased on past interactions and context, remains a pivotal step towards this\nvision. Existing research predominantly focuses on specific domains, neglecting\nthe challenge of modeling diverse event sequences across dynamic contexts.\nLeveraging pre-trained language models (PLMs) offers a promising avenue, yet\nadapting PLMs to on-device user intent prediction presents significant\nchallenges. To address these challenges, we propose PITuning, a\nPopulation-to-Individual Tuning framework. PITuning enhances common pattern\nextraction through dynamic event-to-intent transition modeling and addresses\nlong-tailed preferences via adaptive unlearning strategies. Experimental\nresults on real-world datasets demonstrate PITuning's superior intent\nprediction performance, highlighting its ability to capture long-tailed\npreferences and its practicality for on-device prediction scenarios.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Human-Computer Interaction"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}