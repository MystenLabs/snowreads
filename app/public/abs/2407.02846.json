{"id":"2407.02846","title":"Multi-Task Domain Adaptation for Language Grounding with 3D Objects","authors":"Penglei Sun, Yaoxian Song, Xinglin Pan, Peijie Dong, Xiaofei Yang,\n  Qiang Wang, Zhixu Li, Tiefeng Li, and Xiaowen Chu","authorsParsed":[["Sun","Penglei",""],["Song","Yaoxian",""],["Pan","Xinglin",""],["Dong","Peijie",""],["Yang","Xiaofei",""],["Wang","Qiang",""],["Li","Zhixu",""],["Li","Tiefeng",""],["Chu","Xiaowen",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 06:47:58 GMT"},{"version":"v2","created":"Fri, 5 Jul 2024 08:10:49 GMT"}],"updateDate":"2024-07-08","timestamp":1719989278000,"abstract":"  The existing works on object-level language grounding with 3D objects mostly\nfocus on improving performance by utilizing the off-the-shelf pre-trained\nmodels to capture features, such as viewpoint selection or geometric priors.\nHowever, they have failed to consider exploring the cross-modal representation\nof language-vision alignment in the cross-domain field. To answer this problem,\nwe propose a novel method called Domain Adaptation for Language Grounding\n(DA4LG) with 3D objects. Specifically, the proposed DA4LG consists of a visual\nadapter module with multi-task learning to realize vision-language alignment by\ncomprehensive multimodal feature representation. Experimental results\ndemonstrate that DA4LG competitively performs across visual and non-visual\nlanguage descriptions, independent of the completeness of observation. DA4LG\nachieves state-of-the-art performance in the single-view setting and multi-view\nsetting with the accuracy of 83.8% and 86.8% respectively in the language\ngrounding benchmark SNARE. The simulation experiments show the well-practical\nand generalized performance of DA4LG compared to the existing methods. Our\nproject is available at https://sites.google.com/view/da4lg.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}