{"id":"2408.15484","title":"NAS-BNN: Neural Architecture Search for Binary Neural Networks","authors":"Zhihao Lin, Yongtao Wang, Jinhe Zhang, Xiaojie Chu, Haibin Ling","authorsParsed":[["Lin","Zhihao",""],["Wang","Yongtao",""],["Zhang","Jinhe",""],["Chu","Xiaojie",""],["Ling","Haibin",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 02:17:58 GMT"}],"updateDate":"2024-08-29","timestamp":1724811478000,"abstract":"  Binary Neural Networks (BNNs) have gained extensive attention for their\nsuperior inferencing efficiency and compression ratio compared to traditional\nfull-precision networks. However, due to the unique characteristics of BNNs,\ndesigning a powerful binary architecture is challenging and often requires\nsignificant manpower. A promising solution is to utilize Neural Architecture\nSearch (NAS) to assist in designing BNNs, but current NAS methods for BNNs are\nrelatively straightforward and leave a performance gap between the searched\nmodels and manually designed ones. To address this gap, we propose a novel\nneural architecture search scheme for binary neural networks, named NAS-BNN. We\nfirst carefully design a search space based on the unique characteristics of\nBNNs. Then, we present three training strategies, which significantly enhance\nthe training of supernet and boost the performance of all subnets. Our\ndiscovered binary model family outperforms previous BNNs for a wide range of\noperations (OPs) from 20M to 200M. For instance, we achieve 68.20% top-1\naccuracy on ImageNet with only 57M OPs. In addition, we validate the\ntransferability of these searched BNNs on the object detection task, and our\nbinary detectors with the searched BNNs achieve a novel state-of-the-art\nresult, e.g., 31.6% mAP with 370M OPs, on MS COCO dataset. The source code and\nmodels will be released at https://github.com/VDIGPKU/NAS-BNN.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}