{"id":"2407.03263","title":"A Unified Framework for 3D Scene Understanding","authors":"Wei Xu, Chunsheng Shi, Sifan Tu, Xin Zhou, Dingkang Liang, Xiang Bai","authorsParsed":[["Xu","Wei",""],["Shi","Chunsheng",""],["Tu","Sifan",""],["Zhou","Xin",""],["Liang","Dingkang",""],["Bai","Xiang",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 16:50:07 GMT"}],"updateDate":"2024-07-04","timestamp":1720025407000,"abstract":"  We propose UniSeg3D, a unified 3D segmentation framework that achieves\npanoptic, semantic, instance, interactive, referring, and open-vocabulary\nsemantic segmentation tasks within a single model. Most previous 3D\nsegmentation approaches are specialized for a specific task, thereby limiting\ntheir understanding of 3D scenes to a task-specific perspective. In contrast,\nthe proposed method unifies six tasks into unified representations processed by\nthe same Transformer. It facilitates inter-task knowledge sharing and,\ntherefore, promotes comprehensive 3D scene understanding. To take advantage of\nmulti-task unification, we enhance the performance by leveraging task\nconnections. Specifically, we design a knowledge distillation method and a\ncontrastive learning method to transfer task-specific knowledge across\ndifferent tasks. Benefiting from extensive inter-task knowledge sharing, our\nUniSeg3D becomes more powerful. Experiments on three benchmarks, including the\nScanNet20, ScanRefer, and ScanNet200, demonstrate that the UniSeg3D\nconsistently outperforms current SOTA methods, even those specialized for\nindividual tasks. We hope UniSeg3D can serve as a solid unified baseline and\ninspire future work. The code will be available at\nhttps://dk-liang.github.io/UniSeg3D/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}