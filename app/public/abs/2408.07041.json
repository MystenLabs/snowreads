{"id":"2408.07041","title":"Subjective and Objective Quality Assessment of Rendered Human Avatar\n  Videos in Virtual Reality","authors":"Yu-Chih Chen, Avinab Saha, Alexandre Chapiro, Christian H\\\"ane,\n  Jean-Charles Bazin, Bo Qiu, Stefano Zanetti, Ioannis Katsavounidis, Alan C.\n  Bovik","authorsParsed":[["Chen","Yu-Chih",""],["Saha","Avinab",""],["Chapiro","Alexandre",""],["HÃ¤ne","Christian",""],["Bazin","Jean-Charles",""],["Qiu","Bo",""],["Zanetti","Stefano",""],["Katsavounidis","Ioannis",""],["Bovik","Alan C.",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 17:11:54 GMT"}],"updateDate":"2024-08-14","timestamp":1723569114000,"abstract":"  We study the visual quality judgments of human subjects on digital human\navatars (sometimes referred to as \"holograms\" in the parlance of virtual\nreality [VR] and augmented reality [AR] systems) that have been subjected to\ndistortions. We also study the ability of video quality models to predict human\njudgments. As streaming human avatar videos in VR or AR become increasingly\ncommon, the need for more advanced human avatar video compression protocols\nwill be required to address the tradeoffs between faithfully transmitting\nhigh-quality visual representations while adjusting to changeable bandwidth\nscenarios. During transmission over the internet, the perceived quality of\ncompressed human avatar videos can be severely impaired by visual artifacts. To\noptimize trade-offs between perceptual quality and data volume in practical\nworkflows, video quality assessment (VQA) models are essential tools. However,\nthere are very few VQA algorithms developed specifically to analyze human body\navatar videos, due, at least in part, to the dearth of appropriate and\ncomprehensive datasets of adequate size. Towards filling this gap, we introduce\nthe LIVE-Meta Rendered Human Avatar VQA Database, which contains 720 human\navatar videos processed using 20 different combinations of encoding parameters,\nlabeled by corresponding human perceptual quality judgments that were collected\nin six degrees of freedom VR headsets. To demonstrate the usefulness of this\nnew and unique video resource, we use it to study and compare the performances\nof a variety of state-of-the-art Full Reference and No Reference video quality\nprediction models, including a new model called HoloQA. As a service to the\nresearch community, we will be publicly releasing the metadata of the new\ndatabase at\nhttps://live.ece.utexas.edu/research/LIVE-Meta-rendered-human-avatar/index.html.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cYVugb5mfm8x01asU-gIEOT_VkV3d_W7_yLD4z9nlcY","pdfSize":"20142527"}
