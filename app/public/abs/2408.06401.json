{"id":"2408.06401","title":"High-dimensional optimization for multi-spiked tensor PCA","authors":"G\\'erard Ben Arous, C\\'edric Gerbelot, Vanessa Piccolo","authorsParsed":[["Arous","Gérard Ben",""],["Gerbelot","Cédric",""],["Piccolo","Vanessa",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 12:09:25 GMT"}],"updateDate":"2024-08-14","timestamp":1723464565000,"abstract":"  We study the dynamics of two local optimization algorithms, online stochastic\ngradient descent (SGD) and gradient flow, within the framework of the\nmulti-spiked tensor model in the high-dimensional regime. This multi-index\nmodel arises from the tensor principal component analysis (PCA) problem, which\naims to infer $r$ unknown, orthogonal signal vectors within the $N$-dimensional\nunit sphere through maximum likelihood estimation from noisy observations of an\norder-$p$ tensor. We determine the number of samples and the conditions on the\nsignal-to-noise ratios (SNRs) required to efficiently recover the unknown\nspikes from natural initializations. Specifically, we distinguish between three\ntypes of recovery: exact recovery of each spike, recovery of a permutation of\nall spikes, and recovery of the correct subspace spanned by the signal vectors.\nWe show that with online SGD, it is possible to recover all spikes provided a\nnumber of sample scaling as $N^{p-2}$, aligning with the computational\nthreshold identified in the rank-one tensor PCA problem [Ben Arous, Gheissari,\nJagannath 2020, 2021]. For gradient flow, we show that the algorithmic\nthreshold to efficiently recover the first spike is also of order $N^{p-2}$.\nHowever, recovering the subsequent directions requires the number of samples to\nscale as $N^{p-1}$. Our results are obtained through a detailed analysis of a\nlow-dimensional system that describes the evolution of the correlations between\nthe estimators and the spikes. In particular, the hidden vectors are recovered\none by one according to a sequential elimination phenomenon: as one correlation\nexceeds a critical threshold, all correlations sharing a row or column index\ndecrease and become negligible, allowing the subsequent correlation to grow and\nbecome macroscopic. The sequence in which correlations become macroscopic\ndepends on their initial values and on the associated SNRs.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Probability","Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}