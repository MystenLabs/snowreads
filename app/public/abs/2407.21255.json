{"id":"2407.21255","title":"Responsive ML inference in multi-tenanted environments using AQUA","authors":"Abhishek Vijaya Kumar, Gianni Antichi, Rachee Singh","authorsParsed":[["Kumar","Abhishek Vijaya",""],["Antichi","Gianni",""],["Singh","Rachee",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 00:34:33 GMT"},{"version":"v2","created":"Thu, 1 Aug 2024 02:20:27 GMT"}],"updateDate":"2024-08-02","timestamp":1722386073000,"abstract":"  Modern model serving engines infer prompts on large language models in\nbatches. While batch processing prompts leads to high inference throughput, it\ndelays responding to requests that do not fit in a batch, potentially starving\nthem. We propose that fair scheduling prompts for inference by time-sharing\nGPUs cycles, instead of batch processing them, is key to preventing prompt\nstarvation and achieving responsive inference. However, time-shared prompt\nscheduling incurs the overhead of frequently paging dynamic context needed to\ninfer a prompt back into GPU memory. Today, serving engines support paging\ninference context between GPU memory and the host DRAM. The overhead of\ntransferring context from DRAM to GPU memory is high since it is lower-bounded\nby the limited PCIe bandwidth. We overcome this challenge by offloading\ninference context from a GPU to the memory of another GPU on the same server,\nconnected via inter-GPU interconnects that support magnitudes higher bandwidth\nthan PCIe. We achieve this by developing AQUA, a transparent and elastic GPU\nmemory management framework for responsive LLM inference. We evaluate AQUA by\nhosting eight state-of-the-art large generative ML models of different\nmodalities (e.g., text, audio, vision) on a server with 8 cutting-edge Nvidia\nA100 80G GPUs. Using representative inference workloads, we show that AQUA\nimproves the responsiveness of LLM inference by 4X compared to the\nstate-of-the-art and it improves LLM inference throughput over a single long\nprompt by 6X.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}