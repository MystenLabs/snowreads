{"id":"2407.13743","title":"Optimistic Q-learning for average reward and episodic reinforcement\n  learning","authors":"Priyank Agrawal and Shipra Agrawal","authorsParsed":[["Agrawal","Priyank",""],["Agrawal","Shipra",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:49:09 GMT"}],"updateDate":"2024-07-19","timestamp":1721324949000,"abstract":"  We present an optimistic Q-learning algorithm for regret minimization in\naverage reward reinforcement learning under an additional assumption on the\nunderlying MDP that for all policies, the expected time to visit some frequent\nstate $s_0$ is finite and upper bounded by $H$. Our setting strictly\ngeneralizes the episodic setting and is significantly less restrictive than the\nassumption of bounded hitting time {\\it for all states} made by most previous\nliterature on model-free algorithms in average reward settings. We demonstrate\na regret bound of $\\tilde{O}(H^5 S\\sqrt{AT})$, where $S$ and $A$ are the\nnumbers of states and actions, and $T$ is the horizon. A key technical novelty\nof our work is to introduce an $\\overline{L}$ operator defined as $\\overline{L}\nv = \\frac{1}{H} \\sum_{h=1}^H L^h v$ where $L$ denotes the Bellman operator. We\nshow that under the given assumption, the $\\overline{L}$ operator has a strict\ncontraction (in span) even in the average reward setting. Our algorithm design\nthen uses ideas from episodic Q-learning to estimate and apply this operator\niteratively. Therefore, we provide a unified view of regret minimization in\nepisodic and non-episodic settings that may be of independent interest.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}