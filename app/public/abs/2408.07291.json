{"id":"2408.07291","title":"Evaluating Large Language Model based Personal Information Extraction\n  and Countermeasures","authors":"Yupei Liu, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong","authorsParsed":[["Liu","Yupei",""],["Jia","Yuqi",""],["Jia","Jinyuan",""],["Gong","Neil Zhenqiang",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 04:49:30 GMT"}],"updateDate":"2024-08-15","timestamp":1723610970000,"abstract":"  Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect three datasets including a synthetic dataset generated by GPT-4 and two\nreal-world datasets with manually labeled 8 categories of personal information;\nintroduce a novel mitigation strategy based on \\emph{prompt injection}; and\nsystematically benchmark LLM-based attacks and countermeasures using 10 LLMs\nand our 3 datasets. Our key findings include: LLM can be misused by attackers\nto accurately extract various personal information from personal profiles; LLM\noutperforms conventional methods at such extraction; and prompt injection can\nmitigate such risk to a large extent and outperforms conventional\ncountermeasures. Our code and data are available at:\n\\url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}