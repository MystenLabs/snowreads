{"id":"2408.02025","title":"Contrastive Learning-based Chaining-Cluster for Multilingual Voice-Face\n  Association","authors":"Wuyang Chen, Yanjie Sun, Kele Xu and Yong Dou","authorsParsed":[["Chen","Wuyang",""],["Sun","Yanjie",""],["Xu","Kele",""],["Dou","Yong",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 13:24:36 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 05:14:53 GMT"}],"updateDate":"2024-08-20","timestamp":1722777876000,"abstract":"  The innate correlation between a person's face and voice has recently emerged\nas a compelling area of study, especially within the context of multilingual\nenvironments. This paper introduces our novel solution to the Face-Voice\nAssociation in Multilingual Environments (FAME) 2024 challenge, focusing on a\ncontrastive learning-based chaining-cluster method to enhance face-voice\nassociation. This task involves the challenges of building biometric relations\nbetween auditory and visual modality cues and modelling the prosody\ninterdependence between different languages while addressing both intrinsic and\nextrinsic variability present in the data. To handle these non-trivial\nchallenges, our method employs supervised cross-contrastive (SCC) learning to\nestablish robust associations between voices and faces in multi-language\nscenarios. Following this, we have specifically designed a\nchaining-cluster-based post-processing step to mitigate the impact of outliers\noften found in unconstrained in the wild data. We conducted extensive\nexperiments to investigate the impact of language on face-voice association.\nThe overall results were evaluated on the FAME public evaluation platform,\nwhere we achieved 2nd place. The results demonstrate the superior performance\nof our method, and we validate the robustness and effectiveness of our proposed\napproach. Code is available at https://github.com/colaudiolab/FAME24_solution.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}