{"id":"2407.07482","title":"Rigorous Probabilistic Guarantees for Robust Counterfactual Explanations","authors":"Luca Marzari, Francesco Leofante, Ferdinando Cicalese and Alessandro\n  Farinelli","authorsParsed":[["Marzari","Luca",""],["Leofante","Francesco",""],["Cicalese","Ferdinando",""],["Farinelli","Alessandro",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 09:13:11 GMT"}],"updateDate":"2024-07-11","timestamp":1720602791000,"abstract":"  We study the problem of assessing the robustness of counterfactual\nexplanations for deep learning models. We focus on $\\textit{plausible model\nshifts}$ altering model parameters and propose a novel framework to reason\nabout the robustness property in this setting. To motivate our solution, we\nbegin by showing for the first time that computing the robustness of\ncounterfactuals with respect to plausible model shifts is NP-complete. As this\n(practically) rules out the existence of scalable algorithms for exactly\ncomputing robustness, we propose a novel probabilistic approach which is able\nto provide tight estimates of robustness with strong guarantees while\npreserving scalability. Remarkably, and differently from existing solutions\ntargeting plausible model shifts, our approach does not impose requirements on\nthe network to be analyzed, thus enabling robustness analysis on a wider range\nof architectures. Experiments on four binary classification datasets indicate\nthat our method improves the state of the art in generating robust\nexplanations, outperforming existing methods on a range of metrics.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}