{"id":"2407.11239","title":"From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from\n  Low-Rank Gradients","authors":"Ajay Jaiswal, Lu Yin, Zhenyu Zhang, Shiwei Liu, Jiawei Zhao, Yuandong\n  Tian, Zhangyang Wang","authorsParsed":[["Jaiswal","Ajay",""],["Yin","Lu",""],["Zhang","Zhenyu",""],["Liu","Shiwei",""],["Zhao","Jiawei",""],["Tian","Yuandong",""],["Wang","Zhangyang",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 21:05:20 GMT"}],"updateDate":"2024-07-17","timestamp":1721077520000,"abstract":"  Modern Large Language Models (LLMs) are composed of matrices with billions of\nelements, making their storage and processing quite demanding in terms of\ncomputational resources and memory usage. Being significantly large, such\nmatrices can often be expressed in low-rank format with potential to relax\nresource requirements. Unlike prior works which focus on developing novel\nmatrix decomposition algorithms, in this work we first study the emergence of\nlow-rank structures across matrices within different layers of LLMs and\nestablish a consequential relationship between the gradient dynamics and\nemerging low-rank expressiveness of matrices. Our findings reveal that\ndifferent layers exhibit varying levels of converged low-rank structure,\nnecessitating a non-uniform rank reduction across them to minimize performance\ndrop due to compression. In view of that, we present Weight Low-Rank Projection\n(WeLore) that unifies weight compression and memory-efficient fine-tuning as\nONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail\ndistribution of singular values to identify a suitable rank reduction ratio for\nmatrices within LLMs. Going beyond only as a compression technique, WeLore\ncategorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank\nComponents (N-LRCs) based on their ability to express themselves as low-rank.\nOur gradient perspective and extensive experiments illustrate that LRCs tend to\nhave better finetuning capabilities and can closely mimic (sometimes\noutperform) the training loss trajectory and performance of full-finetuning\nwith notable memory and compute footprint reduction. For example, finetuning a\n50\\% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs\n(WeLore) can outperform its full finetuning with ~3x better throughput and\n~0.6x GPU requirement. Our codes are available at\n\\url{https://github.com/VITA-Group/welore}\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"iqYzbhkDRrDp8spPNMd6GGB4jZNllLIgfb-FK6U1LgY","pdfSize":"3217641","objectId":"0x3cbaf458a007faa0bfc6c92f8e927899e748654496ece728917b547c8715c43a","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
