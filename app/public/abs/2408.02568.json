{"id":"2408.02568","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data\n  Classification","authors":"Pawe{\\l} Zyblewski, Leandro L. Minku","authorsParsed":[["Zyblewski","Pawe≈Ç",""],["Minku","Leandro L.",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 15:43:56 GMT"}],"updateDate":"2024-08-06","timestamp":1722872636000,"abstract":"  Technological advances facilitate the ability to acquire multimodal data,\nposing a challenge for recognition systems while also providing an opportunity\nto use the heterogeneous nature of the information to increase the\ngeneralization capability of models. An often overlooked issue is the cost of\nthe labeling process, which is typically high due to the need for a significant\ninvestment in time and money associated with human experts. Existing\nsemi-supervised learning methods often focus on operating in the feature space\ncreated by the fusion of available modalities, neglecting the potential for\ncross-utilizing complementary information available in each modality. To\naddress this problem, we propose Cross-Modality Clustering-based Self-Labeling\n(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances\nbelonging to each modality in the deep feature space and then propagates known\nlabels within the resulting clusters. Next, information about the instances'\nclass membership in each modality is exchanged based on the Euclidean distance\nto ensure more accurate labeling. Experimental evaluation conducted on 20\ndatasets derived from the MM-IMDb dataset indicates that cross-propagation of\nlabels between modalities -- especially when the number of pre-labeled\ninstances is small -- can allow for more reliable labeling and thus increase\nthe classification performance in each modality.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}