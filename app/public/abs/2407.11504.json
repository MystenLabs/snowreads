{"id":"2407.11504","title":"Bootstrapped Pre-training with Dynamic Identifier Prediction for\n  Generative Retrieval","authors":"Yubao Tang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan,\n  Xueqi Cheng","authorsParsed":[["Tang","Yubao",""],["Zhang","Ruqing",""],["Guo","Jiafeng",""],["de Rijke","Maarten",""],["Fan","Yixing",""],["Cheng","Xueqi",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 08:42:36 GMT"}],"updateDate":"2024-07-17","timestamp":1721119356000,"abstract":"  Generative retrieval uses differentiable search indexes to directly generate\nrelevant document identifiers in response to a query. Recent studies have\nhighlighted the potential of a strong generative retrieval model, trained with\ncarefully crafted pre-training tasks, to enhance downstream retrieval tasks via\nfine-tuning. However, the full power of pre-training for generative retrieval\nremains underexploited due to its reliance on pre-defined static document\nidentifiers, which may not align with evolving model parameters. In this work,\nwe introduce BootRet, a bootstrapped pre-training method for generative\nretrieval that dynamically adjusts document identifiers during pre-training to\naccommodate the continuing memorization of the corpus. BootRet involves three\nkey training phases: (i) initial identifier generation, (ii) pre-training via\ncorpus indexing and relevance prediction tasks, and (iii) bootstrapping for\nidentifier updates. To facilitate the pre-training phase, we further introduce\nnoisy documents and pseudo-queries, generated by large language models, to\nresemble semantic connections in both indexing and retrieval tasks.\nExperimental results demonstrate that BootRet significantly outperforms\nexisting pre-training generative retrieval baselines and performs well even in\nzero-shot settings.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/"}