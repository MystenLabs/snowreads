{"id":"2407.13088","title":"Scheduling Deep Learning Jobs in Multi-Tenant GPU Clusters via Wise\n  Resource Sharing","authors":"Yizhou Luo, Qiang Wang, Shaohuai Shi, Jiaxin Lai, Shuhan Qi, Jiajia\n  Zhang, Xuan Wang","authorsParsed":[["Luo","Yizhou",""],["Wang","Qiang",""],["Shi","Shaohuai",""],["Lai","Jiaxin",""],["Qi","Shuhan",""],["Zhang","Jiajia",""],["Wang","Xuan",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 01:30:49 GMT"}],"updateDate":"2024-07-19","timestamp":1721266249000,"abstract":"  Deep learning (DL) has demonstrated significant success across diverse\nfields, leading to the construction of dedicated GPU accelerators within GPU\nclusters for high-quality training services. Efficient scheduler designs for\nsuch clusters are vital to reduce operational costs and enhance resource\nutilization. While recent schedulers have shown impressive performance in\noptimizing DL job performance and cluster utilization through periodic\nreallocation or selection of GPU resources, they also encounter challenges such\nas preemption and migration overhead, along with potential DL accuracy\ndegradation. Nonetheless, few explore the potential benefits of GPU sharing to\nimprove resource utilization and reduce job queuing times. Motivated by these\ninsights, we present a job scheduling model allowing multiple jobs to share the\nsame set of GPUs without altering job training settings. We introduce SJF-BSBF\n(shortest job first with best sharing benefit first), a straightforward yet\neffective heuristic scheduling algorithm. SJF-BSBF intelligently selects job\npairs for GPU resource sharing and runtime settings (sub-batch size and\nscheduling time point) to optimize overall performance while ensuring DL\nconvergence accuracy through gradient accumulation. In experiments with both\nphysical DL workloads and trace-driven simulations, even as a preemption-free\npolicy, SJF-BSBF reduces the average job completion time by 27-33\\% relative to\nthe state-of-the-art preemptive DL schedulers. Moreover, SJF-BSBF can wisely\ndetermine the optimal resource sharing settings, such as the sharing time point\nand sub-batch size for gradient accumulation, outperforming the aggressive GPU\nsharing approach (baseline SJF-FFS policy) by up to 17\\% in large-scale traces.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}