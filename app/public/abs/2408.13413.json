{"id":"2408.13413","title":"TVG: A Training-free Transition Video Generation Method with Diffusion\n  Models","authors":"Rui Zhang and Yaosen Chen and Yuegen Liu and Wei Wang and Xuming Wen\n  and Hongxia Wang","authorsParsed":[["Zhang","Rui",""],["Chen","Yaosen",""],["Liu","Yuegen",""],["Wang","Wei",""],["Wen","Xuming",""],["Wang","Hongxia",""]],"versions":[{"version":"v1","created":"Sat, 24 Aug 2024 00:33:14 GMT"}],"updateDate":"2024-08-27","timestamp":1724459594000,"abstract":"  Transition videos play a crucial role in media production, enhancing the flow\nand coherence of visual narratives. Traditional methods like morphing often\nlack artistic appeal and require specialized skills, limiting their\neffectiveness. Recent advances in diffusion model-based video generation offer\nnew possibilities for creating transitions but face challenges such as poor\ninter-frame relationship modeling and abrupt content changes. We propose a\nnovel training-free Transition Video Generation (TVG) approach using\nvideo-level diffusion models that addresses these limitations without\nadditional training. Our method leverages Gaussian Process Regression\n($\\mathcal{GPR}$) to model latent representations, ensuring smooth and dynamic\ntransitions between frames. Additionally, we introduce interpolation-based\nconditional controls and a Frequency-aware Bidirectional Fusion (FBiF)\narchitecture to enhance temporal control and transition reliability.\nEvaluations of benchmark datasets and custom image pairs demonstrate the\neffectiveness of our approach in generating high-quality smooth transition\nvideos. The code are provided in https://sobeymil.github.io/tvg.com.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}