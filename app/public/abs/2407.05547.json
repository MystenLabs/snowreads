{"id":"2407.05547","title":"LaSe-E2V: Towards Language-guided Semantic-Aware Event-to-Video\n  Reconstruction","authors":"Kanghao Chen, Hangyu Li, JiaZhou Zhou, Zeyu Wang, Lin Wang","authorsParsed":[["Chen","Kanghao",""],["Li","Hangyu",""],["Zhou","JiaZhou",""],["Wang","Zeyu",""],["Wang","Lin",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 01:40:32 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 09:03:04 GMT"},{"version":"v3","created":"Wed, 17 Jul 2024 13:48:08 GMT"}],"updateDate":"2024-07-18","timestamp":1720402832000,"abstract":"  Event cameras harness advantages such as low latency, high temporal\nresolution, and high dynamic range (HDR), compared to standard cameras. Due to\nthe distinct imaging paradigm shift, a dominant line of research focuses on\nevent-to-video (E2V) reconstruction to bridge event-based and standard computer\nvision. However, this task remains challenging due to its inherently ill-posed\nnature: event cameras only detect the edge and motion information locally.\nConsequently, the reconstructed videos are often plagued by artifacts and\nregional blur, primarily caused by the ambiguous semantics of event data. In\nthis paper, we find language naturally conveys abundant semantic information,\nrendering it stunningly superior in ensuring semantic consistency for E2V\nreconstruction. Accordingly, we propose a novel framework, called LaSe-E2V,\nthat can achieve semantic-aware high-quality E2V reconstruction from a\nlanguage-guided perspective, buttressed by the text-conditional diffusion\nmodels. However, due to diffusion models' inherent diversity and randomness, it\nis hardly possible to directly apply them to achieve spatial and temporal\nconsistency for E2V reconstruction. Thus, we first propose an Event-guided\nSpatiotemporal Attention (ESA) module to condition the event data to the\ndenoising pipeline effectively. We then introduce an event-aware mask loss to\nensure temporal coherence and a noise initialization strategy to enhance\nspatial consistency. Given the absence of event-text-video paired data, we\naggregate existing E2V datasets and generate textual descriptions using the\ntagging models for training and evaluation. Extensive experiments on three\ndatasets covering diverse challenging scenarios (e.g., fast motion, low light)\ndemonstrate the superiority of our method.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}