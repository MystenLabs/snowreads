{"id":"2408.05560","title":"Incremental Gauss-Newton Descent for Machine Learning","authors":"Mikalai Korbit and Mario Zanon","authorsParsed":[["Korbit","Mikalai",""],["Zanon","Mario",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 13:52:40 GMT"}],"updateDate":"2024-08-13","timestamp":1723297960000,"abstract":"  Stochastic Gradient Descent (SGD) is a popular technique used to solve\nproblems arising in machine learning. While very effective, SGD also has some\nweaknesses and various modifications of the basic algorithm have been proposed\nin order to at least partially tackle them, mostly yielding accelerated\nversions of SGD. Filling a gap in the literature, we present a modification of\nthe SGD algorithm exploiting approximate second-order information based on the\nGauss-Newton approach. The new method, which we call Incremental Gauss-Newton\nDescent (IGND), has essentially the same computational burden as standard SGD,\nappears to converge faster on certain classes of problems, and can also be\naccelerated. The key intuition making it possible to implement IGND efficiently\nis that, in the incremental case, approximate second-order information can be\ncondensed into a scalar value that acts as a scaling constant of the update. We\nderive IGND starting from the theory supporting Gauss-Newton methods in a\ngeneral setting and then explain how IGND can also be interpreted as a\nwell-scaled version of SGD, which makes tuning the algorithm simpler, and\nprovides increased robustness. Finally, we show how IGND can be used in\npractice by solving supervised learning tasks as well as reinforcement learning\nproblems. The simulations show that IGND can significantly outperform SGD while\nperforming at least as well as SGD in the worst case.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}