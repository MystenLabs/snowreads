{"id":"2408.03312","title":"MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture\n  Generation","authors":"Xiaofeng Mao, Zhengkai Jiang, Qilin Wang, Chencan Fu, Jiangning Zhang,\n  Jiafu Wu, Yabiao Wang, Chengjie Wang, Wei Li, Mingmin Chi","authorsParsed":[["Mao","Xiaofeng",""],["Jiang","Zhengkai",""],["Wang","Qilin",""],["Fu","Chencan",""],["Zhang","Jiangning",""],["Wu","Jiafu",""],["Wang","Yabiao",""],["Wang","Chengjie",""],["Li","Wei",""],["Chi","Mingmin",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 17:29:01 GMT"}],"updateDate":"2024-08-07","timestamp":1722965341000,"abstract":"  Recent advancements in the field of Diffusion Transformers have substantially\nimproved the generation of high-quality 2D images, 3D videos, and 3D shapes.\nHowever, the effectiveness of the Transformer architecture in the domain of\nco-speech gesture generation remains relatively unexplored, as prior\nmethodologies have predominantly employed the Convolutional Neural Network\n(CNNs) or simple a few transformer layers. In an attempt to bridge this\nresearch gap, we introduce a novel Masked Diffusion Transformer for co-speech\ngesture generation, referred to as MDT-A2G, which directly implements the\ndenoising process on gesture sequences. To enhance the contextual reasoning\ncapability of temporally aligned speech-driven gestures, we incorporate a novel\nMasked Diffusion Transformer. This model employs a mask modeling scheme\nspecifically designed to strengthen temporal relation learning among sequence\ngestures, thereby expediting the learning process and leading to coherent and\nrealistic motions. Apart from audio, Our MDT-A2G model also integrates\nmulti-modal information, encompassing text, emotion, and identity. Furthermore,\nwe propose an efficient inference strategy that diminishes the denoising\ncomputation by leveraging previously calculated results, thereby achieving a\nspeedup with negligible performance degradation. Experimental results\ndemonstrate that MDT-A2G excels in gesture generation, boasting a learning\nspeed that is over 6$\\times$ faster than traditional diffusion transformers and\nan inference speed that is 5.7$\\times$ than the standard diffusion model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}