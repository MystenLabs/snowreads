{"id":"2408.00438","title":"MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D\n  Object Detection","authors":"Youjia Fu, Zihao Xu, Junsong Fu, Huixia Xue, Shuqiu Tan, Lei Li","authorsParsed":[["Fu","Youjia",""],["Xu","Zihao",""],["Fu","Junsong",""],["Xue","Huixia",""],["Tan","Shuqiu",""],["Li","Lei",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 10:16:58 GMT"}],"updateDate":"2024-08-02","timestamp":1722507418000,"abstract":"  Recent advancements in transformer-based monocular 3D object detection\ntechniques have exhibited exceptional performance in inferring 3D attributes\nfrom single 2D images. However, most existing methods rely on\nresource-intensive transformer architectures, which often lead to significant\ndrops in computational efficiency and performance when handling long sequence\ndata. To address these challenges and advance monocular 3D object detection\ntechnology, we propose an innovative network architecture, MonoMM, a\nMulti-scale \\textbf{M}amba-Enhanced network for real-time Monocular 3D object\ndetection. This well-designed architecture primarily includes the following two\ncore modules: Focused Multi-Scale Fusion (FMF) Module, which focuses on\neffectively preserving and fusing image information from different scales with\nlower computational resource consumption. By precisely regulating the\ninformation flow, the FMF module enhances the model adaptability and robustness\nto scale variations while maintaining image details. Depth-Aware Feature\nEnhancement Mamba (DMB) Module: It utilizes the fused features from image\ncharacteristics as input and employs a novel adaptive strategy to globally\nintegrate depth information and visual information. This depth fusion strategy\nnot only improves the accuracy of depth estimation but also enhances the model\nperformance under different viewing angles and environmental conditions.\nMoreover, the modular design of MonoMM provides high flexibility and\nscalability, facilitating adjustments and optimizations according to specific\napplication needs. Extensive experiments conducted on the KITTI dataset show\nthat our method outperforms previous monocular methods and achieves real-time\ndetection.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}