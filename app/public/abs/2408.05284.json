{"id":"2408.05284","title":"Can a Bayesian Oracle Prevent Harm from an Agent?","authors":"Yoshua Bengio, Michael K. Cohen, Nikolay Malkin, Matt MacDermott,\n  Damiano Fornasiere, Pietro Greiner, Younesse Kaddar","authorsParsed":[["Bengio","Yoshua",""],["Cohen","Michael K.",""],["Malkin","Nikolay",""],["MacDermott","Matt",""],["Fornasiere","Damiano",""],["Greiner","Pietro",""],["Kaddar","Younesse",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 18:10:42 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 19:14:27 GMT"}],"updateDate":"2024-08-26","timestamp":1723227042000,"abstract":"  Is there a way to design powerful AI systems based on machine learning\nmethods that would satisfy probabilistic safety guarantees? With the long-term\ngoal of obtaining a probabilistic guarantee that would apply in every context,\nwe consider estimating a context-dependent bound on the probability of\nviolating a given safety specification. Such a risk evaluation would need to be\nperformed at run-time to provide a guardrail against dangerous actions of an\nAI. Noting that different plausible hypotheses about the world could produce\nvery different outcomes, and because we do not know which one is right, we\nderive bounds on the safety violation probability predicted under the true but\nunknown hypothesis. Such bounds could be used to reject potentially dangerous\nactions. Our main results involve searching for cautious but plausible\nhypotheses, obtained by a maximization that involves Bayesian posteriors over\nhypotheses. We consider two forms of this result, in the iid case and in the\nnon-iid case, and conclude with open problems towards turning such theoretical\nresults into practical AI guardrails.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"vwP4R2_-zN8_tM071GTP7rcKok6PqJE1xgVWu4FUp-4","pdfSize":"399185"}
