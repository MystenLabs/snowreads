{"id":"2408.17362","title":"Assessing Generative Language Models in Classification Tasks:\n  Performance and Self-Evaluation Capabilities in the Environmental and Climate\n  Change Domain","authors":"Francesca Grasso and Stefano Locci","authorsParsed":[["Grasso","Francesca",""],["Locci","Stefano",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 15:52:41 GMT"}],"updateDate":"2024-09-02","timestamp":1725033161000,"abstract":"  This paper examines the performance of two Large Language Models (LLMs),\nGPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three\ndifferent classification tasks within the climate change (CC) and environmental\ndomain. Employing BERT-based models as a baseline, we compare their efficacy\nagainst these transformer-based models. Additionally, we assess the models'\nself-evaluation capabilities by analyzing the calibration of verbalized\nconfidence scores in these text classification tasks. Our findings reveal that\nwhile BERT-based models generally outperform both the LLMs and SLM, the\nperformance of the large generative models is still noteworthy. Furthermore,\nour calibration analysis reveals that although Gemma is well-calibrated in\ninitial tasks, it thereafter produces inconsistent results; Llama is reasonably\ncalibrated, and GPT consistently exhibits strong calibration. Through this\nresearch, we aim to contribute to the ongoing discussion on the utility and\neffectiveness of generative LMs in addressing some of the planet's most urgent\nissues, highlighting their strengths and limitations in the context of ecology\nand CC.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}