{"id":"2407.09506","title":"Integrating Large Language Models with Graph-based Reasoning for\n  Conversational Question Answering","authors":"Parag Jain, Mirella Lapata","authorsParsed":[["Jain","Parag",""],["Lapata","Mirella",""]],"versions":[{"version":"v1","created":"Fri, 14 Jun 2024 13:28:03 GMT"}],"updateDate":"2024-07-16","timestamp":1718371683000,"abstract":"  We focus on a conversational question answering task which combines the\nchallenges of understanding questions in context and reasoning over evidence\ngathered from heterogeneous sources like text, knowledge graphs, tables, and\ninfoboxes. Our method utilizes a graph structured representation to aggregate\ninformation about a question and its context (i.e., the conversation so far and\nevidence retrieved to find an answer), while also harnessing the reasoning and\ntext generation capabilities of large language models (LLMs). Graph embeddings\nare directly injected into the LLM, bypassing the token embedding layers, and\nlearned end-to-end by minimizing cross-entropy. Our model maintains a memory\nmodule to track and update past evidence, thus influencing the graph's\nstructure, as the conversation evolves. Experimental results on the ConvMix\nbenchmark(Christmann et al., 2022a) show that graph embeddings enhance the\nLLM's ability to reason, while the memory module provides robustness against\nnoise and retrieval errors.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FrXQa3PVIR1g2iFdWYkfCVkRV5iR7DSulhd52z0qvNg","pdfSize":"1171096"}
