{"id":"2408.16999","title":"A Tighter Convergence Proof of Reverse Experience Replay","authors":"Nan Jiang, Jinzhao Li, Yexiang Xue","authorsParsed":[["Jiang","Nan",""],["Li","Jinzhao",""],["Xue","Yexiang",""]],"versions":[{"version":"v1","created":"Fri, 30 Aug 2024 04:11:35 GMT"}],"updateDate":"2024-09-02","timestamp":1724991095000,"abstract":"  In reinforcement learning, Reverse Experience Replay (RER) is a recently\nproposed algorithm that attains better sample complexity than the classic\nexperience replay method. RER requires the learning algorithm to update the\nparameters through consecutive state-action-reward tuples in reverse order.\nHowever, the most recent theoretical analysis only holds for a minimal learning\nrate and short consecutive steps, which converge slower than those large\nlearning rate algorithms without RER. In view of this theoretical and empirical\ngap, we provide a tighter analysis that mitigates the limitation on the\nlearning rate and the length of consecutive steps. Furthermore, we show\ntheoretically that RER converges with a larger learning rate and a longer\nsequence.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OKPHWmJvN__Hmt6k6fmrPpZz62yHoILQH5VyBzuRcO8","pdfSize":"1234319"}
