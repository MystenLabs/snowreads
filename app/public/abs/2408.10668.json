{"id":"2408.10668","title":"Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation","authors":"Haoyu Wang, Bingzhe Wu, Yatao Bian, Yongzhe Chang, Xueqian Wang,\n  Peilin Zhao","authorsParsed":[["Wang","Haoyu",""],["Wu","Bingzhe",""],["Bian","Yatao",""],["Chang","Yongzhe",""],["Wang","Xueqian",""],["Zhao","Peilin",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 09:11:21 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 07:50:29 GMT"},{"version":"v3","created":"Mon, 26 Aug 2024 05:27:13 GMT"}],"updateDate":"2024-08-27","timestamp":1724145081000,"abstract":"  Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}