{"id":"2408.14467","title":"Explicit Inductive Inference using Large Language Models","authors":"Tianyang Liu, Tianyi Li, Liang Cheng, Mark Steedman","authorsParsed":[["Liu","Tianyang",""],["Li","Tianyi",""],["Cheng","Liang",""],["Steedman","Mark",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 17:58:17 GMT"}],"updateDate":"2024-08-27","timestamp":1724695097000,"abstract":"  Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}