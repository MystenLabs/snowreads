{"id":"2408.15297","title":"YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection","authors":"Xuanru Zhou, Anshul Kashyap, Steve Li, Ayati Sharma, Brittany Morin,\n  David Baquirin, Jet Vonk, Zoe Ezzes, Zachary Miller, Maria Luisa Gorno\n  Tempini, Jiachen Lian, Gopala Krishna Anumanchipalli","authorsParsed":[["Zhou","Xuanru",""],["Kashyap","Anshul",""],["Li","Steve",""],["Sharma","Ayati",""],["Morin","Brittany",""],["Baquirin","David",""],["Vonk","Jet",""],["Ezzes","Zoe",""],["Miller","Zachary",""],["Tempini","Maria Luisa Gorno",""],["Lian","Jiachen",""],["Anumanchipalli","Gopala Krishna",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 11:31:12 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 08:53:18 GMT"},{"version":"v3","created":"Sun, 15 Sep 2024 06:20:19 GMT"}],"updateDate":"2024-09-17","timestamp":1724758272000,"abstract":"  Dysfluent speech detection is the bottleneck for disordered speech analysis\nand spoken language learning. Current state-of-the-art models are governed by\nrule-based systems which lack efficiency and robustness, and are sensitive to\ntemplate design. In this paper, we propose YOLO-Stutter: a first end-to-end\nmethod that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes\nimperfect speech-text alignment as input, followed by a spatial feature\naggregator, and a temporal dependency extractor to perform region-wise boundary\nand class predictions. We also introduce two dysfluency corpus, VCTK-Stutter\nand VCTK-TTS, that simulate natural spoken dysfluencies including repetition,\nblock, missing, replacement, and prolongation. Our end-to-end method achieves\nstate-of-the-art performance with a minimum number of trainable parameters for\non both simulated data and real aphasia speech. Code and datasets are\nopen-sourced at https://github.com/rorizzz/YOLO-Stutter\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}