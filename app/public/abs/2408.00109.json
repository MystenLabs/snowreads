{"id":"2408.00109","title":"Back to the Continuous Attractor","authors":"\\'Abel S\\'agodi, Guillermo Mart\\'in-S\\'anchez, Piotr Sok\\'o\\l, Il\n  Memming Park","authorsParsed":[["Ságodi","Ábel",""],["Martín-Sánchez","Guillermo",""],["Sokół","Piotr",""],["Park","Il Memming",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 18:37:05 GMT"}],"updateDate":"2024-08-02","timestamp":1722451025000,"abstract":"  Continuous attractors offer a unique class of solutions for storing\ncontinuous-valued variables in recurrent system states for indefinitely long\ntime intervals. Unfortunately, continuous attractors suffer from severe\nstructural instability in general--they are destroyed by most infinitesimal\nchanges of the dynamical law that defines them. This fragility limits their\nutility especially in biological systems as their recurrent dynamics are\nsubject to constant perturbations. We observe that the bifurcations from\ncontinuous attractors in theoretical neuroscience models display various\nstructurally stable forms. Although their asymptotic behaviors to maintain\nmemory are categorically distinct, their finite-time behaviors are similar. We\nbuild on the persistent manifold theory to explain the commonalities between\nbifurcations from and approximations of continuous attractors. Fast-slow\ndecomposition analysis uncovers the persistent manifold that survives the\nseemingly destructive bifurcation. Moreover, recurrent neural networks trained\non analog memory tasks display approximate continuous attractors with predicted\nslow manifold structures. Therefore, continuous attractors are functionally\nrobust and remain useful as a universal analogy for understanding analog\nmemory.\n","subjects":["Quantitative Biology/Neurons and Cognition","Computing Research Repository/Neural and Evolutionary Computing","Nonlinear Sciences/Adaptation and Self-Organizing Systems"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}