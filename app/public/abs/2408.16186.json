{"id":"2408.16186","title":"Single-Loop Deterministic and Stochastic Interior-Point Algorithms for\n  Nonlinearly Constrained Optimization","authors":"Frank E. Curtis and Xin Jiang and Qi Wang","authorsParsed":[["Curtis","Frank E.",""],["Jiang","Xin",""],["Wang","Qi",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 00:50:35 GMT"}],"updateDate":"2024-08-30","timestamp":1724892635000,"abstract":"  An interior-point algorithm framework is proposed, analyzed, and tested for\nsolving nonlinearly constrained continuous optimization problems. The main\nsetting of interest is when the objective and constraint functions may be\nnonlinear and/or nonconvex, and when constraint values and derivatives are\ntractable to compute, but objective function values and derivatives can only be\nestimated. The algorithm is intended primarily for a setting that is similar\nfor stochastic-gradient methods for unconstrained optimization, namely, the\nsetting when stochastic-gradient estimates are available and employed in place\nof gradients of the objective, and when no objective function values (nor\nestimates of them) are employed. This is achieved by the interior-point\nframework having a single-loop structure rather than the nested-loop structure\nthat is typical of contemporary interior-point methods. For completeness,\nconvergence guarantees for the framework are provided both for deterministic\nand stochastic settings. Numerical experiments show that the algorithm yields\ngood performance on a large set of test problems.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}