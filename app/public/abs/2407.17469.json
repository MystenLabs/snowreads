{"id":"2407.17469","title":"I Could've Asked That: Reformulating Unanswerable Questions","authors":"Wenting Zhao, Ge Gao, Claire Cardie, Alexander M. Rush","authorsParsed":[["Zhao","Wenting",""],["Gao","Ge",""],["Cardie","Claire",""],["Rush","Alexander M.",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:59:07 GMT"}],"updateDate":"2024-07-25","timestamp":1721843947000,"abstract":"  When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}