{"id":"2408.15903","title":"LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments","authors":"Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston\n  Tan, Dongkyu Choi, Bo Xiong, Bo Ai","authorsParsed":[["Chen","Ruirui",""],["Jiang","Weifeng",""],["Qin","Chengwei",""],["Rawal","Ishaan Singh",""],["Tan","Cheston",""],["Choi","Dongkyu",""],["Xiong","Bo",""],["Ai","Bo",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 16:15:45 GMT"}],"updateDate":"2024-08-29","timestamp":1724861745000,"abstract":"  The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}