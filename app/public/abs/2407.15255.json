{"id":"2407.15255","title":"Explaining Decisions of Agents in Mixed-Motive Games","authors":"Maayan Orner, Oleg Maksimov, Akiva Kleinerman, Charles Ortiz, Sarit\n  Kraus","authorsParsed":[["Orner","Maayan",""],["Maksimov","Oleg",""],["Kleinerman","Akiva",""],["Ortiz","Charles",""],["Kraus","Sarit",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 19:56:04 GMT"}],"updateDate":"2024-07-23","timestamp":1721591764000,"abstract":"  In recent years, agents have become capable of communicating seamlessly via\nnatural language and navigating in environments that involve cooperation and\ncompetition, a fact that can introduce social dilemmas. Due to the interleaving\nof cooperation and competition, understanding agents' decision-making in such\nenvironments is challenging, and humans can benefit from obtaining\nexplanations. However, such environments and scenarios have rarely been\nexplored in the context of explainable AI. While some explanation methods for\ncooperative environments can be applied in mixed-motive setups, they do not\naddress inter-agent competition, cheap-talk, or implicit communication by\nactions. In this work, we design explanation methods to address these issues.\nThen, we proceed to demonstrate their effectiveness and usefulness for humans,\nusing a non-trivial mixed-motive game as a test case. Lastly, we establish\ngenerality and demonstrate the applicability of the methods to other games,\nincluding one where we mimic human game actions using large language models.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}