{"id":"2407.02437","title":"Parameter Matching Attack: Enhancing Practical Applicability of\n  Availability Attacks","authors":"Yu Zhe, Jun Sakuma","authorsParsed":[["Zhe","Yu",""],["Sakuma","Jun",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:15:12 GMT"}],"updateDate":"2024-07-03","timestamp":1719940512000,"abstract":"  The widespread use of personal data for training machine learning models\nraises significant privacy concerns, as individuals have limited control over\nhow their public data is subsequently utilized. Availability attacks have\nemerged as a means for data owners to safeguard their data by desning\nimperceptible perturbations that degrade model performance when incorporated\ninto training datasets. However, existing availability attacks exhibit\nlimitations in practical applicability, particularly when only a portion of the\ndata can be perturbed. To address this challenge, we propose a novel\navailability attack approach termed Parameter Matching Attack (PMA). PMA is the\nfirst availability attack that works when only a portion of data can be\nperturbed. PMA optimizes perturbations so that when the model is trained on a\nmixture of clean and perturbed data, the resulting model will approach a model\ndesigned to perform poorly. Experimental results across four datasets\ndemonstrate that PMA outperforms existing methods, achieving significant model\nperformance degradation when a part of the training data is perturbed. Our code\nis available in the supplementary.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}