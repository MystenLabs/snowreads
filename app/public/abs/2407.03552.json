{"id":"2407.03552","title":"Vision Mamba for Classification of Breast Ultrasound Images","authors":"Ali Nasiri-Sarvi, Mahdi S. Hosseini, Hassan Rivaz","authorsParsed":[["Nasiri-Sarvi","Ali",""],["Hosseini","Mahdi S.",""],["Rivaz","Hassan",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 00:21:47 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 04:37:16 GMT"}],"updateDate":"2024-09-18","timestamp":1720052507000,"abstract":"  Mamba-based models, VMamba and Vim, are a recent family of vision encoders\nthat offer promising performance improvements in many computer vision tasks.\nThis paper compares Mamba-based models with traditional Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) using the breast ultrasound BUSI\ndataset and Breast Ultrasound B dataset. Our evaluation, which includes\nmultiple runs of experiments and statistical significance analysis,\ndemonstrates that some of the Mamba-based architectures often outperform CNN\nand ViT models with statistically significant results. For example, in the B\ndataset, the best Mamba-based models have a 1.98\\% average AUC and a 5.0\\%\naverage Accuracy improvement compared to the best non-Mamba-based model in this\nstudy. These Mamba-based models effectively capture long-range dependencies\nwhile maintaining some inductive biases, making them suitable for applications\nwith limited data. The code is available at\n\\url{https://github.com/anasiri/BU-Mamba}\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}