{"id":"2408.14101","title":"Estimating Causal Effects from Learned Causal Networks","authors":"Anna Raichev, Alexander Ihler, Jin Tian, and Rina Dechter","authorsParsed":[["Raichev","Anna",""],["Ihler","Alexander",""],["Tian","Jin",""],["Dechter","Rina",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 08:39:09 GMT"},{"version":"v2","created":"Tue, 27 Aug 2024 09:54:04 GMT"}],"updateDate":"2024-08-28","timestamp":1724661549000,"abstract":"  The standard approach to answering an identifiable causal-effect query (e.g.,\n$P(Y|do(X)$) when given a causal diagram and observational data is to first\ngenerate an estimand, or probabilistic expression over the observable\nvariables, which is then evaluated using the observational data. In this paper,\nwe propose an alternative paradigm for answering causal-effect queries over\ndiscrete observable variables. We propose to instead learn the causal Bayesian\nnetwork and its confounding latent variables directly from the observational\ndata. Then, efficient probabilistic graphical model (PGM) algorithms can be\napplied to the learned model to answer queries. Perhaps surprisingly, we show\nthat this \\emph{model completion} learning approach can be more effective than\nestimand approaches, particularly for larger models in which the estimand\nexpressions become computationally difficult.\n  We illustrate our method's potential using a benchmark collection of Bayesian\nnetworks and synthetically generated causal models.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}