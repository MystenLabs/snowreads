{"id":"2407.04020","title":"LLMAEL: Large Language Models are Good Context Augmenters for Entity\n  Linking","authors":"Amy Xin, Yunjia Qi, Zijun Yao, Fangwei Zhu, Kaisheng Zeng, Xu Bin, Lei\n  Hou, Juanzi Li","authorsParsed":[["Xin","Amy",""],["Qi","Yunjia",""],["Yao","Zijun",""],["Zhu","Fangwei",""],["Zeng","Kaisheng",""],["Bin","Xu",""],["Hou","Lei",""],["Li","Juanzi",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 15:55:13 GMT"},{"version":"v2","created":"Mon, 15 Jul 2024 12:47:39 GMT"}],"updateDate":"2024-07-16","timestamp":1720108513000,"abstract":"  Entity Linking (EL) models are well-trained at mapping mentions to their\ncorresponding entities according to a given context. However, EL models\nstruggle to disambiguate long-tail entities due to their limited training data.\nMeanwhile, large language models (LLMs) are more robust at interpreting\nuncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at\ngenerating correct entity IDs. Furthermore, training an LLM to perform EL is\ncost-intensive. Building upon these insights, we introduce LLM-Augmented Entity\nLinking LLMAEL, a plug-and-play approach to enhance entity linking through LLM\ndata augmentation. We leverage LLMs as knowledgeable context augmenters,\ngenerating mention-centered descriptions as additional input, while preserving\ntraditional EL models for task specific processing. Experiments on 6 standard\ndatasets show that the vanilla LLMAEL outperforms baseline EL models in most\ncases, while the fine-tuned LLMAEL set the new state-of-the-art results across\nall 6 benchmarks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}