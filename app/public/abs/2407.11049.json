{"id":"2407.11049","title":"Learning by the F-adjoint","authors":"Ahmed Boughammoura","authorsParsed":[["Boughammoura","Ahmed",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 13:49:25 GMT"}],"updateDate":"2024-07-17","timestamp":1720446565000,"abstract":"  A recent paper by Boughammoura (2023) describes the back-propagation\nalgorithm in terms of an alternative formulation called the F-adjoint method.\nIn particular, by the F-adjoint algorithm the computation of the loss gradient,\nwith respect to each weight within the network, is straightforward and can\nsimply be done. In this work, we develop and investigate this theoretical\nframework to improve some supervised learning algorithm for feed-forward neural\nnetwork. Our main result is that by introducing some neural dynamical model\ncombined by the gradient descent algorithm, we derived an equilibrium F-adjoint\nprocess which yields to some local learning rule for deep feed-forward networks\nsetting. Experimental results on MNIST and Fashion-MNIST datasets, demonstrate\nthat the proposed approach provide a significant improvements on the standard\nback-propagation training procedure.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}