{"id":"2408.03092","title":"Extend Model Merging from Fine-Tuned to Pre-Trained Large Language\n  Models via Weight Disentanglement","authors":"Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li","authorsParsed":[["Yu","Le",""],["Yu","Bowen",""],["Yu","Haiyang",""],["Huang","Fei",""],["Li","Yongbin",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 10:46:46 GMT"}],"updateDate":"2024-08-07","timestamp":1722941206000,"abstract":"  Merging Large Language Models (LLMs) aims to amalgamate multiple homologous\nLLMs into one with all the capabilities. Ideally, any LLMs sharing the same\nbackbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)\nwith minor parameter changes or Pre-Trained (PT) with substantial parameter\nshifts. However, existing methods often manually assign the model importance,\nrendering them feasible only for LLMs with similar parameter alterations, such\nas multiple FT LLMs. The diverse parameter changed ranges between FT and PT\nLLMs pose challenges for current solutions in empirically determining the\noptimal combination. In this paper, we make a pioneering effort to broaden the\napplicability of merging techniques from FT to PT LLMs. We initially examine\nthe efficacy of current methods in merging FT and PT LLMs, discovering that\nthey struggle to deal with PT LLMs. Subsequently, we introduce an approach\nbased on WeIght DisENtanglement (WIDEN) to effectively extend the merging\nscope, which first disentangles model weights into magnitude and direction\ncomponents, and then performs adaptive fusion by considering their respective\ncontributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with\ninstruction-following skills) with Sailor (a PT LLM with multilingual\nabilities) across 7B and 14B model scales. Results reveal that: (1) existing\nsolutions usually fail when merging Sailor, either losing both abilities or\nonly retaining instruction-following skills; (2) WIDEN successfully injects the\nmultilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in\nSoutheast Asian languages, achieving enhancements in the fundamental\ncapabilities. In light of previous research, we also merge multiple 13B FT LLMs\nand observe that WIDEN achieves a balanced amalgamation of instruction\nfollowing, mathematical reasoning, and code generation skills.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}