{"id":"2408.15265","title":"Multitask Fine-Tuning and Generative Adversarial Learning for Improved\n  Auxiliary Classification","authors":"Christopher Sun and Abishek Satish","authorsParsed":[["Sun","Christopher",""],["Satish","Abishek",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 20:05:54 GMT"}],"updateDate":"2024-08-29","timestamp":1723406754000,"abstract":"  In this study, we implement a novel BERT architecture for multitask\nfine-tuning on three downstream tasks: sentiment classification, paraphrase\ndetection, and semantic textual similarity prediction. Our model, Multitask\nBERT, incorporates layer sharing and a triplet architecture, custom sentence\npair tokenization, loss pairing, and gradient surgery. Such optimizations yield\na 0.516 sentiment classification accuracy, 0.886 paraphase detection accuracy,\nand 0.864 semantic textual similarity correlation on test data. We also apply\ngenerative adversarial learning to BERT, constructing a conditional generator\nmodel that maps from latent space to create fake embeddings in\n$\\mathbb{R}^{768}$. These fake embeddings are concatenated with real BERT\nembeddings and passed into a discriminator model for auxiliary classification.\nUsing this framework, which we refer to as AC-GAN-BERT, we conduct\nsemi-supervised sensitivity analyses to investigate the effect of increasing\namounts of unlabeled training data on AC-GAN-BERT's test accuracy. Overall,\naside from implementing a high-performing multitask classification system, our\nnovelty lies in the application of adversarial learning to construct a\ngenerator that mimics BERT. We find that the conditional generator successfully\nproduces rich embeddings with clear spatial correlation with class labels,\ndemonstrating avoidance of mode collapse. Our findings validate the GAN-BERT\napproach and point to future directions of generator-aided knowledge\ndistillation.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}