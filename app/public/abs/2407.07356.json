{"id":"2407.07356","title":"Video In-context Learning","authors":"Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, Jiang Bian","authorsParsed":[["Zhang","Wentao",""],["Guo","Junliang",""],["He","Tianyu",""],["Zhao","Li",""],["Xu","Linli",""],["Bian","Jiang",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 04:27:06 GMT"}],"updateDate":"2024-07-11","timestamp":1720585626000,"abstract":"  In-context learning for vision data has been underexplored compared with that\nin natural language. Previous works studied image in-context learning, urging\nmodels to generate a single image guided by demonstrations. In this paper, we\npropose and study video in-context learning, where the model starts from an\nexisting video clip and generates diverse potential future sequences, each\nsemantically guided by the prompted video demonstrations. To achieve this, we\nprovide a clear definition of the task, and train an autoregressive Transformer\non video datasets. We thoroughly analyze the effect of different datasets and\nrepresent frames as discrete tokens, and then model them by next token\npredictions. We design various evaluation metrics, including both objective and\nsubjective measures, to demonstrate the visual quality and semantic accuracy of\ngeneration results. Our model follows the scaling law and generates\nhigh-quality video clips that accurately align with the semantic guidance\nprovided by in-context examples.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}