{"id":"2407.06360","title":"CodeCSE: A Simple Multilingual Model for Code and Comment Sentence\n  Embeddings","authors":"Anthony Varkey, Siyuan Jiang, Weijing Huang","authorsParsed":[["Varkey","Anthony",""],["Jiang","Siyuan",""],["Huang","Weijing",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 20:01:35 GMT"}],"updateDate":"2024-07-10","timestamp":1720468895000,"abstract":"  Pretrained language models for code token embeddings are used in code search,\ncode clone detection, and other code-related tasks. Similarly, code function\nembeddings are useful in such tasks. However, there are no out-of-box models\nfor function embeddings in the current literature. So, this paper proposes\nCodeCSE, a contrastive learning model that learns embeddings for functions and\ntheir descriptions in one space. We evaluated CodeCSE using code search.\nCodeCSE's multi-lingual zero-shot approach is as efficient as the models\nfinetuned from GraphCodeBERT for specific languages. CodeCSE is open source at\nhttps://github.com/emu-se/codecse and the pretrained model is available at the\nHuggingFace public hub: https://huggingface.co/sjiang1/codecse\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/"}