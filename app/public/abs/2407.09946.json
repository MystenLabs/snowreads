{"id":"2407.09946","title":"Low-Rank Interconnected Adaptation Across Layers","authors":"Yibo Zhong, Yao Zhou","authorsParsed":[["Zhong","Yibo",""],["Zhou","Yao",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 17:03:16 GMT"}],"updateDate":"2024-07-16","timestamp":1720890196000,"abstract":"  Low-rank adaptation (LoRA), as one of the most well-known representative\nmethods of parameter-efficient fine-tuning, freezes the backbone model and\nintroduces parallel adapter modules to each layer of the model. These modules\nconsist of two low-rank trainable matrices: a low-dimension projector (LP) and\na high-dimension projector (HP) with their product approximating the change for\nupdating the model weight. However, LoRA's paired LP and HP per layer limit\nlearned weights to specific features, ignoring the varied information extracted\nby stacked layers in models like Transformers. By considering the differences\nbetween layers and establishing connections across them when learning the\nweights, we enhance the capture of relevant information for downstream tasks\nusing this interconnected adaptation when fine-tuning. Meanwhile, preserving\nthe unique characteristics of each layer and thus selectively mix the learning\ntraits of various layers according to a specific ratio can also be crucial in\ncertain tasks. In this paper, we propose Low-rank Interconnected adaptation\nacross layers (Lily). Specifically, we retain layer-specific LPs (local LPs)\nfor low-dimensional feature projection and unify all HPs into a model-wide\nglobal HP, thereby overcoming layer-specific constraints in LoRA. The global\nHP, layer-independent, supports multiple HP sub-modules, or inspired by Mixture\nof Experts (MoE), HP experts capturing learning traits across all layer depths.\nFor the ratio to mix all the experts, we use a router inspired by MoE to\nselectively adapt the features of different layers, thus obtaining a unique\nexpert distribution. We evaluated Lily on a wide range of downstream tasks and\nachieved state-of-the-art results, outperforming LoRA and a range of\ncompetitive methods. Code will be available at\nhttps://github.com/blameitonme1/lily.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}