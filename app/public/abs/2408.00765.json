{"id":"2408.00765","title":"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities","authors":"Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang,\n  Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, Xinchao Wang","authorsParsed":[["Yu","Weihao",""],["Yang","Zhengyuan",""],["Ren","Linfeng",""],["Li","Linjie",""],["Wang","Jianfeng",""],["Lin","Kevin",""],["Lin","Chung-Ching",""],["Liu","Zicheng",""],["Wang","Lijuan",""],["Wang","Xinchao",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:59:54 GMT"}],"updateDate":"2024-08-02","timestamp":1722535194000,"abstract":"  MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}