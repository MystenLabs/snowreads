{"id":"2408.12526","title":"Exploiting Student Parallelism for Low-latency GPU Inference of\n  BERT-like Models in Online Services","authors":"Weiyan Wang, Yilun Jin, Yiming Zhang, Victor Junqiu Wei, Han Tian, Li\n  Chen, Kai Chen","authorsParsed":[["Wang","Weiyan",""],["Jin","Yilun",""],["Zhang","Yiming",""],["Wei","Victor Junqiu",""],["Tian","Han",""],["Chen","Li",""],["Chen","Kai",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 16:31:32 GMT"}],"updateDate":"2024-08-23","timestamp":1724344292000,"abstract":"  Due to high accuracy, BERT-like models have been widely adopted by\ndiscriminative text mining and web searching. However, large BERT-like models\nsuffer from inefficient online inference, as they face the following two\nproblems on GPUs. First, they rely on the large model depth to achieve high\naccuracy, which linearly increases the sequential computation on GPUs. Second,\nstochastic and dynamic online workloads cause extra costs. In this paper, we\npresent Academus for low-latency online inference of BERT-like models. At the\ncore of Academus is the novel student parallelism, which adopts boosting\nensemble and stacking distillation to distill the original deep model into an\nequivalent group of parallel and shallow student models. This enables Academus\nto achieve the lower model depth (e.g., two layers) than baselines and\nconsequently the lowest inference latency without affecting the accuracy.For\noccasional workload bursts, it can temporarily decrease the number of students\nwith minimal accuracy loss to improve throughput. Additionally, it employs\nspecialized system designs for student parallelism to better handle stochastic\nonline workloads. We conduct comprehensive experiments to verify the\neffectiveness. The results show that Academus outperforms the baselines by\n4.1X~1.6X in latency without compromising accuracy, and achieves up to 22.27X\nhigher throughput for workload bursts.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}