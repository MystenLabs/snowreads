{"id":"2407.04600","title":"Understanding the Gains from Repeated Self-Distillation","authors":"Divyansh Pareek, Simon S. Du, Sewoong Oh","authorsParsed":[["Pareek","Divyansh",""],["Du","Simon S.",""],["Oh","Sewoong",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 15:48:34 GMT"}],"updateDate":"2024-07-08","timestamp":1720194514000,"abstract":"  Self-Distillation is a special type of knowledge distillation where the\nstudent model has the same architecture as the teacher model. Despite using the\nsame architecture and the same training data, self-distillation has been\nempirically observed to improve performance, especially when applied\nrepeatedly. For such a process, there is a fundamental question of interest:\nHow much gain is possible by applying multiple steps of self-distillation? To\ninvestigate this relative gain, we propose studying the simple but canonical\ntask of linear regression. Our analysis shows that the excess risk achieved by\nmulti-step self-distillation can significantly improve upon a single step of\nself-distillation, reducing the excess risk by a factor as large as $d$, where\n$d$ is the input dimension. Empirical results on regression tasks from the UCI\nrepository show a reduction in the learnt model's risk (MSE) by up to 47%.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}