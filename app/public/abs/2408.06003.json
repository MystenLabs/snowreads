{"id":"2408.06003","title":"LUT Tensor Core: Lookup Table Enables Efficient Low-Bit LLM Inference\n  Acceleration","authors":"Zhiwen Mo, Lei Wang, Jianyu Wei, Zhichen Zeng, Shijie Cao, Lingxiao\n  Ma, Naifeng Jing, Ting Cao, Jilong Xue, Fan Yang, Mao Yang","authorsParsed":[["Mo","Zhiwen",""],["Wang","Lei",""],["Wei","Jianyu",""],["Zeng","Zhichen",""],["Cao","Shijie",""],["Ma","Lingxiao",""],["Jing","Naifeng",""],["Cao","Ting",""],["Xue","Jilong",""],["Yang","Fan",""],["Yang","Mao",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 08:52:14 GMT"}],"updateDate":"2024-08-13","timestamp":1723452734000,"abstract":"  As large language model (LLM) inference demands ever-greater resources, there\nis a rapid growing trend of using low-bit weights to shrink memory usage and\nboost inference efficiency. However, these low-bit LLMs introduce the need for\nmixed-precision matrix multiplication (mpGEMM), which is a crucial yet\nunder-explored operation that involves multiplying lower-precision weights with\nhigher-precision activations. Unfortunately, current hardware does not natively\nsupport mpGEMM, resulting in indirect and inefficient dequantization-based\nimplementations.\n  To address the mpGEMM requirements in low-bit LLMs, we explored the lookup\ntable (LUT)-based approach for mpGEMM. However, a conventional LUT\nimplementation falls short of its potential. To fully harness the power of\nLUT-based mpGEMM, we introduce LUT Tensor Core, a software-hardware co-design\noptimized for low-bit LLM inference. Specifically, we introduce software-based\noperator fusion and table symmetrization techniques to optimize table\nprecompute and table storage, respectively. Then, LUT Tensor Core proposes the\nhardware design featuring an elongated tiling shape design to enhance table\nreuse and a bit-serial design to support various precision combinations in\nmpGEMM. Moreover, we design an end-to-end compilation stack with new\ninstructions for LUT-based mpGEMM, enabling efficient LLM compilation and\noptimizations. The evaluation on low-bit LLMs (e.g., BitNet, LLAMA) shows that\nLUT Tensor Core achieves more than a magnitude of improvements on both compute\ndensity and energy efficiency.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}