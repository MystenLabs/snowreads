{"id":"2407.19514","title":"Detached and Interactive Multimodal Learning","authors":"Yunfeng Fan, Wenchao Xu, Haozhao Wang, Junhong Liu, and Song Guo","authorsParsed":[["Fan","Yunfeng",""],["Xu","Wenchao",""],["Wang","Haozhao",""],["Liu","Junhong",""],["Guo","Song",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 15:38:58 GMT"}],"updateDate":"2024-07-30","timestamp":1722181138000,"abstract":"  Recently, Multimodal Learning (MML) has gained significant interest as it\ncompensates for single-modality limitations through comprehensive complementary\ninformation within multimodal data. However, traditional MML methods generally\nuse the joint learning framework with a uniform learning objective that can\nlead to the modality competition issue, where feedback predominantly comes from\ncertain modalities, limiting the full potential of others. In response to this\nchallenge, this paper introduces DI-MML, a novel detached MML framework\ndesigned to learn complementary information across modalities under the premise\nof avoiding modality competition. Specifically, DI-MML addresses competition by\nseparately training each modality encoder with isolated learning objectives. It\nfurther encourages cross-modal interaction via a shared classifier that defines\na common feature space and employing a dimension-decoupled unidirectional\ncontrastive (DUC) loss to facilitate modality-level knowledge transfer.\nAdditionally, to account for varying reliability in sample pairs, we devise a\ncertainty-aware logit weighting strategy to effectively leverage complementary\ninformation at the instance level during inference. Extensive experiments\nconducted on audio-visual, flow-image, and front-rear view datasets show the\nsuperior performance of our proposed method. The code is released at\nhttps://github.com/fanyunfeng-bit/DI-MML.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}