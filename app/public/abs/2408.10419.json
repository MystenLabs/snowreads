{"id":"2408.10419","title":"Second-Order Forward-Mode Automatic Differentiation for Optimization","authors":"Adam D. Cobb, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin, Barak A. Pearlmutter,\n  Susmit Jha","authorsParsed":[["Cobb","Adam D.",""],["Baydin","Atılım Güneş",""],["Pearlmutter","Barak A.",""],["Jha","Susmit",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 21:12:41 GMT"}],"updateDate":"2024-08-21","timestamp":1724101961000,"abstract":"  This paper introduces a second-order hyperplane search, a novel optimization\nstep that generalizes a second-order line search from a line to a\n$k$-dimensional hyperplane. This, combined with the forward-mode stochastic\ngradient method, yields a second-order optimization algorithm that consists of\nforward passes only, completely avoiding the storage overhead of\nbackpropagation. Unlike recent work that relies on directional derivatives (or\nJacobian--Vector Products, JVPs), we use hyper-dual numbers to jointly evaluate\nboth directional derivatives and their second-order quadratic terms. As a\nresult, we introduce forward-mode weight perturbation with Hessian information\n(FoMoH). We then use FoMoH to develop a novel generalization of line search by\nextending it to a hyperplane search. We illustrate the utility of this\nextension and how it might be used to overcome some of the recent challenges of\noptimizing machine learning models without backpropagation. Our code is\nopen-sourced at https://github.com/SRI-CSL/fomoh.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}