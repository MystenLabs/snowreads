{"id":"2408.10562","title":"Kalib: Markerless Hand-Eye Calibration with Keypoint Tracking","authors":"Tutian Tang, Minghao Liu, Wenqiang Xu, Cewu Lu","authorsParsed":[["Tang","Tutian",""],["Liu","Minghao",""],["Xu","Wenqiang",""],["Lu","Cewu",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 06:03:40 GMT"}],"updateDate":"2024-08-21","timestamp":1724133820000,"abstract":"  Hand-eye calibration involves estimating the transformation between the\ncamera and the robot. Traditional methods rely on fiducial markers, involving\nmuch manual labor and careful setup. Recent advancements in deep learning offer\nmarkerless techniques, but they present challenges, including the need for\nretraining networks for each robot, the requirement of accurate mesh models for\ndata generation, and the need to address the sim-to-real gap. In this letter,\nwe propose Kalib, an automatic and universal markerless hand-eye calibration\npipeline that leverages the generalizability of visual foundation models to\neliminate these barriers. In each calibration process, Kalib uses keypoint\ntracking and proprioceptive sensors to estimate the transformation between a\nrobot's coordinate space and its corresponding points in camera space. Our\nmethod does not require training new networks or access to mesh models. Through\nevaluations in simulation environments and the real-world dataset DROID, Kalib\ndemonstrates superior accuracy compared to recent baseline methods. This\napproach provides an effective and flexible calibration process for various\nrobot systems by simplifying setup and removing dependency on precise physical\nmarkers.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}