{"id":"2408.04313","title":"Better Locally Private Sparse Estimation Given Multiple Samples Per User","authors":"Yuheng Ma and Ke Jia and Hanfang Yang","authorsParsed":[["Ma","Yuheng",""],["Jia","Ke",""],["Yang","Hanfang",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 08:47:20 GMT"}],"updateDate":"2024-08-09","timestamp":1723106840000,"abstract":"  Previous studies yielded discouraging results for item-level locally\ndifferentially private linear regression with $s^*$-sparsity assumption, where\nthe minimax rate for $nm$ samples is $\\mathcal{O}(s^{*}d / nm\\varepsilon^2)$.\nThis can be challenging for high-dimensional data, where the dimension $d$ is\nextremely large. In this work, we investigate user-level locally differentially\nprivate sparse linear regression. We show that with $n$ users each contributing\n$m$ samples, the linear dependency of dimension $d$ can be eliminated, yielding\nan error upper bound of $\\mathcal{O}(s^{*2} / nm\\varepsilon^2)$. We propose a\nframework that first selects candidate variables and then conducts estimation\nin the narrowed low-dimensional space, which is extendable to general sparse\nestimation problems with tight error bounds. Experiments on both synthetic and\nreal datasets demonstrate the superiority of the proposed methods. Both the\ntheoretical and empirical results suggest that, with the same number of\nsamples, locally private sparse estimation is better conducted when multiple\nsamples per user are available.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Statistics/Methodology"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yVZcGMNiZFU-siDLm--pg5l2JP6t5UQnG98QXmWmbZA","pdfSize":"1833638"}
