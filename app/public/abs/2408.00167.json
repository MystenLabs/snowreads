{"id":"2408.00167","title":"Finch: Prompt-guided Key-Value Cache Compression","authors":"Giulio Corallo and Paolo Papotti","authorsParsed":[["Corallo","Giulio",""],["Papotti","Paolo",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 21:33:56 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 09:08:55 GMT"}],"updateDate":"2024-08-14","timestamp":1722461636000,"abstract":"  Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"KMxlcP3sOTi43G2kgweBiUJodAWV2VaXqCMYOjLGgB4","pdfSize":"919595","txDigest":"DJhcGuivVoXpQ5MaG5edNkTvjh8XxF7P5xt24E7kPmAc","endEpoch":"1","status":"CERTIFIED"}
