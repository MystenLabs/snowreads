{"id":"2407.08850","title":"UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset","authors":"Peitong Duan, Chin-yi Chen, Gang Li, Bjoern Hartmann, Yang Li","authorsParsed":[["Duan","Peitong",""],["Chen","Chin-yi",""],["Li","Gang",""],["Hartmann","Bjoern",""],["Li","Yang",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 20:18:19 GMT"},{"version":"v2","created":"Mon, 15 Jul 2024 18:46:08 GMT"},{"version":"v3","created":"Tue, 13 Aug 2024 23:41:43 GMT"}],"updateDate":"2024-08-15","timestamp":1720729099000,"abstract":"  Automated UI evaluation can be beneficial for the design process; for\nexample, to compare different UI designs, or conduct automated heuristic\nevaluation. LLM-based UI evaluation, in particular, holds the promise of\ngeneralizability to a wide variety of UI types and evaluation tasks. However,\ncurrent LLM-based techniques do not yet match the performance of human\nevaluators. We hypothesize that automatic evaluation can be improved by\ncollecting a targeted UI feedback dataset and then using this dataset to\nenhance the performance of general-purpose LLMs. We present a targeted dataset\nof 3,059 design critiques and quality ratings for 983 mobile UIs, collected\nfrom seven experienced designers. We carried out an in-depth analysis to\ncharacterize the dataset's features. We then applied this dataset to achieve a\n55% performance gain in LLM-generated UI feedback via various few-shot and\nvisual prompting techniques. We also discuss future applications of this\ndataset, including training a reward model for generative UI techniques, and\nfine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}