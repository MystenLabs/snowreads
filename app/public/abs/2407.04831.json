{"id":"2407.04831","title":"Code Hallucination","authors":"Mirza Masfiqur Rahman, Ashish Kundu","authorsParsed":[["Rahman","Mirza Masfiqur",""],["Kundu","Ashish",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 19:37:37 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 01:01:47 GMT"}],"updateDate":"2024-08-09","timestamp":1720208257000,"abstract":"  Generative models such as large language models are extensively used as code\ncopilots and for whole program generation. However, the programs they generate\noften have questionable correctness, authenticity and reliability in terms of\nintegration as they might not follow the user requirements, provide incorrect\nand/or nonsensical outputs, or even contain semantic/syntactic errors - overall\nknown as LLM hallucination. In this work, we present several types of code\nhallucination. We have generated such hallucinated code manually using large\nlanguage models. We also present a technique - HallTrigger, in order to\ndemonstrate efficient ways of generating arbitrary code hallucination. Our\nmethod leverages 3 different dynamic attributes of LLMs to craft prompts that\ncan successfully trigger hallucinations from models without the need to access\nmodel architecture or parameters. Results from popular blackbox models suggest\nthat HallTrigger is indeed effective and the pervasive LLM hallucination have\nsheer impact on software development.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"2gl1VJ6IJQV6-t9lBzyrlNflpAdaiRaIgsCXNK3FFU8","pdfSize":"517704"}
