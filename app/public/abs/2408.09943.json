{"id":"2408.09943","title":"Calibrating Noise for Group Privacy in Subsampled Mechanisms","authors":"Yangfan Jiang, Xinjian Luo, Yin Yang, and Xiaokui Xiao","authorsParsed":[["Jiang","Yangfan",""],["Luo","Xinjian",""],["Yang","Yin",""],["Xiao","Xiaokui",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 12:32:50 GMT"},{"version":"v2","created":"Sat, 24 Aug 2024 13:00:41 GMT"}],"updateDate":"2024-08-27","timestamp":1724070770000,"abstract":"  Given a group size m and a sensitive dataset D, group privacy (GP) releases\ninformation about D with the guarantee that the adversary cannot infer with\nhigh confidence whether the underlying data is D or a neighboring dataset D'\nthat differs from D by m records. GP generalizes the well-established notion of\ndifferential privacy (DP) for protecting individuals' privacy; in particular,\nwhen m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the\nsensitive aggregate information of a group of up to m individuals, e.g., the\naverage annual income among members of a yacht club. Despite its longstanding\npresence in the research literature and its promising applications, GP is often\ntreated as an afterthought, with most approaches first developing a DP\nmechanism and then using a generic conversion to adapt it for GP, treating the\nDP solution as a black box. As we point out in the paper, this methodology is\nsuboptimal when the underlying DP solution involves subsampling, e.g., in the\nclassic DP-SGD method for training deep learning models. In this case, the\nDP-to-GP conversion is overly pessimistic in its analysis, leading to low\nutility in the published results under GP.\n  Motivated by this, we propose a novel analysis framework that provides tight\nprivacy accounting for subsampled GP mechanisms. Instead of converting a\nblack-box DP mechanism to GP, our solution carefully analyzes and utilizes the\ninherent randomness in subsampled mechanisms, leading to a substantially\nimproved bound on the privacy loss with respect to GP. The proposed solution\napplies to a wide variety of foundational mechanisms with subsampling.\nExtensive experiments with real datasets demonstrate that compared to the\nbaseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise\nreductions of over an order of magnitude in several practical settings,\nincluding deep neural network training.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}