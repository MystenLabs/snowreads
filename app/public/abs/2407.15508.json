{"id":"2407.15508","title":"Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners","authors":"Yifei Gao, Jie Ou, Lei Wang, Fanhua Shang, Jaji Wu, Jun Cheng","authorsParsed":[["Gao","Yifei",""],["Ou","Jie",""],["Wang","Lei",""],["Shang","Fanhua",""],["Wu","Jaji",""],["Cheng","Jun",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 09:45:16 GMT"},{"version":"v2","created":"Thu, 15 Aug 2024 15:22:57 GMT"}],"updateDate":"2024-08-16","timestamp":1721641516000,"abstract":"  Large Language Models (LLMs) showcase remarkable performance and robust\ndeductive capabilities, yet their expansive size complicates deployment and\nraises environmental concerns due to substantial resource consumption. The\nrecent development of a quantization technique known as Learnable\nSingular-value Increment (LSI) has addressed some of these quantization\nchallenges. Leveraging insights from LSI and our extensive research, we have\ndeveloped innovative methods that enhance the performance of quantized LLMs,\nparticularly in low-bit settings. Our methods consistently deliver\nstate-of-the-art results across various quantization scenarios and offer deep\ntheoretical insights into the quantization process, elucidating the potential\nof quantized models for widespread application.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}