{"id":"2408.12734","title":"Towards measuring fairness in speech recognition: Fair-Speech dataset","authors":"Irina-Elena Veliche, Zhuangqun Huang, Vineeth Ayyat Kochaniyan, Fuchun\n  Peng, Ozlem Kalinli, Michael L. Seltzer","authorsParsed":[["Veliche","Irina-Elena",""],["Huang","Zhuangqun",""],["Kochaniyan","Vineeth Ayyat",""],["Peng","Fuchun",""],["Kalinli","Ozlem",""],["Seltzer","Michael L.",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 20:55:17 GMT"}],"updateDate":"2024-08-26","timestamp":1724360117000,"abstract":"  The current public datasets for speech recognition (ASR) tend not to focus\nspecifically on the fairness aspect, such as performance across different\ndemographic groups. This paper introduces a novel dataset, Fair-Speech, a\npublicly released corpus to help researchers evaluate their ASR models for\naccuracy across a diverse set of self-reported demographic information, such as\nage, gender, ethnicity, geographic variation and whether the participants\nconsider themselves native English speakers. Our dataset includes approximately\n26.5K utterances in recorded speech by 593 people in the United States, who\nwere paid to record and submit audios of themselves saying voice commands. We\nalso provide ASR baselines, including on models trained on transcribed and\nuntranscribed social media videos and open source models.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}