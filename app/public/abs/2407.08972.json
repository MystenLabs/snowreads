{"id":"2407.08972","title":"Revealing the Dark Secrets of Extremely Large Kernel ConvNets on\n  Robustness","authors":"Honghao Chen, Yurong Zhang, Xiaokun Feng, Xiangxiang Chu, Kaiqi Huang","authorsParsed":[["Chen","Honghao",""],["Zhang","Yurong",""],["Feng","Xiaokun",""],["Chu","Xiangxiang",""],["Huang","Kaiqi",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 03:55:20 GMT"}],"updateDate":"2024-07-15","timestamp":1720756520000,"abstract":"  Robustness is a vital aspect to consider when deploying deep learning models\ninto the wild. Numerous studies have been dedicated to the study of the\nrobustness of vision transformers (ViTs), which have dominated as the\nmainstream backbone choice for vision tasks since the dawn of 2020s. Recently,\nsome large kernel convnets make a comeback with impressive performance and\nefficiency. However, it still remains unclear whether large kernel networks are\nrobust and the attribution of their robustness. In this paper, we first conduct\na comprehensive evaluation of large kernel convnets' robustness and their\ndifferences from typical small kernel counterparts and ViTs on six diverse\nrobustness benchmark datasets. Then to analyze the underlying factors behind\ntheir strong robustness, we design experiments from both quantitative and\nqualitative perspectives to reveal large kernel convnets' intriguing properties\nthat are completely different from typical convnets. Our experiments\ndemonstrate for the first time that pure CNNs can achieve exceptional\nrobustness comparable or even superior to that of ViTs. Our analysis on\nocclusion invariance, kernel attention patterns and frequency characteristics\nprovide novel insights into the source of robustness.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}