{"id":"2408.05035","title":"Examining the Behavior of LLM Architectures Within the Framework of\n  Standardized National Exams in Brazil","authors":"Marcelo Sartori Locatelli, Matheus Prado Miranda, Igor Joaquim da\n  Silva Costa, Matheus Torres Prates, Victor Thom\\'e, Mateus Zaparoli Monteiro,\n  Tomas Lacerda, Adriana Pagano, Eduardo Rios Neto, Wagner Meira Jr., Virgilio\n  Almeida","authorsParsed":[["Locatelli","Marcelo Sartori",""],["Miranda","Matheus Prado",""],["Costa","Igor Joaquim da Silva",""],["Prates","Matheus Torres",""],["Thom√©","Victor",""],["Monteiro","Mateus Zaparoli",""],["Lacerda","Tomas",""],["Pagano","Adriana",""],["Neto","Eduardo Rios",""],["Meira","Wagner","Jr."],["Almeida","Virgilio",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 12:47:28 GMT"}],"updateDate":"2024-08-12","timestamp":1723207648000,"abstract":"  The Exame Nacional do Ensino M\\'edio (ENEM) is a pivotal test for Brazilian\nstudents, required for admission to a significant number of universities in\nBrazil. The test consists of four objective high-school level tests on Math,\nHumanities, Natural Sciences and Languages, and one writing essay. Students'\nanswers to the test and to the accompanying socioeconomic status questionnaire\nare made public every year (albeit anonymized) due to transparency policies\nfrom the Brazilian Government. In the context of large language models (LLMs),\nthese data lend themselves nicely to comparing different groups of humans with\nAI, as we can have access to human and machine answer distributions. We\nleverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4,\nand MariTalk, a model trained using Portuguese data, to humans, aiming to\nascertain how their answers relate to real societal groups and what that may\nreveal about the model biases. We divide the human groups by using\nsocioeconomic status (SES), and compare their answer distribution with LLMs for\neach question and for the essay. We find no significant biases when comparing\nLLM performance to humans on the multiple-choice Brazilian Portuguese tests, as\nthe distance between model and human answers is mostly determined by the human\naccuracy. A similar conclusion is found by looking at the generated text as,\nwhen analyzing the essays, we observe that human and LLM essays differ in a few\nkey factors, one being the choice of words where model essays were easily\nseparable from human ones. The texts also differ syntactically, with LLM\ngenerated essays exhibiting, on average, smaller sentences and less thought\nunits, among other differences. These results suggest that, for Brazilian\nPortuguese in the ENEM context, LLM outputs represent no group of humans, being\nsignificantly different from the answers from Brazilian students across all\ntests.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/"}