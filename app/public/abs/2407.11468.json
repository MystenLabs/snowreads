{"id":"2407.11468","title":"AU-vMAE: Knowledge-Guide Action Units Detection via Video Masked\n  Autoencoder","authors":"Qiaoqiao Jin, Rui Shi, Yishun Dou, Bingbing Ni","authorsParsed":[["Jin","Qiaoqiao",""],["Shi","Rui",""],["Dou","Yishun",""],["Ni","Bingbing",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 08:07:47 GMT"}],"updateDate":"2024-07-17","timestamp":1721117267000,"abstract":"  Current Facial Action Unit (FAU) detection methods generally encounter\ndifficulties due to the scarcity of labeled video training data and the limited\nnumber of training face IDs, which renders the trained feature extractor\ninsufficient coverage for modeling the large diversity of inter-person facial\nstructures and movements. To explicitly address the above challenges, we\npropose a novel video-level pre-training scheme by fully exploring the\nmulti-label property of FAUs in the video as well as the temporal label\nconsistency. At the heart of our design is a pre-trained video feature\nextractor based on the video-masked autoencoder together with a fine-tuning\nnetwork that jointly completes the multi-level video FAUs analysis tasks,\n\\emph{i.e.} integrating both video-level and frame-level FAU detections, thus\ndramatically expanding the supervision set from sparse FAUs annotations to ALL\nvideo frames including masked ones. Moreover, we utilize inter-frame and\nintra-frame AU pair state matrices as prior knowledge to guide network training\ninstead of traditional Graph Neural Networks, for better temporal supervision.\nOur approach demonstrates substantial enhancement in performance compared to\nthe existing state-of-the-art methods used in BP4D and DISFA FAUs datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}