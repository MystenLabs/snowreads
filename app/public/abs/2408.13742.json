{"id":"2408.13742","title":"Multi-modal Integrated Prediction and Decision-making with Adaptive\n  Interaction Modality Explorations","authors":"Tong Li, Lu Zhang, Sikang Liu, Shaojie Shen","authorsParsed":[["Li","Tong",""],["Zhang","Lu",""],["Liu","Sikang",""],["Shen","Shaojie",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 07:04:23 GMT"},{"version":"v2","created":"Wed, 28 Aug 2024 06:44:47 GMT"}],"updateDate":"2024-08-29","timestamp":1724569463000,"abstract":"  Navigating dense and dynamic environments poses a significant challenge for\nautonomous driving systems, owing to the intricate nature of multimodal\ninteraction, wherein the actions of various traffic participants and the\nautonomous vehicle are complex and implicitly coupled. In this paper, we\npropose a novel framework, Multi-modal Integrated predictioN and\nDecision-making (MIND), which addresses the challenges by efficiently\ngenerating joint predictions and decisions covering multiple distinctive\ninteraction modalities. Specifically, MIND leverages learning-based scenario\npredictions to obtain integrated predictions and decisions with\nsocial-consistent interaction modality and utilizes a modality-aware dynamic\nbranching mechanism to generate scenario trees that efficiently capture the\nevolutions of distinctive interaction modalities with low variation of\ninteraction uncertainty along the planning horizon. The scenario trees are\nseamlessly utilized by the contingency planning under interaction uncertainty\nto obtain clear and considerate maneuvers accounting for multi-modal\nevolutions. Comprehensive experimental results in the closed-loop simulation\nbased on the real-world driving dataset showcase superior performance to other\nstrong baselines under various driving contexts.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}