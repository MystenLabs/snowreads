{"id":"2407.13657","title":"FuLG: 150B Romanian Corpus for Language Model Pretraining","authors":"Vlad-Andrei B\\u{a}doiu and Mihai-Valentin Dumitru and Alexandru M.\n  Gherghescu and Alexandru Agache and Costin Raiciu","authorsParsed":[["BÄƒdoiu","Vlad-Andrei",""],["Dumitru","Mihai-Valentin",""],["Gherghescu","Alexandru M.",""],["Agache","Alexandru",""],["Raiciu","Costin",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 16:32:48 GMT"}],"updateDate":"2024-07-19","timestamp":1721320368000,"abstract":"  Research in the field of language models is rapidly evolving, with many open\nmodels being released to the public. Openly available pretraining corpora\nusually focus on only a handful of languages, with many others either missing\ncompletely or extremely underrepresented. In this report, we introduce FuLG, a\nhundred-fifty-billion-token Romanian corpus extracted from CommonCrawl. We\npresent our methodology for filtering FuLG and compare it via ablation studies\nagainst existing Romanian corpora.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}