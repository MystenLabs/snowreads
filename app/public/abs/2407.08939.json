{"id":"2407.08939","title":"LightenDiffusion: Unsupervised Low-Light Image Enhancement with\n  Latent-Retinex Diffusion Models","authors":"Hai Jiang, Ao Luo, Xiaohong Liu, Songchen Han, Shuaicheng Liu","authorsParsed":[["Jiang","Hai",""],["Luo","Ao",""],["Liu","Xiaohong",""],["Han","Songchen",""],["Liu","Shuaicheng",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 02:54:43 GMT"}],"updateDate":"2024-07-15","timestamp":1720752883000,"abstract":"  In this paper, we propose a diffusion-based unsupervised framework that\nincorporates physically explainable Retinex theory with diffusion models for\nlow-light image enhancement, named LightenDiffusion. Specifically, we present a\ncontent-transfer decomposition network that performs Retinex decomposition\nwithin the latent space instead of image space as in previous approaches,\nenabling the encoded features of unpaired low-light and normal-light images to\nbe decomposed into content-rich reflectance maps and content-free illumination\nmaps. Subsequently, the reflectance map of the low-light image and the\nillumination map of the normal-light image are taken as input to the diffusion\nmodel for unsupervised restoration with the guidance of the low-light feature,\nwhere a self-constrained consistency loss is further proposed to eliminate the\ninterference of normal-light content on the restored results to improve overall\nvisual quality. Extensive experiments on publicly available real-world\nbenchmarks show that the proposed LightenDiffusion outperforms state-of-the-art\nunsupervised competitors and is comparable to supervised methods while being\nmore generalizable to various scenes. Our code is available at\nhttps://github.com/JianghaiSCU/LightenDiffusion.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}