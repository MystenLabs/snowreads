{"id":"2408.12095","title":"uMedSum: A Unified Framework for Advancing Medical Abstractive\n  Summarization","authors":"Aishik Nagar, Yutong Liu, Andy T. Liu, Viktor Schlegel, Vijay Prakash\n  Dwivedi, Arun-Kumar Kaliya-Perumal, Guna Pratheep Kalanchiam, Yili Tang,\n  Robby T. Tan","authorsParsed":[["Nagar","Aishik",""],["Liu","Yutong",""],["Liu","Andy T.",""],["Schlegel","Viktor",""],["Dwivedi","Vijay Prakash",""],["Kaliya-Perumal","Arun-Kumar",""],["Kalanchiam","Guna Pratheep",""],["Tang","Yili",""],["Tan","Robby T.",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 03:08:49 GMT"},{"version":"v2","created":"Mon, 26 Aug 2024 02:26:31 GMT"}],"updateDate":"2024-08-27","timestamp":1724296129000,"abstract":"  Medical abstractive summarization faces the challenge of balancing\nfaithfulness and informativeness. Current methods often sacrifice key\ninformation for faithfulness or introduce confabulations when prioritizing\ninformativeness. While recent advancements in techniques like in-context\nlearning (ICL) and fine-tuning have improved medical summarization, they often\noverlook crucial aspects such as faithfulness and informativeness without\nconsidering advanced methods like model reasoning and self-improvement.\nMoreover, the field lacks a unified benchmark, hindering systematic evaluation\ndue to varied metrics and datasets. This paper addresses these gaps by\npresenting a comprehensive benchmark of six advanced abstractive summarization\nmethods across three diverse datasets using five standardized metrics. Building\non these findings, we propose uMedSum, a modular hybrid summarization framework\nthat introduces novel approaches for sequential confabulation removal followed\nby key missing information addition, ensuring both faithfulness and\ninformativeness. Our work improves upon previous GPT-4-based state-of-the-art\n(SOTA) medical summarization methods, significantly outperforming them in both\nquantitative metrics and qualitative domain expert evaluations. Notably, we\nachieve an average relative performance improvement of 11.8% in reference-free\nmetrics over the previous SOTA. Doctors prefer uMedSum's summaries 6 times more\nthan previous SOTA in difficult cases where there are chances of confabulations\nor missing information. These results highlight uMedSum's effectiveness and\ngeneralizability across various datasets and metrics, marking a significant\nadvancement in medical summarization.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qXgYapRhbkIIecKUN2WXEzI1CC3z4ETyPMoQqNCAcno","pdfSize":"474955"}
