{"id":"2407.11798","title":"PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined\n  Speculation","authors":"Branden Butler, Sixing Yu, Arya Mazaheri, and Ali Jannesari","authorsParsed":[["Butler","Branden",""],["Yu","Sixing",""],["Mazaheri","Arya",""],["Jannesari","Ali",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:52:02 GMT"}],"updateDate":"2024-07-17","timestamp":1721141522000,"abstract":"  Inference of Large Language Models (LLMs) across computer clusters has become\na focal point of research in recent times, with many acceleration techniques\ntaking inspiration from CPU speculative execution. These techniques reduce\nbottlenecks associated with memory bandwidth, but also increase end-to-end\nlatency per inference run, requiring high speculation acceptance rates to\nimprove performance. Combined with a variable rate of acceptance across tasks,\nspeculative inference techniques can result in reduced performance.\nAdditionally, pipeline-parallel designs require many user requests to maintain\nmaximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative\nacceleration technique to reduce inter-token latency and improve system\nutilization for single-request scenarios while also improving tolerance to low\nspeculation acceptance rates and low-bandwidth interconnects. PipeInfer\nexhibits up to a 2.15$\\times$ improvement in generation speed over standard\nspeculative inference. PipeInfer achieves its improvement through Continuous\nAsynchronous Speculation and Early Inference Cancellation, the former improving\nlatency and generation speed by running single-token inference simultaneously\nwith several speculative runs, while the latter improves speed and latency by\nskipping the computation of invalidated runs, even in the middle of inference.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}