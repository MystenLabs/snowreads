{"id":"2407.02854","title":"Universal Gloss-level Representation for Gloss-free Sign Language\n  Translation and Production","authors":"Eui Jun Hwang, Sukmin Cho, Huije Lee, Youngwoo Yoon, Jong C. Park","authorsParsed":[["Hwang","Eui Jun",""],["Cho","Sukmin",""],["Lee","Huije",""],["Yoon","Youngwoo",""],["Park","Jong C.",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 07:12:36 GMT"}],"updateDate":"2024-07-04","timestamp":1719990756000,"abstract":"  Sign language, essential for the deaf and hard-of-hearing, presents unique\nchallenges in translation and production due to its multimodal nature and the\ninherent ambiguity in mapping sign language motion to spoken language words.\nPrevious methods often rely on gloss annotations, requiring time-intensive\nlabor and specialized expertise in sign language. Gloss-free methods have\nemerged to address these limitations, but they often depend on external sign\nlanguage data or dictionaries, failing to completely eliminate the need for\ngloss annotations. There is a clear demand for a comprehensive approach that\ncan supplant gloss annotations and be utilized for both Sign Language\nTranslation (SLT) and Sign Language Production (SLP). We introduce Universal\nGloss-level Representation (UniGloR), a unified and self-supervised solution\nfor both SLT and SLP, trained on multiple datasets including PHOENIX14T,\nHow2Sign, and NIASL2021. Our results demonstrate UniGloR's effectiveness in the\ntranslation and production tasks. We further report an encouraging result for\nthe Sign Language Recognition (SLR) on previously unseen data. Our study\nsuggests that self-supervised learning can be made in a unified manner, paving\nthe way for innovative and practical applications in future research.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}