{"id":"2408.08261","title":"mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental\n  Health Text Analysis","authors":"Dae-young Kim (1), Rebecca Hwa (2), Muhammad Mahbubur Rahman (1) ((1)\n  Children's National Hospital, Washington, DC, (2) George Washington\n  University, Washington, DC)","authorsParsed":[["Kim","Dae-young",""],["Hwa","Rebecca",""],["Rahman","Muhammad Mahbubur",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 17:01:57 GMT"}],"updateDate":"2024-08-16","timestamp":1723741317000,"abstract":"  This paper introduces mhGPT, a lightweight generative pre-trained transformer\ntrained on mental health-related social media and PubMed articles. Fine-tuned\nfor specific mental health tasks, mhGPT was evaluated under limited hardware\nconstraints and compared with state-of-the-art models like MentaLLaMA and\nGemma. Despite having only 1.98 billion parameters and using just 5% of the\ndataset, mhGPT outperformed larger models and matched the performance of models\ntrained on significantly more data. The key contributions include integrating\ndiverse mental health data, creating a custom tokenizer, and optimizing a\nsmaller architecture for low-resource settings. This research could advance\nAI-driven mental health care, especially in areas with limited computing power.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}