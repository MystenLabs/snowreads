{"id":"2408.01423","title":"Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM\n  Auto-Prompting","authors":"Xiangyu Zhao, Chengqian Ma","authorsParsed":[["Zhao","Xiangyu",""],["Ma","Chengqian",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 17:59:42 GMT"}],"updateDate":"2024-08-05","timestamp":1722621582000,"abstract":"  Large Language Models (LLMs) exhibit remarkable proficiency in addressing a\ndiverse array of tasks within the Natural Language Processing (NLP) domain,\nwith various prompt design strategies significantly augmenting their\ncapabilities. However, these prompts, while beneficial, each possess inherent\nlimitations. The primary prompt design methodologies are twofold: The first,\nexemplified by the Chain of Thought (CoT), involves manually crafting prompts\nspecific to individual datasets, hence termed Expert-Designed Prompts (EDPs).\nOnce these prompts are established, they are unalterable, and their\neffectiveness is capped by the expertise of the human designers. When applied\nto LLMs, the static nature of EDPs results in a uniform approach to both simple\nand complex problems within the same dataset, leading to the inefficient use of\ntokens for straightforward issues. The second method involves prompts\nautonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which\nprovide tailored solutions to specific problems, mitigating the limitations of\nEDPs. However, LDPs may encounter a decline in performance when tackling\ncomplex problems due to the potential for error accumulation during the\nsolution planning process. To address these challenges, we have conceived a\nnovel Prompt Recursive Search (PRS) framework that leverages the LLM to\ngenerate solutions specific to the problem, thereby conserving tokens. The\nframework incorporates an assessment of problem complexity and an adjustable\nstructure, ensuring a reduction in the likelihood of errors. We have\nsubstantiated the efficacy of PRS framework through extensive experiments using\nLLMs with different numbers of parameters across a spectrum of datasets in\nvarious domains. Compared to the CoT method, the PRS method has increased the\naccuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%\nimprovement.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3gZ3HQ7HnBngsAQmUi_VGuMvLD7zLfOMiCR1HkXOxi0","pdfSize":"1473531"}
