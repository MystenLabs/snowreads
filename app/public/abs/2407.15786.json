{"id":"2407.15786","title":"Concept-Based Interpretable Reinforcement Learning with Limited to No\n  Human Labels","authors":"Zhuorui Ye and Stephanie Milani and Geoffrey J. Gordon and Fei Fang","authorsParsed":[["Ye","Zhuorui",""],["Milani","Stephanie",""],["Gordon","Geoffrey J.",""],["Fang","Fei",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 16:46:33 GMT"}],"updateDate":"2024-07-23","timestamp":1721666793000,"abstract":"  Recent advances in reinforcement learning (RL) have predominantly leveraged\nneural network-based policies for decision-making, yet these models often lack\ninterpretability, posing challenges for stakeholder comprehension and trust.\nConcept bottleneck models offer an interpretable alternative by integrating\nhuman-understandable concepts into neural networks. However, a significant\nlimitation in prior work is the assumption that human annotations for these\nconcepts are readily available during training, necessitating continuous\nreal-time input from human annotators. To overcome this limitation, we\nintroduce a novel training scheme that enables RL algorithms to efficiently\nlearn a concept-based policy by only querying humans to label a small set of\ndata, or in the extreme case, without any human labels. Our algorithm,\nLICORICE, involves three main contributions: interleaving concept learning and\nRL training, using a concept ensembles to actively select informative data\npoints for labeling, and decorrelating the concept data with a simple strategy.\nWe show how LICORICE reduces manual labeling efforts to to 500 or fewer concept\nlabels in three environments. Finally, we present an initial study to explore\nhow we can use powerful vision-language models to infer concepts from raw\nvisual inputs without explicit labels at minimal cost to performance.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}