{"id":"2407.02431","title":"On the Robustness of Graph Reduction Against GNN Backdoor","authors":"Yuxuan Zhu, Michael Mandulak, Kerui Wu, George Slota, Yuseok Jeon,\n  Ka-Ho Chow, Lei Yu","authorsParsed":[["Zhu","Yuxuan",""],["Mandulak","Michael",""],["Wu","Kerui",""],["Slota","George",""],["Jeon","Yuseok",""],["Chow","Ka-Ho",""],["Yu","Lei",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:08:38 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 02:11:47 GMT"}],"updateDate":"2024-07-10","timestamp":1719940118000,"abstract":"  Graph Neural Networks (GNNs) are gaining popularity across various domains\ndue to their effectiveness in learning graph-structured data. Nevertheless,\nthey have been shown to be susceptible to backdoor poisoning attacks, which\npose serious threats to real-world applications. Meanwhile, graph reduction\ntechniques, including coarsening and sparsification, which have long been\nemployed to improve the scalability of large graph computational tasks, have\nrecently emerged as effective methods for accelerating GNN training on\nlarge-scale graphs. However, the current development and deployment of graph\nreduction techniques for large graphs overlook the potential risks of data\npoisoning attacks against GNNs. It is not yet clear how graph reduction\ninteracts with existing backdoor attacks. This paper conducts a thorough\nexamination of the robustness of graph reduction methods in scalable GNN\ntraining in the presence of state-of-the-art backdoor attacks. We performed a\ncomprehensive robustness analysis across six coarsening methods and six\nsparsification methods for graph reduction, under three GNN backdoor attacks\nagainst three GNN architectures. Our findings indicate that the effectiveness\nof graph reduction methods in mitigating attack success rates varies\nsignificantly, with some methods even exacerbating the attacks. Through\ndetailed analyses of triggers and poisoned nodes, we interpret our findings and\nenhance our understanding of how graph reduction influences robustness against\nbackdoor attacks. These results highlight the critical need for incorporating\nrobustness considerations in graph reduction for GNN training, ensuring that\nenhancements in computational efficiency do not compromise the security of GNN\nsystems.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jHD5GAT-rp0JYB8V-v4hgaYwMniPLRXkWuN7OgOQWMc","pdfSize":"965328","objectId":"0xbe96ac3dcb4e999051e5c1df846f7302661708941cd3a9c62524f1993ae5ab32","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
