{"id":"2408.08724","title":"ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target\n  Language","authors":"Yongkang Liu, Feng Shi, Daling Wang, Yifei Zhang and Hinrich Sch\\\"utze","authorsParsed":[["Liu","Yongkang",""],["Shi","Feng",""],["Wang","Daling",""],["Zhang","Yifei",""],["Sch√ºtze","Hinrich",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 13:11:53 GMT"}],"updateDate":"2024-08-19","timestamp":1723813913000,"abstract":"  Although large language models(LLMs) show amazing capabilities, among various\nexciting applications discovered for LLMs fall short in other low-resource\nlanguages. Besides, most existing methods depend on large-scale dialogue\ncorpora and thus building systems for dialogue generation in a zero-shot\nscenario remains a considerable challenge. To address this challenge, we\npropose a novel end-to-end zero-shot dialogue generation model ChatZero based\non cross-lingual code-switching method. First, we construct code-switching\nlanguage and pseudo-target language with placeholders. Then for cross-lingual\nsemantic transfer, we employ unsupervised contrastive learning to minimize the\nsemantics gap of the source language, code-switching language, and\npseudo-target language that are mutually positive examples in the high\ndimensional semantic space. Experiments on the multilingual DailyDialog and\nDSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\\% of the\noriginal performance under the zero-shot case compared to supervised learning,\nand achieve state-of-the-art performance compared with other baselines.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}