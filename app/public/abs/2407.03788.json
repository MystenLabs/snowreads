{"id":"2407.03788","title":"Meta-optimized Angular Margin Contrastive Framework for Video-Language\n  Representation Learning","authors":"Thong Nguyen, Yi Bin, Xiaobao Wu, Xinshuai Dong, Zhiyuan Hu, Khoi Le,\n  Cong-Duy Nguyen, See-Kiong Ng, and Luu Anh Tuan","authorsParsed":[["Nguyen","Thong",""],["Bin","Yi",""],["Wu","Xiaobao",""],["Dong","Xinshuai",""],["Hu","Zhiyuan",""],["Le","Khoi",""],["Nguyen","Cong-Duy",""],["Ng","See-Kiong",""],["Tuan","Luu Anh",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 09:52:17 GMT"},{"version":"v2","created":"Sat, 20 Jul 2024 03:15:26 GMT"}],"updateDate":"2024-07-23","timestamp":1720086737000,"abstract":"  Data quality stands at the forefront of deciding the effectiveness of\nvideo-language representation learning. However, video-text pairs in previous\ndata typically do not align perfectly with each other, which might lead to\nvideo-language representations that do not accurately reflect cross-modal\nsemantics. Moreover, previous data also possess an uneven distribution of\nconcepts, thereby hampering the downstream performance across unpopular\nsubjects. To address these problems, we propose a contrastive objective with a\nsubtractive angular margin to regularize cross-modal representations in their\neffort to reach perfect similarity. Furthermore, to adapt to the non-uniform\nconcept distribution, we propose a multi-layer perceptron (MLP)-parameterized\nweighting function that maps loss values to sample weights which enable dynamic\nadjustment of the model's focus throughout the training. With the training\nguided by a small amount of unbiased meta-data and augmented by video-text data\ngenerated by large vision-language model, we improve video-language\nrepresentations and achieve superior performances on commonly used video\nquestion answering and text-video retrieval datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}