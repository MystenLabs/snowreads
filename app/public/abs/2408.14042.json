{"id":"2408.14042","title":"PAGE: Parametric Generative Explainer for Graph Neural Network","authors":"Yang Qiu and Wei Liu and Jun Wang and Ruixuan Li","authorsParsed":[["Qiu","Yang",""],["Liu","Wei",""],["Wang","Jun",""],["Li","Ruixuan",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 06:39:49 GMT"},{"version":"v2","created":"Fri, 6 Sep 2024 08:13:09 GMT"}],"updateDate":"2024-09-09","timestamp":1724654389000,"abstract":"  This article introduces PAGE, a parameterized generative interpretive\nframework. PAGE is capable of providing faithful explanations for any graph\nneural network without necessitating prior knowledge or internal details.\nSpecifically, we train the auto-encoder to generate explanatory substructures\nby designing appropriate training strategy. Due to the dimensionality reduction\nof features in the latent space of the auto-encoder, it becomes easier to\nextract causal features leading to the model's output, which can be easily\nemployed to generate explanations. To accomplish this, we introduce an\nadditional discriminator to capture the causality between latent causal\nfeatures and the model's output. By designing appropriate optimization\nobjectives, the well-trained discriminator can be employed to constrain the\nencoder in generating enhanced causal features. Finally, these features are\nmapped to substructures of the input graph through the decoder to serve as\nexplanations. Compared to existing methods, PAGE operates at the sample scale\nrather than nodes or edges, eliminating the need for perturbation or encoding\nprocesses as seen in previous methods. Experimental results on both\nartificially synthesized and real-world datasets demonstrate that our approach\nnot only exhibits the highest faithfulness and accuracy but also significantly\noutperforms baseline models in terms of efficiency.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}