{"id":"2408.06481","title":"UniT: Unified Tactile Representation for Robot Learning","authors":"Zhengtong Xu, Raghava Uppuluri, Xinwei Zhang, Cael Fitch, Philip Glen\n  Crandall, Wan Shou, Dongyi Wang, Yu She","authorsParsed":[["Xu","Zhengtong",""],["Uppuluri","Raghava",""],["Zhang","Xinwei",""],["Fitch","Cael",""],["Crandall","Philip Glen",""],["Shou","Wan",""],["Wang","Dongyi",""],["She","Yu",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 20:29:09 GMT"}],"updateDate":"2024-08-14","timestamp":1723494549000,"abstract":"  UniT is a novel approach to tactile representation learning, using VQVAE to\nlearn a compact latent space and serve as the tactile representation. It uses\ntactile images obtained from a single simple object to train the representation\nwith transferability and generalizability. This tactile representation can be\nzero-shot transferred to various downstream tasks, including perception tasks\nand manipulation policy learning. Our benchmarking on an in-hand 3D pose\nestimation task shows that UniT outperforms existing visual and tactile\nrepresentation learning methods. Additionally, UniT's effectiveness in policy\nlearning is demonstrated across three real-world tasks involving diverse\nmanipulated objects and complex robot-object-environment interactions. Through\nextensive experimentation, UniT is shown to be a simple-to-train,\nplug-and-play, yet widely effective method for tactile representation learning.\nFor more details, please refer to our open-source repository\nhttps://github.com/ZhengtongXu/UniT and the project website\nhttps://zhengtongxu.github.io/unifiedtactile.github.io/.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}