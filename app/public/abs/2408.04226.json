{"id":"2408.04226","title":"Evaluating Language Model Math Reasoning via Grounding in Educational\n  Curricula","authors":"Li Lucy, Tal August, Rose E. Wang, Luca Soldaini, Courtney Allison,\n  Kyle Lo","authorsParsed":[["Lucy","Li",""],["August","Tal",""],["Wang","Rose E.",""],["Soldaini","Luca",""],["Allison","Courtney",""],["Lo","Kyle",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 05:28:34 GMT"},{"version":"v2","created":"Fri, 9 Aug 2024 19:51:05 GMT"}],"updateDate":"2024-08-13","timestamp":1723094914000,"abstract":"  Our work presents a novel angle for evaluating language models' (LMs)\nmathematical abilities, by investigating whether they can discern skills and\nconcepts enabled by math content. We contribute two datasets: one consisting of\n385 fine-grained descriptions of K-12 math skills and concepts, or standards,\nfrom Achieve the Core (ATC), and another of 9.9K problems labeled with these\nstandards (MathFish). Working with experienced teachers, we find that LMs\nstruggle to tag and verify standards linked to problems, and instead predict\nlabels that are close to ground truth, but differ in subtle ways. We also show\nthat LMs often generate problems that do not fully align with standards\ndescribed in prompts. Finally, we categorize problems in GSM8k using math\nstandards, allowing us to better understand why some problems are more\ndifficult to solve for models than others.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UVOadNhq3_cIHjRtIhO_chKKQsK-Nq8SoCpuhG-2y_M","pdfSize":"1713910"}
