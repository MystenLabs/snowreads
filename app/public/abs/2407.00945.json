{"id":"2407.00945","title":"Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models:\n  Enhancing Performance and Reducing Inference Costs","authors":"Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Matthew B. Blaschko,\n  Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang","authorsParsed":[["Liu","Enshu",""],["Zhu","Junyi",""],["Lin","Zinan",""],["Ning","Xuefei",""],["Blaschko","Matthew B.",""],["Yan","Shengen",""],["Dai","Guohao",""],["Yang","Huazhong",""],["Wang","Yu",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 03:57:35 GMT"}],"updateDate":"2024-07-02","timestamp":1719806255000,"abstract":"  The rapid advancement of large language models (LLMs) has led to\narchitectures with billions to trillions of parameters, posing significant\ndeployment challenges due to their substantial demands on memory, processing\npower, and energy consumption. Sparse Mixture-of-Experts (SMoE) architectures\nhave emerged as a solution, activating only a subset of parameters per token,\nthereby achieving faster inference while maintaining performance. However, SMoE\nmodels still face limitations in broader deployment due to their large\nparameter counts and significant GPU memory requirements. In this work, we\nintroduce a gradient-free evolutionary strategy named EEP (Efficient Expert\nP}runing) to enhance the pruning of experts in SMoE models. EEP relies solely\non model inference (i.e., no gradient computation) and achieves greater\nsparsity while maintaining or even improving performance on downstream tasks.\nEEP can be used to reduce both the total number of experts (thus saving GPU\nmemory) and the number of active experts (thus accelerating inference). For\nexample, we demonstrate that pruning up to 75% of experts in Mixtral\n$8\\times7$B-Instruct results in a substantial reduction in parameters with\nminimal performance loss. Remarkably, we observe improved performance on\ncertain tasks, such as a significant increase in accuracy on the SQuAD dataset\n(from 53.4% to 75.4%), when pruning half of the experts. With these results,\nEEP not only lowers the barrier to deploying SMoE models,but also challenges\nthe conventional understanding of model pruning by showing that fewer experts\ncan lead to better task-specific performance without any fine-tuning. Code is\navailable at https://github.com/imagination-research/EEP.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}