{"id":"2408.11396","title":"MoE-LPR: Multilingual Extension of Large Language Models through\n  Mixture-of-Experts with Language Priors Routing","authors":"Hao Zhou, Zhijun Wang, Shujian Huang, Xin Huang, Xue Han, Junlan Feng,\n  Chao Deng, Weihua Luo, Jiajun Chen","authorsParsed":[["Zhou","Hao",""],["Wang","Zhijun",""],["Huang","Shujian",""],["Huang","Xin",""],["Han","Xue",""],["Feng","Junlan",""],["Deng","Chao",""],["Luo","Weihua",""],["Chen","Jiajun",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 07:43:49 GMT"}],"updateDate":"2024-08-22","timestamp":1724226229000,"abstract":"  Large Language Models (LLMs) are often English-centric due to the\ndisproportionate distribution of languages in their pre-training data.\nEnhancing non-English language capabilities through post-pretraining often\nresults in catastrophic forgetting of the ability of original languages.\nPrevious methods either achieve good expansion with severe forgetting or slight\nforgetting with poor expansion, indicating the challenge of balancing language\nexpansion while preventing forgetting. In this paper, we propose a method\ncalled MoE-LPR (Mixture-of-Experts with Language Priors Routing) to alleviate\nthis problem. MoE-LPR employs a two-stage training approach to enhance the\nmultilingual capability. First, the model is post-pretrained into a\nMixture-of-Experts (MoE) architecture by upcycling, where all the original\nparameters are frozen and new experts are added. In this stage, we focus\nimproving the ability on expanded languages, without using any original\nlanguage data. Then, the model reviews the knowledge of the original languages\nwith replay data amounting to less than 1% of post-pretraining, where we\nincorporate language priors routing to better recover the abilities of the\noriginal languages. Evaluations on multiple benchmarks show that MoE-LPR\noutperforms other post-pretraining methods. Freezing original parameters\npreserves original language knowledge while adding new experts preserves the\nlearning ability. Reviewing with LPR enables effective utilization of\nmultilingual knowledge within the parameters. Additionally, the MoE\narchitecture maintains the same inference overhead while increasing total model\nparameters. Extensive experiments demonstrate MoE-LPR's effectiveness in\nimproving expanded languages and preserving original language proficiency with\nsuperior scalability. Code and scripts are freely available at\nhttps://github.com/zjwang21/MoE-LPR.git.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}