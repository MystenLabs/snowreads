{"id":"2407.14412","title":"DEAL: Disentangle and Localize Concept-level Explanations for VLMs","authors":"Tang Li, Mengmeng Ma, and Xi Peng","authorsParsed":[["Li","Tang",""],["Ma","Mengmeng",""],["Peng","Xi",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 15:39:19 GMT"}],"updateDate":"2024-07-22","timestamp":1721403559000,"abstract":"  Large pre-trained Vision-Language Models (VLMs) have become ubiquitous\nfoundational components of other models and downstream tasks. Although\npowerful, our empirical results reveal that such models might not be able to\nidentify fine-grained concepts. Specifically, the explanations of VLMs with\nrespect to fine-grained concepts are entangled and mislocalized. To address\nthis issue, we propose to DisEntAngle and Localize (DEAL) the concept-level\nexplanations for VLMs without human annotations. The key idea is encouraging\nthe concept-level explanations to be distinct while maintaining consistency\nwith category-level explanations. We conduct extensive experiments and ablation\nstudies on a wide range of benchmark datasets and vision-language models. Our\nempirical results demonstrate that the proposed method significantly improves\nthe concept-level explanations of the model in terms of disentanglability and\nlocalizability. Surprisingly, the improved explainability alleviates the\nmodel's reliance on spurious correlations, which further benefits the\nprediction accuracy.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"bo14SwTQBAPPoawxwAH6-2kemkj9bbHCFkO7a_Vzbwo","pdfSize":"4209911","objectId":"0x716a44f6930de8b6fb5d183a55b48c81ffc27f9c74893b321e1ff0533ee4cb22","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
