{"id":"2408.05558","title":"Object Re-identification via Spatial-temporal Fusion Networks and Causal\n  Identity Matching","authors":"Hye-Geun Kim, Yong-Hyuk Moon and Yeong-Jun Cho","authorsParsed":[["Kim","Hye-Geun",""],["Moon","Yong-Hyuk",""],["Cho","Yeong-Jun",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 13:50:43 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 11:25:31 GMT"}],"updateDate":"2024-08-23","timestamp":1723297843000,"abstract":"  Object re-identification (ReID) in large camera networks faces numerous\nchallenges. First, the similar appearances of objects degrade ReID performance,\na challenge that needs to be addressed by existing appearance-based ReID\nmethods. Second, most ReID studies are performed in laboratory settings and do\nnot consider real-world scenarios. To overcome these challenges, we introduce a\nnovel ReID framework that leverages a spatial-temporal fusion network and\ncausal identity matching (CIM). Our framework estimates camera network topology\nusing a proposed adaptive Parzen window and combines appearance features with\nspatial-temporal cues within the fusion network. This approach has demonstrated\noutstanding performance across several datasets, including VeRi776, Vehicle-3I,\nand Market-1501, achieving up to 99.70% rank-1 accuracy and 95.5% mAP.\nFurthermore, the proposed CIM approach, which dynamically assigns gallery sets\nbased on camera network topology, has further improved ReID accuracy and\nrobustness in real-world settings, evidenced by a 94.95% mAP and a 95.19% F1\nscore on the Vehicle-3I dataset. The experimental results support the\neffectiveness of incorporating spatial-temporal information and CIM for\nreal-world ReID scenarios, regardless of the data domain (e.g., vehicle,\nperson).\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}