{"id":"2408.07238","title":"Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge\n  Distillation Approach","authors":"Tong Wang, K. Sudhir and Dat Hong","authorsParsed":[["Wang","Tong",""],["Sudhir","K.",""],["Hong","Dat",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 23:59:36 GMT"}],"updateDate":"2024-08-15","timestamp":1723593576000,"abstract":"  Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior\nperformance in complex human-like interactions. But they are costly, or too\nlarge for edge devices such as smartphones and harder to self-host, leading to\nsecurity and privacy concerns. This paper introduces a novel interpretable\nknowledge distillation approach to enhance the performance of smaller, more\neconomical LLMs that firms can self-host. We study this problem in the context\nof building a customer service agent aimed at achieving high customer\nsatisfaction through goal-oriented dialogues. Unlike traditional knowledge\ndistillation, where the \"student\" model learns directly from the \"teacher\"\nmodel's responses via fine-tuning, our interpretable \"strategy\" teaching\napproach involves the teacher providing strategies to improve the student's\nperformance in various scenarios. This method alternates between a \"scenario\ngeneration\" step and a \"strategies for improvement\" step, creating a customized\nlibrary of scenarios and optimized strategies for automated prompting. The\nmethod requires only black-box access to both student and teacher models; hence\nit can be used without manipulating model parameters. In our customer service\napplication, the method improves performance, and the learned strategies are\ntransferable to other LLMs and scenarios beyond the training set. The method's\ninterpretabilty helps safeguard against potential harms through human audit.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"PpUld7MdXXbwM8g2KsKPzOZBLNrTjgQjQDv9oBIYBVY","pdfSize":"967806"}
