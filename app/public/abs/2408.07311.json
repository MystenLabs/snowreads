{"id":"2408.07311","title":"MultiSurf-GPT: Facilitating Context-Aware Reasoning with Large-Scale\n  Language Models for Multimodal Surface Sensing","authors":"Yongquan Hu, Black Sun, Pengcheng An, Zhuying Li, Wen Hu, Aaron J.\n  Quigley","authorsParsed":[["Hu","Yongquan",""],["Sun","Black",""],["An","Pengcheng",""],["Li","Zhuying",""],["Hu","Wen",""],["Quigley","Aaron J.",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 06:07:01 GMT"}],"updateDate":"2024-08-15","timestamp":1723615621000,"abstract":"  Surface sensing is widely employed in health diagnostics, manufacturing and\nsafety monitoring. Advances in mobile sensing affords this potential for\ncontext awareness in mobile computing, typically with a single sensing\nmodality. Emerging multimodal large-scale language models offer new\nopportunities. We propose MultiSurf-GPT, which utilizes the advanced\ncapabilities of GPT-4o to process and interpret diverse modalities (radar,\nmicroscope and multispectral data) uniformly based on prompting strategies\n(zero-shot and few-shot prompting). We preliminarily validated our framework by\nusing MultiSurf-GPT to identify low-level information, and to infer high-level\ncontext-aware analytics, demonstrating the capability of augmenting\ncontext-aware insights. This framework shows promise as a tool to expedite the\ndevelopment of more complex context-aware applications in the future, providing\na faster, more cost-effective, and integrated solution.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/"}