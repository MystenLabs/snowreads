{"id":"2408.05456","title":"Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph\n  Representation","authors":"Wenbo Shang and Xuliang Zhu and Xin Huang","authorsParsed":[["Shang","Wenbo",""],["Zhu","Xuliang",""],["Huang","Xin",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 06:35:11 GMT"}],"updateDate":"2024-08-13","timestamp":1723271711000,"abstract":"  Unified graph representation learning aims to produce node embeddings, which\ncan be applied to multiple downstream applications. However, existing studies\nbased on graph neural networks and language models either suffer from the\nlimitations of numerous training needed toward specific downstream predictions\nor have shallow semantic features. In this work, we propose a novel Path-LLM\nmodel to learn unified graph representation, which leverages a powerful large\nlanguage model (LLM) to incorporate our proposed path features. Our Path-LLM\nframework consists of several well-designed techniques. First, we develop a new\nmechanism of long-to-short shortest path (L2SP) selection, which covers\nessential connections between different dense groups. An in-depth comparison of\ndifferent path selection plans is offered to illustrate the strength of our\ndesigned L2SP. Then, we design path textualization to obtain L2SP-based\ntraining texts. Next, we feed the texts into a self-supervised LLM training\nprocess to learn embeddings. Extensive experiments on benchmarks validate the\nsuperiority of Path-LLM against the state-of-the-art WalkLM method on two\nclassical graph learning tasks (node classification and link prediction) and\none NP-hard graph query processing task (keyword search), meanwhile saving more\nthan 90% of training paths.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}