{"id":"2408.02544","title":"Caution for the Environment: Multimodal Agents are Susceptible to\n  Environmental Distractions","authors":"Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng\n  Zhang, Hai Zhao","authorsParsed":[["Ma","Xinbei",""],["Wang","Yiting",""],["Yao","Yao",""],["Yuan","Tongxin",""],["Zhang","Aston",""],["Zhang","Zhuosheng",""],["Zhao","Hai",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 15:16:22 GMT"}],"updateDate":"2024-08-06","timestamp":1722870982000,"abstract":"  This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in the graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general setting is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing our simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness\n(i.e., action accuracy) of multimodal agents, our findings indicate that these\nagents are prone to environmental distractions, resulting in unfaithful\nbehaviors. Furthermore, we switch to the adversarial perspective and implement\nenvironment injection, demonstrating that such unfaithfulness can be exploited,\nleading to unexpected risks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}