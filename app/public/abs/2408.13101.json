{"id":"2408.13101","title":"Functional Tensor Decompositions for Physics-Informed Neural Networks","authors":"Sai Karthikeya Vemuri, Tim B\\\"uchner, Julia Niebling, Joachim Denzler","authorsParsed":[["Vemuri","Sai Karthikeya",""],["BÃ¼chner","Tim",""],["Niebling","Julia",""],["Denzler","Joachim",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 14:24:43 GMT"}],"updateDate":"2024-08-26","timestamp":1724423083000,"abstract":"  Physics-Informed Neural Networks (PINNs) have shown continuous and increasing\npromise in approximating partial differential equations (PDEs), although they\nremain constrained by the curse of dimensionality. In this paper, we propose a\ngeneralized PINN version of the classical variable separable method. To do\nthis, we first show that, using the universal approximation theorem, a\nmultivariate function can be approximated by the outer product of neural\nnetworks, whose inputs are separated variables. We leverage tensor\ndecomposition forms to separate the variables in a PINN setting. By employing\nCanonic Polyadic (CP), Tensor-Train (TT), and Tucker decomposition forms within\nthe PINN framework, we create robust architectures for learning multivariate\nfunctions from separate neural networks connected by outer products. Our\nmethodology significantly enhances the performance of PINNs, as evidenced by\nimproved results on complex high-dimensional PDEs, including the 3d Helmholtz\nand 5d Poisson equations, among others. This research underscores the potential\nof tensor decomposition-based variably separated PINNs to surpass the\nstate-of-the-art, offering a compelling solution to the dimensionality\nchallenge in PDE approximation.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ds8HSz8FjxDNKT7dkSnHVLmnzSQTAfLxxQTOG0TqcgU","pdfSize":"1647893"}
