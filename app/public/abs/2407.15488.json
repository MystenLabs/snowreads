{"id":"2407.15488","title":"DiffX: Guide Your Layout to Cross-Modal Generative Modeling","authors":"Zeyu Wang, Jingyu Lin, Yifei Qian, Yi Huang, Shicen Tian, Bosong Chai,\n  Juncan Deng, Qu Yang, Lan Du, Cunjian Chen, Yufei Guo, Kejie Huang","authorsParsed":[["Wang","Zeyu",""],["Lin","Jingyu",""],["Qian","Yifei",""],["Huang","Yi",""],["Tian","Shicen",""],["Chai","Bosong",""],["Deng","Juncan",""],["Yang","Qu",""],["Du","Lan",""],["Chen","Cunjian",""],["Guo","Yufei",""],["Huang","Kejie",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 09:05:16 GMT"},{"version":"v2","created":"Sun, 28 Jul 2024 11:57:25 GMT"},{"version":"v3","created":"Tue, 6 Aug 2024 12:54:41 GMT"},{"version":"v4","created":"Sun, 25 Aug 2024 02:14:33 GMT"}],"updateDate":"2024-08-27","timestamp":1721639116000,"abstract":"  Diffusion models have made significant strides in language-driven and\nlayout-driven image generation. However, most diffusion models are limited to\nvisible RGB image generation. In fact, human perception of the world is\nenriched by diverse viewpoints, such as chromatic contrast, thermal\nillumination, and depth information. In this paper, we introduce a novel\ndiffusion model for general layout-guided cross-modal generation, called DiffX.\nNotably, our DiffX presents a simple yet effective cross-modal generative\nmodeling pipeline, which conducts diffusion and denoising processes in the\nmodality-shared latent space. Moreover, we introduce the Joint-Modality\nEmbedder (JME) to enhance the interaction between layout and text conditions by\nincorporating a gated attention mechanism. To facilitate the user-instructed\ntraining, we construct the cross-modal image datasets with detailed text\ncaptions by the Large-Multimodal Model (LMM) and our human-in-the-loop\nrefinement. Through extensive experiments, our DiffX demonstrates robustness in\ncross-modal ''RGB+X'' image generation on FLIR, MFNet, and COME15K datasets,\nguided by various layout conditions. It also shows the potential for the\nadaptive generation of ''RGB+X+Y(+Z)'' images or more diverse modalities on\nCOME15K and MCXFace datasets. Our code and constructed cross-modal image\ndatasets are available at https://github.com/zeyuwang-zju/DiffX.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"k5t4CmUSZeBux9q0WojKx2xifrQLUzshHVaF3KOjSKk","pdfSize":"5055379"}
