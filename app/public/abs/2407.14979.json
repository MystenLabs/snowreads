{"id":"2407.14979","title":"RGB2Point: 3D Point Cloud Generation from Single RGB Images","authors":"Jae Joong Lee, Bedrich Benes","authorsParsed":[["Lee","Jae Joong",""],["Benes","Bedrich",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 21:06:33 GMT"}],"updateDate":"2024-07-23","timestamp":1721509593000,"abstract":"  We introduce RGB2Point, an unposed single-view RGB image to a 3D point cloud\ngeneration based on Transformer. RGB2Point takes an input image of an object\nand generates a dense 3D point cloud. Contrary to prior works based on CNN\nlayers and diffusion denoising approaches, we use pre-trained Transformer\nlayers that are fast and generate high-quality point clouds with consistent\nquality over available categories. Our generated point clouds demonstrate high\nquality on a real-world dataset, as evidenced by improved Chamfer distance\n(51.15%) and Earth Mover's distance (45.96%) metrics compared to the current\nstate-of-the-art. Additionally, our approach shows a better quality on a\nsynthetic dataset, achieving better Chamfer distance (39.26%), Earth Mover's\ndistance (26.95%), and F-score (47.16%). Moreover, our method produces 63.1%\nmore consistent high-quality results across various object categories compared\nto prior works. Furthermore, RGB2Point is computationally efficient, requiring\nonly 2.3GB of VRAM to reconstruct a 3D point cloud from a single RGB image, and\nour implementation generates the results 15,133x faster than a SOTA\ndiffusion-based model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"87DDrorCiANEfi82j9YMZDVlJXhYC7tgcFgoMpcqi4c","pdfSize":"2643279"}
