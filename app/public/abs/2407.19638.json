{"id":"2407.19638","title":"From Pre-training Corpora to Large Language Models: What Factors\n  Influence LLM Performance in Causal Discovery Tasks?","authors":"Tao Feng, Lizhen Qu, Niket Tandon, Zhuang Li, Xiaoxi Kang, Gholamreza\n  Haffari","authorsParsed":[["Feng","Tao",""],["Qu","Lizhen",""],["Tandon","Niket",""],["Li","Zhuang",""],["Kang","Xiaoxi",""],["Haffari","Gholamreza",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 01:45:05 GMT"}],"updateDate":"2024-07-30","timestamp":1722217505000,"abstract":"  Recent advances in artificial intelligence have seen Large Language Models\n(LLMs) demonstrate notable proficiency in causal discovery tasks. This study\nexplores the factors influencing the performance of LLMs in causal discovery\ntasks. Utilizing open-source LLMs, we examine how the frequency of causal\nrelations within their pre-training corpora affects their ability to accurately\nrespond to causal discovery queries. Our findings reveal that a higher\nfrequency of causal mentions correlates with better model performance,\nsuggesting that extensive exposure to causal information during training\nenhances the models' causal discovery capabilities. Additionally, we\ninvestigate the impact of context on the validity of causal relations. Our\nresults indicate that LLMs might exhibit divergent predictions for identical\ncausal relations when presented in different contexts. This paper provides the\nfirst comprehensive analysis of how different factors contribute to LLM\nperformance in causal discovery tasks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}