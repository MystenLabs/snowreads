{"id":"2408.02945","title":"Self-Supervised Learning for Multi-Channel Neural Transducer","authors":"Atsushi Kojima","authorsParsed":[["Kojima","Atsushi",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 04:12:31 GMT"}],"updateDate":"2024-08-07","timestamp":1722917551000,"abstract":"  Self-supervised learning, such as with the wav2vec 2.0 framework\nsignificantly improves the accuracy of end-to-end automatic speech recognition\n(ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In\nthis work, we explored a self-supervised learning method for a multi-channel\nend-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel\nend-to-end ASR model, we focused on a multi-channel neural transducer. In\npre-training, we compared three different methods for feature quantization to\ntrain a multi-channel conformer audio encoder: joint quantization, feature-wise\nquantization and channel-wise quantization. In fine-tuning, we trained the\nmulti-channel conformer-transducer. All experiments were conducted using the\nfar-field in-house and CHiME-4 datasets. The results of the experiments showed\nthat feature-wise quantization was the most effective among the methods. We\nobserved a 66% relative reduction in character error rate compared with the\nmodel without any pre-training for the far-field in-house dataset.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}