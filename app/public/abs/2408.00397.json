{"id":"2408.00397","title":"In-Context Example Selection via Similarity Search Improves Low-Resource\n  Machine Translation","authors":"Armel Zebaze, Beno\\^it Sagot, Rachel Bawden","authorsParsed":[["Zebaze","Armel",""],["Sagot","Beno√Æt",""],["Bawden","Rachel",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 09:07:32 GMT"}],"updateDate":"2024-08-02","timestamp":1722503252000,"abstract":"  The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. In this paper, we focus\non machine translation (MT), a task that has been shown to benefit from\nin-context translation examples. However no systematic studies have been\npublished on how best to select examples, and mixed results have been reported\non the usefulness of similarity-based selection over random selection. We\nprovide a study covering multiple LLMs and multiple in-context example\nretrieval strategies, comparing multilingual sentence embeddings. We cover\nseveral language directions, representing different levels of language\nresourcedness (English into French, German, Swahili and Wolof). Contrarily to\npreviously published results, we find that sentence embedding similarity can\nimprove MT, especially for low-resource language directions, and discuss the\nbalance between selection pool diversity and quality. We also highlight\npotential problems with the evaluation of LLM-based MT and suggest a more\nappropriate evaluation protocol, adapting the COMET metric to the evaluation of\nLLMs. Code and outputs are freely available at\nhttps://github.com/ArmelRandy/ICL-MT.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9naRScXifyASj0t9ysOOI8UMXseOaoPpy7bq6daOfbQ","pdfSize":"639687","txDigest":"5vdYJMkXCEceJETExGckHzQ82kSBhz1U43WJbFqZT6JA","endEpoch":"1","status":"CERTIFIED"}
