{"id":"2408.02533","title":"LMEMs for post-hoc analysis of HPO Benchmarking","authors":"Anton Geburek and Neeratyoy Mallik and Danny Stoll and Xavier\n  Bouthillier and Frank Hutter","authorsParsed":[["Geburek","Anton",""],["Mallik","Neeratyoy",""],["Stoll","Danny",""],["Bouthillier","Xavier",""],["Hutter","Frank",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 15:03:19 GMT"}],"updateDate":"2024-08-06","timestamp":1722870199000,"abstract":"  The importance of tuning hyperparameters in Machine Learning (ML) and Deep\nLearning (DL) is established through empirical research and applications,\nevident from the increase in new hyperparameter optimization (HPO) algorithms\nand benchmarks steadily added by the community. However, current benchmarking\npractices using averaged performance across many datasets may obscure key\ndifferences between HPO methods, especially for pairwise comparisons. In this\nwork, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing\nfor post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible and\nexpressive modeling on the entire experiment data, including information such\nas benchmark meta-features, offering deeper insights than current analysis\npractices. We demonstrate this through a case study on the PriorBand paper's\nexperiment data to find insights not reported in the original work.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}