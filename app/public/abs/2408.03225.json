{"id":"2408.03225","title":"Line-based 6-DoF Object Pose Estimation and Tracking With an Event\n  Camera","authors":"Zibin Liu, Banglei Guan, Yang Shang, Qifeng Yu, Laurent Kneip","authorsParsed":[["Liu","Zibin",""],["Guan","Banglei",""],["Shang","Yang",""],["Yu","Qifeng",""],["Kneip","Laurent",""]],"versions":[{"version":"v1","created":"Tue, 6 Aug 2024 14:36:43 GMT"}],"updateDate":"2024-08-07","timestamp":1722955003000,"abstract":"  Pose estimation and tracking of objects is a fundamental application in 3D\nvision. Event cameras possess remarkable attributes such as high dynamic range,\nlow latency, and resilience against motion blur, which enables them to address\nchallenging high dynamic range scenes or high-speed motion. These features make\nevent cameras an ideal complement over standard cameras for object pose\nestimation. In this work, we propose a line-based robust pose estimation and\ntracking method for planar or non-planar objects using an event camera.\nFirstly, we extract object lines directly from events, then provide an initial\npose using a globally-optimal Branch-and-Bound approach, where 2D-3D line\ncorrespondences are not known in advance. Subsequently, we utilize event-line\nmatching to establish correspondences between 2D events and 3D models.\nFurthermore, object poses are refined and continuously tracked by minimizing\nevent-line distances. Events are assigned different weights based on these\ndistances, employing robust estimation algorithms. To evaluate the precision of\nthe proposed methods in object pose estimation and tracking, we have devised\nand established an event-based moving object dataset. Compared against\nstate-of-the-art methods, the robustness and accuracy of our methods have been\nvalidated both on synthetic experiments and the proposed dataset. The source\ncode is available at https://github.com/Zibin6/LOPET.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YiHlMohzYcvH3VDFWuz2aHzXo8OVAxjgbLTpWO5-M5Q","pdfSize":"4596741"}
