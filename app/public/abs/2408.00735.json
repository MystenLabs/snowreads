{"id":"2408.00735","title":"TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models","authors":"Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, Daniel Cohen-Or","authorsParsed":[["Deutch","Gilad",""],["Gal","Rinon",""],["Garibi","Daniel",""],["Patashnik","Or",""],["Cohen-Or","Daniel",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 17:27:28 GMT"}],"updateDate":"2024-08-02","timestamp":1722533248000,"abstract":"  Diffusion models have opened the path to a wide range of text-based image\nediting frameworks. However, these typically build on the multi-step nature of\nthe diffusion backwards process, and adapting them to distilled, fast-sampling\nmethods has proven surprisingly challenging. Here, we focus on a popular line\nof text-based editing frameworks - the ``edit-friendly'' DDPM-noise inversion\napproach. We analyze its application to fast sampling methods and categorize\nits failures into two classes: the appearance of visual artifacts, and\ninsufficient editing strength. We trace the artifacts to mismatched noise\nstatistics between inverted noises and the expected noise schedule, and suggest\na shifted noise schedule which corrects for this offset. To increase editing\nstrength, we propose a pseudo-guidance approach that efficiently increases the\nmagnitude of edits without introducing new artifacts. All in all, our method\nenables text-based image editing with as few as three diffusion steps, while\nproviding novel insights into the mechanisms behind popular text-based editing\napproaches.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}