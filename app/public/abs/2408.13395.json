{"id":"2408.13395","title":"Task-Oriented Diffusion Inversion for High-Fidelity Text-based Editing","authors":"Yangyang Xu, Wenqi Shao, Yong Du, Haiming Zhu, Yang Zhou, Ping Luo,\n  Shengfeng He","authorsParsed":[["Xu","Yangyang",""],["Shao","Wenqi",""],["Du","Yong",""],["Zhu","Haiming",""],["Zhou","Yang",""],["Luo","Ping",""],["He","Shengfeng",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 22:16:34 GMT"}],"updateDate":"2024-08-27","timestamp":1724451394000,"abstract":"  Recent advancements in text-guided diffusion models have unlocked powerful\nimage manipulation capabilities, yet balancing reconstruction fidelity and\neditability for real images remains a significant challenge. In this work, we\nintroduce \\textbf{T}ask-\\textbf{O}riented \\textbf{D}iffusion \\textbf{I}nversion\n(\\textbf{TODInv}), a novel framework that inverts and edits real images\ntailored to specific editing tasks by optimizing prompt embeddings within the\nextended \\(\\mathcal{P}^*\\) space. By leveraging distinct embeddings across\ndifferent U-Net layers and time steps, TODInv seamlessly integrates inversion\nand editing through reciprocal optimization, ensuring both high fidelity and\nprecise editability. This hierarchical editing mechanism categorizes tasks into\nstructure, appearance, and global edits, optimizing only those embeddings\nunaffected by the current editing task. Extensive experiments on benchmark\ndataset reveal TODInv's superior performance over existing methods, delivering\nboth quantitative and qualitative enhancements while showcasing its versatility\nwith few-step diffusion model.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"N4DcOKW9O4j9IkJ70WXGr-5gwjoniLB1dYIbSUqe4FI","pdfSize":"30340148"}
