{"id":"2408.01023","title":"Distilling interpretable causal trees from causal forests","authors":"Patrick Rehill","authorsParsed":[["Rehill","Patrick",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 05:48:15 GMT"}],"updateDate":"2024-08-05","timestamp":1722577695000,"abstract":"  Machine learning methods for estimating treatment effect heterogeneity\npromise greater flexibility than existing methods that test a few pre-specified\nhypotheses. However, one problem these methods can have is that it can be\nchallenging to extract insights from complicated machine learning models. A\nhigh-dimensional distribution of conditional average treatment effects may give\naccurate, individual-level estimates, but it can be hard to understand the\nunderlying patterns; hard to know what the implications of the analysis are.\nThis paper proposes the Distilled Causal Tree, a method for distilling a\nsingle, interpretable causal tree from a causal forest. This compares well to\nexisting methods of extracting a single tree, particularly in noisy data or\nhigh-dimensional data where there are many correlated features. Here it even\noutperforms the base causal forest in most simulations. Its estimates are\ndoubly robust and asymptotically normal just as those of the causal forest are.\n","subjects":["Economics/Econometrics","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}