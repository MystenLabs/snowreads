{"id":"2407.15241","title":"Temporal Abstraction in Reinforcement Learning with Offline Data","authors":"Ranga Shaarad Ayyagari, Anurita Ghosh, Ambedkar Dukkipati","authorsParsed":[["Ayyagari","Ranga Shaarad",""],["Ghosh","Anurita",""],["Dukkipati","Ambedkar",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 18:10:31 GMT"}],"updateDate":"2024-07-23","timestamp":1721585431000,"abstract":"  Standard reinforcement learning algorithms with a single policy perform\npoorly on tasks in complex environments involving sparse rewards, diverse\nbehaviors, or long-term planning. This led to the study of algorithms that\nincorporate temporal abstraction by training a hierarchy of policies that plan\nover different time scales. The options framework has been introduced to\nimplement such temporal abstraction by learning low-level options that act as\nextended actions controlled by a high-level policy. The main challenge in\napplying these algorithms to real-world problems is that they suffer from high\nsample complexity to train multiple levels of the hierarchy, which is\nimpossible in online settings. Motivated by this, in this paper, we propose an\noffline hierarchical RL method that can learn options from existing offline\ndatasets collected by other unknown agents. This is a very challenging problem\ndue to the distribution mismatch between the learned options and the policies\nresponsible for the offline dataset and to our knowledge, this is the first\nwork in this direction. In this work, we propose a framework by which an online\nhierarchical reinforcement learning algorithm can be trained on an offline\ndataset of transitions collected by an unknown behavior policy. We validate our\nmethod on Gym MuJoCo locomotion environments and robotic gripper block-stacking\ntasks in the standard as well as transfer and goal-conditioned settings.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}