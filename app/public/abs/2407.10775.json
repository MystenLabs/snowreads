{"id":"2407.10775","title":"Last-Iterate Global Convergence of Policy Gradients for Constrained\n  Reinforcement Learning","authors":"Alessandro Montenegro and Marco Mussi and Matteo Papini and Alberto\n  Maria Metelli","authorsParsed":[["Montenegro","Alessandro",""],["Mussi","Marco",""],["Papini","Matteo",""],["Metelli","Alberto Maria",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 14:54:57 GMT"}],"updateDate":"2024-07-16","timestamp":1721055297000,"abstract":"  Constrained Reinforcement Learning (CRL) tackles sequential decision-making\nproblems where agents are required to achieve goals by maximizing the expected\nreturn while meeting domain-specific constraints, which are often formulated as\nexpected costs. In this setting, policy-based methods are widely used since\nthey come with several advantages when dealing with continuous-control\nproblems. These methods search in the policy space with an action-based or\nparameter-based exploration strategy, depending on whether they learn directly\nthe parameters of a stochastic policy or those of a stochastic hyperpolicy. In\nthis paper, we propose a general framework for addressing CRL problems via\ngradient-based primal-dual algorithms, relying on an alternate ascent/descent\nscheme with dual-variable regularization. We introduce an exploration-agnostic\nalgorithm, called C-PG, which exhibits global last-iterate convergence\nguarantees under (weak) gradient domination assumptions, improving and\ngeneralizing existing results. Then, we design C-PGAE and C-PGPE, the\naction-based and the parameter-based versions of C-PG, respectively, and we\nillustrate how they naturally extend to constraints defined in terms of risk\nmeasures over the costs, as it is often requested in safety-critical scenarios.\nFinally, we numerically validate our algorithms on constrained control\nproblems, and compare them with state-of-the-art baselines, demonstrating their\neffectiveness.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}