{"id":"2407.09879","title":"sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting","authors":"Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra,\n  Kriti Aggarwal, Luciano Del Corro, Arindam Mitra, Tejas Indulal Dhamecha,\n  Ahmed Awadallah, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram","authorsParsed":[["Ahuja","Sanchit",""],["Tanmay","Kumar",""],["Chauhan","Hardik Hansrajbhai",""],["Patra","Barun",""],["Aggarwal","Kriti",""],["Del Corro","Luciano",""],["Mitra","Arindam",""],["Dhamecha","Tejas Indulal",""],["Awadallah","Ahmed",""],["Choudhary","Monojit",""],["Chaudhary","Vishrav",""],["Sitaram","Sunayana",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 13:03:45 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 17:23:18 GMT"}],"updateDate":"2024-07-17","timestamp":1720875825000,"abstract":"  Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and\nMistral-7B and then evaluating them across a comprehensive suite of\nmultilingual benchmarks that test reasoning, question answering, and reading\ncomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with\nsPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared\nto the baselines. We also devise a strategy to incorporate N-shot examples in\neach fine-tuning sample which further boosts the performance of these models by\n3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other\nmultilingual instruction tuning datasets on the same benchmarks along with\nbeing sample efficient and diverse, thereby reducing dataset creation costs.\nAdditionally, instruction tuning with sPhinX does not lead to regression on\nmost standard LLM benchmarks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}