{"id":"2408.09703","title":"Partial-Multivariate Model for Forecasting","authors":"Jaehoon Lee, Hankook Lee, Sungik Choi, Sungjun Cho, Moontae Lee","authorsParsed":[["Lee","Jaehoon",""],["Lee","Hankook",""],["Choi","Sungik",""],["Cho","Sungjun",""],["Lee","Moontae",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 05:18:50 GMT"}],"updateDate":"2024-08-20","timestamp":1724044730000,"abstract":"  When solving forecasting problems including multiple time-series features,\nexisting approaches often fall into two extreme categories, depending on\nwhether to utilize inter-feature information: univariate and\ncomplete-multivariate models. Unlike univariate cases which ignore the\ninformation, complete-multivariate models compute relationships among a\ncomplete set of features. However, despite the potential advantage of\nleveraging the additional information, complete-multivariate models sometimes\nunderperform univariate ones. Therefore, our research aims to explore a middle\nground between these two by introducing what we term Partial-Multivariate\nmodels where a neural network captures only partial relationships, that is,\ndependencies within subsets of all features. To this end, we propose PMformer,\na Transformer-based partial-multivariate model, with its training algorithm. We\ndemonstrate that PMformer outperforms various univariate and\ncomplete-multivariate models, providing a theoretical rationale and empirical\nanalysis for its superiority. Additionally, by proposing an inference technique\nfor PMformer, the forecasting accuracy is further enhanced. Finally, we\nhighlight other advantages of PMformer: efficiency and robustness under missing\nfeatures.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}