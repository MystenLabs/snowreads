{"id":"2407.19185","title":"LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models","authors":"Ruiyi Zhang, Yufan Zhou, Jian Chen, Jiuxiang Gu, Changyou Chen, Tong\n  Sun","authorsParsed":[["Zhang","Ruiyi",""],["Zhou","Yufan",""],["Chen","Jian",""],["Gu","Jiuxiang",""],["Chen","Changyou",""],["Sun","Tong",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 05:53:37 GMT"}],"updateDate":"2024-07-30","timestamp":1722059617000,"abstract":"  Large multimodal language models have demonstrated impressive capabilities in\nunderstanding and manipulating images. However, many of these models struggle\nwith comprehending intensive textual contents embedded within the images,\nprimarily due to the limited text recognition and layout understanding ability.\nTo understand the sources of these limitations, we perform an exploratory\nanalysis showing the drawbacks of classical visual encoders on visual text\nunderstanding. Hence, we present LLaVA-Read, a multimodal large language model\nthat utilizes dual visual encoders along with a visual text encoder. Our model\nsurpasses existing state-of-the-art models in various text-rich image\nunderstanding tasks, showcasing enhanced comprehension of textual content\nwithin images. Together, our research suggests visual text understanding\nremains an open challenge and an efficient visual text encoder is crucial for\nfuture successful multimodal systems.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}