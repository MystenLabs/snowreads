{"id":"2408.11518","title":"EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face with\n  Mesh Attention","authors":"Yihong Lin, Liang Peng, Jianqiao Hu, Xiandong Li, Wenxiong Kang,\n  Songju Lei, Xianjia Wu, Huang Xu","authorsParsed":[["Lin","Yihong",""],["Peng","Liang",""],["Hu","Jianqiao",""],["Li","Xiandong",""],["Kang","Wenxiong",""],["Lei","Songju",""],["Wu","Xianjia",""],["Xu","Huang",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 10:51:12 GMT"}],"updateDate":"2024-08-22","timestamp":1724237472000,"abstract":"  The creation of increasingly vivid 3D virtual digital humans has become a hot\ntopic in recent years. Currently, most speech-driven work focuses on training\nmodels to learn the relationship between phonemes and visemes to achieve more\nrealistic lips. However, they fail to capture the correlations between emotions\nand facial expressions effectively. To solve this problem, we propose a new\nmodel, termed EmoFace. EmoFace employs a novel Mesh Attention mechanism, which\nhelps to learn potential feature dependencies between mesh vertices in time and\nspace. We also adopt, for the first time to our knowledge, an effective\nself-growing training scheme that combines teacher-forcing and scheduled\nsampling in a 3D face animation task. Additionally, since EmoFace is an\nautoregressive model, there is no requirement that the first frame of the\ntraining data must be a silent frame, which greatly reduces the data\nlimitations and contributes to solve the current dilemma of insufficient\ndatasets. Comprehensive quantitative and qualitative evaluations on our\nproposed high-quality reconstructed 3D emotional facial animation dataset,\n3D-RAVDESS ($5.0343\\times 10^{-5}$mm for LVE and $1.0196\\times 10^{-5}$mm for\nEVE), and publicly available dataset VOCASET ($2.8669\\times 10^{-5}$mm for LVE\nand $0.4664\\times 10^{-5}$mm for EVE), demonstrate that our algorithm achieves\nstate-of-the-art performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}