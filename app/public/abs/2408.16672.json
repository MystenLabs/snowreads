{"id":"2408.16672","title":"Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever","authors":"Rohan Jha, Bo Wang, Michael G\\\"unther, Georgios Mastrapas, Saba\n  Sturua, Isabelle Mohr, Andreas Koukounas, Mohammad Kalim Akram, Nan Wang and\n  Han Xiao","authorsParsed":[["Jha","Rohan",""],["Wang","Bo",""],["GÃ¼nther","Michael",""],["Mastrapas","Georgios",""],["Sturua","Saba",""],["Mohr","Isabelle",""],["Koukounas","Andreas",""],["Akram","Mohammad Kalim",""],["Wang","Nan",""],["Xiao","Han",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 16:21:00 GMT"},{"version":"v2","created":"Fri, 30 Aug 2024 18:14:24 GMT"},{"version":"v3","created":"Wed, 4 Sep 2024 05:09:00 GMT"},{"version":"v4","created":"Sat, 14 Sep 2024 07:41:06 GMT"}],"updateDate":"2024-09-17","timestamp":1724948460000,"abstract":"  Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis work we propose a number of incremental improvements to the ColBERT model\narchitecture and training pipeline, using methods shown to work in the more\nmature single-vector embedding model training paradigm, particularly those that\napply to heterogeneous multilingual data or boost efficiency with little\ntradeoff. Our new model, Jina-ColBERT-v2, demonstrates strong performance\nacross a range of English and multilingual retrieval tasks.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}