{"id":"2408.02787","title":"Segmentation Style Discovery: Application to Skin Lesion Images","authors":"Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh","authorsParsed":[["Abhishek","Kumar",""],["Kawahara","Jeremy",""],["Hamarneh","Ghassan",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 19:11:05 GMT"}],"updateDate":"2024-08-07","timestamp":1722885065000,"abstract":"  Variability in medical image segmentation, arising from annotator\npreferences, expertise, and their choice of tools, has been well documented.\nWhile the majority of multi-annotator segmentation approaches focus on modeling\nannotator-specific preferences, they require annotator-segmentation\ncorrespondence. In this work, we introduce the problem of segmentation style\ndiscovery, and propose StyleSeg, a segmentation method that learns plausible,\ndiverse, and semantically consistent segmentation styles from a corpus of\nimage-mask pairs without any knowledge of annotator correspondence. StyleSeg\nconsistently outperforms competing methods on four publicly available skin\nlesion segmentation (SLS) datasets. We also curate ISIC-MultiAnnot, the largest\nmulti-annotator SLS dataset with annotator correspondence, and our results show\na strong alignment, using our newly proposed measure AS2, between the predicted\nstyles and annotator preferences. The code and the dataset are available at\nhttps://github.com/sfu-mial/StyleSeg.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}