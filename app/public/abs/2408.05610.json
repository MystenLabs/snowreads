{"id":"2408.05610","title":"Representation Alignment from Human Feedback for Cross-Embodiment Reward\n  Learning from Mixed-Quality Demonstrations","authors":"Connor Mattson, Anurag Aribandi, Daniel S. Brown","authorsParsed":[["Mattson","Connor",""],["Aribandi","Anurag",""],["Brown","Daniel S.",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 18:24:14 GMT"}],"updateDate":"2024-08-13","timestamp":1723314254000,"abstract":"  We study the problem of cross-embodiment inverse reinforcement learning,\nwhere we wish to learn a reward function from video demonstrations in one or\nmore embodiments and then transfer the learned reward to a different embodiment\n(e.g., different action space, dynamics, size, shape, etc.). Learning reward\nfunctions that transfer across embodiments is important in settings such as\nteaching a robot a policy via human video demonstrations or teaching a robot to\nimitate a policy from another robot with a different embodiment. However, prior\nwork has only focused on cases where near-optimal demonstrations are available,\nwhich is often difficult to ensure. By contrast, we study the setting of\ncross-embodiment reward learning from mixed-quality demonstrations. We\ndemonstrate that prior work struggles to learn generalizable reward\nrepresentations when learning from mixed-quality data. We then analyze several\ntechniques that leverage human feedback for representation learning and\nalignment to enable effective cross-embodiment learning. Our results give\ninsight into how different representation learning techniques lead to\nqualitatively different reward shaping behaviors and the importance of human\nfeedback when learning from mixed-quality, mixed-embodiment data.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Gipv0UdphicsTNFJqCxvAYvGfEINUyA8iwVxQJ51f1Q","pdfSize":"1018115"}
