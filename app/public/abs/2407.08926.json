{"id":"2407.08926","title":"Toward Automatic Group Membership Annotation for Group Fairness\n  Evaluation","authors":"Fumian Chen, Dayu Yang, Hui Fang","authorsParsed":[["Chen","Fumian",""],["Yang","Dayu",""],["Fang","Hui",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 02:19:02 GMT"}],"updateDate":"2024-07-15","timestamp":1720750742000,"abstract":"  With the increasing research attention on fairness in information retrieval\nsystems, more and more fairness-aware algorithms have been proposed to ensure\nfairness for a sustainable and healthy retrieval ecosystem. However, as the\nmost adopted measurement of fairness-aware algorithms, group fairness\nevaluation metrics, require group membership information that needs massive\nhuman annotations and is barely available for general information retrieval\ndatasets. This data sparsity significantly impedes the development of\nfairness-aware information retrieval studies. Hence, a practical, scalable,\nlow-cost group membership annotation method is needed to assist or replace\nhuman annotations. This study explored how to leverage language models to\nautomatically annotate group membership for group fairness evaluations,\nfocusing on annotation accuracy and its impact. Our experimental results show\nthat BERT-based models outperformed state-of-the-art large language models,\nincluding GPT and Mistral, achieving promising annotation accuracy with minimal\nsupervision in recent fair-ranking datasets. Our impact-oriented evaluations\nreveal that minimal annotation error will not degrade the effectiveness and\nrobustness of group fairness evaluation. The proposed annotation method reduces\ntremendous human efforts and expands the frontier of fairness-aware studies to\nmore datasets.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"ekira_mnllbIxBkmd4bIxUYCdVirNh0FPDPjrLyQ-NU","pdfSize":"5964153"}
