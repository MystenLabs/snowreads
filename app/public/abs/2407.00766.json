{"id":"2407.00766","title":"An Attribute Interpolation Method in Speech Synthesis by Model Merging","authors":"Masato Murata, Koichi Miyazaki, Tomoki Koriyama","authorsParsed":[["Murata","Masato",""],["Miyazaki","Koichi",""],["Koriyama","Tomoki",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 17:01:36 GMT"}],"updateDate":"2024-07-02","timestamp":1719766896000,"abstract":"  With the development of speech synthesis, recent research has focused on\nchallenging tasks, such as speaker generation and emotion intensity control.\nAttribute interpolation is a common approach to these tasks. However, most\nprevious methods for attribute interpolation require specific modules or\ntraining methods. We propose an attribute interpolation method in speech\nsynthesis by model merging. Model merging is a method that creates new\nparameters by only averaging the parameters of base models. The merged model\ncan generate an output with an intermediate feature of the base models. This\nmethod is easily applicable without specific modules or training methods, as it\nuses only existing trained base models. We merged two text-to-speech models to\nachieve attribute interpolation and evaluated its performance on speaker\ngeneration and emotion intensity control tasks. As a result, our proposed\nmethod achieved smooth attribute interpolation while keeping the linguistic\ncontent in both tasks.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"iD5q4mVbWu7UeD5aTZqieyRkhjcTF945e0PBC6ymEkU","pdfSize":"683615","objectId":"0x6b0f1d5ab88e8ec75d17fff0201a048e729e46c523b1b0d450ec18229e0c07b1","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
