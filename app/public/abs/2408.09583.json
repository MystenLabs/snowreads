{"id":"2408.09583","title":"Convolutional Conditional Neural Processes","authors":"Wessel P. Bruinsma","authorsParsed":[["Bruinsma","Wessel P.",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 19:53:38 GMT"}],"updateDate":"2024-08-20","timestamp":1724010818000,"abstract":"  Neural processes are a family of models which use neural networks to directly\nparametrise a map from data sets to predictions. Directly parametrising this\nmap enables the use of expressive neural networks in small-data problems where\nneural networks would traditionally overfit. Neural processes can produce\nwell-calibrated uncertainties, effectively deal with missing data, and are\nsimple to train. These properties make this family of models appealing for a\nbreadth of applications areas, such as healthcare or environmental sciences.\n  This thesis advances neural processes in three ways.\n  First, we propose convolutional neural processes (ConvNPs). ConvNPs improve\ndata efficiency of neural processes by building in a symmetry called\ntranslation equivariance. ConvNPs rely on convolutional neural networks rather\nthan multi-layer perceptrons.\n  Second, we propose Gaussian neural processes (GNPs). GNPs directly\nparametrise dependencies in the predictions of a neural process. Current\napproaches to modelling dependencies in the predictions depend on a latent\nvariable, which consequently requires approximate inference, undermining the\nsimplicity of the approach.\n  Third, we propose autoregressive conditional neural processes (AR CNPs). AR\nCNPs train a neural process without any modifications to the model or training\nprocedure and, at test time, roll out the model in an autoregressive fashion.\nAR CNPs equip the neural process framework with a new knob where modelling\ncomplexity and computational expense at training time can be traded for\ncomputational expense at test time.\n  In addition to methodological advancements, this thesis also proposes a\nsoftware abstraction that enables a compositional approach to implementing\nneural processes. This approach allows the user to rapidly explore the space of\nneural process models by putting together elementary building blocks in\ndifferent ways.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}