{"id":"2408.11956","title":"The Whole Is Bigger Than the Sum of Its Parts: Modeling Individual\n  Annotators to Capture Emotional Variability","authors":"James Tavernor, Yara El-Tawil, Emily Mower Provost","authorsParsed":[["Tavernor","James",""],["El-Tawil","Yara",""],["Provost","Emily Mower",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 19:24:06 GMT"}],"updateDate":"2024-08-23","timestamp":1724268246000,"abstract":"  Emotion expression and perception are nuanced, complex, and highly subjective\nprocesses. When multiple annotators label emotional data, the resulting labels\ncontain high variability. Most speech emotion recognition tasks address this by\naveraging annotator labels as ground truth. However, this process omits the\nnuance of emotion and inter-annotator variability, which are important signals\nto capture. Previous work has attempted to learn distributions to capture\nemotion variability, but these methods also lose information about the\nindividual annotators. We address these limitations by learning to predict\nindividual annotators and by introducing a novel method to create distributions\nfrom continuous model outputs that permit the learning of emotion distributions\nduring model training. We show that this combined approach can result in\nemotion distributions that are more accurate than those seen in prior work, in\nboth within- and cross-corpus settings.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}