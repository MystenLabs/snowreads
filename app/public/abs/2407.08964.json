{"id":"2407.08964","title":"Communication-Aware Reinforcement Learning for Cooperative Adaptive\n  Cruise Control","authors":"Sicong Jiang, Seongjin Choi, Lijun Sun","authorsParsed":[["Jiang","Sicong",""],["Choi","Seongjin",""],["Sun","Lijun",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 03:28:24 GMT"}],"updateDate":"2024-07-15","timestamp":1720754904000,"abstract":"  Cooperative Adaptive Cruise Control (CACC) plays a pivotal role in enhancing\ntraffic efficiency and safety in Connected and Autonomous Vehicles (CAVs).\nReinforcement Learning (RL) has proven effective in optimizing complex\ndecision-making processes in CACC, leading to improved system performance and\nadaptability. Among RL approaches, Multi-Agent Reinforcement Learning (MARL)\nhas shown remarkable potential by enabling coordinated actions among multiple\nCAVs through Centralized Training with Decentralized Execution (CTDE). However,\nMARL often faces scalability issues, particularly when CACC vehicles suddenly\njoin or leave the platoon, resulting in performance degradation. To address\nthese challenges, we propose Communication-Aware Reinforcement Learning\n(CA-RL). CA-RL includes a communication-aware module that extracts and\ncompresses vehicle communication information through forward and backward\ninformation transmission modules. This enables efficient cyclic information\npropagation within the CACC traffic flow, ensuring policy consistency and\nmitigating the scalability problems of MARL in CACC. Experimental results\ndemonstrate that CA-RL significantly outperforms baseline methods in various\ntraffic scenarios, achieving superior scalability, robustness, and overall\nsystem performance while maintaining reliable performance despite changes in\nthe number of participating vehicles.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}