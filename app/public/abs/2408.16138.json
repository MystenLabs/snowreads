{"id":"2408.16138","title":"Thinner Latent Spaces: Detecting dimension and imposing invariance\n  through autoencoder gradient constraints","authors":"George A. Kevrekidis, Mauro Maggioni, Soledad Villar, Yannis G.\n  Kevrekidis","authorsParsed":[["Kevrekidis","George A.",""],["Maggioni","Mauro",""],["Villar","Soledad",""],["Kevrekidis","Yannis G.",""]],"versions":[{"version":"v1","created":"Wed, 28 Aug 2024 20:56:35 GMT"}],"updateDate":"2024-08-30","timestamp":1724878595000,"abstract":"  Conformal Autoencoders are a neural network architecture that imposes\northogonality conditions between the gradients of latent variables towards\nachieving disentangled representations of data. In this letter we show that\northogonality relations within the latent layer of the network can be leveraged\nto infer the intrinsic dimensionality of nonlinear manifold data sets (locally\ncharacterized by the dimension of their tangent space), while simultaneously\ncomputing encoding and decoding (embedding) maps. We outline the relevant\ntheory relying on differential geometry, and describe the corresponding\ngradient-descent optimization algorithm. The method is applied to standard data\nsets and we highlight its applicability, advantages, and shortcomings. In\naddition, we demonstrate that the same computational technology can be used to\nbuild coordinate invariance to local group actions when defined only on a\n(reduced) submanifold of the embedding space.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Differential Geometry","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}