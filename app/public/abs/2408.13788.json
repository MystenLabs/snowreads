{"id":"2408.13788","title":"3D-VirtFusion: Synthetic 3D Data Augmentation through Generative\n  Diffusion Models and Controllable Editing","authors":"Shichao Dong, Ze Yang, Guosheng Lin","authorsParsed":[["Dong","Shichao",""],["Yang","Ze",""],["Lin","Guosheng",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 09:31:22 GMT"}],"updateDate":"2024-08-27","timestamp":1724578282000,"abstract":"  Data augmentation plays a crucial role in deep learning, enhancing the\ngeneralization and robustness of learning-based models. Standard approaches\ninvolve simple transformations like rotations and flips for generating extra\ndata. However, these augmentations are limited by their initial dataset,\nlacking high-level diversity. Recently, large models such as language models\nand diffusion models have shown exceptional capabilities in perception and\ncontent generation. In this work, we propose a new paradigm to automatically\ngenerate 3D labeled training data by harnessing the power of pretrained large\nfoundation models. For each target semantic class, we first generate 2D images\nof a single object in various structure and appearance via diffusion models and\nchatGPT generated text prompts. Beyond texture augmentation, we propose a\nmethod to automatically alter the shape of objects within 2D images.\nSubsequently, we transform these augmented images into 3D objects and construct\nvirtual scenes by random composition. This method can automatically produce a\nsubstantial amount of 3D scene data without the need of real data, providing\nsignificant benefits in addressing few-shot learning challenges and mitigating\nlong-tailed class imbalances. By providing a flexible augmentation approach,\nour work contributes to enhancing 3D data diversity and advancing model\ncapabilities in scene understanding tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}