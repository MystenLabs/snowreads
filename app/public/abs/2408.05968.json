{"id":"2408.05968","title":"Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction","authors":"C\\'edric Eichler, Nathan Champeil, Nicolas Anciaux, Alexandra\n  Bensamoun, Heber Hwang Arcolezi, Jos\\'e Maria De Fuentes","authorsParsed":[["Eichler","Cédric",""],["Champeil","Nathan",""],["Anciaux","Nicolas",""],["Bensamoun","Alexandra",""],["Arcolezi","Heber Hwang",""],["De Fuentes","José Maria",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 07:49:28 GMT"}],"updateDate":"2024-08-13","timestamp":1723448968000,"abstract":"  The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/"}