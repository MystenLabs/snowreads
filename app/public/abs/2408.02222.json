{"id":"2408.02222","title":"Cross-modulated Attention Transformer for RGBT Tracking","authors":"Yun Xiao, Jiacong Zhao, Andong Lu, Chenglong Li, Yin Lin, Bing Yin,\n  Cong Liu","authorsParsed":[["Xiao","Yun",""],["Zhao","Jiacong",""],["Lu","Andong",""],["Li","Chenglong",""],["Lin","Yin",""],["Yin","Bing",""],["Liu","Cong",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 03:54:40 GMT"}],"updateDate":"2024-08-06","timestamp":1722830080000,"abstract":"  Existing Transformer-based RGBT trackers achieve remarkable performance\nbenefits by leveraging self-attention to extract uni-modal features and\ncross-attention to enhance multi-modal feature interaction and template-search\ncorrelation computation. Nevertheless, the independent search-template\ncorrelation calculations ignore the consistency between branches, which can\nresult in ambiguous and inappropriate correlation weights. It not only limits\nthe intra-modal feature representation, but also harms the robustness of\ncross-attention for multi-modal feature interaction and search-template\ncorrelation computation. To address these issues, we propose a novel approach\ncalled Cross-modulated Attention Transformer (CAFormer), which performs\nintra-modality self-correlation, inter-modality feature interaction, and\nsearch-template correlation computation in a unified attention model, for RGBT\ntracking. In particular, we first independently generate correlation maps for\neach modality and feed them into the designed Correlation Modulated Enhancement\nmodule, modulating inaccurate correlation weights by seeking the consensus\nbetween modalities. Such kind of design unifies self-attention and\ncross-attention schemes, which not only alleviates inaccurate attention weight\ncomputation in self-attention but also eliminates redundant computation\nintroduced by extra cross-attention scheme. In addition, we propose a\ncollaborative token elimination strategy to further improve tracking inference\nefficiency and accuracy. Extensive experiments on five public RGBT tracking\nbenchmarks show the outstanding performance of the proposed CAFormer against\nstate-of-the-art methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cmE4g542dOWTrt0TOV7dwUD6uYhsXhMb4XlpopOK9M4","pdfSize":"2385087"}
