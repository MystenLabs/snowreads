{"id":"2407.09788","title":"Explanation is All You Need in Distillation: Mitigating Bias and\n  Shortcut Learning","authors":"Pedro R. A. S. Bassi, Andrea Cavalli and Sergio Decherchi","authorsParsed":[["Bassi","Pedro R. A. S.",""],["Cavalli","Andrea",""],["Decherchi","Sergio",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 07:04:28 GMT"}],"updateDate":"2024-07-16","timestamp":1720854268000,"abstract":"  Bias and spurious correlations in data can cause shortcut learning,\nundermining out-of-distribution (OOD) generalization in deep neural networks.\nMost methods require unbiased data during training (and/or hyper-parameter\ntuning) to counteract shortcut learning. Here, we propose the use of\nexplanation distillation to hinder shortcut learning. The technique does not\nassume any access to unbiased data, and it allows an arbitrarily sized student\nnetwork to learn the reasons behind the decisions of an unbiased teacher, such\nas a vision-language model or a network processing debiased images. We found\nthat it is possible to train a neural network with explanation (e.g by Layer\nRelevance Propagation, LRP) distillation only, and that the technique leads to\nhigh resistance to shortcut learning, surpassing group-invariant learning,\nexplanation background minimization, and alternative distillation techniques.\nIn the COLOURED MNIST dataset, LRP distillation achieved 98.2% OOD accuracy,\nwhile deep feature distillation and IRM achieved 92.1% and 60.2%, respectively.\nIn COCO-on-Places, the undesirable generalization gap between in-distribution\nand OOD accuracy is only of 4.4% for LRP distillation, while the other two\ntechniques present gaps of 15.1% and 52.1%, respectively.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}