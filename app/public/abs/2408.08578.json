{"id":"2408.08578","title":"TAMER: Tree-Aware Transformer for Handwritten Mathematical Expression\n  Recognition","authors":"Jianhua Zhu, Wenqi Zhao, Yu Li, Xingjian Hu, Liangcai Gao","authorsParsed":[["Zhu","Jianhua",""],["Zhao","Wenqi",""],["Li","Yu",""],["Hu","Xingjian",""],["Gao","Liangcai",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 07:24:19 GMT"}],"updateDate":"2024-08-19","timestamp":1723793059000,"abstract":"  Handwritten Mathematical Expression Recognition (HMER) has extensive\napplications in automated grading and office automation. However, existing\nsequence-based decoding methods, which directly predict $\\LaTeX$ sequences,\nstruggle to understand and model the inherent tree structure of $\\LaTeX$ and\noften fail to ensure syntactic correctness in the decoded results. To address\nthese challenges, we propose a novel model named TAMER (Tree-Aware Transformer)\nfor handwritten mathematical expression recognition. TAMER introduces an\ninnovative Tree-aware Module while maintaining the flexibility and efficient\ntraining of Transformer. TAMER combines the advantages of both sequence\ndecoding and tree decoding models by jointly optimizing sequence prediction and\ntree structure prediction tasks, which enhances the model's understanding and\ngeneralization of complex mathematical expression structures. During inference,\nTAMER employs a Tree Structure Prediction Scoring Mechanism to improve the\nstructural validity of the generated $\\LaTeX$ sequences. Experimental results\non CROHME datasets demonstrate that TAMER outperforms traditional sequence\ndecoding and tree decoding models, especially in handling complex mathematical\nstructures, achieving state-of-the-art (SOTA) performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}