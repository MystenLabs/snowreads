{"id":"2408.04471","title":"What could go wrong? Discovering and describing failure modes in\n  computer vision","authors":"Gabriela Csurka and Tyler L. Hayes and Diane Larlus and Riccardo Volpi","authorsParsed":[["Csurka","Gabriela",""],["Hayes","Tyler L.",""],["Larlus","Diane",""],["Volpi","Riccardo",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 14:01:12 GMT"}],"updateDate":"2024-08-09","timestamp":1723125672000,"abstract":"  Deep learning models are effective, yet brittle. Even carefully trained,\ntheir behavior tends to be hard to predict when confronted with\nout-of-distribution samples. In this work, our goal is to propose a simple yet\neffective solution to predict and describe via natural language potential\nfailure modes of computer vision models. Given a pretrained model and a set of\nsamples, our aim is to find sentences that accurately describe the visual\nconditions in which the model underperforms. In order to study this important\ntopic and foster future research on it, we formalize the problem of\nLanguage-Based Error Explainability (LBEE) and propose a set of metrics to\nevaluate and compare different methods for this task. We propose solutions that\noperate in a joint vision-and-language embedding space, and can characterize\nthrough language descriptions model failures caused, e.g., by objects unseen\nduring training or adverse visual conditions. We experiment with different\ntasks, such as classification under the presence of dataset bias and semantic\nsegmentation in unseen environments, and show that the proposed methodology\nisolates nontrivial sentences associated with specific error causes. We hope\nour work will help practitioners better understand the behavior of models,\nincreasing their overall safety and interpretability.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}