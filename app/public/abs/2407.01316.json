{"id":"2407.01316","title":"Evaluating Model Performance Under Worst-case Subpopulations","authors":"Mike Li, Hongseok Namkoong, Shangzhou Xia","authorsParsed":[["Li","Mike",""],["Namkoong","Hongseok",""],["Xia","Shangzhou",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 14:24:05 GMT"}],"updateDate":"2024-07-02","timestamp":1719843845000,"abstract":"  The performance of ML models degrades when the training population is\ndifferent from that seen under operation. Towards assessing distributional\nrobustness, we study the worst-case performance of a model over all\nsubpopulations of a given size, defined with respect to core attributes Z. This\nnotion of robustness can consider arbitrary (continuous) attributes Z, and\nautomatically accounts for complex intersectionality in disadvantaged groups.\nWe develop a scalable yet principled two-stage estimation procedure that can\nevaluate the robustness of state-of-the-art models. We prove that our procedure\nenjoys several finite-sample convergence guarantees, including dimension-free\nconvergence. Instead of overly conservative notions based on Rademacher\ncomplexities, our evaluation error depends on the dimension of Z only through\nthe out-of-sample error in estimating the performance conditional on Z. On real\ndatasets, we demonstrate that our method certifies the robustness of a model\nand prevents deployment of unreliable models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computers and Society","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}