{"id":"2407.07345","title":"Micro-Expression Recognition by Motion Feature Extraction based on\n  Pre-training","authors":"Ruolin Li, Lu Wang, Tingting Yang, Lisheng Xu, Bingyang Ma, Yongchun\n  Li, Hongchao Wei","authorsParsed":[["Li","Ruolin",""],["Wang","Lu",""],["Yang","Tingting",""],["Xu","Lisheng",""],["Ma","Bingyang",""],["Li","Yongchun",""],["Wei","Hongchao",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 03:51:34 GMT"}],"updateDate":"2024-07-11","timestamp":1720583494000,"abstract":"  Micro-expressions (MEs) are spontaneous, unconscious facial expressions that\nhave promising applications in various fields such as psychotherapy and\nnational security. Thus, micro-expression recognition (MER) has attracted more\nand more attention from researchers. Although various MER methods have emerged\nespecially with the development of deep learning techniques, the task still\nfaces several challenges, e.g. subtle motion and limited training data. To\naddress these problems, we propose a novel motion extraction strategy (MoExt)\nfor the MER task and use additional macro-expression data in the pre-training\nprocess. We primarily pretrain the feature separator and motion extractor using\nthe contrastive loss, thus enabling them to extract representative motion\nfeatures. In MoExt, shape features and texture features are first extracted\nseparately from onset and apex frames, and then motion features related to MEs\nare extracted based on the shape features of both frames. To enable the model\nto more effectively separate features, we utilize the extracted motion features\nand the texture features from the onset frame to reconstruct the apex frame.\nThrough pre-training, the module is enabled to extract inter-frame motion\nfeatures of facial expressions while excluding irrelevant information. The\nfeature separator and motion extractor are ultimately integrated into the MER\nnetwork, which is then fine-tuned using the target ME data. The effectiveness\nof proposed method is validated on three commonly used datasets, i.e., CASME\nII, SMIC, SAMM, and CAS(ME)3 dataset. The results show that our method performs\nfavorably against state-of-the-art methods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}