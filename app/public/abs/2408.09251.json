{"id":"2408.09251","title":"V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large\n  Vision-Language Models","authors":"Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu,\n  Xi Cheng, Xiaopeng Li, Bin Ran","authorsParsed":[["You","Junwei",""],["Shi","Haotian",""],["Jiang","Zhuoyu",""],["Huang","Zilin",""],["Gan","Rui",""],["Wu","Keshu",""],["Cheng","Xi",""],["Li","Xiaopeng",""],["Ran","Bin",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 16:42:13 GMT"},{"version":"v2","created":"Mon, 16 Sep 2024 05:23:07 GMT"}],"updateDate":"2024-09-17","timestamp":1723912933000,"abstract":"  Advancements in autonomous driving have increasingly focused on end-to-end\n(E2E) systems that manage the full spectrum of driving tasks, from\nenvironmental perception to vehicle navigation and control. This paper\nintroduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative\nautonomous driving (VICAD) framework with Vehicle-to-Everything (V2X) systems\nand large vision-language models (VLMs). V2X-VLM is designed to enhance\nsituational awareness, decision-making, and ultimate trajectory planning by\nintegrating multimodel data from vehicle-mounted cameras, infrastructure\nsensors, and textual information. The contrastive learning method is further\nemployed to complement VLM by refining feature discrimination, assisting the\nmodel to learn robust representations of the driving environment. Evaluations\non the DAIR-V2X dataset show that V2X-VLM outperforms state-of-the-art\ncooperative autonomous driving methods, while additional tests on corner cases\nvalidate its robustness in real-world driving conditions.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}