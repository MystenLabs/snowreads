{"id":"2408.14528","title":"Adaptive Resolution Inference (ARI): Energy-Efficient Machine Learning\n  for Internet of Things","authors":"Ziheng Wang, Pedro Reviriego, Farzad Niknia, Javier Conde, Shanshan\n  Liu, Fabrizio Lombardi","authorsParsed":[["Wang","Ziheng",""],["Reviriego","Pedro",""],["Niknia","Farzad",""],["Conde","Javier",""],["Liu","Shanshan",""],["Lombardi","Fabrizio",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 16:00:26 GMT"}],"updateDate":"2024-08-28","timestamp":1724688026000,"abstract":"  The implementation of machine learning in Internet of Things devices poses\nsignificant operational challenges due to limited energy and computation\nresources. In recent years, significant efforts have been made to implement\nsimplified ML models that can achieve reasonable performance while reducing\ncomputation and energy, for example by pruning weights in neural networks, or\nusing reduced precision for the parameters and arithmetic operations. However,\nthis type of approach is limited by the performance of the ML implementation,\ni.e., by the loss for example in accuracy due to the model simplification. In\nthis article, we present adaptive resolution inference (ARI), a novel approach\nthat enables to evaluate new tradeoffs between energy dissipation and model\nperformance in ML implementations. The main principle of the proposed approach\nis to run inferences with reduced precision (quantization) and use the margin\nover the decision threshold to determine if either the result is reliable, or\nthe inference must run with the full model. The rationale is that quantization\nonly introduces small deviations in the inference scores, such that if the\nscores have a sufficient margin over the decision threshold, it is unlikely\nthat the full model would have a different result. Therefore, we can run the\nquantized model first, and only when the scores do not have a sufficient\nmargin, the full model is run. This enables most inferences to run with the\nreduced precision model and only a small fraction requires the full model, so\nsignificantly reducing computation and energy while not affecting model\nperformance. The proposed ARI approach is presented, analyzed in detail, and\nevaluated using different data sets for floating-point and stochastic computing\nimplementations. The results show that ARI can significantly reduce the energy\nfor inference in different configurations with savings between 40% and 85%.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}