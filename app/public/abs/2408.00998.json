{"id":"2408.00998","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","authors":"Xiang Gao, Jiaying Liu","authorsParsed":[["Gao","Xiang",""],["Liu","Jiaying",""]],"versions":[{"version":"v1","created":"Fri, 2 Aug 2024 04:13:38 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 12:01:17 GMT"}],"updateDate":"2024-08-07","timestamp":1722572018000,"abstract":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}