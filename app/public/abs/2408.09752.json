{"id":"2408.09752","title":"A Unified Framework for Iris Anti-Spoofing: Introducing IrisGeneral\n  Dataset and Masked-MoE Method","authors":"Hang Zou, Chenxi Du, Ajian Liu, Yuan Zhang, Jing Liu, Mingchuan Yang,\n  Jun Wan, Hui Zhang","authorsParsed":[["Zou","Hang",""],["Du","Chenxi",""],["Liu","Ajian",""],["Zhang","Yuan",""],["Liu","Jing",""],["Yang","Mingchuan",""],["Wan","Jun",""],["Zhang","Hui",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 07:24:36 GMT"}],"updateDate":"2024-08-20","timestamp":1724052276000,"abstract":"  Iris recognition is widely used in high-security scenarios due to its\nstability and distinctiveness. However, the acquisition of iris images\ntypically requires near-infrared illumination and near-infrared band filters,\nleading to significant and consistent differences in imaging across devices.\nThis underscores the importance of developing cross-domain capabilities in iris\nanti-spoofing methods. Despite this need, there is no dataset available that\ncomprehensively evaluates the generalization ability of the iris anti-spoofing\ntask. To address this gap, we propose the IrisGeneral dataset, which includes\n10 subsets, belonging to 7 databases, published by 4 institutions, collected\nwith 6 types of devices. IrisGeneral is designed with three protocols, aimed at\nevaluating average performance, cross-racial generalization, and cross-device\ngeneralization of iris anti-spoofing models. To tackle the challenge of\nintegrating multiple sub-datasets in IrisGeneral, we employ multiple parameter\nsets to learn from the various subsets. Specifically, we utilize the Mixture of\nExperts (MoE) to fit complex data distributions using multiple sub-neural\nnetworks. To further enhance the generalization capabilities, we introduce a\nnovel method Masked-MoE (MMoE). It randomly masks a portion of tokens for some\nexperts and requires their outputs to be similar to the unmasked experts, which\nimproves the generalization ability and effectively mitigates the overfitting\nissue produced by MoE. We selected ResNet50, VIT-B/16, CLIP, and FLIP as\nrepresentative models and benchmarked them on the IrisGeneral dataset.\nExperimental results demonstrate that our proposed MMoE with CLIP achieves the\nbest performance on IrisGeneral.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}