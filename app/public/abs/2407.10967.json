{"id":"2407.10967","title":"BECAUSE: Bilinear Causal Representation for Generalizable Offline\n  Model-based Reinforcement Learning","authors":"Haohong Lin, Wenhao Ding, Jian Chen, Laixi Shi, Jiacheng Zhu, Bo Li,\n  Ding Zhao","authorsParsed":[["Lin","Haohong",""],["Ding","Wenhao",""],["Chen","Jian",""],["Shi","Laixi",""],["Zhu","Jiacheng",""],["Li","Bo",""],["Zhao","Ding",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 17:59:23 GMT"}],"updateDate":"2024-07-16","timestamp":1721066363000,"abstract":"  Offline model-based reinforcement learning (MBRL) enhances data efficiency by\nutilizing pre-collected datasets to learn models and policies, especially in\nscenarios where exploration is costly or infeasible. Nevertheless, its\nperformance often suffers from the objective mismatch between model and policy\nlearning, resulting in inferior performance despite accurate model predictions.\nThis paper first identifies the primary source of this mismatch comes from the\nunderlying confounders present in offline data for MBRL. Subsequently, we\nintroduce \\textbf{B}ilin\\textbf{E}ar \\textbf{CAUS}al\nr\\textbf{E}presentation~(BECAUSE), an algorithm to capture causal\nrepresentation for both states and actions to reduce the influence of the\ndistribution shift, thus mitigating the objective mismatch problem.\nComprehensive evaluations on 18 tasks that vary in data quality and environment\ncontext demonstrate the superior performance of BECAUSE over existing offline\nRL algorithms. We show the generalizability and robustness of BECAUSE under\nfewer samples or larger numbers of confounders. Additionally, we offer\ntheoretical analysis of BECAUSE to prove its error bound and sample efficiency\nwhen integrating causal representation into offline MBRL.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}