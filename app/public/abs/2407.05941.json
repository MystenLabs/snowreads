{"id":"2407.05941","title":"Pruning One More Token is Enough: Leveraging Latency-Workload\n  Non-Linearities for Vision Transformers on the Edge","authors":"Nick John Eliopoulos, Purvish Jajal, James Davis, Gaowen Liu, George\n  K. Thiravathukal, Yung-Hsiang Lu","authorsParsed":[["Eliopoulos","Nick John",""],["Jajal","Purvish",""],["Davis","James",""],["Liu","Gaowen",""],["Thiravathukal","George K.",""],["Lu","Yung-Hsiang",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:42:40 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 00:23:06 GMT"},{"version":"v3","created":"Wed, 11 Sep 2024 21:53:59 GMT"}],"updateDate":"2024-09-13","timestamp":1719855760000,"abstract":"  This paper investigates how to efficiently deploy vision transformers on edge\ndevices for small workloads. Recent methods reduce the latency of transformer\nneural networks by removing or merging tokens, with small accuracy degradation.\nHowever, these methods are not designed with edge device deployment in mind:\nthey do not leverage information about the latency-workload trends to improve\nefficiency. We address this shortcoming in our work. First, we identify factors\nthat affect ViT latency-workload relationships. Second, we determine token\npruning schedule by leveraging non-linear latency-workload relationships.\nThird, we demonstrate a training-free, token pruning method utilizing this\nschedule. We show other methods may increase latency by 2-30%, while we reduce\nlatency by 9-26%. For similar latency (within 5.2% or 7ms) across devices we\nachieve 78.6%-84.5% ImageNet1K accuracy, while the state-of-the-art, Token\nMerging, achieves 45.8%-85.4%.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}