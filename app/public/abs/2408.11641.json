{"id":"2408.11641","title":"Estimated Audio-Caption Correspondences Improve Language-Based Audio\n  Retrieval","authors":"Paul Primus, Florian Schmid and Gerhard Widmer","authorsParsed":[["Primus","Paul",""],["Schmid","Florian",""],["Widmer","Gerhard",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 14:10:58 GMT"}],"updateDate":"2024-08-22","timestamp":1724249458000,"abstract":"  Dual-encoder-based audio retrieval systems are commonly optimized with\ncontrastive learning on a set of matching and mismatching audio-caption pairs.\nThis leads to a shared embedding space in which corresponding items from the\ntwo modalities end up close together. Since audio-caption datasets typically\nonly contain matching pairs of recordings and descriptions, it has become\ncommon practice to create mismatching pairs by pairing the audio with a caption\nrandomly drawn from the dataset. This is not ideal because the randomly sampled\ncaption could, just by chance, partly or entirely describe the audio recording.\nHowever, correspondence information for all possible pairs is costly to\nannotate and thus typically unavailable; we, therefore, suggest substituting it\nwith estimated correspondences. To this end, we propose a two-staged training\nprocedure in which multiple retrieval models are first trained as usual, i.e.,\nwithout estimated correspondences. In the second stage, the audio-caption\ncorrespondences predicted by these models then serve as prediction targets. We\nevaluate our method on the ClothoV2 and the AudioCaps benchmark and show that\nit improves retrieval performance, even in a restricting self-distillation\nsetting where a single model generates and then learns from the estimated\ncorrespondences. We further show that our method outperforms the current state\nof the art by 1.6 pp. mAP@10 on the ClothoV2 benchmark.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Machine Learning","Computing Research Repository/Sound"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}