{"id":"2407.08903","title":"TensorTEE: Unifying Heterogeneous TEE Granularity for Efficient Secure\n  Collaborative Tensor Computing","authors":"Husheng Han, Xinyao Zheng, Yuanbo Wen, Yifan Hao, Erhu Feng, Ling\n  Liang, Jianan Mu, Xiaqing Li, Tianyun Ma, Pengwei Jin, Xinkai Song, Zidong\n  Du, Qi Guo, Xing Hu","authorsParsed":[["Han","Husheng",""],["Zheng","Xinyao",""],["Wen","Yuanbo",""],["Hao","Yifan",""],["Feng","Erhu",""],["Liang","Ling",""],["Mu","Jianan",""],["Li","Xiaqing",""],["Ma","Tianyun",""],["Jin","Pengwei",""],["Song","Xinkai",""],["Du","Zidong",""],["Guo","Qi",""],["Hu","Xing",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 00:35:18 GMT"}],"updateDate":"2024-07-15","timestamp":1720744518000,"abstract":"  Heterogeneous collaborative computing with NPU and CPU has received\nwidespread attention due to its substantial performance benefits. To ensure\ndata confidentiality and integrity during computing, Trusted Execution\nEnvironments (TEE) is considered a promising solution because of its\ncomparatively lower overhead. However, existing heterogeneous TEE designs are\ninefficient for collaborative computing due to fine and different memory\ngranularities between CPU and NPU. 1) The cacheline granularity of CPU TEE\nintensifies memory pressure due to its extra memory access, and 2) the\ncacheline granularity MAC of NPU escalates the pressure on the limited memory\nstorage. 3) Data transfer across heterogeneous enclaves relies on the transit\nof non-secure regions, resulting in cumbersome re-encryption and scheduling.\n  To address these issues, we propose TensorTEE, a unified tensor-granularity\nheterogeneous TEE for efficient secure collaborative tensor computing. First,\nwe virtually support tensor granularity in CPU TEE to eliminate the off-chip\nmetadata access by detecting and maintaining tensor structures on-chip. Second,\nwe propose tensor-granularity MAC management with predictive execution to avoid\ncomputational stalls while eliminating off-chip MAC storage and access.\nMoreover, based on the unified granularity, we enable direct data transfer\nwithout re-encryption and scheduling dilemmas. Our evaluation is built on\nenhanced Gem5 and a cycle-accurate NPU simulator. The results show that\nTensorTEE improves the performance of Large Language Model (LLM) training\nworkloads by 4.0x compared to existing work and incurs only 2.1% overhead\ncompared to non-secure training, offering a practical security assurance for\nLLM training.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Hardware Architecture"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}