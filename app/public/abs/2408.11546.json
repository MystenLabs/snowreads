{"id":"2408.11546","title":"Memorization In In-Context Learning","authors":"Shahriar Golchin, Mihai Surdeanu, Steven Bethard, Eduardo Blanco,\n  Ellen Riloff","authorsParsed":[["Golchin","Shahriar",""],["Surdeanu","Mihai",""],["Bethard","Steven",""],["Blanco","Eduardo",""],["Riloff","Ellen",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 11:54:22 GMT"}],"updateDate":"2024-08-22","timestamp":1724241262000,"abstract":"  In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind these performance improvements\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance across various ICL regimes: zero-shot, few-shot, and many-shot. Our\nmost notable findings include: (1) ICL significantly surfaces memorization\ncompared to zero-shot learning in most cases; (2) demonstrations, without their\nlabels, are the most effective element in surfacing memorization; (3) ICL\nimproves performance when the surfaced memorization in few-shot regimes reaches\na high level (about 40%); and (4) there is a very strong correlation between\nperformance and memorization in ICL when it outperforms zero-shot learning.\nOverall, our study uncovers a hidden phenomenon -- memorization -- at the core\nof ICL, raising an important question: to what extent do LLMs truly generalize\nfrom demonstrations in ICL, and how much of their success is due to\nmemorization?\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}