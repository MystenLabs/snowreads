{"id":"2408.09831","title":"Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions","authors":"Sebastian Heineking, Jonas Probst, Daniel Steinbach, Martin Potthast,\n  Harrisen Scells","authorsParsed":[["Heineking","Sebastian",""],["Probst","Jonas",""],["Steinbach","Daniel",""],["Potthast","Martin",""],["Scells","Harrisen",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 09:27:45 GMT"}],"updateDate":"2024-08-20","timestamp":1724059665000,"abstract":"  Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Most evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required, such as health, and\nwhere misleading or incorrect answers can have a significant impact on a user's\nhealth. Using human experts to evaluate the quality of LLM answers is generally\nconsidered the gold standard, but expert annotation is costly and slow. We\npresent a method for evaluating LLM answers that uses ranking signals as a\nsubstitute for explicit relevance judgements. Our scoring method correlates\nwith the preferences of human experts. We validate it by investigating the\nwell-known fact that the quality of generated answers improves with the size of\nthe model as well as with more sophisticated prompting strategies.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nLW5CfHJ3ZQcmkcpg5hJAr-iB3nvIwHlIVch3hVvxNM","pdfSize":"498513"}
