{"id":"2408.03867","title":"Surgformer: Surgical Transformer with Hierarchical Temporal Attention\n  for Surgical Phase Recognition","authors":"Shu Yang, Luyang Luo, Qiong Wang, Hao Chen","authorsParsed":[["Yang","Shu",""],["Luo","Luyang",""],["Wang","Qiong",""],["Chen","Hao",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 16:16:31 GMT"}],"updateDate":"2024-08-08","timestamp":1723047391000,"abstract":"  Existing state-of-the-art methods for surgical phase recognition either rely\non the extraction of spatial-temporal features at a short-range temporal\nresolution or adopt the sequential extraction of the spatial and temporal\nfeatures across the entire temporal resolution. However, these methods have\nlimitations in modeling spatial-temporal dependency and addressing\nspatial-temporal redundancy: 1) These methods fail to effectively model\nspatial-temporal dependency, due to the lack of long-range information or joint\nspatial-temporal modeling. 2) These methods utilize dense spatial features\nacross the entire temporal resolution, resulting in significant\nspatial-temporal redundancy. In this paper, we propose the Surgical Transformer\n(Surgformer) to address the issues of spatial-temporal modeling and redundancy\nin an end-to-end manner, which employs divided spatial-temporal attention and\ntakes a limited set of sparse frames as input. Moreover, we propose a novel\nHierarchical Temporal Attention (HTA) to capture both global and local\ninformation within varied temporal resolutions from a target frame-centric\nperspective. Distinct from conventional temporal attention that primarily\nemphasizes dense long-range similarity, HTA not only captures long-term\ninformation but also considers local latent consistency among informative\nframes. HTA then employs pyramid feature aggregation to effectively utilize\ntemporal information across diverse temporal resolutions, thereby enhancing the\noverall temporal representation. Extensive experiments on two challenging\nbenchmark datasets verify that our proposed Surgformer performs favorably\nagainst the state-of-the-art methods. The code is released at\nhttps://github.com/isyangshu/Surgformer.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}