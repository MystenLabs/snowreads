{"id":"2408.15133","title":"Using LLMs for Explaining Sets of Counterfactual Examples to Final Users","authors":"Arturo Fredes and Jordi Vitria","authorsParsed":[["Fredes","Arturo",""],["Vitria","Jordi",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 15:13:06 GMT"}],"updateDate":"2024-08-28","timestamp":1724771586000,"abstract":"  Causality is vital for understanding true cause-and-effect relationships\nbetween variables within predictive models, rather than relying on mere\ncorrelations, making it highly relevant in the field of Explainable AI. In an\nautomated decision-making scenario, causal inference methods can analyze the\nunderlying data-generation process, enabling explanations of a model's decision\nby manipulating features and creating counterfactual examples. These\ncounterfactuals explore hypothetical scenarios where a minimal number of\nfactors are altered, providing end-users with valuable information on how to\nchange their situation. However, interpreting a set of multiple counterfactuals\ncan be challenging for end-users who are not used to analyzing raw data\nrecords. In our work, we propose a novel multi-step pipeline that uses\ncounterfactuals to generate natural language explanations of actions that will\nlead to a change in outcome in classifiers of tabular data using LLMs. This\npipeline is designed to guide the LLM through smaller tasks that mimic human\nreasoning when explaining a decision based on counterfactual cases. We\nconducted various experiments using a public dataset and proposed a method of\nclosed-loop evaluation to assess the coherence of the final explanation with\nthe counterfactuals, as well as the quality of the content. Results are\npromising, although further experiments with other datasets and human\nevaluations should be carried out.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"A-qxURoNGHTEj5epawZ9_z5CWmVywrwUZphCuGt2ZuE","pdfSize":"702233"}
