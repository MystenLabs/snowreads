{"id":"2408.14513","title":"Variational autoencoder-based neural network model compression","authors":"Liang Cheng, Peiyuan Guan, Amir Taherkordi, Lei Liu, Dapeng Lan","authorsParsed":[["Cheng","Liang",""],["Guan","Peiyuan",""],["Taherkordi","Amir",""],["Liu","Lei",""],["Lan","Dapeng",""]],"versions":[{"version":"v1","created":"Sun, 25 Aug 2024 09:06:22 GMT"}],"updateDate":"2024-08-28","timestamp":1724576782000,"abstract":"  Variational Autoencoders (VAEs), as a form of deep generative model, have\nbeen widely used in recent years, and shown great great peformance in a number\nof different domains, including image generation and anomaly detection, etc..\nThis paper aims to explore neural network model compression method based on\nVAE. The experiment uses different neural network models for MNIST recognition\nas compression targets, including Feedforward Neural Network (FNN),\nConvolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Long\nShort-Term Memory (LSTM). These models are the most basic models in deep\nlearning, and other more complex and advanced models are based on them or\ninherit their features and evolve. In the experiment, the first step is to\ntrain the models mentioned above, each trained model will have different\naccuracy and number of total parameters. And then the variants of parameters\nfor each model are processed as training data in VAEs separately, and the\ntrained VAEs are tested by the true model parameters. The experimental results\nshow that using the latent space as a representation of the model compression\ncan improve the compression rate compared to some traditional methods such as\npruning and quantization, meanwhile the accuracy is not greatly affected using\nthe model parameters reconstructed based on the latent space. In the future, a\nvariety of different large-scale deep learning models will be used more widely,\nso exploring different ways to save time and space on saving or transferring\nmodels will become necessary, and the use of VAE in this paper can provide a\nbasis for these further explorations.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}