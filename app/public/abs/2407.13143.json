{"id":"2407.13143","title":"Integrated Hardware Architecture and Device Placement Search","authors":"Irene Wang, Jakub Tarnawski, Amar Phanishayee, Divya Mahajan","authorsParsed":[["Wang","Irene",""],["Tarnawski","Jakub",""],["Phanishayee","Amar",""],["Mahajan","Divya",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 04:02:35 GMT"}],"updateDate":"2024-07-19","timestamp":1721275355000,"abstract":"  Distributed execution of deep learning training involves a dynamic interplay\nbetween hardware accelerator architecture and device placement strategy. This\nis the first work to explore the co-optimization of determining the optimal\narchitecture and device placement strategy through novel algorithms, improving\nthe balance of computational resources, memory usage, and data distribution.\nOur architecture search leverages tensor and vector units, determining their\nquantity and dimensionality, and on-chip and off-chip memory configurations. It\nalso determines the microbatch size and decides whether to recompute or stash\nactivations, balancing the memory footprint of training and storage size. For\neach explored architecture configuration, we use an Integer Linear Program\n(ILP) to find the optimal schedule for executing operators on the accelerator.\nThe ILP results then integrate with a dynamic programming solution to identify\nthe most effective device placement strategy, combining data, pipeline, and\ntensor model parallelism across multiple accelerators. Our approach achieves\nhigher throughput on large language models compared to the state-of-the-art\nTPUv4 and the Spotlight accelerator search framework. The entire source code of\nPHAZE is available at https://github.com/msr-fiddle/phaze.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Hardware Architecture","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pII-gkIGBvnM1MshQqQHugEKMT43KbzB3hnW6z7bx4E","pdfSize":"1425917"}
