{"id":"2408.15138","title":"How transformers learn structured data: insights from hierarchical\n  filtering","authors":"Jerome Garnier-Brun, Marc M\\'ezard, Emanuele Moscato, Luca Saglietti","authorsParsed":[["Garnier-Brun","Jerome",""],["MÃ©zard","Marc",""],["Moscato","Emanuele",""],["Saglietti","Luca",""]],"versions":[{"version":"v1","created":"Tue, 27 Aug 2024 15:23:09 GMT"}],"updateDate":"2024-08-28","timestamp":1724772189000,"abstract":"  We introduce a hierarchical filtering procedure for generative models of\nsequences on trees, enabling control over the range of positional correlations\nin the data. Leveraging this controlled setting, we provide evidence that\nvanilla encoder-only transformer architectures can implement the optimal Belief\nPropagation algorithm on both root classification and masked language modeling\ntasks. Correlations at larger distances corresponding to increasing layers of\nthe hierarchy are sequentially included as the network is trained. We analyze\nhow the transformer layers succeed by focusing on attention maps from models\ntrained with varying degrees of filtering. These attention maps show clear\nevidence for iterative hierarchical reconstruction of correlations, and we can\nrelate these observations to a plausible implementation of the exact inference\nalgorithm for the network sizes considered.\n","subjects":["Computing Research Repository/Machine Learning","Condensed Matter/Disordered Systems and Neural Networks","Condensed Matter/Statistical Mechanics","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}