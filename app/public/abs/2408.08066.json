{"id":"2408.08066","title":"Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense\n  Retrieval","authors":"Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, Jiaxin Mao","authorsParsed":[["Zhang","Hanqi",""],["Chen","Chong",""],["Mei","Lang",""],["Liu","Qi",""],["Mao","Jiaxin",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 10:15:37 GMT"},{"version":"v2","created":"Thu, 22 Aug 2024 15:07:40 GMT"}],"updateDate":"2024-08-23","timestamp":1723716937000,"abstract":"  In the information retrieval (IR) area, dense retrieval (DR) models use deep\nlearning techniques to encode queries and passages into embedding space to\ncompute their semantic relations. It is important for DR models to balance both\nefficiency and effectiveness. Pre-trained language models (PLMs), especially\nTransformer-based PLMs, have been proven to be effective encoders of DR models.\nHowever, the self-attention component in Transformer-based PLM results in a\ncomputational complexity that grows quadratically with sequence length, and\nthus exhibits a slow inference speed for long-text retrieval. Some recently\nproposed non-Transformer PLMs, especially the Mamba architecture PLMs, have\ndemonstrated not only comparable effectiveness to Transformer-based PLMs on\ngenerative language tasks but also better efficiency due to linear time scaling\nin sequence length. This paper implements the Mamba Retriever to explore\nwhether Mamba can serve as an effective and efficient encoder of DR model for\nIR tasks. We fine-tune the Mamba Retriever on the classic short-text MS MARCO\npassage ranking dataset and the long-text LoCoV0 dataset. Experimental results\nshow that (1) on the MS MARCO passage ranking dataset and BEIR, the Mamba\nRetriever achieves comparable or better effectiveness compared to\nTransformer-based retrieval models, and the effectiveness grows with the size\nof the Mamba model; (2) on the long-text LoCoV0 dataset, the Mamba Retriever\ncan extend to longer text length than its pre-trained length after fine-tuning\non retrieval task, and it has comparable or better effectiveness compared to\nother long-text retrieval models; (3) the Mamba Retriever has superior\ninference speed for long-text retrieval. In conclusion, Mamba Retriever is both\neffective and efficient, making it a practical model, especially for long-text\nretrieval.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}