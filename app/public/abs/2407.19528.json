{"id":"2407.19528","title":"Motamot: A Dataset for Revealing the Supremacy of Large Language Models\n  over Transformer Models in Bengali Political Sentiment Analysis","authors":"Fatema Tuj Johora Faria, Mukaffi Bin Moin, Rabeya Islam Mumu, Md\n  Mahabubul Alam Abir, Abrar Nawar Alfy, Mohammad Shafiul Alam","authorsParsed":[["Faria","Fatema Tuj Johora",""],["Moin","Mukaffi Bin",""],["Mumu","Rabeya Islam",""],["Abir","Md Mahabubul Alam",""],["Alfy","Abrar Nawar",""],["Alam","Mohammad Shafiul",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 16:34:53 GMT"}],"updateDate":"2024-07-30","timestamp":1722184493000,"abstract":"  Sentiment analysis is the process of identifying and categorizing people's\nemotions or opinions regarding various topics. Analyzing political sentiment is\ncritical for understanding the complexities of public opinion processes,\nespecially during election seasons. It gives significant information on voter\npreferences, attitudes, and current trends. In this study, we investigate\npolitical sentiment analysis during Bangladeshi elections, specifically\nexamining how effectively Pre-trained Language Models (PLMs) and Large Language\nModels (LLMs) capture complex sentiment characteristics. Our study centers on\nthe creation of the \"Motamot\" dataset, comprising 7,058 instances annotated\nwith positive and negative sentiments, sourced from diverse online newspaper\nportals, forming a comprehensive resource for political sentiment analysis. We\nmeticulously evaluate the performance of various PLMs including BanglaBERT,\nBangla BERT Base, XLM-RoBERTa, mBERT, and sahajBERT, alongside LLMs such as\nGemini 1.5 Pro and GPT 3.5 Turbo. Moreover, we explore zero-shot and few-shot\nlearning strategies to enhance our understanding of political sentiment\nanalysis methodologies. Our findings underscore BanglaBERT's commendable\naccuracy of 88.10% among PLMs. However, the exploration into LLMs reveals even\nmore promising results. Through the adept application of Few-Shot learning\ntechniques, Gemini 1.5 Pro achieves an impressive accuracy of 96.33%,\nsurpassing the remarkable performance of GPT 3.5 Turbo, which stands at 94%.\nThis underscores Gemini 1.5 Pro's status as the superior performer in this\ncomparison.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jzCso6drZ38iF3L6JW__TkWzVdhAuw0txZPpsU6HDCo","pdfSize":"2246625"}
