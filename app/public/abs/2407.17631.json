{"id":"2407.17631","title":"BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic\n  Chunking and Hard Example Learning","authors":"Partha Chakraborty, Mahmoud Alfadel, Meiyappan Nagappan","authorsParsed":[["Chakraborty","Partha",""],["Alfadel","Mahmoud",""],["Nagappan","Meiyappan",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 20:44:36 GMT"},{"version":"v2","created":"Mon, 19 Aug 2024 13:59:30 GMT"}],"updateDate":"2024-08-20","timestamp":1721853876000,"abstract":"  Software bugs require developers to exert significant effort to identify and\nresolve them, often consuming about one-third of their time. Bug localization,\nthe process of pinpointing the exact source code files that need modification,\nis crucial in reducing this effort. Existing bug localization tools, typically\nreliant on deep learning techniques, face limitations in cross-project\napplicability and effectiveness in multi-language environments. Recent\nadvancements with Large Language Models (LLMs) offer detailed representations\nfor bug localization. However, they encounter challenges with limited context\nwindows and mapping accuracy. To address these issues, we propose BLAZE, an\napproach that employs dynamic chunking and hard example learning. First, BLAZE\ndynamically segments source code to minimize continuity loss. Then, BLAZE\nfine-tunes a GPT-based model using challenging bug cases, in order to enhance\ncross-project and cross-language bug localization. To support the capability of\nBLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29\nlarge and thriving open-source projects across five different programming\nlanguages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on\nthree benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate\nsubstantial improvements compared to six state-of-the-art baselines.\nSpecifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144%\nin Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An\nextensive ablation study confirms the contributions of our pipeline components\nto the overall performance enhancement.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}