{"id":"2408.08930","title":"DePrompt: Desensitization and Evaluation of Personal Identifiable\n  Information in Large Language Model Prompts","authors":"Xiongtao Sun, Gan Liu, Zhipeng He, Hui Li, Xiaoguang Li","authorsParsed":[["Sun","Xiongtao",""],["Liu","Gan",""],["He","Zhipeng",""],["Li","Hui",""],["Li","Xiaoguang",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 02:38:25 GMT"}],"updateDate":"2024-08-20","timestamp":1723775905000,"abstract":"  Prompt serves as a crucial link in interacting with large language models\n(LLMs), widely impacting the accuracy and interpretability of model outputs.\nHowever, acquiring accurate and high-quality responses necessitates precise\nprompts, which inevitably pose significant risks of personal identifiable\ninformation (PII) leakage. Therefore, this paper proposes DePrompt, a\ndesensitization protection and effectiveness evaluation framework for prompt,\nenabling users to safely and transparently utilize LLMs. Specifically, by\nleveraging large model fine-tuning techniques as the underlying privacy\nprotection method, we integrate contextual attributes to define privacy types,\nachieving high-precision PII entity identification. Additionally, through the\nanalysis of key features in prompt desensitization scenarios, we devise\nadversarial generative desensitization methods that retain important semantic\ncontent while disrupting the link between identifiers and privacy attributes.\nFurthermore, we present utility evaluation metrics for prompt to better gauge\nand balance privacy and usability. Our framework is adaptable to prompts and\ncan be extended to text usability-dependent scenarios. Through comparison with\nbenchmarks and other model methods, experimental evaluations demonstrate that\nour desensitized prompt exhibit superior privacy protection utility and model\ninference results.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}