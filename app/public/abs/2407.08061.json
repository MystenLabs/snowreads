{"id":"2407.08061","title":"Geospecific View Generation -- Geometry-Context Aware High-resolution\n  Ground View Inference from Satellite Views","authors":"Ningli Xu, Rongjun Qin","authorsParsed":[["Xu","Ningli",""],["Qin","Rongjun",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 21:51:50 GMT"},{"version":"v2","created":"Mon, 29 Jul 2024 14:49:42 GMT"},{"version":"v3","created":"Fri, 6 Sep 2024 14:35:57 GMT"},{"version":"v4","created":"Thu, 12 Sep 2024 18:35:41 GMT"}],"updateDate":"2024-09-16","timestamp":1720648310000,"abstract":"  Predicting realistic ground views from satellite imagery in urban scenes is a\nchallenging task due to the significant view gaps between satellite and\nground-view images. We propose a novel pipeline to tackle this challenge, by\ngenerating geospecifc views that maximally respect the weak geometry and\ntexture from multi-view satellite images. Different from existing approaches\nthat hallucinate images from cues such as partial semantics or geometry from\noverhead satellite images, our method directly predicts ground-view images at\ngeolocation by using a comprehensive set of information from the satellite\nimage, resulting in ground-level images with a resolution boost at a factor of\nten or more. We leverage a novel building refinement method to reduce geometric\ndistortions in satellite data at ground level, which ensures the creation of\naccurate conditions for view synthesis using diffusion networks. Moreover, we\nproposed a novel geospecific prior, which prompts distribution learning of\ndiffusion models to respect image samples that are closer to the geolocation of\nthe predicted images. We demonstrate our pipeline is the first to generate\nclose-to-real and geospecific ground views merely based on satellite images.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}