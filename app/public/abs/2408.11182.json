{"id":"2408.11182","title":"Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large\n  Language Models through Neural Carrier Articles","authors":"Zhilong Wang and Haizhou Wang and Nanqing Luo and Lan Zhang and\n  Xiaoyan Sun and Yebo Cao and Peng Liu","authorsParsed":[["Wang","Zhilong",""],["Wang","Haizhou",""],["Luo","Nanqing",""],["Zhang","Lan",""],["Sun","Xiaoyan",""],["Cao","Yebo",""],["Liu","Peng",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 20:35:04 GMT"}],"updateDate":"2024-08-22","timestamp":1724186104000,"abstract":"  Jailbreak attacks on Language Model Models (LLMs) entail crafting prompts\naimed at exploiting the models to generate malicious content. This paper\nproposes a new type of jailbreak attacks which shift the attention of the LLM\nby inserting a prohibited query into a carrier article. The proposed attack\nleverage the knowledge graph and a composer LLM to automatically generating a\ncarrier article that is similar to the topic of the prohibited query but does\nnot violate LLM's safeguards. By inserting the malicious query to the carrier\narticle, the assembled attack payload can successfully jailbreak LLM. To\nevaluate the effectiveness of our method, we leverage 4 popular categories of\n``harmful behaviors'' adopted by related researches to attack 6 popular LLMs.\nOur experiment results show that the proposed attacking method can successfully\njailbreak all the target LLMs which high success rate, except for Claude-3.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}