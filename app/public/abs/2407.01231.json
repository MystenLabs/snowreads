{"id":"2407.01231","title":"MIRAI: Evaluating LLM Agents for Event Forecasting","authors":"Chenchen Ye, Ziniu Hu, Yihe Deng, Zijie Huang, Mingyu Derek Ma,\n  Yanqiao Zhu, Wei Wang","authorsParsed":[["Ye","Chenchen",""],["Hu","Ziniu",""],["Deng","Yihe",""],["Huang","Zijie",""],["Ma","Mingyu Derek",""],["Zhu","Yanqiao",""],["Wang","Wei",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 12:22:46 GMT"}],"updateDate":"2024-07-02","timestamp":1719836566000,"abstract":"  Recent advancements in Large Language Models (LLMs) have empowered LLM agents\nto autonomously collect world information, over which to conduct reasoning to\nsolve complex problems. Given this capability, increasing interests have been\nput into employing LLM agents for predicting international events, which can\ninfluence decision-making and shape policy development on an international\nscale. Despite such a growing interest, there is a lack of a rigorous benchmark\nof LLM agents' forecasting capability and reliability. To address this gap, we\nintroduce MIRAI, a novel benchmark designed to systematically evaluate LLM\nagents as temporal forecasters in the context of international events. Our\nbenchmark features an agentic environment with tools for accessing an extensive\ndatabase of historical, structured events and textual news articles. We refine\nthe GDELT event database with careful cleaning and parsing to curate a series\nof relational prediction tasks with varying forecasting horizons, assessing LLM\nagents' abilities from short-term to long-term forecasting. We further\nimplement APIs to enable LLM agents to utilize different tools via a code-based\ninterface. In summary, MIRAI comprehensively evaluates the agents' capabilities\nin three dimensions: 1) autonomously source and integrate critical information\nfrom large global databases; 2) write codes using domain-specific APIs and\nlibraries for tool-use; and 3) jointly reason over historical knowledge from\ndiverse formats and time to accurately predict future events. Through\ncomprehensive benchmarking, we aim to establish a reliable framework for\nassessing the capabilities of LLM agents in forecasting international events,\nthereby contributing to the development of more accurate and trustworthy models\nfor international relation analysis.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}