{"id":"2407.17783","title":"How Lightweight Can A Vision Transformer Be","authors":"Jen Hong Tan","authorsParsed":[["Tan","Jen Hong",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 05:23:20 GMT"}],"updateDate":"2024-07-26","timestamp":1721885000000,"abstract":"  In this paper, we explore a strategy that uses Mixture-of-Experts (MoE) to\nstreamline, rather than augment, vision transformers. Each expert in an MoE\nlayer is a SwiGLU feedforward network, where V and W2 are shared across the\nlayer. No complex attention or convolutional mechanisms are employed.\nDepth-wise scaling is applied to progressively reduce the size of the hidden\nlayer and the number of experts is increased in stages. Grouped query attention\nis used. We studied the proposed approach with and without pre-training on\nsmall datasets and investigated whether transfer learning works at this scale.\nWe found that the architecture is competitive even at a size of 0.67M\nparameters.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"R2bDFMzaB2Fwl0Arxhd-C4vcOfwjy8D50R773mFTDA4","pdfSize":"631218"}
