{"id":"2408.08761","title":"SYMPOL: Symbolic Tree-Based On-Policy Reinforcement Learning","authors":"Sascha Marton, Tim Grams, Florian Vogt, Stefan L\\\"udtke, Christian\n  Bartelt, Heiner Stuckenschmidt","authorsParsed":[["Marton","Sascha",""],["Grams","Tim",""],["Vogt","Florian",""],["LÃ¼dtke","Stefan",""],["Bartelt","Christian",""],["Stuckenschmidt","Heiner",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 14:04:40 GMT"},{"version":"v2","created":"Thu, 19 Sep 2024 10:02:44 GMT"}],"updateDate":"2024-09-20","timestamp":1723817080000,"abstract":"  Reinforcement learning (RL) has seen significant success across various\ndomains, but its adoption is often limited by the black-box nature of neural\nnetwork policies, making them difficult to interpret. In contrast, symbolic\npolicies allow representing decision-making strategies in a compact and\ninterpretable way. However, learning symbolic policies directly within\non-policy methods remains challenging. In this paper, we introduce SYMPOL, a\nnovel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based\nmodel integrated with a policy gradient method, enabling the agent to learn and\nadapt its actions while maintaining a high level of interpretability. We\nevaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority\nover alternative tree-based RL approaches in terms of performance and\ninterpretability. To the best of our knowledge, this is the first method, that\nallows a gradient-based end-to-end learning of interpretable, axis-aligned\ndecision trees within existing on-policy RL algorithms. Therefore, SYMPOL can\nbecome the foundation for a new class of interpretable RL based on decision\ntrees. Our implementation is available under:\nhttps://github.com/s-marton/SYMPOL\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"q3C1gIm15SsXC-FseUAYER5cwnUpM295hZpWE8ClPYw","pdfSize":"2039339"}
