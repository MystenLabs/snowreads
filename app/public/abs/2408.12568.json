{"id":"2408.12568","title":"Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune\n  CNNs and Transformers","authors":"Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Reduan Achtibat,\n  Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin","authorsParsed":[["Hatefi","Sayed Mohammad Vakilzadeh",""],["Dreyer","Maximilian",""],["Achtibat","Reduan",""],["Wiegand","Thomas",""],["Samek","Wojciech",""],["Lapuschkin","Sebastian",""]],"versions":[{"version":"v1","created":"Thu, 22 Aug 2024 17:35:18 GMT"}],"updateDate":"2024-08-23","timestamp":1724348118000,"abstract":"  To solve ever more complex problems, Deep Neural Networks are scaled to\nbillions of parameters, leading to huge computational costs. An effective\napproach to reduce computational requirements and increase efficiency is to\nprune unnecessary components of these often over-parameterized networks.\nPrevious work has shown that attribution methods from the field of eXplainable\nAI serve as effective means to extract and prune the least relevant network\ncomponents in a few-shot fashion. We extend the current state by proposing to\nexplicitly optimize hyperparameters of attribution methods for the task of\npruning, and further include transformer-based networks in our analysis. Our\napproach yields higher model compression rates of large transformer- and\nconvolutional architectures (VGG, ResNet, ViT) compared to previous works,\nwhile still attaining high performance on ImageNet classification tasks. Here,\nour experiments indicate that transformers have a higher degree of\nover-parameterization compared to convolutional neural networks. Code is\navailable at\n$\\href{https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch}{\\text{this\nhttps link}}$.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"dYgqlw8ddA--FdwdHnTSOQVKTDv8wXL-jJHnY84naLE","pdfSize":"6354079"}
