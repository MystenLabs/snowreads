{"id":"2408.05749","title":"Efficient and Versatile Robust Fine-Tuning of Zero-shot Models","authors":"Sungyeon Kim, Boseung Jeong, Donghyun Kim, Suha Kwak","authorsParsed":[["Kim","Sungyeon",""],["Jeong","Boseung",""],["Kim","Donghyun",""],["Kwak","Suha",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 11:37:43 GMT"}],"updateDate":"2024-08-13","timestamp":1723376263000,"abstract":"  Large-scale image-text pre-trained models enable zero-shot classification and\nprovide consistent accuracy across various data distributions. Nonetheless,\noptimizing these models in downstream tasks typically requires fine-tuning,\nwhich reduces generalization to out-of-distribution (OOD) data and demands\nextensive computational resources. We introduce Robust Adapter (R-Adapter), a\nnovel method for fine-tuning zero-shot models to downstream tasks while\nsimultaneously addressing both these issues. Our method integrates lightweight\nmodules into the pre-trained model and employs novel self-ensemble techniques\nto boost OOD robustness and reduce storage expenses substantially. Furthermore,\nwe propose MPM-NCE loss designed for fine-tuning on vision-language downstream\ntasks. It ensures precise alignment of multiple image-text pairs and\ndiscriminative feature learning. By extending the benchmark for robust\nfine-tuning beyond classification to include diverse tasks such as cross-modal\nretrieval and open vocabulary segmentation, we demonstrate the broad\napplicability of R-Adapter. Our extensive experiments demonstrate that\nR-Adapter achieves state-of-the-art performance across a diverse set of tasks,\ntuning only 13% of the parameters of the CLIP encoders.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}