{"id":"2408.04682","title":"ToolSandbox: A Stateful, Conversational, Interactive Evaluation\n  Benchmark for LLM Tool Use Capabilities","authors":"Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan,\n  Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, Ruoming Pang","authorsParsed":[["Lu","Jiarui",""],["Holleis","Thomas",""],["Zhang","Yizhe",""],["Aumayer","Bernhard",""],["Nan","Feng",""],["Bai","Felix",""],["Ma","Shuang",""],["Ma","Shen",""],["Li","Mengyu",""],["Yin","Guoli",""],["Wang","Zirui",""],["Pang","Ruoming",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 05:45:42 GMT"}],"updateDate":"2024-08-12","timestamp":1723095942000,"abstract":"  Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"lzPdsCqqN3L8qa3dUpcbQ9751CSL9BfhMURc-6Do3gE","pdfSize":"19614270"}
