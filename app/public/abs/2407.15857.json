{"id":"2407.15857","title":"BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-task Large\n  Language Models","authors":"Simen Eide, Arnoldo Frigessi","authorsParsed":[["Eide","Simen",""],["Frigessi","Arnoldo",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 06:38:50 GMT"}],"updateDate":"2024-07-24","timestamp":1720420730000,"abstract":"  This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel\nmethod for finetuning multi-task Large Language Models (LLMs). Current\nfinetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally\nwell in reducing training parameters and memory usage but face limitations when\napplied to multiple similar tasks. Practitioners usually have to choose between\ntraining separate models for each task or a single model for all tasks, both of\nwhich come with trade-offs in specialization and data utilization.\n  BoRA addresses these trade-offs by leveraging a Bayesian hierarchical model\nthat allows tasks to share information through global hierarchical priors. This\nenables tasks with limited data to benefit from the overall structure derived\nfrom related tasks while allowing tasks with more data to specialize. Our\nexperimental results show that BoRA outperforms both individual and unified\nmodel approaches, achieving lower perplexity and better generalization across\ntasks. This method provides a scalable and efficient solution for multi-task\nLLM finetuning, with significant practical implications for diverse\napplications.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}