{"id":"2407.19897","title":"BEExAI: Benchmark to Evaluate Explainable AI","authors":"Samuel Sithakoul, Sara Meftah and Cl\\'ement Feutry","authorsParsed":[["Sithakoul","Samuel",""],["Meftah","Sara",""],["Feutry","Cl√©ment",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 11:21:17 GMT"}],"updateDate":"2024-07-30","timestamp":1722252077000,"abstract":"  Recent research in explainability has given rise to numerous post-hoc\nattribution methods aimed at enhancing our comprehension of the outputs of\nblack-box machine learning models. However, evaluating the quality of\nexplanations lacks a cohesive approach and a consensus on the methodology for\nderiving quantitative metrics that gauge the efficacy of explainability\npost-hoc attribution methods. Furthermore, with the development of increasingly\ncomplex deep learning models for diverse data applications, the need for a\nreliable way of measuring the quality and correctness of explanations is\nbecoming critical. We address this by proposing BEExAI, a benchmark tool that\nallows large-scale comparison of different post-hoc XAI methods, employing a\nset of selected evaluation metrics.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}