{"id":"2407.04864","title":"Augmented Bayesian Policy Search","authors":"Mahdi Kallel, Debabrota Basu, Riad Akrour, Carlo D'Eramo","authorsParsed":[["Kallel","Mahdi",""],["Basu","Debabrota",""],["Akrour","Riad",""],["D'Eramo","Carlo",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 20:56:45 GMT"}],"updateDate":"2024-07-09","timestamp":1720213005000,"abstract":"  Deterministic policies are often preferred over stochastic ones when\nimplemented on physical systems. They can prevent erratic and harmful behaviors\nwhile being easier to implement and interpret. However, in practice,\nexploration is largely performed by stochastic policies. First-order Bayesian\nOptimization (BO) methods offer a principled way of performing exploration\nusing deterministic policies. This is done through a learned probabilistic\nmodel of the objective function and its gradient. Nonetheless, such approaches\ntreat policy search as a black-box problem, and thus, neglect the reinforcement\nlearning nature of the problem. In this work, we leverage the performance\ndifference lemma to introduce a novel mean function for the probabilistic\nmodel. This results in augmenting BO methods with the action-value function.\nHence, we call our method Augmented Bayesian Search~(ABS). Interestingly, this\nnew mean function enhances the posterior gradient with the deterministic policy\ngradient, effectively bridging the gap between BO and policy gradient methods.\nThe resulting algorithm combines the convenience of the direct policy search\nwith the scalability of reinforcement learning. We validate ABS on\nhigh-dimensional locomotion problems and demonstrate competitive performance\ncompared to existing direct policy search schemes.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mkqYF3G7VOt6j7yBxF_sOvDA3GUVMdcSx60Ua9btEw8","pdfSize":"1146546"}
