{"id":"2407.21571","title":"PMoE: Progressive Mixture of Experts with Asymmetric Transformer for\n  Continual Learning","authors":"Min Jae Jung, JooHee Kim","authorsParsed":[["Jung","Min Jae",""],["Kim","JooHee",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 12:56:14 GMT"}],"updateDate":"2024-08-01","timestamp":1722430574000,"abstract":"  Large Language Models (LLMs) encounter significant challenges in continual\nlearning due to catastrophic forgetting, where new information overwrites\npreviously acquired knowledge. This limitation leads to substantial\nenvironmental and economic waste. In this study, we introduce the PMoE,\nProgressive Mixture of Experts with Asymmetric Transformer, which aims to\nminimize forgetting by utilizing an asymmetric design with shallow layers\ndedicated to general knowledge and deep layers for new knowledge. PMoE\nincorporates progressively added experts in deep layers and a router that\nallocates new knowledge to the appropriate experts efficiently. The router,\npositioned adjacent to the deep layers, utilizes deep features aggregating\nconsolidated information. This enables the router to perform efficiently,\nallocating new knowledge to the appropriate experts, which progressively\nincrease in the deep layers. Extensive experiments on TRACE datasets and\ngeneral language understanding datasets demonstrate that the proposed PMoE\noutperforms previous state-of-the-art approaches.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"hssQwaTVUo0NcUQgK1oFsJbGyGOQ7DU7W6YNQ3mXR5Q","pdfSize":"527433"}
