{"id":"2407.15452","title":"GraphScale: A Framework to Enable Machine Learning over Billion-node\n  Graphs","authors":"Vipul Gupta, Xin Chen, Ruoyun Huang, Fanlong Meng, Jianjun Chen, Yujun\n  Yan","authorsParsed":[["Gupta","Vipul",""],["Chen","Xin",""],["Huang","Ruoyun",""],["Meng","Fanlong",""],["Chen","Jianjun",""],["Yan","Yujun",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 08:09:36 GMT"}],"updateDate":"2024-07-23","timestamp":1721635776000,"abstract":"  Graph Neural Networks (GNNs) have emerged as powerful tools for supervised\nmachine learning over graph-structured data, while sampling-based node\nrepresentation learning is widely utilized in unsupervised learning. However,\nscalability remains a major challenge in both supervised and unsupervised\nlearning for large graphs (e.g., those with over 1 billion nodes). The\nscalability bottleneck largely stems from the mini-batch sampling phase in GNNs\nand the random walk sampling phase in unsupervised methods. These processes\noften require storing features or embeddings in memory. In the context of\ndistributed training, they require frequent, inefficient random access to data\nstored across different workers. Such repeated inter-worker communication for\neach mini-batch leads to high communication overhead and computational\ninefficiency.\n  We propose GraphScale, a unified framework for both supervised and\nunsupervised learning to store and process large graph data distributedly. The\nkey insight in our design is the separation of workers who store data and those\nwho perform the training. This separation allows us to decouple computing and\nstorage in graph training, thus effectively building a pipeline where data\nfetching and data computation can overlap asynchronously. Our experiments show\nthat GraphScale outperforms state-of-the-art methods for distributed training\nof both GNNs and node embeddings. We evaluate GraphScale both on public and\nproprietary graph datasets and observe a reduction of at least 40% in\nend-to-end training times compared to popular distributed frameworks, without\nany loss in performance. While most existing methods don't support billion-node\ngraphs for training node embeddings, GraphScale is currently deployed in\nproduction at TikTok enabling efficient learning over such large graphs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Social and Information Networks"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}