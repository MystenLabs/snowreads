{"id":"2407.01281","title":"Bridging Smoothness and Approximation: Theoretical Insights into\n  Over-Smoothing in Graph Neural Networks","authors":"Guangrui Yang, Jianfei Li, Ming Li, Han Feng, Ding-Xuan Zhou","authorsParsed":[["Yang","Guangrui",""],["Li","Jianfei",""],["Li","Ming",""],["Feng","Han",""],["Zhou","Ding-Xuan",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 13:35:53 GMT"},{"version":"v2","created":"Mon, 5 Aug 2024 15:50:32 GMT"}],"updateDate":"2024-08-06","timestamp":1719840953000,"abstract":"  In this paper, we explore the approximation theory of functions defined on\ngraphs. Our study builds upon the approximation results derived from the\n$K$-functional. We establish a theoretical framework to assess the lower bounds\nof approximation for target functions using Graph Convolutional Networks (GCNs)\nand examine the over-smoothing phenomenon commonly observed in these networks.\nInitially, we introduce the concept of a $K$-functional on graphs, establishing\nits equivalence to the modulus of smoothness. We then analyze a typical type of\nGCN to demonstrate how the high-frequency energy of the output decays, an\nindicator of over-smoothing. This analysis provides theoretical insights into\nthe nature of over-smoothing within GCNs. Furthermore, we establish a lower\nbound for the approximation of target functions by GCNs, which is governed by\nthe modulus of smoothness of these functions. This finding offers a new\nperspective on the approximation capabilities of GCNs. In our numerical\nexperiments, we analyze several widely applied GCNs and observe the phenomenon\nof energy decay. These observations corroborate our theoretical results on\nexponential decay order.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Mathematics/Functional Analysis"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}