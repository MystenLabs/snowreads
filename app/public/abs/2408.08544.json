{"id":"2408.08544","title":"Scaling up Multimodal Pre-training for Sign Language Understanding","authors":"Wengang Zhou, Weichao Zhao, Hezhen Hu, Zecheng Li, Houqiang Li","authorsParsed":[["Zhou","Wengang",""],["Zhao","Weichao",""],["Hu","Hezhen",""],["Li","Zecheng",""],["Li","Houqiang",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 06:04:25 GMT"}],"updateDate":"2024-08-19","timestamp":1723788265000,"abstract":"  Sign language serves as the primary meaning of communication for the\ndeaf-mute community. Different from spoken language, it commonly conveys\ninformation by the collaboration of manual features, i.e., hand gestures and\nbody movements, and non-manual features, i.e., facial expressions and mouth\ncues. To facilitate communication between the deaf-mute and hearing people, a\nseries of sign language understanding (SLU) tasks have been studied in recent\nyears, including isolated/continuous sign language recognition (ISLR/CSLR),\ngloss-free sign language translation (GF-SLT) and sign language retrieval\n(SL-RT). Sign language recognition and translation aims to understand the\nsemantic meaning conveyed by sign languages from gloss-level and\nsentence-level, respectively. In contrast, SL-RT focuses on retrieving sign\nvideos or corresponding texts from a closed-set under the query-by-example\nsearch paradigm. These tasks investigate sign language topics from diverse\nperspectives and raise challenges in learning effective representation of sign\nlanguage videos. To advance the development of sign language understanding,\nexploring a generalized model that is applicable across various SLU tasks is a\nprofound research direction.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}