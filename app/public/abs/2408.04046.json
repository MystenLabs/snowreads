{"id":"2408.04046","title":"Learning Rate-Free Reinforcement Learning: A Case for Model Selection\n  with Non-Stationary Objectives","authors":"Aida Afshar, Aldo Pacchiano","authorsParsed":[["Afshar","Aida",""],["Pacchiano","Aldo",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 18:55:58 GMT"}],"updateDate":"2024-08-09","timestamp":1723056958000,"abstract":"  The performance of reinforcement learning (RL) algorithms is sensitive to the\nchoice of hyperparameters, with the learning rate being particularly\ninfluential. RL algorithms fail to reach convergence or demand an extensive\nnumber of samples when the learning rate is not optimally set. In this work, we\nshow that model selection can help to improve the failure modes of RL that are\ndue to suboptimal choices of learning rate. We present a model selection\nframework for Learning Rate-Free Reinforcement Learning that employs model\nselection methods to select the optimal learning rate on the fly. This approach\nof adaptive learning rate tuning neither depends on the underlying RL algorithm\nnor the optimizer and solely uses the reward feedback to select the learning\nrate; hence, the framework can input any RL algorithm and produce a learning\nrate-free version of it. We conduct experiments for policy optimization methods\nand evaluate various model selection strategies within our framework. Our\nresults indicate that data-driven model selection algorithms are better\nalternatives to standard bandit algorithms when the optimal choice of\nhyperparameter is time-dependent and non-stationary.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}