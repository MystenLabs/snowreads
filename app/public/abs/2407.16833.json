{"id":"2407.16833","title":"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive\n  Study and Hybrid Approach","authors":"Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky","authorsParsed":[["Li","Zhuowan",""],["Li","Cheng",""],["Zhang","Mingyang",""],["Mei","Qiaozhu",""],["Bendersky","Michael",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 20:51:52 GMT"}],"updateDate":"2024-07-25","timestamp":1721767912000,"abstract":"  Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7jJk2VRShFfnEq210cDiMBhc7P0MPQ70jrnaOQDBSqw","pdfSize":"646292","objectId":"0x9bd79965bdd08203687ac84e348373e108ce316c9fab1c1af7d228b434fd09f4","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
