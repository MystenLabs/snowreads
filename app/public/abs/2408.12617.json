{"id":"2408.12617","title":"Can GPT-4 Models Detect Misleading Visualizations?","authors":"Jason Alexander, Priyal Nanda, Kai-Cheng Yang, Ali Sarvghad","authorsParsed":[["Alexander","Jason",""],["Nanda","Priyal",""],["Yang","Kai-Cheng",""],["Sarvghad","Ali",""]],"versions":[{"version":"v1","created":"Thu, 8 Aug 2024 22:51:02 GMT"}],"updateDate":"2024-08-26","timestamp":1723157462000,"abstract":"  The proliferation of misleading visualizations online, particularly during\ncritical events like public health crises and elections, poses a significant\nrisk. This study investigates the capability of GPT-4 models (4V, 4o, and 4o\nmini) to detect misleading visualizations. Utilizing a dataset of\ntweet-visualization pairs containing various visual misleaders, we test these\nmodels under four experimental conditions with different levels of guidance. We\nshow that GPT-4 models can detect misleading visualizations with moderate\naccuracy without prior training (naive zero-shot) and that performance notably\nimproves when provided with definitions of misleaders (guided zero-shot).\nHowever, a single prompt engineering technique does not yield the best results\nfor all misleader types. Specifically, providing the models with misleader\ndefinitions and examples (guided few-shot) proves more effective for reasoning\nmisleaders, while guided zero-shot performs better for design misleaders. This\nstudy underscores the feasibility of using large vision-language models to\ndetect visual misinformation and the importance of prompt engineering for\noptimized detection accuracy.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computers and Society","Computing Research Repository/Social and Information Networks"],"license":"http://creativecommons.org/licenses/by/4.0/"}