{"id":"2407.13135","title":"MLSA4Rec: Mamba Combined with Low-Rank Decomposed Self-Attention for\n  Sequential Recommendation","authors":"Jinzhao Su and Zhenhua Huang","authorsParsed":[["Su","Jinzhao",""],["Huang","Zhenhua",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 03:46:21 GMT"}],"updateDate":"2024-07-19","timestamp":1721274381000,"abstract":"  In applications such as e-commerce, online education, and streaming services,\nsequential recommendation systems play a critical role. Despite the excellent\nperformance of self-attention-based sequential recommendation models in\ncapturing dependencies between items in user interaction history, their\nquadratic complexity and lack of structural bias limit their applicability.\nRecently, some works have replaced the self-attention module in sequential\nrecommenders with Mamba, which has linear complexity and structural bias.\nHowever, these works have not noted the complementarity between the two\napproaches. To address this issue, this paper proposes a new hybrid\nrecommendation framework, Mamba combined with Low-Rank decomposed\nSelf-Attention for Sequential Recommendation (MLSA4Rec), whose complexity is\nlinear with respect to the length of the user's historical interaction\nsequence. Specifically, MLSA4Rec designs an efficient Mamba-LSA interaction\nmodule. This module introduces a low-rank decomposed self-attention (LSA)\nmodule with linear complexity and injects structural bias into it through\nMamba. The LSA module analyzes user preferences from a different perspective\nand dynamically guides Mamba to focus on important information in user\nhistorical interactions through a gated information transmission mechanism.\nFinally, MLSA4Rec combines user preference information refined by the Mamba and\nLSA modules to accurately predict the user's next possible interaction. To our\nknowledge, this is the first study to combine Mamba and self-attention in\nsequential recommendation systems. Experimental results show that MLSA4Rec\noutperforms existing self-attention and Mamba-based sequential recommendation\nmodels in recommendation accuracy on three real-world datasets, demonstrating\nthe great potential of Mamba and self-attention working together.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}