{"id":"2407.02245","title":"Safe CoR: A Dual-Expert Approach to Integrating Imitation Learning and\n  Safe Reinforcement Learning Using Constraint Rewards","authors":"Hyeokjin Kwon, Gunmin Lee, Junseo Lee, Songhwai Oh","authorsParsed":[["Kwon","Hyeokjin",""],["Lee","Gunmin",""],["Lee","Junseo",""],["Oh","Songhwai",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 13:05:16 GMT"}],"updateDate":"2024-07-03","timestamp":1719925516000,"abstract":"  In the realm of autonomous agents, ensuring safety and reliability in complex\nand dynamic environments remains a paramount challenge. Safe reinforcement\nlearning addresses these concerns by introducing safety constraints, but still\nfaces challenges in navigating intricate environments such as complex driving\nsituations. To overcome these challenges, we present the safe constraint reward\n(Safe CoR) framework, a novel method that utilizes two types of expert\ndemonstrations$\\unicode{x2013}$reward expert demonstrations focusing on\nperformance optimization and safe expert demonstrations prioritizing safety. By\nexploiting a constraint reward (CoR), our framework guides the agent to balance\nperformance goals of reward sum with safety constraints. We test the proposed\nframework in diverse environments, including the safety gym, metadrive, and the\nreal$\\unicode{x2013}$world Jackal platform. Our proposed framework enhances the\nperformance of algorithms by $39\\%$ and reduces constraint violations by $88\\%$\non the real-world Jackal platform, demonstrating the framework's efficacy.\nThrough this innovative approach, we expect significant advancements in\nreal-world performance, leading to transformative effects in the realm of safe\nand reliable autonomous agents.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}