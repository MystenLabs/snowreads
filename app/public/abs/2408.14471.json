{"id":"2408.14471","title":"A Practitioner's Guide to Continual Multimodal Pretraining","authors":"Karsten Roth, Vishaal Udandarao, Sebastian Dziadzio, Ameya Prabhu,\n  Mehdi Cherti, Oriol Vinyals, Olivier H\\'enaff, Samuel Albanie, Matthias\n  Bethge, Zeynep Akata","authorsParsed":[["Roth","Karsten",""],["Udandarao","Vishaal",""],["Dziadzio","Sebastian",""],["Prabhu","Ameya",""],["Cherti","Mehdi",""],["Vinyals","Oriol",""],["HÃ©naff","Olivier",""],["Albanie","Samuel",""],["Bethge","Matthias",""],["Akata","Zeynep",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 17:59:01 GMT"}],"updateDate":"2024-08-27","timestamp":1724695141000,"abstract":"  Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}