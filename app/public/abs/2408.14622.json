{"id":"2408.14622","title":"What Makes a Good Story and How Can We Measure It? A Comprehensive\n  Survey of Story Evaluation","authors":"Dingyi Yang, Qin Jin","authorsParsed":[["Yang","Dingyi",""],["Jin","Qin",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 20:35:42 GMT"}],"updateDate":"2024-08-28","timestamp":1724704542000,"abstract":"  With the development of artificial intelligence, particularly the success of\nLarge Language Models (LLMs), the quantity and quality of automatically\ngenerated stories have significantly increased. This has led to the need for\nautomatic story evaluation to assess the generative capabilities of computing\nsystems and analyze the quality of both automatic-generated and human-written\nstories. Evaluating a story can be more challenging than other generation\nevaluation tasks. While tasks like machine translation primarily focus on\nassessing the aspects of fluency and accuracy, story evaluation demands complex\nadditional measures such as overall coherence, character development,\ninterestingness, etc. This requires a thorough review of relevant research. In\nthis survey, we first summarize existing storytelling tasks, including\ntext-to-text, visual-to-text, and text-to-visual. We highlight their evaluation\nchallenges, identify various human criteria to measure stories, and present\nexisting benchmark datasets. Then, we propose a taxonomy to organize evaluation\nmetrics that have been developed or can be adopted for story evaluation. We\nalso provide descriptions of these metrics, along with the discussion of their\nmerits and limitations. Later, we discuss the human-AI collaboration for story\nevaluation and generation. Finally, we suggest potential future research\ndirections, extending from story evaluation to general evaluations.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}