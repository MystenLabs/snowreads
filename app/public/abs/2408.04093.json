{"id":"2408.04093","title":"Tree Attention: Topology-aware Decoding for Long-Context Attention on\n  GPU clusters","authors":"Vasudev Shyam, Jonathan Pilault, Emily Shepperd, Quentin Anthony,\n  Beren Millidge","authorsParsed":[["Shyam","Vasudev",""],["Pilault","Jonathan",""],["Shepperd","Emily",""],["Anthony","Quentin",""],["Millidge","Beren",""]],"versions":[{"version":"v1","created":"Wed, 7 Aug 2024 21:16:55 GMT"},{"version":"v2","created":"Fri, 9 Aug 2024 13:07:21 GMT"},{"version":"v3","created":"Wed, 14 Aug 2024 12:47:31 GMT"}],"updateDate":"2024-08-15","timestamp":1723065415000,"abstract":"  Self-attention is the core mathematical operation of modern transformer\narchitectures and is also a significant computational bottleneck due to its\nquadratic complexity in the sequence length. In this work, we derive the scalar\nenergy function whose gradient computes the self-attention block, thus\nelucidating the theoretical underpinnings of self-attention, providing a\nBayesian interpretation of the operation and linking it closely with\nenergy-based models such as Hopfield Networks. Our formulation reveals that the\nreduction across the sequence axis can be efficiently computed in parallel\nthrough a tree reduction. Our algorithm, for parallelizing attention\ncomputation across multiple GPUs enables cross-device decoding to be performed\nasymptotically faster (up to 8x faster in our experiments) than alternative\napproaches such as Ring Attention, while also requiring significantly less\ncommunication volume and incurring 2x less peak memory. Our code is publicly\navailable here: \\url{https://github.com/Zyphra/tree_attention}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}