{"id":"2407.14507","title":"Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey","authors":"Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai\n  Li, Rong-Hua Li, Yi Wang, Zhonghao Wang, Feiyu Xiong, Zhiyu Li","authorsParsed":[["Liang","Xun",""],["Song","Shichao",""],["Zheng","Zifan",""],["Wang","Hanyu",""],["Yu","Qingchen",""],["Li","Xunkai",""],["Li","Rong-Hua",""],["Wang","Yi",""],["Wang","Zhonghao",""],["Xiong","Feiyu",""],["Li","Zhiyu",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 17:59:03 GMT"},{"version":"v2","created":"Thu, 29 Aug 2024 08:24:42 GMT"},{"version":"v3","created":"Wed, 18 Sep 2024 09:25:20 GMT"}],"updateDate":"2024-09-19","timestamp":1721411943000,"abstract":"  Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we use a unified perspective of internal consistency, offering\nexplanations for reasoning deficiencies and hallucinations. Internal\nconsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce an effective theoretical framework capable of mining internal\nconsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures internal consistency\nsignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}