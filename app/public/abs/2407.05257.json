{"id":"2407.05257","title":"OvSW: Overcoming Silent Weights for Accurate Binary Neural Networks","authors":"Jingyang Xiang, Zuohui Chen, Siqi Li, Qing Wu, Yong Liu","authorsParsed":[["Xiang","Jingyang",""],["Chen","Zuohui",""],["Li","Siqi",""],["Wu","Qing",""],["Liu","Yong",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 05:01:20 GMT"}],"updateDate":"2024-07-09","timestamp":1720328480000,"abstract":"  Binary Neural Networks~(BNNs) have been proven to be highly effective for\ndeploying deep neural networks on mobile and embedded platforms. Most existing\nworks focus on minimizing quantization errors, improving representation\nability, or designing gradient approximations to alleviate gradient mismatch in\nBNNs, while leaving the weight sign flipping, a critical factor for achieving\npowerful BNNs, untouched. In this paper, we investigate the efficiency of\nweight sign updates in BNNs. We observe that, for vanilla BNNs, over 50\\% of\nthe weights remain their signs unchanged during training, and these weights are\nnot only distributed at the tails of the weight distribution but also\nuniversally present in the vicinity of zero. We refer to these weights as\n``silent weights'', which slow down convergence and lead to a significant\naccuracy degradation. Theoretically, we reveal this is due to the independence\nof the BNNs gradient from the latent weight distribution. To address the issue,\nwe propose Overcome Silent Weights~(OvSW). OvSW first employs Adaptive Gradient\nScaling~(AGS) to establish a relationship between the gradient and the latent\nweight distribution, thereby improving the overall efficiency of weight sign\nupdates. Additionally, we design Silence Awareness Decaying~(SAD) to\nautomatically identify ``silent weights'' by tracking weight flipping state,\nand apply an additional penalty to ``silent weights'' to facilitate their\nflipping. By efficiently updating weight signs, our method achieves faster\nconvergence and state-of-the-art performance on CIFAR10 and ImageNet1K dataset\nwith various architectures. For example, OvSW obtains 61.6\\% and 65.5\\% top-1\naccuracy on the ImageNet1K using binarized ResNet18 and ResNet34 architecture\nrespectively. Codes are available at\n\\url{https://github.com/JingyangXiang/OvSW}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}