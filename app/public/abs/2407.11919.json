{"id":"2407.11919","title":"What's Wrong? Refining Meeting Summaries with LLM Feedback","authors":"Frederic Kirstein and Terry Ruas and Bela Gipp","authorsParsed":[["Kirstein","Frederic",""],["Ruas","Terry",""],["Gipp","Bela",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:10:16 GMT"}],"updateDate":"2024-07-17","timestamp":1721149816000,"abstract":"  Meeting summarization has become a critical task since digital encounters\nhave become a common practice. Large language models (LLMs) show great\npotential in summarization, offering enhanced coherence and context\nunderstanding compared to traditional methods. However, they still struggle to\nmaintain relevance and avoid hallucination. We introduce a multi-LLM correction\napproach for meeting summarization using a two-phase process that mimics the\nhuman review process: mistake identification and summary refinement. We release\nQMSum Mistake, a dataset of 200 automatically generated meeting summaries\nannotated by humans on nine error types, including structural, omission, and\nirrelevance errors. Our experiments show that these errors can be identified\nwith high accuracy by an LLM. We transform identified mistakes into actionable\nfeedback to improve the quality of a given summary measured by relevance,\ninformativeness, conciseness, and coherence. This post-hoc refinement\neffectively improves summary quality by leveraging multiple LLMs to validate\noutput quality. Our multi-LLM approach for meeting summarization shows\npotential for similar complex text generation tasks requiring robustness,\naction planning, and discussion towards a goal.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/"}