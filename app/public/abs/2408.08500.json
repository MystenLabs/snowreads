{"id":"2408.08500","title":"CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving","authors":"Shihan Peng, Hanyu Zhou, Hao Dong, Zhiwei Shi, Haoyue Liu, Yuxing\n  Duan, Yi Chang, Luxin Yan","authorsParsed":[["Peng","Shihan",""],["Zhou","Hanyu",""],["Dong","Hao",""],["Shi","Zhiwei",""],["Liu","Haoyue",""],["Duan","Yuxing",""],["Chang","Yi",""],["Yan","Luxin",""]],"versions":[{"version":"v1","created":"Fri, 16 Aug 2024 02:55:10 GMT"}],"updateDate":"2024-08-19","timestamp":1723776910000,"abstract":"  Conventional frame camera is the mainstream sensor of the autonomous driving\nscene perception, while it is limited in adverse conditions, such as low light.\nEvent camera with high dynamic range has been applied in assisting frame camera\nfor the multimodal fusion, which relies heavily on the pixel-level spatial\nalignment between various modalities. Typically, existing multimodal datasets\nmainly place event and frame cameras in parallel and directly align them\nspatially via warping operation. However, this parallel strategy is less\neffective for multimodal fusion, since the large disparity exacerbates spatial\nmisalignment due to the large event-frame baseline. We argue that baseline\nminimization can reduce alignment error between event and frame cameras. In\nthis work, we introduce hybrid coaxial event-frame devices to build the\nmultimodal system, and propose a coaxial stereo event camera (CoSEC) dataset\nfor autonomous driving. As for the multimodal system, we first utilize the\nmicrocontroller to achieve time synchronization, and then spatially calibrate\ndifferent sensors, where we perform intra- and inter-calibration of stereo\ncoaxial devices. As for the multimodal dataset, we filter LiDAR point clouds to\ngenerate depth and optical flow labels using reference depth, which is further\nimproved by fusing aligned event and frame data in nighttime conditions. With\nthe help of the coaxial device, the proposed dataset can promote the all-day\npixel-level multimodal fusion. Moreover, we also conduct experiments to\ndemonstrate that the proposed dataset can improve the performance and\ngeneralization of the multimodal fusion.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}