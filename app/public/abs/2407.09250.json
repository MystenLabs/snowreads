{"id":"2407.09250","title":"FedsLLM: Federated Split Learning for Large Language Models over\n  Communication Networks","authors":"Kai Zhao,Zhaohui Yang,Chongwen Huang,Xiaoming Chen,Zhaoyang Zhang","authorsParsed":[["Zhao","Kai",""],["Yang","Zhaohui",""],["Huang","Chongwen",""],["Chen","Xiaoming",""],["Zhang","Zhaoyang",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 13:23:54 GMT"}],"updateDate":"2024-07-15","timestamp":1720790634000,"abstract":"  Addressing the challenges of deploying large language models in wireless\ncommunication networks, this paper combines low-rank adaptation technology\n(LoRA) with the splitfed learning framework to propose the federated split\nlearning for large language models (FedsLLM) framework. The method introduced\nin this paper utilizes LoRA technology to reduce processing loads by dividing\nthe network into client subnetworks and server subnetworks. It leverages a\nfederated server to aggregate and update client models. As the training data\nare transmitted through a wireless network between clients and both main and\nfederated servers, the training delay is determined by the learning accuracy\nand the allocation of communication bandwidth. This paper models the\nminimization of the training delay by integrating computation and communication\noptimization, simplifying the optimization problem into a convex problem to\nfind the optimal solution. Additionally, it presents a lemma that describes the\nprecise solutions to this problem. Simulation results demonstrate that the\nproposed optimization algorithm reduces delays by an average of 47.63% compared\nto unoptimized scenarios.\n","subjects":["Computing Research Repository/Networking and Internet Architecture","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}