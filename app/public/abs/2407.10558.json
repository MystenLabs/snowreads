{"id":"2407.10558","title":"ConTEXTure: Consistent Multiview Images to Texture","authors":"Jaehoon Ahn, Sumin Cho, Harim Jung, Kibeom Hong, Seonghoon Ban,\n  Moon-Ryul Jung","authorsParsed":[["Ahn","Jaehoon",""],["Cho","Sumin",""],["Jung","Harim",""],["Hong","Kibeom",""],["Ban","Seonghoon",""],["Jung","Moon-Ryul",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 09:15:55 GMT"}],"updateDate":"2024-07-16","timestamp":1721034955000,"abstract":"  We introduce ConTEXTure, a generative network designed to create a texture\nmap/atlas for a given 3D mesh using images from multiple viewpoints. The\nprocess begins with generating a front-view image from a text prompt, such as\n'Napoleon, front view', describing the 3D mesh. Additional images from\ndifferent viewpoints are derived from this front-view image and camera poses\nrelative to it. ConTEXTure builds upon the TEXTure network, which uses text\nprompts for six viewpoints (e.g., 'Napoleon, front view', 'Napoleon, left\nview', etc.). However, TEXTure often generates images for non-front viewpoints\nthat do not accurately represent those viewpoints.To address this issue, we\nemploy Zero123++, which generates multiple view-consistent images for the six\nspecified viewpoints simultaneously, conditioned on the initial front-view\nimage and the depth maps of the mesh for the six viewpoints. By utilizing these\nview-consistent images, ConTEXTure learns the texture atlas from all viewpoint\nimages concurrently, unlike previous methods that do so sequentially. This\napproach ensures that the rendered images from various viewpoints, including\nback, side, bottom, and top, are free from viewpoint irregularities.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}