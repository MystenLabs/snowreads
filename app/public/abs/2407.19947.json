{"id":"2407.19947","title":"Inference acceleration for large language models using \"stairs\" assisted\n  greedy generation","authors":"Domas Grigali\\=unas and Mantas Luko\\v{s}evi\\v{c}ius","authorsParsed":[["Grigaliūnas","Domas",""],["Lukoševičius","Mantas",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 12:29:29 GMT"}],"updateDate":"2024-07-30","timestamp":1722256169000,"abstract":"  Large Language Models (LLMs) with billions of parameters are known for their\nimpressive predicting capabilities but require lots of resources to run. With\ntheir massive rise in popularity, even a small reduction in required resources\ncould have an impact on environment. On the other hand, smaller models require\nfewer resources but may sacrifice accuracy. In this work, we are proposing an\nimplementation of ``stairs'' assisted greedy generation. It is a modified\nassisted generation methodology that makes use of a smaller model's fast\ngeneration, large model's batch prediction, and \"stairs\" validation in order to\nachieve a speed up in prediction generation. Results show between 9.58 and\n17.24 percent inference time reduction compared to a stand-alone large LLM\nprediction in a text generation task without a loss in accuracy.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NQOOCXU7eX--pko3y09W612hXQmUsiDlL789SN8OXVY","pdfSize":"785323","objectId":"0xb9c018d3f2d394da94c942e380bfc119111fdce7eba6639464d3cd276cd6da3f","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
