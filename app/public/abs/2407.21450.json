{"id":"2407.21450","title":"Forecasting Future Videos from Novel Views via Disentangled 3D Scene\n  Representation","authors":"Sudhir Yarram and Junsong Yuan","authorsParsed":[["Yarram","Sudhir",""],["Yuan","Junsong",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 08:54:50 GMT"},{"version":"v2","created":"Fri, 2 Aug 2024 05:01:38 GMT"}],"updateDate":"2024-08-05","timestamp":1722416090000,"abstract":"  Video extrapolation in space and time (VEST) enables viewers to forecast a 3D\nscene into the future and view it from novel viewpoints. Recent methods propose\nto learn an entangled representation, aiming to model layered scene geometry,\nmotion forecasting and novel view synthesis together, while assuming simplified\naffine motion and homography-based warping at each scene layer, leading to\ninaccurate video extrapolation. Instead of entangled scene representation and\nrendering, our approach chooses to disentangle scene geometry from scene\nmotion, via lifting the 2D scene to 3D point clouds, which enables high quality\nrendering of future videos from novel views. To model future 3D scene motion,\nwe propose a disentangled two-stage approach that initially forecasts\nego-motion and subsequently the residual motion of dynamic objects (e.g., cars,\npeople). This approach ensures more precise motion predictions by reducing\ninaccuracies from entanglement of ego-motion with dynamic object motion, where\nbetter ego-motion forecasting could significantly enhance the visual outcomes.\nExtensive experimental analysis on two urban scene datasets demonstrate\nsuperior performance of our proposed method in comparison to strong baselines.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/"}