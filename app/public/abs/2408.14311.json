{"id":"2408.14311","title":"Prediction rigidities for data-driven chemistry","authors":"Sanggyu Chong, Filippo Bigi, Federico Grasselli, Philip Loche,\n  Matthias Kellner, Michele Ceriotti","authorsParsed":[["Chong","Sanggyu",""],["Bigi","Filippo",""],["Grasselli","Federico",""],["Loche","Philip",""],["Kellner","Matthias",""],["Ceriotti","Michele",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 14:42:04 GMT"}],"updateDate":"2024-08-27","timestamp":1724683324000,"abstract":"  The widespread application of machine learning (ML) to the chemical sciences\nis making it very important to understand how the ML models learn to correlate\nchemical structures with their properties, and what can be done to improve the\ntraining efficiency whilst guaranteeing interpretability and transferability.\nIn this work, we demonstrate the wide utility of prediction rigidities, a\nfamily of metrics derived from the loss function, in understanding the\nrobustness of ML model predictions. We show that the prediction rigidities\nallow the assessment of the model not only at the global level, but also on the\nlocal or the component-wise level at which the intermediate (e.g. atomic,\nbody-ordered, or range-separated) predictions are made. We leverage these\nmetrics to understand the learning behavior of different ML models, and to\nguide efficient dataset construction for model training. We finally implement\nthe formalism for a ML model targeting a coarse-grained system to demonstrate\nthe applicability of the prediction rigidities to an even broader class of\natomistic modeling problems.\n","subjects":["Physics/Chemical Physics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}