{"id":"2407.02138","title":"Efficient Nearest Neighbor based Uncertainty Estimation for Natural\n  Language Processing Tasks","authors":"Wataru Hashimoto, Hidetaka Kamigaito, Taro Watanabe","authorsParsed":[["Hashimoto","Wataru",""],["Kamigaito","Hidetaka",""],["Watanabe","Taro",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 10:33:31 GMT"}],"updateDate":"2024-07-03","timestamp":1719916411000,"abstract":"  Trustworthy prediction in Deep Neural Networks (DNNs), including Pre-trained\nLanguage Models (PLMs) is important for safety-critical applications in the\nreal world. However, DNNs often suffer from uncertainty estimation, such as\nmiscalibration. In particular, approaches that require multiple stochastic\ninference can mitigate this problem, but the expensive cost of inference makes\nthem impractical. In this study, we propose $k$-Nearest Neighbor Uncertainty\nEstimation ($k$NN-UE), which is an uncertainty estimation method that uses the\ndistances from the neighbors and label-existence ratio of neighbors.\nExperiments on sentiment analysis, natural language inference, and named entity\nrecognition show that our proposed method outperforms the baselines or recent\ndensity-based methods in confidence calibration, selective prediction, and\nout-of-distribution detection. Moreover, our analyses indicate that introducing\ndimension reduction or approximate nearest neighbor search inspired by recent\n$k$NN-LM studies reduces the inference overhead without significantly degrading\nestimation performance when combined them appropriately.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}