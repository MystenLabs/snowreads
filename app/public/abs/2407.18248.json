{"id":"2407.18248","title":"Self-Training with Direct Preference Optimization Improves\n  Chain-of-Thought Reasoning","authors":"Tianduo Wang, Shichen Li, Wei Lu","authorsParsed":[["Wang","Tianduo",""],["Li","Shichen",""],["Lu","Wei",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 17:59:16 GMT"}],"updateDate":"2024-07-26","timestamp":1721930356000,"abstract":"  Effective training of language models (LMs) for mathematical reasoning tasks\ndemands high-quality supervised fine-tuning data. Besides obtaining annotations\nfrom human experts, a common alternative is sampling from larger and more\npowerful LMs. However, this knowledge distillation approach can be costly and\nunstable, particularly when relying on closed-source, proprietary LMs like\nGPT-4, whose behaviors are often unpredictable. In this work, we demonstrate\nthat the reasoning abilities of small-scale LMs can be enhanced through\nself-training, a process where models learn from their own outputs. We also\nshow that the conventional self-training can be further augmented by a\npreference learning algorithm called Direct Preference Optimization (DPO). By\nintegrating DPO into self-training, we leverage preference data to guide LMs\ntowards more accurate and diverse chain-of-thought reasoning. We evaluate our\nmethod across various mathematical reasoning tasks using different base models.\nOur experiments show that this approach not only improves LMs' reasoning\nperformance but also offers a more cost-effective and scalable solution\ncompared to relying on large proprietary LMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KDwV7g_7FcXS1jGBrktVbU-BXy6bZEESQciRIaWpFWs","pdfSize":"642862"}
