{"id":"2408.09896","title":"Instruction-Based Molecular Graph Generation with Unified Text-Graph\n  Diffusion Model","authors":"Yuran Xiang, Haiteng Zhao, Chang Ma, Zhi-Hong Deng","authorsParsed":[["Xiang","Yuran",""],["Zhao","Haiteng",""],["Ma","Chang",""],["Deng","Zhi-Hong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 11:09:15 GMT"}],"updateDate":"2024-08-20","timestamp":1724065755000,"abstract":"  Recent advancements in computational chemistry have increasingly focused on\nsynthesizing molecules based on textual instructions. Integrating graph\ngeneration with these instructions is complex, leading most current methods to\nuse molecular sequences with pre-trained large language models. In response to\nthis challenge, we propose a novel framework, named $\\textbf{UTGDiff (Unified\nText-Graph Diffusion Model)}$, which utilizes language models for discrete\ngraph diffusion to generate molecular graphs from instructions. UTGDiff\nfeatures a unified text-graph transformer as the denoising network, derived\nfrom pre-trained language models and minimally modified to process graph data\nthrough attention bias. Our experimental results demonstrate that UTGDiff\nconsistently outperforms sequence-based baselines in tasks involving\ninstruction-based molecule generation and editing, achieving superior\nperformance with fewer parameters given an equivalent level of pretraining\ncorpus. Our code is availble at https://github.com/ran1812/UTGDiff.\n","subjects":["Computing Research Repository/Machine Learning","Physics/Chemical Physics","Quantitative Biology/Biomolecules"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1OwyMLrhkwp3KAz_E6O6_j4m2jct40saKlA3ULsQuao","pdfSize":"1382566"}
