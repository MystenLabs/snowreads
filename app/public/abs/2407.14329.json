{"id":"2407.14329","title":"Efficient Audio Captioning with Encoder-Level Knowledge Distillation","authors":"Xuenan Xu, Haohe Liu, Mengyue Wu, Wenwu Wang, Mark D. Plumbley","authorsParsed":[["Xu","Xuenan",""],["Liu","Haohe",""],["Wu","Mengyue",""],["Wang","Wenwu",""],["Plumbley","Mark D.",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 14:09:50 GMT"}],"updateDate":"2024-07-22","timestamp":1721398190000,"abstract":"  Significant improvement has been achieved in automated audio captioning (AAC)\nwith recent models. However, these models have become increasingly large as\ntheir performance is enhanced. In this work, we propose a knowledge\ndistillation (KD) framework for AAC. Our analysis shows that in the\nencoder-decoder based AAC models, it is more effective to distill knowledge\ninto the encoder as compared with the decoder. To this end, we incorporate\nencoder-level KD loss into training, in addition to the standard supervised\nloss and sequence-level KD loss. We investigate two encoder-level KD methods,\nbased on mean squared error (MSE) loss and contrastive loss, respectively.\nExperimental results demonstrate that contrastive KD is more robust than MSE\nKD, exhibiting superior performance in data-scarce situations. By leveraging\naudio-only data into training in the KD framework, our student model achieves\ncompetitive performance, with an inference speed that is 19 times\nfaster\\footnote{An online demo is available at\n\\url{https://huggingface.co/spaces/wsntxxn/efficient_audio_captioning}}.\n","subjects":["Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}