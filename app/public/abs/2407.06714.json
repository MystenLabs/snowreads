{"id":"2407.06714","title":"Improving the Transferability of Adversarial Examples by Feature\n  Augmentation","authors":"Donghua Wang, Wen Yao, Tingsong Jiang, Xiaohu Zheng, Junqi Wu,\n  Xiaoqian Chen","authorsParsed":[["Wang","Donghua",""],["Yao","Wen",""],["Jiang","Tingsong",""],["Zheng","Xiaohu",""],["Wu","Junqi",""],["Chen","Xiaoqian",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 09:41:40 GMT"}],"updateDate":"2024-07-10","timestamp":1720518100000,"abstract":"  Despite the success of input transformation-based attacks on boosting\nadversarial transferability, the performance is unsatisfying due to the\nignorance of the discrepancy across models. In this paper, we propose a simple\nbut effective feature augmentation attack (FAUG) method, which improves\nadversarial transferability without introducing extra computation costs.\nSpecifically, we inject the random noise into the intermediate features of the\nmodel to enlarge the diversity of the attack gradient, thereby mitigating the\nrisk of overfitting to the specific model and notably amplifying adversarial\ntransferability. Moreover, our method can be combined with existing gradient\nattacks to augment their performance further. Extensive experiments conducted\non the ImageNet dataset across CNN and transformer models corroborate the\nefficacy of our method, e.g., we achieve improvement of +26.22% and +5.57% on\ninput transformation-based attacks and combination methods, respectively.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}