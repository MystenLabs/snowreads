{"id":"2408.05120","title":"Cautious Calibration in Binary Classification","authors":"Mari-Liis Allikivi, Joonas J\\\"arve, Meelis Kull","authorsParsed":[["Allikivi","Mari-Liis",""],["JÃ¤rve","Joonas",""],["Kull","Meelis",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 15:19:40 GMT"}],"updateDate":"2024-08-12","timestamp":1723216780000,"abstract":"  Being cautious is crucial for enhancing the trustworthiness of machine\nlearning systems integrated into decision-making pipelines. Although calibrated\nprobabilities help in optimal decision-making, perfect calibration remains\nunattainable, leading to estimates that fluctuate between under- and\noverconfidence. This becomes a critical issue in high-risk scenarios, where\neven occasional overestimation can lead to extreme expected costs. In these\nscenarios, it is important for each predicted probability to lean towards\nunderconfidence, rather than just achieving an average balance. In this study,\nwe introduce the novel concept of cautious calibration in binary\nclassification. This approach aims to produce probability estimates that are\nintentionally underconfident for each predicted probability. We highlight the\nimportance of this approach in a high-risk scenario and propose a theoretically\ngrounded method for learning cautious calibration maps. Through experiments, we\nexplore and compare our method to various approaches, including methods\noriginally not devised for cautious calibration but applicable in this context.\nWe show that our approach is the most consistent in providing cautious\nestimates. Our work establishes a strong baseline for further developments in\nthis novel framework.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}