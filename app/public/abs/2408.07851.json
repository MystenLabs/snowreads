{"id":"2408.07851","title":"SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion\n  Recognition","authors":"Mohamed Osman, Daniel Z. Kaplan, Tamer Nadeem","authorsParsed":[["Osman","Mohamed",""],["Kaplan","Daniel Z.",""],["Nadeem","Tamer",""]],"versions":[{"version":"v1","created":"Wed, 14 Aug 2024 23:33:10 GMT"}],"updateDate":"2024-08-16","timestamp":1723678390000,"abstract":"  Speech emotion recognition (SER) has made significant strides with the advent\nof powerful self-supervised learning (SSL) models. However, the generalization\nof these models to diverse languages and emotional expressions remains a\nchallenge. We propose a large-scale benchmark to evaluate the robustness and\nadaptability of state-of-the-art SER models in both in-domain and out-of-domain\nsettings. Our benchmark includes a diverse set of multilingual datasets,\nfocusing on less commonly used corpora to assess generalization to new data. We\nemploy logit adjustment to account for varying class distributions and\nestablish a single dataset cluster for systematic evaluation. Surprisingly, we\nfind that the Whisper model, primarily designed for automatic speech\nrecognition, outperforms dedicated SSL models in cross-lingual SER. Our results\nhighlight the need for more robust and generalizable SER models, and our\nbenchmark serves as a valuable resource to drive future research in this\ndirection.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"x621U5gSauzh-ERVpHp9ejpJMI4PmoiXN8X7sj4-59U","pdfSize":"248614"}
