{"id":"2407.04082","title":"DASS: Distilled Audio State Space Models Are Stronger and More\n  Duration-Scalable Learners","authors":"Saurabhchand Bhati, Yuan Gong, Leonid Karlinsky, Hilde Kuehne, Rogerio\n  Feris, James Glass","authorsParsed":[["Bhati","Saurabhchand",""],["Gong","Yuan",""],["Karlinsky","Leonid",""],["Kuehne","Hilde",""],["Feris","Rogerio",""],["Glass","James",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 17:46:19 GMT"}],"updateDate":"2024-07-08","timestamp":1720115179000,"abstract":"  State-space models (SSMs) have emerged as an alternative to Transformers for\naudio modeling due to their high computational efficiency with long inputs.\nWhile recent efforts on Audio SSMs have reported encouraging results, two main\nlimitations remain: First, in 10-second short audio tagging tasks, Audio SSMs\nstill underperform compared to Transformer-based models such as Audio\nSpectrogram Transformer (AST). Second, although Audio SSMs theoretically\nsupport long audio inputs, their actual performance with long audio has not\nbeen thoroughly evaluated. To address these limitations, in this paper, 1) We\napplied knowledge distillation in audio space model training, resulting in a\nmodel called Knowledge Distilled Audio SSM (DASS). To the best of our\nknowledge, it is the first SSM that outperforms the Transformers on AudioSet\nand achieves an mAP of 47.6; and 2) We designed a new test called Audio Needle\nIn A Haystack (Audio NIAH). We find that DASS, trained with only 10-second\naudio clips, can retrieve sound events in audio recordings up to 2.5 hours\nlong, while the AST model fails when the input is just 50 seconds,\ndemonstrating SSMs are indeed more duration scalable.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}