{"id":"2408.11811","title":"EmbodiedSAM: Online Segment Any 3D Thing in Real Time","authors":"Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, Jiwen\n  Lu","authorsParsed":[["Xu","Xiuwei",""],["Chen","Huangxing",""],["Zhao","Linqing",""],["Wang","Ziwei",""],["Zhou","Jie",""],["Lu","Jiwen",""]],"versions":[{"version":"v1","created":"Wed, 21 Aug 2024 17:57:06 GMT"}],"updateDate":"2024-08-22","timestamp":1724263026000,"abstract":"  Embodied tasks require the agent to fully understand 3D scenes simultaneously\nwith its exploration, so an online, real-time, fine-grained and\nhighly-generalized 3D perception model is desperately needed. Since\nhigh-quality 3D data is limited, directly training such a model in 3D is almost\ninfeasible. Meanwhile, vision foundation models (VFM) has revolutionized the\nfield of 2D computer vision with superior performance, which makes the use of\nVFM to assist embodied 3D perception a promising direction. However, most\nexisting VFM-assisted 3D perception methods are either offline or too slow that\ncannot be applied in practical embodied tasks. In this paper, we aim to\nleverage Segment Anything Model (SAM) for real-time 3D instance segmentation in\nan online setting. This is a challenging problem since future frames are not\navailable in the input streaming RGB-D video, and an instance may be observed\nin several frames so object matching between frames is required. To address\nthese challenges, we first propose a geometric-aware query lifting module to\nrepresent the 2D masks generated by SAM by 3D-aware queries, which is then\niteratively refined by a dual-level query decoder. In this way, the 2D masks\nare transferred to fine-grained shapes on 3D point clouds. Benefit from the\nquery representation for 3D masks, we can compute the similarity matrix between\nthe 3D masks from different views by efficient matrix operation, which enables\nreal-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan\nshow our method achieves leading performance even compared with offline\nmethods. Our method also demonstrates great generalization ability in several\nzero-shot dataset transferring experiments and show great potential in\nopen-vocabulary and data-efficient setting. Code and demo are available at\nhttps://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for\ntraining and evaluation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}