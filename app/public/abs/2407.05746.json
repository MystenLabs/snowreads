{"id":"2407.05746","title":"MSP-Podcast SER Challenge 2024: L'antenne du Ventoux Multimodal\n  Self-Supervised Learning for Speech Emotion Recognition","authors":"Jarod Duret (LIA), Mickael Rouvier (LIA), Yannick Est\\`eve (LIA)","authorsParsed":[["Duret","Jarod","","LIA"],["Rouvier","Mickael","","LIA"],["Est√®ve","Yannick","","LIA"]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 08:52:06 GMT"}],"updateDate":"2024-07-09","timestamp":1720428726000,"abstract":"  In this work, we detail our submission to the 2024 edition of the MSP-Podcast\nSpeech Emotion Recognition (SER) Challenge. This challenge is divided into two\ndistinct tasks: Categorical Emotion Recognition and Emotional Attribute\nPrediction. We concentrated our efforts on Task 1, which involves the\ncategorical classification of eight emotional states using data from the\nMSP-Podcast dataset. Our approach employs an ensemble of models, each trained\nindependently and then fused at the score level using a Support Vector Machine\n(SVM) classifier. The models were trained using various strategies, including\nSelf-Supervised Learning (SSL) fine-tuning across different modalities: speech\nalone, text alone, and a combined speech and text approach. This joint training\nmethodology aims to enhance the system's ability to accurately classify\nemotional states. This joint training methodology aims to enhance the system's\nability to accurately classify emotional states. Thus, the system obtained\nF1-macro of 0.35\\% on development set.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}