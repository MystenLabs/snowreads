{"id":"2408.07869","title":"A Systematic Evaluation of Generated Time Series and Their Effects in\n  Self-Supervised Pretraining","authors":"Audrey Der, Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng,\n  Yujie Fan, Zhongfang Zhuang, Vivian Lai, Junpeng Wang, Liang Wang, Wei Zhang,\n  Eamonn Keogh","authorsParsed":[["Der","Audrey",""],["Yeh","Chin-Chia Michael",""],["Dai","Xin",""],["Chen","Huiyuan",""],["Zheng","Yan",""],["Fan","Yujie",""],["Zhuang","Zhongfang",""],["Lai","Vivian",""],["Wang","Junpeng",""],["Wang","Liang",""],["Zhang","Wei",""],["Keogh","Eamonn",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 00:53:09 GMT"}],"updateDate":"2024-08-16","timestamp":1723683189000,"abstract":"  Self-supervised Pretrained Models (PTMs) have demonstrated remarkable\nperformance in computer vision and natural language processing tasks. These\nsuccesses have prompted researchers to design PTMs for time series data. In our\nexperiments, most self-supervised time series PTMs were surpassed by simple\nsupervised models. We hypothesize this undesired phenomenon may be caused by\ndata scarcity. In response, we test six time series generation methods, use the\ngenerated data in pretraining in lieu of the real data, and examine the effects\non classification performance. Our results indicate that replacing a real-data\npretraining set with a greater volume of only generated samples produces\nnoticeable improvement.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kjRWDUfct1iyPN2_0gb8VIvGczli_oXShJ8aGhk5_So","pdfSize":"5990463"}
