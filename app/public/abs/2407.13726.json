{"id":"2407.13726","title":"Compressing Structured Tensor Algebra","authors":"Mahdi Ghorbani, Emilien Bauer, Tobias Grosser, Amir Shaikhha","authorsParsed":[["Ghorbani","Mahdi",""],["Bauer","Emilien",""],["Grosser","Tobias",""],["Shaikhha","Amir",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:25:17 GMT"}],"updateDate":"2024-07-19","timestamp":1721323517000,"abstract":"  Tensor algebra is a crucial component for data-intensive workloads such as\nmachine learning and scientific computing. As the complexity of data grows,\nscientists often encounter a dilemma between the highly specialized dense\ntensor algebra and efficient structure-aware algorithms provided by sparse\ntensor algebra. In this paper, we introduce DASTAC, a framework to propagate\nthe tensors's captured high-level structure down to low-level code generation\nby incorporating techniques such as automatic data layout compression,\npolyhedral analysis, and affine code generation. Our methodology reduces memory\nfootprint by automatically detecting the best data layout, heavily benefits\nfrom polyhedral optimizations, leverages further optimizations, and enables\nparallelization through MLIR. Through extensive experimentation, we show that\nDASTAC achieves 1 to 2 orders of magnitude speedup over TACO, a\nstate-of-the-art sparse tensor compiler, and StructTensor, a state-of-the-art\nstructured tensor algebra compiler, with a significantly lower memory\nfootprint.\n","subjects":["Computing Research Repository/Programming Languages","Computing Research Repository/Machine Learning","Computing Research Repository/Mathematical Software"],"license":"http://creativecommons.org/licenses/by/4.0/"}