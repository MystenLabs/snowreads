{"id":"2407.00928","title":"FoldGPT: Simple and Effective Large Language Model Compression Scheme","authors":"Songwei Liu, Chao Zeng, Lianqiang Li, Chenqian Yan, Lean Fu, Xing Mei,\n  Fangmin Chen","authorsParsed":[["Liu","Songwei",""],["Zeng","Chao",""],["Li","Lianqiang",""],["Yan","Chenqian",""],["Fu","Lean",""],["Mei","Xing",""],["Chen","Fangmin",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 03:17:53 GMT"}],"updateDate":"2024-07-02","timestamp":1719803873000,"abstract":"  The demand for deploying large language models(LLMs) on mobile devices\ncontinues to increase, driven by escalating data security concerns and cloud\ncosts. However, network bandwidth and memory limitations pose challenges for\ndeploying billion-level models on mobile devices. In this study, we investigate\nthe outputs of different layers across various scales of LLMs and found that\nthe outputs of most layers exhibit significant similarity. Moreover, this\nsimilarity becomes more pronounced as the model size increases, indicating\nsubstantial redundancy in the depth direction of the LLMs. Based on this\nobservation, we propose an efficient model volume compression strategy, termed\nFoldGPT, which combines block removal and block parameter sharing.This strategy\nconsists of three parts: (1) Based on the learnable gating parameters, we\ndetermine the block importance ranking while modeling the coupling effect\nbetween blocks. Then we delete some redundant layers based on the given removal\nrate. (2) For the retained blocks, we apply a specially designed group\nparameter sharing strategy, where blocks within the same group share identical\nweights, significantly compressing the number of parameters and slightly\nreducing latency overhead. (3) After sharing these Blocks, we \"cure\" the\nmismatch caused by sparsity with a minor amount of fine-tuning and introduce a\ntail-layer distillation strategy to improve the performance. Experiments\ndemonstrate that FoldGPT outperforms previous state-of-the-art(SOTA) methods in\nefficient model compression, demonstrating the feasibility of achieving model\nlightweighting through straightforward block removal and parameter sharing.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}