{"id":"2408.14594","title":"MMR: Evaluating Reading Ability of Large Multimodal Models","authors":"Jian Chen, Ruiyi Zhang, Yufan Zhou, Ryan Rossi, Jiuxiang Gu, Changyou\n  Chen","authorsParsed":[["Chen","Jian",""],["Zhang","Ruiyi",""],["Zhou","Yufan",""],["Rossi","Ryan",""],["Gu","Jiuxiang",""],["Chen","Changyou",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 19:26:50 GMT"}],"updateDate":"2024-08-28","timestamp":1724700410000,"abstract":"  Large multimodal models (LMMs) have demonstrated impressive capabilities in\nunderstanding various types of image, including text-rich images. Most existing\ntext-rich image benchmarks are simple extraction-based question answering, and\nmany LMMs now easily achieve high scores. This means that current benchmarks\nfail to accurately reflect performance of different models, and a natural idea\nis to build a new benchmark to evaluate their complex reasoning and spatial\nunderstanding abilities. In this work, we propose the Multi-Modal Reading (MMR)\nbenchmark in 11 diverse tasks to evaluate LMMs for text-rich image\nunderstanding. MMR is the first text-rich image benchmark built on human\nannotations with the help of language models. By evaluating several\nstate-of-the-art LMMs, including GPT-4o, it reveals the limited capabilities of\nexisting LMMs underscoring the value of our benchmark.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"kyw6mhDZaXKplFjtCAuw-b10duPdgT55DyhLjR0TSkI","pdfSize":"17900933"}
