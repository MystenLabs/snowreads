{"id":"2407.10877","title":"Optimal Neural Summarisation for Full-Field Weak Lensing Cosmological\n  Implicit Inference","authors":"Denise Lanzieri, Justine Zeghal, T. Lucas Makinen, Alexandre Boucaud,\n  Jean-Luc Starck, Fran\\c{c}ois Lanusse","authorsParsed":[["Lanzieri","Denise",""],["Zeghal","Justine",""],["Makinen","T. Lucas",""],["Boucaud","Alexandre",""],["Starck","Jean-Luc",""],["Lanusse","Fran√ßois",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 16:25:29 GMT"}],"updateDate":"2024-07-16","timestamp":1721060729000,"abstract":"  Traditionally, weak lensing cosmological surveys have been analyzed using\nsummary statistics motivated by their analytically tractable likelihoods, or by\ntheir ability to access higher-order information, at the cost of requiring\nSimulation-Based Inference (SBI) approaches. While informative, these\nstatistics are neither designed nor guaranteed to be statistically sufficient.\nWith the rise of deep learning, it becomes possible to create summary\nstatistics optimized to extract the full data information. We compare different\nneural summarization strategies proposed in the weak lensing literature, to\nassess which loss functions lead to theoretically optimal summary statistics to\nperform full-field inference. In doing so, we aim to provide guidelines and\ninsights to the community to help guide future neural-based inference analyses.\nWe design an experimental setup to isolate the impact of the loss function used\nto train neural networks. We have developed the sbi_lens JAX package, which\nimplements an automatically differentiable lognormal wCDM LSST-Y10 weak lensing\nsimulator. The explicit full-field posterior obtained using the\nHamilotnian-Monte-Carlo sampler gives us a ground truth to which to compare\ndifferent compression strategies. We provide theoretical insight into the loss\nfunctions used in the literature and show that some do not necessarily lead to\nsufficient statistics (e.g. Mean Square Error (MSE)), while those motivated by\ninformation theory (e.g. Variational Mutual Information Maximization (VMIM))\ncan. Our numerical experiments confirm these insights and show, in our\nsimulated wCDM scenario, that the Figure of Merit (FoM) of an analysis using\nneural summaries optimized under VMIM achieves 100% of the reference Omega_c -\nsigma_8 full-field FoM, while an analysis using neural summaries trained under\nMSE achieves only 81% of the same reference FoM.\n","subjects":["Astrophysics/Cosmology and Nongalactic Astrophysics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3_cC_EiIs4fWEbKsJshv4w99hIq-ThT4ZeMgCMwrywU","pdfSize":"1792244"}
