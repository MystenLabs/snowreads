{"id":"2407.11294","title":"COHO: Context-Sensitive City-Scale Hierarchical Urban Layout Generation","authors":"Liu He, Daniel Aliaga","authorsParsed":[["He","Liu",""],["Aliaga","Daniel",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 00:49:53 GMT"}],"updateDate":"2024-07-17","timestamp":1721090993000,"abstract":"  The generation of large-scale urban layouts has garnered substantial interest\nacross various disciplines. Prior methods have utilized procedural generation\nrequiring manual rule coding or deep learning needing abundant data. However,\nprior approaches have not considered the context-sensitive nature of urban\nlayout generation. Our approach addresses this gap by leveraging a canonical\ngraph representation for the entire city, which facilitates scalability and\ncaptures the multi-layer semantics inherent in urban layouts. We introduce a\nnovel graph-based masked autoencoder (GMAE) for city-scale urban layout\ngeneration. The method encodes attributed buildings, city blocks, communities\nand cities into a unified graph structure, enabling self-supervised masked\ntraining for graph autoencoder. Additionally, we employ scheduled iterative\nsampling for 2.5D layout generation, prioritizing the generation of important\ncity blocks and buildings. Our approach achieves good realism, semantic\nconsistency, and correctness across the heterogeneous urban styles in 330 US\ncities. Codes and datasets are released at https://github.com/Arking1995/COHO.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}