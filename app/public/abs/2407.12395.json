{"id":"2407.12395","title":"Efficient Depth-Guided Urban View Synthesis","authors":"Sheng Miao, Jiaxin Huang, Dongfeng Bai, Weichao Qiu, Bingbing Liu,\n  Andreas Geiger, Yiyi Liao","authorsParsed":[["Miao","Sheng",""],["Huang","Jiaxin",""],["Bai","Dongfeng",""],["Qiu","Weichao",""],["Liu","Bingbing",""],["Geiger","Andreas",""],["Liao","Yiyi",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 08:16:25 GMT"}],"updateDate":"2024-07-18","timestamp":1721204185000,"abstract":"  Recent advances in implicit scene representation enable high-fidelity street\nview novel view synthesis. However, existing methods optimize a neural radiance\nfield for each scene, relying heavily on dense training images and extensive\ncomputation resources. To mitigate this shortcoming, we introduce a new method\ncalled Efficient Depth-Guided Urban View Synthesis (EDUS) for fast feed-forward\ninference and efficient per-scene fine-tuning. Different from prior\ngeneralizable methods that infer geometry based on feature matching, EDUS\nleverages noisy predicted geometric priors as guidance to enable generalizable\nurban view synthesis from sparse input images. The geometric priors allow us to\napply our generalizable model directly in the 3D space, gaining robustness\nacross various sparsity levels. Through comprehensive experiments on the\nKITTI-360 and Waymo datasets, we demonstrate promising generalization abilities\non novel street scenes. Moreover, our results indicate that EDUS achieves\nstate-of-the-art performance in sparse view settings when combined with fast\ntest-time optimization.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}