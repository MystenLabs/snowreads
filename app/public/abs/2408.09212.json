{"id":"2408.09212","title":"Scalable and Certifiable Graph Unlearning via Lazy Local Propagation","authors":"Lu Yi, Zhewei Wei","authorsParsed":[["Yi","Lu",""],["Wei","Zhewei",""]],"versions":[{"version":"v1","created":"Sat, 17 Aug 2024 14:41:02 GMT"}],"updateDate":"2024-08-20","timestamp":1723905662000,"abstract":"  With the recent adoption of laws supporting the ``right to be forgotten'' and\nthe widespread use of Graph Neural Networks for modeling graph-structured data,\ngraph unlearning has emerged as a crucial research area. Current studies focus\non the efficient update of model parameters. However, they often overlook the\ntime-consuming re-computation of graph propagation required for each removal,\nsignificantly limiting their scalability on large graphs.\n  In this paper, we present ScaleGUN, the first certifiable graph unlearning\nmechanism that scales to billion-edge graphs. ScaleGUN employs a lazy local\npropagation method to facilitate efficient updates of the embedding matrix\nduring data removal. Such lazy local propagation can be proven to ensure\ncertified unlearning under all three graph unlearning scenarios, including node\nfeature, edge, and node unlearning. Extensive experiments on real-world\ndatasets demonstrate the efficiency and efficacy of ScaleGUN. Remarkably,\nScaleGUN accomplishes $(\\epsilon,\\delta)=(1,10^{-4})$ certified unlearning on\nthe billion-edge graph ogbn-papers100M in 20 seconds for a $5K$-random-edge\nremoval request -- of which only 5 seconds are required for updating the\nembedding matrix -- compared to 1.91 hours for retraining and 1.89 hours for\nre-propagation. Our code is available online.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}