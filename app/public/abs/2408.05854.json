{"id":"2408.05854","title":"On the Robustness of Kernel Goodness-of-Fit Tests","authors":"Xing Liu, Fran\\c{c}ois-Xavier Briol","authorsParsed":[["Liu","Xing",""],["Briol","Fran√ßois-Xavier",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 19:54:50 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 08:32:33 GMT"}],"updateDate":"2024-08-26","timestamp":1723406090000,"abstract":"  Goodness-of-fit testing is often criticized for its lack of practical\nrelevance; since ``all models are wrong'', the null hypothesis that the data\nconform to our model is ultimately always rejected when the sample size is\nlarge enough. Despite this, probabilistic models are still used extensively,\nraising the more pertinent question of whether the model is good enough for a\nspecific task. This question can be formalized as a robust goodness-of-fit\ntesting problem by asking whether the data were generated by a distribution\ncorresponding to our model up to some mild perturbation. In this paper, we show\nthat existing kernel goodness-of-fit tests are not robust according to common\nnotions of robustness including qualitative and quantitative robustness. We\nalso show that robust techniques based on tilted kernels from the parameter\nestimation literature are not sufficient for ensuring both types of robustness\nin the context of goodness-of-fit testing. We therefore propose the first\nrobust kernel goodness-of-fit test which resolves this open problem using\nkernel Stein discrepancy balls, which encompass perturbation models such as\nHuber contamination models and density uncertainty bands.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Methodology","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}