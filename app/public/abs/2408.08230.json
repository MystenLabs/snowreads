{"id":"2408.08230","title":"Explaining an Agent's Future Beliefs through Temporally Decomposing\n  Future Reward Estimators","authors":"Mark Towers, Yali Du, Christopher Freeman, Timothy J. Norman","authorsParsed":[["Towers","Mark",""],["Du","Yali",""],["Freeman","Christopher",""],["Norman","Timothy J.",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 15:56:15 GMT"}],"updateDate":"2024-08-16","timestamp":1723737375000,"abstract":"  Future reward estimation is a core component of reinforcement learning\nagents; i.e., Q-value and state-value functions, predicting an agent's sum of\nfuture rewards. Their scalar output, however, obfuscates when or what\nindividual future rewards an agent may expect to receive. We address this by\nmodifying an agent's future reward estimator to predict their next N expected\nrewards, referred to as Temporal Reward Decomposition (TRD). This unlocks novel\nexplanations of agent behaviour. Through TRD we can: estimate when an agent may\nexpect to receive a reward, the value of the reward and the agent's confidence\nin receiving it; measure an input feature's temporal importance to the agent's\naction decisions; and predict the influence of different actions on future\nrewards. Furthermore, we show that DQN agents trained on Atari environments can\nbe efficiently retrained to incorporate TRD with minimal impact on performance.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}