{"id":"2408.10920","title":"Recurrent Neural Networks Learn to Store and Generate Sequences using\n  Non-Linear Representations","authors":"R\\'obert Csord\\'as, Christopher Potts, Christopher D. Manning, Atticus\n  Geiger","authorsParsed":[["Csordás","Róbert",""],["Potts","Christopher",""],["Manning","Christopher D.",""],["Geiger","Atticus",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 15:04:37 GMT"}],"updateDate":"2024-08-21","timestamp":1724166277000,"abstract":"  The Linear Representation Hypothesis (LRH) states that neural networks learn\nto encode concepts as directions in activation space, and a strong version of\nthe LRH states that models learn only such encodings. In this paper, we present\na counterexample to this strong LRH: when trained to repeat an input token\nsequence, gated recurrent neural networks (RNNs) learn to represent the token\nat each position with a particular order of magnitude, rather than a direction.\nThese representations have layered features that are impossible to locate in\ndistinct linear subspaces. To show this, we train interventions to predict and\nmanipulate tokens by learning the scaling factor corresponding to each sequence\nposition. These interventions indicate that the smallest RNNs find only this\nmagnitude-based solution, while larger RNNs have linear representations. These\nfindings strongly indicate that interpretability research should not be\nconfined by the LRH.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/"}