{"id":"2408.05618","title":"UrFound: Towards Universal Retinal Foundation Models via\n  Knowledge-Guided Masked Modeling","authors":"Kai Yu, Yang Zhou, Yang Bai, Zhi Da Soh, Xinxing Xu, Rick Siow Mong\n  Goh, Ching-Yu Cheng, Yong Liu","authorsParsed":[["Yu","Kai",""],["Zhou","Yang",""],["Bai","Yang",""],["Da Soh","Zhi",""],["Xu","Xinxing",""],["Goh","Rick Siow Mong",""],["Cheng","Ching-Yu",""],["Liu","Yong",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 19:31:29 GMT"}],"updateDate":"2024-08-13","timestamp":1723318289000,"abstract":"  Retinal foundation models aim to learn generalizable representations from\ndiverse retinal images, facilitating label-efficient model adaptation across\nvarious ophthalmic tasks. Despite their success, current retinal foundation\nmodels are generally restricted to a single imaging modality, such as Color\nFundus Photography (CFP) or Optical Coherence Tomography (OCT), limiting their\nversatility. Moreover, these models may struggle to fully leverage expert\nannotations and overlook the valuable domain knowledge essential for\ndomain-specific representation learning. To overcome these limitations, we\nintroduce UrFound, a retinal foundation model designed to learn universal\nrepresentations from both multimodal retinal images and domain knowledge.\nUrFound is equipped with a modality-agnostic image encoder and accepts either\nCFP or OCT images as inputs. To integrate domain knowledge into representation\nlearning, we encode expert annotation in text supervision and propose a\nknowledge-guided masked modeling strategy for model pre-training. It involves\nreconstructing randomly masked patches of retinal images while predicting\nmasked text tokens conditioned on the corresponding retinal image. This\napproach aligns multimodal images and textual expert annotations within a\nunified latent space, facilitating generalizable and domain-specific\nrepresentation learning. Experimental results demonstrate that UrFound exhibits\nstrong generalization ability and data efficiency when adapting to various\ntasks in retinal image analysis. By training on ~180k retinal images, UrFound\nsignificantly outperforms the state-of-the-art retinal foundation model trained\non up to 1.6 million unlabelled images across 8 public retinal datasets. Our\ncode and data are available at https://github.com/yukkai/UrFound.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}