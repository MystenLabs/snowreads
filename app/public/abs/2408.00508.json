{"id":"2408.00508","title":"Block-Operations: Using Modular Routing to Improve Compositional\n  Generalization","authors":"Florian Dietz, Dietrich Klakow","authorsParsed":[["Dietz","Florian",""],["Klakow","Dietrich",""]],"versions":[{"version":"v1","created":"Thu, 1 Aug 2024 12:28:22 GMT"}],"updateDate":"2024-08-02","timestamp":1722515302000,"abstract":"  We explore the hypothesis that poor compositional generalization in neural\nnetworks is caused by difficulties with learning effective routing. To solve\nthis problem, we propose the concept of block-operations, which is based on\nsplitting all activation tensors in the network into uniformly sized blocks and\nusing an inductive bias to encourage modular routing and modification of these\nblocks. Based on this concept we introduce the Multiplexer, a new architectural\ncomponent that enhances the Feed Forward Neural Network (FNN). We\nexperimentally confirm that Multiplexers exhibit strong compositional\ngeneralization. On both a synthetic and a realistic task our model was able to\nlearn the underlying process behind the task, whereas both FNNs and\nTransformers were only able to learn heuristic approximations. We propose as\nfuture work to use the principles of block-operations to improve other existing\narchitectures.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bv5xLxK-8ZqEDn_uRysbizIYS7ZdfayeIYyrmJ1so0Y","pdfSize":"677146","txDigest":"FRQDdzB95wtHFTouCKv5in4udzEUSuEGKKL7GTJ72nzn","endEpoch":"1","status":"CERTIFIED"}
