{"id":"2407.05843","title":"Evaluating the Fairness of Neural Collapse in Medical Image\n  Classification","authors":"Kaouther Mouheb, Marawan Elbatel, Stefan Klein, Esther E. Bron","authorsParsed":[["Mouheb","Kaouther",""],["Elbatel","Marawan",""],["Klein","Stefan",""],["Bron","Esther E.",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 11:41:32 GMT"}],"updateDate":"2024-07-09","timestamp":1720438892000,"abstract":"  Deep learning has achieved impressive performance across various medical\nimaging tasks. However, its inherent bias against specific groups hinders its\nclinical applicability in equitable healthcare systems. A recently discovered\nphenomenon, Neural Collapse (NC), has shown potential in improving the\ngeneralization of state-of-the-art deep learning models. Nonetheless, its\nimplications on bias in medical imaging remain unexplored. Our study\ninvestigates deep learning fairness through the lens of NC. We analyze the\ntraining dynamics of models as they approach NC when training using biased\ndatasets, and examine the subsequent impact on test performance, specifically\nfocusing on label bias. We find that biased training initially results in\ndifferent NC configurations across subgroups, before converging to a final NC\nsolution by memorizing all data samples. Through extensive experiments on three\nmedical imaging datasets -- PAPILA, HAM10000, and CheXpert -- we find that in\nbiased settings, NC can lead to a significant drop in F1 score across all\nsubgroups. Our code is available at\nhttps://gitlab.com/radiology/neuro/neural-collapse-fairness\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}