{"id":"2407.00801","title":"Model-Free Active Exploration in Reinforcement Learning","authors":"Alessio Russo, Alexandre Proutiere","authorsParsed":[["Russo","Alessio",""],["Proutiere","Alexandre",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 19:00:49 GMT"}],"updateDate":"2024-07-02","timestamp":1719774049000,"abstract":"  We study the problem of exploration in Reinforcement Learning and present a\nnovel model-free solution. We adopt an information-theoretical viewpoint and\nstart from the instance-specific lower bound of the number of samples that have\nto be collected to identify a nearly-optimal policy. Deriving this lower bound\nalong with the optimal exploration strategy entails solving an intricate\noptimization problem and requires a model of the system. In turn, most existing\nsample optimal exploration algorithms rely on estimating the model. We derive\nan approximation of the instance-specific lower bound that only involves\nquantities that can be inferred using model-free approaches. Leveraging this\napproximation, we devise an ensemble-based model-free exploration strategy\napplicable to both tabular and continuous Markov decision processes. Numerical\nresults demonstrate that our strategy is able to identify efficient policies\nfaster than state-of-the-art exploration approaches\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}