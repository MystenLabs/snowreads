{"id":"2408.09544","title":"No Such Thing as a General Learner: Language models and their dual\n  optimization","authors":"Emmanuel Chemla and Ryan M. Nefdt","authorsParsed":[["Chemla","Emmanuel",""],["Nefdt","Ryan M.",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 17:01:42 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 17:04:08 GMT"}],"updateDate":"2024-08-22","timestamp":1724000502000,"abstract":"  What role can the otherwise successful Large Language Models (LLMs) play in\nthe understanding of human cognition, and in particular in terms of informing\nlanguage acquisition debates? To contribute to this question, we first argue\nthat neither humans nor LLMs are general learners, in a variety of senses. We\nmake a novel case for how in particular LLMs follow a dual-optimization\nprocess: they are optimized during their training (which is typically compared\nto language acquisition), and modern LLMs have also been selected, through a\nprocess akin to natural selection in a species. From this perspective, we argue\nthat the performance of LLMs, whether similar or dissimilar to that of humans,\ndoes not weigh easily on important debates about the importance of human\ncognitive biases for language.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FHlY_LKmuwNU5kV6cX6asSXzaTjLXFOU7doarem6CtI","pdfSize":"409719"}
