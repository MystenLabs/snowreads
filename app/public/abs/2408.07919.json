{"id":"2408.07919","title":"Advancing Multi-grained Alignment for Contrastive Language-Audio\n  Pre-training","authors":"Yiming Li, Zhifang Guo, Xiangdong Wang, Hong Liu","authorsParsed":[["Li","Yiming",""],["Guo","Zhifang",""],["Wang","Xiangdong",""],["Liu","Hong",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 04:09:19 GMT"}],"updateDate":"2024-08-16","timestamp":1723694959000,"abstract":"  Recent advances have been witnessed in audio-language joint learning, such as\nCLAP, that shows much success in multi-modal understanding tasks. These models\nusually aggregate uni-modal local representations, namely frame or word\nfeatures, into global ones, on which the contrastive loss is employed to reach\ncoarse-grained cross-modal alignment. However, frame-level correspondence with\ntexts may be ignored, making it ill-posed on explainability and fine-grained\nchallenges which may also undermine performances on coarse-grained tasks. In\nthis work, we aim to improve both coarse- and fine-grained audio-language\nalignment in large-scale contrastive pre-training. To unify the granularity and\nlatent distribution of two modalities, a shared codebook is adopted to\nrepresent multi-modal global features with common bases, and each codeword is\nregularized to encode modality-shared semantics, bridging the gap between frame\nand word features. Based on it, a locality-aware block is involved to purify\nlocal patterns, and a hard-negative guided loss is devised to boost alignment.\nExperiments on eleven zero-shot coarse- and fine-grained tasks suggest that our\nmodel not only surpasses the baseline CLAP significantly but also yields\nsuperior or competitive results compared to current SOTA works.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}