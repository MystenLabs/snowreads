{"id":"2407.21771","title":"Paying More Attention to Image: A Training-Free Method for Alleviating\n  Hallucination in LVLMs","authors":"Shi Liu, Kecheng Zheng, Wei Chen","authorsParsed":[["Liu","Shi",""],["Zheng","Kecheng",""],["Chen","Wei",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 17:46:57 GMT"}],"updateDate":"2024-08-01","timestamp":1722448017000,"abstract":"  Existing Large Vision-Language Models (LVLMs) primarily align image features\nof vision encoder with Large Language Models (LLMs) to leverage their superior\ntext generation capabilities. However, the scale disparity between vision\nencoder and language model may led to LLMs assuming a predominant role in\nmulti-modal comprehension. This imbalance in LVLMs may result in the instances\nof hallucinatory. Concretely, LVLMs may generate consistent descriptions with\nor without visual input, indicating that certain outputs are influenced solely\nby context text. We refer to this phenomenon as \"text inertia.\" To counteract\nthis issue, we introduce a training-free algorithm to find an equilibrium point\nbetween image comprehension and language inference. Specifically, we adaptively\ninvolve adjusting and amplifying the attention weights assigned to image\ntokens, thereby granting greater prominence to visual elements. Meanwhile, we\nsubtract the logits of multi-modal inputs from ones of pure text input, which\ncan help LVLMs be not biased towards LLMs. By enhancing images tokens and\nreducing the stubborn output of LLM, we can let LVLM pay more attention to\nimages, towards alleviating text inertia and reducing the hallucination in\nLVLMs. Our extensive experiments shows that this method substantially reduces\nthe frequency of hallucinatory outputs in various LVLMs in terms of different\nmetrics. Project page is available at https://lalbj.github.io/projects/PAI/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}