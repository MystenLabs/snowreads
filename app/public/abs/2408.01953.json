{"id":"2408.01953","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","authors":"Yue Chen, Chenrui Tie, Ruihai Wu, Hao Dong","authorsParsed":[["Chen","Yue",""],["Tie","Chenrui",""],["Wu","Ruihai",""],["Dong","Hao",""]],"versions":[{"version":"v1","created":"Sun, 4 Aug 2024 07:59:17 GMT"},{"version":"v2","created":"Wed, 7 Aug 2024 13:24:38 GMT"}],"updateDate":"2024-08-08","timestamp":1722758357000,"abstract":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}