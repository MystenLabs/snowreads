{"id":"2408.05802","title":"Egocentric Vision Language Planning","authors":"Zhirui Fang, Ming Yang, Weishuai Zeng, Boyu Li, Junpeng Yue, Ziluo\n  Ding, Xiu Li, Zongqing Lu","authorsParsed":[["Fang","Zhirui",""],["Yang","Ming",""],["Zeng","Weishuai",""],["Li","Boyu",""],["Yue","Junpeng",""],["Ding","Ziluo",""],["Li","Xiu",""],["Lu","Zongqing",""]],"versions":[{"version":"v1","created":"Sun, 11 Aug 2024 15:37:29 GMT"}],"updateDate":"2024-08-13","timestamp":1723390649000,"abstract":"  We explore leveraging large multi-modal models (LMMs) and text2image models\nto build a more general embodied agent. LMMs excel in planning long-horizon\ntasks over symbolic abstractions but struggle with grounding in the physical\nworld, often failing to accurately identify object positions in images. A\nbridge is needed to connect LMMs to the physical world. The paper proposes a\nnovel approach, egocentric vision language planning (EgoPlan), to handle\nlong-horizon tasks from an egocentric perspective in varying household\nscenarios. This model leverages a diffusion model to simulate the fundamental\ndynamics between states and actions, integrating techniques like style transfer\nand optical flow to enhance generalization across different environmental\ndynamics. The LMM serves as a planner, breaking down instructions into\nsub-goals and selecting actions based on their alignment with these sub-goals,\nthus enabling more generalized and effective decision-making. Experiments show\nthat EgoPlan improves long-horizon task success rates from the egocentric view\ncompared to baselines across household scenarios.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}