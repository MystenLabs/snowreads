{"id":"2408.05364","title":"Spherical World-Locking for Audio-Visual Localization in Egocentric\n  Videos","authors":"Heeseung Yun, Ruohan Gao, Ishwarya Ananthabhotla, Anurag Kumar, Jacob\n  Donley, Chao Li, Gunhee Kim, Vamsi Krishna Ithapu, Calvin Murdock","authorsParsed":[["Yun","Heeseung",""],["Gao","Ruohan",""],["Ananthabhotla","Ishwarya",""],["Kumar","Anurag",""],["Donley","Jacob",""],["Li","Chao",""],["Kim","Gunhee",""],["Ithapu","Vamsi Krishna",""],["Murdock","Calvin",""]],"versions":[{"version":"v1","created":"Fri, 9 Aug 2024 22:29:04 GMT"}],"updateDate":"2024-08-13","timestamp":1723242544000,"abstract":"  Egocentric videos provide comprehensive contexts for user and scene\nunderstanding, spanning multisensory perception to behavioral interaction. We\npropose Spherical World-Locking (SWL) as a general framework for egocentric\nscene representation, which implicitly transforms multisensory streams with\nrespect to measurements of head orientation. Compared to conventional\nhead-locked egocentric representations with a 2D planar field-of-view, SWL\neffectively offsets challenges posed by self-motion, allowing for improved\nspatial synchronization between input modalities. Using a set of multisensory\nembeddings on a worldlocked sphere, we design a unified encoder-decoder\ntransformer architecture that preserves the spherical structure of the scene\nrepresentation, without requiring expensive projections between image and world\ncoordinate systems. We evaluate the effectiveness of the proposed framework on\nmultiple benchmark tasks for egocentric video understanding, including\naudio-visual active speaker localization, auditory spherical source\nlocalization, and behavior anticipation in everyday activities.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}