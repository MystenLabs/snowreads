{"id":"2407.18838","title":"The Role of Temporal Hierarchy in Spiking Neural Networks","authors":"Filippo Moro, Pau Vilimelis Aceituno, Laura Kriener, Melika Payvand","authorsParsed":[["Moro","Filippo",""],["Aceituno","Pau Vilimelis",""],["Kriener","Laura",""],["Payvand","Melika",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 16:00:20 GMT"}],"updateDate":"2024-07-29","timestamp":1722009620000,"abstract":"  Spiking Neural Networks (SNNs) have the potential for rich spatio-temporal\nsignal processing thanks to exploiting both spatial and temporal parameters.\nThe temporal dynamics such as time constants of the synapses and neurons and\ndelays have been recently shown to have computational benefits that help reduce\nthe overall number of parameters required in the network and increase the\naccuracy of the SNNs in solving temporal tasks. Optimizing such temporal\nparameters, for example, through gradient descent, gives rise to a temporal\narchitecture for different problems. As has been shown in machine learning, to\nreduce the cost of optimization, architectural biases can be applied, in this\ncase in the temporal domain. Such inductive biases in temporal parameters have\nbeen found in neuroscience studies, highlighting a hierarchy of temporal\nstructure and input representation in different layers of the cortex. Motivated\nby this, we propose to impose a hierarchy of temporal representation in the\nhidden layers of SNNs, highlighting that such an inductive bias improves their\nperformance. We demonstrate the positive effects of temporal hierarchy in the\ntime constants of feed-forward SNNs applied to temporal tasks (Multi-Time-Scale\nXOR and Keyword Spotting, with a benefit of up to 4.1% in classification\naccuracy). Moreover, we show that such architectural biases, i.e. hierarchy of\ntime constants, naturally emerge when optimizing the time constants through\ngradient descent, initialized as homogeneous values. We further pursue this\nproposal in temporal convolutional SNNs, by introducing the hierarchical bias\nin the size and dilation of temporal kernels, giving rise to competitive\nresults in popular temporal spike-based datasets.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"sYefbXifIW7mM-iiP3edFw0PAtRHwsS7BnH8LBu4mBc","pdfSize":"883321","objectId":"0x5585fb32092a4c86c86bb360eb45fdb0d8f1a4955cd6110843aa5bbfa621d680","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
