{"id":"2408.14037","title":"Re-Mix: Optimizing Data Mixtures for Large Scale Imitation Learning","authors":"Joey Hejna, Chethan Bhateja, Yichen Jian, Karl Pertsch, Dorsa Sadigh","authorsParsed":[["Hejna","Joey",""],["Bhateja","Chethan",""],["Jian","Yichen",""],["Pertsch","Karl",""],["Sadigh","Dorsa",""]],"versions":[{"version":"v1","created":"Mon, 26 Aug 2024 06:14:25 GMT"}],"updateDate":"2024-08-27","timestamp":1724652865000,"abstract":"  Increasingly large imitation learning datasets are being collected with the\ngoal of training foundation models for robotics. However, despite the fact that\ndata selection has been of utmost importance in vision and natural language\nprocessing, little work in robotics has questioned what data such models should\nactually be trained on. In this work we investigate how to weigh different\nsubsets or ``domains'' of robotics datasets for robot foundation model\npre-training. Concrete, we use distributionally robust optimization (DRO) to\nmaximize worst-case performance across all possible downstream domains. Our\nmethod, Re-Mix, addresses the wide range of challenges that arise when applying\nDRO to robotics datasets including variability in action spaces and dynamics\nacross different datasets. Re-Mix employs early stopping, action normalization,\nand discretization to counteract these issues. Through extensive\nexperimentation on the largest open-source robot manipulation dataset, the Open\nX-Embodiment dataset, we demonstrate that data curation can have an outsized\nimpact on downstream performance. Specifically, domain weights learned by\nRe-Mix outperform uniform weights by 38\\% on average and outperform\nhuman-selected weights by 32\\% on datasets used to train existing generalist\nrobot policies, specifically the RT-X models.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}