{"id":"2408.09320","title":"Auptimize: Optimal Placement of Spatial Audio Cues for Extended Reality","authors":"Hyunsung Cho, Alexander Wang, Divya Kartik, Emily Liying Xie, Yukang\n  Yan, David Lindlbauer","authorsParsed":[["Cho","Hyunsung",""],["Wang","Alexander",""],["Kartik","Divya",""],["Xie","Emily Liying",""],["Yan","Yukang",""],["Lindlbauer","David",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 01:14:05 GMT"}],"updateDate":"2024-08-20","timestamp":1723943645000,"abstract":"  Spatial audio in Extended Reality (XR) provides users with better awareness\nof where virtual elements are placed, and efficiently guides them to events\nsuch as notifications, system alerts from different windows, or approaching\navatars. Humans, however, are inaccurate in localizing sound cues, especially\nwith multiple sources due to limitations in human auditory perception such as\nangular discrimination error and front-back confusion. This decreases the\nefficiency of XR interfaces because users misidentify from which XR element a\nsound is coming. To address this, we propose Auptimize, a novel computational\napproach for placing XR sound sources, which mitigates such localization errors\nby utilizing the ventriloquist effect. Auptimize disentangles the sound source\nlocations from the visual elements and relocates the sound sources to optimal\npositions for unambiguous identification of sound cues, avoiding errors due to\ninter-source proximity and front-back confusion. Our evaluation shows that\nAuptimize decreases spatial audio-based source identification errors compared\nto playing sound cues at the paired visual-sound locations. We demonstrate the\napplicability of Auptimize for diverse spatial audio-based interactive XR\nscenarios.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/"}