{"id":"2407.19553","title":"Exploring the Adversarial Robustness of CLIP for AI-generated Image\n  Detection","authors":"Vincenzo De Rosa and Fabrizio Guillaro and Giovanni Poggi and Davide\n  Cozzolino and Luisa Verdoliva","authorsParsed":[["De Rosa","Vincenzo",""],["Guillaro","Fabrizio",""],["Poggi","Giovanni",""],["Cozzolino","Davide",""],["Verdoliva","Luisa",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 18:20:08 GMT"}],"updateDate":"2024-07-30","timestamp":1722190808000,"abstract":"  In recent years, many forensic detectors have been proposed to detect\nAI-generated images and prevent their use for malicious purposes. Convolutional\nneural networks (CNNs) have long been the dominant architecture in this field\nand have been the subject of intense study. However, recently proposed\nTransformer-based detectors have been shown to match or even outperform\nCNN-based detectors, especially in terms of generalization. In this paper, we\nstudy the adversarial robustness of AI-generated image detectors, focusing on\nContrastive Language-Image Pretraining (CLIP)-based methods that rely on Visual\nTransformer backbones and comparing their performance with CNN-based methods.\nWe study the robustness to different adversarial attacks under a variety of\nconditions and analyze both numerical results and frequency-domain patterns.\nCLIP-based detectors are found to be vulnerable to white-box attacks just like\nCNN-based detectors. However, attacks do not easily transfer between CNN-based\nand CLIP-based methods. This is also confirmed by the different distribution of\nthe adversarial noise patterns in the frequency domain. Overall, this analysis\nprovides new insights into the properties of forensic detectors that can help\nto develop more effective strategies.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}