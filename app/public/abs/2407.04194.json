{"id":"2407.04194","title":"Using Synthetic Data to Regularize Maximum Likelihood Estimation","authors":"Weihao Li, Dongming Huang","authorsParsed":[["Li","Weihao",""],["Huang","Dongming",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 00:40:03 GMT"}],"updateDate":"2024-07-08","timestamp":1720140003000,"abstract":"  To overcome challenges in fitting complex models with small samples,\ncatalytic priors have recently been proposed to stabilize the inference by\nsupplementing observed data with synthetic data generated from simpler models.\nBased on a catalytic prior, the Maximum A Posteriori (MAP) estimator is a\nregularized estimator that maximizes the weighted likelihood of the combined\ndata. This estimator is straightforward to compute, and its numerical\nperformance is superior or comparable to other likelihood-based estimators. In\nthis paper, we study several theoretical aspects regarding the MAP estimator in\ngeneralized linear models, with a particular focus on logistic regression. We\nfirst prove that under mild conditions, the MAP estimator exists and is stable\nagainst the randomness in synthetic data. We then establish the consistency of\nthe MAP estimator when the dimension of covariates diverges slower than the\nsample size. Furthermore, we utilize the convex Gaussian min-max theorem to\ncharacterize the asymptotic behavior of the MAP estimator as the dimension\ngrows linearly with the sample size. These theoretical results clarify the role\nof the tuning parameters in a catalytic prior, and provide insights in\npractical applications. We provide numerical studies to confirm the effective\napproximation of our asymptotic theory in finite samples and to illustrate\nadjusting inference based on the theory.\n","subjects":["Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}