{"id":"2407.06153","title":"What's Wrong with Your Code Generated by Large Language Models? An\n  Extensive Study","authors":"Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou,\n  Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu,\n  Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang\n  Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing\n  Huang","authorsParsed":[["Dou","Shihan",""],["Jia","Haoxiang",""],["Wu","Shenxi",""],["Zheng","Huiyuan",""],["Zhou","Weikang",""],["Wu","Muling",""],["Chai","Mingxu",""],["Fan","Jessica",""],["Huang","Caishuang",""],["Tao","Yunbo",""],["Liu","Yan",""],["Zhou","Enyu",""],["Zhang","Ming",""],["Zhou","Yuhao",""],["Wu","Yueming",""],["Zheng","Rui",""],["Wen","Ming",""],["Weng","Rongxiang",""],["Wang","Jingang",""],["Cai","Xunliang",""],["Gui","Tao",""],["Qiu","Xipeng",""],["Zhang","Qi",""],["Huang","Xuanjing",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:27:17 GMT"}],"updateDate":"2024-07-09","timestamp":1720459637000,"abstract":"  The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}