{"id":"2407.08150","title":"Hypergraph Multi-modal Large Language Model: Exploiting EEG and\n  Eye-tracking Modalities to Evaluate Heterogeneous Responses for Video\n  Understanding","authors":"Minghui Wu, Chenxu Zhao, Anyang Su, Donglin Di, Tianyu Fu, Da An, Min\n  He, Ya Gao, Meng Ma, Kun Yan, Ping Wang","authorsParsed":[["Wu","Minghui",""],["Zhao","Chenxu",""],["Su","Anyang",""],["Di","Donglin",""],["Fu","Tianyu",""],["An","Da",""],["He","Min",""],["Gao","Ya",""],["Ma","Meng",""],["Yan","Kun",""],["Wang","Ping",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 03:00:26 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 05:47:20 GMT"},{"version":"v3","created":"Thu, 5 Sep 2024 02:21:05 GMT"}],"updateDate":"2024-09-06","timestamp":1720666826000,"abstract":"  Understanding of video creativity and content often varies among individuals,\nwith differences in focal points and cognitive levels across different ages,\nexperiences, and genders. There is currently a lack of research in this area,\nand most existing benchmarks suffer from several drawbacks: 1) a limited number\nof modalities and answers with restrictive length; 2) the content and scenarios\nwithin the videos are excessively monotonous, transmitting allegories and\nemotions that are overly simplistic. To bridge the gap to real-world\napplications, we introduce a large-scale Subjective Response Indicators for\nAdvertisement Videos dataset, namely SRI-ADV. Specifically, we collected real\nchanges in Electroencephalographic (EEG) and eye-tracking regions from\ndifferent demographics while they viewed identical video content. Utilizing\nthis multi-modal dataset, we developed tasks and protocols to analyze and\nevaluate the extent of cognitive understanding of video content among different\nusers. Along with the dataset, we designed a Hypergraph Multi-modal Large\nLanguage Model (HMLLM) to explore the associations among different\ndemographics, video elements, EEG, and eye-tracking indicators. HMLLM could\nbridge semantic gaps across rich modalities and integrate information beyond\ndifferent modalities to perform logical reasoning. Extensive experimental\nevaluations on SRI-ADV and other additional video-based generative performance\nbenchmarks demonstrate the effectiveness of our method. The codes and dataset\nwill be released at https://github.com/mininglamp-MLLM/HMLLM.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}