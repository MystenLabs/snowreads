{"id":"2408.10473","title":"Enhancing One-shot Pruned Pre-trained Language Models through\n  Sparse-Dense-Sparse Mechanism","authors":"Guanchen Li, Xiandong Zhao, Lian Liu, Zeping Li, Dong Li, Lu Tian, Jie\n  He, Ashish Sirasao, Emad Barsoum","authorsParsed":[["Li","Guanchen",""],["Zhao","Xiandong",""],["Liu","Lian",""],["Li","Zeping",""],["Li","Dong",""],["Tian","Lu",""],["He","Jie",""],["Sirasao","Ashish",""],["Barsoum","Emad",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 01:05:45 GMT"}],"updateDate":"2024-08-21","timestamp":1724115945000,"abstract":"  Pre-trained language models (PLMs) are engineered to be robust in contextual\nunderstanding and exhibit outstanding performance in various natural language\nprocessing tasks. However, their considerable size incurs significant\ncomputational and storage costs. Modern pruning strategies employ one-shot\ntechniques to compress PLMs without the need for retraining on task-specific or\notherwise general data; however, these approaches often lead to an\nindispensable reduction in performance. In this paper, we propose SDS, a\nSparse-Dense-Sparse pruning framework to enhance the performance of the pruned\nPLMs from a weight distribution optimization perspective. We outline the\npruning process in three steps. Initially, we prune less critical connections\nin the model using conventional one-shot pruning methods. Next, we reconstruct\na dense model featuring a pruning-friendly weight distribution by reactivating\npruned connections with sparse regularization. Finally, we perform a second\npruning round, yielding a superior pruned model compared to the initial\npruning. Experimental results demonstrate that SDS outperforms the\nstate-of-the-art pruning techniques SparseGPT and Wanda under an identical\nsparsity configuration. For instance, SDS reduces perplexity by 9.13 on\nRaw-Wikitext2 and improves accuracy by an average of 2.05% across multiple\nzero-shot benchmarks for OPT-125M with 2:4 sparsity.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}