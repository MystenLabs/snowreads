{"id":"2407.04287","title":"MARS: Paying more attention to visual attributes for text-based person\n  search","authors":"Alex Ergasti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi,\n  Andrea Prati","authorsParsed":[["Ergasti","Alex",""],["Fontanini","Tomaso",""],["Ferrari","Claudio",""],["Bertozzi","Massimo",""],["Prati","Andrea",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 06:44:43 GMT"}],"updateDate":"2024-07-08","timestamp":1720161883000,"abstract":"  Text-based person search (TBPS) is a problem that gained significant interest\nwithin the research community. The task is that of retrieving one or more\nimages of a specific individual based on a textual description. The multi-modal\nnature of the task requires learning representations that bridge text and image\ndata within a shared latent space. Existing TBPS systems face two major\nchallenges. One is defined as inter-identity noise that is due to the inherent\nvagueness and imprecision of text descriptions and it indicates how\ndescriptions of visual attributes can be generally associated to different\npeople; the other is the intra-identity variations, which are all those\nnuisances e.g. pose, illumination, that can alter the visual appearance of the\nsame textual attributes for a given subject. To address these issues, this\npaper presents a novel TBPS architecture named MARS\n(Mae-Attribute-Relation-Sensitive), which enhances current state-of-the-art\nmodels by introducing two key components: a Visual Reconstruction Loss and an\nAttribute Loss. The former employs a Masked AutoEncoder trained to reconstruct\nrandomly masked image patches with the aid of the textual description. In doing\nso the model is encouraged to learn more expressive representations and\ntextual-visual relations in the latent space. The Attribute Loss, instead,\nbalances the contribution of different types of attributes, defined as\nadjective-noun chunks of text. This loss ensures that every attribute is taken\ninto consideration in the person retrieval process. Extensive experiments on\nthree commonly used datasets, namely CUHK-PEDES, ICFG-PEDES, and RSTPReid,\nreport performance improvements, with significant gains in the mean Average\nPrecision (mAP) metric w.r.t. the current state of the art.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}