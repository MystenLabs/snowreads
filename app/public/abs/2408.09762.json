{"id":"2408.09762","title":"Sequential Federated Learning in Hierarchical Architecture on Non-IID\n  Datasets","authors":"Xingrun Yan, Shiyuan Zuo, Rongfei Fan, Han Hu, Li Shen, Puning Zhao,\n  Yong Luo","authorsParsed":[["Yan","Xingrun",""],["Zuo","Shiyuan",""],["Fan","Rongfei",""],["Hu","Han",""],["Shen","Li",""],["Zhao","Puning",""],["Luo","Yong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 07:43:35 GMT"}],"updateDate":"2024-08-20","timestamp":1724053415000,"abstract":"  In a real federated learning (FL) system, communication overhead for passing\nmodel parameters between the clients and the parameter server (PS) is often a\nbottleneck. Hierarchical federated learning (HFL) that poses multiple edge\nservers (ESs) between clients and the PS can partially alleviate communication\npressure but still needs the aggregation of model parameters from multiple ESs\nat the PS. To further reduce communication overhead, we bring sequential FL\n(SFL) into HFL for the first time, which removes the central PS and enables the\nmodel training to be completed only through passing the global model between\ntwo adjacent ESs for each iteration, and propose a novel algorithm adaptive to\nsuch a combinational framework, referred to as Fed-CHS. Convergence results are\nderived for strongly convex and non-convex loss functions under various data\nheterogeneity setups, which show comparable convergence performance with the\nalgorithms for HFL or SFL solely. Experimental results provide evidence of the\nsuperiority of our proposed Fed-CHS on both communication overhead saving and\ntest accuracy over baseline methods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}