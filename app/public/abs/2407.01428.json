{"id":"2407.01428","title":"Reinforcement Learning-driven Data-intensive Workflow Scheduling for\n  Volunteer Edge-Cloud","authors":"Motahare Mounesan, Mauro Lemus, Hemanth Yeddulapalli, Prasad Calyam,\n  Saptarshi Debroy","authorsParsed":[["Mounesan","Motahare",""],["Lemus","Mauro",""],["Yeddulapalli","Hemanth",""],["Calyam","Prasad",""],["Debroy","Saptarshi",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 16:21:13 GMT"}],"updateDate":"2024-07-02","timestamp":1719850873000,"abstract":"  In recent times, Volunteer Edge-Cloud (VEC) has gained traction as a\ncost-effective, community computing paradigm to support data-intensive\nscientific workflows. However, due to the highly distributed and heterogeneous\nnature of VEC resources, centralized workflow task scheduling remains a\nchallenge. In this paper, we propose a Reinforcement Learning (RL)-driven\ndata-intensive scientific workflow scheduling approach that takes into\nconsideration: i) workflow requirements, ii) VEC resources' preference on\nworkflows, and iii) diverse VEC resource policies, to ensure robust resource\nallocation. We formulate the long-term average performance optimization problem\nas a Markov Decision Process, which is solved using an event-based Asynchronous\nAdvantage Actor-Critic RL approach. Our extensive simulations and testbed\nimplementations demonstrate our approach's benefits over popular baseline\nstrategies in terms of workflow requirement satisfaction, VEC preference\nsatisfaction, and available VEC resource utilization.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}