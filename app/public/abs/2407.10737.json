{"id":"2407.10737","title":"Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision\n  Models","authors":"Rining Wu, Feixiang Zhou, Ziwei Yin, Jian K. Liu","authorsParsed":[["Wu","Rining",""],["Zhou","Feixiang",""],["Yin","Ziwei",""],["Liu","Jian K.",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 14:06:13 GMT"}],"updateDate":"2024-07-16","timestamp":1721052373000,"abstract":"  Our brains represent the ever-changing environment with neurons in a highly\ndynamic fashion. The temporal features of visual pixels in dynamic natural\nscenes are entrapped in the neuronal responses of the retina. It is crucial to\nestablish the intrinsic temporal relationship between visual pixels and\nneuronal responses. Recent foundation vision models have paved an advanced way\nof understanding image pixels. Yet, neuronal coding in the brain largely lacks\na deep understanding of its alignment with pixels. Most previous studies employ\nstatic images or artificial videos derived from static images for emulating\nmore real and complicated stimuli. Despite these simple scenarios effectively\nhelp to separate key factors influencing visual coding, complex temporal\nrelationships receive no consideration. To decompose the temporal features of\nvisual coding in natural scenes, here we propose Vi-ST, a spatiotemporal\nconvolutional neural network fed with a self-supervised Vision Transformer\n(ViT) prior, aimed at unraveling the temporal-based encoding patterns of\nretinal neuronal populations. The model demonstrates robust predictive\nperformance in generalization tests. Furthermore, through detailed ablation\nexperiments, we demonstrate the significance of each temporal module.\nFurthermore, we introduce a visual coding evaluation metric designed to\nintegrate temporal considerations and compare the impact of different numbers\nof neuronal populations on complementary coding. In conclusion, our proposed\nVi-ST demonstrates a novel modeling framework for neuronal coding of dynamic\nvisual scenes in the brain, effectively aligning our brain representation of\nvideo with neuronal activity. The code is available at\nhttps://github.com/wurining/Vi-ST.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GnRvME8Iw5nBTtn_fKbJHyXu0LC9uwFlzkJlYSFaCsI","pdfSize":"4396945"}
