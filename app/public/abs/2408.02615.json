{"id":"2408.02615","title":"LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local\n  Attention and Mamba","authors":"Yunxiang Fu, Chaoqi Chen, Yizhou Yu","authorsParsed":[["Fu","Yunxiang",""],["Chen","Chaoqi",""],["Yu","Yizhou",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 16:39:39 GMT"},{"version":"v2","created":"Wed, 18 Sep 2024 17:55:17 GMT"},{"version":"v3","created":"Thu, 19 Sep 2024 16:07:14 GMT"}],"updateDate":"2024-09-20","timestamp":1722875979000,"abstract":"  Recent Transformer-based diffusion models have shown remarkable performance,\nlargely attributed to the ability of the self-attention mechanism to accurately\ncapture both global and local contexts by computing all-pair interactions among\ninput tokens. However, their quadratic complexity poses significant\ncomputational challenges for long-sequence inputs. Conversely, a recent state\nspace model called Mamba offers linear complexity by compressing a filtered\nglobal context into a hidden state. Despite its efficiency, compression\ninevitably leads to information loss of fine-grained local dependencies among\ntokens, which are crucial for effective visual generative modeling. Motivated\nby these observations, we introduce Local Attentional Mamba (LaMamba) blocks\nthat combine the strengths of self-attention and Mamba, capturing both global\ncontexts and local details with linear complexity. Leveraging the efficient\nU-Net architecture, our model exhibits exceptional scalability and surpasses\nthe performance of DiT across various model scales on ImageNet at 256x256\nresolution, all while utilizing substantially fewer GFLOPs and a comparable\nnumber of parameters. Compared to state-of-the-art diffusion models on ImageNet\n256x256 and 512x512, our largest model presents notable advantages, such as a\nreduction of up to 62% GFLOPs compared to DiT-XL/2, while achieving superior\nperformance with comparable or fewer parameters. Our code is available at\nhttps://github.com/yunxiangfu2001/LaMamba-Diff.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yttUhF9CW_UMnZgm8MH4aYlGyJtnopYpMbf4dMNay8E","pdfSize":"25265023"}
