{"id":"2407.08551","title":"Autoregressive Speech Synthesis without Vector Quantization","authors":"Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie\n  Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, Helen Meng, Furu Wei","authorsParsed":[["Meng","Lingwei",""],["Zhou","Long",""],["Liu","Shujie",""],["Chen","Sanyuan",""],["Han","Bing",""],["Hu","Shujie",""],["Liu","Yanqing",""],["Li","Jinyu",""],["Zhao","Sheng",""],["Wu","Xixin",""],["Meng","Helen",""],["Wei","Furu",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 14:36:53 GMT"}],"updateDate":"2024-07-12","timestamp":1720708613000,"abstract":"  We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}