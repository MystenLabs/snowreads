{"id":"2408.09357","title":"Meta-Learning Empowered Meta-Face: Personalized Speaking Style\n  Adaptation for Audio-Driven 3D Talking Face Animation","authors":"Xukun Zhou, Fengxin Li, Ziqiao Peng, Kejian Wu, Jun He, Biao Qin,\n  Zhaoxin Fan, Hongyan Liu","authorsParsed":[["Zhou","Xukun",""],["Li","Fengxin",""],["Peng","Ziqiao",""],["Wu","Kejian",""],["He","Jun",""],["Qin","Biao",""],["Fan","Zhaoxin",""],["Liu","Hongyan",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 04:42:43 GMT"}],"updateDate":"2024-08-20","timestamp":1723956163000,"abstract":"  Audio-driven 3D face animation is increasingly vital in live streaming and\naugmented reality applications. While remarkable progress has been observed,\nmost existing approaches are designed for specific individuals with predefined\nspeaking styles, thus neglecting the adaptability to varied speaking styles. To\naddress this limitation, this paper introduces MetaFace, a novel methodology\nmeticulously crafted for speaking style adaptation. Grounded in the novel\nconcept of meta-learning, MetaFace is composed of several key components: the\nRobust Meta Initialization Stage (RMIS) for fundamental speaking style\nadaptation, the Dynamic Relation Mining Neural Process (DRMN) for forging\nconnections between observed and unobserved speaking styles, and the Low-rank\nMatrix Memory Reduction Approach to enhance the efficiency of model\noptimization as well as learning style details. Leveraging these novel designs,\nMetaFace not only significantly outperforms robust existing baselines but also\nestablishes a new state-of-the-art, as substantiated by our experimental\nresults.\n","subjects":["Computing Research Repository/Graphics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/"}