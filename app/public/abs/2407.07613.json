{"id":"2407.07613","title":"Probabilistic learning rate scheduler with provable convergence","authors":"Dahlia Devapriya, Thulasi Tholeti, Janani Suresh, Sheetal Kalyani","authorsParsed":[["Devapriya","Dahlia",""],["Tholeti","Thulasi",""],["Suresh","Janani",""],["Kalyani","Sheetal",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 12:52:24 GMT"}],"updateDate":"2024-07-11","timestamp":1720615944000,"abstract":"  Learning rate schedulers have shown great success in speeding up the\nconvergence of learning algorithms in practice. However, their convergence to a\nminimum has not been proven theoretically. This difficulty mainly arises from\nthe fact that, while traditional convergence analysis prescribes to\nmonotonically decreasing (or constant) learning rates, schedulers opt for rates\nthat often increase and decrease through the training epochs. In this work, we\naim to bridge the gap by proposing a probabilistic learning rate scheduler\n(PLRS), that does not conform to the monotonically decreasing condition, with\nprovable convergence guarantees. In addition to providing detailed convergence\nproofs, we also show experimental results where the proposed PLRS performs\ncompetitively as other state-of-the-art learning rate schedulers across a\nvariety of datasets and architectures.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}