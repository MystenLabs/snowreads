{"id":"2407.15792","title":"Robust Mixture Learning when Outliers Overwhelm Small Groups","authors":"Daniil Dmitriev, Rares-Darius Buhai, Stefan Tiegel, Alexander Wolters,\n  Gleb Novikov, Amartya Sanyal, David Steurer, Fanny Yang","authorsParsed":[["Dmitriev","Daniil",""],["Buhai","Rares-Darius",""],["Tiegel","Stefan",""],["Wolters","Alexander",""],["Novikov","Gleb",""],["Sanyal","Amartya",""],["Steurer","David",""],["Yang","Fanny",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 16:51:05 GMT"}],"updateDate":"2024-07-23","timestamp":1721667065000,"abstract":"  We study the problem of estimating the means of well-separated mixtures when\nan adversary may add arbitrary outliers. While strong guarantees are available\nwhen the outlier fraction is significantly smaller than the minimum mixing\nweight, much less is known when outliers may crowd out low-weight clusters - a\nsetting we refer to as list-decodable mixture learning (LD-ML). In this case,\nadversarial outliers can simulate additional spurious mixture components.\nHence, if all means of the mixture must be recovered up to a small error in the\noutput list, the list size needs to be larger than the number of (true)\ncomponents. We propose an algorithm that obtains order-optimal error guarantees\nfor each mixture mean with a minimal list-size overhead, significantly\nimproving upon list-decodable mean estimation, the only existing method that is\napplicable for LD-ML. Although improvements are observed even when the mixture\nis non-separated, our algorithm achieves particularly strong guarantees when\nthe mixture is separated: it can leverage the mixture structure to partially\ncluster the samples before carefully iterating a base learner for\nlist-decodable mean estimation at different scales.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Data Structures and Algorithms","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}