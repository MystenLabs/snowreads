{"id":"2407.01602","title":"Clustering in pure-attention hardmax transformers and its role in\n  sentiment analysis","authors":"Albert Alcalde, Giovanni Fantuzzi, Enrique Zuazua","authorsParsed":[["Alcalde","Albert",""],["Fantuzzi","Giovanni",""],["Zuazua","Enrique",""]],"versions":[{"version":"v1","created":"Wed, 26 Jun 2024 16:13:35 GMT"}],"updateDate":"2024-07-03","timestamp":1719418415000,"abstract":"  Transformers are extremely successful machine learning models whose\nmathematical properties remain poorly understood. Here, we rigorously\ncharacterize the behavior of transformers with hardmax self-attention and\nnormalization sublayers as the number of layers tends to infinity. By viewing\nsuch transformers as discrete-time dynamical systems describing the evolution\nof points in a Euclidean space, and thanks to a geometric interpretation of the\nself-attention mechanism based on hyperplane separation, we show that the\ntransformer inputs asymptotically converge to a clustered equilibrium\ndetermined by special points called leaders. We then leverage this theoretical\nunderstanding to solve sentiment analysis problems from language processing\nusing a fully interpretable transformer model, which effectively captures\n`context' by clustering meaningless words around leader words carrying the most\nmeaning. Finally, we outline remaining challenges to bridge the gap between the\nmathematical analysis of transformers and their real-life implementation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Mathematics/Dynamical Systems","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"ONFJptF4vLLVLGPEuPqmkFMw5lS2qBYJqCgKUYLgR_c","pdfSize":"1756353"}
