{"id":"2408.09529","title":"Revisiting the Graph Reasoning Ability of Large Language Models: Case\n  Studies in Translation, Connectivity and Shortest Path","authors":"Xinnan Dai, Qihao Wen, Yifei Shen, Hongzhi Wen, Dongsheng Li, Jiliang\n  Tang, Caihua Shan","authorsParsed":[["Dai","Xinnan",""],["Wen","Qihao",""],["Shen","Yifei",""],["Wen","Hongzhi",""],["Li","Dongsheng",""],["Tang","Jiliang",""],["Shan","Caihua",""]],"versions":[{"version":"v1","created":"Sun, 18 Aug 2024 16:26:39 GMT"}],"updateDate":"2024-08-20","timestamp":1723998399000,"abstract":"  Large Language Models (LLMs) have achieved great success in various reasoning\ntasks. In this work, we focus on the graph reasoning ability of LLMs. Although\ntheoretical studies proved that LLMs are capable of handling graph reasoning\ntasks, empirical evaluations reveal numerous failures. To deepen our\nunderstanding on this discrepancy, we revisit the ability of LLMs on three\nfundamental graph tasks: graph description translation, graph connectivity, and\nthe shortest-path problem. Our findings suggest that LLMs can fail to\nunderstand graph structures through text descriptions and exhibit varying\nperformance for all these three fundamental tasks. Meanwhile, we perform a\nreal-world investigation on knowledge graphs and make consistent observations\nwith our findings. The codes and datasets are available.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"O9-7RNN_DUQc5AKHt6HigL92EbNf0YxRrft_yWXVYhk","pdfSize":"613348"}
