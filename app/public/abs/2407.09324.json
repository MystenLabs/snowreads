{"id":"2407.09324","title":"Provable Privacy Advantages of Decentralized Federated Learning via\n  Distributed Optimization","authors":"Wenrui Yu, Qiongxiu Li, Milan Lopuha\\\"a-Zwakenberg, Mads\n  Gr{\\ae}sb{\\o}ll Christensen and Richard Heusdens","authorsParsed":[["Yu","Wenrui",""],["Li","Qiongxiu",""],["Lopuhaä-Zwakenberg","Milan",""],["Christensen","Mads Græsbøll",""],["Heusdens","Richard",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 15:01:09 GMT"}],"updateDate":"2024-07-15","timestamp":1720796469000,"abstract":"  Federated learning (FL) emerged as a paradigm designed to improve data\nprivacy by enabling data to reside at its source, thus embedding privacy as a\ncore consideration in FL architectures, whether centralized or decentralized.\nContrasting with recent findings by Pasquini et al., which suggest that\ndecentralized FL does not empirically offer any additional privacy or security\nbenefits over centralized models, our study provides compelling evidence to the\ncontrary. We demonstrate that decentralized FL, when deploying distributed\noptimization, provides enhanced privacy protection - both theoretically and\nempirically - compared to centralized approaches. The challenge of quantifying\nprivacy loss through iterative processes has traditionally constrained the\ntheoretical exploration of FL protocols. We overcome this by conducting a\npioneering in-depth information-theoretical privacy analysis for both\nframeworks. Our analysis, considering both eavesdropping and passive adversary\nmodels, successfully establishes bounds on privacy leakage. We show information\ntheoretically that the privacy loss in decentralized FL is upper bounded by the\nloss in centralized FL. Compared to the centralized case where local gradients\nof individual participants are directly revealed, a key distinction of\noptimization-based decentralized FL is that the relevant information includes\ndifferences of local gradients over successive iterations and the aggregated\nsum of different nodes' gradients over the network. This information\ncomplicates the adversary's attempt to infer private data. To bridge our\ntheoretical insights with practical applications, we present detailed case\nstudies involving logistic regression and deep neural networks. These examples\ndemonstrate that while privacy leakage remains comparable in simpler models,\ncomplex models like deep neural networks exhibit lower privacy risks under\ndecentralized FL.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}