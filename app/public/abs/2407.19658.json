{"id":"2407.19658","title":"Enhancing CTR Prediction through Sequential Recommendation Pre-training:\n  Introducing the SRP4CTR Framework","authors":"Ruidong Han, Qianzhong Li, He Jiang, Rui Li, Yurou Zhao, Xiang Li, Wei\n  Lin","authorsParsed":[["Han","Ruidong",""],["Li","Qianzhong",""],["Jiang","He",""],["Li","Rui",""],["Zhao","Yurou",""],["Li","Xiang",""],["Lin","Wei",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 02:49:11 GMT"}],"updateDate":"2024-07-30","timestamp":1722221351000,"abstract":"  Understanding user interests is crucial for Click-Through Rate (CTR)\nprediction tasks. In sequential recommendation, pre-training from user\nhistorical behaviors through self-supervised learning can better comprehend\nuser dynamic preferences, presenting the potential for direct integration with\nCTR tasks. Previous methods have integrated pre-trained models into downstream\ntasks with the sole purpose of extracting semantic information or\nwell-represented user features, which are then incorporated as new features.\nHowever, these approaches tend to ignore the additional inference costs to the\ndownstream tasks, and they do not consider how to transfer the effective\ninformation from the pre-trained models for specific estimated items in CTR\nprediction. In this paper, we propose a Sequential Recommendation Pre-training\nframework for CTR prediction (SRP4CTR) to tackle the above problems. Initially,\nwe discuss the impact of introducing pre-trained models on inference costs.\nSubsequently, we introduced a pre-trained method to encode sequence side\ninformation concurrently.During the fine-tuning process, we incorporate a\ncross-attention block to establish a bridge between estimated items and the\npre-trained model at a low cost. Moreover, we develop a querying transformer\ntechnique to facilitate the knowledge transfer from the pre-trained model to\nindustrial CTR models. Offline and online experiments show that our method\noutperforms previous baseline models.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}