{"id":"2408.02279","title":"DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for\n  Long Time-Series Forecasting","authors":"Ruixin Ding, Yuqi Chen, Yu-Ting Lan, Wei Zhang","authorsParsed":[["Ding","Ruixin",""],["Chen","Yuqi",""],["Lan","Yu-Ting",""],["Zhang","Wei",""]],"versions":[{"version":"v1","created":"Mon, 5 Aug 2024 07:26:47 GMT"}],"updateDate":"2024-08-06","timestamp":1722842807000,"abstract":"  Long-term time series forecasting (LTSF) has been widely applied in finance,\ntraffic prediction, and other domains. Recently, patch-based transformers have\nemerged as a promising approach, segmenting data into sub-level patches that\nserve as input tokens. However, existing methods mostly rely on predetermined\npatch lengths, necessitating expert knowledge and posing challenges in\ncapturing diverse characteristics across various scales. Moreover, time series\ndata exhibit diverse variations and fluctuations across different temporal\nscales, which traditional approaches struggle to model effectively. In this\npaper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm\nto capture diverse receptive fields and sparse patterns of time series data. In\norder to build hierarchical receptive fields, we develop a multi-scale\nTransformer model, coupled with multi-scale sequence extraction, capable of\ncapturing multi-resolution features. Additionally, we introduce a group-aware\nrotary position encoding technique to enhance intra- and inter-group position\nawareness among representations across different temporal scales. Our proposed\nmodel, named DRFormer, is evaluated on various real-world datasets, and\nexperimental results demonstrate its superiority compared to existing methods.\nOur code is available at: https://github.com/ruixindingECNU/DRFormer.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}