{"id":"2407.01171","title":"Neural Conditional Probability for Inference","authors":"Vladimir R. Kostic, Karim Lounici, Gregoire Pacreau, Pietro Novelli,\n  Giacomo Turri and Massimiliano Pontil","authorsParsed":[["Kostic","Vladimir R.",""],["Lounici","Karim",""],["Pacreau","Gregoire",""],["Novelli","Pietro",""],["Turri","Giacomo",""],["Pontil","Massimiliano",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 10:44:29 GMT"}],"updateDate":"2024-07-02","timestamp":1719830669000,"abstract":"  We introduce NCP (Neural Conditional Probability), a novel operator-theoretic\napproach for learning conditional distributions with a particular focus on\ninference tasks. NCP can be used to build conditional confidence regions and\nextract important statistics like conditional quantiles, mean, and covariance.\nIt offers streamlined learning through a single unconditional training phase,\nfacilitating efficient inference without the need for retraining even when\nconditioning changes. By tapping into the powerful approximation capabilities\nof neural networks, our method efficiently handles a wide variety of complex\nprobability distributions, effectively dealing with nonlinear relationships\nbetween input and output variables. Theoretical guarantees ensure both\noptimization consistency and statistical accuracy of the NCP method. Our\nexperiments show that our approach matches or beats leading methods using a\nsimple Multi-Layer Perceptron (MLP) with two hidden layers and GELU\nactivations. This demonstrates that a minimalistic architecture with a\ntheoretically grounded loss function can achieve competitive results without\nsacrificing performance, even in the face of more complex architectures.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Methodology","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/"}