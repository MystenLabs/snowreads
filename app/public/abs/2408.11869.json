{"id":"2408.11869","title":"Enhance Lifelong Model Editing with Continuous Data-Adapter Association","authors":"Jiaang Li, Quan Wang, Zhongnan Wang, Yongdong Zhang, Zhendong Mao","authorsParsed":[["Li","Jiaang",""],["Wang","Quan",""],["Wang","Zhongnan",""],["Zhang","Yongdong",""],["Mao","Zhendong",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 02:27:00 GMT"}],"updateDate":"2024-08-23","timestamp":1724034420000,"abstract":"  Large language models (LLMs) require model editing to efficiently update\nspecific knowledge within them and avoid factual errors. Most model editing\nmethods are solely designed for single-time use and lead to a significant\nforgetting effect after sequential edits over time, referred to as lifelong\nediting. Current approaches manage sequential edits by freezing original\nparameters and allocating new adapters for each knowledge modification.\nHowever, these methods lack robustness to minor input variations. To address\nthis challenge, we propose ELDER, \\textbf{E}nhancing \\textbf{L}ifelong\nmo\\textbf{D}el \\textbf{E}diting with mixtu\\textbf{R}e of Low-Rank Adapter\n(LoRA). ELDER is an adaptive approach that integrates multiple LoRAs through a\nrouter network. It learns to create a continuous and smooth association between\ndata and adapters, thereby enhancing robustness and generalization to\nsemantically equivalent inputs. Additionally, we introduce a novel loss to help\nlearn associations between adapter allocations and edit semantics. A deferral\nmechanism is also proposed to retain the original LLM capabilities post-edit.\nExtensive experiments on GPT-2 XL and LLaMA2-7B demonstrate that ELDER\neffectively edits models in the lifelong setting and exhibits strong\nscalability, while retaining LLM's general abilities on downstream tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}