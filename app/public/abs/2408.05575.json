{"id":"2408.05575","title":"In-Context Exploiter for Extensive-Form Games","authors":"Shuxin Li, Chang Yang, Youzhi Zhang, Pengdeng Li, Xinrun Wang, Xiao\n  Huang, Hau Chan, Bo An","authorsParsed":[["Li","Shuxin",""],["Yang","Chang",""],["Zhang","Youzhi",""],["Li","Pengdeng",""],["Wang","Xinrun",""],["Huang","Xiao",""],["Chan","Hau",""],["An","Bo",""]],"versions":[{"version":"v1","created":"Sat, 10 Aug 2024 14:59:09 GMT"}],"updateDate":"2024-08-13","timestamp":1723301949000,"abstract":"  Nash equilibrium (NE) is a widely adopted solution concept in game theory due\nto its stability property. However, we observe that the NE strategy might not\nalways yield the best results, especially against opponents who do not adhere\nto NE strategies. Based on this observation, we pose a new game-solving\nquestion: Can we learn a model that can exploit any, even NE, opponent to\nmaximize their own utility? In this work, we make the first attempt to\ninvestigate this problem through in-context learning. Specifically, we\nintroduce a novel method, In-Context Exploiter (ICE), to train a single model\nthat can act as any player in the game and adaptively exploit opponents\nentirely by in-context learning. Our ICE algorithm involves generating diverse\nopponent strategies, collecting interactive history training data by a\nreinforcement learning algorithm, and training a transformer-based agent within\na well-designed curriculum learning framework. Finally, comprehensive\nexperimental results validate the effectiveness of our ICE algorithm,\nshowcasing its in-context learning ability to exploit any unknown opponent,\nthereby positively answering our initial game-solving question.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Science and Game Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}