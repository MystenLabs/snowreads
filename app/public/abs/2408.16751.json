{"id":"2408.16751","title":"A Gradient Analysis Framework for Rewarding Good and Penalizing Bad\n  Examples in Language Models","authors":"Yi-Lin Tuan, William Yang Wang","authorsParsed":[["Tuan","Yi-Lin",""],["Wang","William Yang",""]],"versions":[{"version":"v1","created":"Thu, 29 Aug 2024 17:46:18 GMT"}],"updateDate":"2024-08-30","timestamp":1724953578000,"abstract":"  Beyond maximum likelihood estimation (MLE), the standard objective of a\nlanguage model (LM) that optimizes good examples probabilities, many studies\nhave explored ways that also penalize bad examples for enhancing the quality of\noutput distribution, including unlikelihood training, exponential maximizing\naverage treatment effect (ExMATE), and direct preference optimization (DPO). To\nsystematically compare these methods and further provide a unified recipe for\nLM optimization, in this paper, we present a unique angle of gradient analysis\nof loss functions that simultaneously reward good examples and penalize bad\nones in LMs. Through both mathematical results and experiments on\nCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional\ncharacteristics among these methods. We find that ExMATE serves as a superior\nsurrogate for MLE, and that combining DPO with ExMATE instead of MLE further\nenhances both the statistical (5-7%) and generative (+18% win rate)\nperformance.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/"}