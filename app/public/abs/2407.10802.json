{"id":"2407.10802","title":"Motion-prior Contrast Maximization for Dense Continuous-Time Motion\n  Estimation","authors":"Friedhelm Hamann, Ziyun Wang, Ioannis Asmanis, Kenneth Chaney,\n  Guillermo Gallego, Kostas Daniilidis","authorsParsed":[["Hamann","Friedhelm",""],["Wang","Ziyun",""],["Asmanis","Ioannis",""],["Chaney","Kenneth",""],["Gallego","Guillermo",""],["Daniilidis","Kostas",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:18:28 GMT"}],"updateDate":"2024-07-16","timestamp":1721056708000,"abstract":"  Current optical flow and point-tracking methods rely heavily on synthetic\ndatasets. Event cameras are novel vision sensors with advantages in challenging\nvisual conditions, but state-of-the-art frame-based methods cannot be easily\nadapted to event data due to the limitations of current event simulators. We\nintroduce a novel self-supervised loss combining the Contrast Maximization\nframework with a non-linear motion prior in the form of pixel-level\ntrajectories and propose an efficient solution to solve the high-dimensional\nassignment problem between non-linear trajectories and events. Their\neffectiveness is demonstrated in two scenarios: In dense continuous-time motion\nestimation, our method improves the zero-shot performance of a synthetically\ntrained model on the real-world dataset EVIMO2 by 29%. In optical flow\nestimation, our method elevates a simple UNet to achieve state-of-the-art\nperformance among self-supervised methods on the DSEC optical flow benchmark.\nOur code is available at https://github.com/tub-rip/MotionPriorCMax.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"gAkC3xB9aDT3gp1JwErcY92VVgIlkrUyOdY7HrRRhto","pdfSize":"15074660"}
