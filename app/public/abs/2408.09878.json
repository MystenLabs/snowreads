{"id":"2408.09878","title":"Transferring Backdoors between Large Language Models by Knowledge\n  Distillation","authors":"Pengzhou Cheng, Zongru Wu, Tianjie Ju, Wei Du, Zhuosheng Zhang\n  Gongshen Liu","authorsParsed":[["Cheng","Pengzhou",""],["Wu","Zongru",""],["Ju","Tianjie",""],["Du","Wei",""],["Liu","Zhuosheng Zhang Gongshen",""]],"versions":[{"version":"v1","created":"Mon, 19 Aug 2024 10:39:45 GMT"}],"updateDate":"2024-08-20","timestamp":1724063985000,"abstract":"  Backdoor Attacks have been a serious vulnerability against Large Language\nModels (LLMs). However, previous methods only reveal such risk in specific\nmodels, or present tasks transferability after attacking the pre-trained phase.\nSo, how risky is the model transferability of a backdoor attack? In this paper,\nwe focus on whether existing mini-LLMs may be unconsciously instructed in\nbackdoor knowledge by poisoned teacher LLMs through knowledge distillation\n(KD). Specifically, we propose ATBA, an adaptive transferable backdoor attack,\nwhich can effectively distill the backdoor of teacher LLMs into small models\nwhen only executing clean-tuning. We first propose the Target Trigger\nGeneration (TTG) module that filters out a set of indicative trigger candidates\nfrom the token list based on cosine similarity distribution. Then, we exploit a\nshadow model to imitate the distilling process and introduce an Adaptive\nTrigger Optimization (ATO) module to realize a gradient-based greedy feedback\nto search optimal triggers. Extensive experiments show that ATBA generates not\nonly positive guidance for student models but also implicitly transfers\nbackdoor knowledge. Our attack is robust and stealthy, with over 80% backdoor\ntransferability, and hopes the attention of security.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}