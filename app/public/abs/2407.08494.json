{"id":"2407.08494","title":"Multivariate root-n-consistent smoothing parameter free matching\n  estimators and estimators of inverse density weighted expectations","authors":"Hajo Holzmann and Alexander Meister","authorsParsed":[["Holzmann","Hajo",""],["Meister","Alexander",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:28:34 GMT"}],"updateDate":"2024-07-12","timestamp":1720704514000,"abstract":"  Expected values weighted by the inverse of a multivariate density or,\nequivalently, Lebesgue integrals of regression functions with multivariate\nregressors occur in various areas of applications, including estimating average\ntreatment effects, nonparametric estimators in random coefficient regression\nmodels or deconvolution estimators in Berkson errors-in-variables models. The\nfrequently used nearest-neighbor and matching estimators suffer from bias\nproblems in multiple dimensions. By using polynomial least squares fits on each\ncell of the $K^{\\text{th}}$-order Voronoi tessellation for sufficiently large\n$K$, we develop novel modifications of nearest-neighbor and matching estimators\nwhich again converge at the parametric $\\sqrt n $-rate under mild smoothness\nassumptions on the unknown regression function and without any smoothness\nconditions on the unknown density of the covariates. We stress that in contrast\nto competing methods for correcting for the bias of matching estimators, our\nestimators do not involve nonparametric function estimators and in particular\ndo not rely on sample-size dependent smoothing parameters. We complement the\nupper bounds with appropriate lower bounds derived from information-theoretic\narguments, which show that some smoothness of the regression function is indeed\nrequired to achieve the parametric rate. Simulations illustrate the practical\nfeasibility of the proposed methods.\n","subjects":["Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}