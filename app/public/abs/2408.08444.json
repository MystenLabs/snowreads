{"id":"2408.08444","title":"W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question\n  Answering","authors":"Jinming Nian, Zhiyuan Peng, Qifan Wang, Yi Fang","authorsParsed":[["Nian","Jinming",""],["Peng","Zhiyuan",""],["Wang","Qifan",""],["Fang","Yi",""]],"versions":[{"version":"v1","created":"Thu, 15 Aug 2024 22:34:44 GMT"}],"updateDate":"2024-08-19","timestamp":1723761284000,"abstract":"  In knowledge-intensive tasks such as open-domain question answering (OpenQA),\nLarge Language Models (LLMs) often struggle to generate factual answers relying\nsolely on their internal (parametric) knowledge. To address this limitation,\nRetrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving\nrelevant information from external sources, thereby positioning the retriever\nas a pivotal component. Although dense retrieval demonstrates state-of-the-art\nperformance, its training poses challenges due to the scarcity of ground-truth\nevidence, largely attributed to the high costs of human annotation. In this\npaper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create\nweakly labeled data for training dense retrievers. Specifically, we rerank the\ntop-$K$ passages retrieved via BM25 by assessing the probability that LLMs will\ngenerate the correct answer based on the question and each passage. The\nhighest-ranking passages are then used as positive training examples for dense\nretrieval. Our comprehensive experiments across four publicly available OpenQA\ndatasets demonstrate that our approach enhances both retrieval and OpenQA\nperformance compared to baseline models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"18qfbEJZ9cXCk-o-ZKUlt-iBaT9SwRHTmXgGa2FSaJA","pdfSize":"1383364"}
