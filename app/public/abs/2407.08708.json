{"id":"2407.08708","title":"eyeballvul: a future-proof benchmark for vulnerability detection in the\n  wild","authors":"Timothee Chauvin","authorsParsed":[["Chauvin","Timothee",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:46:21 GMT"},{"version":"v2","created":"Sat, 13 Jul 2024 10:44:58 GMT"}],"updateDate":"2024-07-16","timestamp":1720719981000,"abstract":"  Long contexts of recent LLMs have enabled a new use case: asking models to\nfind security vulnerabilities in entire codebases. To evaluate model\nperformance on this task, we introduce eyeballvul: a benchmark designed to test\nthe vulnerability detection capabilities of language models at scale, that is\nsourced and updated weekly from the stream of published vulnerabilities in\nopen-source repositories. The benchmark consists of a list of revisions in\ndifferent repositories, each associated with the list of known vulnerabilities\npresent at that revision. An LLM-based scorer is used to compare the list of\npossible vulnerabilities returned by a model to the list of known\nvulnerabilities for each revision. As of July 2024, eyeballvul contains 24,000+\nvulnerabilities across 6,000+ revisions and 5,000+ repositories, and is around\n55GB in size.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}