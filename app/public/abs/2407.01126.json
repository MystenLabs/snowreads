{"id":"2407.01126","title":"Investigating the potential of Sparse Mixtures-of-Experts for\n  multi-domain neural machine translation","authors":"Nadezhda Chirkova, Vassilina Nikoulina, Jean-Luc Meunier, Alexandre\n  B\\'erard","authorsParsed":[["Chirkova","Nadezhda",""],["Nikoulina","Vassilina",""],["Meunier","Jean-Luc",""],["BÃ©rard","Alexandre",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 09:45:22 GMT"}],"updateDate":"2024-07-02","timestamp":1719827122000,"abstract":"  We focus on multi-domain Neural Machine Translation, with the goal of\ndeveloping efficient models which can handle data from various domains seen\nduring training and are robust to domains unseen during training. We\nhypothesize that Sparse Mixture-of-Experts (SMoE) models are a good fit for\nthis task, as they enable efficient model scaling, which helps to accommodate a\nvariety of multi-domain data, and allow flexible sharing of parameters between\ndomains, potentially enabling knowledge transfer between similar domains and\nlimiting negative transfer. We conduct a series of experiments aimed at\nvalidating the utility of SMoE for the multi-domain scenario, and find that a\nstraightforward width scaling of Transformer is a simpler and surprisingly more\nefficient approach in practice, and reaches the same performance level as SMoE.\nWe also search for a better recipe for robustness of multi-domain systems,\nhighlighting the importance of mixing-in a generic domain, i.e. Paracrawl, and\nintroducing a simple technique, domain randomization.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}