{"id":"2408.10474","title":"LeCov: Multi-level Testing Criteria for Large Language Models","authors":"Xuan Xie, Jiayang Song, Yuheng Huang, Da Song, Fuyuan Zhang, Felix\n  Juefei-Xu, Lei Ma","authorsParsed":[["Xie","Xuan",""],["Song","Jiayang",""],["Huang","Yuheng",""],["Song","Da",""],["Zhang","Fuyuan",""],["Juefei-Xu","Felix",""],["Ma","Lei",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 01:17:54 GMT"}],"updateDate":"2024-08-21","timestamp":1724116674000,"abstract":"  Large Language Models (LLMs) are widely used in many different domains, but\nbecause of their limited interpretability, there are questions about how\ntrustworthy they are in various perspectives, e.g., truthfulness and toxicity.\nRecent research has started developing testing methods for LLMs, aiming to\nuncover untrustworthy issues, i.e., defects, before deployment. However,\nsystematic and formalized testing criteria are lacking, which hinders a\ncomprehensive assessment of the extent and adequacy of testing exploration. To\nmitigate this threat, we propose a set of multi-level testing criteria, LeCov,\nfor LLMs. The criteria consider three crucial LLM internal components, i.e.,\nthe attention mechanism, feed-forward neurons, and uncertainty, and contain\nnine types of testing criteria in total. We apply the criteria in two\nscenarios: test prioritization and coverage-guided testing. The experiment\nevaluation, on three models and four datasets, demonstrates the usefulness and\neffectiveness of LeCov.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}