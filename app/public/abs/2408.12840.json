{"id":"2408.12840","title":"HGNAS: Hardware-Aware Graph Neural Architecture Search for Edge Devices","authors":"Ao Zhou, Jianlei Yang, Yingjie Qi, Tong Qiao, Yumeng Shi, Cenlin Duan,\n  Weisheng Zhao, Chunming Hu","authorsParsed":[["Zhou","Ao",""],["Yang","Jianlei",""],["Qi","Yingjie",""],["Qiao","Tong",""],["Shi","Yumeng",""],["Duan","Cenlin",""],["Zhao","Weisheng",""],["Hu","Chunming",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 05:11:22 GMT"}],"updateDate":"2024-08-26","timestamp":1724389882000,"abstract":"  Graph Neural Networks (GNNs) are becoming increasingly popular for\ngraph-based learning tasks such as point cloud processing due to their\nstate-of-the-art (SOTA) performance. Nevertheless, the research community has\nprimarily focused on improving model expressiveness, lacking consideration of\nhow to design efficient GNN models for edge scenarios with real-time\nrequirements and limited resources. Examining existing GNN models reveals\nvaried execution across platforms and frequent Out-Of-Memory (OOM) problems,\nhighlighting the need for hardware-aware GNN design. To address this challenge,\nthis work proposes a novel hardware-aware graph neural architecture search\nframework tailored for resource constraint edge devices, namely HGNAS. To\nachieve hardware awareness, HGNAS integrates an efficient GNN hardware\nperformance predictor that evaluates the latency and peak memory usage of GNNs\nin milliseconds. Meanwhile, we study GNN memory usage during inference and\noffer a peak memory estimation method, enhancing the robustness of architecture\nevaluations when combined with predictor outcomes. Furthermore, HGNAS\nconstructs a fine-grained design space to enable the exploration of extreme\nperformance architectures by decoupling the GNN paradigm. In addition, the\nmulti-stage hierarchical search strategy is leveraged to facilitate the\nnavigation of huge candidates, which can reduce the single search time to a few\nGPU hours. To the best of our knowledge, HGNAS is the first automated GNN\ndesign framework for edge devices, and also the first work to achieve hardware\nawareness of GNNs across different platforms. Extensive experiments across\nvarious applications and edge devices have proven the superiority of HGNAS. It\ncan achieve up to a 10.6x speedup and an 82.5% peak memory reduction with\nnegligible accuracy loss compared to DGCNN on ModelNet40.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}