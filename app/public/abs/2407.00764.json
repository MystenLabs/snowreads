{"id":"2407.00764","title":"Characterizing Stereotypical Bias from Privacy-preserving Pre-Training","authors":"Stefan Arnold and Rene Gr\\\"obner and Annika Schreiner","authorsParsed":[["Arnold","Stefan",""],["Gr√∂bner","Rene",""],["Schreiner","Annika",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 16:54:43 GMT"}],"updateDate":"2024-07-02","timestamp":1719766483000,"abstract":"  Differential Privacy (DP) can be applied to raw text by exploiting the\nspatial arrangement of words in an embedding space. We investigate the\nimplications of such text privatization on Language Models (LMs) and their\ntendency towards stereotypical associations. Since previous studies documented\nthat linguistic proficiency correlates with stereotypical bias, one could\nassume that techniques for text privatization, which are known to degrade\nlanguage modeling capabilities, would cancel out undesirable biases. By testing\nBERT models trained on texts containing biased statements primed with varying\ndegrees of privacy, our study reveals that while stereotypical bias generally\ndiminishes when privacy is tightened, text privatization does not uniformly\nequate to diminishing bias across all social domains. This highlights the need\nfor careful diagnosis of bias in LMs that undergo text privatization.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}