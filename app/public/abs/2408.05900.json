{"id":"2408.05900","title":"Classifier Guidance Enhances Diffusion-based Adversarial Purification by\n  Preserving Predictive Information","authors":"Mingkun Zhang, Jianing Li, Wei Chen, Jiafeng Guo, Xueqi Cheng","authorsParsed":[["Zhang","Mingkun",""],["Li","Jianing",""],["Chen","Wei",""],["Guo","Jiafeng",""],["Cheng","Xueqi",""]],"versions":[{"version":"v1","created":"Mon, 12 Aug 2024 02:48:00 GMT"}],"updateDate":"2024-08-13","timestamp":1723430880000,"abstract":"  Adversarial purification is one of the promising approaches to defend neural\nnetworks against adversarial attacks. Recently, methods utilizing diffusion\nprobabilistic models have achieved great success for adversarial purification\nin image classification tasks. However, such methods fall into the dilemma of\nbalancing the needs for noise removal and information preservation. This paper\npoints out that existing adversarial purification methods based on diffusion\nmodels gradually lose sample information during the core denoising process,\ncausing occasional label shift in subsequent classification tasks. As a remedy,\nwe suggest to suppress such information loss by introducing guidance from the\nclassifier confidence. Specifically, we propose Classifier-cOnfidence gUided\nPurification (COUP) algorithm, which purifies adversarial examples while\nkeeping away from the classifier decision boundary. Experimental results show\nthat COUP can achieve better adversarial robustness under strong attack\nmethods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}