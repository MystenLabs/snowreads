{"id":"2408.13028","title":"In-Context Learning with Reinforcement Learning for Incomplete Utterance\n  Rewriting","authors":"Haowei Du, Dongyan Zhao","authorsParsed":[["Du","Haowei",""],["Zhao","Dongyan",""]],"versions":[{"version":"v1","created":"Fri, 23 Aug 2024 12:32:12 GMT"}],"updateDate":"2024-08-26","timestamp":1724416332000,"abstract":"  In-context learning (ICL) of large language models (LLMs) has attracted\nincreasing attention in the community where LLMs make predictions only based on\ninstructions augmented with a few examples. Existing example selection methods\nfor ICL utilize sparse or dense retrievers and derive effective performance.\nHowever, these methods do not utilize direct feedback of LLM to train the\nretriever and the examples selected can not necessarily improve the analogy\nability of LLM. To tackle this, we propose our policy-based reinforcement\nlearning framework for example selection (RLS), which consists of a language\nmodel (LM) selector and an LLM generator. The LM selector encodes the candidate\nexamples into dense representations and selects the top-k examples into the\ndemonstration for LLM. The outputs of LLM are adopted to compute the reward and\npolicy gradient to optimize the LM selector. We conduct experiments on\ndifferent datasets and significantly outperform existing example selection\nmethods. Moreover, our approach shows advantages over supervised finetuning\n(SFT) models in few shot setting. Further experiments show the balance of\nabundance and the similarity with the test case of examples is important for\nICL performance of LLM.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"P0aSs5sLQqEI_uy8MQRrUk-48TeNZtia7eRZosv8Qdo","pdfSize":"423366"}
