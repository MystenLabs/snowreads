{"id":"2408.11029","title":"Scaling Law with Learning Rate Annealing","authors":"Howe Tissue, Venus Wang, Lu Wang","authorsParsed":[["Tissue","Howe",""],["Wang","Venus",""],["Wang","Lu",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 17:30:48 GMT"}],"updateDate":"2024-08-21","timestamp":1724175048000,"abstract":"  We find that the cross-entropy loss curves of neural language models\nempirically adhere to a scaling law with learning rate (LR) annealing over\ntraining steps ($s$): $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2$$ Where\n$S_1$ is forward area and $S_2$ is learning rate annealing area. This\nformulation takes into account two factors: (1) The forward scaling defined as\ntypical scaling law, and (2) the additional loss drop brought by LR annealing.\nTherefore, this formulation can describe the full loss curve at each step,\nrather than the single loss point at the end of training. Applying the scaling\nlaw with LR annealing and fitting only one or two training curves, we can\naccurately predict the loss of language model training at any given step and\nacross any learning rate scheduler (LRS). Furthermore, this equation accurately\ndescribes the dynamics during training process, and provides a theoretical\nverification and explanation for numerous experimental findings of previous\nstudies, particularly those focusing on LR schedule and LR annealing. The\nresulting insights, also serve as a guide for researchers to select critical\nLRS in advance by prediction using our equation. Most significantly, since all\nthe points in a full training curve follow the equation, we can achieve\naccurate loss prediction at any given step across any learning rate scheduler,\nwhile expending less than 1\\% of the computational cost required by the\nchinchilla scaling law to fit language modeling loss. This approach extremely\ndemocratizes scaling law fitting and predicting in developing large language\nmodels.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}