{"id":"2407.00743","title":"AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension\n  Transformation for Emotion Recognition in Conversations","authors":"Sheng Wu, Jiaxing Liu, Longbiao Wang, Dongxiao He, Xiaobao Wang,\n  Jianwu Dang","authorsParsed":[["Wu","Sheng",""],["Liu","Jiaxing",""],["Wang","Longbiao",""],["He","Dongxiao",""],["Wang","Xiaobao",""],["Dang","Jianwu",""]],"versions":[{"version":"v1","created":"Fri, 12 Apr 2024 11:31:18 GMT"}],"updateDate":"2024-07-02","timestamp":1712921478000,"abstract":"  Emotion Recognition in Conversations (ERC) is a popular task in natural\nlanguage processing, which aims to recognize the emotional state of the speaker\nin conversations. While current research primarily emphasizes contextual\nmodeling, there exists a dearth of investigation into effective multimodal\nfusion methods. We propose a novel framework called AIMDiT to solve the problem\nof multimodal fusion of deep features. Specifically, we design a Modality\nAugmentation Network which performs rich representation learning through\ndimension transformation of different modalities and parameter-efficient\ninception block. On the other hand, the Modality Interaction Network performs\ninteraction fusion of extracted inter-modal features and intra-modal features.\nExperiments conducted using our AIMDiT framework on the public benchmark\ndataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1\nmetrics compared to the state-of-the-art (SOTA) models.\n","subjects":["Computing Research Repository/Multimedia","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}