{"id":"2408.06537","title":"Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality\n  Parallel Data Outperforms Traditional Web-Crawled Data","authors":"Mara Finkelstein, David Vilar, and Markus Freitag","authorsParsed":[["Finkelstein","Mara",""],["Vilar","David",""],["Freitag","Markus",""]],"versions":[{"version":"v1","created":"Tue, 13 Aug 2024 00:06:56 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 18:38:11 GMT"},{"version":"v3","created":"Fri, 16 Aug 2024 18:23:41 GMT"},{"version":"v4","created":"Wed, 21 Aug 2024 04:03:06 GMT"}],"updateDate":"2024-08-22","timestamp":1723507616000,"abstract":"  Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/"}