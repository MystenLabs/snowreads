{"id":"2407.05355","title":"VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool","authors":"Yan Wang, Yawen Zeng, Jingsheng Zheng, Xiaofen Xing, Jin Xu, Xiangmin\n  Xu","authorsParsed":[["Wang","Yan",""],["Zeng","Yawen",""],["Zheng","Jingsheng",""],["Xing","Xiaofen",""],["Xu","Jin",""],["Xu","Xiangmin",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 13:10:23 GMT"}],"updateDate":"2024-07-09","timestamp":1720357823000,"abstract":"  Multimodal large language models (MLLMs) are flourishing, but mainly focus on\nimages with less attention than videos, especially in sub-fields such as prompt\nengineering, video chain-of-thought (CoT), and instruction tuning on videos.\nTherefore, we try to explore the collection of CoT datasets in videos to lead\nto video OpenQA and improve the reasoning ability of MLLMs. Unfortunately,\nmaking such video CoT datasets is not an easy task. Given that human annotation\nis too cumbersome and expensive, while machine-generated is not reliable due to\nthe hallucination issue, we develop an automatic annotation tool that combines\nmachine and human experts, under the active learning paradigm. Active learning\nis an interactive strategy between the model and human experts, in this way,\nthe workload of human labeling can be reduced and the quality of the dataset\ncan be guaranteed. With the help of the automatic annotation tool, we strive to\ncontribute three datasets, namely VideoCoT, TopicQA, TopicCoT. Furthermore, we\npropose a simple but effective benchmark based on the collected datasets, which\nexploits CoT to maximize the complex reasoning capabilities of MLLMs. Extensive\nexperiments demonstrate the effectiveness our solution.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}