{"id":"2407.21118","title":"Palu: Compressing KV-Cache with Low-Rank Projection","authors":"Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang\n  Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze and Kai-Chiang Wu","authorsParsed":[["Chang","Chi-Chih",""],["Lin","Wei-Cheng",""],["Lin","Chien-Yu",""],["Chen","Chong-Yan",""],["Hu","Yu-Fang",""],["Wang","Pei-Shuo",""],["Huang","Ning-Chi",""],["Ceze","Luis",""],["Wu","Kai-Chiang",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 18:19:38 GMT"}],"updateDate":"2024-08-01","timestamp":1722363578000,"abstract":"  KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}