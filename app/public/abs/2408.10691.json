{"id":"2408.10691","title":"Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches","authors":"Yanjie Dong, Xiaoyi Fan, Fangxin Wang, Chengming Li, Victor C. M.\n  Leung, Xiping Hu","authorsParsed":[["Dong","Yanjie",""],["Fan","Xiaoyi",""],["Wang","Fangxin",""],["Li","Chengming",""],["Leung","Victor C. M.",""],["Hu","Xiping",""]],"versions":[{"version":"v1","created":"Tue, 20 Aug 2024 09:42:17 GMT"}],"updateDate":"2024-08-21","timestamp":1724146937000,"abstract":"  Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/"}