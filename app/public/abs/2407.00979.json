{"id":"2407.00979","title":"Cross-Modal Attention Alignment Network with Auxiliary Text Description\n  for zero-shot sketch-based image retrieval","authors":"Hanwen Su, Ge Song, Kai Huang, Jiyan Wang, Ming Yang","authorsParsed":[["Su","Hanwen",""],["Song","Ge",""],["Huang","Kai",""],["Wang","Jiyan",""],["Yang","Ming",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 05:32:06 GMT"}],"updateDate":"2024-07-02","timestamp":1719811926000,"abstract":"  In this paper, we study the problem of zero-shot sketch-based image retrieval\n(ZS-SBIR). The prior methods tackle the problem in a two-modality setting with\nonly category labels or even no textual information involved. However, the\ngrowing prevalence of Large-scale pre-trained Language Models (LLMs), which\nhave demonstrated great knowledge learned from web-scale data, can provide us\nwith an opportunity to conclude collective textual information. Our key\ninnovation lies in the usage of text data as auxiliary information for images,\nthus leveraging the inherent zero-shot generalization ability that language\noffers. To this end, we propose an approach called Cross-Modal Attention\nAlignment Network with Auxiliary Text Description for zero-shot sketch-based\nimage retrieval. The network consists of three components: (i) a Description\nGeneration Module that generates textual descriptions for each training\ncategory by prompting an LLM with several interrogative sentences, (ii) a\nFeature Extraction Module that includes two ViTs for sketch and image data, a\ntransformer for extracting tokens of sentences of each training category,\nfinally (iii) a Cross-modal Alignment Module that exchanges the token features\nof both text-sketch and text-image using cross-attention mechanism, and align\nthe tokens locally and globally. Extensive experiments on three benchmark\ndatasets show our superior performances over the state-of-the-art ZS-SBIR\nmethods.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/"}