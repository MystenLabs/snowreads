{"id":"2407.04358","title":"An Adaptive Stochastic Gradient Method with Non-negative Gauss-Newton\n  Stepsizes","authors":"Antonio Orvieto, Lin Xiao","authorsParsed":[["Orvieto","Antonio",""],["Xiao","Lin",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 08:53:06 GMT"}],"updateDate":"2024-07-08","timestamp":1720169586000,"abstract":"  We consider the problem of minimizing the average of a large number of smooth\nbut possibly non-convex functions. In the context of most machine learning\napplications, each loss function is non-negative and thus can be expressed as\nthe composition of a square and its real-valued square root. This reformulation\nallows us to apply the Gauss-Newton method, or the Levenberg-Marquardt method\nwhen adding a quadratic regularization. The resulting algorithm, while being\ncomputationally as efficient as the vanilla stochastic gradient method, is\nhighly adaptive and can automatically warmup and decay the effective stepsize\nwhile tracking the non-negative loss landscape. We provide a tight convergence\nanalysis, leveraging new techniques, in the stochastic convex and non-convex\nsettings. In particular, in the convex case, the method does not require access\nto the gradient Lipshitz constant for convergence, and is guaranteed to never\ndiverge. The convergence rates and empirical evaluations compare favorably to\nthe classical (stochastic) gradient method as well as to several other adaptive\nmethods.\n","subjects":["Mathematics/Optimization and Control","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"iqC7lt3y-2FibxUl45hAEnzf57M5AG86gRrDnyBk3wo","pdfSize":"3512044"}
