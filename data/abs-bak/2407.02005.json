{"id":"2407.02005","title":"An End-to-End Speech Summarization Using Large Language Model","authors":"Hengchao Shang, Zongyao Li, Jiaxin Guo, Shaojun Li, Zhiqiang Rao,\n  Yuanchang Luo, Daimeng Wei, Hao Yang","authorsParsed":[["Shang","Hengchao",""],["Li","Zongyao",""],["Guo","Jiaxin",""],["Li","Shaojun",""],["Rao","Zhiqiang",""],["Luo","Yuanchang",""],["Wei","Daimeng",""],["Yang","Hao",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 07:22:57 GMT"}],"updateDate":"2024-07-03","timestamp":1719904977000,"abstract":"  Abstractive Speech Summarization (SSum) aims to generate human-like text\nsummaries from spoken content. It encounters difficulties in handling long\nspeech input and capturing the intricate cross-modal mapping between long\nspeech inputs and short text summaries. Research on large language models\n(LLMs) and multimodal information fusion has provided new insights for\naddressing these challenges. In this paper, we propose an end-to-end SSum model\nthat utilizes Q-Former as a connector for the audio-text modality and employs\nLLMs to generate text summaries directly from speech features. We adopt a\nmulti-stage training approach that includes LLM based ASR and Text\nSummarization (TSum) tasks as auxiliary tasks. ASR tasks are used to align\nfeature spaces and enhance the LLM's ability to handle longer speech. Then, we\nutilize a curriculum learning strategy to facilitate the model's transition\nfrom TSum to SSum. Finally, our model achieves competitive performance on the\nHow-2 dataset.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"iz_k-6JW5sTZfi4bTKLdScOnmfNfCFvHEp44jxSAAqI","pdfSize":"385324","objectId":"0xf1f3d5c4688695427d4500ec51c1115a721332e7f3146c81838bfcac2f365aa2","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
