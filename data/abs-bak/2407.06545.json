{"id":"2407.06545","title":"Visual-Geometry GP-based Navigable Space for Autonomous Navigation","authors":"Mahmoud Ali, Durgkant Pushp, Zheng Chen, and Lantao Liu","authorsParsed":[["Ali","Mahmoud",""],["Pushp","Durgkant",""],["Chen","Zheng",""],["Liu","Lantao",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 04:51:50 GMT"}],"updateDate":"2024-07-10","timestamp":1720500710000,"abstract":"  Autonomous navigation in unknown environments is challenging and demands the\nconsideration of both geometric and semantic information in order to parse the\nnavigability of the environment. In this work, we propose a novel space\nmodeling framework, Visual-Geometry Sparse Gaussian Process (VG-SGP), that\nsimultaneously considers semantics and geometry of the scene. Our proposed\napproach can overcome the limitation of visual planners that fail to recognize\ngeometry associated with the semantic and the geometric planners that\ncompletely overlook the semantic information which is very critical in\nreal-world navigation. The proposed method leverages dual Sparse Gaussian\nProcesses in an integrated manner; the first is trained to forecast\ngeometrically navigable spaces while the second predicts the semantically\nnavigable areas. This integrated model is able to pinpoint the overlapping\n(geometric and semantic) navigable space. The simulation and real-world\nexperiments demonstrate that the ability of the proposed VG-SGP model, coupled\nwith our innovative navigation strategy, outperforms models solely reliant on\nvisual or geometric navigation algorithms, highlighting a superior adaptive\nbehavior.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RWDdzjNnU9lNE6hUckBGg9S0Fhsfhl-HMPVG9fl6bWk","pdfSize":"5395519"}
