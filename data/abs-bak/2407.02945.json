{"id":"2407.02945","title":"VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using\n  Learned Priors","authors":"Sungwon Hwang, Min-Jung Kim, Taewoong Kang, Jayeon Kang, and Jaegul\n  Choo","authorsParsed":[["Hwang","Sungwon",""],["Kim","Min-Jung",""],["Kang","Taewoong",""],["Kang","Jayeon",""],["Choo","Jaegul",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 09:23:13 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 02:27:15 GMT"},{"version":"v3","created":"Sat, 13 Jul 2024 17:16:45 GMT"}],"updateDate":"2024-07-16","timestamp":1719998593000,"abstract":"  Neural rendering-based urban scene reconstruction methods commonly rely on\nimages collected from driving vehicles with cameras facing and moving forward.\nAlthough these methods can successfully synthesize from views similar to\ntraining camera trajectory, directing the novel view outside the training\ncamera distribution does not guarantee on-par performance. In this paper, we\ntackle the Extrapolated View Synthesis (EVS) problem by evaluating the\nreconstructions on views such as looking left, right or downwards with respect\nto training camera distributions. To improve rendering quality for EVS, we\ninitialize our model by constructing dense LiDAR map, and propose to leverage\nprior scene knowledge such as surface normal estimator and large-scale\ndiffusion model. Qualitative and quantitative comparisons demonstrate the\neffectiveness of our methods on EVS. To the best of our knowledge, we are the\nfirst to address the EVS problem in urban scene reconstruction. Link to our\nproject page: https://vegs3d.github.io/.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ItbFcxPKLvckwgkQvBf1kH1hxWPYFRC3jev3uioDom0","pdfSize":"4540627","objectId":"0xbe5bb35e97234da07effb9e564ef5c0db75d11e8c1abe194c280e156b18e637b","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
