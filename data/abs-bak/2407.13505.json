{"id":"2407.13505","title":"Robots Can Multitask Too: Integrating a Memory Architecture and LLMs for\n  Enhanced Cross-Task Robot Action Generation","authors":"Hassan Ali, Philipp Allgeuer, Carlo Mazzola, Giulia Belgiovine, Burak\n  Can Kaplan, Stefan Wermter","authorsParsed":[["Ali","Hassan",""],["Allgeuer","Philipp",""],["Mazzola","Carlo",""],["Belgiovine","Giulia",""],["Kaplan","Burak Can",""],["Wermter","Stefan",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 13:38:21 GMT"}],"updateDate":"2024-07-19","timestamp":1721309901000,"abstract":"  Large Language Models (LLMs) have been recently used in robot applications\nfor grounding LLM common-sense reasoning with the robot's perception and\nphysical abilities. In humanoid robots, memory also plays a critical role in\nfostering real-world embodiment and facilitating long-term interactive\ncapabilities, especially in multi-task setups where the robot must remember\nprevious task states, environment states, and executed actions. In this paper,\nwe address incorporating memory processes with LLMs for generating cross-task\nrobot actions, while the robot effectively switches between tasks. Our proposed\ndual-layered architecture features two LLMs, utilizing their complementary\nskills of reasoning and following instructions, combined with a memory model\ninspired by human cognition. Our results show a significant improvement in\nperformance over a baseline of five robotic tasks, demonstrating the potential\nof integrating memory with LLMs for combining the robot's action and perception\nfor adaptive task execution.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"k82ORjZLDpDorxUyMJ0v9I_jJNMnN1ChyxvxYwSCJng","pdfSize":"2820067","objectId":"0x67d360d567f900f90ffbe19802ac454622acefec9788e4ebdefc2e91060c1f8f","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
