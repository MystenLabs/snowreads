{"id":"2407.17284","title":"A Novel Two-Step Fine-Tuning Pipeline for Cold-Start Active Learning in\n  Text Classification Tasks","authors":"Fabiano Bel\\'em, Washington Cunha, Celso Fran\\c{c}a, Claudio Andrade,\n  Leonardo Rocha, Marcos Andr\\'e Gon\\c{c}alves","authorsParsed":[["Belém","Fabiano",""],["Cunha","Washington",""],["França","Celso",""],["Andrade","Claudio",""],["Rocha","Leonardo",""],["Gonçalves","Marcos André",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 13:50:21 GMT"}],"updateDate":"2024-07-25","timestamp":1721829021000,"abstract":"  This is the first work to investigate the effectiveness of BERT-based\ncontextual embeddings in active learning (AL) tasks on cold-start scenarios,\nwhere traditional fine-tuning is infeasible due to the absence of labeled data.\nOur primary contribution is the proposal of a more robust fine-tuning pipeline\n- DoTCAL - that diminishes the reliance on labeled data in AL using two steps:\n(1) fully leveraging unlabeled data through domain adaptation of the embeddings\nvia masked language modeling and (2) further adjusting model weights using\nlabeled data selected by AL. Our evaluation contrasts BERT-based embeddings\nwith other prevalent text representation paradigms, including Bag of Words\n(BoW), Latent Semantic Indexing (LSI), and FastText, at two critical stages of\nthe AL process: instance selection and classification. Experiments conducted on\neight ATC benchmarks with varying AL budgets (number of labeled instances) and\nnumber of instances (about 5,000 to 300,000) demonstrate DoTCAL's superior\neffectiveness, achieving up to a 33% improvement in Macro-F1 while reducing\nlabeling efforts by half compared to the traditional one-step method. We also\nfound that in several tasks, BoW and LSI (due to information aggregation)\nproduce results superior (up to 59% ) to BERT, especially in low-budget\nscenarios and hard-to-classify tasks, which is quite surprising.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Databases","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-zvgreOIPykn0mt2jzC7SeRG4v_y_FjWsEDg1YFc5Jc","pdfSize":"3004707","objectId":"0xf981c58934f0f334b576cba24911b9ad159e87c7e204caade23f2cd52be9af19","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
