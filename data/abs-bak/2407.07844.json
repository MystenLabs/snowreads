{"id":"2407.07844","title":"OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective\n  Fusion","authors":"Hao Wang, Pengzhen Ren, Zequn Jie, Xiao Dong, Chengjian Feng, Yinlong\n  Qian, Lin Ma, Dongmei Jiang, Yaowei Wang, Xiangyuan Lan, Xiaodan Liang","authorsParsed":[["Wang","Hao",""],["Ren","Pengzhen",""],["Jie","Zequn",""],["Dong","Xiao",""],["Feng","Chengjian",""],["Qian","Yinlong",""],["Ma","Lin",""],["Jiang","Dongmei",""],["Wang","Yaowei",""],["Lan","Xiangyuan",""],["Liang","Xiaodan",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 17:05:49 GMT"},{"version":"v2","created":"Mon, 22 Jul 2024 03:26:21 GMT"}],"updateDate":"2024-07-23","timestamp":1720631149000,"abstract":"  Open-vocabulary detection is a challenging task due to the requirement of\ndetecting objects based on class names, including those not encountered during\ntraining. Existing methods have shown strong zero-shot detection capabilities\nthrough pre-training and pseudo-labeling on diverse large-scale datasets.\nHowever, these approaches encounter two main challenges: (i) how to effectively\neliminate data noise from pseudo-labeling, and (ii) how to efficiently leverage\nthe language-aware capability for region-level cross-modality fusion and\nalignment. To address these challenges, we propose a novel unified\nopen-vocabulary detection method called OV-DINO, which is pre-trained on\ndiverse large-scale datasets with language-aware selective fusion in a unified\nframework. Specifically, we introduce a Unified Data Integration (UniDI)\npipeline to enable end-to-end training and eliminate noise from pseudo-label\ngeneration by unifying different data sources into detection-centric data\nformat. In addition, we propose a Language-Aware Selective Fusion (LASF) module\nto enhance the cross-modality alignment through a language-aware query\nselection and fusion process. We evaluate the performance of the proposed\nOV-DINO on popular open-vocabulary detection benchmarks, achieving\nstate-of-the-art results with an AP of 50.6% on the COCO benchmark and 40.1% on\nthe LVIS benchmark in a zero-shot manner, demonstrating its strong\ngeneralization ability. Furthermore, the fine-tuned OV-DINO on COCO achieves\n58.4% AP, outperforming many existing methods with the same backbone. The code\nfor OV-DINO is available at https://github.com/wanghao9610/OV-DINO.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"M6CyFTiwopCcmW1aoaFdvOxLMmO1-rt7NRVOWVr2yOQ","pdfSize":"5339012","objectId":"0x70a18bb465ccae51c49dd8d0b7f90a6db1c2a5a5f4b967c5cdae8a39964fe806","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
