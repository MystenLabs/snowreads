{"id":"2407.21330","title":"Performance of Recent Large Language Models for a Low-Resourced Language","authors":"Ravindu Jayakody, Gihan Dias","authorsParsed":[["Jayakody","Ravindu",""],["Dias","Gihan",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 04:38:07 GMT"}],"updateDate":"2024-08-01","timestamp":1722400687000,"abstract":"  Large Language Models (LLMs) have shown significant advances in the past\nyear. In addition to new versions of GPT and Llama, several other LLMs have\nbeen introduced recently. Some of these are open models available for download\nand modification.\n  Although multilingual large language models have been available for some\ntime, their performance on low-resourced languages such as Sinhala has been\npoor. We evaluated four recent LLMs on their performance directly in the\nSinhala language, and by translation to and from English. We also evaluated\ntheir fine-tunability with a small amount of fine-tuning data. Claude and GPT\n4o perform well out-of-the-box and do significantly better than previous\nversions. Llama and Mistral perform poorly but show some promise of improvement\nwith fine tuning.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"LcLjrqawt1c8euFcbuI4g36Wv2BRwZVISL5YOEcZWm0","pdfSize":"313985","objectId":"0x59febf0b0de11b65313f463e5c3726684407506c74a609f338618d4db19a751e","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
