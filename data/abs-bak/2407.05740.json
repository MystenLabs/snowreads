{"id":"2407.05740","title":"Do Multilingual Large Language Models Mitigate Stereotype Bias?","authors":"Shangrui Nie, Michael Fromm, Charles Welch, Rebekka G\\\"orge, Akbar\n  Karimi, Joan Plepi, Nazia Afsan Mowmita, Nicolas Flores-Herr, Mehdi Ali,\n  Lucie Flek","authorsParsed":[["Nie","Shangrui",""],["Fromm","Michael",""],["Welch","Charles",""],["GÃ¶rge","Rebekka",""],["Karimi","Akbar",""],["Plepi","Joan",""],["Mowmita","Nazia Afsan",""],["Flores-Herr","Nicolas",""],["Ali","Mehdi",""],["Flek","Lucie",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 08:46:50 GMT"},{"version":"v2","created":"Tue, 9 Jul 2024 08:16:24 GMT"}],"updateDate":"2024-07-10","timestamp":1720428410000,"abstract":"  While preliminary findings indicate that multilingual LLMs exhibit reduced\nbias compared to monolingual ones, a comprehensive understanding of the effect\nof multilingual training on bias mitigation, is lacking. This study addresses\nthis gap by systematically training six LLMs of identical size (2.6B\nparameters) and architecture: five monolingual models (English, German, French,\nItalian, and Spanish) and one multilingual model trained on an equal\ndistribution of data across these languages, all using publicly available data.\nTo ensure robust evaluation, standard bias benchmarks were automatically\ntranslated into the five target languages and verified for both translation\nquality and bias preservation by human annotators. Our results consistently\ndemonstrate that multilingual training effectively mitigates bias. Moreover, we\nobserve that multilingual models achieve not only lower bias but also superior\nprediction accuracy when compared to monolingual models with the same amount of\ntraining data, model architecture, and size.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0_4eZjQWF9AJu6lg4mtx4GamitWpcPoNcanhvuDDeiw","pdfSize":"1028792"}
