{"id":"2407.03387","title":"ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages","authors":"Mehant Kammakomati, Sameer Pimparkhede, Srikanth Tamilselvam, Prince\n  Kumar, Pushpak Bhattacharyya","authorsParsed":[["Kammakomati","Mehant",""],["Pimparkhede","Sameer",""],["Tamilselvam","Srikanth",""],["Kumar","Prince",""],["Bhattacharyya","Pushpak",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 08:36:13 GMT"},{"version":"v2","created":"Fri, 30 Aug 2024 09:13:50 GMT"}],"updateDate":"2024-09-02","timestamp":1719995773000,"abstract":"  Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fvvD8Q3k-RoZ4cduWjkFvIJYpS-RK7Yg3RIqldsWh9w","pdfSize":"586701"}
