{"id":"2407.13851","title":"X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs","authors":"Sirnam Swetha, Jinyu Yang, Tal Neiman, Mamshad Nayeem Rizve, Son Tran,\n  Benjamin Yao, Trishul Chilimbi, Mubarak Shah","authorsParsed":[["Swetha","Sirnam",""],["Yang","Jinyu",""],["Neiman","Tal",""],["Rizve","Mamshad Nayeem",""],["Tran","Son",""],["Yao","Benjamin",""],["Chilimbi","Trishul",""],["Shah","Mubarak",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 18:39:54 GMT"}],"updateDate":"2024-07-22","timestamp":1721327994000,"abstract":"  Recent advancements in Multimodal Large Language Models (MLLMs) have\nrevolutionized the field of vision-language understanding by integrating visual\nperception capabilities into Large Language Models (LLMs). The prevailing trend\nin this field involves the utilization of a vision encoder derived from\nvision-language contrastive learning (CL), showing expertise in capturing\noverall representations while facing difficulties in capturing detailed local\npatterns. In this work, we focus on enhancing the visual representations for\nMLLMs by combining high-frequency and detailed visual representations, obtained\nthrough masked image modeling (MIM), with semantically-enriched low-frequency\nrepresentations captured by CL. To achieve this goal, we introduce X-Former\nwhich is a lightweight transformer module designed to exploit the complementary\nstrengths of CL and MIM through an innovative interaction mechanism.\nSpecifically, X-Former first bootstraps vision-language representation learning\nand multimodal-to-multimodal generative learning from two frozen vision\nencoders, i.e., CLIP-ViT (CL-based) and MAE-ViT (MIM-based). It further\nbootstraps vision-to-language generative learning from a frozen LLM to ensure\nvisual features from X-Former can be interpreted by the LLM. To demonstrate the\neffectiveness of our approach, we assess its performance on tasks demanding\ndetailed visual understanding. Extensive evaluations indicate that X-Former\nexcels in visual reasoning tasks involving both structural and semantic\ncategories in the GQA dataset. Assessment on fine-grained visual perception\nbenchmark further confirms its superior capabilities in visual understanding.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"b0vuNVUozmWdl84OBPNuCO9LpAeN-JE5BQHRZUh4xoQ","pdfSize":"2140922","objectId":"0x8c146a1a9430c5830980a31ee0dfbfdf0e44e788767bbc13b2a9495029920daf","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
