{"id":"2407.17817","title":"Demystifying Verbatim Memorization in Large Language Models","authors":"Jing Huang, Diyi Yang, Christopher Potts","authorsParsed":[["Huang","Jing",""],["Yang","Diyi",""],["Potts","Christopher",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 07:10:31 GMT"}],"updateDate":"2024-07-26","timestamp":1721891431000,"abstract":"  Large Language Models (LLMs) frequently memorize long sequences verbatim,\noften with serious legal and privacy implications. Much prior work has studied\nsuch verbatim memorization using observational data. To complement such work,\nwe develop a framework to study verbatim memorization in a controlled setting\nby continuing pre-training from Pythia checkpoints with injected sequences. We\nfind that (1) non-trivial amounts of repetition are necessary for verbatim\nmemorization to happen; (2) later (and presumably better) checkpoints are more\nlikely to verbatim memorize sequences, even for out-of-distribution sequences;\n(3) the generation of memorized sequences is triggered by distributed model\nstates that encode high-level features and makes important use of general\nlanguage modeling capabilities. Guided by these insights, we develop stress\ntests to evaluate unlearning methods and find they often fail to remove the\nverbatim memorized information, while also degrading the LM. Overall, these\nfindings challenge the hypothesis that verbatim memorization stems from\nspecific model weights or mechanisms. Rather, verbatim memorization is\nintertwined with the LM's general capabilities and thus will be very difficult\nto isolate and suppress without degrading model quality.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2lTvTFrnzRsQuJjxlhw2d8kwNsQtg3HJRVUjMrMZzto","pdfSize":"3512475","objectId":"0x406a82ea37ef6b5067858e9df996f4916c5620cc60ce1ef8d86e76888aa4cd61","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
