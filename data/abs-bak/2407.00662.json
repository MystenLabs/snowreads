{"id":"2407.00662","title":"Multi-Agent Training for Pommerman: Curriculum Learning and\n  Population-based Self-Play Approach","authors":"Nhat-Minh Huynh, Hoang-Giang Cao, I-Chen Wu","authorsParsed":[["Huynh","Nhat-Minh",""],["Cao","Hoang-Giang",""],["Wu","I-Chen",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 11:14:29 GMT"}],"updateDate":"2024-07-02","timestamp":1719746069000,"abstract":"  Pommerman is a multi-agent environment that has received considerable\nattention from researchers in recent years. This environment is an ideal\nbenchmark for multi-agent training, providing a battleground for two teams with\ncommunication capabilities among allied agents. Pommerman presents significant\nchallenges for model-free reinforcement learning due to delayed action effects,\nsparse rewards, and false positives, where opponent players can lose due to\ntheir own mistakes. This study introduces a system designed to train\nmulti-agent systems to play Pommerman using a combination of curriculum\nlearning and population-based self-play. We also tackle two challenging\nproblems when deploying the multi-agent training system for competitive games:\nsparse reward and suitable matchmaking mechanism. Specifically, we propose an\nadaptive annealing factor based on agents' performance to adjust the dense\nexploration reward during training dynamically. Additionally, we implement a\nmatchmaking mechanism utilizing the Elo rating system to pair agents\neffectively. Our experimental results demonstrate that our trained agent can\noutperform top learning agents without requiring communication among allied\nagents.\n","subjects":["Computing Research Repository/Multiagent Systems","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Zd2Emsm7ue8DjT5JsbimzXcWZ2qjGQ2lq8WLavR8lwA","pdfSize":"1345810","objectId":"0x8bf7e2c30d0afc678d898202794961f84d9a79e1cdfcdcc557f3986f57e66aa1","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
