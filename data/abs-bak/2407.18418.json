{"id":"2407.18418","title":"Know Your Limits: A Survey of Abstention in Large Language Models","authors":"Bingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu, Yulia Tsvetkov,\n  Bill Howe, Lucy Lu Wang","authorsParsed":[["Wen","Bingbing",""],["Yao","Jihan",""],["Feng","Shangbin",""],["Xu","Chenjun",""],["Tsvetkov","Yulia",""],["Howe","Bill",""],["Wang","Lucy Lu",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 22:31:50 GMT"},{"version":"v2","created":"Thu, 8 Aug 2024 17:39:47 GMT"}],"updateDate":"2024-08-09","timestamp":1721946710000,"abstract":"  Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future work, centered\naround whether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, while still providing opportunities to optimize\nabstention abilities based on context.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8XCKmMFSy1K_ff6tA6Brk6yaf1p1EtcOuSXQYXtxbv8","pdfSize":"715826","objectId":"0xdf125c0087d240ded2139a3d19afa749601b2d2a5e46f38d3e30f00c4c3b8f32","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
