{"id":"2407.01394","title":"Gloss2Text: Sign Language Gloss translation using LLMs and Semantically\n  Aware Label Smoothing","authors":"Pooya Fayyazsanavi, Antonios Anastasopoulos, Jana Ko\\v{s}eck\\'a","authorsParsed":[["Fayyazsanavi","Pooya",""],["Anastasopoulos","Antonios",""],["Košecká","Jana",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 15:46:45 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 14:44:33 GMT"}],"updateDate":"2024-07-15","timestamp":1719848805000,"abstract":"  Sign language translation from video to spoken text presents unique\nchallenges owing to the distinct grammar, expression nuances, and high\nvariation of visual appearance across different speakers and contexts. The\nintermediate gloss annotations of videos aim to guide the translation process.\nIn our work, we focus on {\\em Gloss2Text} translation stage and propose several\nadvances by leveraging pre-trained large language models (LLMs), data\naugmentation, and novel label-smoothing loss function exploiting gloss\ntranslation ambiguities improving significantly the performance of\nstate-of-the-art approaches. Through extensive experiments and ablation studies\non the PHOENIX Weather 2014T dataset, our approach surpasses state-of-the-art\nperformance in {\\em Gloss2Text} translation, indicating its efficacy in\naddressing sign language translation and suggesting promising avenues for\nfuture research and development.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"l4-MYq2a5zyVZZd6hhSv0zJ-VS3ohQKUMV7WZAFy9CU","pdfSize":"797283","objectId":"0xb239d970016276752ad1d2214d9276aa3483188effe17796de176f6d48e7a43b","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
