{"id":"2407.03909","title":"Wide stable neural networks: Sample regularity, functional convergence\n  and Bayesian inverse problems","authors":"Tom\\'as Soto","authorsParsed":[["Soto","Tom√°s",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 13:07:09 GMT"}],"updateDate":"2024-07-08","timestamp":1720098429000,"abstract":"  We study the large-width asymptotics of random fully connected neural\nnetworks with weights drawn from $\\alpha$-stable distributions, a family of\nheavy-tailed distributions arising as the limiting distributions in the\nGnedenko-Kolmogorov heavy-tailed central limit theorem. We show that in an\narbitrary bounded Euclidean domain $\\mathcal{U}$ with smooth boundary, the\nrandom field at the infinite-width limit, characterized in previous literature\nin terms of finite-dimensional distributions, has sample functions in the\nfractional Sobolev-Slobodeckij-type quasi-Banach function space\n$W^{s,p}(\\mathcal{U})$ for integrability indices $p < \\alpha$ and suitable\nsmoothness indices $s$ depending on the activation function of the neural\nnetwork, and establish the functional convergence of the processes in\n$\\mathcal{P}(W^{s,p}(\\mathcal{U}))$. This convergence result is leveraged in\nthe study of functional posteriors for edge-preserving Bayesian inverse\nproblems with stable neural network priors.\n","subjects":["Mathematics/Statistics Theory","Mathematics/Probability","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"VlMXigB8RXPN1pMPYGl1ea6XcnAWm01siEOh73H9ABM","pdfSize":"967652"}
