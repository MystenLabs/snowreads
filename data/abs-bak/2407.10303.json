{"id":"2407.10303","title":"Improving Neural Biasing for Contextual Speech Recognition by Early\n  Context Injection and Text Perturbation","authors":"Ruizhe Huang, Mahsa Yarmohammadi, Sanjeev Khudanpur, Daniel Povey","authorsParsed":[["Huang","Ruizhe",""],["Yarmohammadi","Mahsa",""],["Khudanpur","Sanjeev",""],["Povey","Daniel",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 19:32:33 GMT"}],"updateDate":"2024-07-16","timestamp":1720985553000,"abstract":"  Existing research suggests that automatic speech recognition (ASR) models can\nbenefit from additional contexts (e.g., contact lists, user specified\nvocabulary). Rare words and named entities can be better recognized with\ncontexts. In this work, we propose two simple yet effective techniques to\nimprove context-aware ASR models. First, we inject contexts into the encoders\nat an early stage instead of merely at their last layers. Second, to enforce\nthe model to leverage the contexts during training, we perturb the reference\ntranscription with alternative spellings so that the model learns to rely on\nthe contexts to make correct predictions. On LibriSpeech, our techniques\ntogether reduce the rare word error rate by 60% and 25% relatively compared to\nno biasing and shallow fusion, making the new state-of-the-art performance. On\nSPGISpeech and a real-world dataset ConEC, our techniques also yield good\nimprovements over the baselines.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"QI1T9WxfRPo5ZBeubyWbj0IANxEU4UHFFdYiti61hXA","pdfSize":"265886","objectId":"0x8ee7ae17c99a951ddca26d8f2fda803efdc8043db4f5b802d075ac9d702b035b","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
