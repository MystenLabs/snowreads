{"id":"2407.07860","title":"Controlling Space and Time with Diffusion Models","authors":"Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, David J.\n  Fleet","authorsParsed":[["Watson","Daniel",""],["Saxena","Saurabh",""],["Li","Lala",""],["Tagliasacchi","Andrea",""],["Fleet","David J.",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 17:23:33 GMT"}],"updateDate":"2024-07-11","timestamp":1720632213000,"abstract":"  We present 4DiM, a cascaded diffusion model for 4D novel view synthesis\n(NVS), conditioned on one or more images of a general scene, and a set of\ncamera poses and timestamps. To overcome challenges due to limited availability\nof 4D training data, we advocate joint training on 3D (with camera pose), 4D\n(pose+time) and video (time but no pose) data and propose a new architecture\nthat enables the same. We further advocate the calibration of SfM posed data\nusing monocular metric depth estimators for metric scale camera control. For\nmodel evaluation, we introduce new metrics to enrich and overcome shortcomings\nof current evaluation schemes, demonstrating state-of-the-art results in both\nfidelity and pose control compared to existing diffusion models for 3D NVS,\nwhile at the same time adding the ability to handle temporal dynamics. 4DiM is\nalso used for improved panorama stitching, pose-conditioned video to video\ntranslation, and several other tasks. For an overview see\nhttps://4d-diffusion.github.io\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"O1QZqnzwZK_m7wAPNz3Ez_U9w7aylsTBK53uhK6bikA","pdfSize":"38698851","objectId":"0x93546aad35151b2b893c4822c294bcbcba2e640c94ff600e2db643e355f4304f","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
