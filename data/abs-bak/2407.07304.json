{"id":"2407.07304","title":"Inference Performance Optimization for Large Language Models on CPUs","authors":"Pujiang He and Shan Zhou and Wenhuan Huang and Changqing Li and Duyi\n  Wang and Bin Guo and Chen Meng and Sheng Gui and Weifei Yu and Yi Xie","authorsParsed":[["He","Pujiang",""],["Zhou","Shan",""],["Huang","Wenhuan",""],["Li","Changqing",""],["Wang","Duyi",""],["Guo","Bin",""],["Meng","Chen",""],["Gui","Sheng",""],["Yu","Weifei",""],["Xie","Yi",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 01:53:49 GMT"}],"updateDate":"2024-07-11","timestamp":1720576429000,"abstract":"  Large language models (LLMs) have shown exceptional performance and vast\npotential across diverse tasks. However, the deployment of LLMs with high\nperformance in low-resource environments has garnered significant attention in\nthe industry. When GPU hardware resources are limited, we can explore\nalternative options on CPUs. To mitigate the financial burden and alleviate\nconstraints imposed by hardware resources, optimizing inference performance is\nnecessary. In this paper, we introduce an easily deployable inference\nperformance optimization solution aimed at accelerating LLMs on CPUs. In this\nsolution, we implement an effective way to reduce the KV cache size while\nensuring precision. We propose a distributed inference optimization approach\nand implement it based on oneAPI Collective Communications Library.\nFurthermore, we propose optimization approaches for LLMs on CPU, and conduct\ntailored optimizations for the most commonly used models. The code is\nopen-sourced at https://github.com/intel/xFasterTransformer.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d-qM4BLDxoiSDq6dzQpBjPL0IOY-rlx9ywHFMOnNu4w","pdfSize":"311909"}
