{"id":"2407.18750","title":"FLUE: Federated Learning with Un-Encrypted model weights","authors":"Elie Atallah","authorsParsed":[["Atallah","Elie",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 14:04:57 GMT"}],"updateDate":"2024-07-29","timestamp":1722002697000,"abstract":"  Federated Learning enables diverse devices to collaboratively train a shared\nmodel while keeping training data locally stored, avoiding the need for\ncentralized cloud storage. Despite existing privacy measures, concerns arise\nfrom potential reverse engineering of gradients, even with added noise,\nrevealing private data. To address this, recent research emphasizes using\nencrypted model parameters during training. This paper introduces a novel\nfederated learning algorithm, leveraging coded local gradients without\nencryption, exchanging coded proxies for model parameters, and injecting\nsurplus noise for enhanced privacy. Two algorithm variants are presented,\nshowcasing convergence and learning rates adaptable to coding schemes and raw\ndata characteristics. Two encryption-free implementations with fixed and random\ncoding matrices are provided, demonstrating promising simulation results from\nboth federated optimization and machine learning perspectives.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7QrXAqw_A4V3XwkWzbPpje2JdTEiWk3_k9uLnbeLl2A","pdfSize":"1117735","objectId":"0xf826127afc95b4bc325a44031158004e6b90e85f8cac334b0f74c0f6bd44fa98","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
