{"id":"2407.20196","title":"The generator gradient estimator is an adjoint state method for\n  stochastic differential equations","authors":"Quentin Badolle, Ankit Gupta, Mustafa Khammash","authorsParsed":[["Badolle","Quentin",""],["Gupta","Ankit",""],["Khammash","Mustafa",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 17:21:51 GMT"}],"updateDate":"2024-07-30","timestamp":1722273711000,"abstract":"  Motivated by the increasing popularity of overparameterized Stochastic\nDifferential Equations (SDEs) like Neural SDEs, Wang, Blanchet and Glynn\nrecently introduced the generator gradient estimator, a novel unbiased\nstochastic gradient estimator for SDEs whose computation time remains stable in\nthe number of parameters. In this note, we demonstrate that this estimator is\nin fact an adjoint state method, an approach which is known to scale with the\nnumber of states and not the number of parameters in the case of Ordinary\nDifferential Equations (ODEs). In addition, we show that the generator gradient\nestimator is a close analogue to the exact Integral Path Algorithm (eIPA)\nestimator which was introduced by Gupta, Rathinam and Khammash for a class of\nContinuous-Time Markov Chains (CTMCs) known as stochastic chemical reactions\nnetworks (CRNs).\n","subjects":["Mathematics/Optimization and Control","Mathematics/Probability","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YqsJGaBLVni9n4jsaCZ55AahNIZryeaUqz0D7A8SZ3s","pdfSize":"100248","objectId":"0x13375f73a27b35ffcf7d8d3f71560d69dfc0f5569e30846b0d2e2236b3232e74","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
