{"id":"2407.03456","title":"XferBench: a Data-Driven Benchmark for Emergent Language","authors":"Brendon Boldt, David Mortensen","authorsParsed":[["Boldt","Brendon",""],["Mortensen","David",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 19:02:26 GMT"}],"updateDate":"2024-07-08","timestamp":1720033346000,"abstract":"  In this paper, we introduce a benchmark for evaluating the overall quality of\nemergent languages using data-driven methods. Specifically, we interpret the\nnotion of the \"quality\" of an emergent language as its similarity to human\nlanguage within a deep learning framework. We measure this by using the\nemergent language as pretraining data for a downstream NLP tasks in human\nlanguage -- the better the downstream performance, the better the emergent\nlanguage. We implement this benchmark as an easy-to-use Python package that\nonly requires a text file of utterances from the emergent language to be\nevaluated. Finally, we empirically test the benchmark's validity using human,\nsynthetic, and emergent language baselines.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pHqBRNoKT0m6OqJgntP9iX4z7RMaNy412zCJmVY6dnk","pdfSize":"402964"}
