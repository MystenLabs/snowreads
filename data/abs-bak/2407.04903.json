{"id":"2407.04903","title":"MMSci: A Multimodal Multi-Discipline Dataset for PhD-Level Scientific\n  Comprehension","authors":"Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung\n  Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, Linda Ruth Petzold,\n  Stephen D. Wilson, Woosang Lim, William Yang Wang","authorsParsed":[["Li","Zekun",""],["Yang","Xianjun",""],["Choi","Kyuri",""],["Zhu","Wanrong",""],["Hsieh","Ryan",""],["Kim","HyeonJung",""],["Lim","Jin Hyuk",""],["Ji","Sungyoung",""],["Lee","Byungju",""],["Yan","Xifeng",""],["Petzold","Linda Ruth",""],["Wilson","Stephen D.",""],["Lim","Woosang",""],["Wang","William Yang",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 00:40:53 GMT"}],"updateDate":"2024-07-09","timestamp":1720226453000,"abstract":"  The rapid advancement of Large Language Models (LLMs) and Large Multimodal\nModels (LMMs) has heightened the demand for AI-based scientific assistants\ncapable of understanding scientific articles and figures. Despite progress,\nthere remains a significant gap in evaluating models' comprehension of\nprofessional, graduate-level, and even PhD-level scientific content. Current\ndatasets and benchmarks primarily focus on relatively simple scientific tasks\nand figures, lacking comprehensive assessments across diverse advanced\nscientific disciplines. To bridge this gap, we collected a multimodal,\nmultidisciplinary dataset from open-access scientific articles published in\nNature Communications journals. This dataset spans 72 scientific disciplines,\nensuring both diversity and quality. We created benchmarks with various tasks\nand settings to comprehensively evaluate LMMs' capabilities in understanding\nscientific figures and content. Our evaluation revealed that these tasks are\nhighly challenging: many open-source models struggled significantly, and even\nGPT-4V and GPT-4o faced difficulties. We also explored using our dataset as\ntraining resources by constructing visual instruction-following data, enabling\nthe 7B LLaVA model to achieve performance comparable to GPT-4V/o on our\nbenchmark. Additionally, we investigated the use of our interleaved article\ntexts and figure images for pre-training LMMs, resulting in improvements on the\nmaterial generation task. The source dataset, including articles, figures,\nconstructed benchmarks, and visual instruction-following data, is open-sourced.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"byHA2KkqrEpzWgvclu9agzuF0G1CRxj43lcvo92-gJk","pdfSize":"3612105"}
