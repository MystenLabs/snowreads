{"id":"2407.15798","title":"Robust Facial Reactions Generation: An Emotion-Aware Framework with\n  Modality Compensation","authors":"Guanyu Hu and Jie Wei and Siyang Song and Dimitrios Kollias and Xinyu\n  Yang and Zhonglin Sun and Odysseus Kaloidas","authorsParsed":[["Hu","Guanyu",""],["Wei","Jie",""],["Song","Siyang",""],["Kollias","Dimitrios",""],["Yang","Xinyu",""],["Sun","Zhonglin",""],["Kaloidas","Odysseus",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 17:00:02 GMT"},{"version":"v2","created":"Tue, 23 Jul 2024 23:49:26 GMT"}],"updateDate":"2024-07-25","timestamp":1721667602000,"abstract":"  The objective of the Multiple Appropriate Facial Reaction Generation (MAFRG)\ntask is to produce contextually appropriate and diverse listener facial\nbehavioural responses based on the multimodal behavioural data of the\nconversational partner (i.e., the speaker). Current methodologies typically\nassume continuous availability of speech and facial modality data, neglecting\nreal-world scenarios where these data may be intermittently unavailable, which\noften results in model failures. Furthermore, despite utilising advanced deep\nlearning models to extract information from the speaker's multimodal inputs,\nthese models fail to adequately leverage the speaker's emotional context, which\nis vital for eliciting appropriate facial reactions from human listeners. To\naddress these limitations, we propose an Emotion-aware Modality Compensatory\n(EMC) framework. This versatile solution can be seamlessly integrated into\nexisting models, thereby preserving their advantages while significantly\nenhancing performance and robustness in scenarios with missing modalities. Our\nframework ensures resilience when faced with missing modality data through the\nCompensatory Modality Alignment (CMA) module. It also generates more\nappropriate emotion-aware reactions via the Emotion-aware Attention (EA)\nmodule, which incorporates the speaker's emotional information throughout the\nentire encoding and decoding process. Experimental results demonstrate that our\nframework improves the appropriateness metric FRCorr by an average of 57.2\\%\ncompared to the original model structure. In scenarios where speech modality\ndata is missing, the performance of appropriate generation shows an\nimprovement, and when facial data is missing, it only exhibits minimal\ndegradation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"WBzW_q7p5bMAJ1c8r_sa4ZBKNtVbAz0v7h8WWy7aLoM","pdfSize":"881700","objectId":"0xc9831e402db892a7a1789115cbbdd7ed64291eb0ea2607b41d944d9aba6be301","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
