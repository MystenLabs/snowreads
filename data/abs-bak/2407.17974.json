{"id":"2407.17974","title":"What does Kiki look like? Cross-modal associations between speech sounds\n  and visual shapes in vision-and-language models","authors":"Tessa Verhoef, Kiana Shahrasbi, Tom Kouwenhoven","authorsParsed":[["Verhoef","Tessa",""],["Shahrasbi","Kiana",""],["Kouwenhoven","Tom",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 12:09:41 GMT"}],"updateDate":"2024-07-26","timestamp":1721909381000,"abstract":"  Humans have clear cross-modal preferences when matching certain novel words\nto visual shapes. Evidence suggests that these preferences play a prominent\nrole in our linguistic processing, language learning, and the origins of\nsignal-meaning mappings. With the rise of multimodal models in AI, such as\nvision- and-language (VLM) models, it becomes increasingly important to uncover\nthe kinds of visio-linguistic associations these models encode and whether they\nalign with human representations. Informed by experiments with humans, we probe\nand compare four VLMs for a well-known human cross-modal preference, the\nbouba-kiki effect. We do not find conclusive evidence for this effect but\nsuggest that results may depend on features of the models, such as architecture\ndesign, model size, and training details. Our findings inform discussions on\nthe origins of the bouba-kiki effect in human cognition and future developments\nof VLMs that align well with human cross-modal associations.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9dAPlaSewZAj-d_mQg5N3YWghADuEfMTpZE7Ekcgs2k","pdfSize":"1611933","objectId":"0x6ef0540662bfe1b0ba33c7cd6c84ffed093f66f1b083a08fd921ad0b1853c06d","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
