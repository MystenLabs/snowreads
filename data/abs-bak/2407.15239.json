{"id":"2407.15239","title":"Assessing Brittleness of Image-Text Retrieval Benchmarks from\n  Vision-Language Models Perspective","authors":"Mariya Hendriksen, Shuo Zhang, Ridho Reinanda, Mohamed Yahya, Edgar\n  Meij, and Maarten de Rijke","authorsParsed":[["Hendriksen","Mariya",""],["Zhang","Shuo",""],["Reinanda","Ridho",""],["Yahya","Mohamed",""],["Meij","Edgar",""],["de Rijke","Maarten",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 18:08:44 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 19:52:38 GMT"}],"updateDate":"2024-07-29","timestamp":1721585324000,"abstract":"  Image-text retrieval (ITR), an important task in information retrieval (IR),\nis driven by pretrained vision-language models (VLMs) that consistently achieve\nstate-of-the-art performance. However, a significant challenge lies in the\nbrittleness of existing ITR benchmarks. In standard datasets for the task,\ncaptions often provide broad summaries of scenes, neglecting detailed\ninformation about specific concepts. Additionally, the current evaluation setup\nassumes simplistic binary matches between images and texts and focuses on\nintra-modality rather than cross-modal relationships, which can lead to\nmisinterpretations of model performance. Motivated by this gap, in this study,\nwe focus on examining the brittleness of the ITR evaluation pipeline with a\nfocus on concept granularity. We start by analyzing two common benchmarks,\nMS-COCO and Flickr30k, and compare them with their augmented versions,\nMS-COCO-FG and Flickr30k-FG, given a specified set of linguistic features\ncapturing concept granularity. We discover that Flickr30k-FG and MS COCO-FG\nconsistently achieve higher scores across all the selected features. To\ninvestigate the performance of VLMs on coarse and fine-grained datasets, we\nintroduce a taxonomy of perturbations. We apply these perturbations to the\nselected datasets. We evaluate four state-of-the-art models - ALIGN, AltCLIP,\nCLIP, and GroupViT - on the standard and fine-grained datasets under zero-shot\nconditions, with and without the applied perturbations. The results demonstrate\nthat although perturbations generally degrade model performance, the\nfine-grained datasets exhibit a smaller performance drop than their standard\ncounterparts. Moreover, the relative performance drop across all setups is\nconsistent across all models and datasets, indicating that the issue lies\nwithin the benchmarks. We conclude the paper by providing an agenda for\nimproving ITR evaluation pipelines.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Xf3c6NroqzRrMTpFd9Fa38FRk9iQtHE4nR1TL939CAE","pdfSize":"2291043","objectId":"0x1477f7fb10f0df5583acbb78ea379440375e31dba159fc81de46ad54f154b617","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
