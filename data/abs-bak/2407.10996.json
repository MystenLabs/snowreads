{"id":"2407.10996","title":"Visualization Literacy of Multimodal Large Language Models: A\n  Comparative Study","authors":"Zhimin Li, Haichao Miao, Valerio Pascucci, Shusen Liu","authorsParsed":[["Li","Zhimin",""],["Miao","Haichao",""],["Pascucci","Valerio",""],["Liu","Shusen",""]],"versions":[{"version":"v1","created":"Mon, 24 Jun 2024 17:52:16 GMT"}],"updateDate":"2024-07-17","timestamp":1719251536000,"abstract":"  The recent introduction of multimodal large language models (MLLMs) combine\nthe inherent power of large language models (LLMs) with the renewed\ncapabilities to reason about the multimodal context. The potential usage\nscenarios for MLLMs significantly outpace their text-only counterparts. Many\nrecent works in visualization have demonstrated MLLMs' capability to understand\nand interpret visualization results and explain the content of the\nvisualization to users in natural language. In the machine learning community,\nthe general vision capabilities of MLLMs have been evaluated and tested through\nvarious visual understanding benchmarks. However, the ability of MLLMs to\naccomplish specific visualization tasks based on visual perception has not been\nproperly explored and evaluated, particularly, from a visualization-centric\nperspective.\n  In this work, we aim to fill the gap by utilizing the concept of\nvisualization literacy to evaluate MLLMs. We assess MLLMs' performance over two\npopular visualization literacy evaluation datasets (VLAT and mini-VLAT). Under\nthe framework of visualization literacy, we develop a general setup to compare\ndifferent multimodal large language models (e.g., GPT4-o, Claude 3 Opus, Gemini\n1.5 Pro) as well as against existing human baselines. Our study demonstrates\nMLLMs' competitive performance in visualization literacy, where they outperform\nhumans in certain tasks such as identifying correlations, clusters, and\nhierarchical structures.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"M6evGDH1xh9iZLiDf7bP6-h4-W2zmuILSTbHNgKzS6A","pdfSize":"709124","objectId":"0xfb0b4124a53d6d7b4452d919068dbc85e90a2c17db604214015093f3aa33ec86","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
