{"id":"2407.02238","title":"MIREncoder: Multi-modal IR-based Pretrained Embeddings for Performance\n  Optimizations","authors":"Akash Dutta, Ali Jannesari","authorsParsed":[["Dutta","Akash",""],["Jannesari","Ali",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 13:00:19 GMT"}],"updateDate":"2024-07-03","timestamp":1719925219000,"abstract":"  One of the primary areas of interest in High Performance Computing is the\nimprovement of performance of parallel workloads. Nowadays, compilable source\ncode-based optimization tasks that employ deep learning often exploit LLVM\nIntermediate Representations (IRs) for extracting features from source code.\nMost such works target specific tasks, or are designed with a pre-defined set\nof heuristics. So far, pre-trained models are rare in this domain, but the\npossibilities have been widely discussed. Especially approaches mimicking\nlarge-language models (LLMs) have been proposed. But these have prohibitively\nlarge training costs. In this paper, we propose MIREncoder, a M}ulti-modal\nIR-based Auto-Encoder that can be pre-trained to generate a learned embedding\nspace to be used for downstream tasks by machine learning-based approaches. A\nmulti-modal approach enables us to better extract features from compilable\nprograms. It allows us to better model code syntax, semantics and structure.\nFor code-based performance optimizations, these features are very important\nwhile making optimization decisions. A pre-trained model/embedding implicitly\nenables the usage of transfer learning, and helps move away from task-specific\ntrained models. Additionally, a pre-trained model used for downstream\nperformance optimization should itself have reduced overhead, and be easily\nusable. These considerations have led us to propose a modeling approach that i)\nunderstands code semantics and structure, ii) enables use of transfer learning,\nand iii) is small and simple enough to be easily re-purposed or reused even\nwith low resource availability. Our evaluations will show that our proposed\napproach can outperform the state of the art while reducing overhead.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning","Computing Research Repository/Performance"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2KDWQ65M9tuUSDKedyODi8dKpyOuUTaeNlQG0lUSGeo","pdfSize":"821923","objectId":"0xf0a5c1a0b68ed072cbe632045e2e099e3833a031f92f73226d60eabf147a1026","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
