{"id":"2407.13466","title":"LIMT: Language-Informed Multi-Task Visual World Models","authors":"Elie Aljalbout, Nikolaos Sotirakis, Patrick van der Smagt, Maximilian\n  Karl, Nutan Chen","authorsParsed":[["Aljalbout","Elie",""],["Sotirakis","Nikolaos",""],["van der Smagt","Patrick",""],["Karl","Maximilian",""],["Chen","Nutan",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 12:40:58 GMT"}],"updateDate":"2024-07-19","timestamp":1721306458000,"abstract":"  Most recent successes in robot reinforcement learning involve learning a\nspecialized single-task agent.\n  However, robots capable of performing multiple tasks can be much more\nvaluable in real-world applications.\n  Multi-task reinforcement learning can be very challenging due to the\nincreased sample complexity and the potentially conflicting task objectives.\n  Previous work on this topic is dominated by model-free approaches.\n  The latter can be very sample inefficient even when learning specialized\nsingle-task agents.\n  In this work, we focus on model-based multi-task reinforcement learning.\n  We propose a method for learning multi-task visual world models, leveraging\npre-trained language models to extract semantically meaningful task\nrepresentations.\n  These representations are used by the world model and policy to reason about\ntask similarity in dynamics and behavior.\n  Our results highlight the benefits of using language-driven task\nrepresentations for world models and a clear advantage of model-based\nmulti-task learning over the more common model-free paradigm.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ucCItMFNtYT2CPVHy_5mhoQp4lN8BpUFFtgs6aka87c","pdfSize":"1371064","objectId":"0x0407ca6155d1101684ef37dfc49bdbf69409b7e05151595c61957342d10e326c","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
