{"id":"2407.20962","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","authors":"Xiaowei Chi, Yatian Wang, Aosong Cheng, Pengjun Fang, Zeyue Tian,\n  Yingqing He, Zhaoyang Liu, Xingqun Qi, Jiahao Pan, Rongyu Zhang, Mengfei Li,\n  Ruibin Yuan, Yanbing Jiang, Wei Xue, Wenhan Luo, Qifeng Chen, Shanghang\n  Zhang, Qifeng Liu, Yike Guo","authorsParsed":[["Chi","Xiaowei",""],["Wang","Yatian",""],["Cheng","Aosong",""],["Fang","Pengjun",""],["Tian","Zeyue",""],["He","Yingqing",""],["Liu","Zhaoyang",""],["Qi","Xingqun",""],["Pan","Jiahao",""],["Zhang","Rongyu",""],["Li","Mengfei",""],["Yuan","Ruibin",""],["Jiang","Yanbing",""],["Xue","Wei",""],["Luo","Wenhan",""],["Chen","Qifeng",""],["Zhang","Shanghang",""],["Liu","Qifeng",""],["Guo","Yike",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 16:43:24 GMT"},{"version":"v2","created":"Tue, 6 Aug 2024 12:25:48 GMT"}],"updateDate":"2024-08-07","timestamp":1722357804000,"abstract":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Multimedia","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"vwqzisjTIlOInYsud-nE2Yr7EmffoJ2X66iKeWEbF7M","pdfSize":"16096851","objectId":"0x6df9868e20d023500b3d81b6c512d2a3a8cc40a74297eb696f4031df4ece5a50","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
