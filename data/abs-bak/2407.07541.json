{"id":"2407.07541","title":"Swiss DINO: Efficient and Versatile Vision Framework for On-device\n  Personal Object Search","authors":"Kirill Paramonov, Jia-Xing Zhong, Umberto Michieli, Jijoong Moon, Mete\n  Ozay","authorsParsed":[["Paramonov","Kirill",""],["Zhong","Jia-Xing",""],["Michieli","Umberto",""],["Moon","Jijoong",""],["Ozay","Mete",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 11:05:02 GMT"}],"updateDate":"2024-07-11","timestamp":1720609502000,"abstract":"  In this paper, we address a recent trend in robotic home appliances to\ninclude vision systems on personal devices, capable of personalizing the\nappliances on the fly. In particular, we formulate and address an important\ntechnical task of personal object search, which involves localization and\nidentification of personal items of interest on images captured by robotic\nappliances, with each item referenced only by a few annotated images. The task\nis crucial for robotic home appliances and mobile systems, which need to\nprocess personal visual scenes or to operate with particular personal objects\n(e.g., for grasping or navigation). In practice, personal object search\npresents two main technical challenges. First, a robot vision system needs to\nbe able to distinguish between many fine-grained classes, in the presence of\nocclusions and clutter. Second, the strict resource requirements for the\non-device system restrict the usage of most state-of-the-art methods for\nfew-shot learning and often prevent on-device adaptation. In this work, we\npropose Swiss DINO: a simple yet effective framework for one-shot personal\nobject search based on the recent DINOv2 transformer model, which was shown to\nhave strong zero-shot generalization properties. Swiss DINO handles challenging\non-device personalized scene understanding requirements and does not require\nany adaptation training. We show significant improvement (up to 55%) in\nsegmentation and recognition accuracy compared to the common lightweight\nsolutions, and significant footprint reduction of backbone inference time (up\nto 100x) and GPU consumption (up to 10x) compared to the heavy\ntransformer-based solutions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"qlAgHAVyK-ZBHYVsajq0k2ymLsfaRjkyWI_Fwxtk3T8","pdfSize":"1169176","objectId":"0xd728eeb62856d10587cacf74d2588b81efa941a184eafea1b31f0b5c6ae483d8","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
