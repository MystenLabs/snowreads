{"id":"2407.02754","title":"Is Cross-Validation the Gold Standard to Evaluate Model Performance?","authors":"Garud Iyengar, Henry Lam, Tianyu Wang","authorsParsed":[["Iyengar","Garud",""],["Lam","Henry",""],["Wang","Tianyu",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 02:10:03 GMT"},{"version":"v2","created":"Tue, 20 Aug 2024 20:44:47 GMT"}],"updateDate":"2024-08-22","timestamp":1719972603000,"abstract":"  Cross-Validation (CV) is the default choice for evaluating the performance of\nmachine learning models. Despite its wide usage, their statistical benefits\nhave remained half-understood, especially in challenging nonparametric regimes.\nIn this paper we fill in this gap and show that in fact, for a wide spectrum of\nmodels, CV does not statistically outperform the simple \"plug-in\" approach\nwhere one reuses training data for testing evaluation. Specifically, in terms\nof both the asymptotic bias and coverage accuracy of the associated interval\nfor out-of-sample evaluation, $K$-fold CV provably cannot outperform plug-in\nregardless of the rate at which the parametric or nonparametric models\nconverge. Leave-one-out CV can have a smaller bias as compared to plug-in;\nhowever, this bias improvement is negligible compared to the variability of the\nevaluation, and in some important cases leave-one-out again does not outperform\nplug-in once this variability is taken into account. We obtain our theoretical\ncomparisons via a novel higher-order Taylor analysis that allows us to derive\nnecessary conditions for limit theorems of testing evaluations, which applies\nto model classes that are not amenable to previously known sufficient\nconditions. Our numerical results demonstrate that plug-in performs indeed no\nworse than CV across a wide range of examples.\n","subjects":["Mathematics/Statistics Theory","Statistics/Methodology","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1ZUIzbHxvSuluvhAkGPlLcRtLxO_Ij_syq7oGU-qvrs","pdfSize":"1314033","objectId":"0x9adf68c4b1a5628987bb11d8979b5fcb9cae24cc51a458ec910c6ad31a4700ec","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
