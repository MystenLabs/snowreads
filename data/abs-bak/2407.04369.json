{"id":"2407.04369","title":"ZARRIO @ Ego4D Short Term Object Interaction Anticipation Challenge:\n  Leveraging Affordances and Attention-based models for STA","authors":"Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Josechu Guerrero-Campo and\n  Giovanni Maria Farinella","authorsParsed":[["Mur-Labadia","Lorenzo",""],["Martinez-Cantin","Ruben",""],["Guerrero-Campo","Josechu",""],["Farinella","Giovanni Maria",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 09:16:30 GMT"}],"updateDate":"2024-07-08","timestamp":1720170990000,"abstract":"  Short-Term object-interaction Anticipation (STA) consists of detecting the\nlocation of the next-active objects, the noun and verb categories of the\ninteraction, and the time to contact from the observation of egocentric video.\nWe propose STAformer, a novel attention-based architecture integrating\nframe-guided temporal pooling, dual image-video attention, and multi-scale\nfeature fusion to support STA predictions from an image-input video pair.\nMoreover, we introduce two novel modules to ground STA predictions on human\nbehavior by modeling affordances. First, we integrate an environment affordance\nmodel which acts as a persistent memory of interactions that can take place in\na given physical scene. Second, we predict interaction hotspots from the\nobservation of hands and object trajectories, increasing confidence in STA\npredictions localized around the hotspot. On the test set, our results obtain a\nfinal 33.5 N mAP, 17.25 N+V mAP, 11.77 N+{\\delta} mAP and 6.75 Overall top-5\nmAP metric when trained on the v2 training dataset.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wzxMcyFibqxFACjc-TPIpoFmuYgRG4AUDu7mfmy42hE","pdfSize":"7309222"}
