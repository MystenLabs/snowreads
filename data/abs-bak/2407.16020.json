{"id":"2407.16020","title":"Sparks of Quantum Advantage and Rapid Retraining in Machine Learning","authors":"William Troy","authorsParsed":[["Troy","William",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 19:55:44 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 16:23:55 GMT"},{"version":"v3","created":"Thu, 1 Aug 2024 17:40:36 GMT"},{"version":"v4","created":"Mon, 5 Aug 2024 02:01:12 GMT"}],"updateDate":"2024-08-06","timestamp":1721678144000,"abstract":"  The advent of quantum computing holds the potential to revolutionize various\nfields by solving complex problems more efficiently than classical computers.\nDespite this promise, practical quantum advantage is hindered by current\nhardware limitations, notably the small number of qubits and high noise levels.\nIn this study, we leverage adiabatic quantum computers to optimize\nKolmogorov-Arnold Networks, a powerful neural network architecture for\nrepresenting complex functions with minimal parameters. By modifying the\nnetwork to use Bezier curves as the basis functions and formulating the\noptimization problem into a Quadratic Unconstrained Binary Optimization\nproblem, we create a fixed-sized solution space, independent of the number of\ntraining samples. This strategy allows for the optimization of an entire neural\nnetwork in a single training iteration in which, due to order of operations, a\nmajority of the processing is done using a collapsed version of the training\ndataset. This inherently creates extremely fast training speeds, which are\nvalidated experimentally, compared to classical optimizers including Adam,\nStochastic Gradient Descent, Adaptive Gradient, and simulated annealing.\nAdditionally, we introduce a novel rapid retraining capability, enabling the\nnetwork to be retrained with new data without reprocessing old samples, thus\nenhancing learning efficiency in dynamic environments. Experiments on\nretraining demonstrate a hundred times speed up using adiabatic quantum\ncomputing based optimization compared to that of the gradient descent based\noptimizers, with theoretical models allowing this speed up to be much larger!\nOur findings suggest that with further advancements in quantum hardware and\nalgorithm optimization, quantum-optimized machine learning models could have\nbroad applications across various domains, with initial focus on rapid\nretraining.\n","subjects":["Physics/Quantum Physics","Computing Research Repository/Emerging Technologies","Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GA_uRnsmK8V4dlj94_TVOqhyKyjQdtxwIK04GjcJlww","pdfSize":"709239","objectId":"0x037d482b5884de6726d79444a050c70f0a284d1d09152e681d75b01f648f259e","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
