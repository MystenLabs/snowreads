{"id":"2407.14039","title":"BERTer: The Efficient One","authors":"Pradyumna Saligram, Andrew Lanpouthakoun","authorsParsed":[["Saligram","Pradyumna",""],["Lanpouthakoun","Andrew",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 05:33:09 GMT"}],"updateDate":"2024-07-22","timestamp":1721367189000,"abstract":"  We explore advanced fine-tuning techniques to boost BERT's performance in\nsentiment analysis, paraphrase detection, and semantic textual similarity. Our\napproach leverages SMART regularization to combat overfitting, improves\nhyperparameter choices, employs a cross-embedding Siamese architecture for\nimproved sentence embeddings, and introduces innovative early exiting methods.\nOur fine-tuning findings currently reveal substantial improvements in model\nefficiency and effectiveness when combining multiple fine-tuning architectures,\nachieving a state-of-the-art performance score of on the test set, surpassing\ncurrent benchmarks and highlighting BERT's adaptability in multifaceted\nlinguistic tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d7XDgs9dpRAcjZ6k-I8BF_LJp9VhfV7oa_TAQA62CeQ","pdfSize":"135774","objectId":"0xbb3eb07a7e92d7f0a3fe7dee564ffc0dab3ceda884061f7c3cbd3fd33f9d3c7f","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
