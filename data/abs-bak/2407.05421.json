{"id":"2407.05421","title":"ASRRL-TTS: Agile Speaker Representation Reinforcement Learning for\n  Text-to-Speech Speaker Adaptation","authors":"Ruibo Fu, Xin Qi, Zhengqi Wen, Jianhua Tao, Tao Wang, Chunyu Qiang,\n  Zhiyong Wang, Yi Lu, Xiaopeng Wang, Shuchen Shi, Yukun Liu, Xuefei Liu, Shuai\n  Zhang","authorsParsed":[["Fu","Ruibo",""],["Qi","Xin",""],["Wen","Zhengqi",""],["Tao","Jianhua",""],["Wang","Tao",""],["Qiang","Chunyu",""],["Wang","Zhiyong",""],["Lu","Yi",""],["Wang","Xiaopeng",""],["Shi","Shuchen",""],["Liu","Yukun",""],["Liu","Xuefei",""],["Zhang","Shuai",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 15:58:11 GMT"}],"updateDate":"2024-07-09","timestamp":1720367891000,"abstract":"  Speaker adaptation, which involves cloning voices from unseen speakers in the\nText-to-Speech task, has garnered significant interest due to its numerous\napplications in multi-media fields. Despite recent advancements, existing\nmethods often struggle with inadequate speaker representation accuracy and\noverfitting, particularly in limited reference speeches scenarios. To address\nthese challenges, we propose an Agile Speaker Representation Reinforcement\nLearning strategy to enhance speaker similarity in speaker adaptation tasks.\nASRRL is the first work to apply reinforcement learning to improve the modeling\naccuracy of speaker embeddings in speaker adaptation, addressing the challenge\nof decoupling voice content and timbre. Our approach introduces two action\nstrategies tailored to different reference speeches scenarios. In the\nsingle-sentence scenario, a knowledge-oriented optimal routine searching RL\nmethod is employed to expedite the exploration and retrieval of refinement\ninformation on the fringe of speaker representations. In the few-sentence\nscenario, we utilize a dynamic RL method to adaptively fuse reference speeches,\nenhancing the robustness and accuracy of speaker modeling. To achieve optimal\nresults in the target domain, a multi-scale fusion scoring mechanism based\nreward model that evaluates speaker similarity, speech quality, and\nintelligibility across three dimensions is proposed, ensuring that improvements\nin speaker similarity do not compromise speech quality or intelligibility. The\nexperimental results on the LibriTTS and VCTK datasets within mainstream TTS\nframeworks demonstrate the extensibility and generalization capabilities of the\nproposed ASRRL method. The results indicate that the ASRRL method significantly\noutperforms traditional fine-tuning approaches, achieving higher speaker\nsimilarity and better overall speech quality with limited reference speeches.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"akYIEqEBxEvh4-0CFucc_HtCNG1ktWPFsvFl9E5QRSI","pdfSize":"1427599"}
