{"id":"2407.13657","title":"FuLG: 150B Romanian Corpus for Language Model Pretraining","authors":"Vlad-Andrei B\\u{a}doiu and Mihai-Valentin Dumitru and Alexandru M.\n  Gherghescu and Alexandru Agache and Costin Raiciu","authorsParsed":[["BÄƒdoiu","Vlad-Andrei",""],["Dumitru","Mihai-Valentin",""],["Gherghescu","Alexandru M.",""],["Agache","Alexandru",""],["Raiciu","Costin",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 16:32:48 GMT"}],"updateDate":"2024-07-19","timestamp":1721320368000,"abstract":"  Research in the field of language models is rapidly evolving, with many open\nmodels being released to the public. Openly available pretraining corpora\nusually focus on only a handful of languages, with many others either missing\ncompletely or extremely underrepresented. In this report, we introduce FuLG, a\nhundred-fifty-billion-token Romanian corpus extracted from CommonCrawl. We\npresent our methodology for filtering FuLG and compare it via ablation studies\nagainst existing Romanian corpora.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3qYdtdhhXWVpHQqQondCYn5BJm0sS_ZxR1L4SQ2Wf1s","pdfSize":"111907","objectId":"0xf2db3c37676fb20f5b17fb43b422c8cde17afd3e3032f6126ea05cbb8ee3a4de","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
