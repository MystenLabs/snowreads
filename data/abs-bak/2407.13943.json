{"id":"2407.13943","title":"Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction","authors":"Suma Bailis, Jane Friedhoff, and Feiyang Chen","authorsParsed":[["Bailis","Suma",""],["Friedhoff","Jane",""],["Chen","Feiyang",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 23:41:05 GMT"}],"updateDate":"2024-07-22","timestamp":1721346065000,"abstract":"  This paper introduces Werewolf Arena, a novel framework for evaluating large\nlanguage models (LLMs) through the lens of the classic social deduction game,\nWerewolf. In Werewolf Arena, LLMs compete against each other, navigating the\ngame's complex dynamics of deception, deduction, and persuasion. The framework\nintroduces a dynamic turn-taking system based on bidding, mirroring real-world\ndiscussions where individuals strategically choose when to speak. We\ndemonstrate the framework's utility through an arena-style tournament featuring\nGemini and GPT models. Our results reveal distinct strengths and weaknesses in\nthe models' strategic reasoning and communication. These findings highlight\nWerewolf Arena's potential as a challenging and scalable LLM benchmark.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"75iXnUUm2cy9977nbF13twBloWHNrmBjS7WoonOB3UU","pdfSize":"1196224","objectId":"0xd6484ca358277350873003839c6d13af645ed1f7487f49361398579293a883fe","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
