{"id":"2407.07802","title":"ROSA: Random Subspace Adaptation for Efficient Fine-Tuning","authors":"Marawan Gamal Abdel Hameed, Aristides Milios, Siva Reddy, Guillaume\n  Rabusseau","authorsParsed":[["Hameed","Marawan Gamal Abdel",""],["Milios","Aristides",""],["Reddy","Siva",""],["Rabusseau","Guillaume",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:20:53 GMT"}],"updateDate":"2024-07-11","timestamp":1720628453000,"abstract":"  Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"N4UGggZZwIxdiJM4k0XO9C9QsLGlBBGbmQI-2qjHww0","pdfSize":"1146204","objectId":"0xf9dd222ccb2fae1c51b9eb6b797b5339fa635cb7f8a644d37428a92d6cece9d6","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
