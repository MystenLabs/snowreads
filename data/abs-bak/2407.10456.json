{"id":"2407.10456","title":"Don't Throw Away Data: Better Sequence Knowledge Distillation","authors":"Jun Wang, Eleftheria Briakou, Hamid Dadkhahi, Rishabh Agarwal, Colin\n  Cherry and Trevor Cohn","authorsParsed":[["Wang","Jun",""],["Briakou","Eleftheria",""],["Dadkhahi","Hamid",""],["Agarwal","Rishabh",""],["Cherry","Colin",""],["Cohn","Trevor",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 06:11:18 GMT"}],"updateDate":"2024-07-16","timestamp":1721023878000,"abstract":"  A critical component in knowledge distillation is the means of coupling the\nteacher and student. The predominant sequence knowledge distillation method\ninvolves supervised learning of the student against teacher-decoded outputs,\nand is exemplified by the current state of the art, which incorporates minimum\nBayes risk (MBR) decoding. In this paper we seek to integrate MBR more tightly\nin distillation training, specifically by using several high scoring MBR\ntranslations, rather than a single selected sequence, thus capturing a rich\ndiversity of teacher outputs. Our experiments on English to German and English\nto Japanese translation show consistent improvements over strong baseline\nmethods for both tasks and with varying model sizes. Additionally, we conduct a\ndetailed analysis focusing on data efficiency and capacity curse aspects to\nelucidate MBR-n and explore its further potential.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"H_w9PHYvA9x_AgVo96wkduqL4LvPM7-SmgSdwN_lNIQ","pdfSize":"366365","objectId":"0x91cc2495de20321a253ad09612c46e189d49a6aecbd192d676a0fadc784951cb","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
