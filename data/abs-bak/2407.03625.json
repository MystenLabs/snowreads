{"id":"2407.03625","title":"Augmenting LLMs to Repair Obsolete Test Cases with Static Collector and\n  Neural Reranker","authors":"Jun Liu, Jiwei Yan, Yuanyuan Xie, Jun Yan, Jian Zhang","authorsParsed":[["Liu","Jun",""],["Yan","Jiwei",""],["Xie","Yuanyuan",""],["Yan","Jun",""],["Zhang","Jian",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 04:24:43 GMT"}],"updateDate":"2024-07-08","timestamp":1720067083000,"abstract":"  During software evolution, it is advocated that test code should co-evolve\nwith production code. In real development scenarios, test updating may lag\nbehind production code changing, which may cause the project to fail to compile\nor bring other troubles. Existing techniques based on pre-trained language\nmodels can be adopted to repair obsolete tests caused by such unsynchronized\ncode changes, especially syntactic-related ones. However, the lack of\ntarget-oriented contextual information affects repair accuracy on large-scale\nprojects. Starting from an obsoleted test, the key challenging task is\nprecisely identifying and constructing Test-Repair-Oriented Contexts (TROCtx)\nfrom the whole repository within a limited token size.\n  In this paper, we propose SynBCIATR (Syntactic-Breaking-Change-Induced\nAutomated Test Repair), a novel approach to automatically repair obsolete test\ncases via precise and concise TROCtx construction. Inspired by developers'\nprogramming practices of the task, we design three types of TROCtx: class\ncontexts, usage contexts, and environment contexts. For every type of TROCtx,\nSynBCIATR automatically collects the changed-token-related code information\nthrough static analysis techniques. Then it generates reranking queries to\nidentify the most relevant TROCtxs, which will be taken as the repair-required\nkey context and be input to the Large Language Model for the final test repair.\n  To evaluate the effectiveness of SynBCIATR, we construct a benchmark dataset\nthat contains diverse syntactic breaking changes. The experimental results show\nthat SynBCIATR outperforms baseline approaches both on textual- and\nintent-matching metrics. With the augmentation of TROCtx constructed by\nSynBCIATR, hallucinations are reduced by 57.1%.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LLloqv0gHewm-cqarXOsO-AU1jRGtiPaIuiIwuj6Xjo","pdfSize":"1271856"}
