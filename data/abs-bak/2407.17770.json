{"id":"2407.17770","title":"BotEval: Facilitating Interactive Human Evaluation","authors":"Hyundong Cho, Thamme Gowda, Yuyang Huang, Zixun Lu, Tianli Tong,\n  Jonathan May","authorsParsed":[["Cho","Hyundong",""],["Gowda","Thamme",""],["Huang","Yuyang",""],["Lu","Zixun",""],["Tong","Tianli",""],["May","Jonathan",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 04:57:31 GMT"}],"updateDate":"2024-07-26","timestamp":1721883451000,"abstract":"  Following the rapid progress in natural language processing (NLP) models,\nlanguage models are applied to increasingly more complex interactive tasks such\nas negotiations and conversation moderations. Having human evaluators directly\ninteract with these NLP models is essential for adequately evaluating the\nperformance on such interactive tasks. We develop BotEval, an easily\ncustomizable, open-source, evaluation toolkit that focuses on enabling\nhuman-bot interactions as part of the evaluation process, as opposed to human\nevaluators making judgements for a static input. BotEval balances flexibility\nfor customization and user-friendliness by providing templates for common use\ncases that span various degrees of complexity and built-in compatibility with\npopular crowdsourcing platforms. We showcase the numerous useful features of\nBotEval through a study that evaluates the performance of various chatbots on\ntheir effectiveness for conversational moderation and discuss how BotEval\ndiffers from other annotation tools.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9KgHMQ_9usjBqg1eRt7a8oJjTa3kmSUYRqImsX-BTsk","pdfSize":"1646938","objectId":"0xda284c1078ed5dde053367b2619f5c85edc8b2e8524f49f75d0976b18633bbf0","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
