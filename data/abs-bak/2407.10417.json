{"id":"2407.10417","title":"Proper losses regret at least 1/2-order","authors":"Han Bao, Asuka Takatsu","authorsParsed":[["Bao","Han",""],["Takatsu","Asuka",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 03:46:15 GMT"}],"updateDate":"2024-07-16","timestamp":1721015175000,"abstract":"  A fundamental challenge in machine learning is the choice of a loss as it\ncharacterizes our learning task, is minimized in the training phase, and serves\nas an evaluation criterion for estimators. Proper losses are commonly chosen,\nensuring minimizers of the full risk match the true probability vector.\nEstimators induced from a proper loss are widely used to construct forecasters\nfor downstream tasks such as classification and ranking. In this procedure, how\ndoes the forecaster based on the obtained estimator perform well under a given\ndownstream task? This question is substantially relevant to the behavior of the\n$p$-norm between the estimated and true probability vectors when the estimator\nis updated. In the proper loss framework, the suboptimality of the estimated\nprobability vector from the true probability vector is measured by a surrogate\nregret. First, we analyze a surrogate regret and show that the strict\nproperness of a loss is necessary and sufficient to establish a non-vacuous\nsurrogate regret bound. Second, we solve an important open question that the\norder of convergence in p-norm cannot be faster than the $1/2$-order of\nsurrogate regrets for a broad class of strictly proper losses. This implies\nthat strongly proper losses entail the optimal convergence rate.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"_hWR2LuOHM3lTZtNZPg3JoUh24gSGEqe0ssYwDdbyc0","pdfSize":"1063822","objectId":"0x86f1eed6425df6bd0bc94780ae18426ca72cb3105527fef1135997d838a65b5b","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
