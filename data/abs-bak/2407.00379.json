{"id":"2407.00379","title":"GraphArena: Benchmarking Large Language Models on Graph Computational\n  Problems","authors":"Jianheng Tang, Qifan Zhang, Yuhan Li, Jia Li","authorsParsed":[["Tang","Jianheng",""],["Zhang","Qifan",""],["Li","Yuhan",""],["Li","Jia",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 09:19:23 GMT"}],"updateDate":"2024-07-02","timestamp":1719652763000,"abstract":"  The \"arms race\" of Large Language Models (LLMs) demands novel, challenging,\nand diverse benchmarks to faithfully examine their progresses. We introduce\nGraphArena, a benchmarking tool designed to evaluate LLMs on graph\ncomputational problems using million-scale real-world graphs from diverse\nscenarios such as knowledge graphs, social networks, and molecular structures.\nGraphArena offers a suite of 10 computational tasks, encompassing four\npolynomial-time (e.g., Shortest Distance) and six NP-complete challenges (e.g.,\nTravelling Salesman Problem). It features a rigorous evaluation framework that\nclassifies LLM outputs as correct, suboptimal (feasible but not optimal), or\nhallucinatory (properly formatted but infeasible). Evaluation of 10 leading\nLLMs, including GPT-4o and LLaMA3-70B-Instruct, reveals that even\ntop-performing models struggle with larger, more complex graph problems and\nexhibit hallucination issues. Despite the application of strategies such as\nchain-of-thought prompting, these issues remain unresolved. GraphArena\ncontributes a valuable supplement to the existing LLM benchmarks and is\nopen-sourced at https://github.com/squareRoot3/GraphArena.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0UIEnw0qIoKLZWSi1R-mjQtCars8Qb_gPlErlv7SQ1c","pdfSize":"948172","objectId":"0xa26c93535b90970482b817fbb3dc717a42378037e8d251e53a5bce15048bf1e8","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
