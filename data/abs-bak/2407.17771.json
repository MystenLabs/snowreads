{"id":"2407.17771","title":"Banyan: Improved Representation Learning with Explicit Structure","authors":"Mattia Opper and N. Siddharth","authorsParsed":[["Opper","Mattia",""],["Siddharth","N.",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 04:58:08 GMT"}],"updateDate":"2024-07-26","timestamp":1721883488000,"abstract":"  We present Banyan, an improved model to learn semantic representations by\ninducing explicit structure over data. In contrast to prior approaches using\nstructure spanning single sentences, Banyan learns by resolving multiple\nconstituent structures into a shared one explicitly incorporating global\ncontext. Combined with an improved message-passing scheme inspired by Griffin,\nBanyan learns significantly better representations, avoids spurious false\nnegatives with contrastive learning, and drastically improves memory efficiency\nin such explicit-structured models. Using the Self-StrAE framework, we show\nthat Banyan (a) outperforms baselines using sentential structure across various\nsettings (b) matches or outperforms unstructured baselines like GloVe\n(+augmentations) and a RoBERTa medium (+simcse) pre-trained on 100M tokens,\ndespite having just a handful of (non-embedding) parameters, and (c) also\nlearns effective representations across several low resource (Asian and\nAfrican) languages as measured on SemRel tasks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4bV_7kEF8B2GJjl7q38dGg-pm2VMD9-l-dIf3H-2FOg","pdfSize":"697904","objectId":"0x84643eb6ca674dc75c00d6259d21bd88a255aad079a01ac6fbf83113ac4dd357","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
