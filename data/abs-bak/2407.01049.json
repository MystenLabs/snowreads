{"id":"2407.01049","title":"SE(3)-Hyena Operator for Scalable Equivariant Learning","authors":"Artem Moskalev and Mangal Prakash and Rui Liao and Tommaso Mansi","authorsParsed":[["Moskalev","Artem",""],["Prakash","Mangal",""],["Liao","Rui",""],["Mansi","Tommaso",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 07:56:48 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 15:06:41 GMT"}],"updateDate":"2024-08-14","timestamp":1719820608000,"abstract":"  Modeling global geometric context while maintaining equivariance is crucial\nfor accurate predictions in many fields such as biology, chemistry, or vision.\nYet, this is challenging due to the computational demands of processing\nhigh-dimensional data at scale. Existing approaches such as equivariant\nself-attention or distance-based message passing, suffer from quadratic\ncomplexity with respect to sequence length, while localized methods sacrifice\nglobal information. Inspired by the recent success of state-space and\nlong-convolutional models, in this work, we introduce SE(3)-Hyena operator, an\nequivariant long-convolutional model based on the Hyena operator. The\nSE(3)-Hyena captures global geometric context at sub-quadratic complexity while\nmaintaining equivariance to rotations and translations. Evaluated on\nequivariant associative recall and n-body modeling, SE(3)-Hyena matches or\noutperforms equivariant self-attention while requiring significantly less\nmemory and computational resources for long sequences. Our model processes the\ngeometric context of 20k tokens x3.5 times faster than the equivariant\ntransformer and allows x175 longer a context within the same memory budget.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"IDffvP701Fk8NtqEKfhVH4NGnCWnjaUG3gFjRxO7dkg","pdfSize":"632204","objectId":"0x8f9bd8e257c64be1db80b9e7cf63cc871a2516ba07cd57ccbc1ca6fa1c2e8c5f","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
