{"id":"2407.12815","title":"SMLT-MUGC: Small, Medium, and Large Texts -- Machine versus\n  User-Generated Content Detection and Comparison","authors":"Anjali Rawal, Hui Wang, Youjia Zheng, Yu-Hsuan Lin, Shanu Sushmita","authorsParsed":[["Rawal","Anjali",""],["Wang","Hui",""],["Zheng","Youjia",""],["Lin","Yu-Hsuan",""],["Sushmita","Shanu",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 22:19:01 GMT"}],"updateDate":"2024-07-19","timestamp":1719613141000,"abstract":"  Large language models (LLMs) have gained significant attention due to their\nability to mimic human language. Identifying texts generated by LLMs is crucial\nfor understanding their capabilities and mitigating potential consequences.\nThis paper analyzes datasets of varying text lengths: small, medium, and large.\nWe compare the performance of machine learning algorithms on four datasets: (1)\nsmall (tweets from Election, FIFA, and Game of Thrones), (2) medium (Wikipedia\nintroductions and PubMed abstracts), and (3) large (OpenAI web text dataset).\nOur results indicate that LLMs with very large parameters (such as the XL-1542\nvariant of GPT2 with 1542 million parameters) were harder (74%) to detect using\ntraditional machine learning methods. However, detecting texts of varying\nlengths from LLMs with smaller parameters (762 million or less) can be done\nwith high accuracy (96% and above). We examine the characteristics of human and\nmachine-generated texts across multiple dimensions, including linguistics,\npersonality, sentiment, bias, and morality. Our findings indicate that\nmachine-generated texts generally have higher readability and closely mimic\nhuman moral judgments but differ in personality traits. SVM and Voting\nClassifier (VC) models consistently achieve high performance across most\ndatasets, while Decision Tree (DT) models show the lowest performance. Model\nperformance drops when dealing with rephrased texts, particularly shorter texts\nlike tweets. This study underscores the challenges and importance of detecting\nLLM-generated texts and suggests directions for future research to improve\ndetection methods and understand the nuanced capabilities of LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"M0zZFQb4Zv2SjQuac4M8yhddmc3JyaRIc4guhPfFblU","pdfSize":"179916","objectId":"0x3d28124d72304e6be83f4b07281a2cc64da447fd9a8fa21da2d51f7a8cd0711d","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
