{"id":"2407.14645","title":"Performance Modeling and Workload Analysis of Distributed Large Language\n  Model Training and Inference","authors":"Joyjit Kundu, Wenzhe Guo, Ali BanaGozar, Udari De Alwis, Sourav\n  Sengupta, Puneet Gupta and Arindam Mallik","authorsParsed":[["Kundu","Joyjit",""],["Guo","Wenzhe",""],["BanaGozar","Ali",""],["De Alwis","Udari",""],["Sengupta","Sourav",""],["Gupta","Puneet",""],["Mallik","Arindam",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 19:49:05 GMT"}],"updateDate":"2024-07-23","timestamp":1721418545000,"abstract":"  Aligning future system design with the ever-increasing compute needs of large\nlanguage models (LLMs) is undoubtedly an important problem in today's world.\nHere, we propose a general performance modeling methodology and workload\nanalysis of distributed LLM training and inference through an analytical\nframework that accurately considers compute, memory sub-system, network, and\nvarious parallelization strategies (model parallel, data parallel, pipeline\nparallel, and sequence parallel). We validate our performance predictions with\npublished data from literature and relevant industry vendors (e.g., NVIDIA).\nFor distributed training, we investigate the memory footprint of LLMs for\ndifferent activation re-computation methods, dissect the key factors behind the\nmassive performance gain from A100 to B200 ($\\sim$ 35x speed-up closely\nfollowing NVIDIA's scaling trend), and further run a design space exploration\nat different technology nodes (12 nm to 1 nm) to study the impact of logic,\nmemory, and network scaling on the performance. For inference, we analyze the\ncompute versus memory boundedness of different operations at a matrix-multiply\nlevel for different GPU systems and further explore the impact of DRAM memory\ntechnology scaling on inference latency. Utilizing our modeling framework, we\nreveal the evolution of performance bottlenecks for both LLM training and\ninference with technology scaling, thus, providing insights to design future\nsystems for LLM training and inference.\n","subjects":["Computing Research Repository/Hardware Architecture","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"f0kd0UCCkAf7IN1M9u09BSwdjP-bVXcoYi8xrdlVCic","pdfSize":"790243","objectId":"0x222c0f13d4c477a233b7dfb4149f74b64083154757e81ed4f223c75a66ab07ce","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
