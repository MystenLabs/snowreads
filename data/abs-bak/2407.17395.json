{"id":"2407.17395","title":"Which distribution were you sampled from? Towards a more tangible\n  conception of data","authors":"Benedikt H\\\"oltgen and Robert C. Williamson","authorsParsed":[["HÃ¶ltgen","Benedikt",""],["Williamson","Robert C.",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 16:17:14 GMT"},{"version":"v2","created":"Wed, 14 Aug 2024 12:17:38 GMT"},{"version":"v3","created":"Thu, 12 Sep 2024 09:22:25 GMT"}],"updateDate":"2024-09-13","timestamp":1721837834000,"abstract":"  Machine Learning research, as most of Statistics, heavily relies on the\nconcept of a data-generating probability distribution. The standard presumption\nis that since data points are `sampled from' such a distribution, one can learn\nfrom observed data about this distribution and, thus, predict future data\npoints which, it is presumed, are also drawn from it. Drawing on scholarship\nacross disciplines, we here argue that this framework is not always a good\nmodel. Not only do such true probability distributions not exist; the framework\ncan also be misleading and obscure both the choices made and the goals pursued\nin machine learning practice. We suggest an alternative framework that focuses\non finite populations rather than abstract distributions; while classical\nlearning theory can be left almost unchanged, it opens new opportunities,\nespecially to model sampling. We compile these considerations into five reasons\nfor modelling machine learning -- in some settings -- with finite populations\nrather than generative distributions, both to be more faithful to practice and\nto provide novel theoretical insights.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"oziyTt1uEOIgvNQHmJ31ETAqvCTRTfKg4rpUoDxGShY","pdfSize":"215208","objectId":"0xadf778cd18b6577677d1a10431c22594e481d2307d83677d0015fa02cfb25a1f","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
