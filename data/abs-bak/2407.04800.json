{"id":"2407.04800","title":"Segmentation-Free Guidance for Text-to-Image Diffusion Models","authors":"Kambiz Azarian, Debasmit Das, Qiqi Hou, Fatih Porikli","authorsParsed":[["Azarian","Kambiz",""],["Das","Debasmit",""],["Hou","Qiqi",""],["Porikli","Fatih",""]],"versions":[{"version":"v1","created":"Mon, 3 Jun 2024 17:51:00 GMT"}],"updateDate":"2024-07-09","timestamp":1717437060000,"abstract":"  We introduce segmentation-free guidance, a novel method designed for\ntext-to-image diffusion models like Stable Diffusion. Our method does not\nrequire retraining of the diffusion model. At no additional compute cost, it\nuses the diffusion model itself as an implied segmentation network, hence named\nsegmentation-free guidance, to dynamically adjust the negative prompt for each\npatch of the generated image, based on the patch's relevance to concepts in the\nprompt. We evaluate segmentation-free guidance both objectively, using FID,\nCLIP, IS, and PickScore, and subjectively, through human evaluators. For the\nsubjective evaluation, we also propose a methodology for subsampling the\nprompts in a dataset like MS COCO-30K to keep the number of human evaluations\nmanageable while ensuring that the selected subset is both representative in\nterms of content and fair in terms of model performance. The results\ndemonstrate the superiority of our segmentation-free guidance to the widely\nused classifier-free method. Human evaluators preferred segmentation-free\nguidance over classifier-free 60% to 19%, with 18% of occasions showing a\nstrong preference. Additionally, PickScore win-rate, a recently proposed metric\nmimicking human preference, also indicates a preference for our method over\nclassifier-free.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PCj7koSTzyE4Zv1sqGM72O9Y97u4g_EuzTog9-h76WI","pdfSize":"3528518"}
