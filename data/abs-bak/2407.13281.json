{"id":"2407.13281","title":"Auditing Local Explanations is Hard","authors":"Robi Bhattacharjee, Ulrike von Luxburg","authorsParsed":[["Bhattacharjee","Robi",""],["von Luxburg","Ulrike",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 08:34:05 GMT"}],"updateDate":"2024-07-19","timestamp":1721291645000,"abstract":"  In sensitive contexts, providers of machine learning algorithms are\nincreasingly required to give explanations for their algorithms' decisions.\nHowever, explanation receivers might not trust the provider, who potentially\ncould output misleading or manipulated explanations. In this work, we\ninvestigate an auditing framework in which a third-party auditor or a\ncollective of users attempts to sanity-check explanations: they can query model\ndecisions and the corresponding local explanations, pool all the information\nreceived, and then check for basic consistency properties. We prove upper and\nlower bounds on the amount of queries that are needed for an auditor to succeed\nwithin this framework. Our results show that successful auditing requires a\npotentially exorbitant number of queries -- particularly in high dimensional\ncases. Our analysis also reveals that a key property is the ``locality'' of the\nprovided explanations -- a quantity that so far has not been paid much\nattention to in the explainability literature. Looking forward, our results\nsuggest that for complex high-dimensional settings, merely providing a\npointwise prediction and explanation could be insufficient, as there is no way\nfor the users to verify that the provided explanations are not completely\nmade-up.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"hvWynWdg7VJWMB3WdUTM4flIi2wRqkucZbIujR5DtsQ","pdfSize":"954641","objectId":"0xdc89744416b9dbd3aef308bdfacaf44c7ec1cf7250297a80ff6e2c80073901cb","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
