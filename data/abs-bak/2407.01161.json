{"id":"2407.01161","title":"GazeNoter: Co-Piloted AR Note-Taking via Gaze Selection of LLM\n  Suggestions to Match Users' Intentions","authors":"Hsin-Ruey Tsai, Shih-Kang Chiu, Bryan Wang","authorsParsed":[["Tsai","Hsin-Ruey",""],["Chiu","Shih-Kang",""],["Wang","Bryan",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 10:29:56 GMT"}],"updateDate":"2024-07-02","timestamp":1719829796000,"abstract":"  Note-taking is critical during speeches and discussions, serving not only for\nlater summarization and organization but also for real-time question and\nopinion reminding in question-and-answer sessions or timely contributions in\ndiscussions. Manually typing on smartphones for note-taking could be\ndistracting and increase cognitive load for users. While large language models\n(LLMs) are used to automatically generate summaries and highlights, the content\ngenerated by artificial intelligence (AI) may not match users' intentions\nwithout user input or interaction. Therefore, we propose an AI-copiloted\naugmented reality (AR) system, GazeNoter, to allow users to swiftly select\ndiverse LLM-generated suggestions via gaze on an AR headset for real-time\nnote-taking. GazeNoter leverages an AR headset as a medium for users to swiftly\nadjust the LLM output to match their intentions, forming a user-in-the-loop AI\nsystem for both within-context and beyond-context notes. We conducted two user\nstudies to verify the usability of GazeNoter in attending speeches in a static\nsitting condition and walking meetings and discussions in a mobile walking\ncondition, respectively.\n","subjects":["Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d7ANxc3XJCjGPqW556jsLyrlV_YfyuHUZGfMKPFdgT4","pdfSize":"22864476","objectId":"0x65fb32babf9a26570d2a5ba8c59d10b420feb0ae212840d7cb4082e2fbf79bba","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
