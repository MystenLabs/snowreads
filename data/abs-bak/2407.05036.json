{"id":"2407.05036","title":"Enhance the Robustness of Text-Centric Multimodal Alignments","authors":"Ting-Yu Yen, Yun-Da Tsai, Keng-Te Liao, Shou-De Lin","authorsParsed":[["Yen","Ting-Yu",""],["Tsai","Yun-Da",""],["Liao","Keng-Te",""],["Lin","Shou-De",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 10:12:29 GMT"}],"updateDate":"2024-07-09","timestamp":1720260749000,"abstract":"  Converting different modalities into general text, serving as input prompts\nfor large language models (LLMs), is a common method to align multimodal models\nwhen there is limited pairwise data. This text-centric approach leverages the\nunique properties of text as a modality space, transforming diverse inputs into\na unified textual representation. This enables downstream models to effectively\ninterpret various modal inputs. This study assesses the quality and robustness\nof multimodal representations in the presence of missing entries, noise, or\nabsent modalities, revealing that current text-centric alignment methods\ncompromise downstream robustness. To address this issue, we propose a new\ntext-centric approach that achieves superior robustness compared to previous\nmethods across various modalities in different settings. Our findings highlight\nthe potential of this approach to enhance the robustness and adaptability of\nmultimodal representations, offering a promising solution for dynamic and\nreal-world applications.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"gCD3ukP0dnZDKn78du2CLQm2ubIKg5sJrBfqOijlgys","pdfSize":"1126426"}
