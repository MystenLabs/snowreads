{"id":"2407.11033","title":"Hadamard Adapter: An Extreme Parameter-Efficient Adapter Tuning Method\n  for Pre-trained Language Models","authors":"Yuyan Chen, Qiang Fu, Ge Fan, Lun Du, Jian-Guang Lou, Shi Han, Dongmei\n  Zhang, Zhixu Li, Yanghua Xiao","authorsParsed":[["Chen","Yuyan",""],["Fu","Qiang",""],["Fan","Ge",""],["Du","Lun",""],["Lou","Jian-Guang",""],["Han","Shi",""],["Zhang","Dongmei",""],["Li","Zhixu",""],["Xiao","Yanghua",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 18:21:28 GMT"}],"updateDate":"2024-07-17","timestamp":1720117288000,"abstract":"  Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8SYMEOn5SUv4bzomqjR1cGIlBY1zbybXPJRO1UhQfIg","pdfSize":"7639266","objectId":"0x7f9f01ec724f7cbc59d8930bd3a93438886784d1d5bab64ecd4db56a16240091","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
