{"id":"2407.07801","title":"AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning","authors":"Jongsuk Kim, Jiwon Shin, Junmo Kim","authorsParsed":[["Kim","Jongsuk",""],["Shin","Jiwon",""],["Kim","Junmo",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:17:49 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 02:38:14 GMT"}],"updateDate":"2024-07-12","timestamp":1720628269000,"abstract":"  In recent years, advancements in representation learning and language models\nhave propelled Automated Captioning (AC) to new heights, enabling the\ngeneration of human-level descriptions. Leveraging these advancements, we\npropose AVCap, an Audio-Visual Captioning framework, a simple yet powerful\nbaseline approach applicable to audio-visual captioning. AVCap utilizes\naudio-visual features as text tokens, which has many advantages not only in\nperformance but also in the extensibility and scalability of the model. AVCap\nis designed around three pivotal dimensions: the exploration of optimal\naudio-visual encoder architectures, the adaptation of pre-trained models\naccording to the characteristics of generated text, and the investigation into\nthe efficacy of modality fusion in captioning. Our method outperforms existing\naudio-visual captioning methods across all metrics and the code is available on\nhttps://github.com/JongSuk1/AVCap\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"9mfkDAYmqmZA_09foLkUXdoar1h9jD1Y5gRBgqnZBiI","pdfSize":"441315","objectId":"0xacaeb9e5e3f97f9bc1e41e794d876533f46caa7f5d8fe7d4c3db827b5230b19f","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
