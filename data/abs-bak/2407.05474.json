{"id":"2407.05474","title":"Enhancing Hallucination Detection through Perturbation-Based Synthetic\n  Data Generation in System Responses","authors":"Dongxu Zhang, Varun Gangal, Barrett Martin Lattimer, Yi Yang","authorsParsed":[["Zhang","Dongxu",""],["Gangal","Varun",""],["Lattimer","Barrett Martin",""],["Yang","Yi",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 19:19:32 GMT"}],"updateDate":"2024-07-09","timestamp":1720379972000,"abstract":"  Detecting hallucinations in large language model (LLM) outputs is pivotal,\nyet traditional fine-tuning for this classification task is impeded by the\nexpensive and quickly outdated annotation process, especially across numerous\nvertical domains and in the face of rapid LLM advancements. In this study, we\nintroduce an approach that automatically generates both faithful and\nhallucinated outputs by rewriting system responses. Experimental findings\ndemonstrate that a T5-base model, fine-tuned on our generated dataset,\nsurpasses state-of-the-art zero-shot detectors and existing synthetic\ngeneration methods in both accuracy and latency, indicating efficacy of our\napproach.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dv49ZWpLmhP0byUsJ93wXfxWCCL7LM59PU24xWf-LhY","pdfSize":"1011557"}
