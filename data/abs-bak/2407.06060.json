{"id":"2407.06060","title":"MERGE -- A Bimodal Dataset for Static Music Emotion Recognition","authors":"Pedro Lima Louro, Hugo Redinho, Ricardo Santos, Ricardo Malheiro,\n  Renato Panda, Rui Pedro Paiva","authorsParsed":[["Louro","Pedro Lima",""],["Redinho","Hugo",""],["Santos","Ricardo",""],["Malheiro","Ricardo",""],["Panda","Renato",""],["Paiva","Rui Pedro",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 16:01:04 GMT"}],"updateDate":"2024-07-09","timestamp":1720454464000,"abstract":"  The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a severe lack of\npublic and sizeable bimodal databases has hampered the development and\nimprovement of bimodal audio-lyrics systems. This article proposes three new\naudio, lyrics, and bimodal MER research datasets, collectively called MERGE,\ncreated using a semi-automatic approach. To comprehensively assess the proposed\ndatasets and establish a baseline for benchmarking, we conducted several\nexperiments for each modality, using feature engineering, machine learning, and\ndeep learning methodologies. In addition, we propose and validate fixed\ntrain-validate-test splits. The obtained results confirm the viability of the\nproposed datasets, achieving the best overall result of 79.21% F1-score for\nbimodal classification using a deep neural network.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning","Computing Research Repository/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"PamzsDSdCaQaUonCTzRNueTX5aJmzmkugJJFXNaGdag","pdfSize":"2914612"}
