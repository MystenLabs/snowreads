{"id":"2412.17692","title":"FedTLU: Federated Learning with Targeted Layer Updates","authors":"Jong-Ik Park and Carlee Joe-Wong","authorsParsed":[["Park","Jong-Ik",""],["Joe-Wong","Carlee",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 16:17:46 GMT"},{"version":"v2","created":"Sun, 26 Jan 2025 05:21:54 GMT"}],"updateDate":"2025-01-28","timestamp":1734970666000,"abstract":"  Federated learning (FL) addresses privacy concerns in training language\nmodels by enabling multiple clients to contribute to the training, without\nsending their data to others. However, non-IID (identically and independently\ndistributed) data across clients often limits FL's performance. This issue is\nespecially challenging during model fine-tuning, as noise due to variations in\nclients' data distributions can harm model convergence near stationary points.\nThis paper proposes a targeted layer update strategy for fine-tuning in FL.\nInstead of randomly updating layers of the language model, as often done in\npractice, we use a scoring mechanism to identify and update the most critical\nlayers, avoiding excessively noisy or even poisoned updates by freezing the\nparameters in other layers. We show in extensive experiments that our method\nimproves convergence and performance in non-IID settings, offering a more\nefficient approach to fine-tuning federated language models.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"a1yL9r1fi5X4pMNW9cWcbS1ynHV0i1Ajqiksw6hPcJI","pdfSize":"1032081"}