{"id":"2412.10635","title":"Do LLMs Act as Repositories of Causal Knowledge?","authors":"Nick Huntington-Klein, Eleanor J. Murray","authorsParsed":[["Huntington-Klein","Nick",""],["Murray","Eleanor J.",""]],"versions":[{"version":"v1","created":"Sat, 14 Dec 2024 01:28:38 GMT"}],"updateDate":"2024-12-17","timestamp":1734139718000,"abstract":"  Large language models (LLMs) offer the potential to automate a large number\nof tasks that previously have not been possible to automate, including some in\nscience. There is considerable interest in whether LLMs can automate the\nprocess of causal inference by providing the information about causal links\nnecessary to build a structural model. We use the case of confounding in the\nCoronary Drug Project (CDP), for which there are several studies listing\nexpert-selected confounders that can serve as a ground truth. LLMs exhibit\nmediocre performance in identifying confounders in this setting, even though\ntext about the ground truth is in their training data. Variables that experts\nidentify as confounders are only slightly more likely to be labeled as\nconfounders by LLMs compared to variables that experts consider\nnon-confounders. Further, LLM judgment on confounder status is highly\ninconsistent across models, prompts, and irrelevant concerns like\nmultiple-choice option ordering. LLMs do not yet have the ability to automate\nthe reporting of causal links.\n","subjects":["Economics/Econometrics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ce8GngPWJAuhiTEkvk0WQA-4ZNLgw3XyDlGVL2GabUs","pdfSize":"3779435"}