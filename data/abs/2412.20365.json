{"id":"2412.20365","title":"Accelerated regularized learning in finite N-person games","authors":"Kyriakos Lotidis and Angeliki Giannou and Panayotis Mertikopoulos and\n  Nicholas Bambos","authorsParsed":[["Lotidis","Kyriakos",""],["Giannou","Angeliki",""],["Mertikopoulos","Panayotis",""],["Bambos","Nicholas",""]],"versions":[{"version":"v1","created":"Sun, 29 Dec 2024 06:09:26 GMT"}],"updateDate":"2024-12-31","timestamp":1735452566000,"abstract":"  Motivated by the success of Nesterov's accelerated gradient algorithm for\nconvex minimization problems, we examine whether it is possible to achieve\nsimilar performance gains in the context of online learning in games. To that\nend, we introduce a family of accelerated learning methods, which we call\n\"follow the accelerated leader\" (FTXL), and which incorporates the use of\nmomentum within the general framework of regularized learning - and, in\nparticular, the exponential/multiplicative weights algorithm and its variants.\nDrawing inspiration and techniques from the continuous-time analysis of\nNesterov's algorithm, we show that FTXL converges locally to strict Nash\nequilibria at a superlinear rate, achieving in this way an exponential speed-up\nover vanilla regularized learning methods (which, by comparison, converge to\nstrict equilibria at a geometric, linear rate). Importantly, FTXL maintains its\nsuperlinear convergence rate in a broad range of feedback structures, from\ndeterministic, full information models to stochastic, realization-based ones,\nand even when run with bandit, payoff-based information, where players are only\nable to observe their individual realized payoffs.\n","subjects":["Computer Science/Computer Science and Game Theory","Computer Science/Machine Learning","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DtP_zOqbRXzAilXdHvLwYOk7XtsnwOmZ5rhs0VHA_hI","pdfSize":"1255682"}