{"id":"2407.11922","title":"Learning secondary tool affordances of human partners using iCub robot's\n  egocentric data","authors":"Bosong Ding, Erhan Oztop, Giacomo Spigler, Murat Kirtay","authorsParsed":[["Ding","Bosong",""],["Oztop","Erhan",""],["Spigler","Giacomo",""],["Kirtay","Murat",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:14:13 GMT"}],"updateDate":"2024-07-17","timestamp":1721150053000,"abstract":"  Objects, in particular tools, provide several action possibilities to the\nagents that can act on them, which are generally associated with the term of\naffordances. A tool is typically designed for a specific purpose, such as\ndriving a nail in the case of a hammer, which we call as the primary\naffordance. A tool can also be used beyond its primary purpose, in which case\nwe can associate this auxiliary use with the term secondary affordance.\nPrevious work on affordance perception and learning has been mostly focused on\nprimary affordances. Here, we address the less explored problem of learning the\nsecondary tool affordances of human partners. To do this, we use the iCub robot\nto observe human partners with three cameras while they perform actions on\ntwenty objects using four different tools. In our experiments, human partners\nutilize tools to perform actions that do not correspond to their primary\naffordances. For example, the iCub robot observes a human partner using a ruler\nfor pushing, pulling, and moving objects instead of measuring their lengths. In\nthis setting, we constructed a dataset by taking images of objects before and\nafter each action is executed. We then model learning secondary affordances by\ntraining three neural networks (ResNet-18, ResNet-50, and ResNet-101) each on\nthree tasks, using raw images showing the `initial' and `final' position of\nobjects as input: (1) predicting the tool used to move an object, (2)\npredicting the tool used with an additional categorical input that encoded the\naction performed, and (3) joint prediction of both tool used and action\nperformed. Our results indicate that deep learning architectures enable the\niCub robot to predict secondary tool affordances, thereby paving the road for\nhuman-robot collaborative object manipulation involving complex affordances.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"29ZZCJZg3rCWf8d3wwAwXMs9bK9hlJoc2VCJlHvuDno","pdfSize":"4939235","objectId":"0x0ce90eb351a8e8ff3e4c5d21f659deb2786ce69256de2cffbac31bb90bf19ec6","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
