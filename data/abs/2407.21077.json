{"id":"2407.21077","title":"Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions\n  for Large Language Models","authors":"Somshubra Majumdar, Vahid Noroozi, Sean Narenthiran, Aleksander Ficek,\n  Jagadeesh Balam, Boris Ginsburg","authorsParsed":[["Majumdar","Somshubra",""],["Noroozi","Vahid",""],["Narenthiran","Sean",""],["Ficek","Aleksander",""],["Balam","Jagadeesh",""],["Ginsburg","Boris",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 20:42:59 GMT"}],"updateDate":"2024-08-01","timestamp":1722285779000,"abstract":"  Large Language Models (LLMs) rely on instruction samples for alignment, but\ncreating these datasets poses challenges, particularly in expert-dependent\ntasks like coding, which can be cost-prohibitive. One approach to mitigate\nthese challenges is synthesizing data using another LLM. In this paper, we\nintroduce a scalable method for generating synthetic instructions to enhance\nthe code generation capability of LLMs. The proposed algorithm,\nGenetic-Instruct, mimics evolutionary processes, utilizing self-instruction to\ncreate numerous synthetic samples from a limited number of seeds.\nGenetic-Instruct is designed for efficient scaling of the generation process.\nFine-tuning multiple coding LLMs with the synthetic samples demonstrates a\nsignificant improvement in their code generation accuracy compared to the\nbaselines.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KCAuzkkBnJy9kyy59zB4NumzeBVNmBn_OfJQnSQyHFE","pdfSize":"851036","objectId":"0x73cb73dd800da886e82a94bcfe73db6dee7344c249a5940f384c2d6aae36ba01","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
