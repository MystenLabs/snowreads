{"id":"2407.11854","title":"Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in\n  Grammatical Error Detection","authors":"Gaetan Lopez Latouche and Marc-Andr\\'e Carbonneau and Ben Swanson","authorsParsed":[["Latouche","Gaetan Lopez",""],["Carbonneau","Marc-Andr√©",""],["Swanson","Ben",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:35:15 GMT"}],"updateDate":"2024-07-17","timestamp":1721144115000,"abstract":"  Grammatical Error Detection (GED) methods rely heavily on human annotated\nerror corpora. However, these annotations are unavailable in many low-resource\nlanguages. In this paper, we investigate GED in this context. Leveraging the\nzero-shot cross-lingual transfer capabilities of multilingual pre-trained\nlanguage models, we train a model using data from a diverse set of languages to\ngenerate synthetic errors in other languages. These synthetic error corpora are\nthen used to train a GED model. Specifically we propose a two-stage fine-tuning\npipeline where the GED model is first fine-tuned on multilingual synthetic data\nfrom target languages followed by fine-tuning on human-annotated GED corpora\nfrom source languages. This approach outperforms current state-of-the-art\nannotation-free GED methods. We also analyse the errors produced by our method\nand other strong baselines, finding that our approach produces errors that are\nmore diverse and more similar to human errors.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"ct2a5SfyE2dcZ7T0fF3IAiw6BmjUSQN8-dwkBvvDPc8","pdfSize":"524018","objectId":"0xfba9695e303de7115aab5c78255e7a4c945452d65b2192073a3feeb14ae024c0","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
