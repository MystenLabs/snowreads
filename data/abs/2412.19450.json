{"id":"2412.19450","title":"Find the Intention of Instruction: Comprehensive Evaluation of\n  Instruction Understanding for Large Language Models","authors":"Hyeonseok Moon, Jaehyung Seo, Seungyoon Lee, Chanjun Park, Heuiseok\n  Lim","authorsParsed":[["Moon","Hyeonseok",""],["Seo","Jaehyung",""],["Lee","Seungyoon",""],["Park","Chanjun",""],["Lim","Heuiseok",""]],"versions":[{"version":"v1","created":"Fri, 27 Dec 2024 04:37:39 GMT"},{"version":"v2","created":"Thu, 23 Jan 2025 00:45:55 GMT"}],"updateDate":"2025-01-24","timestamp":1735274259000,"abstract":"  One of the key strengths of Large Language Models (LLMs) is their ability to\ninteract with humans by generating appropriate responses to given instructions.\nThis ability, known as instruction-following capability, has established a\nfoundation for the use of LLMs across various fields and serves as a crucial\nmetric for evaluating their performance. While numerous evaluation benchmarks\nhave been developed, most focus solely on clear and coherent instructions.\nHowever, we have noted that LLMs can become easily distracted by\ninstruction-formatted statements, which may lead to an oversight of their\ninstruction comprehension skills. To address this issue, we introduce the\nIntention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs'\ncapacity to remain focused and understand instructions without being misled by\nextraneous instructions. The primary objective of this benchmark is to identify\nthe appropriate instruction that accurately guides the generation of a given\ncontext. Our findings suggest that even recently introduced state-of-the-art\nmodels still lack instruction understanding capability. Along with the\nproposition of IoInst in this study, we also present broad analyses of the\nseveral strategies potentially applicable to IoInst.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AKDn5DlUCJw6WUoeskWvJmtbEbAqc8qhASgO8enx3aA","pdfSize":"823945"}