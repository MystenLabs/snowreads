{"id":"2412.08268","title":"LCFO: Long Context and Long Form Output Dataset and Benchmarking","authors":"Marta R. Costa-juss\\`a, Pierre Andrews, Mariano Coria Meglioli, Joy\n  Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo\n  S\\'anchez, Holger Schwenk, Tuan Tran, Arina Turkatenko, Carleigh Wood","authorsParsed":[["Costa-jussà","Marta R.",""],["Andrews","Pierre",""],["Meglioli","Mariano Coria",""],["Chen","Joy",""],["Chuang","Joe",""],["Dale","David",""],["Ropers","Christophe",""],["Mourachko","Alexandre",""],["Sánchez","Eduardo",""],["Schwenk","Holger",""],["Tran","Tuan",""],["Turkatenko","Arina",""],["Wood","Carleigh",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 10:35:45 GMT"},{"version":"v2","created":"Thu, 12 Dec 2024 17:32:23 GMT"}],"updateDate":"2024-12-13","timestamp":1733913345000,"abstract":"  This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"VnQg4uvTYV7TkV-A5GKl7LpmRvUVcSpXNWKRPLOys-E","pdfSize":"445786"}