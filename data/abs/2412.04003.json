{
  "id": "2412.04003",
  "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for\n  Cross-Lingual Enhancement",
  "authors": "Lingfeng Ming, Bo Zeng, Chenyang Lyu, Tianqi Shi, Yu Zhao, Xue Yang,\n  Yefeng Liu, Yiyu Wang, Linlong Xu, Yangyang Liu, Xiaohu Zhao, Hao Wang, Heng\n  Liu, Hao Zhou, Huifeng Yin, Zifu Shang, Haijun Li, Longyue Wang, Weihua Luo,\n  Kaifu Zhang",
  "authorsParsed": [
    [
      "Ming",
      "Lingfeng",
      ""
    ],
    [
      "Zeng",
      "Bo",
      ""
    ],
    [
      "Lyu",
      "Chenyang",
      ""
    ],
    [
      "Shi",
      "Tianqi",
      ""
    ],
    [
      "Zhao",
      "Yu",
      ""
    ],
    [
      "Yang",
      "Xue",
      ""
    ],
    [
      "Liu",
      "Yefeng",
      ""
    ],
    [
      "Wang",
      "Yiyu",
      ""
    ],
    [
      "Xu",
      "Linlong",
      ""
    ],
    [
      "Liu",
      "Yangyang",
      ""
    ],
    [
      "Zhao",
      "Xiaohu",
      ""
    ],
    [
      "Wang",
      "Hao",
      ""
    ],
    [
      "Liu",
      "Heng",
      ""
    ],
    [
      "Zhou",
      "Hao",
      ""
    ],
    [
      "Yin",
      "Huifeng",
      ""
    ],
    [
      "Shang",
      "Zifu",
      ""
    ],
    [
      "Li",
      "Haijun",
      ""
    ],
    [
      "Wang",
      "Longyue",
      ""
    ],
    [
      "Luo",
      "Weihua",
      ""
    ],
    [
      "Zhang",
      "Kaifu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 09:26:58 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733390818000,
  "abstract": "  Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "t3xj8PYZ-VzgRQV1Cm7uZRxDexVFARDgMb3xgHTaHmc",
  "pdfSize": "2173936"
}