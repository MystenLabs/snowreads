{
  "id": "2412.15252",
  "title": "NER- RoBERTa: Fine-Tuning RoBERTa for Named Entity Recognition (NER)\n  within low-resource languages",
  "authors": "Abdulhady Abas Abdullah, Srwa Hasan Abdulla, Dalia Mohammad Toufiq,\n  Halgurd S. Maghdid, Tarik A. Rashid, Pakshan F. Farho, Shadan Sh. Sabr, Akar\n  H. Taher, Darya S. Hamad, Hadi Veisi, and Aras T. Asaad",
  "authorsParsed": [
    [
      "Abdullah",
      "Abdulhady Abas",
      ""
    ],
    [
      "Abdulla",
      "Srwa Hasan",
      ""
    ],
    [
      "Toufiq",
      "Dalia Mohammad",
      ""
    ],
    [
      "Maghdid",
      "Halgurd S.",
      ""
    ],
    [
      "Rashid",
      "Tarik A.",
      ""
    ],
    [
      "Farho",
      "Pakshan F.",
      ""
    ],
    [
      "Sabr",
      "Shadan Sh.",
      ""
    ],
    [
      "Taher",
      "Akar H.",
      ""
    ],
    [
      "Hamad",
      "Darya S.",
      ""
    ],
    [
      "Veisi",
      "Hadi",
      ""
    ],
    [
      "Asaad",
      "Aras T.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 15 Dec 2024 07:07:17 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734246437000,
  "abstract": "  Nowadays, Natural Language Processing (NLP) is an important tool for most\npeople's daily life routines, ranging from understanding speech, translation,\nnamed entity recognition (NER), and text categorization, to generative text\nmodels such as ChatGPT. Due to the existence of big data and consequently large\ncorpora for widely used languages like English, Spanish, Turkish, Persian, and\nmany more, these applications have been developed accurately. However, the\nKurdish language still requires more corpora and large datasets to be included\nin NLP applications. This is because Kurdish has a rich linguistic structure,\nvaried dialects, and a limited dataset, which poses unique challenges for\nKurdish NLP (KNLP) application development. While several studies have been\nconducted in KNLP for various applications, Kurdish NER (KNER) remains a\nchallenge for many KNLP tasks, including text analysis and classification. In\nthis work, we address this limitation by proposing a methodology for\nfine-tuning the pre-trained RoBERTa model for KNER. To this end, we first\ncreate a Kurdish corpus, followed by designing a modified model architecture\nand implementing the training procedures. To evaluate the trained model, a set\nof experiments is conducted to demonstrate the performance of the KNER model\nusing different tokenization methods and trained models. The experimental\nresults show that fine-tuned RoBERTa with the SentencePiece tokenization method\nsubstantially improves KNER performance, achieving a 12.8% improvement in\nF1-score compared to traditional models, and consequently establishes a new\nbenchmark for KNLP.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "rpHftDy9_glWALifP69eSWpsK0apNo1DgzwkGQLypKw",
  "pdfSize": "698967"
}