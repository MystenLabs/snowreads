{
  "id": "2412.14231",
  "title": "ViTmiX: Vision Transformer Explainability Augmented by Mixed\n  Visualization Methods",
  "authors": "Eduard Hogea, Darian M. Onchis, Ana Coporan, Adina Magda Florea,\n  Codruta Istin",
  "authorsParsed": [
    [
      "Hogea",
      "Eduard",
      ""
    ],
    [
      "Onchis",
      "Darian M.",
      ""
    ],
    [
      "Coporan",
      "Ana",
      ""
    ],
    [
      "Florea",
      "Adina Magda",
      ""
    ],
    [
      "Istin",
      "Codruta",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 18:18:19 GMT"
    }
  ],
  "updateDate": "2024-12-20",
  "timestamp": 1734545899000,
  "abstract": "  Recent advancements in Vision Transformers (ViT) have demonstrated\nexceptional results in various visual recognition tasks, owing to their ability\nto capture long-range dependencies in images through self-attention mechanisms.\nHowever, the complex nature of ViT models requires robust explainability\nmethods to unveil their decision-making processes. Explainable Artificial\nIntelligence (XAI) plays a crucial role in improving model transparency and\ntrustworthiness by providing insights into model predictions. Current\napproaches to ViT explainability, based on visualization techniques such as\nLayer-wise Relevance Propagation (LRP) and gradient-based methods, have shown\npromising but sometimes limited results. In this study, we explore a hybrid\napproach that mixes multiple explainability techniques to overcome these\nlimitations and enhance the interpretability of ViT models. Our experiments\nreveal that this hybrid approach significantly improves the interpretability of\nViT models compared to individual methods. We also introduce modifications to\nexisting techniques, such as using geometric mean for mixing, which\ndemonstrates notable results in object segmentation tasks. To quantify the\nexplainability gain, we introduced a novel post-hoc explainability measure by\napplying the Pigeonhole principle. These findings underscore the importance of\nrefining and optimizing explainability methods for ViT models, paving the way\nto reliable XAI-based segmentations.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "jGcNeFoQLd6lEX1yZdntRFm-f96z9OkFXxflt2UZrs4",
  "pdfSize": "9582817"
}