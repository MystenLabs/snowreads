{
  "id": "2412.07722",
  "title": "Feel my Speech: Automatic Speech Emotion Conversion for Tangible,\n  Haptic, or Proxemic Interaction Design",
  "authors": "Ilhan Aslan",
  "authorsParsed": [
    [
      "Aslan",
      "Ilhan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 18:14:45 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733854485000,
  "abstract": "  Innovations in interaction design are increasingly driven by progress in\nmachine learning fields. Automatic speech emotion recognition (SER) is such an\nexample field on the rise, creating well performing models, which typically\ntake as input a speech audio sample and provide as output digital labels or\nvalues describing the human emotion(s) embedded in the speech audio sample.\nSuch labels and values are only abstract representations of the felt or\nexpressed emotions, making it challenging to analyse them as experiences and\nwork with them as design material for physical interactions, including\ntangible, haptic, or proxemic interactions. This paper argues that both the\nanalysis of emotions and their use in interaction designs would benefit from\nalternative physical representations, which can be directly felt and socially\ncommunicated as bodily sensations or spatial behaviours. To this end, a method\nis described and a starter kit for speech emotion conversion is provided.\nFurthermore, opportunities of speech emotion conversion for new interaction\ndesigns are introduced, such as for interacting with animals or robots.\n",
  "subjects": [
    "Computer Science/Human-Computer Interaction"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "aA2SFlqdcgjDfyBZ8dbbvut7wSJCuZFXFoXZ_qh37uE",
  "pdfSize": "695542"
}