{"id":"2407.21260","title":"Tractable and Provably Efficient Distributional Reinforcement Learning\n  with General Value Function Approximation","authors":"Taehyun Cho, Seungyub Han, Kyungjae Lee, Seokhun Ju, Dohyeong Kim,\n  Jungwoo Lee","authorsParsed":[["Cho","Taehyun",""],["Han","Seungyub",""],["Lee","Kyungjae",""],["Ju","Seokhun",""],["Kim","Dohyeong",""],["Lee","Jungwoo",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 00:43:51 GMT"}],"updateDate":"2024-08-01","timestamp":1722386631000,"abstract":"  Distributional reinforcement learning improves performance by effectively\ncapturing environmental stochasticity, but a comprehensive theoretical\nunderstanding of its effectiveness remains elusive. In this paper, we present a\nregret analysis for distributional reinforcement learning with general value\nfunction approximation in a finite episodic Markov decision process setting. We\nfirst introduce a key notion of Bellman unbiasedness for a tractable and\nexactly learnable update via statistical functional dynamic programming. Our\ntheoretical results show that approximating the infinite-dimensional return\ndistribution with a finite number of moment functionals is the only method to\nlearn the statistical information unbiasedly, including nonlinear statistical\nfunctionals. Second, we propose a provably efficient algorithm,\n$\\texttt{SF-LSVI}$, achieving a regret bound of $\\tilde{O}(d_E\nH^{\\frac{3}{2}}\\sqrt{K})$ where $H$ is the horizon, $K$ is the number of\nepisodes, and $d_E$ is the eluder dimension of a function class.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"q2O5rXzfBYn_EiHjib3nyMkdd_N339Ps3JhX-tPQ7Z8","pdfSize":"827458"}