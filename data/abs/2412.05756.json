{"id":"2412.05756","title":"Compositional Image Retrieval via Instruction-Aware Contrastive Learning","authors":"Wenliang Zhong, Weizhi An, Feng Jiang, Hehuan Ma, Yuzhi Guo, and\n  Junzhou Huang","authorsParsed":[["Zhong","Wenliang",""],["An","Weizhi",""],["Jiang","Feng",""],["Ma","Hehuan",""],["Guo","Yuzhi",""],["Huang","Junzhou",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 22:46:52 GMT"}],"updateDate":"2024-12-10","timestamp":1733611612000,"abstract":"  Composed Image Retrieval (CIR) involves retrieving a target image based on a\ncomposed query of an image paired with text that specifies modifications or\nchanges to the visual reference. CIR is inherently an instruction-following\ntask, as the model needs to interpret and apply modifications to the image. In\npractice, due to the scarcity of annotated data in downstream tasks, Zero-Shot\nCIR (ZS-CIR) is desirable. While existing ZS-CIR models based on CLIP have\nshown promising results, their capability in interpreting and following\nmodification instructions remains limited. Some research attempts to address\nthis by incorporating Large Language Models (LLMs). However, these approaches\nstill face challenges in effectively integrating multimodal information and\ninstruction understanding. To tackle above challenges, we propose a novel\nembedding method utilizing an instruction-tuned Multimodal LLM (MLLM) to\ngenerate composed representation, which significantly enhance the instruction\nfollowing capability for a comprehensive integration between images and\ninstructions. Nevertheless, directly applying MLLMs introduces a new challenge\nsince MLLMs are primarily designed for text generation rather than embedding\nextraction as required in CIR. To address this, we introduce a two-stage\ntraining strategy to efficiently learn a joint multimodal embedding space and\nfurther refining the ability to follow modification instructions by tuning the\nmodel in a triplet dataset similar to the CIR format. Extensive experiments on\nfour public datasets: FashionIQ, CIRR, GeneCIS, and CIRCO demonstrates the\nsuperior performance of our model, outperforming state-of-the-art baselines by\na significant margin. Codes are available at the GitHub repository.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pUqDKWF_1KSDHjqltu6za2d307qAWc_N5QofqCj7j7U","pdfSize":"2836369"}