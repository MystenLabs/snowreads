{"id":"2412.10558","title":"Too Big to Fool: Resisting Deception in Language Models","authors":"Mohammad Reza Samsami, Mats Leon Richter, Juan Rodriguez, Megh\n  Thakkar, Sarath Chandar, Maxime Gasse","authorsParsed":[["Samsami","Mohammad Reza",""],["Richter","Mats Leon",""],["Rodriguez","Juan",""],["Thakkar","Megh",""],["Chandar","Sarath",""],["Gasse","Maxime",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 21:03:10 GMT"}],"updateDate":"2024-12-17","timestamp":1734123790000,"abstract":"  Large language models must balance their weight-encoded knowledge with\nin-context information from prompts to generate accurate responses. This paper\ninvestigates this interplay by analyzing how models of varying capacities\nwithin the same family handle intentionally misleading in-context information.\nOur experiments demonstrate that larger models exhibit higher resilience to\ndeceptive prompts, showcasing an advanced ability to interpret and integrate\nprompt information with their internal knowledge. Furthermore, we find that\nlarger models outperform smaller ones in following legitimate instructions,\nindicating that their resilience is not due to disregarding in-context\ninformation. We also show that this phenomenon is likely not a result of\nmemorization but stems from the models' ability to better leverage implicit\ntask-relevant information from the prompt alongside their internally stored\nknowledge.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eoPbDB8nFaCxCwZaIaLKaasK-TF9B5oUEW3CXJ6R59M","pdfSize":"914308"}