{"id":"2407.07595","title":"Scaling Law in Neural Data: Non-Invasive Speech Decoding with 175 Hours\n  of EEG Data","authors":"Motoshige Sato, Kenichi Tomeoka, Ilya Horiguchi, Kai Arulkumaran,\n  Ryota Kanai, Shuntaro Sasai","authorsParsed":[["Sato","Motoshige",""],["Tomeoka","Kenichi",""],["Horiguchi","Ilya",""],["Arulkumaran","Kai",""],["Kanai","Ryota",""],["Sasai","Shuntaro",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 12:29:01 GMT"}],"updateDate":"2024-07-11","timestamp":1720614541000,"abstract":"  Brain-computer interfaces (BCIs) hold great potential for aiding individuals\nwith speech impairments. Utilizing electroencephalography (EEG) to decode\nspeech is particularly promising due to its non-invasive nature. However,\nrecordings are typically short, and the high variability in EEG data has led\nresearchers to focus on classification tasks with a few dozen classes. To\nassess its practical applicability for speech neuroprostheses, we investigate\nthe relationship between the size of EEG data and decoding accuracy in the open\nvocabulary setting. We collected extensive EEG data from a single participant\n(175 hours) and conducted zero-shot speech segment classification using\nself-supervised representation learning. The model trained on the entire\ndataset achieved a top-1 accuracy of 48\\% and a top-10 accuracy of 76\\%, while\nmitigating the effects of myopotential artifacts. Conversely, when the data was\nlimited to the typical amount used in practice ($\\sim$10 hours), the top-1\naccuracy dropped to 2.5\\%, revealing a significant scaling effect.\nAdditionally, as the amount of training data increased, the EEG latent\nrepresentation progressively exhibited clearer temporal structures of spoken\nphrases. This indicates that the decoder can recognize speech segments in a\ndata-driven manner without explicit measurements of word recognition. This\nresearch marks a significant step towards the practical realization of\nEEG-based speech BCIs.\n","subjects":["Quantitative Biology/Neurons and Cognition","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"FkJVDt7NdwwAKmq-7slXfLBEhr7dyMzqncuCfSYBaRA","pdfSize":"2641774","objectId":"0x6ff3830f6cb97f902550ed99f1917542b7894dc68bbef3bdbdc895eaca05e4ff","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
