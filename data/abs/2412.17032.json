{"id":"2412.17032","title":"MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on\n  New and Tail Knowledge","authors":"Jie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, Jeff Z. Pan","authorsParsed":[["He","Jie",""],["Hu","Nan",""],["Long","Wanqiu",""],["Chen","Jiaoyan",""],["Pan","Jeff Z.",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 14:17:12 GMT"},{"version":"v2","created":"Tue, 28 Jan 2025 16:28:10 GMT"}],"updateDate":"2025-01-29","timestamp":1734877032000,"abstract":"  Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks but face significant challenges with complex,\nknowledge-intensive multi-hop queries, particularly those involving new or\nlong-tail knowledge. Existing benchmarks often fail to fully address these\nchallenges. To bridge this gap, we introduce MINTQA (Multi-hop Question\nAnswering on New and Tail Knowledge), a comprehensive benchmark to evaluate\nLLMs' capabilities in multi-hop reasoning across four critical dimensions:\nquestion handling strategy, sub-question generation, retrieval-augmented\ngeneration, and iterative or dynamic decomposition and retrieval. MINTQA\ncomprises 10,479 question-answer pairs for evaluating new knowledge and 17,887\npairs for assessing long-tail knowledge, with each question equipped with\ncorresponding sub-questions and answers. Our systematic evaluation of 22\nstate-of-the-art LLMs on MINTQA reveals significant limitations in their\nability to handle complex knowledge base queries, particularly in handling new\nor unpopular knowledge. Our findings highlight critical challenges and offer\ninsights for advancing multi-hop reasoning capabilities. The MINTQA benchmark\nis available at https://github.com/probe2/multi-hop/.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"Elo2_9graHVmOu-DMwHF_0UrlmlzmvZ2MFI6DRlPnOI","pdfSize":"16942040"}