{"id":"2407.04255","title":"Second Place Solution of WSDM2023 Toloka Visual Question Answering\n  Challenge","authors":"Xiangyu Wu, Zhouyang Chi, Yang Yang, Jianfeng Lu","authorsParsed":[["Wu","Xiangyu",""],["Chi","Zhouyang",""],["Yang","Yang",""],["Lu","Jianfeng",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 04:56:05 GMT"}],"updateDate":"2024-07-08","timestamp":1720155365000,"abstract":"  In this paper, we present our solution for the WSDM2023 Toloka Visual\nQuestion Answering Challenge. Inspired by the application of multimodal\npre-trained models to various downstream tasks(e.g., visual question answering,\nvisual grounding, and cross-modal retrieval), we approached this competition as\na visual grounding task, where the input is an image and a question, guiding\nthe model to answer the question and display the answer as a bounding box on\nthe image. We designed a three-stage solution for this task. Specifically, we\nused the visual-language pre-trained model OFA as the foundation. In the first\nstage, we constructed a large-scale synthetic dataset similar to the\ncompetition dataset and coarse-tuned the model to learn generalized semantic\ninformation. In the second stage, we treated the competition task as a visual\ngrounding task, loaded the weights from the previous stage, and continued to\nfine-tune the model on the competition dataset, transferring the semantic\ninformation learned in the first stage to the competition task. Finally, we\ndesigned a bounding box matching and replacing post-processing strategy to\ncorrect the model's prediction results. Our team achieved a score of 76.342 on\nthe final leaderboard, ranking second.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZbdkIUp2VGkHQ_cdcYbywOX--VWibRXetPTBeUOYBGM","pdfSize":"517979"}