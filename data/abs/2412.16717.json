{"id":"2412.16717","title":"GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space","authors":"Souhaib Attaiki, Paul Guerrero, Duygu Ceylan, Niloy J. Mitra, Maks\n  Ovsjanikov","authorsParsed":[["Attaiki","Souhaib",""],["Guerrero","Paul",""],["Ceylan","Duygu",""],["Mitra","Niloy J.",""],["Ovsjanikov","Maks",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 17:59:17 GMT"}],"updateDate":"2024-12-24","timestamp":1734803957000,"abstract":"  We train a feed-forward text-to-3D diffusion generator for human characters\nusing only single-view 2D data for supervision. Existing 3D generative models\ncannot yet match the fidelity of image or video generative models.\nState-of-the-art 3D generators are either trained with explicit 3D supervision\nand are thus limited by the volume and diversity of existing 3D data.\nMeanwhile, generators that can be trained with only 2D data as supervision\ntypically produce coarser results, cannot be text-conditioned, or must revert\nto test-time optimization. We observe that GAN- and diffusion-based generators\nhave complementary qualities: GANs can be trained efficiently with 2D\nsupervision to produce high-quality 3D objects but are hard to condition on\ntext. In contrast, denoising diffusion models can be conditioned efficiently\nbut tend to be hard to train with only 2D supervision. We introduce GANFusion,\nwhich starts by generating unconditional triplane features for 3D data using a\nGAN architecture trained with only single-view 2D data. We then generate random\nsamples from the GAN, caption them, and train a text-conditioned diffusion\nmodel that directly learns to sample from the space of good triplane features\nthat can be decoded into 3D objects.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"_4BoXb3AqGAROQukF-YkGYAO7Fey4Jnxov2mhaKFxs8","pdfSize":"2986789"}