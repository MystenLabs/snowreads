{"id":"2407.07630","title":"A Review of the Challenges with Massive Web-mined Corpora Used in Large\n  Language Models Pre-Training","authors":"Micha{\\l} Pere{\\l}kiewicz and Rafa{\\l} Po\\'swiata","authorsParsed":[["Perełkiewicz","Michał",""],["Poświata","Rafał",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 13:09:23 GMT"}],"updateDate":"2024-07-11","timestamp":1720616963000,"abstract":"  This article presents a comprehensive review of the challenges associated\nwith using massive web-mined corpora for the pre-training of large language\nmodels (LLMs). This review identifies key challenges in this domain, including\nchallenges such as noise (irrelevant or misleading information), duplication of\ncontent, the presence of low-quality or incorrect information, biases, and the\ninclusion of sensitive or personal information in web-mined corpora. Addressing\nthese issues is crucial for the development of accurate, reliable, and\nethically responsible language models. Through an examination of current\nmethodologies for data cleaning, pre-processing, bias detection and mitigation,\nwe highlight the gaps in existing approaches and suggest directions for future\nresearch. Our discussion aims to catalyze advancements in developing more\nsophisticated and ethically responsible LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"angUOz2TSshhXES5OVccvx-tomR0KKwWrno_me9PNvs","pdfSize":"182214"}