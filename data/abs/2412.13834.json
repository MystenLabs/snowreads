{
  "id": "2412.13834",
  "title": "Maybe you are looking for CroQS: Cross-modal Query Suggestion for\n  Text-to-Image Retrieval",
  "authors": "Giacomo Pacini, Fabio Carrara, Nicola Messina, Nicola Tonellotto,\n  Giuseppe Amato and Fabrizio Falchi",
  "authorsParsed": [
    [
      "Pacini",
      "Giacomo",
      ""
    ],
    [
      "Carrara",
      "Fabio",
      ""
    ],
    [
      "Messina",
      "Nicola",
      ""
    ],
    [
      "Tonellotto",
      "Nicola",
      ""
    ],
    [
      "Amato",
      "Giuseppe",
      ""
    ],
    [
      "Falchi",
      "Fabrizio",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 13:24:09 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734528249000,
  "abstract": "  Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/\n",
  "subjects": [
    "Computer Science/Information Retrieval",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "E5n05iFdnSCjNGwXHjjmrm_Jl4ndB9fRps1z1rMZO1M",
  "pdfSize": "4010190"
}