{
  "id": "2412.00911",
  "title": "SOUL: A Semi-supervised Open-world continUal Learning method for Network\n  Intrusion Detection",
  "authors": "Suresh Kumar Amalapuram, Shreya Kumar, Bheemarjuna Reddy Tamma,\n  Sumohana Channappayya",
  "authorsParsed": [
    [
      "Amalapuram",
      "Suresh Kumar",
      ""
    ],
    [
      "Kumar",
      "Shreya",
      ""
    ],
    [
      "Tamma",
      "Bheemarjuna Reddy",
      ""
    ],
    [
      "Channappayya",
      "Sumohana",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 1 Dec 2024 17:57:34 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733075854000,
  "abstract": "  Fully supervised continual learning methods have shown improved attack\ntraffic detection in a closed-world learning setting. However, obtaining fully\nannotated data is an arduous task in the security domain. Further, our research\nfinds that after training a classifier on two days of network traffic, the\nperformance decay of attack class detection over time (computed using the area\nunder the time on precision-recall AUC of the attack class) drops from 0.985 to\n0.506 on testing with three days of new test samples. In this work, we focus on\nlabel scarcity and open-world learning (OWL) settings to improve the attack\nclass detection of the continual learning-based network intrusion detection\n(NID). We formulate OWL for NID as a semi-supervised continual learning-based\nmethod, dubbed SOUL, to achieve the classifier performance on par with fully\nsupervised models while using limited annotated data. The proposed method is\nmotivated by our empirical observation that using gradient projection memory\n(constructed using buffer memory samples) can significantly improve the\ndetection performance of the attack (minority) class when trained using\npartially labeled data. Further, using the classifier's confidence in\nconjunction with buffer memory, SOUL generates high-confidence labels whenever\nit encounters OWL tasks closer to seen tasks, thus acting as a label generator.\nInterestingly, SOUL efficiently utilizes samples in the buffer memory for\nsample replay to avoid catastrophic forgetting, construct the projection\nmemory, and assist in generating labels for unseen tasks. The proposed method\nis evaluated on four standard network intrusion detection datasets, and the\nperformance results are closer to the fully supervised baselines using at most\n20% labeled data while reducing the data annotation effort in the range of 11\nto 45% for unseen data.\n",
  "subjects": [
    "Computer Science/Cryptography and Security"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "J-g0vO5Zc1l_hsaXXP8hAaid0Q2RtIyEctVMzf76MgM",
  "pdfSize": "618290"
}