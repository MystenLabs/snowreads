{"id":"2412.09722","title":"GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong\n  Prompt Optimizers","authors":"Sarkar Snigdha Sarathi Das, Ryo Kamoi, Bo Pang, Yusen Zhang, Caiming\n  Xiong, Rui Zhang","authorsParsed":[["Das","Sarkar Snigdha Sarathi",""],["Kamoi","Ryo",""],["Pang","Bo",""],["Zhang","Yusen",""],["Xiong","Caiming",""],["Zhang","Rui",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 20:59:43 GMT"}],"updateDate":"2024-12-16","timestamp":1734037183000,"abstract":"  The effectiveness of large language models (LLMs) is closely tied to the\ndesign of prompts, making prompt optimization essential for enhancing their\nperformance across a wide range of tasks. Many existing approaches to\nautomating prompt engineering rely exclusively on textual feedback, refining\nprompts based solely on inference errors identified by large, computationally\nexpensive LLMs. Unfortunately, smaller models struggle to generate high-quality\nfeedback, resulting in complete dependence on large LLM judgment. Moreover,\nthese methods fail to leverage more direct and finer-grained information, such\nas gradients, due to operating purely in text space. To this end, we introduce\nGReaTer, a novel prompt optimization technique that directly incorporates\ngradient information over task-specific reasoning. By utilizing task loss\ngradients, GReaTer enables self-optimization of prompts for open-source,\nlightweight language models without the need for costly closed-source LLMs.\nThis allows high-performance prompt optimization without dependence on massive\nLLMs, closing the gap between smaller models and the sophisticated reasoning\noften needed for prompt refinement. Extensive evaluations across diverse\nreasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer\nconsistently outperforms previous state-of-the-art prompt optimization methods,\neven those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts\nfrequently exhibit better transferability and, in some cases, boost task\nperformance to levels comparable to or surpassing those achieved by larger\nlanguage models, highlighting the effectiveness of prompt optimization guided\nby gradients over reasoning. Code of GReaTer is available at\nhttps://github.com/psunlpgroup/GreaTer.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"gPa4TVqtmNj9QB8gPM3mJKMcMN_qgr0DMD_xmFs_Wug","pdfSize":"1636779"}