{
  "id": "2412.13023",
  "title": "Relational Neurosymbolic Markov Models",
  "authors": "Lennert De Smet, Gabriele Venturato, Luc De Raedt, Giuseppe Marra",
  "authorsParsed": [
    [
      "De Smet",
      "Lennert",
      ""
    ],
    [
      "Venturato",
      "Gabriele",
      ""
    ],
    [
      "De Raedt",
      "Luc",
      ""
    ],
    [
      "Marra",
      "Giuseppe",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 15:41:51 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734450111000,
  "abstract": "  Sequential problems are ubiquitous in AI, such as in reinforcement learning\nor natural language processing. State-of-the-art deep sequential models, like\ntransformers, excel in these settings but fail to guarantee the satisfaction of\nconstraints necessary for trustworthy deployment. In contrast, neurosymbolic AI\n(NeSy) provides a sound formalism to enforce constraints in deep probabilistic\nmodels but scales exponentially on sequential problems. To overcome these\nlimitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a\nnew class of end-to-end differentiable sequential models that integrate and\nprovably satisfy relational logical constraints. We propose a strategy for\ninference and learning that scales on sequential settings, and that combines\napproximate Bayesian inference, automated reasoning, and gradient estimation.\nOur experiments show that NeSy-MMs can solve problems beyond the current\nstate-of-the-art in neurosymbolic AI and still provide strong guarantees with\nrespect to desired properties. Moreover, we show that our models are more\ninterpretable and that constraints can be adapted at test time to\nout-of-distribution scenarios.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "hzCdmlcHvnWlLo4r2xz0GNXS6NcTN2WXJgPFT9SFfY0",
  "pdfSize": "5916813"
}