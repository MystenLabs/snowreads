{
  "id": "2412.05819",
  "title": "[CLS] Token Tells Everything Needed for Training-free Efficient MLLMs",
  "authors": "Ao Wang, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, Guiguang Ding",
  "authorsParsed": [
    [
      "Wang",
      "Ao",
      ""
    ],
    [
      "Sun",
      "Fengyuan",
      ""
    ],
    [
      "Chen",
      "Hui",
      ""
    ],
    [
      "Lin",
      "Zijia",
      ""
    ],
    [
      "Han",
      "Jungong",
      ""
    ],
    [
      "Ding",
      "Guiguang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 8 Dec 2024 05:29:39 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733635779000,
  "abstract": "  Multimodal Large Language Models (MLLMs) have recently demonstrated strong\nperformance across a wide range of vision-language tasks, garnering significant\nattention in the computer vision. However, their efficient deployment remains a\nsubstantial challenge due to high computational costs and memory requirements.\nRecognizing the redundancy of information within the vision modality, recent\nstudies have explored methods for compressing visual tokens in MLLMs to enhance\nefficiency in a training-free manner. Despite their effectiveness, existing\nmethods like Fast rely on the attention between visual tokens and prompt text\ntokens as the importance indicator, overlooking the relevance to response text\nand thus introducing perception bias. In this paper, we demonstrate that in\nMLLMs, the [CLS] token in the visual encoder inherently knows which visual\ntokens are important for MLLMs. Building on this prior, we introduce a simple\nyet effective method for train-free visual token compression, called VTC-CLS.\nFirstly, it leverages the attention score of the [CLS] token on visual tokens\nas an importance indicator for pruning visual tokens. Besides, we also explore\nensembling the importance scores derived by the [CLS] token from different\nlayers to capture the key visual information more comprehensively. Extensive\nexperiments demonstrate that our VTC-CLS achieves the state-of-the-art\nperformance across various tasks compared with baseline methods. It also brings\nnotably less computational costs in a training-free manner, highlighting its\neffectiveness and superiority. Code and models are available at\n\\url{https://github.com/THU-MIG/VTC-CLS}.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "V83LW7VRtFxtQ0iPvoTdrwJDF9i6ApwgPz0hP5Pwdq0",
  "pdfSize": "1168112"
}