{"id":"2407.19203","title":"Towards Clean-Label Backdoor Attacks in the Physical World","authors":"Thinh Dao, Cuong Chi Le, Khoa D Doan, Kok-Seng Wong","authorsParsed":[["Dao","Thinh",""],["Le","Cuong Chi",""],["Doan","Khoa D",""],["Wong","Kok-Seng",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 08:13:07 GMT"}],"updateDate":"2024-07-30","timestamp":1722067987000,"abstract":"  Deep Neural Networks (DNNs) are vulnerable to backdoor poisoning attacks,\nwith most research focusing on digital triggers, special patterns digitally\nadded to test-time inputs to induce targeted misclassification. In contrast,\nphysical triggers, which are natural objects within a physical scene, have\nemerged as a desirable alternative since they enable real-time backdoor\nactivations without digital manipulation. However, current physical attacks\nrequire that poisoned inputs have incorrect labels, making them easily\ndetectable upon human inspection. In this paper, we collect a facial dataset of\n21,238 images with 7 common accessories as triggers and use it to study the\nthreat of clean-label backdoor attacks in the physical world. Our study reveals\ntwo findings. First, the success of physical attacks depends on the poisoning\nalgorithm, physical trigger, and the pair of source-target classes. Second,\nalthough clean-label poisoned samples preserve ground-truth labels, their\nperceptual quality could be seriously degraded due to conspicuous artifacts in\nthe images. Such samples are also vulnerable to statistical filtering methods\nbecause they deviate from the distribution of clean samples in the feature\nspace. To address these issues, we propose replacing the standard $\\ell_\\infty$\nregularization with a novel pixel regularization and feature regularization\nthat could enhance the imperceptibility of poisoned samples without\ncompromising attack performance. Our study highlights accidental backdoor\nactivations as a key limitation of clean-label physical backdoor attacks. This\nhappens when unintended objects or classes accidentally cause the model to\nmisclassify as the target class.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mmz4rgFVuUu9ciHiSxz0R3K_7vXsjikbt-wXgvLliP0","pdfSize":"39123276"}