{"id":"2407.04439","title":"XLSR-Transducer: Streaming ASR for Self-Supervised Pretrained Models","authors":"Shashi Kumar, Srikanth Madikeri, Juan Zuluaga-Gomez, Esa\\'u\n  Villatoro-Tello, Iuliia Nigmatulina, Petr Motlicek, Manjunath K E, Aravind\n  Ganapathiraju","authorsParsed":[["Kumar","Shashi",""],["Madikeri","Srikanth",""],["Zuluaga-Gomez","Juan",""],["Villatoro-Tello","Esa√∫",""],["Nigmatulina","Iuliia",""],["Motlicek","Petr",""],["E","Manjunath K",""],["Ganapathiraju","Aravind",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 11:41:11 GMT"}],"updateDate":"2024-07-08","timestamp":1720179671000,"abstract":"  Self-supervised pretrained models exhibit competitive performance in\nautomatic speech recognition on finetuning, even with limited in-domain\nsupervised data for training. However, popular pretrained models are not\nsuitable for streaming ASR because they are trained with full attention\ncontext. In this paper, we introduce XLSR-Transducer, where the XLSR-53 model\nis used as encoder in transducer setup. Our experiments on the AMI dataset\nreveal that the XLSR-Transducer achieves 4% absolute WER improvement over\nWhisper large-v2 and 8% over a Zipformer transducer model trained from\nscratch.To enable streaming capabilities, we investigate different attention\nmasking patterns in the self-attention computation of transformer layers within\nthe XLSR-53 model. We validate XLSR-Transducer on AMI and 5 languages from\nCommonVoice under low-resource scenarios. Finally, with the introduction of\nattention sinks, we reduce the left context by half while achieving a relative\n12% improvement in WER.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"oKTJr5Qij2G-bhtGtMdd1tQbgt-mHZKi8tufz8oNbDM","pdfSize":"507984"}