{"id":"2412.19444","title":"Towards Simple and Provable Parameter-Free Adaptive Gradient Methods","authors":"Yuanzhe Tao, Huizhuo Yuan, Xun Zhou, Yuan Cao, Quanquan Gu","authorsParsed":[["Tao","Yuanzhe",""],["Yuan","Huizhuo",""],["Zhou","Xun",""],["Cao","Yuan",""],["Gu","Quanquan",""]],"versions":[{"version":"v1","created":"Fri, 27 Dec 2024 04:22:02 GMT"}],"updateDate":"2024-12-30","timestamp":1735273322000,"abstract":"  Optimization algorithms such as AdaGrad and Adam have significantly advanced\nthe training of deep models by dynamically adjusting the learning rate during\nthe optimization process. However, adhoc tuning of learning rates poses a\nchallenge, leading to inefficiencies in practice. To address this issue, recent\nresearch has focused on developing \"learning-rate-free\" or \"parameter-free\"\nalgorithms that operate effectively without the need for learning rate tuning.\nDespite these efforts, existing parameter-free variants of AdaGrad and Adam\ntend to be overly complex and/or lack formal convergence guarantees. In this\npaper, we present AdaGrad++ and Adam++, novel and simple parameter-free\nvariants of AdaGrad and Adam with convergence guarantees. We prove that\nAdaGrad++ achieves comparable convergence rates to AdaGrad in convex\noptimization without predefined learning rate assumptions. Similarly, Adam++\nmatches the convergence rate of Adam without relying on any conditions on the\nlearning rates. Experimental results across various deep learning tasks\nvalidate the competitive performance of AdaGrad++ and Adam++.\n","subjects":["Computer Science/Machine Learning","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"m5fKsigdSc7vY50qMYBYpDgA-k5LrkVJXlEX6PBFtrE","pdfSize":"2895439"}