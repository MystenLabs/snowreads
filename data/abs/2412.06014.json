{"id":"2412.06014","title":"Post-hoc Probabilistic Vision-Language Models","authors":"Anton Baumann, Rui Li, Marcus Klasson, Santeri Mentu, Shyamgopal\n  Karthik, Zeynep Akata, Arno Solin, Martin Trapp","authorsParsed":[["Baumann","Anton",""],["Li","Rui",""],["Klasson","Marcus",""],["Mentu","Santeri",""],["Karthik","Shyamgopal",""],["Akata","Zeynep",""],["Solin","Arno",""],["Trapp","Martin",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 18:16:13 GMT"}],"updateDate":"2024-12-10","timestamp":1733681773000,"abstract":"  Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable\nsuccess in classification, retrieval, and generative tasks. For this, VLMs\ndeterministically map images and text descriptions to a joint latent space in\nwhich their similarity is assessed using the cosine similarity. However, a\ndeterministic mapping of inputs fails to capture uncertainties over concepts\narising from domain shifts when used in downstream tasks. In this work, we\npropose post-hoc uncertainty estimation in VLMs that does not require\nadditional training. Our method leverages a Bayesian posterior approximation\nover the last layers in VLMs and analytically quantifies uncertainties over\ncosine similarities. We demonstrate its effectiveness for uncertainty\nquantification and support set selection in active learning. Compared to\nbaselines, we obtain improved and well-calibrated predictive uncertainties,\ninterpretable uncertainty estimates, and sample-efficient active learning. Our\nresults show promise for safety-critical applications of large-scale models.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"m_Lw5oeyUi_qwM-iuCGJOTJacn3vMzYeRk1HoSAPR0g","pdfSize":"953775"}