{"id":"2412.03230","title":"PERL: Pinyin Enhanced Rephrasing Language Model for Chinese ASR N-best\n  Error Correction","authors":"Junhong Liang","authorsParsed":[["Liang","Junhong",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 11:28:52 GMT"}],"updateDate":"2024-12-05","timestamp":1733311732000,"abstract":"  ASR correction methods have predominantly focused on general datasets and\nhave not effectively utilized Pinyin information, unique to the Chinese\nlanguage. In this study, we address this gap by proposing a Pinyin Enhanced\nRephrasing Language Model (PERL), specifically designed for N-best correction\nscenarios. Additionally, we implement a length predictor module to address the\nvariable-length problem. We conduct experiments on the Aishell-1 dataset and\nour newly proposed DoAD dataset. The results show that our approach outperforms\nbaseline methods, achieving a 29.11% reduction in Character Error Rate (CER) on\nAishell-1 and around 70% CER reduction on domain-specific datasets.\nFurthermore, our approach leverages Pinyin similarity at the token level,\nproviding an advantage over baselines and leading to superior performance.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LNlQlwvOEutMdccZbvb7j61zaKDVM_aK0WjrjGkPi4U","pdfSize":"486941"}