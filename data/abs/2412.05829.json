{"id":"2412.05829","title":"SABER: Model-agnostic Backdoor Attack on Chain-of-Thought in Neural Code\n  Generation","authors":"Naizhu Jin, Zhong Li, Yinggang Guo, Chao Su, Tian Zhang and Qingkai\n  Zeng","authorsParsed":[["Jin","Naizhu",""],["Li","Zhong",""],["Guo","Yinggang",""],["Su","Chao",""],["Zhang","Tian",""],["Zeng","Qingkai",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 06:36:00 GMT"}],"updateDate":"2024-12-10","timestamp":1733639760000,"abstract":"  Recent studies have proposed integrating Chain-of-Thought (CoT) reasoning to\nfurther enhance the reliability of Code Language Models (CLMs) in generating\ncode, a step-by-step approach that breaks down complex programming tasks into\nmanageable sub-problems. Advances in this area have introduced CoT models,\nspecifically designed to integrate CoT reasoning effectively into language\nmodels, achieving notable improvements in code generation. Despite these\nadvancements, the security of CoT models has not been systematically studied.\nIn this study, we aim to fill this gap by investigating the vulnerability of\nCoT models to backdoor injection in code generation tasks. To address this, we\npropose a model-agnostic backdoor attack method SABER\n(\\textbf{S}elf-\\textbf{A}ttention-\\textbf{B}as\\textbf{E}d backdoo\\textbf{R})\nbased on the self-attention mechanism. SABER begins by selecting a malicious\noutput as the backdoor using code mutation operations. It then identifies\ntokens most relevant to poisoned content by analyzing self-attention scores in\nthe CodeBERT model. Finally, it applies semantic-preserving perturbations to\ngenerate adaptive and natural triggers. Our experiments on HumanEval-CoT and\nOpenEval-CoT test sets demonstrate that CoT models are susceptible to backdoor\nattacks via data poisoning. Taking the OpenEval-CoT dataset as an example,\nSABER achieves an ASR of 76.19%, representing an improvement of 14.29% over\nRIPPLe and a substantial 23.08% enhancement compared to BadPre. Further\nevaluations using ONION for automated detection and human studies reveal that\nSABER is stealthier and harder to detect, bypassing 77.27% of automated\ndetection, with a human detection rate of just 3.17%. Our findings reveal that\nbackdoors can be injected into CoT models to manipulate downstream code\ngeneration tasks.\n","subjects":["Computer Science/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wg6hSvoVklW4scvbZzqtlw4ynDTuNBwsTUj4nHL6xkI","pdfSize":"781322"}