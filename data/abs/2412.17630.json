{"id":"2412.17630","title":"Detail-Preserving Latent Diffusion for Stable Shadow Removal","authors":"Jiamin Xu, Yuxin Zheng, Zelong Li, Chi Wang, Renshu Gu, Weiwei Xu,\n  Gang Xu","authorsParsed":[["Xu","Jiamin",""],["Zheng","Yuxin",""],["Li","Zelong",""],["Wang","Chi",""],["Gu","Renshu",""],["Xu","Weiwei",""],["Xu","Gang",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 15:06:46 GMT"}],"updateDate":"2024-12-24","timestamp":1734966406000,"abstract":"  Achieving high-quality shadow removal with strong generalizability is\nchallenging in scenes with complex global illumination. Due to the limited\ndiversity in shadow removal datasets, current methods are prone to overfitting\ntraining data, often leading to reduced performance on unseen cases. To address\nthis, we leverage the rich visual priors of a pre-trained Stable Diffusion (SD)\nmodel and propose a two-stage fine-tuning pipeline to adapt the SD model for\nstable and efficient shadow removal. In the first stage, we fix the VAE and\nfine-tune the denoiser in latent space, which yields substantial shadow removal\nbut may lose some high-frequency details. To resolve this, we introduce a\nsecond stage, called the detail injection stage. This stage selectively\nextracts features from the VAE encoder to modulate the decoder, injecting fine\ndetails into the final results. Experimental results show that our method\noutperforms state-of-the-art shadow removal techniques. The cross-dataset\nevaluation further demonstrates that our method generalizes effectively to\nunseen data, enhancing the applicability of shadow removal methods.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uJxKRVDsC6C51J-62wbF_ihrSEXwU8j22-KR5Tw9Mek","pdfSize":"4552907"}