{
  "id": "2412.03150",
  "title": "Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis",
  "authors": "Siyoon Jin, Jisu Nam, Jiyoung Kim, Dahyun Chung, Yeong-Seok Kim,\n  Joonhyung Park, Heonjeong Chu, Seungryong Kim",
  "authorsParsed": [
    [
      "Jin",
      "Siyoon",
      ""
    ],
    [
      "Nam",
      "Jisu",
      ""
    ],
    [
      "Kim",
      "Jiyoung",
      ""
    ],
    [
      "Chung",
      "Dahyun",
      ""
    ],
    [
      "Kim",
      "Yeong-Seok",
      ""
    ],
    [
      "Park",
      "Joonhyung",
      ""
    ],
    [
      "Chu",
      "Heonjeong",
      ""
    ],
    [
      "Kim",
      "Seungryong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 09:17:47 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733303867000,
  "abstract": "  Exemplar-based semantic image synthesis aims to generate images aligned with\ngiven semantic content while preserving the appearance of an exemplar image.\nConventional structure-guidance models, such as ControlNet, are limited in that\nthey cannot directly utilize exemplar images as input, relying instead solely\non text prompts to control appearance. Recent tuning-free approaches address\nthis limitation by transferring local appearance from the exemplar image to the\nsynthesized image through implicit cross-image matching in the augmented\nself-attention mechanism of pre-trained diffusion models. However, these\nmethods face challenges when applied to content-rich scenes with significant\ngeometric deformations, such as driving scenes. In this paper, we propose the\nAppearance Matching Adapter (AM-Adapter), a learnable framework that enhances\ncross-image matching within augmented self-attention by incorporating semantic\ninformation from segmentation maps. To effectively disentangle generation and\nmatching processes, we adopt a stage-wise training approach. Initially, we\ntrain the structure-guidance and generation networks, followed by training the\nAM-Adapter while keeping the other networks frozen. During inference, we\nintroduce an automated exemplar retrieval method to efficiently select exemplar\nimage-segmentation pairs. Despite utilizing a limited number of learnable\nparameters, our method achieves state-of-the-art performance, excelling in both\nsemantic alignment preservation and local appearance fidelity. Extensive\nablation studies further validate our design choices. Code and pre-trained\nweights will be publicly available.: https://cvlab-kaist.github.io/AM-Adapter/\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "1AW4njW9SLo2BdntfbBcSFGGphe_5ovimc2L0L47PJ8",
  "pdfSize": "26220722"
}