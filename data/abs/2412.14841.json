{
  "id": "2412.14841",
  "title": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis",
  "authors": "Greta Dolcetti, Vincenzo Arceri, Eleonora Iotti, Sergio Maffeis,\n  Agostino Cortesi, Enea Zaffanella",
  "authorsParsed": [
    [
      "Dolcetti",
      "Greta",
      ""
    ],
    [
      "Arceri",
      "Vincenzo",
      ""
    ],
    [
      "Iotti",
      "Eleonora",
      ""
    ],
    [
      "Maffeis",
      "Sergio",
      ""
    ],
    [
      "Cortesi",
      "Agostino",
      ""
    ],
    [
      "Zaffanella",
      "Enea",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 19 Dec 2024 13:34:14 GMT"
    },
    {
      "version": "v2",
      "created": "Tue, 7 Jan 2025 15:30:56 GMT"
    }
  ],
  "updateDate": "2025-01-08",
  "timestamp": 1734615254000,
  "abstract": "  Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools.\n",
  "subjects": [
    "Computer Science/Software Engineering",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Y_nNeZdCs--YvZko8MKv35IebyV2ZDUSE2eZ8iuV0jg",
  "pdfSize": "2986146"
}