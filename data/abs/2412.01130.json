{"id":"2412.01130","title":"Enhancing Function-Calling Capabilities in LLMs: Strategies for Prompt\n  Formats, Data Integration, and Multilingual Translation","authors":"Yi-Chang Chen and Po-Chun Hsu and Chan-Jan Hsu and Da-shan Shiu","authorsParsed":[["Chen","Yi-Chang",""],["Hsu","Po-Chun",""],["Hsu","Chan-Jan",""],["Shiu","Da-shan",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 05:10:41 GMT"},{"version":"v2","created":"Wed, 4 Dec 2024 03:34:42 GMT"}],"updateDate":"2024-12-05","timestamp":1733116241000,"abstract":"  Large language models (LLMs) have significantly advanced autonomous agents,\nparticularly in zero-shot tool usage, also known as function calling. This\nresearch delves into enhancing the function-calling capabilities of LLMs by\nexploring different approaches, including prompt formats for integrating\nfunction descriptions, blending function-calling and instruction-following\ndata, introducing a novel Decision Token for conditional prompts, leveraging\nchain-of-thought reasoning, and overcoming multilingual challenges with a\ntranslation pipeline. Our key findings and contributions are as follows: (1)\nInstruction-following data improves both function-calling accuracy and\nrelevance detection. (2) The use of the newly proposed Decision Token, combined\nwith synthetic non-function-call data, enhances relevance detection. (3) A\ntailored translation pipeline effectively overcomes multilingual limitations,\ndemonstrating significant improvements in Traditional Chinese. These insights\nhighlight the potential for improved function-calling capabilities and\nmultilingual applications in LLMs.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WdmIVKzzMKnp19dmWWDOmk-3VPYM25ofrQuf6fSG1sg","pdfSize":"361191"}