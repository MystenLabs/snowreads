{"id":"2412.03673","title":"Interpreting Transformers for Jet Tagging","authors":"Aaron Wang, Abhijith Gandrakota, Jennifer Ngadiuba, Vivekanand Sahu,\n  Priyansh Bhatnagar, Elham E Khoda, Javier Duarte","authorsParsed":[["Wang","Aaron",""],["Gandrakota","Abhijith",""],["Ngadiuba","Jennifer",""],["Sahu","Vivekanand",""],["Bhatnagar","Priyansh",""],["Khoda","Elham E",""],["Duarte","Javier",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 19:06:40 GMT"},{"version":"v2","created":"Mon, 9 Dec 2024 03:47:39 GMT"}],"updateDate":"2024-12-10","timestamp":1733339200000,"abstract":"  Machine learning (ML) algorithms, particularly attention-based transformer\nmodels, have become indispensable for analyzing the vast data generated by\nparticle physics experiments like ATLAS and CMS at the CERN LHC. Particle\nTransformer (ParT), a state-of-the-art model, leverages particle-level\nattention to improve jet-tagging tasks, which are critical for identifying\nparticles resulting from proton collisions. This study focuses on interpreting\nParT by analyzing attention heat maps and particle-pair correlations on the\n$\\eta$-$\\phi$ plane, revealing a binary attention pattern where each particle\nattends to at most one other particle. At the same time, we observe that ParT\nshows varying focus on important particles and subjets depending on decay,\nindicating that the model learns traditional jet substructure observables.\nThese insights enhance our understanding of the model's internal workings and\nlearning process, offering potential avenues for improving the efficiency of\ntransformer architectures in future high-energy physics applications.\n","subjects":["Physics/High Energy Physics - Phenomenology","Computer Science/Machine Learning","Physics/High Energy Physics - Experiment","Physics/Data Analysis, Statistics and Probability"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yyuo1nCMqMNBp9sk0AjyVGnbWF0Am8uW6BzYTj2HjKQ","pdfSize":"884917"}