{"id":"2407.21546","title":"Black box meta-learning intrinsic rewards for sparse-reward environments","authors":"Octavio Pappalardo, Rodrigo Ramele, Juan Miguel Santos","authorsParsed":[["Pappalardo","Octavio",""],["Ramele","Rodrigo",""],["Santos","Juan Miguel",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 12:09:33 GMT"}],"updateDate":"2024-08-01","timestamp":1722427773000,"abstract":"  Despite the successes and progress of deep reinforcement learning over the\nlast decade, several challenges remain that hinder its broader application.\nSome fundamental aspects to improve include data efficiency, generalization\ncapability, and ability to learn in sparse-reward environments, which often\nrequire human-designed dense rewards. Meta-learning has emerged as a promising\napproach to address these issues by optimizing components of the learning\nalgorithm to meet desired characteristics. Additionally, a different line of\nwork has extensively studied the use of intrinsic rewards to enhance the\nexploration capabilities of algorithms. This work investigates how\nmeta-learning can improve the training signal received by RL agents. The focus\nis on meta-learning intrinsic rewards under a framework that doesn't rely on\nthe use of meta-gradients. We analyze and compare this approach to the use of\nextrinsic rewards and a meta-learned advantage function. The developed\nalgorithms are evaluated on distributions of continuous control tasks with both\nparametric and non-parametric variations, and with only sparse rewards\naccessible for the evaluation tasks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YtPAHoMT5yc9yFY1dqtJ1C6Ekq4jR2nk0hLzbZ90YK0","pdfSize":"759465"}