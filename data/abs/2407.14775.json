{"id":"2407.14775","title":"Phase Re-service in Reinforcement Learning Traffic Signal Control","authors":"Zhiyao Zhang, George Gunter, Marcos Quinones-Grueiro, Yuhang Zhang,\n  William Barbour, Gautam Biswas, Daniel Work","authorsParsed":[["Zhang","Zhiyao",""],["Gunter","George",""],["Quinones-Grueiro","Marcos",""],["Zhang","Yuhang",""],["Barbour","William",""],["Biswas","Gautam",""],["Work","Daniel",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 06:52:38 GMT"},{"version":"v2","created":"Fri, 2 Aug 2024 04:07:36 GMT"}],"updateDate":"2024-08-05","timestamp":1721458358000,"abstract":"  This article proposes a novel approach to traffic signal control that\ncombines phase re-service with reinforcement learning (RL). The RL agent\ndirectly determines the duration of the next phase in a pre-defined sequence.\nBefore the RL agent's decision is executed, we use the shock wave theory to\nestimate queue expansion at the designated movement allowed for re-service and\ndecide if phase re-service is necessary. If necessary, a temporary phase\nre-service is inserted before the next regular phase. We formulate the RL\nproblem as a semi-Markov decision process (SMDP) and solve it with proximal\npolicy optimization (PPO). We conducted a series of experiments that showed\nsignificant improvements thanks to the introduction of phase re-service.\nVehicle delays are reduced by up to 29.95% of the average and up to 59.21% of\nthe standard deviation. The number of stops is reduced by 26.05% on average\nwith 45.77% less standard deviation.\n","subjects":["Electrical Engineering and Systems Science/Systems and Control","Computing Research Repository/Systems and Control"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"KwCElvGGn3bWbSTB2CISjzdZXky1iGQz4oA8GQcHN5o","pdfSize":"4834432"}