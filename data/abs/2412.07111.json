{"id":"2412.07111","title":"Predictable Emergent Abilities of LLMs: Proxy Tasks Are All You Need","authors":"Bo-Wen Zhang, Yan Yan, Boxiang Yang, Yifei Xue, Guang Liu","authorsParsed":[["Zhang","Bo-Wen",""],["Yan","Yan",""],["Yang","Boxiang",""],["Xue","Yifei",""],["Liu","Guang",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 01:56:30 GMT"}],"updateDate":"2024-12-11","timestamp":1733795790000,"abstract":"  While scaling laws optimize training configurations for large language models\n(LLMs) through experiments on smaller or early-stage models, they fail to\npredict emergent abilities due to the absence of such capabilities in these\nmodels. To address this, we propose a method that predicts emergent abilities\nby leveraging proxy tasks. We begin by establishing relevance metrics between\nthe target task and candidate tasks based on performance differences across\nmultiple models. These candidate tasks are then validated for robustness with\nsmall model ensembles, leading to the selection of the most appropriate proxy\ntasks. The predicted performance on the target task is then derived by\nintegrating the evaluation results of these proxies. In a case study on tool\nutilization capabilities, our method demonstrated a strong correlation between\npredicted and actual performance, confirming its effectiveness.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WAbi0ZA7tdO8HRxECvr8ncnDGHg5GMTsYwJzVkPV3NE","pdfSize":"558795"}