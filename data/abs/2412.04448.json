{
  "id": "2412.04448",
  "title": "MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation",
  "authors": "Longtao Zheng, Yifan Zhang, Hanzhong Guo, Jiachun Pan, Zhenxiong Tan,\n  Jiahao Lu, Chuanxin Tang, Bo An, Shuicheng Yan",
  "authorsParsed": [
    [
      "Zheng",
      "Longtao",
      ""
    ],
    [
      "Zhang",
      "Yifan",
      ""
    ],
    [
      "Guo",
      "Hanzhong",
      ""
    ],
    [
      "Pan",
      "Jiachun",
      ""
    ],
    [
      "Tan",
      "Zhenxiong",
      ""
    ],
    [
      "Lu",
      "Jiahao",
      ""
    ],
    [
      "Tang",
      "Chuanxin",
      ""
    ],
    [
      "An",
      "Bo",
      ""
    ],
    [
      "Yan",
      "Shuicheng",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 18:57:26 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733425046000,
  "abstract": "  Recent advances in video diffusion models have unlocked new potential for\nrealistic audio-driven talking video generation. However, achieving seamless\naudio-lip synchronization, maintaining long-term identity consistency, and\nproducing natural, audio-aligned expressions in generated talking videos remain\nsignificant challenges. To address these challenges, we propose Memory-guided\nEMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animation\napproach to generate identity-consistent and expressive talking videos. Our\napproach is built around two key modules: (1) a memory-guided temporal module,\nwhich enhances long-term identity consistency and motion smoothness by\ndeveloping memory states to store information from a longer past context to\nguide temporal modeling via linear attention; and (2) an emotion-aware audio\nmodule, which replaces traditional cross attention with multi-modal attention\nto enhance audio-video interaction, while detecting emotions from audio to\nrefine facial expressions via emotion adaptive layer norm. Extensive\nquantitative and qualitative results demonstrate that MEMO generates more\nrealistic talking videos across diverse image and audio types, outperforming\nstate-of-the-art methods in overall quality, audio-lip synchronization,\nidentity consistency, and expression-emotion alignment.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "R_WQwptCU2yc3VW_F3344_4nakH7WOI0vu9YgUG_8Cs",
  "pdfSize": "14960430"
}