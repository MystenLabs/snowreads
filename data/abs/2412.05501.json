{"id":"2412.05501","title":"Computational models of learning and synaptic plasticity","authors":"Danil Tyulmankov","authorsParsed":[["Tyulmankov","Danil",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 02:03:05 GMT"}],"updateDate":"2024-12-10","timestamp":1733536985000,"abstract":"  Many mathematical models of synaptic plasticity have been proposed to explain\nthe diversity of plasticity phenomena observed in biological organisms. These\nmodels range from simple interpretations of Hebb's postulate, which suggests\nthat correlated neural activity leads to increases in synaptic strength, to\nmore complex rules that allow bidirectional synaptic updates, ensure stability,\nor incorporate additional signals like reward or error. At the same time, a\nrange of learning paradigms can be observed behaviorally, from Pavlovian\nconditioning to motor learning and memory recall. Although it is difficult to\ndirectly link synaptic updates to learning outcomes experimentally,\ncomputational models provide a valuable tool for building evidence of this\nconnection. In this chapter, we discuss several fundamental learning paradigms,\nalong with the synaptic plasticity rules that might be used to implement them.\n","subjects":["Quantitative Biology/Neurons and Cognition","Computer Science/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"r_qviLwP5LJBSFKej_F2A70cNskxq5BrlDKJsbJXHJQ","pdfSize":"907421"}