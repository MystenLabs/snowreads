{"id":"2407.02514","title":"LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations","authors":"Shashank Kirtania, Priyanshu Gupta, Arjun Radhakirshna","authorsParsed":[["Kirtania","Shashank",""],["Gupta","Priyanshu",""],["Radhakirshna","Arjun",""]],"versions":[{"version":"v1","created":"Sat, 22 Jun 2024 12:50:41 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 21:49:07 GMT"},{"version":"v3","created":"Tue, 6 Aug 2024 06:39:02 GMT"}],"updateDate":"2024-08-07","timestamp":1719060641000,"abstract":"  In this paper we examine the limitations of Large Language Models (LLMs) for\ncomplex reasoning tasks. Although recent works have started to employ formal\nlanguages as an intermediate representation for reasoning tasks, they often\nface challenges in accurately generating and refining these formal\nspecifications to ensure correctness. To address these issues, this paper\nproposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs\nto do pairwise comparisons, allowing the evaluation of the refinements\nsuggested by the LLM. The paper demonstrates that Logic-LM++ outperforms\nLogic-LM and other contemporary techniques across natural language reasoning\ntasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average\nimprovement of 18.5% on standard prompting, 12.3% on chain of thought prompting\nand 5% on Logic-LM.\n","subjects":["Computing Research Repository/Logic in Computer Science","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ddodFl37WVY5TXLiSpELldcVm9AB5y9Vyg_LTyfNC3A","pdfSize":"385055"}