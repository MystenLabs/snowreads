{"id":"2412.11430","title":"Efficient Multiagent Planning via Shared Action Suggestions","authors":"Dylan M. Asmar and Mykel J. Kochenderfer","authorsParsed":[["Asmar","Dylan M.",""],["Kochenderfer","Mykel J.",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 04:03:46 GMT"}],"updateDate":"2024-12-17","timestamp":1734321826000,"abstract":"  Decentralized partially observable Markov decision processes with\ncommunication (Dec-POMDP-Com) provide a framework for multiagent decision\nmaking under uncertainty, but the NEXP-complete complexity renders solutions\nintractable in general. While sharing actions and observations can reduce the\ncomplexity to PSPACE-complete, we propose an approach that bridges POMDPs and\nDec-POMDPs by communicating only suggested joint actions, eliminating the need\nto share observations while maintaining performance comparable to fully\ncentralized planning and execution. Our algorithm estimates joint beliefs using\nshared actions to prune infeasible beliefs. Each agent maintains possible\nbelief sets for other agents, pruning them based on suggested actions to form\nan estimated joint belief usable with any centralized policy. This approach\nrequires solving a POMDP for each agent, reducing computational complexity\nwhile preserving performance. We demonstrate its effectiveness on several\nDec-POMDP benchmarks showing performance comparable to centralized methods when\nshared actions enable effective belief pruning. This action-based communication\nframework offers a natural avenue for integrating human-agent cooperation,\nopening new directions for scalable multiagent planning under uncertainty, with\napplications in both autonomous systems and human-agent teams.\n","subjects":["Computer Science/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3AKyVh_DN3RnLUAIQcWxuaSmJucAwlArMiKsLwqy7Pw","pdfSize":"733129"}