{"id":"2412.16491","title":"ImagePiece: Content-aware Re-tokenization for Efficient Image\n  Recognition","authors":"Seungdong Yoa, Seungjun Lee, Hyeseung Cho, Bumsoo Kim, Woohyung Lim","authorsParsed":[["Yoa","Seungdong",""],["Lee","Seungjun",""],["Cho","Hyeseung",""],["Kim","Bumsoo",""],["Lim","Woohyung",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 05:38:20 GMT"}],"updateDate":"2024-12-24","timestamp":1734759500000,"abstract":"  Vision Transformers (ViTs) have achieved remarkable success in various\ncomputer vision tasks. However, ViTs have a huge computational cost due to\ntheir inherent reliance on multi-head self-attention (MHSA), prompting efforts\nto accelerate ViTs for practical applications. To this end, recent works aim to\nreduce the number of tokens, mainly focusing on how to effectively prune or\nmerge them. Nevertheless, since ViT tokens are generated from non-overlapping\ngrid patches, they usually do not convey sufficient semantics, making it\nincompatible with efficient ViTs. To address this, we propose ImagePiece, a\nnovel re-tokenization strategy for Vision Transformers. Following the MaxMatch\nstrategy of NLP tokenization, ImagePiece groups semantically insufficient yet\nlocally coherent tokens until they convey meaning. This simple retokenization\nis highly compatible with previous token reduction methods, being able to\ndrastically narrow down relevant tokens, enhancing the inference speed of\nDeiT-S by 54% (nearly 1.5$\\times$ faster) while achieving a 0.39% improvement\nin ImageNet classification accuracy. For hyper-speed inference scenarios (with\n251% acceleration), our approach surpasses other baselines by an accuracy over\n8%.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"upnDbX6AYjdAj38gFqdBfuxdWNS5hHegE9L6kh8qSzA","pdfSize":"2478039"}