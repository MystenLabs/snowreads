{
  "id": "2412.03782",
  "title": "The broader spectrum of in-context learning",
  "authors": "Andrew Kyle Lampinen, Stephanie C. Y. Chan, Aaditya K. Singh, Murray\n  Shanahan",
  "authorsParsed": [
    [
      "Lampinen",
      "Andrew Kyle",
      ""
    ],
    [
      "Chan",
      "Stephanie C. Y.",
      ""
    ],
    [
      "Singh",
      "Aaditya K.",
      ""
    ],
    [
      "Shanahan",
      "Murray",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 00:05:11 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 9 Dec 2024 18:28:06 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733357111000,
  "abstract": "  The ability of language models to learn a task from a few examples in context\nhas generated substantial interest. Here, we provide a perspective that\nsituates this type of supervised few-shot learning within a much broader\nspectrum of meta-learned in-context learning. Indeed, we suggest that any\ndistribution of sequences in which context non-trivially decreases loss on\nsubsequent predictions can be interpreted as eliciting a kind of in-context\nlearning. We suggest that this perspective helps to unify the broad set of\nin-context abilities that language models exhibit $\\unicode{x2014}$ such as\nadapting to tasks from instructions or role play, or extrapolating time series.\nThis perspective also sheds light on potential roots of in-context learning in\nlower-level processing of linguistic dependencies (e.g. coreference or parallel\nstructures). Finally, taking this perspective highlights the importance of\ngeneralization, which we suggest can be studied along several dimensions: not\nonly the ability to learn something novel, but also flexibility in learning\nfrom different presentations, and in applying what is learned. We discuss\nbroader connections to past literature in meta-learning and goal-conditioned\nagents, and other perspectives on learning and adaptation. We close by\nsuggesting that research on in-context learning should consider this broader\nspectrum of in-context capabilities and types of generalization.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "0V43GSFl8St0ZcY1L_xjhGhdeZyYeA-MPXb3mtinh1s",
  "pdfSize": "1230660"
}