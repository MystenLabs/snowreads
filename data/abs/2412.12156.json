{
  "id": "2412.12156",
  "title": "Deep Distributed Optimization for Large-Scale Quadratic Programming",
  "authors": "Augustinos D. Saravanos, Hunter Kuperman, Alex Oshin, Arshiya Taj\n  Abdul, Vincent Pacelli, Evangelos A. Theodorou",
  "authorsParsed": [
    [
      "Saravanos",
      "Augustinos D.",
      ""
    ],
    [
      "Kuperman",
      "Hunter",
      ""
    ],
    [
      "Oshin",
      "Alex",
      ""
    ],
    [
      "Abdul",
      "Arshiya Taj",
      ""
    ],
    [
      "Pacelli",
      "Vincent",
      ""
    ],
    [
      "Theodorou",
      "Evangelos A.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 09:45:00 GMT"
    },
    {
      "version": "v2",
      "created": "Sat, 25 Jan 2025 20:47:23 GMT"
    }
  ],
  "updateDate": "2025-01-28",
  "timestamp": 1733910300000,
  "abstract": "  Quadratic programming (QP) forms a crucial foundation in optimization,\nencompassing a broad spectrum of domains and serving as the basis for more\nadvanced algorithms. Consequently, as the scale and complexity of modern\napplications continue to grow, the development of efficient and reliable QP\nalgorithms is becoming increasingly vital. In this context, this paper\nintroduces a novel deep learning-aided distributed optimization architecture\ndesigned for tackling large-scale QP problems. First, we combine the\nstate-of-the-art Operator Splitting QP (OSQP) method with a consensus approach\nto derive DistributedQP, a new method tailored for network-structured problems,\nwith convergence guarantees to optimality. Subsequently, we unfold this\noptimizer into a deep learning framework, leading to DeepDistributedQP, which\nleverages learned policies to accelerate reaching to desired accuracy within a\nrestricted amount of iterations. Our approach is also theoretically grounded\nthrough Probably Approximately Correct (PAC)-Bayes theory, providing\ngeneralization bounds on the expected optimality gap for unseen problems. The\nproposed framework, as well as its centralized version DeepQP, significantly\noutperform their standard optimization counterparts on a variety of tasks such\nas randomly generated problems, optimal control, linear regression,\ntransportation networks and others. Notably, DeepDistributedQP demonstrates\nstrong generalization by training on small problems and scaling to solve much\nlarger ones (up to 50K variables and 150K constraints) using the same policy.\nMoreover, it achieves orders-of-magnitude improvements in wall-clock time\ncompared to OSQP. The certifiable performance guarantees of our approach are\nalso demonstrated, ensuring higher-quality solutions over traditional\noptimizers.\n",
  "subjects": [
    "Mathematics/Optimization and Control",
    "Computer Science/Machine Learning",
    "Computer Science/Multiagent Systems"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "6YxN2HYFGfehAvJG3tTsERGSvjX27ziMtsTsjEPtfa4",
  "pdfSize": "5951213"
}