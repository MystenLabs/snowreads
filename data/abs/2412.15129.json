{"id":"2412.15129","title":"Jet: A Modern Transformer-Based Normalizing Flow","authors":"Alexander Kolesnikov, Andr\\'e Susano Pinto, Michael Tschannen","authorsParsed":[["Kolesnikov","Alexander",""],["Pinto","Andr√© Susano",""],["Tschannen","Michael",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 18:09:42 GMT"}],"updateDate":"2024-12-20","timestamp":1734631782000,"abstract":"  In the past, normalizing generative flows have emerged as a promising class\nof generative models for natural images. This type of model has many modeling\nadvantages: the ability to efficiently compute log-likelihood of the input\ndata, fast generation and simple overall structure. Normalizing flows remained\na topic of active research but later fell out of favor, as visual quality of\nthe samples was not competitive with other model classes, such as GANs,\nVQ-VAE-based approaches or diffusion models. In this paper we revisit the\ndesign of the coupling-based normalizing flow models by carefully ablating\nprior design choices and using computational blocks based on the Vision\nTransformer architecture, not convolutional neural networks. As a result, we\nachieve state-of-the-art quantitative and qualitative performance with a much\nsimpler architecture. While the overall visual quality is still behind the\ncurrent state-of-the-art models, we argue that strong normalizing flow models\ncan help advancing research frontier by serving as building components of more\npowerful generative models.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FvAb9Mbq6j6Zmw0sAFYhh0RNSBwSoVFGyTejUZBwIo0","pdfSize":"5582149"}