{"id":"2407.18807","title":"Robust Learning in Bayesian Parallel Branching Graph Neural Networks:\n  The Narrow Width Limit","authors":"Zechen Zhang, Haim Sompolinsky","authorsParsed":[["Zhang","Zechen",""],["Sompolinsky","Haim",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 15:14:22 GMT"}],"updateDate":"2024-07-29","timestamp":1722006862000,"abstract":"  The infinite width limit of random neural networks is known to result in\nNeural Networks as Gaussian Process (NNGP) (Lee et al. [2018]), characterized\nby task-independent kernels. It is widely accepted that larger network widths\ncontribute to improved generalization (Park et al. [2019]). However, this work\nchallenges this notion by investigating the narrow width limit of the Bayesian\nParallel Branching Graph Neural Network (BPB-GNN), an architecture that\nresembles residual networks. We demonstrate that when the width of a BPB-GNN is\nsignificantly smaller compared to the number of training examples, each branch\nexhibits more robust learning due to a symmetry breaking of branches in kernel\nrenormalization. Surprisingly, the performance of a BPB-GNN in the narrow width\nlimit is generally superior or comparable to that achieved in the wide width\nlimit in bias-limited scenarios. Furthermore, the readout norms of each branch\nin the narrow width limit are mostly independent of the architectural\nhyperparameters but generally reflective of the nature of the data. Our results\ncharacterize a newly defined narrow-width regime for parallel branching\nnetworks in general.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9DkQAKtZ-rpga155VBRAf4ARILdueag83g7PbxlPLaM","pdfSize":"1505067"}