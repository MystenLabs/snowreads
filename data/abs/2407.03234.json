{"id":"2407.03234","title":"Self-Evaluation as a Defense Against Adversarial Attacks on LLMs","authors":"Hannah Brown, Leon Lin, Kenji Kawaguchi, Michael Shieh","authorsParsed":[["Brown","Hannah",""],["Lin","Leon",""],["Kawaguchi","Kenji",""],["Shieh","Michael",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 16:03:42 GMT"},{"version":"v2","created":"Mon, 15 Jul 2024 05:20:18 GMT"},{"version":"v3","created":"Tue, 6 Aug 2024 11:15:00 GMT"}],"updateDate":"2024-08-07","timestamp":1720022622000,"abstract":"  We introduce a defense against adversarial attacks on LLMs utilizing\nself-evaluation. Our method requires no model fine-tuning, instead using\npre-trained models to evaluate the inputs and outputs of a generator model,\nsignificantly reducing the cost of implementation in comparison to other,\nfinetuning-based methods. Our method can significantly reduce the attack\nsuccess rate of attacks on both open and closed-source LLMs, beyond the\nreductions demonstrated by Llama-Guard2 and commonly used content moderation\nAPIs. We present an analysis of the effectiveness of our method, including\nattempts to attack the evaluator in various settings, demonstrating that it is\nalso more resilient to attacks than existing methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bKTf1QoXvnAZOypPsL9pshETX5477hWECy7JQLBYbhU","pdfSize":"782582"}