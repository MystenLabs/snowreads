{
  "id": "2412.17970",
  "title": "CARL-GT: Evaluating Causal Reasoning Capabilities of Large Language\n  Models",
  "authors": "Ruibo Tu, Hedvig Kjellstr\\\"om, Gustav Eje Henter, Cheng Zhang",
  "authorsParsed": [
    [
      "Tu",
      "Ruibo",
      ""
    ],
    [
      "Kjellstr√∂m",
      "Hedvig",
      ""
    ],
    [
      "Henter",
      "Gustav Eje",
      ""
    ],
    [
      "Zhang",
      "Cheng",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 20:34:32 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1734986072000,
  "abstract": "  Causal reasoning capabilities are essential for large language models (LLMs)\nin a wide range of applications, such as education and healthcare. But there is\nstill a lack of benchmarks for a better understanding of such capabilities.\nCurrent LLM benchmarks are mainly based on conversational tasks, academic math\ntests, and coding tests. Such benchmarks evaluate LLMs in well-regularized\nsettings, but they are limited in assessing the skills and abilities to solve\nreal-world problems. In this work, we provide a benchmark, named by CARL-GT,\nwhich evaluates CAusal Reasoning capabilities of large Language models using\nGraphs and Tabular data. The benchmark has a diverse range of tasks for\nevaluating LLMs from causal graph reasoning, knowledge discovery, and\ndecision-making aspects. In addition, effective zero-shot learning prompts are\ndeveloped for the tasks. In our experiments, we leverage the benchmark for\nevaluating open-source LLMs and provide a detailed comparison of LLMs for\ncausal reasoning abilities. We found that LLMs are still weak in casual\nreasoning, especially with tabular data to discover new insights. Furthermore,\nwe investigate and discuss the relationships of different benchmark tasks by\nanalyzing the performance of LLMs. The experimental results show that LLMs have\ndifferent strength over different tasks and that their performance on tasks in\ndifferent categories, i.e., causal graph reasoning, knowledge discovery, and\ndecision-making, shows stronger correlation than tasks in the same category.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning",
    "Statistics/Methodology"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "yqdMmJYLc0SWvLXVFwbMSYwIe-wZlKNlXd_KmGo9X48",
  "pdfSize": "1446260"
}