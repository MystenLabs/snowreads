{
  "id": "2412.18645",
  "title": "Dissecting CLIP: Decomposition with a Schur Complement-based Approach",
  "authors": "Azim Ospanov and Mohammad Jalali and Farzan Farnia",
  "authorsParsed": [
    [
      "Ospanov",
      "Azim",
      ""
    ],
    [
      "Jalali",
      "Mohammad",
      ""
    ],
    [
      "Farnia",
      "Farzan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 24 Dec 2024 18:07:57 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735063677000,
  "abstract": "  The use of CLIP embeddings to assess the alignment of samples produced by\ntext-to-image generative models has been extensively explored in the\nliterature. While the widely adopted CLIPScore, derived from the cosine\nsimilarity of text and image embeddings, effectively measures the relevance of\na generated image, it does not quantify the diversity of images generated by a\ntext-to-image model. In this work, we extend the application of CLIP embeddings\nto quantify and interpret the intrinsic diversity of text-to-image models,\nwhich is responsible for generating diverse images from similar text prompts.\nTo achieve this, we propose a decomposition of the CLIP-based kernel covariance\nmatrix of image data into text-based and non-text-based components. Using the\nSchur complement of the joint image-text kernel covariance matrix, we perform\nthis decomposition and define the matrix-based entropy of the decomposed\ncomponent as the \\textit{Schur Complement Entropy (SCE)} score, a measure of\nthe intrinsic diversity of a text-to-image model based on data collected with\nvarying text prompts. Additionally, we demonstrate the use of the Schur\ncomplement-based decomposition to nullify the influence of a given prompt in\nthe CLIP embedding of an image, enabling focus or defocus of embeddings on\nspecific objects or properties for downstream tasks. We present several\nnumerical results that apply our Schur complement-based approach to evaluate\ntext-to-image models and modify CLIP image embeddings. The codebase is\navailable at https://github.com/aziksh-ospanov/CLIP-DISSECTION\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "atfqUiYRc3JFH_3yx8zRKSGDBblmEiQaybN7M6iH234",
  "pdfSize": "18725229"
}