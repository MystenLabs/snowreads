{
  "id": "2412.04415",
  "title": "Targeting the Core: A Simple and Effective Method to Attack RAG-based\n  Agents via Direct LLM Manipulation",
  "authors": "Xuying Li, Zhuo Li, Yuji Kosuga, Yasuhiro Yoshida, Victor Bian",
  "authorsParsed": [
    [
      "Li",
      "Xuying",
      ""
    ],
    [
      "Li",
      "Zhuo",
      ""
    ],
    [
      "Kosuga",
      "Yuji",
      ""
    ],
    [
      "Yoshida",
      "Yasuhiro",
      ""
    ],
    [
      "Bian",
      "Victor",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 18:38:30 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733423910000,
  "abstract": "  AI agents, powered by large language models (LLMs), have transformed\nhuman-computer interactions by enabling seamless, natural, and context-aware\ncommunication. While these advancements offer immense utility, they also\ninherit and amplify inherent safety risks such as bias, fairness,\nhallucinations, privacy breaches, and a lack of transparency. This paper\ninvestigates a critical vulnerability: adversarial attacks targeting the LLM\ncore within AI agents. Specifically, we test the hypothesis that a deceptively\nsimple adversarial prefix, such as \\textit{Ignore the document}, can compel\nLLMs to produce dangerous or unintended outputs by bypassing their contextual\nsafeguards. Through experimentation, we demonstrate a high attack success rate\n(ASR), revealing the fragility of existing LLM defenses. These findings\nemphasize the urgent need for robust, multi-layered security measures tailored\nto mitigate vulnerabilities at the LLM level and within broader agent-based\narchitectures.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/publicdomain/zero/1.0/",
  "blobId": "lxlb76BXGM7UnzqjONOKBMF4NthjZmACXVGc-wCQJ4Y",
  "pdfSize": "95862"
}