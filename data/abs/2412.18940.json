{"id":"2412.18940","title":"Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations","authors":"Yewon Kim, Sung-Ju Lee, Chris Donahue","authorsParsed":[["Kim","Yewon",""],["Lee","Sung-Ju",""],["Donahue","Chris",""]],"versions":[{"version":"v1","created":"Wed, 25 Dec 2024 16:23:32 GMT"},{"version":"v2","created":"Fri, 14 Feb 2025 14:22:55 GMT"}],"updateDate":"2025-02-17","timestamp":1735143812000,"abstract":"  Songwriting is often driven by multimodal inspirations, such as imagery,\nnarratives, or existing music, yet songwriters remain unsupported by current\nmusic AI systems in incorporating these multimodal inputs into their creative\nprocesses. We introduce Amuse, a songwriting assistant that transforms\nmultimodal (image, text, or audio) inputs into chord progressions that can be\nseamlessly incorporated into songwriters' creative processes. A key feature of\nAmuse is its novel method for generating coherent chords that are relevant to\nmusic keywords in the absence of datasets with paired examples of multimodal\ninputs and chords. Specifically, we propose a method that leverages multimodal\nlarge language models (LLMs) to convert multimodal inputs into noisy chord\nsuggestions and uses a unimodal chord model to filter the suggestions. A user\nstudy with songwriters shows that Amuse effectively supports transforming\nmultimodal ideas into coherent musical suggestions, enhancing users' agency and\ncreativity throughout the songwriting process.\n","subjects":["Computer Science/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YMYQ4eypNVLsDwUiNSkT_F5O_0al6L3Plq3tH7wYxKo","pdfSize":"6183582"}