{"id":"2407.01812","title":"Equivariant Diffusion Policy","authors":"Dian Wang, Stephen Hart, David Surovik, Tarik Kelestemur, Haojie\n  Huang, Haibo Zhao, Mark Yeatman, Jiuguang Wang, Robin Walters, Robert Platt","authorsParsed":[["Wang","Dian",""],["Hart","Stephen",""],["Surovik","David",""],["Kelestemur","Tarik",""],["Huang","Haojie",""],["Zhao","Haibo",""],["Yeatman","Mark",""],["Wang","Jiuguang",""],["Walters","Robin",""],["Platt","Robert",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 21:23:26 GMT"}],"updateDate":"2024-07-03","timestamp":1719869006000,"abstract":"  Recent work has shown diffusion models are an effective approach to learning\nthe multimodal distributions arising from demonstration data in behavior\ncloning. However, a drawback of this approach is the need to learn a denoising\nfunction, which is significantly more complex than learning an explicit policy.\nIn this work, we propose Equivariant Diffusion Policy, a novel diffusion policy\nlearning method that leverages domain symmetries to obtain better sample\nefficiency and generalization in the denoising function. We theoretically\nanalyze the $\\mathrm{SO}(2)$ symmetry of full 6-DoF control and characterize\nwhen a diffusion model is $\\mathrm{SO}(2)$-equivariant. We furthermore evaluate\nthe method empirically on a set of 12 simulation tasks in MimicGen, and show\nthat it obtains a success rate that is, on average, 21.9% higher than the\nbaseline Diffusion Policy. We also evaluate the method on a real-world system\nto show that effective policies can be learned with relatively few training\nsamples, whereas the baseline Diffusion Policy cannot.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"tCkZ0w_n87v9fJavw11dJ0A1UwhcNgmG_c84_X6Bn-g","pdfSize":"31147805"}