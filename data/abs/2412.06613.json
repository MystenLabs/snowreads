{"id":"2412.06613","title":"3D Spatial Understanding in MLLMs: Disambiguation and Evaluation","authors":"Chun-Peng Chang, Alain Pagani, Didier Stricker","authorsParsed":[["Chang","Chun-Peng",""],["Pagani","Alain",""],["Stricker","Didier",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 16:04:32 GMT"}],"updateDate":"2024-12-10","timestamp":1733760272000,"abstract":"  Multimodal Large Language Models (MLLMs) have made significant progress in\ntasks such as image captioning and question answering. However, while these\nmodels can generate realistic captions, they often struggle with providing\nprecise instructions, particularly when it comes to localizing and\ndisambiguating objects in complex 3D environments. This capability is critical\nas MLLMs become more integrated with collaborative robotic systems. In\nscenarios where a target object is surrounded by similar objects (distractors),\nrobots must deliver clear, spatially-aware instructions to guide humans\neffectively. We refer to this challenge as contextual object localization and\ndisambiguation, which imposes stricter constraints than conventional 3D dense\ncaptioning, especially regarding ensuring target exclusivity. In response, we\npropose simple yet effective techniques to enhance the model's ability to\nlocalize and disambiguate target objects. Our approach not only achieves\nstate-of-the-art performance on conventional metrics that evaluate sentence\nsimilarity, but also demonstrates improved 3D spatial understanding through 3D\nvisual grounding model.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"g1tZR_nkVr2xEODehk42Wr1VcneOpgujXejvxrDE_6A","pdfSize":"3361838"}