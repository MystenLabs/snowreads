{"id":"2412.18296","title":"Navigating Data Corruption in Machine Learning: Balancing Quality,\n  Quantity, and Imputation Strategies","authors":"Qi Liu and Wanjing Ma","authorsParsed":[["Liu","Qi",""],["Ma","Wanjing",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 09:04:06 GMT"}],"updateDate":"2024-12-25","timestamp":1735031046000,"abstract":"  Data corruption, including missing and noisy data, poses significant\nchallenges in real-world machine learning. This study investigates the effects\nof data corruption on model performance and explores strategies to mitigate\nthese effects through two experimental setups: supervised learning with NLP\ntasks (NLP-SL) and deep reinforcement learning for traffic signal optimization\n(Signal-RL). We analyze the relationship between data corruption levels and\nmodel performance, evaluate the effectiveness of data imputation methods, and\nassess the utility of enlarging datasets to address data corruption.\n  Our results show that model performance under data corruption follows a\ndiminishing return curve, modeled by the exponential function. Missing data,\nwhile detrimental, is less harmful than noisy data, which causes severe\nperformance degradation and training instability, particularly in sequential\ndecision-making tasks like Signal-RL. Imputation strategies involve a\ntrade-off: they recover missing information but may introduce noise. Their\neffectiveness depends on imputation accuracy and corruption ratio. We identify\ndistinct regions in the imputation advantage heatmap, including an \"imputation\nadvantageous corner\" and an \"imputation disadvantageous edge\" and classify\ntasks as \"noise-sensitive\" or \"noise-insensitive\" based on their decision\nboundaries.\n  Furthermore, we find that increasing dataset size mitigates but cannot fully\novercome the effects of data corruption. The marginal utility of additional\ndata diminishes as corruption increases. An empirical rule emerges:\napproximately 30% of the data is critical for determining performance, while\nthe remaining 70% has minimal impact.\n  These findings provide actionable insights into data preprocessing,\nimputation strategies, and data collection practices, guiding the development\nof robust machine learning systems in noisy environments.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"SbkkxpnUuvGVctRLPSjLl3vFojP61vq7KssT_L8byW8","pdfSize":"15712457"}