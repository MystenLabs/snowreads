{"id":"2407.06167","title":"D{\\epsilon}pS: Delayed {\\epsilon}-Shrinking for Faster Once-For-All\n  Training","authors":"Aditya Annavajjala, Alind Khare, Animesh Agrawal, Igor Fedorov, Hugo\n  Latapie, Myungjin Lee, Alexey Tumanov","authorsParsed":[["Annavajjala","Aditya",""],["Khare","Alind",""],["Agrawal","Animesh",""],["Fedorov","Igor",""],["Latapie","Hugo",""],["Lee","Myungjin",""],["Tumanov","Alexey",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:45:40 GMT"}],"updateDate":"2024-07-09","timestamp":1720460740000,"abstract":"  CNNs are increasingly deployed across different hardware, dynamic\nenvironments, and low-power embedded devices. This has led to the design and\ntraining of CNN architectures with the goal of maximizing accuracy subject to\nsuch variable deployment constraints. As the number of deployment scenarios\ngrows, there is a need to find scalable solutions to design and train\nspecialized CNNs. Once-for-all training has emerged as a scalable approach that\njointly co-trains many models (subnets) at once with a constant training cost\nand finds specialized CNNs later. The scalability is achieved by training the\nfull model and simultaneously reducing it to smaller subnets that share model\nweights (weight-shared shrinking). However, existing once-for-all training\napproaches incur huge training costs reaching 1200 GPU hours. We argue this is\nbecause they either start the process of shrinking the full model too early or\ntoo late. Hence, we propose Delayed $\\epsilon$-Shrinking (D$\\epsilon$pS) that\nstarts the process of shrinking the full model when it is partially trained\n(~50%) which leads to training cost improvement and better in-place knowledge\ndistillation to smaller models. The proposed approach also consists of novel\nheuristics that dynamically adjust subnet learning rates incrementally (E),\nleading to improved weight-shared knowledge distillation from larger to smaller\nsubnets as well. As a result, DEpS outperforms state-of-the-art once-for-all\ntraining techniques across different datasets including CIFAR10/100,\nImageNet-100, and ImageNet-1k on accuracy and cost. It achieves 1.83% higher\nImageNet-1k top1 accuracy or the same accuracy with 1.3x reduction in FLOPs and\n2.5x drop in training cost (GPU*hrs)\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_v9BUBXhnvzoniLnitcBDxwPmyuOHgYaEEBS-KTdjes","pdfSize":"11089014"}