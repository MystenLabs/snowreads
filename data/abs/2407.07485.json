{"id":"2407.07485","title":"Zero-Shot Class Unlearning in CLIP with Synthetic Samples","authors":"A. Kravets, V. Namboodiri","authorsParsed":[["Kravets","A.",""],["Namboodiri","V.",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 09:16:14 GMT"}],"updateDate":"2024-07-11","timestamp":1720602974000,"abstract":"  Machine unlearning is a crucial area of research. It is driven by the need to\nremove sensitive information from models to safeguard individuals' right to be\nforgotten under rigorous regulations such as GDPR. In this work, we focus on\nunlearning within CLIP, a dual vision-language encoder model trained on a\nmassive dataset of image-text pairs using contrastive loss. To achieve\nforgetting we expand the application of Lipschitz regularization to the\nmultimodal context of CLIP. Specifically, we ensure the smoothing of both\nvisual and textual embeddings associated with the class intended to be\nforgotten relative to the perturbation introduced to the samples from that\nclass. Additionally, importantly, we remove the necessity for real forgetting\ndata by generating synthetic samples through gradient ascent maximizing the\ntarget class. Our forgetting procedure is iterative, where we track accuracy on\na synthetic forget set and stop when accuracy falls below a chosen threshold.\nWe employ a selective layers update strategy based on their average absolute\ngradient value to mitigate over-forgetting. We validate our approach on several\nstandard datasets and provide thorough ablation analysis and comparisons with\nprevious work.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"fvVYm2Uqbc5WF32EOsWTGjhG58wcjDtAofNsiPOSn3U","pdfSize":"10905224"}