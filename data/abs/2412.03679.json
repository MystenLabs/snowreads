{
  "id": "2412.03679",
  "title": "Evaluating Language Models as Synthetic Data Generators",
  "authors": "Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee,\n  Yizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, Graham\n  Neubig",
  "authorsParsed": [
    [
      "Kim",
      "Seungone",
      ""
    ],
    [
      "Suk",
      "Juyoung",
      ""
    ],
    [
      "Yue",
      "Xiang",
      ""
    ],
    [
      "Viswanathan",
      "Vijay",
      ""
    ],
    [
      "Lee",
      "Seongyun",
      ""
    ],
    [
      "Wang",
      "Yizhong",
      ""
    ],
    [
      "Gashteovski",
      "Kiril",
      ""
    ],
    [
      "Lawrence",
      "Carolin",
      ""
    ],
    [
      "Welleck",
      "Sean",
      ""
    ],
    [
      "Neubig",
      "Graham",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 19:20:32 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733340032000,
  "abstract": "  Given the increasing use of synthetic data in language model (LM)\npost-training, an LM's ability to generate high-quality data has become nearly\nas crucial as its ability to solve problems directly. While prior works have\nfocused on developing effective data generation methods, they lack systematic\ncomparison of different LMs as data generators in a unified setting. To address\nthis gap, we propose AgoraBench, a benchmark that provides standardized\nsettings and metrics to evaluate LMs' data generation abilities. Through\nsynthesizing 1.26 million training instances using 6 LMs and training 99\nstudent models, we uncover key insights about LMs' data generation\ncapabilities. First, we observe that LMs exhibit distinct strengths. For\ninstance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet\nperforms better at enhancing existing ones. Furthermore, our analysis reveals\nthat an LM's data generation ability doesn't necessarily correlate with its\nproblem-solving ability. Instead, multiple intrinsic features of data\nquality-including response quality, perplexity, and instruction\ndifficulty-collectively serve as better indicators. Finally, we demonstrate\nthat strategic choices in output format and cost-conscious model selection\nsignificantly impact data generation effectiveness.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "3u9fWiR4V-6oVhz0xYTHHwMto0lUIIEdAxL5iaRMeFI",
  "pdfSize": "2177886"
}