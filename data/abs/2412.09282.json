{"id":"2412.09282","title":"CRVQ: Channel-Relaxed Vector Quantization for Extreme Compression of\n  LLMs","authors":"Yuzhuang Xu, Shiyu Ji, Qingfu Zhu, Wanxiang Che","authorsParsed":[["Xu","Yuzhuang",""],["Ji","Shiyu",""],["Zhu","Qingfu",""],["Che","Wanxiang",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 13:45:11 GMT"},{"version":"v2","created":"Wed, 19 Feb 2025 05:50:37 GMT"}],"updateDate":"2025-02-20","timestamp":1734011111000,"abstract":"  Powerful large language models (LLMs) are increasingly expected to be\ndeployed with lower computational costs, enabling their capabilities on\nresource-constrained devices. Post-training quantization (PTQ) has emerged as a\nstar approach to achieve this ambition, with best methods compressing weights\nto less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector\nQuantization (CRVQ), a novel technique that significantly improves the\nperformance of PTQ baselines at the cost of only minimal additional bits. This\nstate-of-the-art extreme compression method achieves its results through two\nkey innovations: (1) carefully selecting and reordering a very small subset of\ncritical weight channels, and (2) leveraging extended codebooks to relax the\nconstraint of critical channels. With our method, we demonstrate a 38.9\\%\nimprovement over the current strongest sub-2-bit PTQ baseline, enabling nearer\nlossless 1-bit compression. Furthermore, our approach offers flexible\ncustomization of quantization bit-width and performance, providing a wider\nrange of deployment options for diverse hardware platforms.\n","subjects":["Computer Science/Machine Learning","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kOHr574PotePI15-tdmntIfUK3VAd60rMFKvOgz5jSk","pdfSize":"4087786"}