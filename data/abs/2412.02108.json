{
  "id": "2412.02108",
  "title": "Evaluating the Impact of Data Augmentation on Predictive Model\n  Performance",
  "authors": "Valdemar \\v{S}v\\'abensk\\'y, Conrad Borchers, Elizabeth B. Cloude,\n  Atsushi Shimada",
  "authorsParsed": [
    [
      "Švábenský",
      "Valdemar",
      ""
    ],
    [
      "Borchers",
      "Conrad",
      ""
    ],
    [
      "Cloude",
      "Elizabeth B.",
      ""
    ],
    [
      "Shimada",
      "Atsushi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 03:03:04 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733194984000,
  "abstract": "  In supervised machine learning (SML) research, large training datasets are\nessential for valid results. However, obtaining primary data in learning\nanalytics (LA) is challenging. Data augmentation can address this by expanding\nand diversifying data, though its use in LA remains underexplored. This paper\nsystematically compares data augmentation techniques and their impact on\nprediction performance in a typical LA task: prediction of academic outcomes.\nAugmentation is demonstrated on four SML models, which we successfully\nreplicated from a previous LAK study based on AUC values. Among 21 augmentation\ntechniques, SMOTE-ENN sampling performed the best, improving the average AUC by\n0.01 and approximately halving the training time compared to the baseline\nmodels. In addition, we compared 99 combinations of chaining 21 techniques, and\nfound minor, although statistically significant, improvements across models\nwhen adding noise to SMOTE-ENN (+0.014). Notably, some augmentation techniques\nsignificantly lowered predictive performance or increased performance\nfluctuation related to random chance. This paper's contribution is twofold.\nPrimarily, our empirical findings show that sampling techniques provide the\nmost statistically reliable performance improvements for LA applications of\nSML, and are computationally more efficient than deep generation methods with\ncomplex hyperparameter settings. Second, the LA community may benefit from\nvalidating a recent study through independent replication.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Computers and Society"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "E2x3-8qSHzfldXsgLGkrTwMhBmQMiBmlG_btI0-t7fE",
  "pdfSize": "906025"
}