{"id":"2412.04429","title":"Grounding Descriptions in Images informs Zero-Shot Visual Recognition","authors":"Shaunak Halbe, Junjiao Tian, K J Joseph, James Seale Smith, Katherine\n  Stevo, Vineeth N Balasubramanian, Zsolt Kira","authorsParsed":[["Halbe","Shaunak",""],["Tian","Junjiao",""],["Joseph","K J",""],["Smith","James Seale",""],["Stevo","Katherine",""],["Balasubramanian","Vineeth N",""],["Kira","Zsolt",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 18:52:00 GMT"}],"updateDate":"2024-12-06","timestamp":1733424720000,"abstract":"  Vision-language models (VLMs) like CLIP have been cherished for their ability\nto perform zero-shot visual recognition on open-vocabulary concepts. This is\nachieved by selecting the object category whose textual representation bears\nthe highest similarity with the query image. While successful in some domains,\nthis method struggles with identifying fine-grained entities as well as\ngeneralizing to unseen concepts that are not captured by the training\ndistribution. Recent works attempt to mitigate these challenges by integrating\ncategory descriptions at test time, albeit yielding modest improvements. We\nattribute these limited gains to a fundamental misalignment between image and\ndescription representations, which is rooted in the pretraining structure of\nCLIP. In this paper, we propose GRAIN, a new pretraining strategy aimed at\naligning representations at both fine and coarse levels simultaneously. Our\napproach learns to jointly ground textual descriptions in image regions along\nwith aligning overarching captions with global image representations. To drive\nthis pre-training, we leverage frozen Multimodal Large Language Models (MLLMs)\nto derive large-scale synthetic annotations. We demonstrate the enhanced\nzero-shot performance of our model compared to current state-of-the art methods\nacross 11 diverse image classification datasets. Additionally, we introduce\nProducts-2023, a newly curated, manually labeled dataset featuring novel\nconcepts, and showcase our model's ability to recognize these concepts by\nbenchmarking on it. Significant improvements achieved by our model on other\ndownstream tasks like retrieval further highlight the superior quality of\nrepresentations learned by our approach. Code available at\nhttps://github.com/shaunak27/grain-clip .\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ayTagIBn_yWs1fJmtfS4rMLeFBm93ErjOFUNrgkRzxE","pdfSize":"13282878"}