{"id":"2412.06685","title":"Policy Agnostic RL: Offline RL and Online RL Fine-Tuning of Any Class\n  and Backbone","authors":"Max Sobol Mark, Tian Gao, Georgia Gabriela Sampaio, Mohan Kumar\n  Srirama, Archit Sharma, Chelsea Finn, and Aviral Kumar","authorsParsed":[["Mark","Max Sobol",""],["Gao","Tian",""],["Sampaio","Georgia Gabriela",""],["Srirama","Mohan Kumar",""],["Sharma","Archit",""],["Finn","Chelsea",""],["Kumar","Aviral",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 17:28:03 GMT"}],"updateDate":"2024-12-10","timestamp":1733765283000,"abstract":"  Recent advances in learning decision-making policies can largely be\nattributed to training expressive policy models, largely via imitation\nlearning. While imitation learning discards non-expert data, reinforcement\nlearning (RL) can still learn from suboptimal data. However, instantiating RL\ntraining of a new policy class often presents a different challenge: most deep\nRL machinery is co-developed with assumptions on the policy class and backbone,\nresulting in poor performance when the policy class changes. For instance, SAC\nutilizes a low-variance reparameterization policy gradient for Gaussian\npolicies, but this is unstable for diffusion policies and intractable for\nautoregressive categorical policies. To address this issue, we develop an\noffline RL and online fine-tuning approach called policy-agnostic RL (PA-RL)\nthat can effectively train multiple policy classes, with varying architectures\nand sizes. We build off the basic idea that a universal supervised learning\nloss can replace the policy improvement step in RL, as long as it is applied on\n\"optimized\" actions. To obtain these optimized actions, we first sample\nmultiple actions from a base policy, and run global optimization (i.e.,\nre-ranking multiple action samples using the Q-function) and local optimization\n(i.e., running gradient steps on an action sample) to maximize the critic on\nthese candidates. PA-RL enables fine-tuning diffusion and transformer policies\nwith either autoregressive tokens or continuous action outputs, at different\nsizes, entirely via actor-critic RL. Moreover, PA-RL improves the performance\nand sample-efficiency by up to 2 times compared to existing offline RL and\nonline fine-tuning methods. We show the first result that successfully\nfine-tunes OpenVLA, a 7B generalist robot policy, autonomously with Cal-QL, an\nonline RL fine-tuning algorithm, improving from 40% to 70% in the real world in\n40 minutes.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"vF455VSJBPZrsL0Q8gTM6XCOkrbpsVE-vVDf8NJKiPA","pdfSize":"6073585"}