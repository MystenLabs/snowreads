{"id":"2407.03169","title":"Investigating Decoder-only Large Language Models for Speech-to-text\n  Translation","authors":"Chao-Wei Huang, Hui Lu, Hongyu Gong, Hirofumi Inaguma, Ilia Kulikov,\n  Ruslan Mavlyutov, Sravya Popuri","authorsParsed":[["Huang","Chao-Wei",""],["Lu","Hui",""],["Gong","Hongyu",""],["Inaguma","Hirofumi",""],["Kulikov","Ilia",""],["Mavlyutov","Ruslan",""],["Popuri","Sravya",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:42:49 GMT"}],"updateDate":"2024-07-04","timestamp":1720017769000,"abstract":"  Large language models (LLMs), known for their exceptional reasoning\ncapabilities, generalizability, and fluency across diverse domains, present a\npromising avenue for enhancing speech-related tasks. In this paper, we focus on\nintegrating decoder-only LLMs to the task of speech-to-text translation (S2TT).\nWe propose a decoder-only architecture that enables the LLM to directly consume\nthe encoded speech representation and generate the text translation.\nAdditionally, we investigate the effects of different parameter-efficient\nfine-tuning techniques and task formulation. Our model achieves\nstate-of-the-art performance on CoVoST 2 and FLEURS among models trained\nwithout proprietary data. We also conduct analyses to validate the design\nchoices of our proposed model and bring insights to the integration of LLMs to\nS2TT.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"OjWwgw9PSjmGOLYpDQi9p8E5gaSn7UfZSXAQ-Bs2uNQ","pdfSize":"283077"}