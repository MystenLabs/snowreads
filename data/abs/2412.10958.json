{
  "id": "2412.10958",
  "title": "SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer",
  "authors": "Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu,\n  Jindong Wang, Bhiksha Raj, Zicheng Liu, Emad Barsoum",
  "authorsParsed": [
    [
      "Chen",
      "Hao",
      ""
    ],
    [
      "Wang",
      "Ze",
      ""
    ],
    [
      "Li",
      "Xiang",
      ""
    ],
    [
      "Sun",
      "Ximeng",
      ""
    ],
    [
      "Chen",
      "Fangyi",
      ""
    ],
    [
      "Liu",
      "Jiang",
      ""
    ],
    [
      "Wang",
      "Jindong",
      ""
    ],
    [
      "Raj",
      "Bhiksha",
      ""
    ],
    [
      "Liu",
      "Zicheng",
      ""
    ],
    [
      "Barsoum",
      "Emad",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 14 Dec 2024 20:29:29 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 20 Dec 2024 16:59:40 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734208169000,
  "abstract": "  Efficient image tokenization with high compression ratios remains a critical\nchallenge for training generative models. We present SoftVQ-VAE, a continuous\nimage tokenizer that leverages soft categorical posteriors to aggregate\nmultiple codewords into each latent token, substantially increasing the\nrepresentation capacity of the latent space. When applied to Transformer-based\narchitectures, our approach compresses 256x256 and 512x512 images using as few\nas 32 or 64 1-dimensional tokens. Not only does SoftVQ-VAE show consistent and\nhigh-quality reconstruction, more importantly, it also achieves\nstate-of-the-art and significantly faster image generation results across\ndifferent denoising-based generative models. Remarkably, SoftVQ-VAE improves\ninference throughput by up to 18x for generating 256x256 images and 55x for\n512x512 images while achieving competitive FID scores of 1.78 and 2.21 for\nSiT-XL. It also improves the training efficiency of the generative models by\nreducing the number of training iterations by 2.3x while maintaining comparable\nperformance. With its fully-differentiable design and semantic-rich latent\nspace, our experiment demonstrates that SoftVQ-VAE achieves efficient\ntokenization without compromising generation quality, paving the way for more\nefficient generative models. Code and model are released.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "GrRfbB0Vm2FvwB9aND0rLEyR3w9t17J4OattrHHffS4",
  "pdfSize": "21515930"
}