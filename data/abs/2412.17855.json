{
  "id": "2412.17855",
  "title": "Foxtsage vs. Adam: Revolution or Evolution in Optimization?",
  "authors": "Sirwan A. Aula and Tarik A. Rashid",
  "authorsParsed": [
    [
      "Aula",
      "Sirwan A.",
      ""
    ],
    [
      "Rashid",
      "Tarik A.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 03:20:58 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1734664858000,
  "abstract": "  Optimization techniques are pivotal in neural network training, shaping both\npredictive performance and convergence efficiency. This study introduces\nFoxtsage, a novel hybrid optimisation approach that integrates the Hybrid\nFOX-TSA with Stochastic Gradient Descent for training Multi-Layer Perceptron\nmodels. The proposed Foxtsage method is benchmarked against the widely adopted\nAdam optimizer across multiple standard datasets, focusing on key performance\nmetrics such as training loss, accuracy, precision, recall, F1-score, and\ncomputational time. Experimental results demonstrate that Foxtsage achieves a\n42.03% reduction in loss mean (Foxtsage: 9.508, Adam: 16.402) and a 42.19%\nimprovement in loss standard deviation (Foxtsage: 20.86, Adam: 36.085),\nreflecting enhanced consistency and robustness. Modest improvements in accuracy\nmean (0.78%), precision mean (0.91%), recall mean (1.02%), and F1-score mean\n(0.89%) further underscore its predictive performance. However, these gains are\naccompanied by an increased computational cost, with a 330.87% rise in time\nmean (Foxtsage: 39.541 seconds, Adam: 9.177 seconds). By effectively combining\nthe global search capabilities of FOX-TSA with the stability and adaptability\nof SGD, Foxtsage presents itself as a robust and viable alternative for neural\nnetwork optimization tasks.\n",
  "subjects": [
    "Computer Science/Neural and Evolutionary Computing",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "TxS3DmxNF0scfY5JMi9o1YAEw8kkYvY_MHlj29q-LlM",
  "pdfSize": "1277602"
}