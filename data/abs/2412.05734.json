{"id":"2412.05734","title":"PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage","authors":"Yuzhou Nie, Zhun Wang, Ye Yu, Xian Wu, Xuandong Zhao, Wenbo Guo, Dawn\n  Song","authorsParsed":[["Nie","Yuzhou",""],["Wang","Zhun",""],["Yu","Ye",""],["Wu","Xian",""],["Zhao","Xuandong",""],["Guo","Wenbo",""],["Song","Dawn",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 20:09:01 GMT"}],"updateDate":"2024-12-10","timestamp":1733602141000,"abstract":"  Recent studies have discovered that LLMs have serious privacy leakage\nconcerns, where an LLM may be fooled into outputting private information under\ncarefully crafted adversarial prompts. These risks include leaking system\nprompts, personally identifiable information, training data, and model\nparameters. Most existing red-teaming approaches for privacy leakage rely on\nhumans to craft the adversarial prompts. A few automated methods are proposed\nfor system prompt extraction, but they cannot be applied to more severe risks\n(e.g., training data extraction) and have limited effectiveness even for system\nprompt extraction.\n  In this paper, we propose PrivAgent, a novel black-box red-teaming framework\nfor LLM privacy leakage. We formulate different risks as a search problem with\na unified attack goal. Our framework trains an open-source LLM through\nreinforcement learning as the attack agent to generate adversarial prompts for\ndifferent target models under different risks. We propose a novel reward\nfunction to provide effective and fine-grained rewards for the attack agent.\nFinally, we introduce customizations to better fit our general framework to\nsystem prompt extraction and training data extraction. Through extensive\nevaluations, we first show that PrivAgent outperforms existing automated\nmethods in system prompt leakage against six popular LLMs. Notably, our\napproach achieves a 100% success rate in extracting system prompts from\nreal-world applications in OpenAI's GPT Store. We also show PrivAgent's\neffectiveness in extracting training data from an open-source LLM with a\nsuccess rate of 5.9%. We further demonstrate PrivAgent's effectiveness in\nevading the existing guardrail defense and its helpfulness in enabling better\nsafety alignment. Finally, we validate our customized designs through a\ndetailed ablation study. We release our code here\nhttps://github.com/rucnyz/RedAgent.\n","subjects":["Computer Science/Cryptography and Security","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"xlGxZQbucRna8lNyk5-P9I85jE-mTev0mEh8DcoPTZc","pdfSize":"860443"}