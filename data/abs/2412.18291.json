{
  "id": "2412.18291",
  "title": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation",
  "authors": "Junyi Lu, Xiaojia Li, Zihan Hua, Lei Yu, Shiqi Cheng, Li Yang, Fengjun\n  Zhang, and Chun Zuo",
  "authorsParsed": [
    [
      "Lu",
      "Junyi",
      ""
    ],
    [
      "Li",
      "Xiaojia",
      ""
    ],
    [
      "Hua",
      "Zihan",
      ""
    ],
    [
      "Yu",
      "Lei",
      ""
    ],
    [
      "Cheng",
      "Shiqi",
      ""
    ],
    [
      "Yang",
      "Li",
      ""
    ],
    [
      "Zhang",
      "Fengjun",
      ""
    ],
    [
      "Zuo",
      "Chun",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 24 Dec 2024 08:53:54 GMT"
    },
    {
      "version": "v2",
      "created": "Sat, 25 Jan 2025 08:48:32 GMT"
    }
  ],
  "updateDate": "2025-01-28",
  "timestamp": 1735030434000,
  "abstract": "  Code review is a vital but demanding aspect of software development,\ngenerating significant interest in automating review comments. Traditional\nevaluation methods for these comments, primarily based on text similarity, face\ntwo major challenges: inconsistent reliability of human-authored comments in\nopen-source projects and the weak correlation of text similarity with\nobjectives like enhancing code quality and detecting defects.\n  This study empirically analyzes benchmark comments using a novel set of\ncriteria informed by prior research and developer interviews. We then similarly\nrevisit the evaluation of existing methodologies. Our evaluation framework,\nDeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a\ncomprehensive reassessment of current techniques based on the criteria set.\nBesides, we also introduce an innovative and efficient baseline, LLM-Reviewer,\nleveraging the few-shot learning capabilities of LLMs for a target-oriented\ncomparison.\n  Our research highlights the limitations of text similarity metrics, finding\nthat less than 10% of benchmark comments are high quality for automation. In\ncontrast, DeepCRCEval effectively distinguishes between high and low-quality\ncomments, proving to be a more reliable evaluation mechanism. Incorporating LLM\nevaluators into DeepCRCEval significantly boosts efficiency, reducing time and\ncost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates\nsignificant potential of focusing task real targets in comment generation.\n",
  "subjects": [
    "Computer Science/Software Engineering",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "i_jFi2miuMXVIOR7Ws4UBSN9XpVCRD_2M0BzwO2gUdY",
  "pdfSize": "1059272"
}