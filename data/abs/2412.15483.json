{
  "id": "2412.15483",
  "title": "Task-Specific Preconditioner for Cross-Domain Few-Shot Learning",
  "authors": "Suhyun Kang, Jungwon Park, Wonseok Lee, Wonjong Rhee",
  "authorsParsed": [
    [
      "Kang",
      "Suhyun",
      ""
    ],
    [
      "Park",
      "Jungwon",
      ""
    ],
    [
      "Lee",
      "Wonseok",
      ""
    ],
    [
      "Rhee",
      "Wonjong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 01:33:43 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734658423000,
  "abstract": "  Cross-Domain Few-Shot Learning~(CDFSL) methods typically parameterize models\nwith task-agnostic and task-specific parameters. To adapt task-specific\nparameters, recent approaches have utilized fixed optimization strategies,\ndespite their potential sub-optimality across varying domains or target tasks.\nTo address this issue, we propose a novel adaptation mechanism called\nTask-Specific Preconditioned gradient descent~(TSP). Our method first\nmeta-learns Domain-Specific Preconditioners~(DSPs) that capture the\ncharacteristics of each meta-training domain, which are then linearly combined\nusing task-coefficients to form the Task-Specific Preconditioner. The\npreconditioner is applied to gradient descent, making the optimization adaptive\nto the target task. We constrain our preconditioners to be positive definite,\nguiding the preconditioned gradient toward the direction of steepest descent.\nEmpirical evaluations on the Meta-Dataset show that TSP achieves\nstate-of-the-art performance across diverse experimental scenarios.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "QIGjHaUiSihFY-9VFcqFWaQvQtMQTLVFlJlp6v8fP8U",
  "pdfSize": "3310971"
}