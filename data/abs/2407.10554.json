{"id":"2407.10554","title":"Beyond Generative Artificial Intelligence: Roadmap for Natural Language\n  Generation","authors":"Mar\\'ia Mir\\'o Maestre, Iv\\'an Mart\\'inez-Murillo, Tania J. Martin,\n  Borja Navarro-Colorado, Antonio Ferr\\'andez, Armando Su\\'arez Cueto and Elena\n  Lloret","authorsParsed":[["Maestre","María Miró",""],["Martínez-Murillo","Iván",""],["Martin","Tania J.",""],["Navarro-Colorado","Borja",""],["Ferrández","Antonio",""],["Cueto","Armando Suárez",""],["Lloret","Elena",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 09:07:07 GMT"}],"updateDate":"2024-07-16","timestamp":1721034427000,"abstract":"  Generative Artificial Intelligence has grown exponentially as a result of\nLarge Language Models (LLMs). This has been possible because of the impressive\nperformance of deep learning methods created within the field of Natural\nLanguage Processing (NLP) and its subfield Natural Language Generation (NLG),\nwhich is the focus of this paper. Within the growing LLM family are the popular\nGPT-4, Bard and more specifically, tools such as ChatGPT have become a\nbenchmark for other LLMs when solving most of the tasks involved in NLG\nresearch. This scenario poses new questions about the next steps for NLG and\nhow the field can adapt and evolve to deal with new challenges in the era of\nLLMs. To address this, the present paper conducts a review of a representative\nsample of surveys recently published in NLG. By doing so, we aim to provide the\nscientific community with a research roadmap to identify which NLG aspects are\nstill not suitably addressed by LLMs, as well as suggest future lines of\nresearch that should be addressed going forward.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"35AfMO0bKl3RTwXZ3yFVpDO_W-YcZn2RIHeL-vnZVMg","pdfSize":"10431036","objectId":"0x535c4c551fa667ed9cf9d4bd85be1cabb1ec663050fcb0a4c246a6210b2dd5e7","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
