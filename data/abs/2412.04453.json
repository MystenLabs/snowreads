{"id":"2412.04453","title":"NaVILA: Legged Robot Vision-Language-Action Model for Navigation","authors":"An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou,\n  Jan Kautz, Erdem B{\\i}y{\\i}k, Hongxu Yin, Sifei Liu, Xiaolong Wang","authorsParsed":[["Cheng","An-Chieh",""],["Ji","Yandong",""],["Yang","Zhaojing",""],["Gongye","Zaitian",""],["Zou","Xueyan",""],["Kautz","Jan",""],["Bıyık","Erdem",""],["Yin","Hongxu",""],["Liu","Sifei",""],["Wang","Xiaolong",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 18:58:17 GMT"},{"version":"v2","created":"Mon, 17 Feb 2025 18:27:27 GMT"}],"updateDate":"2025-02-18","timestamp":1733425097000,"abstract":"  This paper proposes to solve the problem of Vision-and-Language Navigation\nwith legged robots, which not only provides a flexible way for humans to\ncommand but also allows the robot to navigate through more challenging and\ncluttered scenes. However, it is non-trivial to translate human language\ninstructions all the way to low-level leg joint actions. We propose NaVILA, a\n2-level framework that unifies a Vision-Language-Action model (VLA) with\nlocomotion skills. Instead of directly predicting low-level actions from VLA,\nNaVILA first generates mid-level actions with spatial information in the form\nof language, (e.g., \"moving forward 75cm\"), which serves as an input for a\nvisual locomotion RL policy for execution. NaVILA substantially improves\nprevious approaches on existing benchmarks. The same advantages are\ndemonstrated in our newly developed benchmarks with IsaacLab, featuring more\nrealistic scenes, low-level controls, and real-world robot experiments. We show\nmore results at https://navila-bot.github.io/\n","subjects":["Computer Science/Robotics","Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"p495EYEy7cS5pQPyssvj3VTkBZvMfr6F55z4xORVxWw","pdfSize":"15510695"}