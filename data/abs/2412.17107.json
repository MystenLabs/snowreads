{
  "id": "2412.17107",
  "title": "Grams: Gradient Descent with Adaptive Momentum Scaling",
  "authors": "Yang Cao, Xiaoyu Li, Zhao Song",
  "authorsParsed": [
    [
      "Cao",
      "Yang",
      ""
    ],
    [
      "Li",
      "Xiaoyu",
      ""
    ],
    [
      "Song",
      "Zhao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 22 Dec 2024 17:39:32 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734889172000,
  "abstract": "  We introduce \\textbf{Gr}adient Descent with \\textbf{A}daptive\n\\textbf{M}omentum \\textbf{S}caling (\\textbf{Grams}), a novel optimization\nalgorithm that decouples the direction and magnitude of parameter updates in\ndeep learning. Unlike traditional optimizers that directly integrate momentum\ninto updates, Grams separates the update direction, derived from current\ngradients, from momentum, which is used solely for adaptive magnitude scaling.\nThis approach enables Grams to achieve improved loss descent compared to\nstate-of-the-art cautious and momentum-based optimizers. We establish a global\nconvergence guarantee for Grams and validate its effectiveness through\nextensive empirical evaluations. The results demonstrate Grams' superior\nperformance, including faster convergence and better generalization, compared\nto widely-used optimizers such as Adam, Lion, and their cautious variants. Our\nresults highlight Grams' potential as a transformative approach for efficient\noptimization in large-scale machine learning.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Data Structures and Algorithms",
    "Mathematics/Optimization and Control"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "F2S3cKccRpMf8mZAb_UQRTy3BuojBWDxSqNlKrPUk7o",
  "pdfSize": "756431"
}