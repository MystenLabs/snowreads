{
  "id": "2412.04738",
  "title": "DHIL-GT: Scalable Graph Transformer with Decoupled Hierarchy Labeling",
  "authors": "Ningyi Liao, Zihao Yu, Siqiang Luo",
  "authorsParsed": [
    [
      "Liao",
      "Ningyi",
      ""
    ],
    [
      "Yu",
      "Zihao",
      ""
    ],
    [
      "Luo",
      "Siqiang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 6 Dec 2024 02:59:01 GMT"
    }
  ],
  "updateDate": "2024-12-09",
  "timestamp": 1733453941000,
  "abstract": "  Graph Transformer (GT) has recently emerged as a promising neural network\narchitecture for learning graph-structured data. However, its global attention\nmechanism with quadratic complexity concerning the graph scale prevents wider\napplication to large graphs. While current methods attempt to enhance GT\nscalability by altering model architecture or encoding hierarchical graph data,\nour analysis reveals that these models still suffer from the computational\nbottleneck related to graph-scale operations. In this work, we target the GT\nscalability issue and propose DHIL-GT, a scalable Graph Transformer that\nsimplifies network learning by fully decoupling the graph computation to a\nseparate stage in advance. DHIL-GT effectively retrieves hierarchical\ninformation by exploiting the graph labeling technique, as we show that the\ngraph label hierarchy is more informative than plain adjacency by offering\nglobal connections while promoting locality, and is particularly suitable for\nhandling complex graph patterns such as heterophily. We further design subgraph\nsampling and positional encoding schemes for precomputing model input on top of\ngraph labels in an end-to-end manner. The training stage thus favorably removes\ngraph-related computations, leading to ideal mini-batch capability and GPU\nutilization. Notably, the precomputation and training processes of DHIL-GT\nachieve complexities linear to the number of graph edges and nodes,\nrespectively. Extensive experiments demonstrate that DHIL-GT is efficient in\nterms of computational boost and mini-batch capability over existing scalable\nGraph Transformer designs on large-scale benchmarks, while achieving top-tier\neffectiveness on both homophilous and heterophilous graphs.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "Rpi1DphmoJbm_yLLBDiiZMtZ667kuhAgiFeTP3F82s4",
  "pdfSize": "3220143"
}