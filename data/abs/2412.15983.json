{"id":"2412.15983","title":"Never Reset Again: A Mathematical Framework for Continual Inference in\n  Recurrent Neural Networks","authors":"Bojian Yin, Federico Corradi","authorsParsed":[["Yin","Bojian",""],["Corradi","Federico",""]],"versions":[{"version":"v1","created":"Fri, 20 Dec 2024 15:24:28 GMT"}],"updateDate":"2024-12-23","timestamp":1734708268000,"abstract":"  Recurrent Neural Networks (RNNs) are widely used for sequential processing\nbut face fundamental limitations with continual inference due to state\nsaturation, requiring disruptive hidden state resets. However, reset-based\nmethods impose synchronization requirements with input boundaries and increase\ncomputational costs at inference. To address this, we propose an adaptive loss\nfunction that eliminates the need for resets during inference while preserving\nhigh accuracy over extended sequences. By combining cross-entropy and\nKullback-Leibler divergence, the loss dynamically modulates the gradient based\non input informativeness, allowing the network to differentiate meaningful data\nfrom noise and maintain stable representations over time. Experimental results\ndemonstrate that our reset-free approach outperforms traditional reset-based\nmethods when applied to a variety of RNNs, particularly in continual tasks,\nenhancing both the theoretical and practical capabilities of RNNs for streaming\napplications.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ckxsOUbyIigmR5ccPnbMiOWiE3y82QL2n4Bl1LlluOo","pdfSize":"1638173"}