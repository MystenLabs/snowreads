{
  "id": "2412.20382",
  "title": "Natural Language Fine-Tuning",
  "authors": "Jia Liu, Yue Wang, Zhiqi Lin, Min Chen, Yixue Hao, Long Hu",
  "authorsParsed": [
    [
      "Liu",
      "Jia",
      ""
    ],
    [
      "Wang",
      "Yue",
      ""
    ],
    [
      "Lin",
      "Zhiqi",
      ""
    ],
    [
      "Chen",
      "Min",
      ""
    ],
    [
      "Hao",
      "Yixue",
      ""
    ],
    [
      "Hu",
      "Long",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 29 Dec 2024 07:02:45 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735455765000,
  "abstract": "  Large language model fine-tuning techniques typically depend on extensive\nlabeled data, external guidance, and feedback, such as human alignment, scalar\nrewards, and demonstration. However, in practical application, the scarcity of\nspecific knowledge poses unprecedented challenges to existing fine-tuning\ntechniques. In this paper, focusing on fine-tuning tasks in specific domains\nwith limited data, we introduce Natural Language Fine-Tuning (NLFT), which\nutilizes natural language for fine-tuning for the first time. By leveraging the\nstrong language comprehension capability of the target LM, NLFT attaches the\nguidance of natural language to the token-level outputs. Then, saliency tokens\nare identified with calculated probabilities. Since linguistic information is\neffectively utilized in NLFT, our proposed method significantly reduces\ntraining costs. It markedly enhances training efficiency, comprehensively\noutperforming reinforcement fine-tuning algorithms in accuracy, time-saving,\nand resource conservation. Additionally, on the macro level, NLFT can be viewed\nas a token-level fine-grained optimization of SFT, thereby efficiently\nreplacing the SFT process without the need for warm-up (as opposed to ReFT\nrequiring multiple rounds of warm-up with SFT). Compared to SFT, NLFT does not\nincrease the algorithmic complexity, maintaining O(n). Extensive experiments on\nthe GSM8K dataset demonstrate that NLFT, with only 50 data instances, achieves\nan accuracy increase that exceeds SFT by 219%. Compared to ReFT, the time\ncomplexity and space complexity of NLFT are reduced by 78.27% and 92.24%,\nrespectively. The superior technique of NLFT is paving the way for the\ndeployment of various innovative LLM fine-tuning applications when resources\nare limited at network edges.\n  Our code has been released at https://github.com/Julia-LiuJ/NLFT.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "sZPIH22A0dq9pYomkMujWPX-CK3HmuW137ieKlF18uw",
  "pdfSize": "2063438"
}