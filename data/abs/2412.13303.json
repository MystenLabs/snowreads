{
  "id": "2412.13303",
  "title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
  "authors": "Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc,\n  Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel\n  Tuzel, Hadi Pouransari",
  "authorsParsed": [
    [
      "Vasu",
      "Pavan Kumar Anasosalu",
      ""
    ],
    [
      "Faghri",
      "Fartash",
      ""
    ],
    [
      "Li",
      "Chun-Liang",
      ""
    ],
    [
      "Koc",
      "Cem",
      ""
    ],
    [
      "True",
      "Nate",
      ""
    ],
    [
      "Antony",
      "Albert",
      ""
    ],
    [
      "Santhanam",
      "Gokul",
      ""
    ],
    [
      "Gabriel",
      "James",
      ""
    ],
    [
      "Grasch",
      "Peter",
      ""
    ],
    [
      "Tuzel",
      "Oncel",
      ""
    ],
    [
      "Pouransari",
      "Hadi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 20:09:55 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734466195000,
  "abstract": "  Scaling the input image resolution is essential for enhancing the performance\nof Vision Language Models (VLMs), particularly in text-rich image understanding\ntasks. However, popular visual encoders such as ViTs become inefficient at high\nresolutions due to the large number of tokens and high encoding latency caused\nby stacked self-attention layers. At different operational resolutions, the\nvision encoder of a VLM can be optimized along two axes: reducing encoding\nlatency and minimizing the number of visual tokens passed to the LLM, thereby\nlowering overall latency. Based on a comprehensive efficiency analysis of the\ninterplay between image resolution, vision latency, token count, and LLM size,\nwe introduce FastVLM, a model that achieves an optimized trade-off between\nlatency, model size and accuracy. FastVLM incorporates FastViTHD, a novel\nhybrid vision encoder designed to output fewer tokens and significantly reduce\nencoding time for high-resolution images. Unlike previous methods, FastVLM\nachieves the optimal balance between visual token count and image resolution\nsolely by scaling the input image, eliminating the need for additional token\npruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM\nachieves 3.2$\\times$ improvement in time-to-first-token (TTFT) while\nmaintaining similar performance on VLM benchmarks compared to prior works.\nCompared to LLaVa-OneVision at the highest resolution (1152$\\times$1152),\nFastVLM achieves comparable performance on key benchmarks like SeedBench and\nMMMU, using the same 0.5B LLM, but with 85$\\times$ faster TTFT and a vision\nencoder that is 3.4$\\times$ smaller.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "HujmrtLJT0_nlkmRVAVV69SxjiFqvCCEwSVxCAEFnDU",
  "pdfSize": "2515907"
}