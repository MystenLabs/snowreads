{"id":"2412.13817","title":"Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection","authors":"Le Yang, Ziwei Zheng, Boxu Chen, Zhengyu Zhao, Chenhao Lin, Chao Shen","authorsParsed":[["Yang","Le",""],["Zheng","Ziwei",""],["Chen","Boxu",""],["Zhao","Zhengyu",""],["Lin","Chenhao",""],["Shen","Chao",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 13:04:30 GMT"},{"version":"v2","created":"Sun, 29 Dec 2024 13:32:10 GMT"}],"updateDate":"2024-12-31","timestamp":1734527070000,"abstract":"  Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"17iINO1U85KgRK1h3Kf8SDDrRoBNeUk59KbQFkWCGjs","pdfSize":"6935697"}