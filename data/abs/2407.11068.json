{"id":"2407.11068","title":"Show, Don't Tell: Evaluating Large Language Models Beyond Textual\n  Understanding with ChildPlay","authors":"Gon\\c{c}alo Hora de Carvalho and Oscar Knap and Robert Pollice","authorsParsed":[["de Carvalho","Gon√ßalo Hora",""],["Knap","Oscar",""],["Pollice","Robert",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 14:17:26 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 13:47:30 GMT"},{"version":"v3","created":"Sun, 18 Aug 2024 19:44:35 GMT"}],"updateDate":"2024-08-20","timestamp":1720793846000,"abstract":"  We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess\nbroader cognitive functions, particularly in non-linguistic domains. Our\napproach extends beyond standard linguistic benchmarks by incorporating games\nlike Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess\nstrategic thinking and decision-making. To evaluate the models' ability to\ngeneralize beyond their training data, we introduce two additional games. The\nfirst game, LEGO Connect Language (LCL), tests the models' capacity to\nunderstand spatial logic and follow assembly instructions. The second game, the\ngame of shapes, challenges the models to identify shapes represented by 1s\nwithin a matrix of zeros, further testing their spatial reasoning skills. This\n\"show, don't tell\" strategy uses games instead of simply querying the models.\nOur results show that despite their proficiency on standard benchmarks, GPT-3.5\nand GPT-4's abilities to play and reason about fully observable games without\npre-training is mediocre. Both models fail to anticipate losing moves in\nTic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly.\nWhile GPT-4 shows some success in the game of shapes, both models fail at the\nassembly tasks presented in the LCL game. These results suggest that while GPT\nmodels can emulate conversational proficiency and basic rule comprehension,\ntheir performance in strategic gameplay and spatial reasoning tasks is very\nlimited. Importantly, this reveals a blind spot in current LLM benchmarks that\nwe highlight with our gameplay benchmark suite ChildPlay\n(https://github.com/child-play-neurips/child-play). Our findings provide a\ncautionary tale about claims of emergent intelligence and reasoning\ncapabilities of LLMs that are roughly the size of GPT-3.5 and GPT-4.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-3i7ohh9pqM5-tuFI1-a2meglVNP-bliyRlMCUUds14","pdfSize":"2682339"}