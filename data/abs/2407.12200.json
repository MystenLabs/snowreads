{"id":"2407.12200","title":"This Probably Looks Exactly Like That: An Invertible Prototypical\n  Network","authors":"Zachariah Carmichael, Timothy Redgrave, Daniel Gonzalez Cedre, Walter\n  J. Scheirer","authorsParsed":[["Carmichael","Zachariah",""],["Redgrave","Timothy",""],["Cedre","Daniel Gonzalez",""],["Scheirer","Walter J.",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 21:51:02 GMT"}],"updateDate":"2024-07-18","timestamp":1721166662000,"abstract":"  We combine concept-based neural networks with generative, flow-based\nclassifiers into a novel, intrinsically explainable, exactly invertible\napproach to supervised learning. Prototypical neural networks, a type of\nconcept-based neural network, represent an exciting way forward in realizing\nhuman-comprehensible machine learning without concept annotations, but a\nhuman-machine semantic gap continues to haunt current approaches. We find that\nreliance on indirect interpretation functions for prototypical explanations\nimposes a severe limit on prototypes' informative power. From this, we posit\nthat invertibly learning prototypes as distributions over the latent space\nprovides more robust, expressive, and interpretable modeling. We propose one\nsuch model, called ProtoFlow, by composing a normalizing flow with Gaussian\nmixture models. ProtoFlow (1) sets a new state-of-the-art in joint generative\nand predictive modeling and (2) achieves predictive performance comparable to\nexisting prototypical neural networks while enabling richer interpretation.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gWFZ_HFiXlCDVUnxmNsMiGPwxZnCjG3cswZcun7E3uE","pdfSize":"10063014"}