{"id":"2407.02039","title":"Prompt Stability Scoring for Text Annotation with Large Language Models","authors":"Christopher Barrie, Elli Palaiologou, Petter T\\\"ornberg","authorsParsed":[["Barrie","Christopher",""],["Palaiologou","Elli",""],["TÃ¶rnberg","Petter",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 08:11:18 GMT"}],"updateDate":"2024-07-04","timestamp":1719907878000,"abstract":"  Researchers are increasingly using language models (LMs) for text annotation.\nThese approaches rely only on a prompt telling the model to return a given\noutput according to a set of instructions. The reproducibility of LM outputs\nmay nonetheless be vulnerable to small changes in the prompt design. This calls\ninto question the replicability of classification routines. To tackle this\nproblem, researchers have typically tested a variety of semantically similar\nprompts to determine what we call \"prompt stability.\" These approaches remain\nad-hoc and task specific. In this article, we propose a general framework for\ndiagnosing prompt stability by adapting traditional approaches to intra- and\ninter-coder reliability scoring. We call the resulting metric the Prompt\nStability Score (PSS) and provide a Python package PromptStability for its\nestimation. Using six different datasets and twelve outcomes, we classify >150k\nrows of data to: a) diagnose when prompt stability is low; and b) demonstrate\nthe functionality of the package. We conclude by providing best practice\nrecommendations for applied researchers.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mIJAtX7EJ8YJ3o4Gf-vST5mu6fexHhw3WiT9iX5vJVY","pdfSize":"2610895"}