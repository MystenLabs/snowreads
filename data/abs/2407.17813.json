{"id":"2407.17813","title":"Enhancing Model Performance: Another Approach to Vision-Language\n  Instruction Tuning","authors":"Vedanshu, MM Tripathi and Bhavnesh Jaint","authorsParsed":[["Vedanshu","",""],["Tripathi","MM",""],["Jaint","Bhavnesh",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 06:59:15 GMT"}],"updateDate":"2024-07-26","timestamp":1721890755000,"abstract":"  The integration of large language models (LLMs) with vision-language (VL)\ntasks has been a transformative development in the realm of artificial\nintelligence, highlighting the potential of LLMs as a versatile general-purpose\nchatbot. However, the current trend in this evolution focuses on the\nintegration of vision and language to create models that can operate in more\ndiverse and real-world contexts. We present a novel approach, termed Bottleneck\nAdapter, specifically crafted for enhancing the multimodal functionalities of\nthese complex models, enabling joint optimization of the entire multimodal LLM\nframework through a process known as Multimodal Model Tuning (MMT). Our\napproach utilizes lightweight adapters to connect the image encoder and LLM\nwithout the need for large, complex neural networks. Unlike the conventional\nmodular training schemes, our approach adopts an end-to-end optimization\nregime, which, when combined with the adapters, facilitates the joint\noptimization using a significantly smaller parameter set. Our method exhibits\nrobust performance with 90.12\\% accuracy, outperforming both human-level\nperformance (88.4\\%) and LaVIN-7B (89.41\\%).\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"34Eur-c5D5RJwm_Epd49s5_Wg1yXTQQEpJ-rO2frwhw","pdfSize":"156928"}