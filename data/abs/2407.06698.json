{"id":"2407.06698","title":"PSPU: Enhanced Positive and Unlabeled Learning by Leveraging Pseudo\n  Supervision","authors":"Chengjie Wang, Chengming Xu, Zhenye Gan, Jianlong Hu, Wenbing Zhu,\n  Lizhuag Ma","authorsParsed":[["Wang","Chengjie",""],["Xu","Chengming",""],["Gan","Zhenye",""],["Hu","Jianlong",""],["Zhu","Wenbing",""],["Ma","Lizhuag",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 09:19:01 GMT"}],"updateDate":"2024-07-10","timestamp":1720516741000,"abstract":"  Positive and Unlabeled (PU) learning, a binary classification model trained\nwith only positive and unlabeled data, generally suffers from overfitted risk\nestimation due to inconsistent data distributions. To address this, we\nintroduce a pseudo-supervised PU learning framework (PSPU), in which we train\nthe PU model first, use it to gather confident samples for the pseudo\nsupervision, and then apply these supervision to correct the PU model's weights\nby leveraging non-PU objectives. We also incorporate an additional consistency\nloss to mitigate noisy sample effects. Our PSPU outperforms recent PU learning\nmethods significantly on MNIST, CIFAR-10, CIFAR-100 in both balanced and\nimbalanced settings, and enjoys competitive performance on MVTecAD for\nindustrial anomaly detection.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"0nln5aGAzytiagr2w0f6Bt6Zkh-ur3TmHF4WWXkTOyE","pdfSize":"559395"}