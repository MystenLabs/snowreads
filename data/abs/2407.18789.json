{"id":"2407.18789","title":"Granularity is crucial when applying differential privacy to text: An\n  investigation for neural machine translation","authors":"Doan Nam Long Vu, Timour Igamberdiev, Ivan Habernal","authorsParsed":[["Vu","Doan Nam Long",""],["Igamberdiev","Timour",""],["Habernal","Ivan",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 14:52:37 GMT"}],"updateDate":"2024-07-29","timestamp":1722005557000,"abstract":"  Applying differential privacy (DP) by means of the DP-SGD algorithm to\nprotect individual data points during training is becoming increasingly popular\nin NLP. However, the choice of granularity at which DP is applied is often\nneglected. For example, neural machine translation (NMT) typically operates on\nthe sentence-level granularity. From the perspective of DP, this setup assumes\nthat each sentence belongs to a single person and any two sentences in the\ntraining dataset are independent. This assumption is however violated in many\nreal-world NMT datasets, e.g. those including dialogues. For proper application\nof DP we thus must shift from sentences to entire documents. In this paper, we\ninvestigate NMT at both the sentence and document levels, analyzing the\nprivacy/utility trade-off for both scenarios, and evaluating the risks of not\nusing the appropriate privacy granularity in terms of leaking personally\nidentifiable information (PII). Our findings indicate that the document-level\nNMT system is more resistant to membership inference attacks, emphasizing the\nsignificance of using the appropriate granularity when working with DP.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"9RVdQ1uBJDs73gnfqamXy0gCviMDoEOFB_SfgQmAr3k","pdfSize":"990202"}