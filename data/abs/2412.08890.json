{
  "id": "2412.08890",
  "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
  "authors": "Junhyuck Kim, Jongho Park, Jaewoong Cho, Dimitris Papailiopoulos",
  "authorsParsed": [
    [
      "Kim",
      "Junhyuck",
      ""
    ],
    [
      "Park",
      "Jongho",
      ""
    ],
    [
      "Cho",
      "Jaewoong",
      ""
    ],
    [
      "Papailiopoulos",
      "Dimitris",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 12 Dec 2024 03:00:29 GMT"
    }
  ],
  "updateDate": "2024-12-13",
  "timestamp": 1733972429000,
  "abstract": "  We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "-qrMRzqyQzIqf8MhCFxwPFUjvZj71kraMgtSuHc-F9A",
  "pdfSize": "1855012"
}