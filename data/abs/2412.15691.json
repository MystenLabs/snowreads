{
  "id": "2412.15691",
  "title": "Exploiting Multimodal Spatial-temporal Patterns for Video Object\n  Tracking",
  "authors": "Xiantao Hu, Ying Tai, Xu Zhao, Chen Zhao, Zhenyu Zhang, Jun Li, Bineng\n  Zhong, Jian Yang",
  "authorsParsed": [
    [
      "Hu",
      "Xiantao",
      ""
    ],
    [
      "Tai",
      "Ying",
      ""
    ],
    [
      "Zhao",
      "Xu",
      ""
    ],
    [
      "Zhao",
      "Chen",
      ""
    ],
    [
      "Zhang",
      "Zhenyu",
      ""
    ],
    [
      "Li",
      "Jun",
      ""
    ],
    [
      "Zhong",
      "Bineng",
      ""
    ],
    [
      "Yang",
      "Jian",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 09:10:17 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734685817000,
  "abstract": "  Multimodal tracking has garnered widespread attention as a result of its\nability to effectively address the inherent limitations of traditional RGB\ntracking. However, existing multimodal trackers mainly focus on the fusion and\nenhancement of spatial features or merely leverage the sparse temporal\nrelationships between video frames. These approaches do not fully exploit the\ntemporal correlations in multimodal videos, making it difficult to capture the\ndynamic changes and motion information of targets in complex scenarios. To\nalleviate this problem, we propose a unified multimodal spatial-temporal\ntracking approach named STTrack. In contrast to previous paradigms that solely\nrelied on updating reference information, we introduced a temporal state\ngenerator (TSG) that continuously generates a sequence of tokens containing\nmultimodal temporal information. These temporal information tokens are used to\nguide the localization of the target in the next time state, establish\nlong-range contextual relationships between video frames, and capture the\ntemporal trajectory of the target. Furthermore, at the spatial level, we\nintroduced the mamba fusion and background suppression interactive (BSI)\nmodules. These modules establish a dual-stage mechanism for coordinating\ninformation interaction and fusion between modalities. Extensive comparisons on\nfive benchmark datasets illustrate that STTrack achieves state-of-the-art\nperformance across various multimodal tracking scenarios. Code is available at:\nhttps://github.com/NJU-PCALab/STTrack.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "wrYjEjccaWOlgl18bITfvG_NglVqR_LT6DbNvwHsePk",
  "pdfSize": "2357203"
}