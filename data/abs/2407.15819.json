{"id":"2407.15819","title":"Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight","authors":"Ziyuan Huang, Kaixiang Ji, Biao Gong, Zhiwu Qing, Qinglong Zhang,\n  Kecheng Zheng, Jian Wang, Jingdong Chen, Ming Yang","authorsParsed":[["Huang","Ziyuan",""],["Ji","Kaixiang",""],["Gong","Biao",""],["Qing","Zhiwu",""],["Zhang","Qinglong",""],["Zheng","Kecheng",""],["Wang","Jian",""],["Chen","Jingdong",""],["Yang","Ming",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 17:33:49 GMT"}],"updateDate":"2024-07-23","timestamp":1721669629000,"abstract":"  This paper introduces Chain-of-Sight, a vision-language bridge module that\naccelerates the pre-training of Multimodal Large Language Models (MLLMs). Our\napproach employs a sequence of visual resamplers that capture visual details at\nvarious spacial scales. This architecture not only leverages global and local\nvisual contexts effectively, but also facilitates the flexible extension of\nvisual tokens through a compound token scaling strategy, allowing up to a 16x\nincrease in the token count post pre-training. Consequently, Chain-of-Sight\nrequires significantly fewer visual tokens in the pre-training phase compared\nto the fine-tuning phase. This intentional reduction of visual tokens during\npre-training notably accelerates the pre-training process, cutting down the\nwall-clock training time by ~73%. Empirical results on a series of\nvision-language benchmarks reveal that the pre-train acceleration through\nChain-of-Sight is achieved without sacrificing performance, matching or\nsurpassing the standard pipeline of utilizing all visual tokens throughout the\nentire training process. Further scaling up the number of visual tokens for\npre-training leads to stronger performances, competitive to existing approaches\nin a series of benchmarks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"koTpMRgUw1m0KjPr1JlVfMGRjQ6R0pfD53ZPuYDbZqs","pdfSize":"1334166"}