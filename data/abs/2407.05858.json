{"id":"2407.05858","title":"Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU","authors":"Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu,\n  Xuanzhe Liu","authorsParsed":[["Xu","Daliang",""],["Zhang","Hao",""],["Yang","Liming",""],["Liu","Ruiqi",""],["Huang","Gang",""],["Xu","Mengwei",""],["Liu","Xuanzhe",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 12:20:45 GMT"}],"updateDate":"2024-07-09","timestamp":1720441245000,"abstract":"  On-device large language models (LLMs) are catalyzing novel mobile\napplications such as UI task automation and personalized email auto-reply,\nwithout giving away users' private data. However, on-device LLMs still suffer\nfrom unacceptably long inference latency, especially the time to first token\n(prefill stage) due to the need of long context for accurate, personalized\ncontent generation, as well as the lack of parallel computing capacity of\nmobile CPU/GPU.\n  To enable practical on-device LLM, we present mllm-NPU, the first-of-its-kind\nLLM inference system that efficiently leverages on-device Neural Processing\nUnit (NPU) offloading. Essentially, mllm-NPU is an algorithm-system co-design\nthat tackles a few semantic gaps between the LLM architecture and contemporary\nNPU design. Specifically, it re-constructs the prompt and model in three\nlevels: (1) At prompt level, it divides variable-length prompts into multiple\nfixed-sized chunks while maintaining data dependencies; (2) At tensor level, it\nidentifies and extracts significant outliers to run on the CPU/GPU in parallel\nwith minimal overhead; (3) At block level, it schedules Transformer blocks in\nan out-of-order manner to the CPU/GPU and NPU based on their hardware affinity\nand sensitivity to accuracy. Compared to competitive baselines, mllm-NPU\nachieves 22.4x faster prefill speed and 30.7x energy savings on average, and up\nto 32.8x speedup in an end-to-end real-world application. For the first time,\nmllm-NPU achieves more than 1,000 tokens/sec prefilling for a billion-sized\nmodel (Qwen1.5-1.8B), paving the way towards practical on-device LLM.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"t4Xe93R0_rvO0FMA5wLB0WbPzeVD8goJactnPouqt50","pdfSize":"2171306"}