{"id":"2412.05469","title":"Multi-Objective Alignment of Large Language Models Through Hypervolume\n  Maximization","authors":"Subhojyoti Mukherjee, Anusha Lalitha, Sailik Sengupta, Aniket\n  Deshmukh, Branislav Kveton","authorsParsed":[["Mukherjee","Subhojyoti",""],["Lalitha","Anusha",""],["Sengupta","Sailik",""],["Deshmukh","Aniket",""],["Kveton","Branislav",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 23:51:47 GMT"}],"updateDate":"2024-12-10","timestamp":1733529107000,"abstract":"  Multi-objective alignment from human feedback (MOAHF) in large language\nmodels (LLMs) is a challenging problem as human preferences are complex,\nmultifaceted, and often conflicting. Recent works on MOAHF considered a-priori\nmulti-objective optimization (MOO), where human preferences are known at\ntraining or inference time. In contrast, when human preferences are unknown or\ndifficult to quantify, a natural approach is to cover the Pareto front by\nmultiple diverse solutions. We propose an algorithm HaM for learning diverse\nLLM policies that maximizes their hypervolume. This is the first application of\na-posteriori MOO to MOAHF. HaM is computationally and space efficient, and\nempirically superior across objectives such as harmlessness, helpfulness,\nhumor, faithfulness, and hallucination, on various datasets.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YVwIzvmfMEBzP-m20vxMBsEcZr9PhbTNfloRXsrNBXk","pdfSize":"1392298"}