{
  "id": "2412.00556",
  "title": "Accelerating Multimodal Large Language Models by Searching Optimal\n  Vision Token Reduction",
  "authors": "Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu,\n  Xiaofang Wang, Mingfu Liang, Ning Zhang, Dimitris N. Metaxas, Licheng Yu",
  "authorsParsed": [
    [
      "Zhao",
      "Shiyu",
      ""
    ],
    [
      "Wang",
      "Zhenting",
      ""
    ],
    [
      "Juefei-Xu",
      "Felix",
      ""
    ],
    [
      "Xia",
      "Xide",
      ""
    ],
    [
      "Liu",
      "Miao",
      ""
    ],
    [
      "Wang",
      "Xiaofang",
      ""
    ],
    [
      "Liang",
      "Mingfu",
      ""
    ],
    [
      "Zhang",
      "Ning",
      ""
    ],
    [
      "Metaxas",
      "Dimitris N.",
      ""
    ],
    [
      "Yu",
      "Licheng",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 30 Nov 2024 18:54:32 GMT"
    },
    {
      "version": "v2",
      "created": "Sun, 8 Dec 2024 04:41:32 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1732992872000,
  "abstract": "  Prevailing Multimodal Large Language Models (MLLMs) encode the input image(s)\nas vision tokens and feed them into the language backbone, similar to how Large\nLanguage Models (LLMs) process the text tokens. However, the number of vision\ntokens increases quadratically as the image resolutions, leading to huge\ncomputational costs. In this paper, we consider improving MLLM's efficiency\nfrom two scenarios, (I) Reducing computational cost without degrading the\nperformance. (II) Improving the performance with given budgets. We start with\nour main finding that the ranking of each vision token sorted by attention\nscores is similar in each layer except the first layer. Based on it, we assume\nthat the number of essential top vision tokens does not increase along layers.\nAccordingly, for Scenario I, we propose a greedy search algorithm (G-Search) to\nfind the least number of vision tokens to keep at each layer from the shallow\nto the deep. Interestingly, G-Search is able to reach the optimal reduction\nstrategy based on our assumption. For Scenario II, based on the reduction\nstrategy from G-Search, we design a parametric sigmoid function (P-Sigmoid) to\nguide the reduction at each layer of the MLLM, whose parameters are optimized\nby Bayesian Optimization. Extensive experiments demonstrate that our approach\ncan significantly accelerate those popular MLLMs, e.g. LLaVA, and InternVL2\nmodels, by more than $2 \\times$ without performance drops. Our approach also\nfar outperforms other token reduction methods when budgets are limited,\nachieving a better trade-off between efficiency and effectiveness.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "jeJospSmXrjfuiP8RHTm46aBZGCED9NtPRd-H89ardg",
  "pdfSize": "1326231"
}