{
  "id": "2412.11408",
  "title": "Federated Domain Generalization with Label Smoothing and Balanced\n  Decentralized Training",
  "authors": "Milad Soltany, Farhad Pourpanah, Mahdiyar Molahasani, Michael\n  Greenspan, Ali Etemad",
  "authorsParsed": [
    [
      "Soltany",
      "Milad",
      ""
    ],
    [
      "Pourpanah",
      "Farhad",
      ""
    ],
    [
      "Molahasani",
      "Mahdiyar",
      ""
    ],
    [
      "Greenspan",
      "Michael",
      ""
    ],
    [
      "Etemad",
      "Ali",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 03:25:19 GMT"
    },
    {
      "version": "v2",
      "created": "Sat, 8 Feb 2025 15:41:08 GMT"
    }
  ],
  "updateDate": "2025-02-11",
  "timestamp": 1734319519000,
  "abstract": "  In this paper, we propose a novel approach, Federated Domain Generalization\nwith Label Smoothing and Balanced Decentralized Training (FedSB), to address\nthe challenges of data heterogeneity within a federated learning framework.\nFedSB utilizes label smoothing at the client level to prevent overfitting to\ndomain-specific features, thereby enhancing generalization capabilities across\ndiverse domains when aggregating local models into a global model.\nAdditionally, FedSB incorporates a decentralized budgeting mechanism which\nbalances training among clients, which is shown to improve the performance of\nthe aggregated global model. Extensive experiments on four commonly used\nmulti-domain datasets, PACS, VLCS, OfficeHome, and TerraInc, demonstrate that\nFedSB outperforms competing methods, achieving state-of-the-art results on\nthree out of four datasets, indicating the effectiveness of FedSB in addressing\ndata heterogeneity.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "gnlschguMrutD_gLwetlEpE7CgWXSRb2ifwdOCWg_5w",
  "pdfSize": "704983"
}