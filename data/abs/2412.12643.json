{"id":"2412.12643","title":"LLM-based Discriminative Reasoning for Knowledge Graph Question\n  Answering","authors":"Mufan Xu, Kehai Chen, Xuefeng Bai, Muyun Yang, Tiejun Zhao, Min Zhang","authorsParsed":[["Xu","Mufan",""],["Chen","Kehai",""],["Bai","Xuefeng",""],["Yang","Muyun",""],["Zhao","Tiejun",""],["Zhang","Min",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 08:07:16 GMT"}],"updateDate":"2024-12-18","timestamp":1734422836000,"abstract":"  Large language models (LLMs) based on generative pre-trained Transformer have\nachieved remarkable performance on knowledge graph question-answering (KGQA)\ntasks. However, LLMs often produce ungrounded subgraph planning or reasoning\nresults in KGQA due to the hallucinatory behavior brought by the generative\nparadigm, which may hinder the advancement of the LLM-based KGQA model. To deal\nwith the issue, we propose a novel LLM-based Discriminative Reasoning (LDR)\nmethod to explicitly model the subgraph retrieval and answer inference process.\nBy adopting discriminative strategies, the proposed LDR method not only\nenhances the capability of LLMs to retrieve question-related subgraphs but also\nalleviates the issue of ungrounded reasoning brought by the generative paradigm\nof LLMs. Experimental results show that the proposed approach outperforms\nmultiple strong comparison methods, along with achieving state-of-the-art\nperformance on two widely used WebQSP and CWQ benchmarks.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"PSXewV0YC2fw4E2Pfp-PwkbV71j96TE7g1y0881IyN4","pdfSize":"7192428"}