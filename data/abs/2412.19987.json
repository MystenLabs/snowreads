{
  "id": "2412.19987",
  "title": "Delayed Random Partial Gradient Averaging for Federated Learning",
  "authors": "Xinyi Hu",
  "authorsParsed": [
    [
      "Hu",
      "Xinyi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 28 Dec 2024 03:14:27 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735355667000,
  "abstract": "  Federated learning (FL) is a distributed machine learning paradigm that\nenables multiple clients to train a shared model collaboratively while\npreserving privacy. However, the scaling of real-world FL systems is often\nlimited by two communication bottlenecks:(a) while the increasing computing\npower of edge devices enables the deployment of large-scale Deep Neural\nNetworks (DNNs), the limited bandwidth constraints frequent transmissions over\nlarge DNNs; and (b) high latency cost greatly degrades the performance of FL.\nIn light of these bottlenecks, we propose a Delayed Random Partial Gradient\nAveraging (DPGA) to enhance FL. Under DPGA, clients only share partial local\nmodel gradients with the server. The size of the shared part in a local model\nis determined by the update rate, which is coarsely initialized and\nsubsequently refined over the temporal dimension. Moreover, DPGA largely\nreduces the system run time by enabling computation in parallel with\ncommunication. We conduct experiments on non-IID CIFAR-10/100 to demonstrate\nthe efficacy of our method.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Distributed, Parallel, and Cluster Computing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "4Boa7n3sQPwl4uVgnNzD25IxkaGTCZqN9TipUpGOhqU",
  "pdfSize": "857580"
}