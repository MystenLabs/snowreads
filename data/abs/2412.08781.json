{
  "id": "2412.08781",
  "title": "GMem: A Modular Approach for Ultra-Efficient Generative Models",
  "authors": "Yi Tang, Peng Sun, Zhenglin Cheng and Tao Lin",
  "authorsParsed": [
    [
      "Tang",
      "Yi",
      ""
    ],
    [
      "Sun",
      "Peng",
      ""
    ],
    [
      "Cheng",
      "Zhenglin",
      ""
    ],
    [
      "Lin",
      "Tao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 21:23:24 GMT"
    },
    {
      "version": "v2",
      "created": "Tue, 11 Feb 2025 23:05:30 GMT"
    }
  ],
  "updateDate": "2025-02-13",
  "timestamp": 1733952204000,
  "abstract": "  Recent studies indicate that the denoising process in deep generative\ndiffusion models implicitly learns and memorizes semantic information from the\ndata distribution. These findings suggest that capturing more complex data\ndistributions requires larger neural networks, leading to a substantial\nincrease in computational demands, which in turn become the primary bottleneck\nin both training and inference of diffusion models. To this end, we introduce\nGMem: A Modular Approach for Ultra-Efficient Generative Models. Our approach\nGMem decouples the memory capacity from model and implements it as a separate,\nimmutable memory set that preserves the essential semantic information in the\ndata. The results are significant: GMem enhances both training, sampling\nefficiency, and diversity generation. This design on one hand reduces the\nreliance on network for memorize complex data distribution and thus enhancing\nboth training and sampling efficiency. On ImageNet at $256 \\times 256$\nresolution, GMem achieves a $50\\times$ training speedup compared to SiT,\nreaching FID $=7.66$ in fewer than $28$ epochs ($\\sim 4$ hours training time),\nwhile SiT requires $1400$ epochs. Without classifier-free guidance, GMem\nachieves state-of-the-art (SoTA) performance FID $=1.53$ in $160$ epochs with\nonly $\\sim 20$ hours of training, outperforming LightningDiT which requires\n$800$ epochs and $\\sim 95$ hours to attain FID $=2.17$.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "JjALQOd_qXOvVIjdwRC7l5zK3JXhl5-1g7pLLnqZiyw",
  "pdfSize": "34234249"
}