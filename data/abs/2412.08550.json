{"id":"2412.08550","title":"Sketch2Sound: Controllable Audio Generation via Time-Varying Signals and\n  Sonic Imitations","authors":"Hugo Flores Garc\\'ia, Oriol Nieto, Justin Salamon, Bryan Pardo, Prem\n  Seetharaman","authorsParsed":[["Garc√≠a","Hugo Flores",""],["Nieto","Oriol",""],["Salamon","Justin",""],["Pardo","Bryan",""],["Seetharaman","Prem",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 17:11:21 GMT"}],"updateDate":"2024-12-12","timestamp":1733937081000,"abstract":"  We present Sketch2Sound, a generative audio model capable of creating\nhigh-quality sounds from a set of interpretable time-varying control signals:\nloudness, brightness, and pitch, as well as text prompts. Sketch2Sound can\nsynthesize arbitrary sounds from sonic imitations (i.e.,~a vocal imitation or a\nreference sound-shape). Sketch2Sound can be implemented on top of any\ntext-to-audio latent diffusion transformer (DiT), and requires only 40k steps\nof fine-tuning and a single linear layer per control, making it more\nlightweight than existing methods like ControlNet. To synthesize from\nsketchlike sonic imitations, we propose applying random median filters to the\ncontrol signals during training, allowing Sketch2Sound to be prompted using\ncontrols with flexible levels of temporal specificity. We show that\nSketch2Sound can synthesize sounds that follow the gist of input controls from\na vocal imitation while retaining the adherence to an input text prompt and\naudio quality compared to a text-only baseline. Sketch2Sound allows sound\nartists to create sounds with the semantic flexibility of text prompts and the\nexpressivity and precision of a sonic gesture or vocal imitation. Sound\nexamples are available at https://hugofloresgarcia.art/sketch2sound/.\n","subjects":["Computer Science/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0Euqi_gULpsAflg2blyPZ3X2UZl3SLt3F5smI4ZQ0PM","pdfSize":"1269517"}