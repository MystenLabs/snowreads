{
  "id": "2412.17052",
  "title": "VilBias: A Study of Bias Detection through Linguistic and Visual Cues ,\n  presenting Annotation Strategies, Evaluation, and Key Challenges",
  "authors": "Shaina Raza, Caesar Saleh, Emrul Hasan, Franklin Ogidi, Maximus\n  Powers, Veronica Chatrath, Marcelo Lotif, Roya Javadi, Anam Zahid, Vahid Reza\n  Khazaie",
  "authorsParsed": [
    [
      "Raza",
      "Shaina",
      ""
    ],
    [
      "Saleh",
      "Caesar",
      ""
    ],
    [
      "Hasan",
      "Emrul",
      ""
    ],
    [
      "Ogidi",
      "Franklin",
      ""
    ],
    [
      "Powers",
      "Maximus",
      ""
    ],
    [
      "Chatrath",
      "Veronica",
      ""
    ],
    [
      "Lotif",
      "Marcelo",
      ""
    ],
    [
      "Javadi",
      "Roya",
      ""
    ],
    [
      "Zahid",
      "Anam",
      ""
    ],
    [
      "Khazaie",
      "Vahid Reza",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 22 Dec 2024 15:05:30 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 9 Jan 2025 02:33:14 GMT"
    },
    {
      "version": "v3",
      "created": "Tue, 18 Feb 2025 22:01:51 GMT"
    }
  ],
  "updateDate": "2025-02-20",
  "timestamp": 1734879930000,
  "abstract": "  The integration of Large Language Models (LLMs) and Vision-Language Models\n(VLMs) opens new avenues for addressing complex challenges in multimodal\ncontent analysis, particularly in biased news detection. This study introduces\nVLBias, a framework that leverages state-of-the-art LLMs and VLMs to detect\nlinguistic and visual biases in news content. We present a multimodal dataset\ncomprising textual content and corresponding images from diverse news sources.\nWe propose a hybrid annotation framework that combines LLM-based annotations\nwith human review to ensure high-quality labeling while reducing costs and\nenhancing scalability. Our evaluation compares the performance of\nstate-of-the-art SLMs and LLMs for both modalities (text and images) and the\nresults reveal that while SLMs are computationally efficient, LLMs demonstrate\nsuperior accuracy in identifying subtle framing and text-visual\ninconsistencies. Furthermore, empirical analysis shows that incorporating\nvisual cues alongside textual data improves bias detection accuracy by 3 to 5%.\nThis study provides a comprehensive exploration of LLMs, SLMs, and VLMs as\ntools for detecting multimodal biases in news content and highlights their\nrespective strengths, limitations, and potential for future applications\n",
  "subjects": [
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "CmT9nL3p6s8rlK8RSItDiXk8H3SyaEPmDvisYlR3F4U",
  "pdfSize": "4441326"
}