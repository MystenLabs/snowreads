{"id":"2407.18707","title":"Finite Neural Networks as Mixtures of Gaussian Processes: From Provable\n  Error Bounds to Prior Selection","authors":"Steven Adams, Patan\\`e, Morteza Lahijanian, and Luca Laurenti","authorsParsed":[["Adams","Steven",""],["PatanÃ¨","",""],["Lahijanian","Morteza",""],["Laurenti","Luca",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 12:45:53 GMT"}],"updateDate":"2024-07-29","timestamp":1721997953000,"abstract":"  Infinitely wide or deep neural networks (NNs) with independent and\nidentically distributed (i.i.d.) parameters have been shown to be equivalent to\nGaussian processes. Because of the favorable properties of Gaussian processes,\nthis equivalence is commonly employed to analyze neural networks and has led to\nvarious breakthroughs over the years. However, neural networks and Gaussian\nprocesses are equivalent only in the limit; in the finite case there are\ncurrently no methods available to approximate a trained neural network with a\nGaussian model with bounds on the approximation error. In this work, we present\nan algorithmic framework to approximate a neural network of finite width and\ndepth, and with not necessarily i.i.d. parameters, with a mixture of Gaussian\nprocesses with error bounds on the approximation error. In particular, we\nconsider the Wasserstein distance to quantify the closeness between\nprobabilistic models and, by relying on tools from optimal transport and\nGaussian processes, we iteratively approximate the output distribution of each\nlayer of the neural network as a mixture of Gaussian processes. Crucially, for\nany NN and $\\epsilon >0$ our approach is able to return a mixture of Gaussian\nprocesses that is $\\epsilon$-close to the NN at a finite set of input points.\nFurthermore, we rely on the differentiability of the resulting error bound to\nshow how our approach can be employed to tune the parameters of a NN to mimic\nthe functional behavior of a given Gaussian process, e.g., for prior selection\nin the context of Bayesian inference. We empirically investigate the\neffectiveness of our results on both regression and classification problems\nwith various neural network architectures. Our experiments highlight how our\nresults can represent an important step towards understanding neural network\npredictions and formally quantifying their uncertainty.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"doJvo8qhx56plHqs9FT8l13lXVUjwmzT7chBpDQn6hQ","pdfSize":"1621753","objectId":"0x06601820b62cc55b0f7eef5ac7ffed17436019d185299d90c4ff219f0f51dce5","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
