{"id":"2412.14711","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing","authors":"Ziteng Wang, Jun Zhu, Jianfei Chen","authorsParsed":[["Wang","Ziteng",""],["Zhu","Jun",""],["Chen","Jianfei",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 10:21:20 GMT"},{"version":"v2","created":"Thu, 27 Feb 2025 16:33:09 GMT"}],"updateDate":"2025-02-28","timestamp":1734603680000,"abstract":"  Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to\nscale up model capacity without increasing the computation budget. However,\nvanilla TopK routers are trained in a discontinuous, non-differentiable way,\nlimiting their performance and scalability. To address this issue, we propose\nReMoE, a fully differentiable MoE architecture that offers a simple yet\neffective drop-in replacement for the conventional TopK+Softmax routing,\nutilizing ReLU as the router instead. We further propose methods to regulate\nthe router's sparsity while balancing the load among experts. ReMoE's\ncontinuous nature enables efficient dynamic allocation of computation across\ntokens and layers, while also exhibiting domain specialization. Our experiments\ndemonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across\nvarious model sizes, expert counts, and levels of granularity. Furthermore,\nReMoE exhibits superior scalability with respect to the number of experts,\nsurpassing traditional MoE architectures. The implementation based on\nMegatron-LM is available at https://github.com/thu-ml/ReMoE.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kR7pJ-tTb2mxVz_an-1N6gnVeoAE4YuonPd1KRELJZ4","pdfSize":"1020162"}