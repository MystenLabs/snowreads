{"id":"2407.12034","title":"Understanding Transformers via N-gram Statistics","authors":"Timothy Nguyen","authorsParsed":[["Nguyen","Timothy",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 22:18:49 GMT"}],"updateDate":"2024-07-18","timestamp":1719785929000,"abstract":"  Transformer based large-language models (LLMs) display extreme proficiency\nwith language yet a precise understanding of how they work remains elusive. One\nway of demystifying transformer predictions would be to describe how they\ndepend on their context in terms of simple template functions. This paper takes\na first step in this direction by considering families of functions (i.e.\nrules) formed out of simple N-gram based statistics of the training data. By\nstudying how well these rulesets approximate transformer predictions, we obtain\na variety of novel discoveries: a simple method to detect overfitting during\ntraining without using a holdout set, a quantitative measure of how\ntransformers progress from learning simple to more complex statistical rules\nover the course of training, a model-variance criterion governing when\ntransformer predictions tend to be described by N-gram rules, and insights into\nhow well transformers can be approximated by N-gram rulesets in the limit where\nthese rulesets become increasingly complex. In this latter direction, we find\nthat for 78% of LLM next-token distributions on TinyStories, their top-1\npredictions agree with those provided by our N-gram rulesets.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1cmc2-Xoiuh45p6wA4eC28aZ192znKgBfrJjKmwbtIw","pdfSize":"2903853"}