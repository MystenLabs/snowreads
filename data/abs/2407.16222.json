{"id":"2407.16222","title":"PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of\n  Multilingual Alignment","authors":"Jiahuan Li, Shujian Huang, Xinyu Dai and Jiajun Chen","authorsParsed":[["Li","Jiahuan",""],["Huang","Shujian",""],["Dai","Xinyu",""],["Chen","Jiajun",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 06:59:53 GMT"}],"updateDate":"2024-07-24","timestamp":1721717993000,"abstract":"  Large language models demonstrate reasonable multilingual abilities, despite\npredominantly English-centric pretraining. However, the spontaneous\nmultilingual alignment in these models is shown to be weak, leading to\nunsatisfactory cross-lingual transfer and knowledge sharing. Previous works\nattempt to address this issue by explicitly injecting multilingual alignment\ninformation during or after pretraining. Thus for the early stage in\npretraining, the alignment is weak for sharing information or knowledge across\nlanguages. In this paper, we propose PreAlign, a framework that establishes\nmultilingual alignment prior to language model pretraining. PreAlign injects\nmultilingual alignment by initializing the model to generate similar\nrepresentations of aligned words and preserves this alignment using a\ncode-switching strategy during pretraining. Extensive experiments in a\nsynthetic English to English-Clone setting demonstrate that PreAlign\nsignificantly outperforms standard multilingual joint training in language\nmodeling, zero-shot cross-lingual transfer, and cross-lingual knowledge\napplication. Further experiments in real-world scenarios further validate\nPreAlign's effectiveness across various model sizes.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3P4qRENWXGzpQG3NunoduajwbmdrRnkj6D2jdtakvu8","pdfSize":"1244101"}