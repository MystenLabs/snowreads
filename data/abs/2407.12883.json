{"id":"2407.12883","title":"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive\n  Retrieval","authors":"Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff,\n  Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun,\n  Jinsung Yoon, Sercan O. Arik, Danqi Chen, Tao Yu","authorsParsed":[["Su","Hongjin",""],["Yen","Howard",""],["Xia","Mengzhou",""],["Shi","Weijia",""],["Muennighoff","Niklas",""],["Wang","Han-yu",""],["Liu","Haisu",""],["Shi","Quan",""],["Siegel","Zachary S.",""],["Tang","Michael",""],["Sun","Ruoxi",""],["Yoon","Jinsung",""],["Arik","Sercan O.",""],["Chen","Danqi",""],["Yu","Tao",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:58:27 GMT"}],"updateDate":"2024-07-19","timestamp":1721152707000,"abstract":"  Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UCwCIwFh1SVFuI3tD-Dd0Nm-hMwQJVQAq42oZVx5Pxo","pdfSize":"2849377"}