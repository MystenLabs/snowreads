{"id":"2407.05182","title":"A Novel Bifurcation Method for Observation Perturbation Attacks on\n  Reinforcement Learning Agents: Load Altering Attacks on a Cyber Physical\n  Power System","authors":"Kiernan Broda-Milian, Ranwa Al-Mallah, Hanane Dagdougui","authorsParsed":[["Broda-Milian","Kiernan",""],["Al-Mallah","Ranwa",""],["Dagdougui","Hanane",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 20:55:24 GMT"}],"updateDate":"2024-07-09","timestamp":1720299324000,"abstract":"  Components of cyber physical systems, which affect real-world processes, are\noften exposed to the internet. Replacing conventional control methods with Deep\nReinforcement Learning (DRL) in energy systems is an active area of research,\nas these systems become increasingly complex with the advent of renewable\nenergy sources and the desire to improve their efficiency. Artificial Neural\nNetworks (ANN) are vulnerable to specific perturbations of their inputs or\nfeatures, called adversarial examples. These perturbations are difficult to\ndetect when properly regularized, but have significant effects on the ANN's\noutput. Because DRL uses ANN to map optimal actions to observations, they are\nsimilarly vulnerable to adversarial examples. This work proposes a novel attack\ntechnique for continuous control using Group Difference Logits loss with a\nbifurcation layer. By combining aspects of targeted and untargeted attacks, the\nattack significantly increases the impact compared to an untargeted attack,\nwith drastically smaller distortions than an optimally targeted attack. We\ndemonstrate the impacts of powerful gradient-based attacks in a realistic smart\nenergy environment, show how the impacts change with different DRL agents and\ntraining procedures, and use statistical and time-series analysis to evaluate\nattacks' stealth. The results show that adversarial attacks can have\nsignificant impacts on DRL controllers, and constraining an attack's\nperturbations makes it difficult to detect. However, certain DRL architectures\nare far more robust, and robust training methods can further reduce the impact.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Ibcg22fBhK7EaKGS27iCTXuLMP12_Nh2BlnSLWezC6M","pdfSize":"1220227"}
