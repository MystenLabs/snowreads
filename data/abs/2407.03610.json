{"id":"2407.03610","title":"VDMA: Video Question Answering with Dynamically Generated Multi-Agents","authors":"Noriyuki Kugo, Tatsuya Ishibashi, Kosuke Ono, Yuji Sato","authorsParsed":[["Kugo","Noriyuki",""],["Ishibashi","Tatsuya",""],["Ono","Kosuke",""],["Sato","Yuji",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 03:40:36 GMT"}],"updateDate":"2024-07-08","timestamp":1720064436000,"abstract":"  This technical report provides a detailed description of our approach to the\nEgoSchema Challenge 2024. The EgoSchema Challenge aims to identify the most\nappropriate responses to questions regarding a given video clip. In this paper,\nwe propose Video Question Answering with Dynamically Generated Multi-Agents\n(VDMA). This method is a complementary approach to existing response generation\nsystems by employing a multi-agent system with dynamically generated expert\nagents. This method aims to provide the most accurate and contextually\nappropriate responses. This report details the stages of our approach, the\ntools employed, and the results of our experiments.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"cjJqnN_x3m-SPVpD-pDO5xAFxD8wiH180KTx8P8WhPA","pdfSize":"845804"}