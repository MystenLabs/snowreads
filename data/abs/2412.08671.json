{
  "id": "2412.08671",
  "title": "A Deep Semantic Segmentation Network with Semantic and Contextual\n  Refinements",
  "authors": "Zhiyan Wang, Deyin Liu, Lin Yuanbo Wu, Song Wang, Xin Guo, Lin Qi",
  "authorsParsed": [
    [
      "Wang",
      "Zhiyan",
      ""
    ],
    [
      "Liu",
      "Deyin",
      ""
    ],
    [
      "Wu",
      "Lin Yuanbo",
      ""
    ],
    [
      "Wang",
      "Song",
      ""
    ],
    [
      "Guo",
      "Xin",
      ""
    ],
    [
      "Qi",
      "Lin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 03:40:46 GMT"
    }
  ],
  "updateDate": "2024-12-13",
  "timestamp": 1733888446000,
  "abstract": "  Semantic segmentation is a fundamental task in multimedia processing, which\ncan be used for analyzing, understanding, editing contents of images and\nvideos, among others. To accelerate the analysis of multimedia data, existing\nsegmentation researches tend to extract semantic information by progressively\nreducing the spatial resolutions of feature maps. However, this approach\nintroduces a misalignment problem when restoring the resolution of high-level\nfeature maps. In this paper, we design a Semantic Refinement Module (SRM) to\naddress this issue within the segmentation network. Specifically, SRM is\ndesigned to learn a transformation offset for each pixel in the upsampled\nfeature maps, guided by high-resolution feature maps and neighboring offsets.\nBy applying these offsets to the upsampled feature maps, SRM enhances the\nsemantic representation of the segmentation network, particularly for pixels\naround object boundaries. Furthermore, a Contextual Refinement Module (CRM) is\npresented to capture global context information across both spatial and channel\ndimensions. To balance dimensions between channel and space, we aggregate the\nsemantic maps from all four stages of the backbone to enrich channel context\ninformation. The efficacy of these proposed modules is validated on three\nwidely used datasets-Cityscapes, Bdd100K, and ADE20K-demonstrating superior\nperformance compared to state-of-the-art methods. Additionally, this paper\nextends these modules to a lightweight segmentation network, achieving an mIoU\nof 82.5% on the Cityscapes validation set with only 137.9 GFLOPs.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning",
    "Electrical Engineering and Systems Science/Image and Video Processing"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "5sh_8SqfzM04SS-yuXUjyi2v2Fnjf14PhllyYtjwEJA",
  "pdfSize": "6564433"
}