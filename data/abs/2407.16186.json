{"id":"2407.16186","title":"Automatic Environment Shaping is the Next Frontier in RL","authors":"Younghyo Park, Gabriel B. Margolis, Pulkit Agrawal","authorsParsed":[["Park","Younghyo",""],["Margolis","Gabriel B.",""],["Agrawal","Pulkit",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 05:22:29 GMT"}],"updateDate":"2024-07-24","timestamp":1721712149000,"abstract":"  Many roboticists dream of presenting a robot with a task in the evening and\nreturning the next morning to find the robot capable of solving the task. What\nis preventing us from achieving this? Sim-to-real reinforcement learning (RL)\nhas achieved impressive performance on challenging robotics tasks, but requires\nsubstantial human effort to set up the task in a way that is amenable to RL.\nIt's our position that algorithmic improvements in policy optimization and\nother ideas should be guided towards resolving the primary bottleneck of\nshaping the training environment, i.e., designing observations, actions,\nrewards and simulation dynamics. Most practitioners don't tune the RL\nalgorithm, but other environment parameters to obtain a desirable controller.\nWe posit that scaling RL to diverse robotic tasks will only be achieved if the\ncommunity focuses on automating environment shaping procedures.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yME7Xg3peTDl--3mDu3OzqXsT8aIbvRp8KWCxz9XrX8","pdfSize":"4164504"}