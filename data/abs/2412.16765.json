{
  "id": "2412.16765",
  "title": "Optimization Insights into Deep Diagonal Linear Networks",
  "authors": "Hippolyte Labarri\\`ere and Cesare Molinari and Lorenzo Rosasco and\n  Silvia Villa and Cristian Vega",
  "authorsParsed": [
    [
      "Labarri√®re",
      "Hippolyte",
      ""
    ],
    [
      "Molinari",
      "Cesare",
      ""
    ],
    [
      "Rosasco",
      "Lorenzo",
      ""
    ],
    [
      "Villa",
      "Silvia",
      ""
    ],
    [
      "Vega",
      "Cristian",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 20:23:47 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734812627000,
  "abstract": "  Overparameterized models trained with (stochastic) gradient descent are\nubiquitous in modern machine learning. These large models achieve unprecedented\nperformance on test data, but their theoretical understanding is still limited.\nIn this paper, we take a step towards filling this gap by adopting an\noptimization perspective. More precisely, we study the implicit regularization\nproperties of the gradient flow \"algorithm\" for estimating the parameters of a\ndeep diagonal neural network. Our main contribution is showing that this\ngradient flow induces a mirror flow dynamic on the model, meaning that it is\nbiased towards a specific solution of the problem depending on the\ninitialization of the network. Along the way, we prove several properties of\nthe trajectory.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Mathematics/Optimization and Control",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "XTvEPCnTvCVg6PlMyLeoJPF5uwHXDGiucwH1Zb2arrs",
  "pdfSize": "530791"
}