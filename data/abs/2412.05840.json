{"id":"2412.05840","title":"LVP-CLIP:Revisiting CLIP for Continual Learning with Label Vector Pool","authors":"Yue Ma, Huantao Ren, Boyu Wang, Jingang Jin, Senem Velipasalar, Qinru\n  Qiu","authorsParsed":[["Ma","Yue",""],["Ren","Huantao",""],["Wang","Boyu",""],["Jin","Jingang",""],["Velipasalar","Senem",""],["Qiu","Qinru",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 07:22:39 GMT"}],"updateDate":"2024-12-10","timestamp":1733642559000,"abstract":"  Continual learning aims to update a model so that it can sequentially learn\nnew tasks without forgetting previously acquired knowledge. Recent continual\nlearning approaches often leverage the vision-language model CLIP for its\nhigh-dimensional feature space and cross-modality feature matching. Traditional\nCLIP-based classification methods identify the most similar text label for a\ntest image by comparing their embeddings. However, these methods are sensitive\nto the quality of text phrases and less effective for classes lacking\nmeaningful text labels. In this work, we rethink CLIP-based continual learning\nand introduce the concept of Label Vector Pool (LVP). LVP replaces text labels\nwith training images as similarity references, eliminating the need for ideal\ntext descriptions. We present three variations of LVP and evaluate their\nperformance on class and domain incremental learning tasks. Leveraging CLIP's\nhigh dimensional feature space, LVP learning algorithms are task-order\ninvariant. The new knowledge does not modify the old knowledge, hence, there is\nminimum forgetting. Different tasks can be learned independently and in\nparallel with low computational and memory demands. Experimental results show\nthat proposed LVP-based methods outperform the current state-of-the-art\nbaseline by a significant margin of 40.7%.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"IJJ45XERBdbVPPph72E5lOGZFe_XV8ByDPbLlbnbljs","pdfSize":"34843753"}