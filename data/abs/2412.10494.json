{
  "id": "2412.10494",
  "title": "SnapGen-V: Generating a Five-Second Video within Five Seconds on a\n  Mobile Device",
  "authors": "Yushu Wu, Zhixing Zhang, Yanyu Li, Yanwu Xu, Anil Kag, Yang Sui,\n  Huseyin Coskun, Ke Ma, Aleksei Lebedev, Ju Hu, Dimitris Metaxas, Yanzhi Wang,\n  Sergey Tulyakov, Jian Ren",
  "authorsParsed": [
    [
      "Wu",
      "Yushu",
      ""
    ],
    [
      "Zhang",
      "Zhixing",
      ""
    ],
    [
      "Li",
      "Yanyu",
      ""
    ],
    [
      "Xu",
      "Yanwu",
      ""
    ],
    [
      "Kag",
      "Anil",
      ""
    ],
    [
      "Sui",
      "Yang",
      ""
    ],
    [
      "Coskun",
      "Huseyin",
      ""
    ],
    [
      "Ma",
      "Ke",
      ""
    ],
    [
      "Lebedev",
      "Aleksei",
      ""
    ],
    [
      "Hu",
      "Ju",
      ""
    ],
    [
      "Metaxas",
      "Dimitris",
      ""
    ],
    [
      "Wang",
      "Yanzhi",
      ""
    ],
    [
      "Tulyakov",
      "Sergey",
      ""
    ],
    [
      "Ren",
      "Jian",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 18:59:56 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734116396000,
  "abstract": "  We have witnessed the unprecedented success of diffusion-based video\ngeneration over the past year. Recently proposed models from the community have\nwielded the power to generate cinematic and high-resolution videos with smooth\nmotions from arbitrary input prompts. However, as a supertask of image\ngeneration, video generation models require more computation and are thus\nhosted mostly on cloud servers, limiting broader adoption among content\ncreators. In this work, we propose a comprehensive acceleration framework to\nbring the power of the large-scale video diffusion model to the hands of edge\nusers. From the network architecture scope, we initialize from a compact image\nbackbone and search out the design and arrangement of temporal layers to\nmaximize hardware efficiency. In addition, we propose a dedicated adversarial\nfine-tuning algorithm for our efficient model and reduce the denoising steps to\n4. Our model, with only 0.6B parameters, can generate a 5-second video on an\niPhone 16 PM within 5 seconds. Compared to server-side models that take minutes\non powerful GPUs to generate a single video, we accelerate the generation by\nmagnitudes while delivering on-par quality.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning",
    "Computer Science/Performance"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "mZ1JIDZzh8MxtYw1yY_9EF5f4Tx7IgGXXFj0MnFsv84",
  "pdfSize": "19638654"
}