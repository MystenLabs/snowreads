{"id":"2407.12057","title":"NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker\n  and AWS Trainium and Inferentia2","authors":"Tengfei Xue, Xuefeng Li, Roman Smirnov, Tahir Azim, Arash Sadrieh and\n  Babak Pahlavan","authorsParsed":[["Xue","Tengfei",""],["Li","Xuefeng",""],["Smirnov","Roman",""],["Azim","Tahir",""],["Sadrieh","Arash",""],["Pahlavan","Babak",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 05:04:44 GMT"}],"updateDate":"2024-07-18","timestamp":1720674284000,"abstract":"  Retrieval-augmented generation (RAG) techniques are widely used today to\nretrieve and present information in a conversational format. This paper\npresents a set of enhancements to traditional RAG techniques, focusing on large\nlanguage models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI\nchips via SageMaker. These chips are characterized by their elasticity,\naffordability, and efficient performance for AI compute tasks. Besides enabling\ndeployment on these chips, this work aims to improve tool usage, add citation\ncapabilities, and mitigate the risks of hallucinations and unsafe responses due\nto context bias. We benchmark our RAG system's performance on the Natural\nQuestions and HotPotQA datasets, achieving an accuracy of 62% and 59%\nrespectively, exceeding other models such as DBRX and Mixtral Instruct.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VpE--fZ5S9fXGSn5X3FKUpi2t4cnAEyjCsyW1r-XJ6g","pdfSize":"95045","objectId":"0xe81742ff56c079c200502ef55f3fb1d3a0145d46d3c41ba2779ad2ece93ad841","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
