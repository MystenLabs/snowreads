{"id":"2407.06041","title":"MST5 -- Multilingual Question Answering over Knowledge Graphs","authors":"Nikit Srivastava, Mengshi Ma, Daniel Vollmers, Hamada Zahera, Diego\n  Moussallem, Axel-Cyrille Ngonga Ngomo","authorsParsed":[["Srivastava","Nikit",""],["Ma","Mengshi",""],["Vollmers","Daniel",""],["Zahera","Hamada",""],["Moussallem","Diego",""],["Ngomo","Axel-Cyrille Ngonga",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 15:37:51 GMT"}],"updateDate":"2024-07-09","timestamp":1720453071000,"abstract":"  Knowledge Graph Question Answering (KGQA) simplifies querying vast amounts of\nknowledge stored in a graph-based model using natural language. However, the\nresearch has largely concentrated on English, putting non-English speakers at a\ndisadvantage. Meanwhile, existing multilingual KGQA systems face challenges in\nachieving performance comparable to English systems, highlighting the\ndifficulty of generating SPARQL queries from diverse languages. In this\nresearch, we propose a simplified approach to enhance multilingual KGQA systems\nby incorporating linguistic context and entity information directly into the\nprocessing pipeline of a language model. Unlike existing methods that rely on\nseparate encoders for integrating auxiliary information, our strategy leverages\na single, pretrained multilingual transformer-based language model to manage\nboth the primary input and the auxiliary data. Our methodology significantly\nimproves the language model's ability to accurately convert a natural language\nquery into a relevant SPARQL query. It demonstrates promising results on the\nmost recent QALD datasets, namely QALD-9-Plus and QALD-10. Furthermore, we\nintroduce and evaluate our approach on Chinese and Japanese, thereby expanding\nthe language diversity of the existing datasets.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"F0vYrJsRgld0JUPCvxG7aQboubN6pkL9KcwPov659OQ","pdfSize":"598999"}
