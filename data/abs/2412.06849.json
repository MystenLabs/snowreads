{
  "id": "2412.06849",
  "title": "GL-Fusion: Rethinking the Combination of Graph Neural Network and Large\n  Language model",
  "authors": "Haotong Yang, Xiyuan Wang, Qian Tao, Shuxian Hu, Zhouchen Lin, Muhan\n  Zhang",
  "authorsParsed": [
    [
      "Yang",
      "Haotong",
      ""
    ],
    [
      "Wang",
      "Xiyuan",
      ""
    ],
    [
      "Tao",
      "Qian",
      ""
    ],
    [
      "Hu",
      "Shuxian",
      ""
    ],
    [
      "Lin",
      "Zhouchen",
      ""
    ],
    [
      "Zhang",
      "Muhan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 8 Dec 2024 05:49:58 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733636998000,
  "abstract": "  Recent research on integrating Large Language Models (LLMs) with Graph Neural\nNetworks (GNNs) typically follows two approaches: LLM-centered models, which\nconvert graph data into tokens for LLM processing, and GNN-centered models,\nwhich use LLMs to encode text features into node and edge representations for\nGNN input. LLM-centered models often struggle to capture graph structures\neffectively, while GNN-centered models compress variable-length textual data\ninto fixed-size vectors, limiting their ability to understand complex\nsemantics. Additionally, GNN-centered approaches require converting tasks into\na uniform, manually-designed format, restricting them to classification tasks\nand preventing language output. To address these limitations, we introduce a\nnew architecture that deeply integrates GNN with LLM, featuring three key\ninnovations: (1) Structure-Aware Transformers, which incorporate GNN's\nmessage-passing capabilities directly into LLM's transformer layers, allowing\nsimultaneous processing of textual and structural information and generating\noutputs from both GNN and LLM; (2) Graph-Text Cross-Attention, which processes\nfull, uncompressed text from graph nodes and edges, ensuring complete semantic\nintegration; and (3) GNN-LLM Twin Predictor, enabling LLM's flexible\nautoregressive generation alongside GNN's scalable one-pass prediction.\nGL-Fusion achieves outstand performance on various tasks. Notably, it achieves\nstate-of-the-art performance on OGBN-Arxiv and OGBG-Code2.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "hS5AzxQZyF46SfcHYUKADNzc-jouV9tjNL9laDJmczU",
  "pdfSize": "1591604"
}