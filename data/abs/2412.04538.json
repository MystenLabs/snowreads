{"id":"2412.04538","title":"Communication Compression for Distributed Learning without Control\n  Variates","authors":"Tomas Ortega, Chun-Yin Huang, Xiaoxiao Li and Hamid Jafarkhani","authorsParsed":[["Ortega","Tomas",""],["Huang","Chun-Yin",""],["Li","Xiaoxiao",""],["Jafarkhani","Hamid",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 18:46:20 GMT"}],"updateDate":"2024-12-09","timestamp":1733424380000,"abstract":"  Distributed learning algorithms, such as the ones employed in Federated\nLearning (FL), require communication compression to reduce the cost of client\nuploads. The compression methods used in practice are often biased, which\nrequire error feedback to achieve convergence when the compression is\naggressive. In turn, error feedback requires client-specific control variates,\nwhich directly contradicts privacy-preserving principles and requires stateful\nclients. In this paper, we propose Compressed Aggregate Feedback (CAFe), a\nnovel distributed learning framework that allows highly compressible client\nupdates by exploiting past aggregated updates, and does not require control\nvariates. We consider Distributed Gradient Descent (DGD) as a representative\nalgorithm and provide a theoretical proof of CAFe's superiority to Distributed\nCompressed Gradient Descent (DCGD) with biased compression in the non-smooth\nregime with bounded gradient dissimilarity. Experimental results confirm that\nCAFe consistently outperforms distributed learning with direct compression and\nhighlight the compressibility of the client updates with CAFe.\n","subjects":["Computer Science/Machine Learning","Electrical Engineering and Systems Science/Signal Processing","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"iVjHNcXAPL0Hjozm3ocoUpD1KrbSws8sJ4jiiBY5F_M","pdfSize":"1164492"}