{
  "id": "2412.19685",
  "title": "A Large-scale Interpretable Multi-modality Benchmark for Facial Image\n  Forgery Localization",
  "authors": "Jingchun Lian, Lingyu Liu, Yaxiong Wang, Yujiao Wu, Li Zhu, Zhedong\n  Zheng",
  "authorsParsed": [
    [
      "Lian",
      "Jingchun",
      ""
    ],
    [
      "Liu",
      "Lingyu",
      ""
    ],
    [
      "Wang",
      "Yaxiong",
      ""
    ],
    [
      "Wu",
      "Yujiao",
      ""
    ],
    [
      "Zhu",
      "Li",
      ""
    ],
    [
      "Zheng",
      "Zhedong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 27 Dec 2024 15:23:39 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735313019000,
  "abstract": "  Image forgery localization, which centers on identifying tampered pixels\nwithin an image, has seen significant advancements. Traditional approaches\noften model this challenge as a variant of image segmentation, treating the\nbinary segmentation of forged areas as the end product. We argue that the basic\nbinary forgery mask is inadequate for explaining model predictions. It doesn't\nclarify why the model pinpoints certain areas and treats all forged pixels the\nsame, making it hard to spot the most fake-looking parts. In this study, we\nmitigate the aforementioned limitations by generating salient region-focused\ninterpretation for the forgery images. To support this, we craft a Multi-Modal\nTramper Tracing (MMTT) dataset, comprising facial images manipulated using\ndeepfake techniques and paired with manual, interpretable textual annotations.\nTo harvest high-quality annotation, annotators are instructed to meticulously\nobserve the manipulated images and articulate the typical characteristics of\nthe forgery regions. Subsequently, we collect a dataset of 128,303 image-text\npairs. Leveraging the MMTT dataset, we develop ForgeryTalker, an architecture\ndesigned for concurrent forgery localization and interpretation. ForgeryTalker\nfirst trains a forgery prompter network to identify the pivotal clues within\nthe explanatory text. Subsequently, the region prompter is incorporated into\nmultimodal large language model for finetuning to achieve the dual goals of\nlocalization and interpretation. Extensive experiments conducted on the MMTT\ndataset verify the superior performance of our proposed model. The dataset,\ncode as well as pretrained checkpoints will be made publicly available to\nfacilitate further research and ensure the reproducibility of our results.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "HWJjFhyYI14Bo72Q0bReVSvalBeHZ4CzHNmAoQYa2v0",
  "pdfSize": "1584281"
}