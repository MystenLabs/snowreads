{"id":"2407.04407","title":"Trustworthy Classification through Rank-Based Conformal Prediction Sets","authors":"Rui Luo and Zhixin Zhou","authorsParsed":[["Luo","Rui",""],["Zhou","Zhixin",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 10:43:41 GMT"}],"updateDate":"2024-07-08","timestamp":1720176221000,"abstract":"  Machine learning classification tasks often benefit from predicting a set of\npossible labels with confidence scores to capture uncertainty. However,\nexisting methods struggle with the high-dimensional nature of the data and the\nlack of well-calibrated probabilities from modern classification models. We\npropose a novel conformal prediction method that employs a rank-based score\nfunction suitable for classification models that predict the order of labels\ncorrectly, even if not well-calibrated. Our approach constructs prediction sets\nthat achieve the desired coverage rate while managing their size. We provide a\ntheoretical analysis of the expected size of the conformal prediction sets\nbased on the rank distribution of the underlying classifier. Through extensive\nexperiments, we demonstrate that our method outperforms existing techniques on\nvarious datasets, providing reliable uncertainty quantification. Our\ncontributions include a novel conformal prediction method, theoretical\nanalysis, and empirical evaluation. This work advances the practical deployment\nof machine learning systems by enabling reliable uncertainty quantification.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"S5O_TNvPwTX5aj5UEDQoEQj95uctZV7IC9FYh248UVY","pdfSize":"952857"}