{
  "id": "2412.01256",
  "title": "NLPrompt: Noise-Label Prompt Learning for Vision-Language Models",
  "authors": "Bikang Pan, Qun Li, Xiaoying Tang, Wei Huang, Zhen Fang, Feng Liu,\n  Jingya Wang, Jingyi Yu, Ye Shi",
  "authorsParsed": [
    [
      "Pan",
      "Bikang",
      ""
    ],
    [
      "Li",
      "Qun",
      ""
    ],
    [
      "Tang",
      "Xiaoying",
      ""
    ],
    [
      "Huang",
      "Wei",
      ""
    ],
    [
      "Fang",
      "Zhen",
      ""
    ],
    [
      "Liu",
      "Feng",
      ""
    ],
    [
      "Wang",
      "Jingya",
      ""
    ],
    [
      "Yu",
      "Jingyi",
      ""
    ],
    [
      "Shi",
      "Ye",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 08:25:09 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733127909000,
  "abstract": "  The emergence of vision-language foundation models, such as CLIP, has\nrevolutionized image-text representation, enabling a broad range of\napplications via prompt learning. Despite its promise, real-world datasets\noften contain noisy labels that can degrade prompt learning performance. In\nthis paper, we demonstrate that using mean absolute error (MAE) loss in prompt\nlearning, named PromptMAE, significantly enhances robustness against noisy\nlabels while maintaining high accuracy. Though MAE is straightforward and\nrecognized for its robustness, it is rarely used in noisy-label learning due to\nits slow convergence and poor performance outside prompt learning scenarios. To\nelucidate the robustness of PromptMAE, we leverage feature learning theory to\nshow that MAE can suppress the influence of noisy samples, thereby improving\nthe signal-to-noise ratio and enhancing overall robustness. Additionally, we\nintroduce PromptOT, a prompt-based optimal transport data purification method\nto enhance the robustness further. PromptOT employs text encoder\nrepresentations in vision-language models as prototypes to construct an optimal\ntransportation matrix. This matrix effectively partitions datasets into clean\nand noisy subsets, allowing for the application of cross-entropy loss to the\nclean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning\nmethod, named NLPrompt, offers a simple and efficient approach that leverages\nthe expressive representation and precise alignment capabilities of\nvision-language models for robust prompt learning. We validate NLPrompt through\nextensive experiments across various noise settings, demonstrating significant\nperformance improvements.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "YBtAlT3Fxj9CEZS9wNA7ckUvsUzq0pd78vTlTjUmXLQ",
  "pdfSize": "618123"
}