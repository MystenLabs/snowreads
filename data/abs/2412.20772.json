{"id":"2412.20772","title":"Large Language Model Enabled Multi-Task Physical Layer Network","authors":"Tianyue Zheng, Linglong Dai","authorsParsed":[["Zheng","Tianyue",""],["Dai","Linglong",""]],"versions":[{"version":"v1","created":"Mon, 30 Dec 2024 07:47:30 GMT"}],"updateDate":"2024-12-31","timestamp":1735544850000,"abstract":"  The recent advance of Artificial Intelligence (AI) is continuously reshaping\nthe future 6G wireless communications. Recently, the development of Large\nLanguage Models (LLMs) offers a promising approach to effectively improve the\nperformance and generalization for different physical layer tasks. However,\nmost existing works finetune dedicated LLM networks for a single wireless\ncommunication task separately. Thus performing diverse physical layer tasks\nintroduces extremely high training resources, memory usage, and deployment\ncosts. To solve the problem, we propose a LLM-enabled multi-task physical layer\nnetwork to unify multiple tasks with a single LLM. Specifically, we first\npropose a multi-task LLM framework, which finetunes LLM to perform multi-user\nprecoding, signal detection and channel prediction simultaneously. Besides,\nmulti-task instruction module, input encoders, as well as output decoders, are\nelaborately designed to distinguish multiple tasks and adapted the features of\ndifferent formats of wireless data for the features of LLM. Numerical\nsimulations are also displayed to verify the effectiveness of the proposed\nmethod.\n","subjects":["Computer Science/Information Theory","Electrical Engineering and Systems Science/Signal Processing","Mathematics/Information Theory"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"hKRYePlE5IeAeN5eXfgc0ovJ_LS1IjhOpH2Csc1HhCA","pdfSize":"3754313"}