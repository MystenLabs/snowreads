{
  "id": "2412.06910",
  "title": "Using Large Language Models to Assign Partial Credit to Students'\n  Explanations of Problem-Solving Process: Grade at Human Level Accuracy with\n  Grading Confidence Index and Personalized Student-facing Feedback",
  "authors": "Zhongzhou Chen and Tong Wan",
  "authorsParsed": [
    [
      "Chen",
      "Zhongzhou",
      ""
    ],
    [
      "Wan",
      "Tong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 19:02:07 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 13 Dec 2024 03:25:08 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1733770927000,
  "abstract": "  This study examines the feasibility and potential advantages of using large\nlanguage models, in particular GPT-4o, to perform partial credit grading of\nlarge numbers of student written responses to introductory level physics\nproblems. Students were instructed to write down verbal explanations of their\nreasoning process when solving one conceptual and two numerical calculation\nproblems on in class exams. The explanations were then graded according to a\n3-item rubric with each item grades as binary (1 or 0). We first demonstrate\nthat machine grading using GPT-4o with no examples nor reference answer can\nreliably agree with human graders on 70%-80% of all cases, which is equal to or\nhigher than the level at which two human graders agree with each other. Two\nmethods are essential for achieving this level of accuracy: 1. Adding\nexplanation language to each rubric item that targets the errors of initial\nmachine grading. 2. Running the grading process 5 times and taking the most\nfrequent outcome. Next, we show that the variation in outcomes across 5 machine\ngrading attempts as measured by the Shannon Entropy can serve as a grading\nconfidence index, allowing a human instructor to identify ~40% of all\npotentially incorrect gradings by reviewing just 10 - 15% of all responses.\nFinally, we show that it is straightforward to use GPT-4o to write clear\nexplanations of the partial credit grading outcomes. Those explanations can be\nused as feedback for students, which will allow students to understand their\ngrades and raise different opinions when necessary. Almost all feedback\nmessages generated were rated 3 or above on a 5-point scale by two experienced\ninstructors. The entire grading and feedback generating process cost roughly $5\nper 100 student answers, which shows immense promise for automating\nlabor-intensive grading process by a combination of machine grading with human\ninput and supervision.\n",
  "subjects": [
    "Physics/Physics Education"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "EnbZ67WOvlYAFr5hNOlXR1mD8WYIFIRCaPpL9Ib1yVU",
  "pdfSize": "1247838"
}