{"id":"2412.11521","title":"On the Ability of Deep Networks to Learn Symmetries from Data: A Neural\n  Kernel Theory","authors":"Andrea Perin and Stephane Deny","authorsParsed":[["Perin","Andrea",""],["Deny","Stephane",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 07:56:54 GMT"}],"updateDate":"2024-12-17","timestamp":1734335814000,"abstract":"  Symmetries (transformations by group actions) are present in many datasets,\nand leveraging them holds significant promise for improving predictions in\nmachine learning. In this work, we aim to understand when and how deep networks\ncan learn symmetries from data. We focus on a supervised classification\nparadigm where data symmetries are only partially observed during training:\nsome classes include all transformations of a cyclic group, while others\ninclude only a subset. We ask: can deep networks generalize symmetry invariance\nto the partially sampled classes? In the infinite-width limit, where kernel\nanalogies apply, we derive a neural kernel theory of symmetry learning to\naddress this question. The group-cyclic nature of the dataset allows us to\nanalyze the spectrum of neural kernels in the Fourier domain; here we find a\nsimple characterization of the generalization error as a function of the\ninteraction between class separation (signal) and class-orbit density (noise).\nWe observe that generalization can only be successful when the local structure\nof the data prevails over its non-local, symmetric, structure, in the kernel\nspace defined by the architecture. This occurs when (1) classes are\nsufficiently distinct and (2) class orbits are sufficiently dense. Our\nframework also applies to equivariant architectures (e.g., CNNs), and recovers\ntheir success in the special case where the architecture matches the inherent\nsymmetry of the data. Empirically, our theory reproduces the generalization\nfailure of finite-width networks (MLP, CNN, ViT) trained on partially observed\nversions of rotated-MNIST. We conclude that conventional networks trained with\nsupervision lack a mechanism to learn symmetries that have not been explicitly\nembedded in their architecture a priori. Our framework could be extended to\nguide the design of architectures and training procedures able to learn\nsymmetries from data.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"____b7PAq8RcwP0hcA9QiDm63Uh9PiD5qLhB87jmGIM","pdfSize":"10377168"}