{"id":"2412.18283","title":"On the Local Complexity of Linear Regions in Deep ReLU Networks","authors":"Niket Patel, Guido Mont\\'ufar","authorsParsed":[["Patel","Niket",""],["Mont√∫far","Guido",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 08:42:39 GMT"},{"version":"v2","created":"Wed, 25 Dec 2024 02:14:07 GMT"}],"updateDate":"2024-12-30","timestamp":1735029759000,"abstract":"  We define the local complexity of a neural network with continuous piecewise\nlinear activations as a measure of the density of linear regions over an input\ndata distribution. We show theoretically that ReLU networks that learn\nlow-dimensional feature representations have a lower local complexity. This\nallows us to connect recent empirical observations on feature learning at the\nlevel of the weight matrices with concrete properties of the learned functions.\nIn particular, we show that the local complexity serves as an upper bound on\nthe total variation of the function over the input data distribution and thus\nthat feature learning can be related to adversarial robustness. Lastly, we\nconsider how optimization drives ReLU networks towards solutions with lower\nlocal complexity. Overall, this work contributes a theoretical framework\ntowards relating geometric properties of ReLU networks to different aspects of\nlearning such as feature learning and representation cost.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"F2Q6ac7iTUSMyAOtq-45Y3PPqHFcwcSbfO7AoqkO8Pc","pdfSize":"16790731"}