{"id":"2407.04009","title":"A Critical Assessment of Interpretable and Explainable Machine Learning\n  for Intrusion Detection","authors":"Omer Subasi, Johnathan Cree, Joseph Manzano, Elena Peterson","authorsParsed":[["Subasi","Omer",""],["Cree","Johnathan",""],["Manzano","Joseph",""],["Peterson","Elena",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 15:35:42 GMT"}],"updateDate":"2024-07-08","timestamp":1720107342000,"abstract":"  There has been a large number of studies in interpretable and explainable ML\nfor cybersecurity, in particular, for intrusion detection. Many of these\nstudies have significant amount of overlapping and repeated evaluations and\nanalysis. At the same time, these studies overlook crucial model, data,\nlearning process, and utility related issues and many times completely\ndisregard them. These issues include the use of overly complex and opaque ML\nmodels, unaccounted data imbalances and correlated features, inconsistent\ninfluential features across different explanation methods, the inconsistencies\nstemming from the constituents of a learning process, and the implausible\nutility of explanations. In this work, we empirically demonstrate these issues,\nanalyze them and propose practical solutions in the context of feature-based\nmodel explanations. Specifically, we advise avoiding complex opaque models such\nas Deep Neural Networks and instead using interpretable ML models such as\nDecision Trees as the available intrusion datasets are not difficult for such\ninterpretable models to classify successfully. Then, we bring attention to the\nbinary classification metrics such as Matthews Correlation Coefficient (which\nare well-suited for imbalanced datasets. Moreover, we find that feature-based\nmodel explanations are most often inconsistent across different settings. In\nthis respect, to further gauge the extent of inconsistencies, we introduce the\nnotion of cross explanations which corroborates that the features that are\ndetermined to be impactful by one explanation method most often differ from\nthose by another method. Furthermore, we show that strongly correlated data\nfeatures and the constituents of a learning process, such as hyper-parameters\nand the optimization routine, become yet another source of inconsistent\nexplanations. Finally, we discuss the utility of feature-based explanations.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"b5XJDk3KItVN2IG8DypoHWsp49J3pwFya9f48FtkRNI","pdfSize":"1277659"}
