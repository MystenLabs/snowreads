{"id":"2412.09899","title":"TTAQ: Towards Stable Post-training Quantization in Continuous Domain\n  Adaptation","authors":"Junrui Xiao, Zhikai Li, Lianwei Yang, Yiduo Mei, Qingyi Gu","authorsParsed":[["Xiao","Junrui",""],["Li","Zhikai",""],["Yang","Lianwei",""],["Mei","Yiduo",""],["Gu","Qingyi",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 06:34:59 GMT"}],"updateDate":"2024-12-16","timestamp":1734071699000,"abstract":"  Post-training quantization (PTQ) reduces excessive hardware cost by\nquantizing full-precision models into lower bit representations on a tiny\ncalibration set, without retraining. Despite the remarkable progress made\nthrough recent efforts, traditional PTQ methods typically encounter failure in\ndynamic and ever-changing real-world scenarios, involving unpredictable data\nstreams and continual domain shifts, which poses greater challenges. In this\npaper, we propose a novel and stable quantization process for test-time\nadaptation (TTA), dubbed TTAQ, to address the performance degradation of\ntraditional PTQ in dynamically evolving test domains. To tackle domain shifts\nin quantizer, TTAQ proposes the Perturbation Error Mitigation (PEM) and\nPerturbation Consistency Reconstruction (PCR). Specifically, PEM analyzes the\nerror propagation and devises a weight regularization scheme to mitigate the\nimpact of input perturbations. On the other hand, PCR introduces consistency\nlearning to ensure that quantized models provide stable predictions for same\nsample. Furthermore, we introduce Adaptive Balanced Loss (ABL) to adjust the\nlogits by taking advantage of the frequency and complexity of the class, which\ncan effectively address the class imbalance caused by unpredictable data\nstreams during optimization. Extensive experiments are conducted on multiple\ndatasets with generic TTA methods, proving that TTAQ can outperform existing\nbaselines and encouragingly improve the accuracy of low bit PTQ models in\ncontinually changing test domains. For instance, TTAQ decreases the mean error\nof 2-bit models on ImageNet-C dataset by an impressive 10.1\\%.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8ZlyV2WtBqxviv1ViO1ryNMnfdv1BVV18sDH-R-3Hao","pdfSize":"2536851"}