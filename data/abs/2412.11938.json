{
  "id": "2412.11938",
  "title": "Are the Latent Representations of Foundation Models for Pathology\n  Invariant to Rotation?",
  "authors": "Matou\\v{s} Elphick, Samra Turajlic, Guang Yang",
  "authorsParsed": [
    [
      "Elphick",
      "Matou≈°",
      ""
    ],
    [
      "Turajlic",
      "Samra",
      ""
    ],
    [
      "Yang",
      "Guang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 16:23:05 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734366185000,
  "abstract": "  Self-supervised foundation models for digital pathology encode small patches\nfrom H\\&E whole slide images into latent representations used for downstream\ntasks. However, the invariance of these representations to patch rotation\nremains unexplored. This study investigates the rotational invariance of latent\nrepresentations across twelve foundation models by quantifying the alignment\nbetween non-rotated and rotated patches using mutual $k$-nearest neighbours and\ncosine distance. Models that incorporated rotation augmentation during\nself-supervised training exhibited significantly greater invariance to\nrotations. We hypothesise that the absence of rotational inductive bias in the\ntransformer architecture necessitates rotation augmentation during training to\nachieve learned invariance. Code:\nhttps://github.com/MatousE/rot-invariance-analysis.\n",
  "subjects": [
    "Electrical Engineering and Systems Science/Image and Video Processing",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "V34V6ppcwr-TvUPqMSmVLcBojygF1ujoA2qHX1rRAok",
  "pdfSize": "1129963"
}