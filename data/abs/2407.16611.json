{"id":"2407.16611","title":"Local vs Global continual learning","authors":"Giulia Lanzillotta, Sidak Pal Singh, Benjamin F. Grewe, and Thomas\n  Hofmann","authorsParsed":[["Lanzillotta","Giulia",""],["Singh","Sidak Pal",""],["Grewe","Benjamin F.",""],["Hofmann","Thomas",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 16:18:00 GMT"}],"updateDate":"2024-07-24","timestamp":1721751480000,"abstract":"  Continual learning is the problem of integrating new information in a model\nwhile retaining the knowledge acquired in the past. Despite the tangible\nimprovements achieved in recent years, the problem of continual learning is\nstill an open one. A better understanding of the mechanisms behind the\nsuccesses and failures of existing continual learning algorithms can unlock the\ndevelopment of new successful strategies. In this work, we view continual\nlearning from the perspective of the multi-task loss approximation, and we\ncompare two alternative strategies, namely local and global approximations. We\nclassify existing continual learning algorithms based on the approximation\nused, and we assess the practical effects of this distinction in common\ncontinual learning settings.Additionally, we study optimal continual learning\nobjectives in the case of local polynomial approximations and we provide\nexamples of existing algorithms implementing the optimal objectives\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"oa61U1R7rIWDC_47KKm09yBz0ZdC65ExrH6Z4glYCTY","pdfSize":"1136211"}