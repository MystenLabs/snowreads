{"id":"2412.10869","title":"TinySubNets: An efficient and low capacity continual learning strategy","authors":"Marcin Pietro\\'n, Kamil Faber, Dominik \\.Zurek, Roberto Corizzo","authorsParsed":[["Pietroń","Marcin",""],["Faber","Kamil",""],["Żurek","Dominik",""],["Corizzo","Roberto",""]],"versions":[{"version":"v1","created":"Sat, 14 Dec 2024 15:43:38 GMT"},{"version":"v2","created":"Tue, 25 Feb 2025 16:10:06 GMT"}],"updateDate":"2025-02-26","timestamp":1734191018000,"abstract":"  Continual Learning (CL) is a highly relevant setting gaining traction in\nrecent machine learning research. Among CL works, architectural and hybrid\nstrategies are particularly effective due to their potential to adapt the model\narchitecture as new tasks are presented. However, many existing solutions do\nnot efficiently exploit model sparsity, and are prone to capacity saturation\ndue to their inefficient use of available weights, which limits the number of\nlearnable tasks. In this paper, we propose TinySubNets (TSN), a novel\narchitectural CL strategy that addresses the issues through the unique\ncombination of pruning with different sparsity levels, adaptive quantization,\nand weight sharing. Pruning identifies a subset of weights that preserve model\nperformance, making less relevant weights available for future tasks. Adaptive\nquantization allows a single weight to be separated into multiple parts which\ncan be assigned to different tasks. Weight sharing between tasks boosts the\nexploitation of capacity and task similarity, allowing for the identification\nof a better trade-off between model accuracy and capacity. These features allow\nTSN to efficiently leverage the available capacity, enhance knowledge transfer,\nand reduce computational resource consumption. Experimental results involving\ncommon benchmark CL datasets and scenarios show that our proposed strategy\nachieves better results in terms of accuracy than existing state-of-the-art CL\nstrategies. Moreover, our strategy is shown to provide a significantly improved\nmodel capacity exploitation. Code released at:\nhttps://github.com/lifelonglab/tinysubnets.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ftWx16TTwlU60OVuHcEzUlF7nM5pCR0E4AAAikSmIkA","pdfSize":"290366"}