{
  "id": "2412.11767",
  "title": "IDEA-Bench: How Far are Generative Models from Professional Designing?",
  "authors": "Chen Liang, Lianghua Huang, Jingwu Fang, Huanzhang Dou, Wei Wang,\n  Zhi-Fan Wu, Yupeng Shi, Junge Zhang, Xin Zhao, Yu Liu",
  "authorsParsed": [
    [
      "Liang",
      "Chen",
      ""
    ],
    [
      "Huang",
      "Lianghua",
      ""
    ],
    [
      "Fang",
      "Jingwu",
      ""
    ],
    [
      "Dou",
      "Huanzhang",
      ""
    ],
    [
      "Wang",
      "Wei",
      ""
    ],
    [
      "Wu",
      "Zhi-Fan",
      ""
    ],
    [
      "Shi",
      "Yupeng",
      ""
    ],
    [
      "Zhang",
      "Junge",
      ""
    ],
    [
      "Zhao",
      "Xin",
      ""
    ],
    [
      "Liu",
      "Yu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 13:39:32 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734356372000,
  "abstract": "  Real-world design tasks - such as picture book creation, film storyboard\ndevelopment using character sets, photo retouching, visual effects, and font\ntransfer - are highly diverse and complex, requiring deep interpretation and\nextraction of various elements from instructions, descriptions, and reference\nimages. The resulting images often implicitly capture key features from\nreferences or user inputs, making it challenging to develop models that can\neffectively address such varied tasks. While existing visual generative models\ncan produce high-quality images based on prompts, they face significant\nlimitations in professional design scenarios that involve varied forms and\nmultiple inputs and outputs, even when enhanced with adapters like ControlNets\nand LoRAs. To address this, we introduce IDEA-Bench, a comprehensive benchmark\nencompassing 100 real-world design tasks, including rendering, visual effects,\nstoryboarding, picture books, fonts, style-based, and identity-preserving\ngeneration, with 275 test cases to thoroughly evaluate a model's\ngeneral-purpose generation capabilities. Notably, even the best-performing\nmodel only achieves 22.48 on IDEA-Bench, while the best general-purpose model\nonly achieves 6.81. We provide a detailed analysis of these results,\nhighlighting the inherent challenges and providing actionable directions for\nimprovement. Additionally, we provide a subset of 18 representative tasks\nequipped with multimodal large language model (MLLM)-based auto-evaluation\ntechniques to facilitate rapid model development and comparison. We releases\nthe benchmark data, evaluation toolkits, and an online leaderboard at\nhttps://github.com/ali-vilab/IDEA-Bench, aiming to drive the advancement of\ngenerative models toward more versatile and applicable intelligent design\nsystems.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "iRvxb_a2urJ2QyIwim384Q0fzCYNDbGyArw9lqu1a-E",
  "pdfSize": "49584203"
}