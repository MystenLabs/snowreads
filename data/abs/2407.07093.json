{"id":"2407.07093","title":"FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive\n  Distillation","authors":"Liqun Ma and Mingjie Sun and Zhiqiang Shen","authorsParsed":[["Ma","Liqun",""],["Sun","Mingjie",""],["Shen","Zhiqiang",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 17:59:48 GMT"}],"updateDate":"2024-07-10","timestamp":1720547988000,"abstract":"  This work presents a Fully BInarized Large Language Model (FBI-LLM),\ndemonstrating for the first time how to train a large-scale binary language\nmodel from scratch (not the partial binary or ternary LLM like BitNet b1.58) to\nmatch the performance of its full-precision counterparts (e.g., FP16 or BF16)\nin transformer-based LLMs. It achieves this by employing an autoregressive\ndistillation (AD) loss with maintaining equivalent model dimensions (130M,\n1.3B, 7B) and training data volume as regular LLM pretraining, while delivering\ncompetitive results in terms of perplexity and task-specific effectiveness.\nIntriguingly, by analyzing the training trajectory, we find that the pretrained\nweight is not necessary for training binarized LLMs from scratch. This research\nencourages a new computational framework and may facilitate the future design\nof specialized hardware tailored for fully 1-bit LLMs. We make all models,\ncode, and training dataset fully accessible and transparent to support further\nresearch (Code: https://github.com/LiqunMa/FBI-LLM. Model:\nhttps://huggingface.co/LiqunMa/).\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zZSFs0m1JFYD1K_hxtpDoJsTVgAG9YFWnOyMBzQUEJM","pdfSize":"808889"}