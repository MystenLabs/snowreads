{"id":"2412.11637","title":"On Crowdsourcing Task Design for Discourse Relation Annotation","authors":"Frances Yung and Vera Demberg","authorsParsed":[["Yung","Frances",""],["Demberg","Vera",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 10:26:11 GMT"}],"updateDate":"2024-12-17","timestamp":1734344771000,"abstract":"  Interpreting implicit discourse relations involves complex reasoning,\nrequiring the integration of semantic cues with background knowledge, as overt\nconnectives like because or then are absent. These relations often allow\nmultiple interpretations, best represented as distributions. In this study, we\ncompare two established methods that crowdsource English implicit discourse\nrelation annotation by connective insertion: a free-choice approach, which\nallows annotators to select any suitable connective, and a forced-choice\napproach, which asks them to select among a set of predefined options.\nSpecifically, we re-annotate the whole DiscoGeM 1.0 corpus -- initially\nannotated with the free-choice method -- using the forced-choice approach. The\nfree-choice approach allows for flexible and intuitive insertion of various\nconnectives, which are context-dependent. Comparison among over 130,000\nannotations, however, shows that the free-choice strategy produces less diverse\nannotations, often converging on common labels. Analysis of the results reveals\nthe interplay between task design and the annotators' abilities to interpret\nand produce discourse relations.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"BfoUXZ9jENiZXyPE2bpTsaw-FC5HH6RtTzs6htYiMmg","pdfSize":"275643"}