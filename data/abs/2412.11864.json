{"id":"2412.11864","title":"Investigating Mixture of Experts in Dense Retrieval","authors":"Effrosyni Sokli, Pranav Kasela, Georgios Peikos, Gabriella Pasi","authorsParsed":[["Sokli","Effrosyni",""],["Kasela","Pranav",""],["Peikos","Georgios",""],["Pasi","Gabriella",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 15:20:13 GMT"}],"updateDate":"2024-12-17","timestamp":1734362413000,"abstract":"  While Dense Retrieval Models (DRMs) have advanced Information Retrieval (IR),\none limitation of these neural models is their narrow generalizability and\nrobustness. To cope with this issue, one can leverage the Mixture-of-Experts\n(MoE) architecture. While previous IR studies have incorporated MoE\narchitectures within the Transformer layers of DRMs, our work investigates an\narchitecture that integrates a single MoE block (SB-MoE) after the output of\nthe final Transformer layer. Our empirical evaluation investigates how SB-MoE\ncompares, in terms of retrieval effectiveness, to standard fine-tuning. In\ndetail, we fine-tune three DRMs (TinyBERT, BERT, and Contriever) across four\nbenchmark collections with and without adding the MoE block. Moreover, since\nMoE showcases performance variations with respect to its parameters (i.e., the\nnumber of experts), we conduct additional experiments to investigate this\naspect further. The findings show the effectiveness of SB-MoE especially for\nDRMs with a low number of parameters (i.e., TinyBERT), as it consistently\noutperforms the fine-tuned underlying model on all four benchmarks. For DRMs\nwith a higher number of parameters (i.e., BERT and Contriever), SB-MoE requires\nlarger numbers of training samples to yield better retrieval performance.\n","subjects":["Computer Science/Information Retrieval","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"U7e-4VXgQRbqZJ17Y9G7sBklB_WB3zG9C_RiHELPTCw","pdfSize":"441841"}