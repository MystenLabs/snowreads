{"id":"2407.14156","title":"Learning Firmly Nonexpansive Operators","authors":"Kristian Bredies, Jonathan Chirinos-Rodriguez and Emanuele Naldi","authorsParsed":[["Bredies","Kristian",""],["Chirinos-Rodriguez","Jonathan",""],["Naldi","Emanuele",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 09:37:43 GMT"}],"updateDate":"2024-07-22","timestamp":1721381863000,"abstract":"  This paper proposes a data-driven approach for constructing firmly\nnonexpansive operators. We demonstrate its applicability in Plug-and-Play\nmethods, where classical algorithms such as forward-backward splitting,\nChambolle--Pock primal-dual iteration, Douglas--Rachford iteration or\nalternating directions method of multipliers (ADMM), are modified by replacing\none proximal map by a learned firmly nonexpansive operator. We provide sound\nmathematical background to the problem of learning such an operator via\nexpected and empirical risk minimization. We prove that, as the number of\ntraining points increases, the empirical risk minimization problem converges\n(in the sense of Gamma-convergence) to the expected risk minimization problem.\nFurther, we derive a solution strategy that ensures firmly nonexpansive and\npiecewise affine operators within the convex envelope of the training set. We\nshow that this operator converges to the best empirical solution as the number\nof points in the envelope increases in an appropriate sense. Finally, the\nexperimental section details practical implementations of the method and\npresents an application in image denoising.\n","subjects":["Mathematics/Optimization and Control","Mathematics/Functional Analysis","Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"y1QTx3AqW6BXSaldyrsqRVBB9un5KvaVCJ2CCE0wN84","pdfSize":"2138890","objectId":"0xb575557956aca6838932cda8511c6ed405d4731a341fc735227efe8957cf4ace","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
