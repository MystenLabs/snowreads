{"id":"2407.18119","title":"Tracking linguistic information in transformer-based sentence embeddings\n  through targeted sparsification","authors":"Vivi Nastase and Paola Merlo","authorsParsed":[["Nastase","Vivi",""],["Merlo","Paola",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 15:27:08 GMT"}],"updateDate":"2024-07-26","timestamp":1721921228000,"abstract":"  Analyses of transformer-based models have shown that they encode a variety of\nlinguistic information from their textual input. While these analyses have shed\na light on the relation between linguistic information on one side, and\ninternal architecture and parameters on the other, a question remains\nunanswered: how is this linguistic information reflected in sentence\nembeddings? Using datasets consisting of sentences with known structure, we\ntest to what degree information about chunks (in particular noun, verb or\nprepositional phrases), such as grammatical number, or semantic role, can be\nlocalized in sentence embeddings. Our results show that such information is not\ndistributed over the entire sentence embedding, but rather it is encoded in\nspecific regions. Understanding how the information from an input text is\ncompressed into sentence embeddings helps understand current transformer models\nand help build future explainable neural models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3JXGBdJhtEuKJlfB-2m6U0b9saVgePxFl6l96vKtbls","pdfSize":"1066851"}