{"id":"2407.03216","title":"Learning Disentangled Representation in Object-Centric Models for Visual\n  Dynamics Prediction via Transformers","authors":"Sanket Gandhi, Atul, Samanyu Mahajan, Vishal Sharma, Rushil Gupta,\n  Arnab Kumar Mondal, Parag Singla","authorsParsed":[["Gandhi","Sanket",""],["Atul","",""],["Mahajan","Samanyu",""],["Sharma","Vishal",""],["Gupta","Rushil",""],["Mondal","Arnab Kumar",""],["Singla","Parag",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 15:43:54 GMT"}],"updateDate":"2024-07-04","timestamp":1720021434000,"abstract":"  Recent work has shown that object-centric representations can greatly help\nimprove the accuracy of learning dynamics while also bringing interpretability.\nIn this work, we take this idea one step further, ask the following question:\n\"can learning disentangled representation further improve the accuracy of\nvisual dynamics prediction in object-centric models?\" While there has been some\nattempt to learn such disentangled representations for the case of static\nimages \\citep{nsb}, to the best of our knowledge, ours is the first work which\ntries to do this in a general setting for video, without making any specific\nassumptions about the kind of attributes that an object might have. The key\nbuilding block of our architecture is the notion of a {\\em block}, where\nseveral blocks together constitute an object. Each block is represented as a\nlinear combination of a given number of learnable concept vectors, which is\niteratively refined during the learning process. The blocks in our model are\ndiscovered in an unsupervised manner, by attending over object masks, in a\nstyle similar to discovery of slots \\citep{slot_attention}, for learning a\ndense object-centric representation. We employ self-attention via transformers\nover the discovered blocks to predict the next state resulting in discovery of\nvisual dynamics. We perform a series of experiments on several benchmark 2-D,\nand 3-D datasets demonstrating that our architecture (1) can discover\nsemantically meaningful blocks (2) help improve accuracy of dynamics prediction\ncompared to SOTA object-centric models (3) perform significantly better in OOD\nsetting where the specific attribute combinations are not seen earlier during\ntraining. Our experiments highlight the importance discovery of disentangled\nrepresentation for visual dynamics prediction.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Hwvq2GIiwSXeZcg9A61k3wntSo6dW1OxUolWVw1vtQQ","pdfSize":"3694197"}