{"id":"2407.08159","title":"Model-agnostic clean-label backdoor mitigation in cybersecurity\n  environments","authors":"Giorgio Severi, Simona Boboila, John Holodnak, Kendra Kratkiewicz,\n  Rauf Izmailov, Alina Oprea","authorsParsed":[["Severi","Giorgio",""],["Boboila","Simona",""],["Holodnak","John",""],["Kratkiewicz","Kendra",""],["Izmailov","Rauf",""],["Oprea","Alina",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 03:25:40 GMT"},{"version":"v2","created":"Wed, 18 Sep 2024 03:38:16 GMT"}],"updateDate":"2024-09-19","timestamp":1720668340000,"abstract":"  The training phase of machine learning models is a delicate step, especially\nin cybersecurity contexts. Recent research has surfaced a series of insidious\ntraining-time attacks that inject backdoors in models designed for security\nclassification tasks without altering the training labels. With this work, we\npropose new techniques that leverage insights in cybersecurity threat models to\neffectively mitigate these clean-label poisoning attacks, while preserving the\nmodel utility. By performing density-based clustering on a carefully chosen\nfeature subspace, and progressively isolating the suspicious clusters through a\nnovel iterative scoring procedure, our defensive mechanism can mitigate the\nattacks without requiring many of the common assumptions in the existing\nbackdoor defense literature. To show the generality of our proposed mitigation,\nwe evaluate it on two clean-label model-agnostic attacks on two different\nclassic cybersecurity data modalities: network flows classification and malware\nclassification, using gradient boosting and neural network models.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6o4RVT1Oa3l7rOM-o5hfs8xkuFVh5pxzdlaNLwqwLM0","pdfSize":"2024456"}