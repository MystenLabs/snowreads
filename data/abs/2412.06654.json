{"id":"2412.06654","title":"GEAR: A Simple GENERATE, EMBED, AVERAGE AND RANK Approach for\n  Unsupervised Reverse Dictionary","authors":"Fatemah Almeman, Luis Espinosa-Anke","authorsParsed":[["Almeman","Fatemah",""],["Espinosa-Anke","Luis",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 16:54:54 GMT"}],"updateDate":"2024-12-10","timestamp":1733763294000,"abstract":"  Reverse Dictionary (RD) is the task of obtaining the most relevant word or\nset of words given a textual description or dictionary definition. Effective RD\nmethods have applications in accessibility, translation or writing support\nsystems. Moreover, in NLP research we find RD to be used to benchmark text\nencoders at various granularities, as it often requires word, definition and\nsentence embeddings. In this paper, we propose a simple approach to RD that\nleverages LLMs in combination with embedding models. Despite its simplicity,\nthis approach outperforms supervised baselines in well studied RD datasets,\nwhile also showing less over-fitting. We also conduct a number of experiments\non different dictionaries and analyze how different styles, registers and\ntarget audiences impact the quality of RD systems. We conclude that, on\naverage, untuned embeddings alone fare way below an LLM-only baseline (although\nthey are competitive in highly technical dictionaries), but are crucial for\nboosting performance in combined methods.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fFhaWL6_J5b5LfD9miKRlHFqiGfpE6TXErff9dRZQwo","pdfSize":"634158"}