{"id":"2412.04139","title":"Monet: Mixture of Monosemantic Experts for Transformers","authors":"Jungwoo Park, Young Jin Ahn, Kee-Eung Kim, Jaewoo Kang","authorsParsed":[["Park","Jungwoo",""],["Ahn","Young Jin",""],["Kim","Kee-Eung",""],["Kang","Jaewoo",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 13:06:03 GMT"},{"version":"v2","created":"Mon, 9 Dec 2024 07:49:01 GMT"}],"updateDate":"2024-12-10","timestamp":1733403963000,"abstract":"  Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"_TmIrRZSTXVmXAb3aj8dVrx0qTsQQ_jPNCowFsSVY9E","pdfSize":"4550724"}