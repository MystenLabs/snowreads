{"id":"2407.05863","title":"Almost Sure Convergence and Non-asymptotic Concentration Bounds for\n  Stochastic Mirror Descent Algorithm","authors":"Anik Kumar Paul, Arun D Mahindrakar and Rachel K Kalaimani","authorsParsed":[["Paul","Anik Kumar",""],["Mahindrakar","Arun D",""],["Kalaimani","Rachel K",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 12:29:10 GMT"}],"updateDate":"2024-07-09","timestamp":1720441750000,"abstract":"  This letter investigates the convergence and concentration properties of the\nStochastic Mirror Descent (SMD) algorithm utilizing biased stochastic\nsubgradients. We establish the almost sure convergence of the algorithm's\niterates under the assumption of diminishing bias. Furthermore, we derive\nconcentration bounds for the discrepancy between the iterates' function values\nand the optimal value, based on standard assumptions. Subsequently, leveraging\nthe assumption of Sub-Gaussian noise in stochastic subgradients, we present\nrefined concentration bounds for this discrepancy.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"NVIeHOH-ML9ndWdACemY6QTOYaa2_MP7DrHnIzhb03s","pdfSize":"175001"}