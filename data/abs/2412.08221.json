{
  "id": "2412.08221",
  "title": "Generate Any Scene: Evaluating and Improving Text-to-Vision Generation\n  with Scene Graph Programming",
  "authors": "Ziqi Gao, Weikai Huang, Jieyu Zhang, Aniruddha Kembhavi, Ranjay\n  Krishna",
  "authorsParsed": [
    [
      "Gao",
      "Ziqi",
      ""
    ],
    [
      "Huang",
      "Weikai",
      ""
    ],
    [
      "Zhang",
      "Jieyu",
      ""
    ],
    [
      "Kembhavi",
      "Aniruddha",
      ""
    ],
    [
      "Krishna",
      "Ranjay",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 09:17:39 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 16 Dec 2024 09:54:46 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1733908659000,
  "abstract": "  DALL-E and Sora have gained attention by producing implausible images, such\nas \"astronauts riding a horse in space.\" Despite the proliferation of\ntext-to-vision models that have inundated the internet with synthetic visuals,\nfrom images to 3D assets, current benchmarks predominantly evaluate these\nmodels on real-world scenes paired with captions. We introduce Generate Any\nScene, a framework that systematically enumerates scene graphs representing a\nvast array of visual scenes, spanning realistic to imaginative compositions.\nGenerate Any Scene leverages 'scene graph programming', a method for\ndynamically constructing scene graphs of varying complexity from a structured\ntaxonomy of visual elements. This taxonomy includes numerous objects,\nattributes, and relations, enabling the synthesis of an almost infinite variety\nof scene graphs. Using these structured representations, Generate Any Scene\ntranslates each scene graph into a caption, enabling scalable evaluation of\ntext-to-vision models through standard metrics. We conduct extensive\nevaluations across multiple text-to-image, text-to-video, and text-to-3D\nmodels, presenting key findings on model performance. We find that DiT-backbone\ntext-to-image models align more closely with input captions than UNet-backbone\nmodels. Text-to-video models struggle with balancing dynamics and consistency,\nwhile both text-to-video and text-to-3D models show notable gaps in human\npreference alignment. We demonstrate the effectiveness of Generate Any Scene by\nconducting three practical applications leveraging captions generated by\nGenerate Any Scene: 1) a self-improving framework where models iteratively\nenhance their performance using generated data, 2) a distillation process to\ntransfer specific strengths from proprietary models to open-source\ncounterparts, and 3) improvements in content moderation by identifying and\ngenerating challenging synthetic data.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "updX_NRTKIqmES4xm_hWQ7IXKVTFVNBaL8VgxGDuBt0",
  "pdfSize": "8274160"
}