{
  "id": "2412.13705",
  "title": "Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation",
  "authors": "Minkyoung Kim, Yunha Kim, Hyeram Seo, Heejung Choi, Jiye Han, Gaeun\n  Kee, Soyoung Ko, HyoJe Jung, Byeolhee Kim, Young-Hak Kim, Sanghyun Park, Tae\n  Joon Jun",
  "authorsParsed": [
    [
      "Kim",
      "Minkyoung",
      ""
    ],
    [
      "Kim",
      "Yunha",
      ""
    ],
    [
      "Seo",
      "Hyeram",
      ""
    ],
    [
      "Choi",
      "Heejung",
      ""
    ],
    [
      "Han",
      "Jiye",
      ""
    ],
    [
      "Kee",
      "Gaeun",
      ""
    ],
    [
      "Ko",
      "Soyoung",
      ""
    ],
    [
      "Jung",
      "HyoJe",
      ""
    ],
    [
      "Kim",
      "Byeolhee",
      ""
    ],
    [
      "Kim",
      "Young-Hak",
      ""
    ],
    [
      "Park",
      "Sanghyun",
      ""
    ],
    [
      "Jun",
      "Tae Joon",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 10:49:41 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734518981000,
  "abstract": "  Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "GmzqA0pbUejvurEkN0hY8QK31XPAHVa0U_eUd9ROdXw",
  "pdfSize": "442542"
}