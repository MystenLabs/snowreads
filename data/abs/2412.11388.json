{"id":"2412.11388","title":"INTERACT: Enabling Interactive, Question-Driven Learning in Large\n  Language Models","authors":"Aum Kendapadi, Kerem Zaman, Rakesh R. Menon, Shashank Srivastava","authorsParsed":[["Kendapadi","Aum",""],["Zaman","Kerem",""],["Menon","Rakesh R.",""],["Srivastava","Shashank",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 02:28:53 GMT"}],"updateDate":"2024-12-17","timestamp":1734316133000,"abstract":"  Large language models (LLMs) excel at answering questions but remain passive\nlearners--absorbing static data without the ability to question and refine\nknowledge. This paper explores how LLMs can transition to interactive,\nquestion-driven learning through student-teacher dialogues. We introduce\nINTERACT (INTEReractive Learning for Adaptive Concept Transfer), a framework in\nwhich a \"student\" LLM engages a \"teacher\" LLM through iterative inquiries to\nacquire knowledge across 1,347 contexts, including song lyrics, news articles,\nmovie plots, academic papers, and images. Our experiments show that across a\nwide range of scenarios and LLM architectures, interactive learning\nconsistently enhances performance, achieving up to a 25% improvement, with\n'cold-start' student models matching static learning baselines in as few as\nfive dialogue turns. Interactive setups can also mitigate the disadvantages of\nweaker teachers, showcasing the robustness of question-driven learning.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"F7UpEmSFzpo1MCDPAWbRWKUt3x5fElI9d8mZhY-GZrM","pdfSize":"1423185"}