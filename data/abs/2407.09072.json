{"id":"2407.09072","title":"New Desiderata for Direct Preference Optimization","authors":"Xiangkun Hu, Tong He, David Wipf","authorsParsed":[["Hu","Xiangkun",""],["He","Tong",""],["Wipf","David",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 07:52:32 GMT"}],"updateDate":"2024-07-15","timestamp":1720770752000,"abstract":"  Large language models in the past have typically relied on some form of\nreinforcement learning with human feedback (RLHF) to better align model\nresponses with human preferences. However, because of oft-observed\ninstabilities when implementing these RLHF pipelines, various\nreparameterization techniques have recently been introduced to sidestep the\nneed for separately learning an RL reward model. Instead, directly fine-tuning\nfor human preferences is achieved via the minimization of a single closed-form\ntraining objective, a process originally referred to as direct preference\noptimization (DPO) and followed by several notable descendants. Although\neffective in certain real-world settings, we introduce new evaluation criteria\nthat serve to highlight unresolved shortcomings in the ability of existing DPO\nmethods to interpolate between a pre-trained reference model and empirical\nmeasures of human preferences, as well as unavoidable trade-offs in how low-\nand high-quality responses are regularized and constraints are handled. Our\ninsights then motivate an alternative DPO-like loss that provably mitigates\nthese limitations. Empirical results serve to corroborate notable aspects of\nour analyses.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"IzbYhPEcRfgQJpDplDCpeFy8Q7Wn8MxeEy7MC3vOwkg","pdfSize":"1183464","objectId":"0xb6d9eec738a4cef2f474a325c8f26637a219587c4d633a85fa20604f7e3c9651","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
