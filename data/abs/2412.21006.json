{"id":"2412.21006","title":"Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria","authors":"Joonwon Jang, Jaehee Kim, Wonbin Kweon, Hwanjo Yu","authorsParsed":[["Jang","Joonwon",""],["Kim","Jaehee",""],["Kweon","Wonbin",""],["Yu","Hwanjo",""]],"versions":[{"version":"v1","created":"Mon, 30 Dec 2024 15:15:08 GMT"},{"version":"v2","created":"Tue, 31 Dec 2024 03:06:15 GMT"}],"updateDate":"2025-01-03","timestamp":1735571708000,"abstract":"  Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2rvKLKy5KYIo6nWUVn0p_iI3i3vVNOeAm9u2jI8dzcM","pdfSize":"1038457"}