{"id":"2407.12831","title":"Truth is Universal: Robust Detection of Lies in LLMs","authors":"Lennart B\\\"urger, Fred A. Hamprecht, Boaz Nadler","authorsParsed":[["BÃ¼rger","Lennart",""],["Hamprecht","Fred A.",""],["Nadler","Boaz",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 13:01:54 GMT"}],"updateDate":"2024-07-19","timestamp":1720011714000,"abstract":"  Large Language Models (LLMs) have revolutionised natural language processing,\nexhibiting impressive human-like capabilities. In particular, LLMs are capable\nof \"lying\", knowingly outputting false statements. Hence, it is of interest and\nimportance to develop methods to detect when LLMs lie. Indeed, several authors\ntrained classifiers to detect LLM lies based on their internal model\nactivations. However, other researchers showed that these classifiers may fail\nto generalise, for example to negated statements. In this work, we aim to\ndevelop a robust method to detect when an LLM is lying. To this end, we make\nthe following key contributions: (i) We demonstrate the existence of a\ntwo-dimensional subspace, along which the activation vectors of true and false\nstatements can be separated. Notably, this finding is universal and holds for\nvarious LLMs, including Gemma-7B, LLaMA2-13B and LLaMA3-8B. Our analysis\nexplains the generalisation failures observed in previous studies and sets the\nstage for more robust lie detection; (ii) Building upon (i), we construct an\naccurate LLM lie detector. Empirically, our proposed classifier achieves\nstate-of-the-art performance, distinguishing simple true and false statements\nwith 94% accuracy and detecting more complex real-world lies with 95% accuracy.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FxPTuizzOyniQ_YrRN9jTDzd7Z2kYBYKh_ZTDDOgSjU","pdfSize":"2500253"}