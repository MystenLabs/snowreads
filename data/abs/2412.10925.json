{"id":"2412.10925","title":"Video Representation Learning with Joint-Embedding Predictive\n  Architectures","authors":"Katrina Drozdov, Ravid Shwartz-Ziv, Yann LeCun","authorsParsed":[["Drozdov","Katrina",""],["Shwartz-Ziv","Ravid",""],["LeCun","Yann",""]],"versions":[{"version":"v1","created":"Sat, 14 Dec 2024 18:33:29 GMT"}],"updateDate":"2024-12-17","timestamp":1734201209000,"abstract":"  Video representation learning is an increasingly important topic in machine\nlearning research. We present Video JEPA with Variance-Covariance\nRegularization (VJ-VCR): a joint-embedding predictive architecture for\nself-supervised video representation learning that employs variance and\ncovariance regularization to avoid representation collapse. We show that hidden\nrepresentations from our VJ-VCR contain abstract, high-level information about\nthe input data. Specifically, they outperform representations obtained from a\ngenerative baseline on downstream tasks that require understanding of the\nunderlying dynamics of moving objects in the videos. Additionally, we explore\ndifferent ways to incorporate latent variables into the VJ-VCR framework that\ncapture information about uncertainty in the future in non-deterministic\nsettings.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bdjfiY6HTTmdXod7kUIk4lVqAevByfE-T1E9gBsxkNk","pdfSize":"2628903"}