{
  "id": "2412.12571",
  "title": "ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting\n  with Diffusion Transformers",
  "authors": "Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Chen Liang, Tong\n  Shen, Han Zhang, Huanzhang Dou, Yu Liu, Jingren Zhou",
  "authorsParsed": [
    [
      "Huang",
      "Lianghua",
      ""
    ],
    [
      "Wang",
      "Wei",
      ""
    ],
    [
      "Wu",
      "Zhi-Fan",
      ""
    ],
    [
      "Shi",
      "Yupeng",
      ""
    ],
    [
      "Liang",
      "Chen",
      ""
    ],
    [
      "Shen",
      "Tong",
      ""
    ],
    [
      "Zhang",
      "Han",
      ""
    ],
    [
      "Dou",
      "Huanzhang",
      ""
    ],
    [
      "Liu",
      "Yu",
      ""
    ],
    [
      "Zhou",
      "Jingren",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 06:03:05 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734415385000,
  "abstract": "  Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the\ninherent in-context generation capabilities of pretrained diffusion\ntransformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks\nwith minimal or no architectural modifications. These capabilities are unlocked\nby concatenating self-attention tokens across multiple input and target images,\ncombined with grouped and masked generation pipelines. Building upon this\nfoundation, we present ChatDiT, a zero-shot, general-purpose, and interactive\nvisual generation framework that leverages pretrained diffusion transformers in\ntheir original form, requiring no additional tuning, adapters, or\nmodifications. Users can interact with ChatDiT to create interleaved text-image\narticles, multi-page picture books, edit images, design IP derivatives, or\ndevelop character design settings, all through free-form natural language\nacross one or more conversational rounds. At its core, ChatDiT employs a\nmulti-agent system comprising three key components: an Instruction-Parsing\nagent that interprets user-uploaded images and instructions, a\nStrategy-Planning agent that devises single-step or multi-step generation\nactions, and an Execution agent that performs these actions using an in-context\ntoolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench\narXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with\ndiverse instructions and varying numbers of input and target images. Despite\nits simplicity and training-free approach, ChatDiT surpasses all competitors,\nincluding those specifically designed and trained on extensive multi-task\ndatasets. We further identify key limitations of pretrained DiTs in zero-shot\nadapting to tasks. We release all code, agents, results, and intermediate\noutputs to facilitate further research at https://github.com/ali-vilab/ChatDiT\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "0B18L9xwi-fq_bXYY4WuRH1jhy5xy_9KdHb8kGK2sto",
  "pdfSize": "18381427"
}