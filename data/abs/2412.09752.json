{
  "id": "2412.09752",
  "title": "A Quasilinear Algorithm for Computing Higher-Order Derivatives of Deep\n  Feed-Forward Neural Networks",
  "authors": "Kyle R. Chickering",
  "authorsParsed": [
    [
      "Chickering",
      "Kyle R.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 12 Dec 2024 22:57:28 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734044248000,
  "abstract": "  The use of neural networks for solving differential equations is practically\ndifficult due to the exponentially increasing runtime of autodifferentiation\nwhen computing high-order derivatives. We propose $n$-TangentProp, the natural\nextension of the TangentProp formalism \\cite{simard1991tangent} to arbitrarily\nmany derivatives. $n$-TangentProp computes the exact derivative $d^n/dx^n f(x)$\nin quasilinear, instead of exponential time, for a densely connected,\nfeed-forward neural network $f$ with a smooth, parameter-free activation\nfunction. We validate our algorithm empirically across a range of depths,\nwidths, and number of derivatives. We demonstrate that our method is\nparticularly beneficial in the context of physics-informed neural networks\nwhere \\ntp allows for significantly faster training times than previous methods\nand has favorable scaling with respect to both model size and loss-function\ncomplexity as measured by the number of required derivatives. The code for this\npaper can be found at https://github.com/kyrochi/n\\_tangentprop.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "-zRG4WhFpSLjRF20kDgxhg5NAXsz2TNMcLNd9fhK0yY",
  "pdfSize": "4572271"
}