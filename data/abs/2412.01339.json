{
  "id": "2412.01339",
  "title": "Negative Token Merging: Image-based Adversarial Feature Guidance",
  "authors": "Jaskirat Singh, Lindsey Li, Weijia Shi, Ranjay Krishna, Yejin Choi,\n  Pang Wei Koh, Michael F. Cohen, Stephen Gould, Liang Zheng, Luke Zettlemoyer",
  "authorsParsed": [
    [
      "Singh",
      "Jaskirat",
      ""
    ],
    [
      "Li",
      "Lindsey",
      ""
    ],
    [
      "Shi",
      "Weijia",
      ""
    ],
    [
      "Krishna",
      "Ranjay",
      ""
    ],
    [
      "Choi",
      "Yejin",
      ""
    ],
    [
      "Koh",
      "Pang Wei",
      ""
    ],
    [
      "Cohen",
      "Michael F.",
      ""
    ],
    [
      "Gould",
      "Stephen",
      ""
    ],
    [
      "Zheng",
      "Liang",
      ""
    ],
    [
      "Zettlemoyer",
      "Luke",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 10:06:57 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 5 Dec 2024 18:43:25 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733134017000,
  "abstract": "  Text-based adversarial guidance using a negative prompt has emerged as a\nwidely adopted approach to steer diffusion models away from producing undesired\nconcepts. While useful, performing adversarial guidance using text alone can be\ninsufficient to capture complex visual concepts or avoid specific visual\nelements like copyrighted characters. In this paper, for the first time we\nexplore an alternate modality in this direction by performing adversarial\nguidance directly using visual features from a reference image or other images\nin a batch. We introduce negative token merging (NegToMe), a simple but\neffective training-free approach which performs adversarial guidance through\nimages by selectively pushing apart matching visual features between reference\nand generated images during the reverse diffusion process. By simply adjusting\nthe used reference, NegToMe enables a diverse range of applications. Notably,\nwhen using other images in same batch as reference, we find that NegToMe\nsignificantly enhances output diversity (e.g., racial, gender, visual) by\nguiding features of each image away from others. Similarly, when used w.r.t.\ncopyrighted reference images, NegToMe reduces visual similarity to copyrighted\ncontent by 34.57%. NegToMe is simple to implement using just few-lines of code,\nuses only marginally higher (<4%) inference time and is compatible with\ndifferent diffusion architectures, including those like Flux, which don't\nnatively support the use of a negative prompt. Code is available at\nhttps://negtome.github.io\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Graphics",
    "Computer Science/Machine Learning",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "kPCAEJzYqO801VwUcD2u8uCglsy6qHs2F5pSIKJkiuo",
  "pdfSize": "48247581"
}