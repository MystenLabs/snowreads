{"id":"2412.13110","title":"Improving Explainability of Sentence-level Metrics via Edit-level\n  Attribution for Grammatical Error Correction","authors":"Takumi Goto, Justin Vasselli, Taro Watanabe","authorsParsed":[["Goto","Takumi",""],["Vasselli","Justin",""],["Watanabe","Taro",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 17:31:17 GMT"}],"updateDate":"2024-12-18","timestamp":1734456677000,"abstract":"  Various evaluation metrics have been proposed for Grammatical Error\nCorrection (GEC), but many, particularly reference-free metrics, lack\nexplainability. This lack of explainability hinders researchers from analyzing\nthe strengths and weaknesses of GEC models and limits the ability to provide\ndetailed feedback for users. To address this issue, we propose attributing\nsentence-level scores to individual edits, providing insight into how specific\ncorrections contribute to the overall performance. For the attribution method,\nwe use Shapley values, from cooperative game theory, to compute the\ncontribution of each edit. Experiments with existing sentence-level metrics\ndemonstrate high consistency across different edit granularities and show\napproximately 70\\% alignment with human evaluations. In addition, we analyze\nbiases in the metrics based on the attribution results, revealing trends such\nas the tendency to ignore orthographic edits. Our implementation is available\nat \\url{https://github.com/naist-nlp/gec-attribute}.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"xtBL0g0PfPgXNz2RR4DvwI28fYY0rRe4dZlJh1ZfnWY","pdfSize":"1442384"}