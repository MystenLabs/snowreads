{"id":"2412.03398","title":"RedStone: Curating General, Code, Math, and QA Data for Large Language\n  Models","authors":"Yaoyao Chang, Lei Cui, Li Dong, Shaohan Huang, Yangyu Huang, Yupan\n  Huang, Scarlett Li, Tengchao Lv, Shuming Ma, Qinzheng Sun, Wenhui Wang, Furu\n  Wei, Ying Xin, Mao Yang, Qiufeng Yin, Xingxing Zhang","authorsParsed":[["Chang","Yaoyao",""],["Cui","Lei",""],["Dong","Li",""],["Huang","Shaohan",""],["Huang","Yangyu",""],["Huang","Yupan",""],["Li","Scarlett",""],["Lv","Tengchao",""],["Ma","Shuming",""],["Sun","Qinzheng",""],["Wang","Wenhui",""],["Wei","Furu",""],["Xin","Ying",""],["Yang","Mao",""],["Yin","Qiufeng",""],["Zhang","Xingxing",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 15:27:39 GMT"}],"updateDate":"2024-12-05","timestamp":1733326059000,"abstract":"  Pre-training Large Language Models (LLMs) on high-quality, meticulously\ncurated datasets is widely recognized as critical for enhancing their\nperformance and generalization capabilities. This study explores the untapped\npotential of Common Crawl as a comprehensive and flexible resource for\npre-training LLMs, addressing both general-purpose language understanding and\nspecialized domain knowledge. We introduce RedStone, an innovative and scalable\npipeline engineered to extract and process data from Common Crawl, facilitating\nthe creation of extensive and varied pre-training datasets. Unlike traditional\ndatasets, which often require expensive curation and domain-specific expertise,\nRedStone leverages the breadth of Common Crawl to deliver datasets tailored to\na wide array of domains. In this work, we exemplify its capability by\nconstructing pre-training datasets across multiple fields, including general\nlanguage understanding, code, mathematics, and question-answering tasks. The\nflexibility of RedStone allows for easy adaptation to other specialized\ndomains, significantly lowering the barrier to creating valuable\ndomain-specific datasets. Our findings demonstrate that Common Crawl, when\nharnessed through effective pipelines like RedStone, can serve as a rich,\nrenewable source of pre-training data, unlocking new avenues for domain\nadaptation and knowledge discovery in LLMs. This work also underscores the\nimportance of innovative data acquisition strategies and highlights the role of\nweb-scale data as a powerful resource in the continued evolution of LLMs.\nRedStone code and data samples will be publicly available at\n\\url{https://aka.ms/redstone}.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"9_Eg_AuC-gzExa6bPXDrgIDd3PyurxXmLTDWGA5y3n8","pdfSize":"2298099"}