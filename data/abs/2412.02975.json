{"id":"2412.02975","title":"Theoretical limitations of multi-layer Transformer","authors":"Lijie Chen, Binghui Peng, Hongxun Wu","authorsParsed":[["Chen","Lijie",""],["Peng","Binghui",""],["Wu","Hongxun",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 02:37:31 GMT"}],"updateDate":"2024-12-05","timestamp":1733279851000,"abstract":"  Transformers, especially the decoder-only variants, are the backbone of most\nmodern large language models; yet we do not have much understanding of their\nexpressive power except for the simple $1$-layer case.\n  Due to the difficulty of analyzing multi-layer models, all previous work\nrelies on unproven complexity conjectures to show limitations for multi-layer\nTransformers. In this work, we prove the first $\\textit{unconditional}$ lower\nbound against multi-layer decoder-only transformers. For any constant $L$, we\nprove that any $L$-layer decoder-only transformer needs a polynomial model\ndimension ($n^{\\Omega(1)}$) to perform sequential composition of $L$ functions\nover an input of $n$ tokens.\n  As a consequence, our results give: (1) the first depth-width trade-off for\nmulti-layer transformers, exhibiting that the $L$-step composition task is\nexponentially harder for $L$-layer models compared to $(L+1)$-layer ones; (2)\nan unconditional separation between encoder and decoder, exhibiting a hard task\nfor decoders that can be solved by an exponentially shallower and smaller\nencoder; (3) a provable advantage of chain-of-thought, exhibiting a task that\nbecomes exponentially easier with chain-of-thought.\n  On the technical side, we propose the multi-party $\\textit{autoregressive}$\n$\\textit{communication}$ $\\textit{model}$ that captures the computation of a\ndecoder-only Transformer. We also introduce a new proof technique that finds a\ncertain $\\textit{indistinguishable}$ $\\textit{decomposition}$ of all possible\ninputs iteratively for proving lower bounds in this model. We believe our new\ncommunication model and proof technique will be helpful to further understand\nthe computational power of transformers.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Computational Complexity","Computer Science/Data Structures and Algorithms"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"B97IPVwZyIXUtpFspYMDCIt1ElU-ppntqGE7bBz_Avg","pdfSize":"430534"}