{"id":"2407.11542","title":"Understanding Counting in Small Transformers: The Interplay between\n  Attention and Feed-Forward Layers","authors":"Freya Behrens, Luca Biggio, Lenka Zdeborov\\'a","authorsParsed":[["Behrens","Freya",""],["Biggio","Luca",""],["Zdeborov√°","Lenka",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 09:48:10 GMT"}],"updateDate":"2024-07-17","timestamp":1721123290000,"abstract":"  We provide a comprehensive analysis of simple transformer models trained on\nthe histogram task, where the goal is to count the occurrences of each item in\nthe input sequence from a fixed alphabet. Despite its apparent simplicity, this\ntask exhibits a rich phenomenology that allows us to characterize how different\narchitectural components contribute towards the emergence of distinct\nalgorithmic solutions. In particular, we showcase the existence of two\nqualitatively different mechanisms that implement a solution, relation- and\ninventory-based counting. Which solution a model can implement depends\nnon-trivially on the precise choice of the attention mechanism, activation\nfunction, memorization capacity and the presence of a beginning-of-sequence\ntoken. By introspecting learned models on the counting task, we find evidence\nfor the formation of both mechanisms. From a broader perspective, our analysis\noffers a framework to understand how the interaction of different architectural\ncomponents of transformer models shapes diverse algorithmic solutions and\napproximations.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yHU2M8dGYvaopbZAwLiLlggMzW2pU898nuVlMmaUuNA","pdfSize":"8526069"}