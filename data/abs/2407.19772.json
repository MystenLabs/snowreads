{"id":"2407.19772","title":"Generating Unseen Code Tests In Infinitum","authors":"Marcel Zalmanovici and Orna Raz and Eitan Farchi and Iftach Freund","authorsParsed":[["Zalmanovici","Marcel",""],["Raz","Orna",""],["Farchi","Eitan",""],["Freund","Iftach",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 08:11:20 GMT"}],"updateDate":"2024-07-30","timestamp":1722240680000,"abstract":"  Large Language Models (LLMs) are used for many tasks, including those related\nto coding. An important aspect of being able to utilize LLMs is the ability to\nassess their fitness for specific usages. The common practice is to evaluate\nLLMs against a set of benchmarks. While benchmarks provide a sound foundation\nfor evaluation and comparison of alternatives, they suffer from the well-known\nweakness of leaking into the training data \\cite{Xu2024Benchmarking}. We\npresent a method for creating benchmark variations that generalize across\ncoding tasks and programming languages, and may also be applied to in-house\ncode bases. Our approach enables ongoing generation of test-data thus\nmitigating the leaking into the training data issue. We implement one\nbenchmark, called \\textit{auto-regression}, for the task of text-to-code\ngeneration in Python. Auto-regression is specifically created to aid in\ndebugging and in tracking model generation changes as part of the LLM\nregression testing process.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FXN8h2JQQjJ2kT2A4hvspSxNLQfN4Mw6hLfcdKx5W6M","pdfSize":"309835"}