{
  "id": "2412.15660",
  "title": "Adaptable and Precise: Enterprise-Scenario LLM Function-Calling\n  Capability Training Pipeline",
  "authors": "Guancheng Zeng, Wentao Ding, Beining Xu, Chi Zhang, Wenqiang Han, Gang\n  Li, Jingjing Mo, Pengxu Qiu, Xinran Tao, Wang Tao, Haowen Hu",
  "authorsParsed": [
    [
      "Zeng",
      "Guancheng",
      ""
    ],
    [
      "Ding",
      "Wentao",
      ""
    ],
    [
      "Xu",
      "Beining",
      ""
    ],
    [
      "Zhang",
      "Chi",
      ""
    ],
    [
      "Han",
      "Wenqiang",
      ""
    ],
    [
      "Li",
      "Gang",
      ""
    ],
    [
      "Mo",
      "Jingjing",
      ""
    ],
    [
      "Qiu",
      "Pengxu",
      ""
    ],
    [
      "Tao",
      "Xinran",
      ""
    ],
    [
      "Tao",
      "Wang",
      ""
    ],
    [
      "Hu",
      "Haowen",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 08:20:21 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734682821000,
  "abstract": "  Enterprises possess a vast array of API assets scattered across various\nfunctions, forming the backbone of existing business processes. By leveraging\nthese APIs as functional tools, enterprises can design diverse,\nscenario-specific agent applications, driven by on-premise function-calling\nmodels as the core engine. However, generic models often fail to meet\nenterprise requirements in terms of computational efficiency, output accuracy,\nand stability, necessitating scenario-specific adaptation. In this paper, we\npropose a training pipeline for function-calling capabilities tailored to\nreal-world business scenarios. This pipeline includes the synthesis and\naugmentation of scenario-specific function-calling data, model fine-tuning, and\nperformance evaluation and analysis. Using this pipeline, we generated 1,260\nfully AI-generated samples and 1,035 augmented manually-labeled samples in\ndigital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as\nthe base model and fine-tuned using the LoRA method on four GPUs with 24GB\nVRAM. Our fine-tuned model demonstrated outstanding performance in evaluations\nand practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test\nset. These results validate the reliability of the proposed pipeline for\ntraining scenario-specific function-calling models.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language",
    "Computer Science/Software Engineering"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "m-SK33Y0xMQDaB34UfqK0sy84u2dQ9HxK3zEEpuReMc",
  "pdfSize": "875476"
}