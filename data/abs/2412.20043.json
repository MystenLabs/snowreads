{"id":"2412.20043","title":"STAYKATE: Hybrid In-Context Example Selection Combining\n  Representativeness Sampling and Retrieval-based Approach -- A Case Study on\n  Science Domains","authors":"Chencheng Zhu and Kazutaka Shimada and Tomoki Taniguchi and Tomoko\n  Ohkuma","authorsParsed":[["Zhu","Chencheng",""],["Shimada","Kazutaka",""],["Taniguchi","Tomoki",""],["Ohkuma","Tomoko",""]],"versions":[{"version":"v1","created":"Sat, 28 Dec 2024 06:13:50 GMT"}],"updateDate":"2024-12-31","timestamp":1735366430000,"abstract":"  Large language models (LLMs) demonstrate the ability to learn in-context,\noffering a potential solution for scientific information extraction, which\noften contends with challenges such as insufficient training data and the high\ncost of annotation processes. Given that the selection of in-context examples\ncan significantly impact performance, it is crucial to design a proper method\nto sample the efficient ones. In this paper, we propose STAYKATE, a\nstatic-dynamic hybrid selection method that combines the principles of\nrepresentativeness sampling from active learning with the prevalent\nretrieval-based approach. The results across three domain-specific datasets\nindicate that STAYKATE outperforms both the traditional supervised methods and\nexisting selection methods. The enhancement in performance is particularly\npronounced for entity types that other methods pose challenges.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"613qGAwxxC0cxid_L9qxmNbXQIPQrZO2WKoDNgl4i7U","pdfSize":"1758372"}