{"id":"2412.09191","title":"RAD: Region-Aware Diffusion Models for Image Inpainting","authors":"Sora Kim, Sungho Suh, Minsik Lee","authorsParsed":[["Kim","Sora",""],["Suh","Sungho",""],["Lee","Minsik",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 11:38:46 GMT"},{"version":"v2","created":"Tue, 17 Dec 2024 05:21:52 GMT"},{"version":"v3","created":"Thu, 19 Dec 2024 02:44:14 GMT"}],"updateDate":"2024-12-20","timestamp":1734003526000,"abstract":"  Diffusion models have achieved remarkable success in image generation, with\napplications broadening across various domains. Inpainting is one such\napplication that can benefit significantly from diffusion models. Existing\nmethods either hijack the reverse process of a pretrained diffusion model or\ncast the problem into a larger framework, \\ie, conditioned generation. However,\nthese approaches often require nested loops in the generation process or\nadditional components for conditioning. In this paper, we present region-aware\ndiffusion models (RAD) for inpainting with a simple yet effective reformulation\nof the vanilla diffusion models. RAD utilizes a different noise schedule for\neach pixel, which allows local regions to be generated asynchronously while\nconsidering the global image context. A plain reverse process requires no\nadditional components, enabling RAD to achieve inference time up to 100 times\nfaster than the state-of-the-art approaches. Moreover, we employ low-rank\nadaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models,\nreducing computational burdens in training as well. Experiments demonstrated\nthat RAD provides state-of-the-art results both qualitatively and\nquantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"pCK23EL3hppNjihLfRWpaTiHxVPSeyjAqd3k8-AeCJA","pdfSize":"28188489"}