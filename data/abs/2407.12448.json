{"id":"2407.12448","title":"Energy-Guided Diffusion Sampling for Offline-to-Online Reinforcement\n  Learning","authors":"Xu-Hui Liu, Tian-Shuo Liu, Shengyi Jiang, Ruifeng Chen, Zhilong Zhang,\n  Xinwei Chen, Yang Yu","authorsParsed":[["Liu","Xu-Hui",""],["Liu","Tian-Shuo",""],["Jiang","Shengyi",""],["Chen","Ruifeng",""],["Zhang","Zhilong",""],["Chen","Xinwei",""],["Yu","Yang",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 09:56:51 GMT"},{"version":"v2","created":"Tue, 3 Sep 2024 18:40:47 GMT"}],"updateDate":"2024-09-05","timestamp":1721210211000,"abstract":"  Combining offline and online reinforcement learning (RL) techniques is indeed\ncrucial for achieving efficient and safe learning where data acquisition is\nexpensive. Existing methods replay offline data directly in the online phase,\nresulting in a significant challenge of data distribution shift and\nsubsequently causing inefficiency in online fine-tuning. To address this issue,\nwe introduce an innovative approach, \\textbf{E}nergy-guided \\textbf{DI}ffusion\n\\textbf{S}ampling (EDIS), which utilizes a diffusion model to extract prior\nknowledge from the offline dataset and employs energy functions to distill this\nknowledge for enhanced data generation in the online phase. The theoretical\nanalysis demonstrates that EDIS exhibits reduced suboptimality compared to\nsolely utilizing online data or directly reusing offline data. EDIS is a\nplug-in approach and can be combined with existing methods in offline-to-online\nRL setting. By implementing EDIS to off-the-shelf methods Cal-QL and IQL, we\nobserve a notable 20% average improvement in empirical performance on MuJoCo,\nAntMaze, and Adroit environments. Code is available at\n\\url{https://github.com/liuxhym/EDIS}.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qOZ3XSwh7djUl-uSgpNTHGjYFBg1qSocDpu-In9Q2no","pdfSize":"751785"}