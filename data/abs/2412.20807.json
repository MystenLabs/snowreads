{"id":"2412.20807","title":"Two Heads Are Better Than One: Averaging along Fine-Tuning to Improve\n  Targeted Transferability","authors":"Hui Zeng, Sanshuai Cui, Biwei Chen and Anjie Peng","authorsParsed":[["Zeng","Hui",""],["Cui","Sanshuai",""],["Chen","Biwei",""],["Peng","Anjie",""]],"versions":[{"version":"v1","created":"Mon, 30 Dec 2024 09:01:27 GMT"}],"updateDate":"2024-12-31","timestamp":1735549287000,"abstract":"  With much longer optimization time than that of untargeted attacks\nnotwithstanding, the transferability of targeted attacks is still far from\nsatisfactory. Recent studies reveal that fine-tuning an existing adversarial\nexample (AE) in feature space can efficiently boost its targeted\ntransferability. However, existing fine-tuning schemes only utilize the\nendpoint and ignore the valuable information in the fine-tuning trajectory.\nNoting that the vanilla fine-tuning trajectory tends to oscillate around the\nperiphery of a flat region of the loss surface, we propose averaging over the\nfine-tuning trajectory to pull the crafted AE towards a more centered region.\nWe compare the proposed method with existing fine-tuning schemes by integrating\nthem with state-of-the-art targeted attacks in various attacking scenarios.\nExperimental results uphold the superiority of the proposed method in boosting\ntargeted transferability. The code is available at github.com/zengh5/Avg_FT.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PglwU8yux9ut97HXbC-fC_OtxzYlHu7c0Ch5-Ovd6R4","pdfSize":"2271955"}