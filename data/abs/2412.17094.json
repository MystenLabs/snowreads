{"id":"2412.17094","title":"Analysis on LLMs Performance for Code Summarization","authors":"Md. Ahnaf Akib, Md. Muktadir Mazumder, Salman Ahsan","authorsParsed":[["Akib","Md. Ahnaf",""],["Mazumder","Md. Muktadir",""],["Ahsan","Salman",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 17:09:34 GMT"},{"version":"v2","created":"Fri, 24 Jan 2025 08:46:41 GMT"}],"updateDate":"2025-01-27","timestamp":1734887374000,"abstract":"  Code summarization aims to generate concise natural language descriptions for\nsource code. Deep learning has been used more and more recently in software\nengineering, particularly for tasks like code creation and summarization.\nSpecifically, it appears that the most current Large Language Models with\ncoding perform well on these tasks. Large Language Models (LLMs) have\nsignificantly advanced the field of code summarization, providing sophisticated\nmethods for generating concise and accurate summaries of source code. This\nstudy aims to perform a comparative analysis of several open-source LLMs,\nnamely LLaMA-3, Phi-3, Mistral, and Gemma. These models' performance is\nassessed using important metrics such as BLEU\\textsubscript{3.1} and\nROUGE\\textsubscript{3.2}.\n  Through this analysis, we seek to identify the strengths and weaknesses of\neach model, offering insights into their applicability and effectiveness in\ncode summarization tasks. Our findings contribute to the ongoing development\nand refinement of LLMs, supporting their integration into tools that enhance\nsoftware development and maintenance processes.\n","subjects":["Computer Science/Software Engineering","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"pIlmFc5tSm_g2UBsLxamiqlpEx_AF2PEd3Pf806FP1I","pdfSize":"2171576"}