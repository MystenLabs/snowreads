{"id":"2412.13173","title":"Locate n' Rotate: Two-stage Openable Part Detection with Foundation\n  Model Priors","authors":"Siqi Li, Xiaoxue Chen, Haoyu Cheng, Guyue Zhou, Hao Zhao and Guanzhong\n  Tian","authorsParsed":[["Li","Siqi",""],["Chen","Xiaoxue",""],["Cheng","Haoyu",""],["Zhou","Guyue",""],["Zhao","Hao",""],["Tian","Guanzhong",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 18:52:30 GMT"}],"updateDate":"2024-12-18","timestamp":1734461550000,"abstract":"  Detecting the openable parts of articulated objects is crucial for downstream\napplications in intelligent robotics, such as pulling a drawer. This task poses\na multitasking challenge due to the necessity of understanding object\ncategories and motion. Most existing methods are either category-specific or\ntrained on specific datasets, lacking generalization to unseen environments and\nobjects. In this paper, we propose a Transformer-based Openable Part Detection\n(OPD) framework named Multi-feature Openable Part Detection (MOPD) that\nincorporates perceptual grouping and geometric priors, outperforming previous\nmethods in performance. In the first stage of the framework, we introduce a\nperceptual grouping feature model that provides perceptual grouping feature\npriors for openable part detection, enhancing detection results through a\ncross-attention mechanism. In the second stage, a geometric understanding\nfeature model offers geometric feature priors for predicting motion parameters.\nCompared to existing methods, our proposed approach shows better performance in\nboth detection and motion parameter prediction. Codes and models are publicly\navailable at https://github.com/lisiqi-zju/MOPD\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"55_xYANbpMuYVgQ0SLS1ojLjgO1r14m5rFUFgRETfV4","pdfSize":"9372000"}