{"id":"2412.13170","title":"Re-calibrating methodologies in social media research: Challenge the\n  visual, work with Speech","authors":"Hongrui Jin","authorsParsed":[["Jin","Hongrui",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 18:47:57 GMT"}],"updateDate":"2024-12-18","timestamp":1734461277000,"abstract":"  This article methodologically reflects on how social media scholars can\neffectively engage with speech-based data in their analyses. While contemporary\nmedia studies have embraced textual, visual, and relational data, the aural\ndimension remained comparatively under-explored. Building on the notion of\nsecondary orality and rejection towards purely visual culture, the paper argues\nthat considering voice and speech at scale enriches our understanding of\nmultimodal digital content. The paper presents the TikTok Subtitles Toolkit\nthat offers accessible speech processing readily compatible with existing\nworkflows. In doing so, it opens new avenues for large-scale inquiries that\nblend quantitative insights with qualitative precision. Two illustrative cases\nhighlight both opportunities and limitations of speech research: while genres\nlike #storytime on TikTok benefit from the exploration of spoken narratives,\nnonverbal or music-driven content may not yield significant insights using\nspeech data. The article encourages researchers to integrate aural exploration\nthoughtfully to complement existing methods, rather than replacing them. I\nconclude that the expansion of our methodological repertoire enables richer\ninterpretations of platformised content, and our capacity to unpack digital\ncultures as they become increasingly multimodal.\n","subjects":["Computer Science/Social and Information Networks","Computer Science/Information Retrieval"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"XYgiY4U53pysNqaQlt_hmNqEyKbWCAuPdvGnRBx-fMU","pdfSize":"7053309"}