{
  "id": "2412.13378",
  "title": "SummExecEdit: A Factual Consistency Benchmark in Summarization with\n  Executable Edits",
  "authors": "Onkar Thorat, Philippe Laban, Chien-Sheng Wu",
  "authorsParsed": [
    [
      "Thorat",
      "Onkar",
      ""
    ],
    [
      "Laban",
      "Philippe",
      ""
    ],
    [
      "Wu",
      "Chien-Sheng",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 23:26:44 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734478004000,
  "abstract": "  Detecting factual inconsistencies in summarization is critical, yet existing\nbenchmarks lack the necessary challenge and interpretability for robust\nevaluation. In this paper, we introduce SummExecEdit, a novel benchmark\nleveraging executable edits to assess models on their ability to both detect\nfactual errors and provide accurate explanations. The top-performing model,\nClaude3-Opus, achieves a joint detection and explanation score of only 0.49 in\nour benchmark, with individual scores of 0.67 for detection and 0.73 for\nexplanation. Furthermore, we identify four primary types of explanation errors,\nwith 45.4% of errors focusing on completely unrelated parts of the summary.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "GvChxgPvWy1QISFR03Gr1FTfrtZaG1b6ARPu-uQjJ_g",
  "pdfSize": "722882"
}