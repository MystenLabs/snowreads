{"id":"2412.00985","title":"Provable Partially Observable Reinforcement Learning with Privileged\n  Information","authors":"Yang Cai, Xiangyu Liu, Argyris Oikonomou, Kaiqing Zhang","authorsParsed":[["Cai","Yang",""],["Liu","Xiangyu",""],["Oikonomou","Argyris",""],["Zhang","Kaiqing",""]],"versions":[{"version":"v1","created":"Sun, 1 Dec 2024 22:26:27 GMT"},{"version":"v2","created":"Fri, 17 Jan 2025 22:55:07 GMT"},{"version":"v3","created":"Thu, 20 Feb 2025 21:41:07 GMT"}],"updateDate":"2025-02-24","timestamp":1733091987000,"abstract":"  Partial observability of the underlying states generally presents significant\nchallenges for reinforcement learning (RL). In practice, certain\n\\emph{privileged information}, e.g., the access to states from simulators, has\nbeen exploited in training and has achieved prominent empirical successes. To\nbetter understand the benefits of privileged information, we revisit and\nexamine several simple and practically used paradigms in this setting.\nSpecifically, we first formalize the empirical paradigm of \\emph{expert\ndistillation} (also known as \\emph{teacher-student} learning), demonstrating\nits pitfall in finding near-optimal policies. We then identify a condition of\nthe partially observable environment, the \\emph{deterministic filter\ncondition}, under which expert distillation achieves sample and computational\ncomplexities that are \\emph{both} polynomial. Furthermore, we investigate\nanother useful empirical paradigm of \\emph{asymmetric actor-critic}, and focus\non the more challenging setting of observable partially observable Markov\ndecision processes. We develop a belief-weighted asymmetric actor-critic\nalgorithm with polynomial sample and quasi-polynomial computational\ncomplexities, in which one key component is a new provable oracle for learning\nbelief states that preserve \\emph{filter stability} under a misspecified model,\nwhich may be of independent interest. Finally, we also investigate the provable\nefficiency of partially observable multi-agent RL (MARL) with privileged\ninformation. We develop algorithms featuring\n\\emph{centralized-training-with-decentralized-execution}, a popular framework\nin empirical MARL, with polynomial sample and (quasi-)polynomial computational\ncomplexities in both paradigms above. Compared with a few recent related\ntheoretical studies, our focus is on understanding practically inspired\nalgorithmic paradigms, without computationally intractable oracles.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"sWezFGOUotq8VykPayfbdfeqoHGpw6W_OhcShfHv5U4","pdfSize":"1091087"}