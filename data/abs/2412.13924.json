{
  "id": "2412.13924",
  "title": "Language verY Rare for All",
  "authors": "Ibrahim Merad, Amos Wolf, Ziad Mazzawi, Yannick L\\'eo",
  "authorsParsed": [
    [
      "Merad",
      "Ibrahim",
      ""
    ],
    [
      "Wolf",
      "Amos",
      ""
    ],
    [
      "Mazzawi",
      "Ziad",
      ""
    ],
    [
      "LÃ©o",
      "Yannick",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 15:07:23 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734534443000,
  "abstract": "  In the quest to overcome language barriers, encoder-decoder models like NLLB\nhave expanded machine translation to rare languages, with some models (e.g.,\nNLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform\nwell in translation, open LLMs prove highly competitive when fine-tuned for\nspecific tasks involving unknown corpora. We introduce LYRA (Language verY Rare\nfor All), a novel approach that combines open LLM fine-tuning,\nretrieval-augmented generation (RAG), and transfer learning from related\nhigh-resource languages. This study is exclusively focused on single-GPU\ntraining to facilitate ease of adoption. Our study focuses on two-way\ntranslation between French and Mon\\'egasque, a rare language unsupported by\nexisting translation tools due to limited corpus availability. Our results\ndemonstrate LYRA's effectiveness, frequently surpassing and consistently\nmatching state-of-the-art encoder-decoder models in rare language translation.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "aRgiYHa3-b_bluplRU5bJ1pVAKAAAn9zVvZ0asdqg80",
  "pdfSize": "478302"
}