{"id":"2412.07207","title":"MAPLE: A Framework for Active Preference Learning Guided by Large\n  Language Models","authors":"Saaduddin Mahmud, Mason Nakamura, Shlomo Zilberstein","authorsParsed":[["Mahmud","Saaduddin",""],["Nakamura","Mason",""],["Zilberstein","Shlomo",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 05:55:14 GMT"},{"version":"v2","created":"Fri, 20 Dec 2024 01:08:20 GMT"}],"updateDate":"2024-12-23","timestamp":1733810114000,"abstract":"  The advent of large language models (LLMs) has sparked significant interest\nin using natural language for preference learning. However, existing methods\noften suffer from high computational burdens, taxing human supervision, and\nlack of interpretability. To address these issues, we introduce MAPLE, a\nframework for large language model-guided Bayesian active preference learning.\nMAPLE leverages LLMs to model the distribution over preference functions,\nconditioning it on both natural language feedback and conventional preference\nlearning feedback, such as pairwise trajectory rankings. MAPLE also employs\nactive learning to systematically reduce uncertainty in this distribution and\nincorporates a language-conditioned active query selection mechanism to\nidentify informative and easy-to-answer queries, thus reducing human burden. We\nevaluate MAPLE's sample efficiency and preference inference quality across two\nbenchmarks, including a real-world vehicle route planning benchmark using\nOpenStreetMap data. Our results demonstrate that MAPLE accelerates the learning\nprocess and effectively improves humans' ability to answer queries.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wzLEm_ilTzbrNbwtYh27uxB8WdIkN_XVkzahAcqVrb0","pdfSize":"2036385"}