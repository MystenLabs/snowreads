{"id":"2412.16871","title":"Teaching LLMs to Refine with Tools","authors":"Dian Yu, Yuheng Zhang, Jiahao Xu, Tian Liang, Linfeng Song, Zhaopeng\n  Tu, Haitao Mi, Dong Yu","authorsParsed":[["Yu","Dian",""],["Zhang","Yuheng",""],["Xu","Jiahao",""],["Liang","Tian",""],["Song","Linfeng",""],["Tu","Zhaopeng",""],["Mi","Haitao",""],["Yu","Dong",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 05:43:50 GMT"}],"updateDate":"2024-12-24","timestamp":1734846230000,"abstract":"  Large language models (LLMs) can refine their responses based on feedback,\nenabling self-improvement through iterative training or test-time refinement.\nHowever, existing methods predominantly focus on refinement within the same\nreasoning format, which may lead to non-correcting behaviors. We propose CaP, a\nnovel approach that uses external tools to refine chain-of-thought (CoT)\nresponses generated by the same or other LLMs. CaP employs a two-stage training\nprocess: supervised fine-tuning followed by preference optimization with DPO\nvariants. Our observations highlight the critical role of preference\noptimization in enabling effective refinement. Additionally, we compare several\nsampling strategies to leverage CoT and tools at inference time. Experimental\nresults demonstrate CaP's potential for effective cross-reasoning refinement\nand efficient inference.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NuZZwr6nTExMNaDddEL-Uk0SthDr9OjGoYu80AmgLOQ","pdfSize":"697885"}