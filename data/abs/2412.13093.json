{
  "id": "2412.13093",
  "title": "Reservoir Computing for Fast, Simplified Reinforcement Learning on\n  Memory Tasks",
  "authors": "Kevin McKee",
  "authorsParsed": [
    [
      "McKee",
      "Kevin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 17:02:06 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734454926000,
  "abstract": "  Tasks in which rewards depend upon past information not available in the\ncurrent observation set can only be solved by agents that are equipped with\nshort-term memory. Usual choices for memory modules include trainable recurrent\nhidden layers, often with gated memory. Reservoir computing presents an\nalternative, in which a recurrent layer is not trained, but rather has a set of\nfixed, sparse recurrent weights. The weights are scaled to produce stable\ndynamical behavior such that the reservoir state contains a high-dimensional,\nnonlinear impulse response function of the inputs. An output decoder network\ncan then be used to map the compressive history represented by the reservoir's\nstate to any outputs, including agent actions or predictions. In this study, we\nfind that reservoir computing greatly simplifies and speeds up reinforcement\nlearning on memory tasks by (1) eliminating the need for backpropagation of\ngradients through time, (2) presenting all recent history simultaneously to the\ndownstream network, and (3) performing many useful and generic nonlinear\ncomputations upstream from the trained modules. In particular, these findings\noffer significant benefit to meta-learning that depends primarily on efficient\nand highly general memory systems.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "3HjF481YAIrzw2TgMwCojZHRCkOZfMDWM_cZAZT72Wk",
  "pdfSize": "588814"
}