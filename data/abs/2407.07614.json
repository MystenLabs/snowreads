{"id":"2407.07614","title":"MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image\n  Synthesis","authors":"Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun\n  Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, Ziwei Huang, LeiLei Gan, Hao\n  Jiang","authorsParsed":[["He","Wanggui",""],["Fu","Siming",""],["Liu","Mushui",""],["Wang","Xierui",""],["Xiao","Wenyi",""],["Shu","Fangxun",""],["Wang","Yi",""],["Zhang","Lei",""],["Yu","Zhelun",""],["Li","Haoyuan",""],["Huang","Ziwei",""],["Gan","LeiLei",""],["Jiang","Hao",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 12:52:49 GMT"},{"version":"v2","created":"Thu, 11 Jul 2024 11:05:53 GMT"}],"updateDate":"2024-07-12","timestamp":1720615969000,"abstract":"  Auto-regressive models have made significant progress in the realm of\nlanguage generation, yet they do not perform on par with diffusion models in\nthe domain of image synthesis. In this work, we introduce MARS, a novel\nframework for T2I generation that incorporates a specially designed Semantic\nVision-Language Integration Expert (SemVIE). This innovative component\nintegrates pre-trained LLMs by independently processing linguistic and visual\ninformation, freezing the textual component while fine-tuning the visual\ncomponent. This methodology preserves the NLP capabilities of LLMs while\nimbuing them with exceptional visual understanding. Building upon the powerful\nbase of the pre-trained Qwen-7B, MARS stands out with its bilingual generative\ncapabilities corresponding to both English and Chinese language prompts and the\ncapacity for joint image and text generation. The flexibility of this framework\nlends itself to migration towards any-to-any task adaptability. Furthermore,\nMARS employs a multi-stage training strategy that first establishes robust\nimage-text alignment through complementary bidirectional tasks and subsequently\nconcentrates on refining the T2I generation process, significantly augmenting\ntext-image synchrony and the granularity of image details. Notably, MARS\nrequires only 9% of the GPU days needed by SD1.5, yet it achieves remarkable\nresults across a variety of benchmarks, illustrating the training efficiency\nand the potential for swift deployment in various applications.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pouw0RLXkn8C3d0KtMihoQpw-7IwsIw_a6uwl_Li8LQ","pdfSize":"8915074"}