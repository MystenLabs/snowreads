{
  "id": "2412.17747",
  "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
  "authors": "Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, Arthur Szlam",
  "authorsParsed": [
    [
      "Liu",
      "Luyang",
      ""
    ],
    [
      "Pfeiffer",
      "Jonas",
      ""
    ],
    [
      "Wu",
      "Jiaxing",
      ""
    ],
    [
      "Xie",
      "Jun",
      ""
    ],
    [
      "Szlam",
      "Arthur",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 18:02:25 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734976945000,
  "abstract": "  Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "rXcKddNd_45hWk1-DwJPhfDvdr-47NzaLsf1XcpJmBs",
  "pdfSize": "800878"
}