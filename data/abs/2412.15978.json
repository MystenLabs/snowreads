{
  "id": "2412.15978",
  "title": "BabyHGRN: Exploring RNNs for Sample-Efficient Training of Language\n  Models",
  "authors": "Patrick Haller and Jonas Golde and Alan Akbik",
  "authorsParsed": [
    [
      "Haller",
      "Patrick",
      ""
    ],
    [
      "Golde",
      "Jonas",
      ""
    ],
    [
      "Akbik",
      "Alan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 15:21:41 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734708101000,
  "abstract": "  This paper explores the potential of recurrent neural networks (RNNs) and\nother subquadratic architectures as competitive alternatives to\ntransformer-based models in low-resource language modeling scenarios. We\nutilize HGRN2 (Qin et al., 2024), a recently proposed RNN-based architecture,\nand comparatively evaluate its effectiveness against transformer-based\nbaselines and other subquadratic architectures (LSTM, xLSTM, Mamba). Our\nexperimental results show that BABYHGRN, our HGRN2 language model, outperforms\ntransformer-based models in both the 10M and 100M word tracks of the challenge,\nas measured by their performance on the BLiMP, EWoK, GLUE and BEAR benchmarks.\nFurther, we show the positive impact of knowledge distillation. Our findings\nchallenge the prevailing focus on transformer architectures and indicate the\nviability of RNN-based models, particularly in resource-constrained\nenvironments.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "mShuxOdEJTMGucXQYiqzwb7ONQk60llVCWdYH9kvTq8",
  "pdfSize": "971205"
}