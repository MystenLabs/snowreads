{
  "id": "2412.04315",
  "title": "Densing Law of LLMs",
  "authors": "Chaojun Xiao, Jie Cai, Weilin Zhao, Guoyang Zeng, Biyuan Lin, Jie\n  Zhou, Zhi Zheng, Xu Han, Zhiyuan Liu, Maosong Sun",
  "authorsParsed": [
    [
      "Xiao",
      "Chaojun",
      ""
    ],
    [
      "Cai",
      "Jie",
      ""
    ],
    [
      "Zhao",
      "Weilin",
      ""
    ],
    [
      "Zeng",
      "Guoyang",
      ""
    ],
    [
      "Lin",
      "Biyuan",
      ""
    ],
    [
      "Zhou",
      "Jie",
      ""
    ],
    [
      "Zheng",
      "Zhi",
      ""
    ],
    [
      "Han",
      "Xu",
      ""
    ],
    [
      "Liu",
      "Zhiyuan",
      ""
    ],
    [
      "Sun",
      "Maosong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 16:31:13 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 6 Dec 2024 11:39:27 GMT"
    }
  ],
  "updateDate": "2024-12-09",
  "timestamp": 1733416273000,
  "abstract": "  Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Y6sT9x2b5Bq8rPneS5ItYpZNQzeqzXZgz_awvlfFEHU",
  "pdfSize": "621919"
}