{"id":"2407.15399","title":"Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned\n  Large Language Models","authors":"Xiao Liu, Liangzhi Li, Tong Xiang, Fuying Ye, Lu Wei, Wangyue Li, Noa\n  Garcia","authorsParsed":[["Liu","Xiao",""],["Li","Liangzhi",""],["Xiang","Tong",""],["Ye","Fuying",""],["Wei","Lu",""],["Li","Wangyue",""],["Garcia","Noa",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 06:04:29 GMT"}],"updateDate":"2024-07-23","timestamp":1721628269000,"abstract":"  With the development of large language models (LLMs) like ChatGPT, both their\nvast applications and potential vulnerabilities have come to the forefront.\nWhile developers have integrated multiple safety mechanisms to mitigate their\nmisuse, a risk remains, particularly when models encounter adversarial inputs.\nThis study unveils an attack mechanism that capitalizes on human conversation\nstrategies to extract harmful information from LLMs. We delineate three pivotal\nstrategies: (i) decomposing malicious questions into seemingly innocent\nsub-questions; (ii) rewriting overtly malicious questions into more covert,\nbenign-sounding ones; (iii) enhancing the harmfulness of responses by prompting\nmodels for illustrative examples. Unlike conventional methods that target\nexplicit malicious responses, our approach delves deeper into the nature of the\ninformation provided in responses. Through our experiments conducted on\nGPT-3.5-turbo, GPT-4, and Llama2, our method has demonstrated a marked efficacy\ncompared to conventional attack methods. In summary, this work introduces a\nnovel attack method that outperforms previous approaches, raising an important\nquestion: How to discern whether the ultimate intent in a dialogue is\nmalicious?\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"eMpGRoRCaa66O62dKMA3Y_sj-syKL3Exwcxz2JHxPic","pdfSize":"3228434"}