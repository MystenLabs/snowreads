{"id":"2412.20215","title":"IMSSA: Deploying modern state-space models on memristive in-memory\n  compute hardware","authors":"Sebastian Siegel, Ming-Jay Yang, and John-Paul Strachan","authorsParsed":[["Siegel","Sebastian",""],["Yang","Ming-Jay",""],["Strachan","John-Paul",""]],"versions":[{"version":"v1","created":"Sat, 28 Dec 2024 16:58:31 GMT"}],"updateDate":"2024-12-31","timestamp":1735405111000,"abstract":"  Processing long temporal sequences is a key challenge in deep learning. In\nrecent years, Transformers have become state-of-the-art for this task, but\nsuffer from excessive memory requirements due to the need to explicitly store\nthe sequences. To address this issue, structured state-space sequential (S4)\nmodels recently emerged, offering a fixed memory state while still enabling the\nprocessing of very long sequence contexts. The recurrent linear update of the\nstate in these models makes them highly efficient on modern graphics processing\nunits (GPU) by unrolling the recurrence into a convolution. However, this\napproach demands significant memory and massively parallel computation, which\nis only available on the latest GPUs. In this work, we aim to bring the power\nof S4 models to edge hardware by significantly reducing the size and\ncomputational demand of an S4D model through quantization-aware training, even\nachieving ternary weights for a simple real-world task. To this end, we extend\nconventional quantization-aware training to tailor it for analog in-memory\ncompute hardware. We then demonstrate the deployment of recurrent S4D kernels\non memrisitve crossbar arrays, enabling their computation in an in-memory\ncompute fashion. To our knowledge, this is the first implementation of S4\nkernels on in-memory compute hardware.\n","subjects":["Computer Science/Machine Learning","Computer Science/Hardware Architecture"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"tXhpn6Rb4vLk-izy1-_rlQw6qRLYrV0rmAf2oRD0On0","pdfSize":"396096"}