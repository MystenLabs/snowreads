{"id":"2412.01801","title":"SceneFactor: Factored Latent 3D Diffusion for Controllable 3D Scene\n  Generation","authors":"Alexey Bokhovkin, Quan Meng, Shubham Tulsiani, Angela Dai","authorsParsed":[["Bokhovkin","Alexey",""],["Meng","Quan",""],["Tulsiani","Shubham",""],["Dai","Angela",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 18:47:41 GMT"},{"version":"v2","created":"Tue, 3 Dec 2024 10:32:05 GMT"}],"updateDate":"2024-12-04","timestamp":1733165261000,"abstract":"  We present SceneFactor, a diffusion-based approach for large-scale 3D scene\ngeneration that enables controllable generation and effortless editing.\nSceneFactor enables text-guided 3D scene synthesis through our factored\ndiffusion formulation, leveraging latent semantic and geometric manifolds for\ngeneration of arbitrary-sized 3D scenes. While text input enables easy,\ncontrollable generation, text guidance remains imprecise for intuitive,\nlocalized editing and manipulation of the generated 3D scenes. Our factored\nsemantic diffusion generates a proxy semantic space composed of semantic 3D\nboxes that enables controllable editing of generated scenes by adding,\nremoving, changing the size of the semantic 3D proxy boxes that guides\nhigh-fidelity, consistent 3D geometric editing. Extensive experiments\ndemonstrate that our approach enables high-fidelity 3D scene synthesis with\neffective controllable editing through our factored diffusion approach.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"mnaGVL4ZZScl4kJMnxsIskrmopEVhbHJnGAuWVh-kzk","pdfSize":"19944771"}