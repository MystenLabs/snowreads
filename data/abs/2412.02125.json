{
  "id": "2412.02125",
  "title": "Optimizing Latent Goal by Learning from Trajectory Preference",
  "authors": "Guangyu Zhao, Kewei Lian, Haowei Lin, Haobo Fu, Qiang Fu, Shaofei Cai,\n  Zihao Wang, Yitao Liang",
  "authorsParsed": [
    [
      "Zhao",
      "Guangyu",
      ""
    ],
    [
      "Lian",
      "Kewei",
      ""
    ],
    [
      "Lin",
      "Haowei",
      ""
    ],
    [
      "Fu",
      "Haobo",
      ""
    ],
    [
      "Fu",
      "Qiang",
      ""
    ],
    [
      "Cai",
      "Shaofei",
      ""
    ],
    [
      "Wang",
      "Zihao",
      ""
    ],
    [
      "Liang",
      "Yitao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 03:27:48 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733196468000,
  "abstract": "  A glowing body of work has emerged focusing on instruction-following policies\nfor open-world agents, aiming to better align the agent's behavior with human\nintentions. However, the performance of these policies is highly susceptible to\nthe initial prompt, which leads to extra efforts in selecting the best\ninstructions. We propose a framework named Preference Goal Tuning (PGT). PGT\nallows an instruction following policy to interact with the environment to\ncollect several trajectories, which will be categorized into positive and\nnegative samples based on preference. Then we use preference learning to\nfine-tune the initial goal latent representation with the categorized\ntrajectories while keeping the policy backbone frozen. The experiment result\nshows that with minimal data and training, PGT achieves an average relative\nimprovement of 72.0% and 81.6% over 17 tasks in 2 different foundation policies\nrespectively, and outperforms the best human-selected instructions. Moreover,\nPGT surpasses full fine-tuning in the out-of-distribution (OOD) task-execution\nenvironments by 13.4%, indicating that our approach retains strong\ngeneralization capabilities. Since our approach stores a single latent\nrepresentation for each task independently, it can be viewed as an efficient\nmethod for continual learning, without the risk of catastrophic forgetting or\ntask interference. In short, PGT enhances the performance of agents across\nnearly all tasks in the Minecraft Skillforge benchmark and demonstrates\nrobustness to the execution environment.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "sJf_fojes1wBJ3Xhi0EuZlVmvnyb1RfD4d030p3C-_Y",
  "pdfSize": "1997046"
}