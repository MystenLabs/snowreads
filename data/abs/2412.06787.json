{"id":"2412.06787","title":"[MASK] is All You Need","authors":"Vincent Tao Hu, Bj\\\"orn Ommer","authorsParsed":[["Hu","Vincent Tao",""],["Ommer","Bj√∂rn",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 18:59:56 GMT"},{"version":"v2","created":"Tue, 10 Dec 2024 14:09:22 GMT"}],"updateDate":"2024-12-11","timestamp":1733770796000,"abstract":"  In generative models, two paradigms have gained attraction in various\napplications: next-set prediction-based Masked Generative Models and next-noise\nprediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this\nwork, we propose using discrete-state models to connect them and explore their\nscalability in the vision domain. First, we conduct a step-by-step analysis in\na unified design space across two types of models including\ntimestep-independence, noise schedule, temperature, guidance strength, etc in a\nscalable manner. Second, we re-cast typical discriminative tasks, e.g., image\nsegmentation, as an unmasking process from [MASK] tokens on a discrete-state\nmodel. This enables us to perform various sampling processes, including\nflexible conditional sampling by only training once to model the joint\ndistribution. All aforementioned explorations lead to our framework named\nDiscrete Interpolants, which enables us to achieve state-of-the-art or\ncompetitive performance compared to previous discrete-state based methods in\nvarious benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics.\nIn summary, by leveraging [MASK] in discrete-state models, we can bridge Masked\nGenerative and Non-autoregressive Diffusion models, as well as generative and\ndiscriminative tasks.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-bhhjW7PhDEXj9uhuyd3Pwa_uO2wtwefUCi1zYTVakQ","pdfSize":"34156281"}