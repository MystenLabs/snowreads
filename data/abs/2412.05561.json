{"id":"2412.05561","title":"Exploring the Use of LLMs for SQL Equivalence Checking","authors":"Rajat Singh, Srikanta Bedathur","authorsParsed":[["Singh","Rajat",""],["Bedathur","Srikanta",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 06:50:12 GMT"}],"updateDate":"2024-12-10","timestamp":1733554212000,"abstract":"  Equivalence checking of two SQL queries is an intractable problem encountered\nin diverse contexts ranging from grading student submissions in a DBMS course\nto debugging query rewriting rules in an optimizer, and many more. While a lot\nof progress has been made in recent years in developing practical solutions for\nthis problem, the existing methods can handle only a small subset of SQL, even\nfor bounded equivalence checking. They cannot support sophisticated SQL\nexpressions one encounters in practice. At the same time, large language models\n(LLMs) -- such as GPT-4 -- have emerged as power generators of SQL from natural\nlanguage specifications. This paper explores whether LLMs can also demonstrate\nthe ability to reason with SQL queries and help advance SQL equivalence\nchecking. Towards this, we conducted a detailed evaluation of several LLMs over\ncollections with SQL pairs of varying levels of complexity. We explored the\nefficacy of different prompting techniques, the utility of synthetic examples &\nexplanations, as well as logical plans generated by query parsers. Our main\nfinding is that with well-designed prompting using an unoptimized SQL Logical\nPlan, LLMs can perform equivalence checking beyond the capabilities of current\ntechniques, achieving nearly 100% accuracy for equivalent pairs and up to 70%\nfor non-equivalent pairs of SQL queries. While LLMs lack the ability to\ngenerate formal proofs, their synthetic examples and human-readable\nexplanations offer valuable insights to students (& instructors) in a classroom\nsetting and to database administrators (DBAs) managing large database\ninstallations. Additionally, we also show that with careful fine-tuning, we can\nclose the performance gap between smaller (and efficient) models and larger\nmodels such as GPT, thus paving the way for potential LLM-integration in\nstandalone data processing systems.\n","subjects":["Computer Science/Databases","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2ruhOMazaoy1UNGKudffesgB5ub5b96DGseN4rpWqf8","pdfSize":"1761477"}