{
  "id": "2412.20720",
  "title": "4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives",
  "authors": "Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang, Yu-Gang Jiang, Philip\n  H.S. Torr",
  "authorsParsed": [
    [
      "Yang",
      "Zeyu",
      ""
    ],
    [
      "Pan",
      "Zijie",
      ""
    ],
    [
      "Zhu",
      "Xiatian",
      ""
    ],
    [
      "Zhang",
      "Li",
      ""
    ],
    [
      "Jiang",
      "Yu-Gang",
      ""
    ],
    [
      "Torr",
      "Philip H. S.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 30 Dec 2024 05:30:26 GMT"
    }
  ],
  "updateDate": "2025-01-01",
  "timestamp": 1735536626000,
  "abstract": "  Dynamic 3D scene representation and novel view synthesis from captured videos\nare crucial for enabling immersive experiences required by AR/VR and metaverse\napplications. However, this task is challenging due to the complexity of\nunconstrained real-world scenes and their temporal dynamics. In this paper, we\nframe dynamic scenes as a spatio-temporal 4D volume learning problem, offering\na native explicit reformulation with minimal assumptions about motion, which\nserves as a versatile dynamic scene learning framework. Specifically, we\nrepresent a target dynamic scene using a collection of 4D Gaussian primitives\nwith explicit geometry and appearance features, dubbed as 4D Gaussian splatting\n(4DGS). This approach can capture relevant information in space and time by\nfitting the underlying spatio-temporal volume. Modeling the spacetime as a\nwhole with 4D Gaussians parameterized by anisotropic ellipses that can rotate\narbitrarily in space and time, our model can naturally learn view-dependent and\ntime-evolved appearance with 4D spherindrical harmonics. Notably, our 4DGS\nmodel is the first solution that supports real-time rendering of\nhigh-resolution, photorealistic novel views for complex dynamic scenes. To\nenhance efficiency, we derive several compact variants that effectively reduce\nmemory footprint and mitigate the risk of overfitting. Extensive experiments\nvalidate the superiority of 4DGS in terms of visual quality and efficiency\nacross a range of dynamic scene-related tasks (e.g., novel view synthesis, 4D\ngeneration, scene understanding) and scenarios (e.g., single object, indoor\nscenes, driving environments, synthetic and real data).\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "vQYdCV3rnpcVwNrRWpWFXQ04z-a1xRczmCxE9LzDKq8",
  "pdfSize": "32976013"
}