{
  "id": "2412.00319",
  "title": "Improving speaker verification robustness with synthetic emotional\n  utterances",
  "authors": "Nikhil Kumar Koditala, Chelsea Jui-Ting Ju, Ruirui Li, Minho Jin, Aman\n  Chadha, Andreas Stolcke",
  "authorsParsed": [
    [
      "Koditala",
      "Nikhil Kumar",
      ""
    ],
    [
      "Ju",
      "Chelsea Jui-Ting",
      ""
    ],
    [
      "Li",
      "Ruirui",
      ""
    ],
    [
      "Jin",
      "Minho",
      ""
    ],
    [
      "Chadha",
      "Aman",
      ""
    ],
    [
      "Stolcke",
      "Andreas",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 30 Nov 2024 02:18:26 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1732933106000,
  "abstract": "  A speaker verification (SV) system offers an authentication service designed\nto confirm whether a given speech sample originates from a specific speaker.\nThis technology has paved the way for various personalized applications that\ncater to individual preferences. A noteworthy challenge faced by SV systems is\ntheir ability to perform consistently across a range of emotional spectra. Most\nexisting models exhibit high error rates when dealing with emotional utterances\ncompared to neutral ones. Consequently, this phenomenon often leads to missing\nout on speech of interest. This issue primarily stems from the limited\navailability of labeled emotional speech data, impeding the development of\nrobust speaker representations that encompass diverse emotional states.\n  To address this concern, we propose a novel approach employing the CycleGAN\nframework to serve as a data augmentation method. This technique synthesizes\nemotional speech segments for each specific speaker while preserving the unique\nvocal identity. Our experimental findings underscore the effectiveness of\nincorporating synthetic emotional data into the training process. The models\ntrained using this augmented dataset consistently outperform the baseline\nmodels on the task of verifying speakers in emotional speech scenarios,\nreducing equal error rate by as much as 3.64% relative.\n",
  "subjects": [
    "Computer Science/Sound",
    "Computer Science/Artificial Intelligence",
    "Electrical Engineering and Systems Science/Audio and Speech Processing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "blyf7RcokpX-iHMhaZixOQLnLDATdl19la1CG-OtXYQ",
  "pdfSize": "455549"
}