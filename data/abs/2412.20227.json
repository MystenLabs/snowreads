{"id":"2412.20227","title":"LLM Reasoning Engine: Specialized Training for Enhanced Mathematical\n  Reasoning","authors":"Shuguang Chen and Guang Lin","authorsParsed":[["Chen","Shuguang",""],["Lin","Guang",""]],"versions":[{"version":"v1","created":"Sat, 28 Dec 2024 17:48:33 GMT"}],"updateDate":"2024-12-31","timestamp":1735408113000,"abstract":"  Large Language Models (LLMs) have shown remarkable performance in various\nnatural language processing tasks but face challenges in mathematical\nreasoning, where complex problem-solving requires both linguistic understanding\nand mathematical reasoning skills. Existing approaches to address this\nchallenge often rely on ensemble methods and suffer from the problem of data\nscarcity in target domains. In this work, we present a novel method to enhance\nLLMs' capabilities in mathematical reasoning tasks. Motivated by the need to\nbridge this gap, our approach incorporates a question paraphrase strategy,\nwhich aims at diversifying the linguistic forms of mathematical questions to\nimprove generalization. Additionally, specialized training objectives are\nemployed to guide the model's learning process, focusing on enhancing its\nunderstanding of mathematical concepts and reasoning processes. We conduct\nexperiments on four datasets using different LLMs, and demonstrate the\neffectiveness of our approach in improving LLMs' performance on mathematical\nreasoning tasks. Our findings underscore the significance of our methodology in\nthe advancement of large language models and its potential implications for\nreal-world applications that require mathematical reasoning abilities.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HLdqWngsPveBuHHJGlxde9jYPV1boGccjYK7harBZSE","pdfSize":"1778207"}