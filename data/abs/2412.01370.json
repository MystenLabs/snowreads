{"id":"2412.01370","title":"Understanding the World's Museums through Vision-Language Reasoning","authors":"Ada-Astrid Balauca, Sanjana Garai, Stefan Balauca, Rasesh Udayakumar\n  Shetty, Naitik Agrawal, Dhwanil Subhashbhai Shah, Yuqian Fu, Xi Wang,\n  Kristina Toutanova, Danda Pani Paudel, Luc Van Gool","authorsParsed":[["Balauca","Ada-Astrid",""],["Garai","Sanjana",""],["Balauca","Stefan",""],["Shetty","Rasesh Udayakumar",""],["Agrawal","Naitik",""],["Shah","Dhwanil Subhashbhai",""],["Fu","Yuqian",""],["Wang","Xi",""],["Toutanova","Kristina",""],["Paudel","Danda Pani",""],["Van Gool","Luc",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 10:54:31 GMT"}],"updateDate":"2024-12-03","timestamp":1733136871000,"abstract":"  Museums serve as vital repositories of cultural heritage and historical\nartifacts spanning diverse epochs, civilizations, and regions, preserving\nwell-documented collections. Data reveal key attributes such as age, origin,\nmaterial, and cultural significance. Understanding museum exhibits from their\nimages requires reasoning beyond visual features. In this work, we facilitate\nsuch reasoning by (a) collecting and curating a large-scale dataset of 65M\nimages and 200M question-answer pairs in the standard museum catalog format for\nexhibits from all around the world; (b) training large vision-language models\non the collected dataset; (c) benchmarking their ability on five visual\nquestion answering tasks. The complete dataset is labeled by museum experts,\nensuring the quality as well as the practical significance of the labels. We\ntrain two VLMs from different categories: the BLIP model, with vision-language\naligned embeddings, but lacking the expressive power of large language models,\nand the LLaVA model, a powerful instruction-tuned LLM enriched with\nvision-language reasoning capabilities. Through exhaustive experiments, we\nprovide several insights on the complex and fine-grained understanding of\nmuseum exhibits. In particular, we show that some questions whose answers can\noften be derived directly from visual features are well answered by both types\nof models. On the other hand, questions that require the grounding of the\nvisual features in repositories of human knowledge are better answered by the\nlarge vision-language models, thus demonstrating their superior capacity to\nperform the desired reasoning. Find our dataset, benchmarks, and source code\nat: https://github.com/insait-institute/Museum-65\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HR4S5h4vB-hFmDvA0htap212uhvV9C31sh8ZSoXfueI","pdfSize":"14617223"}