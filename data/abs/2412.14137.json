{"id":"2412.14137","title":"Design choices made by LLM-based test generators prevent them from\n  finding bugs","authors":"Noble Saji Mathews and Meiyappan Nagappan","authorsParsed":[["Mathews","Noble Saji",""],["Nagappan","Meiyappan",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 18:33:26 GMT"}],"updateDate":"2024-12-19","timestamp":1734546806000,"abstract":"  There is an increasing amount of research and commercial tools for automated\ntest case generation using Large Language Models (LLMs). This paper critically\nexamines whether recent LLM-based test generation tools, such as Codium\nCoverAgent and CoverUp, can effectively find bugs or unintentionally validate\nfaulty code. Considering bugs are only exposed by failing test cases, we\nexplore the question: can these tools truly achieve the intended objectives of\nsoftware testing when their test oracles are designed to pass? Using real\nhuman-written buggy code as input, we evaluate these tools, showing how\nLLM-generated tests can fail to detect bugs and, more alarmingly, how their\ndesign can worsen the situation by validating bugs in the generated test suite\nand rejecting bug-revealing tests. These findings raise important questions\nabout the validity of the design behind LLM-based test generation tools and\ntheir impact on software quality and test suite reliability.\n","subjects":["Computer Science/Software Engineering","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"x3GlJXxhoy1fgP-aEhwFKTTWKVjle7gkHgDP0yjHh3o","pdfSize":"691437"}