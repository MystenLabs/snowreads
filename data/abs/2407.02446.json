{"id":"2407.02446","title":"Predicting vs. Acting: A Trade-off Between World Modeling & Agent\n  Modeling","authors":"Margaret Li, Weijia Shi, Artidoro Pagnoni, Peter West, Ari Holtzman","authorsParsed":[["Li","Margaret",""],["Shi","Weijia",""],["Pagnoni","Artidoro",""],["West","Peter",""],["Holtzman","Ari",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:22:54 GMT"}],"updateDate":"2024-07-03","timestamp":1719940974000,"abstract":"  RLHF-aligned LMs have shown unprecedented ability on both benchmarks and\nlong-form text generation, yet they struggle with one foundational task:\nnext-token prediction. As RLHF models become agent models aimed at interacting\nwith humans, they seem to lose their world modeling -- the ability to predict\nwhat comes next in arbitrary documents, which is the foundational training\nobjective of the Base LMs that RLHF adapts.\n  Besides empirically demonstrating this trade-off, we propose a potential\nexplanation: to perform coherent long-form generation, RLHF models restrict\nrandomness via implicit blueprints. In particular, RLHF models concentrate\nprobability on sets of anchor spans that co-occur across multiple generations\nfor the same prompt, serving as textual scaffolding but also limiting a model's\nability to generate documents that do not include these spans. We study this\ntrade-off on the most effective current agent models, those aligned with RLHF,\nwhile exploring why this may remain a fundamental trade-off between models that\nact and those that predict, even as alignment techniques improve.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"I2pzlNc5U6D38lYWvU2UTSKw4kBDbyr7yI8xwj8Z4Y0","pdfSize":"3084945","objectId":"0xa539710f9a2516fa28e3bf9497f7c04846d6781f3c287a6426510ba92be5a0c3","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
