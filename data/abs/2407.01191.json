{"id":"2407.01191","title":"MARS: Multimodal Active Robotic Sensing for Articulated Characterization","authors":"Hongliang Zeng, Ping Zhang, Chengjiong Wu, Jiahua Wang, Tingyu Ye and\n  Fang Li","authorsParsed":[["Zeng","Hongliang",""],["Zhang","Ping",""],["Wu","Chengjiong",""],["Wang","Jiahua",""],["Ye","Tingyu",""],["Li","Fang",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 11:32:39 GMT"}],"updateDate":"2024-07-02","timestamp":1719833559000,"abstract":"  Precise perception of articulated objects is vital for empowering service\nrobots. Recent studies mainly focus on point cloud, a single-modal approach,\noften neglecting vital texture and lighting details and assuming ideal\nconditions like optimal viewpoints, unrepresentative of real-world scenarios.\nTo address these limitations, we introduce MARS, a novel framework for\narticulated object characterization. It features a multi-modal fusion module\nutilizing multi-scale RGB features to enhance point cloud features, coupled\nwith reinforcement learning-based active sensing for autonomous optimization of\nobservation viewpoints. In experiments conducted with various articulated\nobject instances from the PartNet-Mobility dataset, our method outperformed\ncurrent state-of-the-art methods in joint parameter estimation accuracy.\nAdditionally, through active sensing, MARS further reduces errors,\ndemonstrating enhanced efficiency in handling suboptimal viewpoints.\nFurthermore, our method effectively generalizes to real-world articulated\nobjects, enhancing robot interactions. Code is available at\nhttps://github.com/robhlzeng/MARS.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"xA2DHzz7MUtDQwQIstkGomDzMUvrVRBB3Nqmwu8dZHg","pdfSize":"5653235"}