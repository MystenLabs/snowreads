{"id":"2407.04622","title":"On scalable oversight with weak LLMs judging strong LLMs","authors":"Zachary Kenton, Noah Y. Siegel, J\\'anos Kram\\'ar, Jonah Brown-Cohen,\n  Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang,\n  Noah D. Goodman, Rohin Shah","authorsParsed":[["Kenton","Zachary",""],["Siegel","Noah Y.",""],["Kramár","János",""],["Brown-Cohen","Jonah",""],["Albanie","Samuel",""],["Bulian","Jannis",""],["Agarwal","Rishabh",""],["Lindner","David",""],["Tang","Yunhao",""],["Goodman","Noah D.",""],["Shah","Rohin",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 16:29:15 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 16:38:12 GMT"}],"updateDate":"2024-07-15","timestamp":1720196955000,"abstract":"  Scalable oversight protocols aim to enable humans to accurately supervise\nsuperhuman AI. In this paper we study debate, where two AI's compete to\nconvince a judge; consultancy, where a single AI tries to convince a judge that\nasks questions; and compare to a baseline of direct question-answering, where\nthe judge just answers outright without the AI. We use large language models\n(LLMs) as both AI agents and as stand-ins for human judges, taking the judge\nmodels to be weaker than agent models. We benchmark on a diverse range of\nasymmetries between judges and agents, extending previous work on a single\nextractive QA task with information asymmetry, to also include mathematics,\ncoding, logic and multimodal reasoning asymmetries. We find that debate\noutperforms consultancy across all tasks when the consultant is randomly\nassigned to argue for the correct/incorrect answer. Comparing debate to direct\nquestion answering, the results depend on the type of task: in extractive QA\ntasks with information asymmetry debate outperforms direct question answering,\nbut in other tasks without information asymmetry the results are mixed.\nPrevious work assigned debaters/consultants an answer to argue for. When we\nallow them to instead choose which answer to argue for, we find judges are less\nfrequently convinced by the wrong answer in debate than in consultancy.\nFurther, we find that stronger debater models increase judge accuracy, though\nmore modestly than in previous studies.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"c2-l0Bti0KcHOe-1ttdAipxPCD4TSiEZVKo-YCmxjmE","pdfSize":"4084703"}