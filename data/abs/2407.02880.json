{"id":"2407.02880","title":"Knowledge Composition using Task Vectors with Learned Anisotropic\n  Scaling","authors":"Frederic Z. Zhang, Paul Albert, Cristian Rodriguez-Opazo, Anton van\n  den Hengel and Ehsan Abbasnejad","authorsParsed":[["Zhang","Frederic Z.",""],["Albert","Paul",""],["Rodriguez-Opazo","Cristian",""],["Hengel","Anton van den",""],["Abbasnejad","Ehsan",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 07:54:08 GMT"}],"updateDate":"2024-07-04","timestamp":1719993248000,"abstract":"  Pre-trained models produce strong generic representations that can be adapted\nvia fine-tuning. The learned weight difference relative to the pre-trained\nmodel, known as a task vector, characterises the direction and stride of\nfine-tuning. The significance of task vectors is such that simple arithmetic\noperations on them can be used to combine diverse representations from\ndifferent domains. This paper builds on these properties of task vectors and\naims to answer (1) whether components of task vectors, particularly parameter\nblocks, exhibit similar characteristics, and (2) how such blocks can be used to\nenhance knowledge composition and transfer. To this end, we introduce aTLAS, an\nalgorithm that linearly combines parameter blocks with different learned\ncoefficients, resulting in anisotropic scaling at the task vector level. We\nshow that such linear combinations explicitly exploit the low intrinsic\ndimensionality of pre-trained models, with only a few coefficients being the\nlearnable parameters. Furthermore, composition of parameter blocks leverages\nthe already learned representations, thereby reducing the dependency on large\namounts of data. We demonstrate the effectiveness of our method in task\narithmetic, few-shot recognition and test-time adaptation, with supervised or\nunsupervised objectives. In particular, we show that (1) learned anisotropic\nscaling allows task vectors to be more disentangled, causing less interference\nin composition; (2) task vector composition excels with scarce or no labeled\ndata and is less prone to domain shift, thus leading to better\ngeneralisability; (3) mixing the most informative parameter blocks across\ndifferent task vectors prior to training can reduce the memory footprint and\nimprove the flexibility of knowledge transfer. Moreover, we show the potential\nof aTLAS as a PEFT method, particularly with less data, and demonstrate that\nits scalibility.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"e9K8OwY7l_kGzcbzKPmpoedLvR66QTpAcZSKLv--dGY","pdfSize":"5159329"}