{
  "id": "2412.04914",
  "title": "Achieving Group Fairness through Independence in Predictive Process\n  Monitoring",
  "authors": "Jari Peeperkorn and Simon De Vos",
  "authorsParsed": [
    [
      "Peeperkorn",
      "Jari",
      ""
    ],
    [
      "De Vos",
      "Simon",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 6 Dec 2024 10:10:47 GMT"
    }
  ],
  "updateDate": "2024-12-09",
  "timestamp": 1733479847000,
  "abstract": "  Predictive process monitoring focuses on forecasting future states of ongoing\nprocess executions, such as predicting the outcome of a particular case. In\nrecent years, the application of machine learning models in this domain has\ngarnered significant scientific attention. When using historical execution\ndata, which may contain biases or exhibit unfair behavior, these biases may be\nencoded into the trained models. Consequently, when such models are deployed to\nmake decisions or guide interventions for new cases, they risk perpetuating\nthis unwanted behavior. This work addresses group fairness in predictive\nprocess monitoring by investigating independence, i.e. ensuring predictions are\nunaffected by sensitive group membership. We explore independence through\nmetrics for demographic parity such as $\\Delta$DP, as well as recently\nintroduced, threshold-independent distribution-based alternatives.\nAdditionally, we propose a composite loss functions existing of binary\ncross-entropy and a distribution-based loss (Wasserstein) to train models that\nbalance predictive performance and fairness, and allow for customizable\ntrade-offs. The effectiveness of both the fairness metrics and the composite\nloss functions is validated through a controlled experimental setup.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "haDDnI5EOPa57U3WCd03U2cjHqt268Rw3ntemWLuN5I",
  "pdfSize": "806140"
}