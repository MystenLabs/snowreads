{"id":"2412.17264","title":"ACECode: A Reinforcement Learning Framework for Aligning Code Efficiency\n  and Correctness in Code Language Models","authors":"Chengran Yang, Hong Jin Kang, Jieke Shi, David Lo","authorsParsed":[["Yang","Chengran",""],["Kang","Hong Jin",""],["Shi","Jieke",""],["Lo","David",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 04:19:45 GMT"}],"updateDate":"2024-12-24","timestamp":1734927585000,"abstract":"  CodeLLMs have demonstrated remarkable advancements in software engineering\ntasks. However, while these models can generate functionally correct code, they\noften produce code that is inefficient in terms of runtime. This inefficiency\nis particularly problematic in resource-constrained environments, impacting\nsoftware performance and sustainability.\n  Existing approaches for optimizing code efficiency for CodeLLMs like SOAP and\nPIE exhibit certain limitations. SOAP requires a compatible execution\nenvironment and predefined test cases for iterative code modification, while\nPIE focuses on instruction tuning, improving efficiency but compromising\ncorrectness. These shortcomings highlight the need for a fine-tuning framework\nthat optimizes both efficiency and correctness without relying on predefined\ntest cases or specific execution environments.\n  To bridge this gap, we introduce ACECode, a reinforcement learning-based\nfine-tuning framework that aligns CodeLLMs with dual objectives of efficiency\nand correctness. ACECode combines three key steps: (1) generating code with an\nactor CodeLLM, (2) calculating a training-free reward signal derived from code\nexecution feedback for each generated code, and (3) optimizing the CodeLLM via\nProximal Policy Optimization (PPO) algorithm. This reward signal enables joint\nassessment of efficiency and correctness without manual labeling.\n  We evaluate ACECode by fine-tuning four SOTA (state-of-the-art) CodeLLMs and\ncomparing their code with three baselines: original, instruction-tuned, and\nPIE-tuned CodeLLMs. Extensive experiment results suggest that \\tool{}\nsignificantly improves the efficiency and correctness of generated code against\nall baselines for all CodeLLMs. Specifically, CodeLLMs fine-tuned with ACECode\nimprove pass@1 by 1.84% to 14.51% and reduce runtime in 65% to 72% of cases\ncompared to original CodeLLMs.\n","subjects":["Computer Science/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Flf-aNhesbNB0yTvyUuZ6r_yK2quEME-hRIkdrSl9Wk","pdfSize":"964292"}