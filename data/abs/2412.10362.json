{"id":"2412.10362","title":"OP-LoRA: The Blessing of Dimensionality","authors":"Piotr Teterwak, Kate Saenko, Bryan A. Plummer, Ser-Nam Lim","authorsParsed":[["Teterwak","Piotr",""],["Saenko","Kate",""],["Plummer","Bryan A.",""],["Lim","Ser-Nam",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 18:55:19 GMT"}],"updateDate":"2024-12-16","timestamp":1734116119000,"abstract":"  Low-rank adapters enable fine-tuning of large models with only a small number\nof parameters, thus reducing storage costs and minimizing the risk of\ncatastrophic forgetting. However, they often pose optimization challenges, with\npoor convergence. To overcome these challenges, we introduce an\nover-parameterized approach that accelerates training without increasing\ninference costs. This method reparameterizes low-rank adaptation by employing a\nseparate MLP and learned embedding for each layer. The learned embedding is\ninput to the MLP, which generates the adapter parameters. Such\noverparamaterization has been shown to implicitly function as an adaptive\nlearning rate and momentum, accelerating optimization. At inference time, the\nMLP can be discarded, leaving behind a standard low-rank adapter. To study the\neffect of MLP overparameterization on a small yet difficult proxy task, we\nimplement it for matrix factorization, and find it achieves faster convergence\nand lower final loss. Extending this approach to larger-scale tasks, we observe\nconsistent performance gains across domains. We achieve improvements in\nvision-language tasks and especially notable increases in image generation,\nwith CMMD scores improving by up to 15 points.\n","subjects":["Computer Science/Machine Learning","Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"lTzGUptGi-zPQvjEb1C_axrDo3ZBMcxfxjImpXlreto","pdfSize":"29040003"}