{
  "id": "2412.07772",
  "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
  "authors": "Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo\n  Durand, Eli Shechtman, Xun Huang",
  "authorsParsed": [
    [
      "Yin",
      "Tianwei",
      ""
    ],
    [
      "Zhang",
      "Qiang",
      ""
    ],
    [
      "Zhang",
      "Richard",
      ""
    ],
    [
      "Freeman",
      "William T.",
      ""
    ],
    [
      "Durand",
      "Fredo",
      ""
    ],
    [
      "Shechtman",
      "Eli",
      ""
    ],
    [
      "Huang",
      "Xun",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 18:59:50 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 6 Jan 2025 01:26:42 GMT"
    }
  ],
  "updateDate": "2025-01-07",
  "timestamp": 1733857190000,
  "abstract": "  Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "CS1OH-YwGq_DGf1eJdLdc9urnrZJGMoxnLclnY4Fs7o",
  "pdfSize": "13364528"
}