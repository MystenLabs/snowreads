{
  "id": "2412.18826",
  "title": "RapGuard: Safeguarding Multimodal Large Language Models via\n  Rationale-aware Defensive Prompting",
  "authors": "Yilei Jiang, Yingshui Tan, Xiangyu Yue",
  "authorsParsed": [
    [
      "Jiang",
      "Yilei",
      ""
    ],
    [
      "Tan",
      "Yingshui",
      ""
    ],
    [
      "Yue",
      "Xiangyu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 25 Dec 2024 08:31:53 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735115513000,
  "abstract": "  While Multimodal Large Language Models (MLLMs) have made remarkable progress\nin vision-language reasoning, they are also more susceptible to producing\nharmful content compared to models that focus solely on text. Existing\ndefensive prompting techniques rely on a static, unified safety guideline that\nfails to account for the specific risks inherent in different multimodal\ncontexts. To address these limitations, we propose RapGuard, a novel framework\nthat uses multimodal chain-of-thought reasoning to dynamically generate\nscenario-specific safety prompts. RapGuard enhances safety by adapting its\nprompts to the unique risks of each input, effectively mitigating harmful\noutputs while maintaining high performance on benign tasks. Our experimental\nresults across multiple MLLM benchmarks demonstrate that RapGuard achieves\nstate-of-the-art safety performance, significantly reducing harmful content\nwithout degrading the quality of responses.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "8erjzlRayeqTji9BbYMSwggr8OIPdYOqYFJDAvUyVTs",
  "pdfSize": "1374218"
}