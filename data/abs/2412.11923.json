{"id":"2412.11923","title":"PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection","authors":"Sepideh Mamooler, Syrielle Montariol, Alexander Mathis, Antoine\n  Bosselut","authorsParsed":[["Mamooler","Sepideh",""],["Montariol","Syrielle",""],["Mathis","Alexander",""],["Bosselut","Antoine",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 16:09:35 GMT"}],"updateDate":"2024-12-17","timestamp":1734365375000,"abstract":"  In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"h3WNLNe4Gzk5fhbKVoCxTmQy3WGBseDGK0Iu58DiFvc","pdfSize":"1043366"}