{"id":"2407.14843","title":"A Tale of Two Scales: Reconciling Horizontal and Vertical Scaling for\n  Inference Serving Systems","authors":"Kamran Razavi, Mehran Salmani, Max M\\\"uhlh\\\"auser, Boris Koldehofe,\n  Lin Wang","authorsParsed":[["Razavi","Kamran",""],["Salmani","Mehran",""],["Mühlhäuser","Max",""],["Koldehofe","Boris",""],["Wang","Lin",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 11:16:59 GMT"}],"updateDate":"2024-07-23","timestamp":1721474219000,"abstract":"  Inference serving is of great importance in deploying machine learning models\nin real-world applications, ensuring efficient processing and quick responses\nto inference requests. However, managing resources in these systems poses\nsignificant challenges, particularly in maintaining performance under varying\nand unpredictable workloads. Two primary scaling strategies, horizontal and\nvertical scaling, offer different advantages and limitations. Horizontal\nscaling adds more instances to handle increased loads but can suffer from cold\nstart issues and increased management complexity. Vertical scaling boosts the\ncapacity of existing instances, allowing for quicker responses but is limited\nby hardware and model parallelization capabilities.\n  This paper introduces Themis, a system designed to leverage the benefits of\nboth horizontal and vertical scaling in inference serving systems. Themis\nemploys a two-stage autoscaling strategy: initially using in-place vertical\nscaling to handle workload surges and then switching to horizontal scaling to\noptimize resource efficiency once the workload stabilizes. The system profiles\nthe processing latency of deep learning models, calculates queuing delays, and\nemploys different dynamic programming algorithms to solve the joint horizontal\nand vertical scaling problem optimally based on the workload situation.\nExtensive evaluations with real-world workload traces demonstrate over\n$10\\times$ SLO violation reduction compared to the state-of-the-art horizontal\nor vertical autoscaling approaches while maintaining resource efficiency when\nthe workload is stable.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zaneVDfs6dAUhTp7CVkWlywltS4DYsnNEW8QwR9Z4Uo","pdfSize":"971824"}