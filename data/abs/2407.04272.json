{"id":"2407.04272","title":"Accelerating Communication in Deep Learning Recommendation Model\n  Training with Dual-Level Adaptive Lossy Compression","authors":"Hao Feng, Boyuan Zhang, Fanjiang Ye, Min Si, Ching-Hsiang Chu, Jiannan\n  Tian, Chunxing Yin, Summer Deng, Yuchen Hao, Pavan Balaji, Tong Geng, Dingwen\n  Tao","authorsParsed":[["Feng","Hao",""],["Zhang","Boyuan",""],["Ye","Fanjiang",""],["Si","Min",""],["Chu","Ching-Hsiang",""],["Tian","Jiannan",""],["Yin","Chunxing",""],["Deng","Summer",""],["Hao","Yuchen",""],["Balaji","Pavan",""],["Geng","Tong",""],["Tao","Dingwen",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 05:55:18 GMT"},{"version":"v2","created":"Mon, 8 Jul 2024 05:53:10 GMT"},{"version":"v3","created":"Thu, 11 Jul 2024 15:31:53 GMT"},{"version":"v4","created":"Sun, 25 Aug 2024 06:47:44 GMT"}],"updateDate":"2024-08-27","timestamp":1720158918000,"abstract":"  DLRM is a state-of-the-art recommendation system model that has gained\nwidespread adoption across various industry applications. The large size of\nDLRM models, however, necessitates the use of multiple devices/GPUs for\nefficient training. A significant bottleneck in this process is the\ntime-consuming all-to-all communication required to collect embedding data from\nall devices. To mitigate this, we introduce a method that employs error-bounded\nlossy compression to reduce the communication data size and accelerate DLRM\ntraining. We develop a novel error-bounded lossy compression algorithm,\ninformed by an in-depth analysis of embedding data features, to achieve high\ncompression ratios. Moreover, we introduce a dual-level adaptive strategy for\nerror-bound adjustment, spanning both table-wise and iteration-wise aspects, to\nbalance the compression benefits with the potential impacts on accuracy. We\nfurther optimize our compressor for PyTorch tensors on GPUs, minimizing\ncompression overhead. Evaluation shows that our method achieves a 1.38$\\times$\ntraining speedup with a minimal accuracy impact.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Ctb4vIKxSzbPQeYIVVfcx8WQhgxA94b-GMEMf-wmiII","pdfSize":"2441468"}