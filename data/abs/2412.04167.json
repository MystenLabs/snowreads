{"id":"2412.04167","title":"Bench-CoE: a Framework for Collaboration of Experts from Benchmark","authors":"Yuanshuai Wang, Xingjian Zhang, Jinkun Zhao, Siwei Wen, Peilin Feng,\n  Shuhao Liao, Lei Huang, Wenjun Wu","authorsParsed":[["Wang","Yuanshuai",""],["Zhang","Xingjian",""],["Zhao","Jinkun",""],["Wen","Siwei",""],["Feng","Peilin",""],["Liao","Shuhao",""],["Huang","Lei",""],["Wu","Wenjun",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 14:03:41 GMT"}],"updateDate":"2024-12-06","timestamp":1733407421000,"abstract":"  Large Language Models (LLMs) are key technologies driving intelligent systems\nto handle multiple tasks. To meet the demands of various tasks, an increasing\nnumber of LLMs-driven experts with diverse capabilities have been developed,\naccompanied by corresponding benchmarks to evaluate their performance. This\npaper proposes the Bench-CoE framework, which enables Collaboration of Experts\n(CoE) by effectively leveraging benchmark evaluations to achieve optimal\nperformance across various tasks. Bench-CoE includes a set of expert models, a\nrouter for assigning tasks to corresponding experts, and a benchmark dataset\nfor training the router. Moreover, we formulate Query-Level and Subject-Level\napproaches based on our framework, and analyze the merits and drawbacks of\nthese two approaches. Finally, we conduct a series of experiments with vary\ndata distributions on both language and multimodal tasks to validate that our\nproposed Bench-CoE outperforms any single model in terms of overall\nperformance. We hope this method serves as a baseline for further research in\nthis area. The code is available at\n\\url{https://github.com/ZhangXJ199/Bench-CoE}.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"E1tbcBxeRXE9XCjqPn5JDDgv4guqnEM-QJwv52xzP_s","pdfSize":"1267893"}