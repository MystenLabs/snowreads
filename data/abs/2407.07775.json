{"id":"2407.07775","title":"Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs\n  and Topological Graphs","authors":"Hao-Tien Lewis Chiang, Zhuo Xu, Zipeng Fu, Mithun George Jacob,\n  Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David\n  Rendleman, Dhruv Shah, Fei Xia, Jasmine Hsu, Jonathan Hoech, Pete Florence,\n  Sean Kirmani, Sumeet Singh, Vikas Sindhwani, Carolina Parada, Chelsea Finn,\n  Peng Xu, Sergey Levine, Jie Tan","authorsParsed":[["Chiang","Hao-Tien Lewis",""],["Xu","Zhuo",""],["Fu","Zipeng",""],["Jacob","Mithun George",""],["Zhang","Tingnan",""],["Lee","Tsang-Wei Edward",""],["Yu","Wenhao",""],["Schenck","Connor",""],["Rendleman","David",""],["Shah","Dhruv",""],["Xia","Fei",""],["Hsu","Jasmine",""],["Hoech","Jonathan",""],["Florence","Pete",""],["Kirmani","Sean",""],["Singh","Sumeet",""],["Sindhwani","Vikas",""],["Parada","Carolina",""],["Finn","Chelsea",""],["Xu","Peng",""],["Levine","Sergey",""],["Tan","Jie",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 15:49:07 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 14:37:08 GMT"}],"updateDate":"2024-07-15","timestamp":1720626547000,"abstract":"  An elusive goal in navigation research is to build an intelligent agent that\ncan understand multimodal instructions including natural language and image,\nand perform useful navigation. To achieve this, we study a widely useful\ncategory of navigation tasks we call Multimodal Instruction Navigation with\ndemonstration Tours (MINT), in which the environment prior is provided through\na previously recorded demonstration video. Recent advances in Vision Language\nModels (VLMs) have shown a promising path in achieving this goal as it\ndemonstrates capabilities in perceiving and reasoning about multimodal inputs.\nHowever, VLMs are typically trained to predict textual output and it is an open\nresearch question about how to best utilize them in navigation. To solve MINT,\nwe present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation\npolicy that combines the environment understanding and common sense reasoning\npower of long-context VLMs and a robust low-level navigation policy based on\ntopological graphs. The high-level policy consists of a long-context VLM that\ntakes the demonstration tour video and the multimodal user instruction as input\nto find the goal frame in the tour video. Next, a low-level policy uses the\ngoal frame and an offline constructed topological graph to generate robot\nactions at every timestep. We evaluated Mobility VLA in a 836m^2 real world\nenvironment and show that Mobility VLA has a high end-to-end success rates on\npreviously unsolved multimodal instructions such as \"Where should I return\nthis?\" while holding a plastic bin. A video demonstrating Mobility VLA can be\nfound here: https://youtu.be/-Tof__Q8_5s\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"obUZg7DIf9N3pdyRZVw68nNjr64S5lgp0BCzHPSj7vE","pdfSize":"22456079"}