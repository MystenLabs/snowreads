{
  "id": "2412.02140",
  "title": "SparseGrasp: Robotic Grasping via 3D Semantic Gaussian Splatting from\n  Sparse Multi-View RGB Images",
  "authors": "Junqiu Yu, Xinlin Ren, Yongchong Gu, Haitao Lin, Tianyu Wang, Yi Zhu,\n  Hang Xu, Yu-Gang Jiang, Xiangyang Xue, Yanwei Fu",
  "authorsParsed": [
    [
      "Yu",
      "Junqiu",
      ""
    ],
    [
      "Ren",
      "Xinlin",
      ""
    ],
    [
      "Gu",
      "Yongchong",
      ""
    ],
    [
      "Lin",
      "Haitao",
      ""
    ],
    [
      "Wang",
      "Tianyu",
      ""
    ],
    [
      "Zhu",
      "Yi",
      ""
    ],
    [
      "Xu",
      "Hang",
      ""
    ],
    [
      "Jiang",
      "Yu-Gang",
      ""
    ],
    [
      "Xue",
      "Xiangyang",
      ""
    ],
    [
      "Fu",
      "Yanwei",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 03:56:01 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733198161000,
  "abstract": "  Language-guided robotic grasping is a rapidly advancing field where robots\nare instructed using human language to grasp specific objects. However,\nexisting methods often depend on dense camera views and struggle to quickly\nupdate scenes, limiting their effectiveness in changeable environments.\n  In contrast, we propose SparseGrasp, a novel open-vocabulary robotic grasping\nsystem that operates efficiently with sparse-view RGB images and handles scene\nupdates fastly. Our system builds upon and significantly enhances existing\ncomputer vision modules in robotic learning. Specifically, SparseGrasp utilizes\nDUSt3R to generate a dense point cloud as the initialization for 3D Gaussian\nSplatting (3DGS), maintaining high fidelity even under sparse supervision.\nImportantly, SparseGrasp incorporates semantic awareness from recent vision\nfoundation models. To further improve processing efficiency, we repurpose\nPrincipal Component Analysis (PCA) to compress features from 2D models.\nAdditionally, we introduce a novel render-and-compare strategy that ensures\nrapid scene updates, enabling multi-turn grasping in changeable environments.\n  Experimental results show that SparseGrasp significantly outperforms\nstate-of-the-art methods in terms of both speed and adaptability, providing a\nrobust solution for multi-turn grasping in changeable environment.\n",
  "subjects": [
    "Computer Science/Robotics",
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "cc0KnO9WhzhwwthnHhs2MoO2Ng6tTXTI1KuEDP7t4LM",
  "pdfSize": "8360531"
}