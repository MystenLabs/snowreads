{"id":"2412.14226","title":"FedSTaS: Client Stratification and Client Level Sampling for Efficient\n  Federated Learning","authors":"Jordan Slessor, Dezheng Kong, Xiaofen Tang, Zheng En Than, Linglong\n  Kong","authorsParsed":[["Slessor","Jordan",""],["Kong","Dezheng",""],["Tang","Xiaofen",""],["Than","Zheng En",""],["Kong","Linglong",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 16:31:34 GMT"},{"version":"v2","created":"Sun, 29 Dec 2024 19:15:59 GMT"}],"updateDate":"2024-12-31","timestamp":1734539494000,"abstract":"  Federated learning (FL) is a machine learning methodology that involves the\ncollaborative training of a global model across multiple decentralized clients\nin a privacy-preserving way. Several FL methods are introduced to tackle\ncommunication inefficiencies but do not address how to sample participating\nclients in each round effectively and in a privacy-preserving manner. In this\npaper, we propose \\textit{FedSTaS}, a client and data-level sampling method\ninspired by \\textit{FedSTS} and \\textit{FedSampling}. In each federated\nlearning round, \\textit{FedSTaS} stratifies clients based on their compressed\ngradients, re-allocate the number of clients to sample using an optimal Neyman\nallocation, and sample local data from each participating clients using a data\nuniform sampling strategy. Experiments on three datasets show that\n\\textit{FedSTaS} can achieve higher accuracy scores than those of\n\\textit{FedSTS} within a fixed number of training rounds.\n","subjects":["Computer Science/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ksabW5g4i74aOvR0udS0wPztbIHj3BULVUh72K6poa0","pdfSize":"749795"}