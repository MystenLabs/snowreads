{
  "id": "2412.02612",
  "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken\n  Chatbot",
  "authors": "Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang,\n  Lei Zhao, Yuxiao Dong, Jie Tang",
  "authorsParsed": [
    [
      "Zeng",
      "Aohan",
      ""
    ],
    [
      "Du",
      "Zhengxiao",
      ""
    ],
    [
      "Liu",
      "Mingdao",
      ""
    ],
    [
      "Wang",
      "Kedong",
      ""
    ],
    [
      "Jiang",
      "Shengmin",
      ""
    ],
    [
      "Zhao",
      "Lei",
      ""
    ],
    [
      "Dong",
      "Yuxiao",
      ""
    ],
    [
      "Tang",
      "Jie",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 17:41:24 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733247684000,
  "abstract": "  We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken\nchatbot. It supports both Chinese and English, engages in real-time voice\nconversations, and varies vocal nuances such as emotion, intonation, speech\nrate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low\nbitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate\nderived from an automatic speech recognition (ASR) model by incorporating a\nvector-quantized bottleneck into the encoder. To efficiently transfer knowledge\nfrom text to speech modalities, we synthesize speech-text interleaved data from\nexisting text pre-training corpora using a text-to-token model. We continue\npre-training from the pre-trained text language model GLM-4-9B with a\ncombination of unsupervised speech data, interleaved speech-text data, and\nsupervised speech-text data, scaling up to 1 trillion tokens, achieving\nstate-of-the-art performance in both speech language modeling and spoken\nquestion answering. We then fine-tune the pre-trained model with high-quality\nconversational speech data, achieving superior performance compared to existing\nbaselines in both conversational ability and speech quality. The open models\ncan be accessed through https://github.com/THUDM/GLM-4-Voice and\nhttps://huggingface.co/THUDM/glm-4-voice-9b.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Sound",
    "Electrical Engineering and Systems Science/Audio and Speech Processing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "EW8a-SbaCnu-ts9utaQtktiWsZabk9kFUD4INpNjaYA",
  "pdfSize": "605922"
}