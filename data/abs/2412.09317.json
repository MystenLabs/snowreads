{"id":"2412.09317","title":"Multimodal Sentiment Analysis based on Video and Audio Inputs","authors":"Antonio Fernandez, Suzan Awinat","authorsParsed":[["Fernandez","Antonio",""],["Awinat","Suzan",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 14:42:10 GMT"}],"updateDate":"2024-12-13","timestamp":1734014530000,"abstract":"  Despite the abundance of current researches working on the sentiment analysis\nfrom videos and audios, finding the best model that gives the highest accuracy\nrate is still considered a challenge for researchers in this field. The main\nobjective of this paper is to prove the usability of emotion recognition models\nthat take video and audio inputs. The datasets used to train the models are the\nCREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned\nmodels that been used are: Facebook/wav2vec2-large for audio and the\nGoogle/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for\neach emotion generated by the two previous models is utilized in the decision\nmaking framework. After disparity in the results, if one of the models gets\nmuch higher accuracy, another test framework is created. The methods used are\nthe Weighted Average method, the Confidence Level Threshold method, the Dynamic\nWeighting Based on Confidence method, and the Rule-Based Logic method. This\nlimited approach gives encouraging results that make future research into these\nmethods viable.\n","subjects":["Computer Science/Sound","Computer Science/Artificial Intelligence","Computer Science/Computer Vision and Pattern Recognition","Computer Science/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_9k6fu1cFTGggrQgokbtGIndy6MMhfZhLeH1X91PFr8","pdfSize":"573981"}