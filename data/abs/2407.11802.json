{"id":"2407.11802","title":"Invariant Causal Knowledge Distillation in Neural Networks","authors":"Nikolaos Giakoumoglou, Tania Stathaki","authorsParsed":[["Giakoumoglou","Nikolaos",""],["Stathaki","Tania",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 14:53:35 GMT"},{"version":"v2","created":"Mon, 9 Sep 2024 06:17:15 GMT"}],"updateDate":"2024-09-10","timestamp":1721141615000,"abstract":"  Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nboth discriminative and invariant with respect to the teacher's outputs. Our\napproach is based on causal inference principles and combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation. ICD uses an efficient,\nparameter-free approach for flexible teacher-student alignment. We provide a\ntheoretical foundation for ICD and demonstrate its effectiveness through\nextensive experiments. Our results on CIFAR-100 and ImageNet ILSVRC-2012 show\nthat ICD outperforms traditional KD techniques and surpasses state-of-the-art\nmethods. In some cases, the student model even exceeds the teacher model in\nterms of accuracy. Furthermore, we successfully apply our method to other\ndatasets, such as Tiny ImageNet and STL-10, demonstrating superior\ncross-dataset generalization. Code is available at\nhttps://github.com/giakoumoglou/distillers.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dzTuo_2jzdm8WyONO7QNF_ETZ1nM5_2LONj935Gmq3o","pdfSize":"985267"}