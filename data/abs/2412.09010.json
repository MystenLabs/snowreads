{"id":"2412.09010","title":"Training Physical Neural Networks for Analog In-Memory Computing","authors":"Yusuke Sakemi, Yuji Okamoto, Takashi Morie, Sou Nobukawa, Takeo\n  Hosomi, Kazuyuki Aihara","authorsParsed":[["Sakemi","Yusuke",""],["Okamoto","Yuji",""],["Morie","Takashi",""],["Nobukawa","Sou",""],["Hosomi","Takeo",""],["Aihara","Kazuyuki",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 07:22:23 GMT"}],"updateDate":"2024-12-13","timestamp":1733988143000,"abstract":"  In-memory computing (IMC) architectures mitigate the von Neumann bottleneck\nencountered in traditional deep learning accelerators. Its energy efficiency\ncan realize deep learning-based edge applications. However, because IMC is\nimplemented using analog circuits, inherent non-idealities in the hardware pose\nsignificant challenges. This paper presents physical neural networks (PNNs) for\nconstructing physical models of IMC. PNNs can address the synaptic current's\ndependence on membrane potential, a challenge in charge-domain IMC systems. The\nproposed model is mathematically equivalent to spiking neural networks with\nreversal potentials. With a novel technique called differentiable spike-time\ndiscretization, the PNNs are efficiently trained. We show that hardware\nnon-idealities traditionally viewed as detrimental can enhance the model's\nlearning performance. This bottom-up methodology was validated by designing an\nIMC circuit with non-ideal characteristics using the sky130 process. When\nemploying this bottom-up approach, the modeling error reduced by an order of\nmagnitude compared to conventional top-down methods in post-layout simulations.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jNpmNw01knwJ5evYFRX1H6hOk2KXaOeQMqAJPtKNrfg","pdfSize":"12364624"}