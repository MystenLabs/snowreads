{"id":"2407.01558","title":"Visual grounding for desktop graphical user interfaces","authors":"Tassnim Dardouri and Laura Minkova and Jessica L\\'opez Espejel and\n  Walid Dahhane and El Hassane Ettifouri","authorsParsed":[["Dardouri","Tassnim",""],["Minkova","Laura",""],["Espejel","Jessica LÃ³pez",""],["Dahhane","Walid",""],["Ettifouri","El Hassane",""]],"versions":[{"version":"v1","created":"Sun, 5 May 2024 19:10:19 GMT"},{"version":"v2","created":"Tue, 17 Sep 2024 10:15:07 GMT"}],"updateDate":"2024-09-18","timestamp":1714936219000,"abstract":"  Most instance perception and image understanding solutions focus mainly on\nnatural images. However, applications for synthetic images, and more\nspecifically, images of Graphical User Interfaces (GUI) remain limited. This\nhinders the development of autonomous computer-vision-powered Artificial\nIntelligence (AI) agents. In this work, we present Instruction Visual Grounding\nor IVG, a multi-modal solution for object identification in a GUI. More\nprecisely, given a natural language instruction and GUI screen, IVG locates the\ncoordinates of the element on the screen where the instruction would be\nexecuted. To this end, we develop two methods. The first method is a three-part\narchitecture that relies on a combination of a Large Language Model (LLM) and\nan object detection model. The second approach uses a multi-modal foundation\nmodel.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"aapH1mBhPds3mX9_CG5y3lCncLBv1zzGRDBDWOqUIVc","pdfSize":"581109","objectId":"0x9d569a52042db65a76760e526e02d6ef558076202000ccace09fafa90320f1ac","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
