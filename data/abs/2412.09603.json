{"id":"2412.09603","title":"Do Multimodal Large Language Models See Like Humans?","authors":"Jiaying Lin, Shuquan Ye and Rynson W.H. Lau","authorsParsed":[["Lin","Jiaying",""],["Ye","Shuquan",""],["Lau","Rynson W. H.",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 18:59:25 GMT"}],"updateDate":"2024-12-13","timestamp":1734029965000,"abstract":"  Multimodal Large Language Models (MLLMs) have achieved impressive results on\nvarious vision tasks, leveraging recent advancements in large language models.\nHowever, a critical question remains unaddressed: do MLLMs perceive visual\ninformation similarly to humans? Current benchmarks lack the ability to\nevaluate MLLMs from this perspective. To address this challenge, we introduce\nHVSBench, a large-scale benchmark designed to assess the alignment between\nMLLMs and the human visual system (HVS) on fundamental vision tasks that mirror\nhuman vision. HVSBench curated over 85K multimodal samples, spanning 13\ncategories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,\nFree-Viewing, and Searching. Extensive experiments demonstrate the\neffectiveness of our benchmark in providing a comprehensive evaluation of\nMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best models\nshow significant room for improvement, with most achieving only moderate\nresults. Our experiments reveal that HVSBench presents a new and significant\nchallenge for cutting-edge MLLMs. We believe that HVSBench will facilitate\nresearch on human-aligned and explainable MLLMs, marking a key step in\nunderstanding how MLLMs perceive and process visual information.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3nilre3hiaGRTaaMrPZu8WLLF4TCrvknQpMu2wctbUA","pdfSize":"19776871"}