{"id":"2407.03953","title":"Generalizing Graph Transformers Across Diverse Graphs and Tasks via\n  Pre-Training on Industrial-Scale Data","authors":"Yufei He, Zhenyu Hou, Yukuo Cen, Feng He, Xu Cheng, Bryan Hooi","authorsParsed":[["He","Yufei",""],["Hou","Zhenyu",""],["Cen","Yukuo",""],["He","Feng",""],["Cheng","Xu",""],["Hooi","Bryan",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 14:14:09 GMT"},{"version":"v2","created":"Sun, 1 Sep 2024 13:40:23 GMT"},{"version":"v3","created":"Fri, 13 Sep 2024 08:55:10 GMT"}],"updateDate":"2024-09-16","timestamp":1720102449000,"abstract":"  Graph pre-training has been concentrated on graph-level on small graphs\n(e.g., molecular graphs) or learning node representations on a fixed graph.\nExtending graph pre-trained models to web-scale graphs with billions of nodes\nin industrial scenarios, while avoiding negative transfer across graphs or\ntasks, remains a challenge. We aim to develop a general graph pre-trained model\nwith inductive ability that can make predictions for unseen new nodes and even\nnew graphs. In this work, we introduce a scalable transformer-based graph\npre-training framework called PGT (Pre-trained Graph Transformer).\nSpecifically, we design a flexible and scalable graph transformer as the\nbackbone network. Meanwhile, based on the masked autoencoder architecture, we\ndesign two pre-training tasks: one for reconstructing node features and the\nother one for reconstructing local structures. Unlike the original autoencoder\narchitecture where the pre-trained decoder is discarded, we propose a novel\nstrategy that utilizes the decoder for feature augmentation. We have deployed\nour framework on Tencent's online game data. Extensive experiments have\ndemonstrated that our framework can perform pre-training on real-world\nweb-scale graphs with over 540 million nodes and 12 billion edges and\ngeneralizes effectively to unseen new graphs with different downstream tasks.\nWe further conduct experiments on the publicly available ogbn-papers100M\ndataset, which consists of 111 million nodes and 1.6 billion edges. Our\nframework achieves state-of-the-art performance on both industrial datasets and\npublic datasets, while also enjoying scalability and efficiency.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Social and Information Networks"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XddsBKtlFHg0zmMaw4f-kzg32OhJ2M07KldWIvL90kc","pdfSize":"1490716"}