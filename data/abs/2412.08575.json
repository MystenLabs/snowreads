{"id":"2412.08575","title":"Annotation-Efficient Task Guidance for Medical Segment Anything","authors":"Tyler Ward and Abdullah-Al-Zubaer Imran","authorsParsed":[["Ward","Tyler",""],["Imran","Abdullah-Al-Zubaer",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 17:47:00 GMT"}],"updateDate":"2024-12-12","timestamp":1733939220000,"abstract":"  Medical image segmentation is a key task in the imaging workflow, influencing\nmany image-based decisions. Traditional, fully-supervised segmentation models\nrely on large amounts of labeled training data, typically obtained through\nmanual annotation, which can be an expensive, time-consuming, and error-prone\nprocess. This signals a need for accurate, automatic, and annotation-efficient\nmethods of training these models. We propose SAM-Mix, a novel multitask\nlearning framework for medical image segmentation that uses class activation\nmaps produced by an auxiliary classifier to guide the predictions of the\nsemi-supervised segmentation branch, which is based on the SAM framework.\nExperimental evaluations on the public LiTS dataset confirm the effectiveness\nof SAM-Mix for simultaneous classification and segmentation of the liver from\nabdominal computed tomography (CT) scans. When trained for 90% fewer epochs on\nonly 50 labeled 2D slices, representing just 0.04% of the available labeled\ntraining data, SAM-Mix achieves a Dice improvement of 5.1% over the best\nbaseline model. The generalization results for SAM-Mix are even more\nimpressive, with the same model configuration yielding a 25.4% Dice improvement\non a cross-domain segmentation task. Our code is available at\nhttps://github.com/tbwa233/SAM-Mix.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ROPa4JmOOGN6wa7YW1s_5sw6tEnf_Y0s2QUW3tVjgok","pdfSize":"1018773"}