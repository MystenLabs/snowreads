{
  "id": "2412.10749",
  "title": "Patch-level Sounding Object Tracking for Audio-Visual Question Answering",
  "authors": "Zhangbin Li, Jinxing Zhou, Jing Zhang, Shengeng Tang, Kun Li, Dan Guo",
  "authorsParsed": [
    [
      "Li",
      "Zhangbin",
      ""
    ],
    [
      "Zhou",
      "Jinxing",
      ""
    ],
    [
      "Zhang",
      "Jing",
      ""
    ],
    [
      "Tang",
      "Shengeng",
      ""
    ],
    [
      "Li",
      "Kun",
      ""
    ],
    [
      "Guo",
      "Dan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 14 Dec 2024 08:34:44 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734165284000,
  "abstract": "  Answering questions related to audio-visual scenes, i.e., the AVQA task, is\nbecoming increasingly popular. A critical challenge is accurately identifying\nand tracking sounding objects related to the question along the timeline. In\nthis paper, we present a new Patch-level Sounding Object Tracking (PSOT)\nmethod. It begins with a Motion-driven Key Patch Tracking (M-KPT) module, which\nrelies on visual motion information to identify salient visual patches with\nsignificant movements that are more likely to relate to sounding objects and\nquestions. We measure the patch-wise motion intensity map between neighboring\nvideo frames and utilize it to construct and guide a motion-driven graph\nnetwork. Meanwhile, we design a Sound-driven KPT (S-KPT) module to explicitly\ntrack sounding patches. This module also involves a graph network, with the\nadjacency matrix regularized by the audio-visual correspondence map. The M-KPT\nand S-KPT modules are performed in parallel for each temporal segment, allowing\nbalanced tracking of salient and sounding objects. Based on the tracked\npatches, we further propose a Question-driven KPT (Q-KPT) module to retain\npatches highly relevant to the question, ensuring the model focuses on the most\ninformative clues. The audio-visual-question features are updated during the\nprocessing of these modules, which are then aggregated for final answer\nprediction. Extensive experiments on standard datasets demonstrate the\neffectiveness of our method, achieving competitive performance even compared to\nrecent large-scale pretraining-based approaches.\n",
  "subjects": [
    "Computer Science/Multimedia",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "WeBhcCtLW5v6FEOshYoQHS3c-UHQOA8T2ZhpZssKEI0",
  "pdfSize": "1301833"
}