{
  "id": "2412.08147",
  "title": "How to Weight Multitask Finetuning? Fast Previews via Bayesian\n  Model-Merging",
  "authors": "Hugo Monz\\'on Maldonado, Thomas M\\\"ollenhoff, Nico Daheim, Iryna\n  Gurevych, Mohammad Emtiyaz Khan",
  "authorsParsed": [
    [
      "Maldonado",
      "Hugo Monzón",
      ""
    ],
    [
      "Möllenhoff",
      "Thomas",
      ""
    ],
    [
      "Daheim",
      "Nico",
      ""
    ],
    [
      "Gurevych",
      "Iryna",
      ""
    ],
    [
      "Khan",
      "Mohammad Emtiyaz",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 07:06:36 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733900796000,
  "abstract": "  When finetuning multiple tasks altogether, it is important to carefully weigh\nthem to get a good performance, but searching for good weights can be difficult\nand costly. Here, we propose to aid the search with fast previews to quickly\nget a rough idea of different reweighting options. We use model merging to\ncreate previews by simply reusing and averaging parameters of models trained on\neach task separately (no retraining required). To improve the quality of\npreviews, we propose a Bayesian approach to design new merging strategies by\nusing more flexible posteriors. We validate our findings on vision and\nnatural-language transformers. Our work shows the benefits of model merging via\nBayes to improve multitask finetuning.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "VeaMhC_ATKZ8VypEMe9PqWiLqGwVm8vIrNmrJ6fCCkk",
  "pdfSize": "2353491"
}