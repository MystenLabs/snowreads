{"id":"2407.10827","title":"LLM Circuit Analyses Are Consistent Across Training and Scale","authors":"Curt Tigges, Michael Hanna, Qinan Yu, Stella Biderman","authorsParsed":[["Tigges","Curt",""],["Hanna","Michael",""],["Yu","Qinan",""],["Biderman","Stella",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 15:38:51 GMT"}],"updateDate":"2024-07-16","timestamp":1721057931000,"abstract":"  Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"hv2IPie4-MkhHavNzhr079ODqCySmjGlyIoLGtMnXGM","pdfSize":"918135"}