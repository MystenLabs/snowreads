{"id":"2412.08905","title":"Phi-4 Technical Report","authors":"Marah Abdin, Jyoti Aneja, Harkirat Behl, S\\'ebastien Bubeck, Ronen\n  Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan\n  Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung\n  Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli\n  Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu,\n  Cyril Zhang, and Yi Zhang","authorsParsed":[["Abdin","Marah",""],["Aneja","Jyoti",""],["Behl","Harkirat",""],["Bubeck","SÃ©bastien",""],["Eldan","Ronen",""],["Gunasekar","Suriya",""],["Harrison","Michael",""],["Hewett","Russell J.",""],["Javaheripi","Mojan",""],["Kauffmann","Piero",""],["Lee","James R.",""],["Lee","Yin Tat",""],["Li","Yuanzhi",""],["Liu","Weishung",""],["Mendes","Caio C. T.",""],["Nguyen","Anh",""],["Price","Eric",""],["de Rosa","Gustavo",""],["Saarikivi","Olli",""],["Salim","Adil",""],["Shah","Shital",""],["Wang","Xin",""],["Ward","Rachel",""],["Wu","Yue",""],["Yu","Dingli",""],["Zhang","Cyril",""],["Zhang","Yi",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 03:37:41 GMT"}],"updateDate":"2024-12-13","timestamp":1733974661000,"abstract":"  We present phi-4, a 14-billion parameter language model developed with a\ntraining recipe that is centrally focused on data quality. Unlike most language\nmodels, where pre-training is based primarily on organic data sources such as\nweb content or code, phi-4 strategically incorporates synthetic data throughout\nthe training process. While previous models in the Phi family largely distill\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\nthat our data-generation and post-training techniques go beyond distillation.\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size -- especially on reasoning-focused benchmarks\n-- due to improved data, training curriculum, and innovations in the\npost-training scheme.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_wpxQV3N6JM0UGWQ9qDbBcIMR7Utdps5ufRt5HNwUL8","pdfSize":"1016588"}