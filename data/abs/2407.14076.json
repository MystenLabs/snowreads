{"id":"2407.14076","title":"Domain-Specific Pretraining of Language Models: A Comparative Study in\n  the Medical Field","authors":"Tobias Kerner","authorsParsed":[["Kerner","Tobias",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 07:12:43 GMT"},{"version":"v2","created":"Sun, 28 Jul 2024 07:46:26 GMT"}],"updateDate":"2024-07-30","timestamp":1721373163000,"abstract":"  There are many cases where LLMs are used for specific tasks in a single\ndomain. These usually require less general, but more domain-specific knowledge.\nHighly capable, general-purpose state-of-the-art language models like GPT-4 or\nClaude-3-opus can often be used for such tasks, but they are very large and\ncannot be run locally, even if they were not proprietary. This can be a problem\nwhen working with sensitive data. This paper focuses on domain-specific and\nmixed-domain pretraining as potentially more efficient methods than general\npretraining for specialized language models. We will take a look at work\nrelated to domain-specific pretraining, specifically in the medical area, and\ncompare benchmark results of specialized language models to general-purpose\nlanguage models.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DVObS2urEm4yeCozQaosfh-FlKX89HsiYYQdmN43j3Y","pdfSize":"314471"}