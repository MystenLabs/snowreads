{"id":"2407.03757","title":"DiffRetouch: Using Diffusion to Retouch on the Shoulder of Experts","authors":"Zheng-Peng Duan, Jiawei zhang, Zheng Lin, Xin Jin, Dongqing Zou,\n  Chunle Guo, Chongyi Li","authorsParsed":[["Duan","Zheng-Peng",""],["zhang","Jiawei",""],["Lin","Zheng",""],["Jin","Xin",""],["Zou","Dongqing",""],["Guo","Chunle",""],["Li","Chongyi",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 09:09:42 GMT"}],"updateDate":"2024-07-08","timestamp":1720084182000,"abstract":"  Image retouching aims to enhance the visual quality of photos. Considering\nthe different aesthetic preferences of users, the target of retouching is\nsubjective. However, current retouching methods mostly adopt deterministic\nmodels, which not only neglects the style diversity in the expert-retouched\nresults and tends to learn an average style during training, but also lacks\nsample diversity during inference. In this paper, we propose a diffusion-based\nmethod, named DiffRetouch. Thanks to the excellent distribution modeling\nability of diffusion, our method can capture the complex fine-retouched\ndistribution covering various visual-pleasing styles in the training data.\nMoreover, four image attributes are made adjustable to provide a user-friendly\nediting mechanism. By adjusting these attributes in specified ranges, users are\nallowed to customize preferred styles within the learned fine-retouched\ndistribution. Additionally, the affine bilateral grid and contrastive learning\nscheme are introduced to handle the problem of texture distortion and control\ninsensitivity respectively. Extensive experiments have demonstrated the\nsuperior performance of our method on visually appealing and sample diversity.\nThe code will be made available to the community.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"K3l9ECPFvQYNUtcseh4ROzXidB0eVQ9KG4VcNDlmy1M","pdfSize":"28017646"}