{"id":"2407.18938","title":"Mitigating Cognitive Biases in Multi-Criteria Crowd Assessment","authors":"Shun Ito and Hisashi Kashima","authorsParsed":[["Ito","Shun",""],["Kashima","Hisashi",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:00:23 GMT"}],"updateDate":"2024-07-30","timestamp":1720627223000,"abstract":"  Crowdsourcing is an easy, cheap, and fast way to perform large scale quality\nassessment; however, human judgments are often influenced by cognitive biases,\nwhich lowers their credibility. In this study, we focus on cognitive biases\nassociated with a multi-criteria assessment in crowdsourcing; crowdworkers who\nrate targets with multiple different criteria simultaneously may provide biased\nresponses due to prominence of some criteria or global impressions of the\nevaluation targets. To identify and mitigate such biases, we first create\nevaluation datasets using crowdsourcing and investigate the effect of\ninter-criteria cognitive biases on crowdworker responses. Then, we propose two\nspecific model structures for Bayesian opinion aggregation models that consider\ninter-criteria relations. Our experiments show that incorporating our proposed\nstructures into the aggregation model is effective to reduce the cognitive\nbiases and help obtain more accurate aggregation results.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TizIPUobVjk515U6fpEKb5ByEMSATVOAQ_mrTnS5S-8","pdfSize":"1113097"}