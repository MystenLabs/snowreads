{"id":"2412.00029","title":"Planning vs Reasoning: Ablations to Test Capabilities of LoRA layers","authors":"Neel Redkar","authorsParsed":[["Redkar","Neel",""]],"versions":[{"version":"v1","created":"Tue, 19 Nov 2024 10:51:49 GMT"},{"version":"v2","created":"Wed, 5 Feb 2025 10:01:29 GMT"}],"updateDate":"2025-02-06","timestamp":1732013509000,"abstract":"  Low-Rank Adaptation (LoRA) layers have emerged as a promising approach for\nefficient model fine-tuning, but their capabilities and limitations have not\nbeen fully explored. This paper: 1) Investigates the fundamental question of\nwhether LoRA layers are effective at increasing reasoning + planning abilities\n2) We introduce HashChain Reasoning, a novel evaluation dataset that\ndeterministically tests reasoning capabilities.\n  Through systematic ablation studies on GPT-2, we demonstrate that reasoning\ncapabilities appear to exist primarily in low-rank spaces and can be\neffectively enhanced using LoRA layers. The effective rank analysis of trained\nLoRA matrices reveals a 2-3x lower rank requirement for reasoning tasks\ncompared to planning tasks, giving context on where LoRA layers would be\neffective. This also provides evidence for reasoning fundamentally preferring\nlow-parameter spaces for generalization.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WlDuXVow0h75VOgtyZkU3yPPoGDNpOvnd4uBDMVgG6g","pdfSize":"924725"}