{"id":"2407.01496","title":"Fast Iterative Solver For Neural Network Method: II. 1D\n  Diffusion-Reaction Problems And Data Fitting","authors":"Zhiqiang Cai, Anastassia Doktorova, Robert D. Falgout, C\\'esar Herrera","authorsParsed":[["Cai","Zhiqiang",""],["Doktorova","Anastassia",""],["Falgout","Robert D.",""],["Herrera","CÃ©sar",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:42:29 GMT"}],"updateDate":"2024-07-03","timestamp":1719855749000,"abstract":"  This paper expands the damped block Newton (dBN) method introduced recently\nin [4] for 1D diffusion-reaction equations and least-squares data fitting\nproblems. To determine the linear parameters (the weights and bias of the\noutput layer) of the neural network (NN), the dBN method requires solving\nsystems of linear equations involving the mass matrix. While the mass matrix\nfor local hat basis functions is tri-diagonal and well-conditioned, the mass\nmatrix for NNs is dense and ill-conditioned. For example, the condition number\nof the NN mass matrix for quasi-uniform meshes is at least ${\\cal O}(n^4)$. We\npresent a factorization of the mass matrix that enables solving the systems of\nlinear equations in ${\\cal O}(n)$ operations. To determine the non-linear\nparameters (the weights and bias of the hidden layer), one step of a damped\nNewton method is employed at each iteration. A Gauss-Newton method is used in\nplace of Newton for the instances in which the Hessian matrices are singular.\nThis modified dBN is referred to as dBGN. For both methods, the computational\ncost per iteration is ${\\cal O}(n)$. Numerical results demonstrate the ability\ndBN and dBGN to efficiently achieve accurate results and outperform BFGS for\nselect examples.\n","subjects":["Mathematics/Numerical Analysis","Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"hih46BGKErHx-aXjqWJhEuhpgIZ5M1V9nlA3aMXuQPo","pdfSize":"788824"}