{"id":"2407.09450","title":"Human-like Episodic Memory for Infinite Context LLMs","authors":"Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia\n  Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang","authorsParsed":[["Fountas","Zafeirios",""],["Benfeghoul","Martin A",""],["Oomerjee","Adnan",""],["Christopoulou","Fenia",""],["Lampouras","Gerasimos",""],["Bou-Ammar","Haitham",""],["Wang","Jun",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 17:34:03 GMT"}],"updateDate":"2024-07-15","timestamp":1720805643000,"abstract":"  Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning","Quantitative Biology/Neurons and Cognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dYG9skMMFM8___oh1RuZkuLnxCFKUYINDtDmBXm_R_c","pdfSize":"822924"}