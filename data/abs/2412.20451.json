{
  "id": "2412.20451",
  "title": "Improving Vision-Language-Action Models via Chain-of-Affordance",
  "authors": "Jinming Li, Yichen Zhu, Zhibin Tang, Junjie Wen, Minjie Zhu, Xiaoyu\n  Liu, Chengmeng Li, Ran Cheng, Yaxin Peng, Feifei Feng",
  "authorsParsed": [
    [
      "Li",
      "Jinming",
      ""
    ],
    [
      "Zhu",
      "Yichen",
      ""
    ],
    [
      "Tang",
      "Zhibin",
      ""
    ],
    [
      "Wen",
      "Junjie",
      ""
    ],
    [
      "Zhu",
      "Minjie",
      ""
    ],
    [
      "Liu",
      "Xiaoyu",
      ""
    ],
    [
      "Li",
      "Chengmeng",
      ""
    ],
    [
      "Cheng",
      "Ran",
      ""
    ],
    [
      "Peng",
      "Yaxin",
      ""
    ],
    [
      "Feng",
      "Feifei",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 29 Dec 2024 12:24:31 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735475071000,
  "abstract": "  Robot foundation models, particularly Vision-Language-Action (VLA) models,\nhave garnered significant attention for their ability to enhance robot policy\nlearning, greatly improving robot generalization and robustness. OpenAI recent\nmodel, o1, showcased impressive capabilities in solving complex problems by\nutilizing extensive reasoning chains. This prompts an important question: can\nrobot models achieve better performance in multi-task, complex environments by\nreviewing prior observations and then providing task-specific reasoning to\nguide action prediction? In this paper, we introduce\n\\textbf{Chain-of-Affordance (CoA)}, a novel approach to scaling robot models by\nincorporating reasoning in the format of sequential robot affordances to\nfacilitate task completion. Specifically, we prompt the model to consider the\nfollowing four types of affordances before taking action: a) object affordance\n- what object to manipulate and where it is; b) grasp affordance - the specific\nobject part to grasp; c) spatial affordance - the optimal space to place the\nobject; and d) movement affordance - the collision-free path for movement. By\nintegrating this knowledge into the policy model, the robot gains essential\ncontext, allowing it to act with increased precision and robustness during\ninference. Our experiments demonstrate that CoA achieves superior performance\nthan state-of-the-art robot foundation models, such as OpenVLA and Octo.\nAdditionally, CoA shows strong generalization to unseen object poses,\nidentifies free space, and avoids obstacles in novel environments.\n",
  "subjects": [
    "Computer Science/Robotics"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "JvIlcpj81J3PMNz1hGHJ-8fAjz41DB7Ab_jSNnv6GZI",
  "pdfSize": "7141268"
}