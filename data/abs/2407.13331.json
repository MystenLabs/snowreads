{"id":"2407.13331","title":"Reconstruct the Pruned Model without Any Retraining","authors":"Pingjie Wang, Ziqing Fan, Shengchao Hu, Zhe Chen, Yanfeng Wang, Yu\n  Wang","authorsParsed":[["Wang","Pingjie",""],["Fan","Ziqing",""],["Hu","Shengchao",""],["Chen","Zhe",""],["Wang","Yanfeng",""],["Wang","Yu",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 09:30:44 GMT"}],"updateDate":"2024-07-19","timestamp":1721295044000,"abstract":"  Structured pruning is a promising hardware-friendly compression technique for\nlarge language models (LLMs), which is expected to be retraining-free to avoid\nthe enormous retraining cost. This retraining-free paradigm involves (1)\npruning criteria to define the architecture and (2) distortion reconstruction\nto restore performance. However, existing methods often emphasize pruning\ncriteria while using reconstruction techniques that are specific to certain\nmodules or criteria, resulting in limited generalizability. To address this, we\nintroduce the Linear Interpolation-based Adaptive Reconstruction (LIAR)\nframework, which is both efficient and effective. LIAR does not require\nback-propagation or retraining and is compatible with various pruning criteria\nand modules. By applying linear interpolation to the preserved weights, LIAR\nminimizes reconstruction error and effectively reconstructs the pruned output.\nOur evaluations on benchmarks such as GLUE, SQuAD, WikiText, and common sense\nreasoning show that LIAR enables a BERT model to maintain 98% accuracy even\nafter removing 50% of its parameters and achieves top performance for LLaMA in\njust a few minutes.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9yeQC5qQwrn3AFB3XHGtmukroiljtbsbRFTyYbtXTwQ","pdfSize":"1403713"}