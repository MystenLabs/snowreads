{"id":"2412.06359","title":"On-Device Self-Supervised Learning of Low-Latency Monocular Depth from\n  Only Events","authors":"Jesse Hagenaars, Yilun Wu, Federico Paredes-Vall\\'es, Stein\n  Stroobants, Guido de Croon","authorsParsed":[["Hagenaars","Jesse",""],["Wu","Yilun",""],["Paredes-Vall√©s","Federico",""],["Stroobants","Stein",""],["de Croon","Guido",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 10:23:03 GMT"}],"updateDate":"2024-12-10","timestamp":1733739783000,"abstract":"  Event cameras provide low-latency perception for only milliwatts of power.\nThis makes them highly suitable for resource-restricted, agile robots such as\nsmall flying drones. Self-supervised learning based on contrast maximization\nholds great potential for event-based robot vision, as it foregoes the need to\nhigh-frequency ground truth and allows for online learning in the robot's\noperational environment. However, online, onboard learning raises the major\nchallenge of achieving sufficient computational efficiency for real-time\nlearning, while maintaining competitive visual perception performance. In this\nwork, we improve the time and memory efficiency of the contrast maximization\nlearning pipeline. Benchmarking experiments show that the proposed pipeline\nachieves competitive results with the state of the art on the task of depth\nestimation from events. Furthermore, we demonstrate the usability of the\nlearned depth for obstacle avoidance through real-world flight experiments.\nFinally, we compare the performance of different combinations of pre-training\nand fine-tuning of the depth estimation networks, showing that on-board domain\nadaptation is feasible given a few minutes of flight.\n","subjects":["Computer Science/Robotics","Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JfUDIgU6wIHKIYOBsj9_jJqP9C52NyMnPWo2nhO1S7o","pdfSize":"22193294"}