{"id":"2412.10321","title":"AdvPrefix: An Objective for Nuanced LLM Jailbreaks","authors":"Sicheng Zhu, Brandon Amos, Yuandong Tian, Chuan Guo, Ivan Evtimov","authorsParsed":[["Zhu","Sicheng",""],["Amos","Brandon",""],["Tian","Yuandong",""],["Guo","Chuan",""],["Evtimov","Ivan",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 18:00:57 GMT"}],"updateDate":"2024-12-16","timestamp":1734112857000,"abstract":"  Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Computation and Language","Computer Science/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"2IWTJB39EiTMDLM_THQ5iAYR4Ri1-uRRo6BV8iKlIbg","pdfSize":"3643013"}