{
  "id": "2412.03599",
  "title": "CPTQuant - A Novel Mixed Precision Post-Training Quantization Techniques\n  for Large Language Models",
  "authors": "Amitash Nanda, Sree Bhargavi Balija, Debashis Sahoo",
  "authorsParsed": [
    [
      "Nanda",
      "Amitash",
      ""
    ],
    [
      "Balija",
      "Sree Bhargavi",
      ""
    ],
    [
      "Sahoo",
      "Debashis",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 04:52:41 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733201561000,
  "abstract": "  Large language models have transformed the comprehension and generation of\nnatural language tasks, but they come with substantial memory and computational\nrequirements. Quantization techniques have emerged as a promising avenue for\naddressing these challenges while preserving accuracy and making energy\nefficient. We propose CPTQuant, a comprehensive strategy that introduces\ncorrelation-based (CMPQ), pruning-based (PMPQ), and Taylor decomposition-based\n(TDMPQ) mixed precision techniques. CMPQ adapts the precision level based on\ncanonical correlation analysis of different layers. PMPQ optimizes precision\nlayer-wise based on their sensitivity to sparsity. TDMPQ modifies precision\nusing Taylor decomposition to assess each layer's sensitivity to input\nperturbation. These strategies allocate higher precision to more sensitive\nlayers while diminishing precision to robust layers. CPTQuant assesses the\nperformance across BERT, OPT-125M, OPT-350M, OPT-1.3B, and OPT-2.7B. We\ndemonstrate up to 4x compression and a 2x-fold increase in efficiency with\nminimal accuracy drop compared to Hugging Face FP16. PMPQ stands out for\nachieving a considerably higher model compression. Sensitivity analyses across\nvarious LLMs show that the initial and final 30% of layers exhibit higher\nsensitivities than the remaining layers. PMPQ demonstrates an 11% higher\ncompression ratio than other methods for classification tasks, while TDMPQ\nachieves a 30% greater compression ratio for language modeling tasks.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "v2hF1_cohOPNhjMXcYr4nausq61_XEdhp83jzw_dcfI",
  "pdfSize": "2522191"
}