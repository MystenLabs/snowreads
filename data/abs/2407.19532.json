{"id":"2407.19532","title":"The Interpretability of Codebooks in Model-Based Reinforcement Learning\n  is Limited","authors":"Kenneth Eaton, Jonathan Balloch, Julia Kim, Mark Riedl","authorsParsed":[["Eaton","Kenneth",""],["Balloch","Jonathan",""],["Kim","Julia",""],["Riedl","Mark",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 16:40:20 GMT"}],"updateDate":"2024-07-30","timestamp":1722184820000,"abstract":"  Interpretability of deep reinforcement learning systems could assist\noperators with understanding how they interact with their environment. Vector\nquantization methods -- also called codebook methods -- discretize a neural\nnetwork's latent space that is often suggested to yield emergent\ninterpretability. We investigate whether vector quantization in fact provides\ninterpretability in model-based reinforcement learning. Our experiments,\nconducted in the reinforcement learning environment Crafter, show that the\ncodes of vector quantization models are inconsistent, have no guarantee of\nuniqueness, and have a limited impact on concept disentanglement, all of which\nare necessary traits for interpretability. We share insights on why vector\nquantization may be fundamentally insufficient for model interpretability.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LCpgAY4Rjgt6b_VH4wm-FxA4SsifOmuWzK_lDCh6IDA","pdfSize":"2291141"}