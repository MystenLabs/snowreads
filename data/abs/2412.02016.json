{"id":"2412.02016","title":"Explore Reinforced: Equilibrium Approximation with Reinforcement\n  Learning","authors":"Ryan Yu, Mateusz Nowak, Qintong Xie, Michelle Yilin Feng and Peter\n  Chin","authorsParsed":[["Yu","Ryan",""],["Nowak","Mateusz",""],["Xie","Qintong",""],["Feng","Michelle Yilin",""],["Chin","Peter",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 22:37:59 GMT"}],"updateDate":"2024-12-04","timestamp":1733179079000,"abstract":"  Current approximate Coarse Correlated Equilibria (CCE) algorithms struggle\nwith equilibrium approximation for games in large stochastic environments but\nare theoretically guaranteed to converge to a strong solution concept. In\ncontrast, modern Reinforcement Learning (RL) algorithms provide faster training\nyet yield weaker solutions. We introduce Exp3-IXrl - a blend of RL and\ngame-theoretic approach, separating the RL agent's action selection from the\nequilibrium computation while preserving the integrity of the learning process.\nWe demonstrate that our algorithm expands the application of equilibrium\napproximation algorithms to new environments. Specifically, we show the\nimproved performance in a complex and adversarial cybersecurity network\nenvironment - the Cyber Operations Research Gym - and in the classical\nmulti-armed bandit settings.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Computer Science and Game Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YVdMVr_4yk8Yroh1JgEIYuDCrHHaaNgN-J2LSv1YVz8","pdfSize":"752871"}