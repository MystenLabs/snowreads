{"id":"2412.10610","title":"Comparing large language models for supervised analysis of students' lab\n  notes","authors":"Rebeckah K. Fussell, Megan Flynn, Anil Damle, Michael F.J. Fox, N. G.\n  Holmes","authorsParsed":[["Fussell","Rebeckah K.",""],["Flynn","Megan",""],["Damle","Anil",""],["Fox","Michael F. J.",""],["Holmes","N. G.",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 23:32:44 GMT"},{"version":"v2","created":"Mon, 24 Feb 2025 15:38:45 GMT"}],"updateDate":"2025-02-25","timestamp":1734132764000,"abstract":"  Recent advancements in large language models (LLMs) hold significant promise\nin improving physics education research that uses machine learning. In this\nstudy, we compare the application of various models to perform large-scale\nanalysis of written text grounded in a physics education research\nclassification problem: identifying skills in students' typed lab notes through\nsentence-level labeling. Specifically, we use training data to fine-tune two\ndifferent LLMs, BERT and LLaMA, and compare the performance of these models to\nboth a traditional bag of words approach and a few-shot LLM (without\nfine-tuning).} We evaluate the models based on their resource use, performance\nmetrics, and research outcomes when identifying skills in lab notes. We find\nthat higher-resource models often, but not necessarily, perform better than\nlower-resource models. We also find that all models estimate similar trends in\nresearch outcomes, although the absolute values of the estimated measurements\nare not always within uncertainties of each other. We use the results to\ndiscuss relevant considerations for education researchers seeking to select a\nmodel type to use as a classifier.\n","subjects":["Physics/Physics Education"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rE9oaHzd1HaJyxJkrqr0knV_w1yPvh9saXyVL0eYuxk","pdfSize":"611951"}