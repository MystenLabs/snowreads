{"id":"2412.02713","title":"Applying IRT to Distinguish Between Human and Generative AI Responses to\n  Multiple-Choice Assessments","authors":"Alona Strugatski and Giora Alexandron","authorsParsed":[["Strugatski","Alona",""],["Alexandron","Giora",""]],"versions":[{"version":"v1","created":"Thu, 28 Nov 2024 09:43:06 GMT"},{"version":"v2","created":"Thu, 12 Dec 2024 13:28:20 GMT"}],"updateDate":"2024-12-13","timestamp":1732786986000,"abstract":"  Generative AI is transforming the educational landscape, raising significant\nconcerns about cheating. Despite the widespread use of multiple-choice\nquestions in assessments, the detection of AI cheating in MCQ-based tests has\nbeen almost unexplored, in contrast to the focus on detecting AI-cheating on\ntext-rich student outputs. In this paper, we propose a method based on the\napplication of Item Response Theory to address this gap. Our approach operates\non the assumption that artificial and human intelligence exhibit different\nresponse patterns, with AI cheating manifesting as deviations from the expected\npatterns of human responses. These deviations are modeled using Person-Fit\nStatistics. We demonstrate that this method effectively highlights the\ndifferences between human responses and those generated by premium versions of\nleading chatbots (ChatGPT, Claude, and Gemini), but that it is also sensitive\nto the amount of AI cheating in the data. Furthermore, we show that the\nchatbots differ in their reasoning profiles. Our work provides both a\ntheoretical foundation and empirical evidence for the application of IRT to\nidentify AI cheating in MCQ-based assessments.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"QrFBgI_U7S0iZfSi62qs75NFiog7P5E7YHhq6HXV5NY","pdfSize":"1372386"}