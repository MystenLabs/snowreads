{
  "id": "2412.20226",
  "title": "Embodiment-Agnostic Navigation Policy Trained with Visual Demonstrations",
  "authors": "Nimrod Curtis, Osher Azulay and Avishai Sintov",
  "authorsParsed": [
    [
      "Curtis",
      "Nimrod",
      ""
    ],
    [
      "Azulay",
      "Osher",
      ""
    ],
    [
      "Sintov",
      "Avishai",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 28 Dec 2024 17:47:47 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735408067000,
  "abstract": "  Learning to navigate in unstructured environments is a challenging task for\nrobots. While reinforcement learning can be effective, it often requires\nextensive data collection and can pose risk. Learning from expert\ndemonstrations, on the other hand, offers a more efficient approach. However,\nmany existing methods rely on specific robot embodiments, pre-specified target\nimages and require large datasets. We propose the Visual Demonstration-based\nEmbodiment-agnostic Navigation (ViDEN) framework, a novel framework that\nleverages visual demonstrations to train embodiment-agnostic navigation\npolicies. ViDEN utilizes depth images to reduce input dimensionality and relies\non relative target positions, making it more adaptable to diverse environments.\nBy training a diffusion-based policy on task-centric and embodiment-agnostic\ndemonstrations, ViDEN can generate collision-free and adaptive trajectories in\nreal-time. Our experiments on human reaching and tracking demonstrate that\nViDEN outperforms existing methods, requiring a small amount of data and\nachieving superior performance in various indoor and outdoor navigation\nscenarios. Project website: https://nimicurtis.github.io/ViDEN/.\n",
  "subjects": [
    "Computer Science/Robotics"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "6lQUTru23ygUE4tPKbCX9fQEpanwT4XmS7Rv5RK9EJI",
  "pdfSize": "9300296"
}