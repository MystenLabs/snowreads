{"id":"2412.12902","title":"DoPTA: Improving Document Layout Analysis using Patch-Text Alignment","authors":"Nikitha SR, Tarun Ram Menta, Mausoom Sarkar","authorsParsed":[["SR","Nikitha",""],["Menta","Tarun Ram",""],["Sarkar","Mausoom",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 13:26:31 GMT"}],"updateDate":"2024-12-18","timestamp":1734441991000,"abstract":"  The advent of multimodal learning has brought a significant improvement in\ndocument AI. Documents are now treated as multimodal entities, incorporating\nboth textual and visual information for downstream analysis. However, works in\nthis space are often focused on the textual aspect, using the visual space as\nauxiliary information. While some works have explored pure vision based\ntechniques for document image understanding, they require OCR identified text\nas input during inference, or do not align with text in their learning\nprocedure. Therefore, we present a novel image-text alignment technique\nspecially designed for leveraging the textual information in document images to\nimprove performance on visual tasks. Our document encoder model DoPTA - trained\nwith this technique demonstrates strong performance on a wide range of document\nimage understanding tasks, without requiring OCR during inference. Combined\nwith an auxiliary reconstruction objective, DoPTA consistently outperforms\nlarger models, while using significantly lesser pre-training compute. DoPTA\nalso sets new state-of-the art results on D4LA, and FUNSD, two challenging\ndocument visual analysis benchmarks.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"5hq8i2YSrxd_vcar-hU4NcB2m6l1mlHbqtTKQZnwpZQ","pdfSize":"3836429"}