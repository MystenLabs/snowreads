{"id":"2407.16772","title":"VisMin: Visual Minimal-Change Understanding","authors":"Rabiul Awal, Saba Ahmadi, Le Zhang, Aishwarya Agrawal","authorsParsed":[["Awal","Rabiul",""],["Ahmadi","Saba",""],["Zhang","Le",""],["Agrawal","Aishwarya",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 18:10:43 GMT"}],"updateDate":"2024-07-25","timestamp":1721758243000,"abstract":"  Fine-grained understanding of objects, attributes, and relationships between\nobjects is crucial for visual-language models (VLMs). Existing benchmarks\nprimarily focus on evaluating VLMs' capability to distinguish between two very\nsimilar \\textit{captions} given an image. In this paper, we introduce a new,\nchallenging benchmark termed \\textbf{Vis}ual \\textbf{Min}imal-Change\nUnderstanding (VisMin), which requires models to predict the correct\nimage-caption match given two images and two captions. The image pair and\ncaption pair contain minimal changes, i.e., only one aspect changes at a time\nfrom among the following: \\textit{object}, \\textit{attribute}, \\textit{count},\nand \\textit{spatial relation}. These changes test the models' understanding of\nobjects, attributes (such as color, material, shape), counts, and spatial\nrelationships between objects. We built an automatic framework using large\nlanguage models and diffusion models, followed by a rigorous 4-step\nverification process by human annotators. Empirical experiments reveal that\ncurrent VLMs exhibit notable deficiencies in understanding spatial\nrelationships and counting abilities. We also generate a large-scale training\ndataset to finetune CLIP and Idefics2, showing significant improvements in\nfine-grained understanding across benchmarks and in CLIP's general image-text\nalignment. We release all resources, including the benchmark, training data,\nand finetuned model checkpoints, at \\url{https://vismin.net/}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Q89oxatb2odnJQmYwzTyMVhwUhpm5ZZUPCKtNU8yKZM","pdfSize":"29226313"}