{"id":"2412.10357","title":"The Correlated Gaussian Sparse Histogram Mechanism","authors":"Christian Janos Lebeda, Lukas Retschmeier","authorsParsed":[["Lebeda","Christian Janos",""],["Retschmeier","Lukas",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 18:51:33 GMT"}],"updateDate":"2024-12-16","timestamp":1734115893000,"abstract":"  We consider the problem of releasing a sparse histogram under $(\\varepsilon,\n\\delta)$-differential privacy. The stability histogram independently adds noise\nfrom a Laplace or Gaussian distribution to the non-zero entries and removes\nthose noisy counts below a threshold.\n  Thereby, the introduction of new non-zero values between neighboring\nhistograms is only revealed with probability at most $\\delta$, and typically,\nthe value of the threshold dominates the error of the mechanism. We consider\nthe variant of the stability histogram with Gaussian noise.\n  Recent works ([Joseph and Yu, COLT '24] and [Lebeda, SOSA '25]) reduced the\nerror for private histograms using correlated Gaussian noise. However, these\ntechniques can not be directly applied in the very sparse setting. Instead, we\nadopt Lebeda's technique and show that adding correlated noise to the non-zero\ncounts only allows us to reduce the magnitude of noise when we have a sparsity\nbound. This, in turn, allows us to use a lower threshold by up to a factor of\n$1/2$ compared to the non-correlated noise mechanism. We then extend our\nmechanism to a setting without a known bound on sparsity. Additionally, we show\nthat correlated noise can give a similar improvement for the more practical\ndiscrete Gaussian mechanism.\n","subjects":["Computer Science/Data Structures and Algorithms","Computer Science/Cryptography and Security","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"S0ngj6YW0rLWc0h4kmDIEwA3x4cLW4lx2EH-0ELZ6Pc","pdfSize":"1174382"}