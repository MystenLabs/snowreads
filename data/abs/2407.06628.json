{"id":"2407.06628","title":"Masked Video and Body-worn IMU Autoencoder for Egocentric Action\n  Recognition","authors":"Mingfang Zhang, Yifei Huang, Ruicong Liu, Yoichi Sato","authorsParsed":[["Zhang","Mingfang",""],["Huang","Yifei",""],["Liu","Ruicong",""],["Sato","Yoichi",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 07:53:16 GMT"}],"updateDate":"2024-07-10","timestamp":1720511596000,"abstract":"  Compared with visual signals, Inertial Measurement Units (IMUs) placed on\nhuman limbs can capture accurate motion signals while being robust to lighting\nvariation and occlusion. While these characteristics are intuitively valuable\nto help egocentric action recognition, the potential of IMUs remains\nunder-explored. In this work, we present a novel method for action recognition\nthat integrates motion data from body-worn IMUs with egocentric video. Due to\nthe scarcity of labeled multimodal data, we design an MAE-based self-supervised\npretraining method, obtaining strong multi-modal representations via modeling\nthe natural correlation between visual and motion signals. To model the complex\nrelation of multiple IMU devices placed across the body, we exploit the\ncollaborative dynamics in multiple IMU devices and propose to embed the\nrelative motion features of human joints into a graph structure. Experiments\nshow our method can achieve state-of-the-art performance on multiple public\ndatasets. The effectiveness of our MAE-based pretraining and graph-based IMU\nmodeling are further validated by experiments in more challenging scenarios,\nincluding partially missing IMU devices and video quality corruption, promoting\nmore flexible usages in the real world.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Jo0c67Ad0T4RRiAuKUtN3XqWbuNFB_NiPso8h7prmTU","pdfSize":"1607707"}