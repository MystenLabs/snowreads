{
  "id": "2412.17411",
  "title": "Pretraining with random noise for uncertainty calibration",
  "authors": "Jeonghwan Cheon and Se-Bum Paik",
  "authorsParsed": [
    [
      "Cheon",
      "Jeonghwan",
      ""
    ],
    [
      "Paik",
      "Se-Bum",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 09:22:00 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734945720000,
  "abstract": "  Uncertainty calibration, the process of aligning confidence with accuracy, is\na hallmark of human intelligence. However, most machine learning models\nstruggle to achieve this alignment, particularly when the training dataset is\nsmall relative to the network's capacity. Here, we demonstrate that uncertainty\ncalibration can be effectively achieved through a pretraining method inspired\nby developmental neuroscience. Specifically, training with random noise before\ndata training allows neural networks to calibrate their uncertainty, ensuring\nthat confidence levels are aligned with actual accuracy. We show that randomly\ninitialized, untrained networks tend to exhibit erroneously high confidence,\nbut pretraining with random noise effectively calibrates these networks,\nbringing their confidence down to chance levels across input spaces. As a\nresult, networks pretrained with random noise exhibit optimal calibration, with\nconfidence closely aligned with accuracy throughout subsequent data training.\nThese pre-calibrated networks also perform better at identifying \"unknown data\"\nby exhibiting lower confidence for out-of-distribution samples. Our findings\nprovide a fundamental solution for uncertainty calibration in both\nin-distribution and out-of-distribution contexts.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Neural and Evolutionary Computing"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "iF8D6yvWH_5VUDcLKLuCYbyxgJfsgJBbSyOLXQcZRa4",
  "pdfSize": "3685513"
}