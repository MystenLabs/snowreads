{"id":"2407.11959","title":"Faster Algorithms for Schatten-p Low Rank Approximation","authors":"Praneeth Kacham and David P. Woodruff","authorsParsed":[["Kacham","Praneeth",""],["Woodruff","David P.",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:54:48 GMT"}],"updateDate":"2024-07-17","timestamp":1721152488000,"abstract":"  We study algorithms for the Schatten-$p$ Low Rank Approximation (LRA)\nproblem. First, we show that by using fast rectangular matrix multiplication\nalgorithms and different block sizes, we can improve the running time of the\nalgorithms in the recent work of Bakshi, Clarkson and Woodruff (STOC 2022). We\nthen show that by carefully combining our new algorithm with the algorithm of\nLi and Woodruff (ICML 2020), we can obtain even faster algorithms for\nSchatten-$p$ LRA.\n  While the block-based algorithms are fast in the real number model, we do not\nhave a stability analysis which shows that the algorithms work when implemented\non a machine with polylogarithmic bits of precision. We show that the LazySVD\nalgorithm of Allen-Zhu and Li (NeurIPS 2016) can be implemented on a floating\npoint machine with only logarithmic, in the input parameters, bits of\nprecision. As far as we are aware, this is the first stability analysis of any\nalgorithm using $O((k/\\sqrt{\\varepsilon})\\text{poly}(\\log n))$ matrix-vector\nproducts with the matrix $A$ to output a $1+\\varepsilon$ approximate solution\nfor the rank-$k$ Schatten-$p$ LRA problem.\n","subjects":["Computing Research Repository/Data Structures and Algorithms"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"6tGNCGlEvkDnoPj-0Qj_RzxvPe6bu6F7lzl9i_hhyWg","pdfSize":"727790"}