{
  "id": "2412.07808",
  "title": "Boosting Alignment for Post-Unlearning Text-to-Image Generative Models",
  "authors": "Myeongseob Ko, Henry Li, Zhun Wang, Jonathan Patsenker, Jiachen T.\n  Wang, Qinbin Li, Ming Jin, Dawn Song, Ruoxi Jia",
  "authorsParsed": [
    [
      "Ko",
      "Myeongseob",
      ""
    ],
    [
      "Li",
      "Henry",
      ""
    ],
    [
      "Wang",
      "Zhun",
      ""
    ],
    [
      "Patsenker",
      "Jonathan",
      ""
    ],
    [
      "Wang",
      "Jiachen T.",
      ""
    ],
    [
      "Li",
      "Qinbin",
      ""
    ],
    [
      "Jin",
      "Ming",
      ""
    ],
    [
      "Song",
      "Dawn",
      ""
    ],
    [
      "Jia",
      "Ruoxi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 21:36:10 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733780170000,
  "abstract": "  Large-scale generative models have shown impressive image-generation\ncapabilities, propelled by massive data. However, this often inadvertently\nleads to the generation of harmful or inappropriate content and raises\ncopyright concerns. Driven by these concerns, machine unlearning has become\ncrucial to effectively purge undesirable knowledge from models. While existing\nliterature has studied various unlearning techniques, these often suffer from\neither poor unlearning quality or degradation in text-image alignment after\nunlearning, due to the competitive nature of these objectives. To address these\nchallenges, we propose a framework that seeks an optimal model update at each\nunlearning iteration, ensuring monotonic improvement on both objectives. We\nfurther derive the characterization of such an update.\n  In addition, we design procedures to strategically diversify the unlearning\nand remaining datasets to boost performance improvement. Our evaluation\ndemonstrates that our method effectively removes target classes from recent\ndiffusion-based generative models and concepts from stable diffusion models\nwhile maintaining close alignment with the models' original trained states,\nthus outperforming state-of-the-art baselines. Our code will be made available\nat\n\\url{https://github.com/reds-lab/Restricted_gradient_diversity_unlearning.git}.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "h3VyT9ZX4jwKx888gjrGtv96Q5y7gwHRPdapQwKwl78",
  "pdfSize": "14165933"
}