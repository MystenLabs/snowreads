{"id":"2412.19114","title":"Discrete vs. Continuous Trade-offs for Generative Models","authors":"Jathin Korrapati, Tanish Baranwal, Rahul Shah","authorsParsed":[["Korrapati","Jathin",""],["Baranwal","Tanish",""],["Shah","Rahul",""]],"versions":[{"version":"v1","created":"Thu, 26 Dec 2024 08:14:27 GMT"}],"updateDate":"2024-12-30","timestamp":1735200867000,"abstract":"  This work explores the theoretical and practical foundations of denoising\ndiffusion probabilistic models (DDPMs) and score-based generative models, which\nleverage stochastic processes and Brownian motion to model complex data\ndistributions. These models employ forward and reverse diffusion processes\ndefined through stochastic differential equations (SDEs) to iteratively add and\nremove noise, enabling high-quality data generation. By analyzing the\nperformance bounds of these models, we demonstrate how score estimation errors\npropagate through the reverse process and bound the total variation distance\nusing discrete Girsanov transformations, Pinsker's inequality, and the data\nprocessing inequality (DPI) for an information theoretic lens.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Information Theory","Computer Science/Numerical Analysis","Mathematics/Information Theory","Mathematics/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TFY9h3kmaEmlGzevxas4hHAc25GoqcJackuJWzPob24","pdfSize":"854560"}