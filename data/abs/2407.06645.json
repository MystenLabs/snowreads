{"id":"2407.06645","title":"Entropy Law: The Story Behind Data Compression and LLM Performance","authors":"Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang,\n  Yong Liu, Ruiming Tang, Defu Lian, Enhong Chen","authorsParsed":[["Yin","Mingjia",""],["Wu","Chuhan",""],["Wang","Yufei",""],["Wang","Hao",""],["Guo","Wei",""],["Wang","Yasheng",""],["Liu","Yong",""],["Tang","Ruiming",""],["Lian","Defu",""],["Chen","Enhong",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 08:14:29 GMT"},{"version":"v2","created":"Wed, 10 Jul 2024 01:55:29 GMT"},{"version":"v3","created":"Thu, 11 Jul 2024 03:06:45 GMT"}],"updateDate":"2024-07-12","timestamp":1720512869000,"abstract":"  Data is the cornerstone of large language models (LLMs), but not all data is\nuseful for model learning. Carefully selected data can better elicit the\ncapabilities of LLMs with much less computational overhead. Most methods\nconcentrate on evaluating the quality of individual samples in data selection,\nwhile the combinatorial effects among samples are neglected. Even if each\nsample is of perfect quality, their combinations may be suboptimal in teaching\nLLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim\nto uncover the underlying relationships between LLM performance and data\nselection. Inspired by the information compression nature of LLMs, we uncover\nan ``entropy law'' that connects LLM performance with data compression ratio\nand first-epoch training loss, which reflect the information redundancy of a\ndataset and the mastery of inherent knowledge encoded in this dataset,\nrespectively. Through both theoretical deduction and empirical evaluation, we\nfind that model performance is negatively correlated to the compression ratio\nof training data, which usually yields a lower training loss. Based on the\nfindings of the entropy law, we propose a quite efficient and universal data\nselection method named \\textbf{ZIP} for training LLMs, which aim to prioritize\ndata subsets exhibiting a low compression ratio. Based on a multi-stage\nalgorithm that selects diverse data in a greedy manner, we can obtain a good\ndata subset with satisfactory diversity. Extensive experiments have been\nconducted to validate the entropy law and the superiority of ZIP across\ndifferent LLM backbones and alignment stages. We also present an interesting\napplication of entropy law that can detect potential performance risks at the\nbeginning of model training.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mBcgZXllC4bL5Q092H_l1tIfljYqNG8DjSTkLWuwzI8","pdfSize":"707800"}
