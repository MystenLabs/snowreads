{"id":"2407.19236","title":"Approximate learning of parsimonious Bayesian context trees","authors":"Daniyar Ghani, Nicholas A. Heard, Francesco Sanna Passino","authorsParsed":[["Ghani","Daniyar",""],["Heard","Nicholas A.",""],["Passino","Francesco Sanna",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 11:50:40 GMT"}],"updateDate":"2024-07-30","timestamp":1722081040000,"abstract":"  Models for categorical sequences typically assume exchangeable or first-order\ndependent sequence elements. These are common assumptions, for example, in\nmodels of computer malware traces and protein sequences. Although such\nsimplifying assumptions lead to computational tractability, these models fail\nto capture long-range, complex dependence structures that may be harnessed for\ngreater predictive power. To this end, a Bayesian modelling framework is\nproposed to parsimoniously capture rich dependence structures in categorical\nsequences, with memory efficiency suitable for real-time processing of data\nstreams. Parsimonious Bayesian context trees are introduced as a form of\nvariable-order Markov model with conjugate prior distributions. The novel\nframework requires fewer parameters than fixed-order Markov models by dropping\nredundant dependencies and clustering sequential contexts. Approximate\ninference on the context tree structure is performed via a computationally\nefficient model-based agglomerative clustering procedure. The proposed\nframework is tested on synthetic and real-world data examples, and it\noutperforms existing sequence models when fitted to real protein sequences and\nhoneypot computer terminal sessions.\n","subjects":["Statistics/Computation","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"b5leDVrDEbEv8lNJOXfEDnTcM0y-_1jzQm0PN6uSESg","pdfSize":"687138"}