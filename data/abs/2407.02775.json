{"id":"2407.02775","title":"MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language\n  Models","authors":"Ying Zhang and Ziheng Yang and Shufan Ji","authorsParsed":[["Zhang","Ying",""],["Yang","Ziheng",""],["Ji","Shufan",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 03:03:30 GMT"}],"updateDate":"2024-07-04","timestamp":1719975810000,"abstract":"  Knowledge distillation is an effective technique for pre-trained language\nmodel compression. Although existing knowledge distillation methods perform\nwell for the most typical model BERT, they could be further improved in two\naspects: the relation-level knowledge could be further explored to improve\nmodel performance; and the setting of student attention head number could be\nmore flexible to decrease inference time. Therefore, we are motivated to\npropose a novel knowledge distillation method MLKD-BERT to distill multi-level\nknowledge in teacher-student framework. Extensive experiments on GLUE benchmark\nand extractive question answering tasks demonstrate that our method outperforms\nstate-of-the-art knowledge distillation methods on BERT. In addition, MLKD-BERT\ncan flexibly set student attention head number, allowing for substantial\ninference time decrease with little performance drop.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uz3BGkE8_rLgxCE9DyP2vN9iOjywJ8DjgnOuNeSELAE","pdfSize":"4094797"}