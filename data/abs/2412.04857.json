{"id":"2412.04857","title":"Neuro-Symbolic Data Generation for Math Reasoning","authors":"Zenan Li, Zhi Zhou, Yuan Yao, Yu-Feng Li, Chun Cao, Fan Yang, Xian\n  Zhang, Xiaoxing Ma","authorsParsed":[["Li","Zenan",""],["Zhou","Zhi",""],["Yao","Yuan",""],["Li","Yu-Feng",""],["Cao","Chun",""],["Yang","Fan",""],["Zhang","Xian",""],["Ma","Xiaoxing",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 08:49:49 GMT"}],"updateDate":"2024-12-09","timestamp":1733474989000,"abstract":"  A critical question about Large Language Models (LLMs) is whether their\napparent deficiency in mathematical reasoning is inherent, or merely a result\nof insufficient exposure to high-quality mathematical data. To explore this, we\ndeveloped an automated method for generating high-quality, supervised\nmathematical datasets. The method carefully mutates existing math problems,\nensuring both diversity and validity of the newly generated problems. This is\nachieved by a neuro-symbolic data generation framework combining the intuitive\ninformalization strengths of LLMs, and the precise symbolic reasoning of math\nsolvers along with projected Markov chain Monte Carlo sampling in the\nhighly-irregular symbolic space. Empirical experiments demonstrate the high\nquality of data generated by the proposed method, and that the LLMs,\nspecifically LLaMA-2 and Mistral, when realigned with the generated data,\nsurpass their state-of-the-art counterparts.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"vJV3TQv4esiaXU1EElQKCEW0Xr-KiKKHfn_lhbfIlJg","pdfSize":"802018"}