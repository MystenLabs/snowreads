{
  "id": "2412.07067",
  "title": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems",
  "authors": "Yao Fu, Yinsicheng Jiang, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue,\n  Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Kai Zou, Edoardo\n  Ponti, Luo Mai",
  "authorsParsed": [
    [
      "Fu",
      "Yao",
      ""
    ],
    [
      "Jiang",
      "Yinsicheng",
      ""
    ],
    [
      "Huang",
      "Yeqi",
      ""
    ],
    [
      "Nie",
      "Ping",
      ""
    ],
    [
      "Lu",
      "Zhan",
      ""
    ],
    [
      "Xue",
      "Leyang",
      ""
    ],
    [
      "He",
      "Congjie",
      ""
    ],
    [
      "Sit",
      "Man-Kit",
      ""
    ],
    [
      "Xue",
      "Jilong",
      ""
    ],
    [
      "Dong",
      "Li",
      ""
    ],
    [
      "Miao",
      "Ziming",
      ""
    ],
    [
      "Zou",
      "Kai",
      ""
    ],
    [
      "Ponti",
      "Edoardo",
      ""
    ],
    [
      "Mai",
      "Luo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 00:19:28 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 26 Feb 2025 00:28:08 GMT"
    }
  ],
  "updateDate": "2025-02-27",
  "timestamp": 1733789968000,
  "abstract": "  The Mixture-of-Experts (MoE) architecture is increasingly favored for scaling\nLarge Language Models (LLMs). Its key feature, sparse activation, selectively\nactivates only a subset of parameters (experts) per token, reducing memory\nbandwidth and compute FLOPs compared to dense models. To capitalize on this,\nMoE designers leverage heterogeneous compute and memory hardware to lower\nsystem costs. However, the interaction between model sparsity and hardware\nheterogeneity introduces trade-offs in Cost, Accuracy, and Performance (CAP).\nTo address this, we introduce MoE-CAP, a benchmarking method for evaluating\nsparse MoE systems across these three dimensions. Its key innovation is a\nsparsity-aware CAP analysis model, the first to integrate cost, performance,\nand accuracy metrics into a single diagram while estimating the impact of\nsparsity on system performance. MoE-CAP helps practitioners optimize hardware\nprovisioning for an MoE model-or vice versa. MoE-CAP supports various MoE\nmodels and provides more accurate metrics than existing methods.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Distributed, Parallel, and Cluster Computing"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "Zrbo2gye_BYzet66RQXCWqBAnPnB5-yfJ9i1lsMsUek",
  "pdfSize": "542231"
}