{
  "id": "2412.03160",
  "title": "Byte BPE Tokenization as an Inverse string Homomorphism",
  "authors": "Saibo Geng, Sankalp Gambhir, Chris Wendler, Robert West",
  "authorsParsed": [
    [
      "Geng",
      "Saibo",
      ""
    ],
    [
      "Gambhir",
      "Sankalp",
      ""
    ],
    [
      "Wendler",
      "Chris",
      ""
    ],
    [
      "West",
      "Robert",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 09:38:11 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733305091000,
  "abstract": "  Tokenization is an important preprocessing step in the training and inference\nof large language models (LLMs). While there has been extensive research on the\nexpressive power of the neural achitectures used in LLMs, the impact of\ntokenization has not been well understood. In this work, we demonstrate that\ntokenization, irrespective of the algorithm used, acts as an inverse\nhomomorphism between strings and tokens. This suggests that the character space\nof the source language and the token space of the tokenized language are\nhomomorphic, preserving the structural properties of the source language.\nAdditionally, we explore the concept of proper tokenization, which refers to an\nunambiguous tokenization returned from the tokenizer. Our analysis reveals that\nthe expressiveness of neural architectures in recognizing context-free\nlanguages is not affected by tokenization.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "izgaTqHOqrbRpaF7OMK6-cyhS_Qy1UwBpTljdDqfCFU",
  "pdfSize": "269203"
}