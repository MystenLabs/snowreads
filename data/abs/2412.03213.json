{
  "id": "2412.03213",
  "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
  "authors": "Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, Minyi Guo",
  "authorsParsed": [
    [
      "Liu",
      "Guangda",
      ""
    ],
    [
      "Li",
      "Chengwei",
      ""
    ],
    [
      "Zhao",
      "Jieru",
      ""
    ],
    [
      "Zhang",
      "Chenqi",
      ""
    ],
    [
      "Guo",
      "Minyi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 10:58:27 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733309907000,
  "abstract": "  Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Performance"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "9BRx9vudMJOaiIYQoxSefGA3J-zl8cgzqu4Z4VvtFjU",
  "pdfSize": "2712440"
}