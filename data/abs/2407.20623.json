{"id":"2407.20623","title":"SharkTrack: an accurate, generalisable software for streamlining shark\n  and ray underwater video analysis","authors":"Filippo Varini, Francesco Ferretti, Jeremy Jenrette, Joel H. Gayford,\n  Mark E. Bond, Matthew J. Witt, Michael R. Heithaus, Sophie Wilday, Ben\n  Glocker","authorsParsed":[["Varini","Filippo",""],["Ferretti","Francesco",""],["Jenrette","Jeremy",""],["Gayford","Joel H.",""],["Bond","Mark E.",""],["Witt","Matthew J.",""],["Heithaus","Michael R.",""],["Wilday","Sophie",""],["Glocker","Ben",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 07:59:28 GMT"}],"updateDate":"2024-07-31","timestamp":1722326368000,"abstract":"  Elasmobranchs (sharks and rays) can be important components of marine\necosystems but are experiencing global population declines. Effective\nmonitoring of these populations is essential to their protection. Baited Remote\nUnderwater Video Stations (BRUVS) have been a key tool for monitoring, but\nrequire time-consuming manual analysis. To address these challenges, we\ndeveloped SharkTrack, an AI-enhanced BRUVS analysis software. SharkTrack uses\nConvolutional Neural Networks and Multi-Object Tracking to detect and track\nelasmobranchs and provides an annotation pipeline to manually classify\nelasmobranch species and compute MaxN, the standard metric of relative\nabundance. We tested SharkTrack on BRUVS footage from locations unseen by the\nmodel during training. SharkTrack computed MaxN with 89% accuracy over 207\nhours of footage. The semi-automatic SharkTrack pipeline required two minutes\nof manual classification per hour of video, a 97% reduction of manual BRUVS\nanalysis time compared to traditional methods, estimated conservatively at one\nhour per hour of video. Furthermore, we demonstrate SharkTrack application\nacross diverse marine ecosystems and elasmobranch species, an advancement\ncompared to previous models, which were limited to specific species or\nlocations. SharkTrack applications extend beyond BRUVS analysis, facilitating\nrapid annotation of unlabeled videos, aiding the development of further models\nto classify elasmobranch species. We provide public access to the software and\nan unprecedentedly diverse dataset, facilitating future research in an\nimportant area of marine conservation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning","Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"h_hTHgU8i9yKJtZcNg25Vm0mOjfJ2MmyjMkk0jUIImY","pdfSize":"3658398","objectId":"0x6c4d6900d0e3c62f859d0f3cb42b319ff19cefc4bc8d1014b6c7dafcfd1b44a3","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
