{"id":"2407.01456","title":"Information-Theoretic Foundations for Neural Scaling Laws","authors":"Hong Jun Jeon, Benjamin Van Roy","authorsParsed":[["Jeon","Hong Jun",""],["Van Roy","Benjamin",""]],"versions":[{"version":"v1","created":"Fri, 28 Jun 2024 02:20:54 GMT"}],"updateDate":"2024-07-02","timestamp":1719541254000,"abstract":"  Neural scaling laws aim to characterize how out-of-sample error behaves as a\nfunction of model and training dataset size. Such scaling laws guide allocation\nof a computational resources between model and data processing to minimize\nerror. However, existing theoretical support for neural scaling laws lacks\nrigor and clarity, entangling the roles of information and optimization. In\nthis work, we develop rigorous information-theoretic foundations for neural\nscaling laws. This allows us to characterize scaling laws for data generated by\na two-layer neural network of infinite width. We observe that the optimal\nrelation between data and model size is linear, up to logarithmic factors,\ncorroborating large-scale empirical investigations. Concise yet general results\nof the kind we establish may bring clarity to this topic and inform future\ninvestigations.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"a-QsWW6oirXDkMT_K7Bn85v_jMLMBxvcLSee8rQh8Uk","pdfSize":"1434154","objectId":"0x9792174536c08852309e056adaec9cabe38eb3517863d2aede04f90c7b943895","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
