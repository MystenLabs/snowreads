{
  "id": "2412.16187",
  "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
  "authors": "Minghui Liu, Tahseen Rabbani, Tony O'Halloran, Ananth Sankaralingam,\n  Mary-Anne Hartley, Brian Gravelle, Furong Huang, Cornelia Ferm\\\"uller,\n  Yiannis Aloimonos",
  "authorsParsed": [
    [
      "Liu",
      "Minghui",
      ""
    ],
    [
      "Rabbani",
      "Tahseen",
      ""
    ],
    [
      "O'Halloran",
      "Tony",
      ""
    ],
    [
      "Sankaralingam",
      "Ananth",
      ""
    ],
    [
      "Hartley",
      "Mary-Anne",
      ""
    ],
    [
      "Gravelle",
      "Brian",
      ""
    ],
    [
      "Huang",
      "Furong",
      ""
    ],
    [
      "Ferm√ºller",
      "Cornelia",
      ""
    ],
    [
      "Aloimonos",
      "Yiannis",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 06:00:27 GMT"
    },
    {
      "version": "v2",
      "created": "Tue, 24 Dec 2024 13:04:45 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1734069627000,
  "abstract": "  Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language",
    "Computer Science/Data Structures and Algorithms",
    "Computer Science/Performance"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "FXIabjxoH7Cbg9NMYhpFreaEq-KDQ0aTTUIXHxP9g4g",
  "pdfSize": "1536717"
}