{"id":"2407.10867","title":"Provable Robustness of (Graph) Neural Networks Against Data Poisoning\n  and Backdoor Attacks","authors":"Lukas Gosch and Mahalakshmi Sabanayagam and Debarghya Ghoshdastidar\n  and Stephan G\\\"unnemann","authorsParsed":[["Gosch","Lukas",""],["Sabanayagam","Mahalakshmi",""],["Ghoshdastidar","Debarghya",""],["GÃ¼nnemann","Stephan",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 16:12:51 GMT"}],"updateDate":"2024-07-16","timestamp":1721059971000,"abstract":"  Generalization of machine learning models can be severely compromised by data\npoisoning, where adversarial changes are applied to the training data, as well\nas backdoor attacks that additionally manipulate the test data. These\nvulnerabilities have led to interest in certifying (i.e., proving) that such\nchanges up to a certain magnitude do not affect test predictions. We, for the\nfirst time, certify Graph Neural Networks (GNNs) against poisoning and backdoor\nattacks targeting the node features of a given graph. Our certificates are\nwhite-box and based upon $(i)$ the neural tangent kernel, which characterizes\nthe training dynamics of sufficiently wide networks; and $(ii)$ a novel\nreformulation of the bilevel optimization problem describing poisoning as a\nmixed-integer linear program. Consequently, we leverage our framework to\nprovide fundamental insights into the role of graph structure and its\nconnectivity on the worst-case robustness behavior of convolution-based and\nPageRank-based GNNs. We note that our framework is more general and constitutes\nthe first approach to derive white-box poisoning certificates for NNs, which\ncan be of independent interest beyond graph-related tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"a05QcK0pZDQY-uklhZhCDaNmTxz0t1sJ-7Y8z_BMvHo","pdfSize":"1517669"}