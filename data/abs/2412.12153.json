{"id":"2412.12153","title":"Revisiting Weight Averaging for Model Merging","authors":"Jiho Choi, Donggyun Kim, Chanhyuk Lee, Seunghoon Hong","authorsParsed":[["Choi","Jiho",""],["Kim","Donggyun",""],["Lee","Chanhyuk",""],["Hong","Seunghoon",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 06:29:20 GMT"}],"updateDate":"2024-12-18","timestamp":1733898560000,"abstract":"  Model merging aims to build a multi-task learner by combining the parameters\nof individually fine-tuned models without additional training. While a\nstraightforward approach is to average model parameters across tasks, this\noften results in suboptimal performance due to interference among parameters\nacross tasks. In this paper, we present intriguing results that weight\naveraging implicitly induces task vectors centered around the weight averaging\nitself and that applying a low-rank approximation to these centered task\nvectors significantly improves merging performance. Our analysis shows that\ncentering the task vectors effectively separates core task-specific knowledge\nand nuisance noise within the fine-tuned parameters into the top and lower\nsingular vectors, respectively, allowing us to reduce inter-task interference\nthrough its low-rank approximation. We evaluate our method on eight image\nclassification tasks, demonstrating that it outperforms prior methods by a\nsignificant margin, narrowing the performance gap with traditional multi-task\nlearning to within 1-3%\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"ioC1SxtOLl3Zi1eq0W-5LNdpBwJpwhEkd4VD4F7saNM","pdfSize":"3345655"}