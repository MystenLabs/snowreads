{"id":"2412.06090","title":"Trust No AI: Prompt Injection Along The CIA Security Triad","authors":"Johann Rehberger (Independent Researcher, Embrace The Red)","authorsParsed":[["Rehberger","Johann","","Independent Researcher, Embrace The Red"]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 22:46:30 GMT"}],"updateDate":"2024-12-10","timestamp":1733697990000,"abstract":"  The CIA security triad - Confidentiality, Integrity, and Availability - is a\ncornerstone of data and cybersecurity. With the emergence of large language\nmodel (LLM) applications, a new class of threat, known as prompt injection, was\nfirst identified in 2022. Since then, numerous real-world vulnerabilities and\nexploits have been documented in production LLM systems, including those from\nleading vendors like OpenAI, Microsoft, Anthropic and Google. This paper\ncompiles real-world exploits and proof-of concept examples, based on the\nresearch conducted and publicly documented by the author, demonstrating how\nprompt injection undermines the CIA triad and poses ongoing risks to\ncybersecurity and AI systems at large.\n","subjects":["Computer Science/Cryptography and Security","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"COuPMlJLsxkO64RiJt2NFjo7Jd6RLEPlNkIAa3xNbSY","pdfSize":"4188986"}