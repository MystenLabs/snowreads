{"id":"2412.09037","title":"Beyond Confusion: A Fine-grained Dialectical Examination of Human\n  Activity Recognition Benchmark Datasets","authors":"Daniel Geissler, Dominique Nshimyimana, Vitor Fortes Rey, Sungho Suh,\n  Bo Zhou, Paul Lukowicz","authorsParsed":[["Geissler","Daniel",""],["Nshimyimana","Dominique",""],["Rey","Vitor Fortes",""],["Suh","Sungho",""],["Zhou","Bo",""],["Lukowicz","Paul",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 07:53:17 GMT"}],"updateDate":"2024-12-13","timestamp":1733989997000,"abstract":"  The research of machine learning (ML) algorithms for human activity\nrecognition (HAR) has made significant progress with publicly available\ndatasets. However, most research prioritizes statistical metrics over examining\nnegative sample details. While recent models like transformers have been\napplied to HAR datasets with limited success from the benchmark metrics, their\ncounterparts have effectively solved problems on similar levels with near 100%\naccuracy. This raises questions about the limitations of current approaches.\nThis paper aims to address these open questions by conducting a fine-grained\ninspection of six popular HAR benchmark datasets. We identified for some parts\nof the data, none of the six chosen state-of-the-art ML methods can correctly\nclassify, denoted as the intersect of false classifications (IFC). Analysis of\nthe IFC reveals several underlying problems, including ambiguous annotations,\nirregularities during recording execution, and misaligned transition periods.\nWe contribute to the field by quantifying and characterizing annotated data\nambiguities, providing a trinary categorization mask for dataset patching, and\nstressing potential improvements for future data collections.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eKMsQ6C46GEjU1J9FCBnUOh8emKQiyZHUAvjssdjsLM","pdfSize":"32538212"}