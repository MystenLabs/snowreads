{
  "id": "2412.14166",
  "title": "MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data",
  "authors": "Hanwen Jiang, Zexiang Xu, Desai Xie, Ziwen Chen, Haian Jin, Fujun\n  Luan, Zhixin Shu, Kai Zhang, Sai Bi, Xin Sun, Jiuxiang Gu, Qixing Huang,\n  Georgios Pavlakos, Hao Tan",
  "authorsParsed": [
    [
      "Jiang",
      "Hanwen",
      ""
    ],
    [
      "Xu",
      "Zexiang",
      ""
    ],
    [
      "Xie",
      "Desai",
      ""
    ],
    [
      "Chen",
      "Ziwen",
      ""
    ],
    [
      "Jin",
      "Haian",
      ""
    ],
    [
      "Luan",
      "Fujun",
      ""
    ],
    [
      "Shu",
      "Zhixin",
      ""
    ],
    [
      "Zhang",
      "Kai",
      ""
    ],
    [
      "Bi",
      "Sai",
      ""
    ],
    [
      "Sun",
      "Xin",
      ""
    ],
    [
      "Gu",
      "Jiuxiang",
      ""
    ],
    [
      "Huang",
      "Qixing",
      ""
    ],
    [
      "Pavlakos",
      "Georgios",
      ""
    ],
    [
      "Tan",
      "Hao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 18:59:38 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 24 Feb 2025 06:06:52 GMT"
    }
  ],
  "updateDate": "2025-02-25",
  "timestamp": 1734548378000,
  "abstract": "  We propose scaling up 3D scene reconstruction by training with synthesized\ndata. At the core of our work is MegaSynth, a procedurally generated 3D dataset\ncomprising 700K scenes - over 50 times larger than the prior real dataset DL3DV\n- dramatically scaling the training data. To enable scalable data generation,\nour key idea is eliminating semantic information, removing the need to model\ncomplex semantic priors such as object affordances and scene composition.\nInstead, we model scenes with basic spatial structures and geometry primitives,\noffering scalability. Besides, we control data complexity to facilitate\ntraining while loosely aligning it with real-world data distribution to benefit\nreal-world generalization. We explore training LRMs with both MegaSynth and\navailable real data. Experiment results show that joint training or\npre-training with MegaSynth improves reconstruction quality by 1.2 to 1.8 dB\nPSNR across diverse image domains. Moreover, models trained solely on MegaSynth\nperform comparably to those trained on real data, underscoring the low-level\nnature of 3D reconstruction. Additionally, we provide an in-depth analysis of\nMegaSynth's properties for enhancing model capability, training stability, and\ngeneralization, as well as application to other tasks.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "NQM8lhJM4rjABHdLU4k23-BfIO891TyiU4vtRNSJdZ4",
  "pdfSize": "5701527"
}