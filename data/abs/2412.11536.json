{"id":"2412.11536","title":"Let your LLM generate a few tokens and you will reduce the need for\n  retrieval","authors":"Herv\\'e D\\'ejean","authorsParsed":[["Déjean","Hervé",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 08:13:14 GMT"}],"updateDate":"2024-12-17","timestamp":1734336794000,"abstract":"  In this paper, we investigate how efficiently large language models (LLM) can\nbe trained to check whether an answer is already stored in their parametric\nmemory. We distill an LLM-as-a-judge to compute the IK (I Know) score. We found\nthat this method is particularly beneficial in the context of\nretrieval-assisted augmented generation (RAG), with a respectable accuracy of\n80%. It enables a significant reduction (more than 50%) in the number of search\nand reranking steps required for certain data sets. We have also introduced the\nIK score, which serves as a useful tool for characterising datasets by\nfacilitating the classification task. Interestingly, through the inclusion of\nresponse tokens as input, our results suggest that only about 20,000 training\nsamples are required to achieve good performance. The central element of this\nwork is the use of a teacher model - the LLM as a judge - to generate training\ndata. We also assess the robustness of the IK classifier by evaluating it with\nvarious types of teachers, including both string-based methods and LLMs, with\nthe latter providing better results.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"i08LJx6am0iXfTi_edEZkqnYaMmDr6smoNMxG5DK4Ss","pdfSize":"744130"}