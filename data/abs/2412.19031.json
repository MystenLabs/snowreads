{"id":"2412.19031","title":"Repository Structure-Aware Training Makes SLMs Better Issue Resolver","authors":"Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Bing Xie","authorsParsed":[["Ma","Zexiong",""],["An","Shengnan",""],["Lin","Zeqi",""],["Zou","Yanzhen",""],["Xie","Bing",""]],"versions":[{"version":"v1","created":"Thu, 26 Dec 2024 03:01:32 GMT"}],"updateDate":"2024-12-30","timestamp":1735182092000,"abstract":"  Language models have been applied to various software development tasks, but\nthe performance varies according to the scale of the models. Large Language\nModels (LLMs) outperform Small Language Models (SLMs) in complex tasks like\nrepository-level issue resolving, but raise concerns about privacy and cost. In\ncontrast, SLMs are more accessible but under-perform in complex tasks. In this\npaper, we introduce ReSAT (Repository Structure-Aware Training), construct\ntraining data based on a large number of issues and corresponding pull requests\nfrom open-source communities to enhance the model's understanding of repository\nstructure and issue resolving ability. We construct two types of training data:\n(1) localization training data, a multi-level progressive localization data to\nimprove code understanding and localization capability; (2) code edit training\ndata, which improves context-based code editing capability. The evaluation\nresults on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively\nenhances SLMs' issue-resolving and repository-level long-context understanding\ncapabilities.\n","subjects":["Computer Science/Software Engineering","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"7zfNVOUI2HfYBHyUZDlgcw3nDiSVhTC0uOoWSdBSb8U","pdfSize":"783699"}