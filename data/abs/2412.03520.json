{
  "id": "2412.03520",
  "title": "Seeing Beyond Views: Multi-View Driving Scene Video Generation with\n  Holistic Attention",
  "authors": "Hannan Lu, Xiaohe Wu, Shudong Wang, Xiameng Qin, Xinyu Zhang, Junyu\n  Han, Wangmeng Zuo, Ji Tao",
  "authorsParsed": [
    [
      "Lu",
      "Hannan",
      ""
    ],
    [
      "Wu",
      "Xiaohe",
      ""
    ],
    [
      "Wang",
      "Shudong",
      ""
    ],
    [
      "Qin",
      "Xiameng",
      ""
    ],
    [
      "Zhang",
      "Xinyu",
      ""
    ],
    [
      "Han",
      "Junyu",
      ""
    ],
    [
      "Zuo",
      "Wangmeng",
      ""
    ],
    [
      "Tao",
      "Ji",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 18:02:49 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 9 Dec 2024 06:58:05 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733335369000,
  "abstract": "  Generating multi-view videos for autonomous driving training has recently\ngained much attention, with the challenge of addressing both cross-view and\ncross-frame consistency. Existing methods typically apply decoupled attention\nmechanisms for spatial, temporal, and view dimensions. However, these\napproaches often struggle to maintain consistency across dimensions,\nparticularly when handling fast-moving objects that appear at different times\nand viewpoints. In this paper, we present CogDriving, a novel network designed\nfor synthesizing high-quality multi-view driving videos. CogDriving leverages a\nDiffusion Transformer architecture with holistic-4D attention modules, enabling\nsimultaneous associations across the spatial, temporal, and viewpoint\ndimensions. We also propose a lightweight controller tailored for CogDriving,\ni.e., Micro-Controller, which uses only 1.1% of the parameters of the standard\nControlNet, enabling precise control over Bird's-Eye-View layouts. To enhance\nthe generation of object instances crucial for autonomous driving, we propose a\nre-weighted learning objective, dynamically adjusting the learning weights for\nobject instances during training. CogDriving demonstrates strong performance on\nthe nuScenes validation set, achieving an FVD score of 37.8, highlighting its\nability to generate realistic driving videos. The project can be found at\nhttps://luhannan.github.io/CogDrivingPage/.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "KSOowA_QJbrNPbv1aQ4ww-9en2HAzvJcyF2AXywhDuI",
  "pdfSize": "6541569"
}