{"id":"2407.03624","title":"Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks","authors":"Dharunish Yugeswardeenoo, Kevin Zhu, Sean O'Brien","authorsParsed":[["Yugeswardeenoo","Dharunish",""],["Zhu","Kevin",""],["O'Brien","Sean",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 04:19:50 GMT"},{"version":"v2","created":"Mon, 26 Aug 2024 08:09:39 GMT"}],"updateDate":"2024-08-27","timestamp":1720066790000,"abstract":"  Although LLMs have the potential to transform many fields, they still\nunderperform humans in reasoning tasks. Existing methods induce the model to\nproduce step-by-step calculations, but this research explores the question:\nDoes making the LLM analyze the question improve its performance? We propose a\nnovel prompting strategy called Question Analysis Prompting (QAP), in which the\nmodel is prompted to explain the question in $n$ words before solving. The\nvalue of $n$ influences the length of response generated by the model. QAP is\nevaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K, AQuA,\nand SAT and commonsense dataset StrategyQA. QAP is compared with other\nstate-of-the-art prompts including Chain-of-Thought (CoT), Plan and Solve\nPrompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all\nstate-of-the-art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP\nconsistently ranks among the top-2 prompts on 75\\% of the tests. A key factor\nof QAP performance can be attributed to response length, where detailed\nresponses are beneficial when answering harder questions, but can negatively\naffect easy questions.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"j3t9ApHNdug_BXOO1UpBPqPsWFgmMPmZUKMvAKzSWyM","pdfSize":"1418151"}