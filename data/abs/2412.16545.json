{
  "id": "2412.16545",
  "title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context\n  Encoding with Full-attention-based Pre-trained Language Models",
  "authors": "Zhisong Zhang, Yan Wang, Xinting Huang, Tianqing Fang, Hongming Zhang,\n  Chenlong Deng, Shuaiyi Li, Dong Yu",
  "authorsParsed": [
    [
      "Zhang",
      "Zhisong",
      ""
    ],
    [
      "Wang",
      "Yan",
      ""
    ],
    [
      "Huang",
      "Xinting",
      ""
    ],
    [
      "Fang",
      "Tianqing",
      ""
    ],
    [
      "Zhang",
      "Hongming",
      ""
    ],
    [
      "Deng",
      "Chenlong",
      ""
    ],
    [
      "Li",
      "Shuaiyi",
      ""
    ],
    [
      "Yu",
      "Dong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 09:04:51 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734771891000,
  "abstract": "  Large language models have shown remarkable performance across a wide range\nof language tasks, owing to their exceptional capabilities in context modeling.\nThe most commonly used method of context modeling is full self-attention, as\nseen in standard decoder-only Transformers. Although powerful, this method can\nbe inefficient for long sequences and may overlook inherent input structures.\nTo address these problems, an alternative approach is parallel context\nencoding, which splits the context into sub-pieces and encodes them parallelly.\nBecause parallel patterns are not encountered during training, naively applying\nparallel encoding leads to performance degradation. However, the underlying\nreasons and potential mitigations are unclear. In this work, we provide a\ndetailed analysis of this issue and identify that unusually high attention\nentropy can be a key factor. Furthermore, we adopt two straightforward methods\nto reduce attention entropy by incorporating attention sinks and selective\nmechanisms. Experiments on various tasks reveal that these methods effectively\nlower irregular attention entropy and narrow performance gaps. We hope this\nstudy can illuminate ways to enhance context modeling mechanisms.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "1ZEUcKzFEBYflfyraKoLP5KWi8GL1E_EkC3c3DDRFno",
  "pdfSize": "1202489"
}