{"id":"2412.10509","title":"Do Large Language Models Show Biases in Causal Learning?","authors":"Maria Victoria Carro, Francisca Gauna Selasco, Denise Alejandra\n  Mester, Margarita Gonzales, Mario A. Leiva, Maria Vanina Martinez, Gerardo I.\n  Simari","authorsParsed":[["Carro","Maria Victoria",""],["Selasco","Francisca Gauna",""],["Mester","Denise Alejandra",""],["Gonzales","Margarita",""],["Leiva","Mario A.",""],["Martinez","Maria Vanina",""],["Simari","Gerardo I.",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 19:03:48 GMT"}],"updateDate":"2024-12-17","timestamp":1734116628000,"abstract":"  Causal learning is the cognitive process of developing the capability of\nmaking causal inferences based on available information, often guided by\nnormative principles. This process is prone to errors and biases, such as the\nillusion of causality, in which people perceive a causal relationship between\ntwo variables despite lacking supporting evidence. This cognitive bias has been\nproposed to underlie many societal problems, including social prejudice,\nstereotype formation, misinformation, and superstitious thinking. In this\nresearch, we investigate whether large language models (LLMs) develop causal\nillusions, both in real-world and controlled laboratory contexts of causal\nlearning and inference. To this end, we built a dataset of over 2K samples\nincluding purely correlational cases, situations with null contingency, and\ncases where temporal information excludes the possibility of causality by\nplacing the potential effect before the cause. We then prompted the models to\nmake statements or answer causal questions to evaluate their tendencies to\ninfer causation erroneously in these structured settings. Our findings show a\nstrong presence of causal illusion bias in LLMs. Specifically, in open-ended\ngeneration tasks involving spurious correlations, the models displayed bias at\nlevels comparable to, or even lower than, those observed in similar studies on\nhuman subjects. However, when faced with null-contingency scenarios or temporal\ncues that negate causal relationships, where it was required to respond on a\n0-100 scale, the models exhibited significantly higher bias. These findings\nsuggest that the models have not uniformly, consistently, or reliably\ninternalized the normative principles essential for accurate causal learning.\n","subjects":["Computer Science/Artificial Intelligence","Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3puIXZ2ScrN4dasXckc837rSx65Ly5-LhvElwCoAQFU","pdfSize":"1365066"}