{"id":"2412.07406","title":"Learning Self-Supervised Audio-Visual Representations for Sound\n  Recommendations","authors":"Sudha Krishnamurthy","authorsParsed":[["Krishnamurthy","Sudha",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 10:56:02 GMT"}],"updateDate":"2024-12-11","timestamp":1733828162000,"abstract":"  We propose a novel self-supervised approach for learning audio and visual\nrepresentations from unlabeled videos, based on their correspondence. The\napproach uses an attention mechanism to learn the relative importance of\nconvolutional features extracted at different resolutions from the audio and\nvisual streams and uses the attention features to encode the audio and visual\ninput based on their correspondence. We evaluated the representations learned\nby the model to classify audio-visual correlation as well as to recommend sound\neffects for visual scenes. Our results show that the representations generated\nby the attention model improves the correlation accuracy compared to the\nbaseline, by 18% and the recommendation accuracy by 10% for VGG-Sound, which is\na public video dataset. Additionally, audio-visual representations learned by\ntraining the attention model with cross-modal contrastive learning further\nimproves the recommendation performance, based on our evaluation using\nVGG-Sound and a more challenging dataset consisting of gameplay video\nrecordings.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Multimedia","Computer Science/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"R3F6IhYDtBJuGxoYwu5hdGt4RElfUnJZBFfJSBS1zvE","pdfSize":"12432742"}