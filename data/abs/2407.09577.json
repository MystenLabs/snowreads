{"id":"2407.09577","title":"Flash normalization: fast RMSNorm for LLMs","authors":"Nils Graef, Matthew Clapp, Andrew Wasielewski","authorsParsed":[["Graef","Nils",""],["Clapp","Matthew",""],["Wasielewski","Andrew",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 00:37:55 GMT"}],"updateDate":"2024-07-16","timestamp":1720744675000,"abstract":"  RMSNorm is used by many LLMs such as Llama, Mistral, and OpenELM. This paper\ndetails FlashNorm, which is an exact but faster implementation of RMSNorm\nfollowed by linear layers. See https://huggingface.co/open-machine/FlashNorm\nfor code and more transformer tricks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"TpNrCCMWQyBEIMi7Ui1X99C1R7proQHn-Wdrf4RHf24","pdfSize":"694659"}