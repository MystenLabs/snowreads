{"id":"2407.04118","title":"MAPO: Boosting Large Language Model Performance with Model-Adaptive\n  Prompt Optimization","authors":"Yuyan Chen, Zhihao Wen, Ge Fan, Zhengyu Chen, Wei Wu, Dayiheng Liu,\n  Zhixu Li, Bang Liu, Yanghua Xiao","authorsParsed":[["Chen","Yuyan",""],["Wen","Zhihao",""],["Fan","Ge",""],["Chen","Zhengyu",""],["Wu","Wei",""],["Liu","Dayiheng",""],["Li","Zhixu",""],["Liu","Bang",""],["Xiao","Yanghua",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 18:39:59 GMT"}],"updateDate":"2024-07-08","timestamp":1720118399000,"abstract":"  Prompt engineering, as an efficient and effective way to leverage Large\nLanguage Models (LLM), has drawn a lot of attention from the research\ncommunity. The existing research primarily emphasizes the importance of\nadapting prompts to specific tasks, rather than specific LLMs. However, a good\nprompt is not solely defined by its wording, but also binds to the nature of\nthe LLM in question. In this work, we first quantitatively demonstrate that\ndifferent prompts should be adapted to different LLMs to enhance their\ncapabilities across various downstream tasks in NLP. Then we novelly propose a\nmodel-adaptive prompt optimizer (MAPO) method that optimizes the original\nprompts for each specific LLM in downstream tasks. Extensive experiments\nindicate that the proposed method can effectively refine prompts for an LLM,\nleading to significant improvements over various downstream tasks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HYzdxFmSNte11_mt4Qr6ZVAFQzdwcWOcAqfppN1ap9Y","pdfSize":"843772"}