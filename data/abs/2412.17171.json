{"id":"2412.17171","title":"Enhancing Item Tokenization for Generative Recommendation through\n  Self-Improvement","authors":"Runjin Chen, Mingxuan Ju, Ngoc Bui, Dimosthenis Antypas, Stanley Cai,\n  Xiaopeng Wu, Leonardo Neves, Zhangyang Wang, Neil Shah, Tong Zhao","authorsParsed":[["Chen","Runjin",""],["Ju","Mingxuan",""],["Bui","Ngoc",""],["Antypas","Dimosthenis",""],["Cai","Stanley",""],["Wu","Xiaopeng",""],["Neves","Leonardo",""],["Wang","Zhangyang",""],["Shah","Neil",""],["Zhao","Tong",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 21:56:15 GMT"}],"updateDate":"2024-12-24","timestamp":1734904575000,"abstract":"  Generative recommendation systems, driven by large language models (LLMs),\npresent an innovative approach to predicting user preferences by modeling items\nas token sequences and generating recommendations in a generative manner. A\ncritical challenge in this approach is the effective tokenization of items,\nensuring that they are represented in a form compatible with LLMs. Current item\ntokenization methods include using text descriptions, numerical strings, or\nsequences of discrete tokens. While text-based representations integrate\nseamlessly with LLM tokenization, they are often too lengthy, leading to\ninefficiencies and complicating accurate generation. Numerical strings, while\nconcise, lack semantic depth and fail to capture meaningful item relationships.\nTokenizing items as sequences of newly defined tokens has gained traction, but\nit often requires external models or algorithms for token assignment. These\nexternal processes may not align with the LLM's internal pretrained\ntokenization schema, leading to inconsistencies and reduced model performance.\nTo address these limitations, we propose a self-improving item tokenization\nmethod that allows the LLM to refine its own item tokenizations during training\nprocess. Our approach starts with item tokenizations generated by any external\nmodel and periodically adjusts these tokenizations based on the LLM's learned\npatterns. Such alignment process ensures consistency between the tokenization\nand the LLM's internal understanding of the items, leading to more accurate\nrecommendations. Furthermore, our method is simple to implement and can be\nintegrated as a plug-and-play enhancement into existing generative\nrecommendation systems. Experimental results on multiple datasets and using\nvarious initial tokenization strategies demonstrate the effectiveness of our\nmethod, with an average improvement of 8\\% in recommendation performance.\n","subjects":["Computer Science/Machine Learning","Computer Science/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6WcgXS4HeMwXr_4qdhriG4NaIyBsz_5ptRDLulgczcQ","pdfSize":"1004603"}