{"id":"2407.02820","title":"Investigating the Contextualised Word Embedding Dimensions Responsible\n  for Contextual and Temporal Semantic Changes","authors":"Taichi Aida, Danushka Bollegala","authorsParsed":[["Aida","Taichi",""],["Bollegala","Danushka",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 05:42:20 GMT"}],"updateDate":"2024-07-04","timestamp":1719985340000,"abstract":"  Words change their meaning over time as well as in different contexts. The\nsense-aware contextualised word embeddings (SCWEs) such as the ones produced by\nXL-LEXEME by fine-tuning masked langauge models (MLMs) on Word-in-Context (WiC)\ndata attempt to encode such semantic changes of words within the contextualised\nword embedding (CWE) spaces. Despite the superior performance of SCWEs in\ncontextual/temporal semantic change detection (SCD) benchmarks, it remains\nunclear as to how the meaning changes are encoded in the embedding space. To\nstudy this, we compare pre-trained CWEs and their fine-tuned versions on\ncontextual and temporal semantic change benchmarks under Principal Component\nAnalysis (PCA) and Independent Component Analysis (ICA) transformations. Our\nexperimental results reveal several novel insights such as (a) although there\nexist a smaller number of axes that are responsible for semantic changes of\nwords in the pre-trained CWE space, this information gets distributed across\nall dimensions when fine-tuned, and (b) in contrast to prior work studying the\ngeometry of CWEs, we find that PCA to better represent semantic changes than\nICA. Source code is available at https://github.com/LivNLP/svp-dims .\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Pp9_gF9FC5fcBfcWIeDlRm3lKXWuwNnRC-u_YRGqWbY","pdfSize":"971862"}