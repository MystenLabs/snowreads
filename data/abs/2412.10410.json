{"id":"2412.10410","title":"GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents","authors":"Shaofei Cai, Bowei Zhang, Zihao Wang, Haowei Lin, Xiaojian Ma, Anji\n  Liu, Yitao Liang","authorsParsed":[["Cai","Shaofei",""],["Zhang","Bowei",""],["Wang","Zihao",""],["Lin","Haowei",""],["Ma","Xiaojian",""],["Liu","Anji",""],["Liang","Yitao",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 05:47:49 GMT"}],"updateDate":"2024-12-17","timestamp":1733550469000,"abstract":"  Developing agents that can follow multimodal instructions remains a\nfundamental challenge in robotics and AI. Although large-scale pre-training on\nunlabeled datasets (no language instruction) has enabled agents to learn\ndiverse behaviors, these agents often struggle with following instructions.\nWhile augmenting the dataset with instruction labels can mitigate this issue,\nacquiring such high-quality annotations at scale is impractical. To address\nthis issue, we frame the problem as a semi-supervised learning task and\nintroduce GROOT-2, a multimodal instructable agent trained using a novel\napproach that combines weak supervision with latent variable models. Our method\nconsists of two key components: constrained self-imitating, which utilizes\nlarge amounts of unlabeled demonstrations to enable the policy to learn diverse\nbehaviors, and human intention alignment, which uses a smaller set of labeled\ndemonstrations to ensure the latent space reflects human intentions. GROOT-2's\neffectiveness is validated across four diverse environments, ranging from video\ngames to robotic manipulation, demonstrating its robust multimodal\ninstruction-following capabilities.\n","subjects":["Computer Science/Artificial Intelligence","Computer Science/Machine Learning","Computer Science/Robotics"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"AWXLVuOshcLdnCSKqAksmKoX2Q8BDxhO2Fjv791M7dk","pdfSize":"14525940"}