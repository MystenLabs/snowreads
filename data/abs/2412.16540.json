{
  "id": "2412.16540",
  "title": "Prior2Posterior: Model Prior Correction for Long-Tailed Learning",
  "authors": "S Divakar Bhat, Amit More, Mudit Soni, Surbhi Agrawal",
  "authorsParsed": [
    [
      "Bhat",
      "S Divakar",
      ""
    ],
    [
      "More",
      "Amit",
      ""
    ],
    [
      "Soni",
      "Mudit",
      ""
    ],
    [
      "Agrawal",
      "Surbhi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 08:49:02 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734770942000,
  "abstract": "  Learning-based solutions for long-tailed recognition face difficulties in\ngeneralizing on balanced test datasets. Due to imbalanced data prior, the\nlearned \\textit{a posteriori} distribution is biased toward the most frequent\n(head) classes, leading to an inferior performance on the least frequent (tail)\nclasses. In general, the performance can be improved by removing such a bias by\neliminating the effect of imbalanced prior modeled using the number of class\nsamples (frequencies). We first observe that the \\textit{effective prior} on\nthe classes, learned by the model at the end of the training, can differ from\nthe empirical prior obtained using class frequencies. Thus, we propose a novel\napproach to accurately model the effective prior of a trained model using\n\\textit{a posteriori} probabilities. We propose to correct the imbalanced prior\nby adjusting the predicted \\textit{a posteriori} probabilities\n(Prior2Posterior: P2P) using the calculated prior in a post-hoc manner after\nthe training, and show that it can result in improved model performance. We\npresent theoretical analysis showing the optimality of our approach for models\ntrained with naive cross-entropy loss as well as logit adjusted loss. Our\nexperiments show that the proposed approach achieves new state-of-the-art\n(SOTA) on several benchmark datasets from the long-tail literature in the\ncategory of logit adjustment methods. Further, the proposed approach can be\nused to inspect any existing method to capture the \\textit{effective prior} and\nremove any residual bias to improve its performance, post-hoc, without model\nretraining. We also show that by using the proposed post-hoc approach, the\nperformance of many existing methods can be improved further.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "ULBj340fN6rG_ZuPzkjavEtXXHstVEeh93lGGF-Xqcg",
  "pdfSize": "1489667"
}