{
  "id": "2412.13442",
  "title": "Communication-Efficient Personalized Federal Graph Learning via Low-Rank\n  Decomposition",
  "authors": "Ruyue Liu, Rong Yin, Xiangzhen Bo, Xiaoshuai Hao, Xingrui Zhou, Yong\n  Liu, Can Ma, Weiping Wang",
  "authorsParsed": [
    [
      "Liu",
      "Ruyue",
      ""
    ],
    [
      "Yin",
      "Rong",
      ""
    ],
    [
      "Bo",
      "Xiangzhen",
      ""
    ],
    [
      "Hao",
      "Xiaoshuai",
      ""
    ],
    [
      "Zhou",
      "Xingrui",
      ""
    ],
    [
      "Liu",
      "Yong",
      ""
    ],
    [
      "Ma",
      "Can",
      ""
    ],
    [
      "Wang",
      "Weiping",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 02:26:07 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734488767000,
  "abstract": "  Federated graph learning (FGL) has gained significant attention for enabling\nheterogeneous clients to process their private graph data locally while\ninteracting with a centralized server, thus maintaining privacy. However, graph\ndata on clients are typically non-IID, posing a challenge for a single model to\nperform well across all clients. Another major bottleneck of FGL is the high\ncost of communication. To address these challenges, we propose a\ncommunication-efficient personalized federated graph learning algorithm, CEFGL.\nOur method decomposes the model parameters into low-rank generic and sparse\nprivate models. We employ a dual-channel encoder to learn sparse local\nknowledge in a personalized manner and low-rank global knowledge in a shared\nmanner. Additionally, we perform multiple local stochastic gradient descent\niterations between communication phases and integrate efficient compression\ntechniques into the algorithm. The advantage of CEFGL lies in its ability to\ncapture common and individual knowledge more precisely. By utilizing low-rank\nand sparse parameters along with compression techniques, CEFGL significantly\nreduces communication complexity. Extensive experiments demonstrate that our\nmethod achieves optimal classification accuracy in a variety of heterogeneous\nenvironments across sixteen datasets. Specifically, compared to the\nstate-of-the-art method FedStar, the proposed method (with GIN as the base\nmodel) improves accuracy by 5.64\\% on cross-datasets setting CHEM, reduces\ncommunication bits by a factor of 18.58, and reduces the communication time by\na factor of 1.65.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Distributed, Parallel, and Cluster Computing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "_l6pwFMnsLje-9naOCwv-WgyulzDeTa6pe-c5uoMxoY",
  "pdfSize": "1484097"
}