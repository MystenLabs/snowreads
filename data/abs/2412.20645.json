{
  "id": "2412.20645",
  "title": "YOLO-UniOW: Efficient Universal Open-World Object Detection",
  "authors": "Lihao Liu, Juexiao Feng, Hui Chen, Ao Wang, Lin Song, Jungong Han,\n  Guiguang Ding",
  "authorsParsed": [
    [
      "Liu",
      "Lihao",
      ""
    ],
    [
      "Feng",
      "Juexiao",
      ""
    ],
    [
      "Chen",
      "Hui",
      ""
    ],
    [
      "Wang",
      "Ao",
      ""
    ],
    [
      "Song",
      "Lin",
      ""
    ],
    [
      "Han",
      "Jungong",
      ""
    ],
    [
      "Ding",
      "Guiguang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 30 Dec 2024 01:34:14 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735522454000,
  "abstract": "  Traditional object detection models are constrained by the limitations of\nclosed-set datasets, detecting only categories encountered during training.\nWhile multimodal models have extended category recognition by aligning text and\nimage modalities, they introduce significant inference overhead due to\ncross-modality fusion and still remain restricted by predefined vocabulary,\nleaving them ineffective at handling unknown objects in open-world scenarios.\nIn this work, we introduce Universal Open-World Object Detection (Uni-OWD), a\nnew paradigm that unifies open-vocabulary and open-world object detection\ntasks. To address the challenges of this setting, we propose YOLO-UniOW, a\nnovel model that advances the boundaries of efficiency, versatility, and\nperformance. YOLO-UniOW incorporates Adaptive Decision Learning to replace\ncomputationally expensive cross-modality fusion with lightweight alignment in\nthe CLIP latent space, achieving efficient detection without compromising\ngeneralization. Additionally, we design a Wildcard Learning strategy that\ndetects out-of-distribution objects as \"unknown\" while enabling dynamic\nvocabulary expansion without the need for incremental learning. This design\nempowers YOLO-UniOW to seamlessly adapt to new categories in open-world\nenvironments. Extensive experiments validate the superiority of YOLO-UniOW,\nachieving achieving 34.6 AP and 30.0 APr on LVIS with an inference speed of\n69.6 FPS. The model also sets benchmarks on M-OWODB, S-OWODB, and nuScenes\ndatasets, showcasing its unmatched performance in open-world object detection.\nCode and models are available at https://github.com/THU-MIG/YOLO-UniOW.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "-vSFwjkaDE35NFVHmXcCRxVxs8xDUtLvMiFBU0M4bvI",
  "pdfSize": "2363951"
}