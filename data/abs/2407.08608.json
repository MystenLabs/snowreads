{"id":"2407.08608","title":"FlashAttention-3: Fast and Accurate Attention with Asynchrony and\n  Low-precision","authors":"Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani,\n  Tri Dao","authorsParsed":[["Shah","Jay",""],["Bikshandi","Ganesh",""],["Zhang","Ying",""],["Thakkar","Vijay",""],["Ramani","Pradeep",""],["Dao","Tri",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 15:44:48 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 22:15:02 GMT"}],"updateDate":"2024-07-16","timestamp":1720712688000,"abstract":"  Attention, as a core layer of the ubiquitous Transformer architecture, is the\nbottleneck for large language models and long-context applications.\nFlashAttention elaborated an approach to speed up attention on GPUs through\nminimizing memory reads/writes. However, it has yet to take advantage of new\ncapabilities present in recent hardware, with FlashAttention-2 achieving only\n35% utilization on the H100 GPU. We develop three main techniques to speed up\nattention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to\n(1) overlap overall computation and data movement via warp-specialization and\n(2) interleave block-wise matmul and softmax operations, and (3) block\nquantization and incoherent processing that leverages hardware support for FP8\nlow-precision. We demonstrate that our method, FlashAttention-3, achieves\nspeedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s\n(75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate\nthat FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a\nbaseline FP8 attention.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uH7m4_gZCjzDx6y7DGWzF6Phrm4IWaMbyxSifq0kE9s","pdfSize":"1096663"}