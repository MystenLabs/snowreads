{"id":"2412.12583","title":"Process-Supervised Reward Models for Verifying Clinical Note Generation:\n  A Scalable Approach Guided by Domain Expertise","authors":"Hanyin Wang, Chufan Gao, Qiping Xu, Bolun Liu, Guleid Hussein,\n  Hariprasad Korsapati, Mohamad El Labban, Kingsley Iheasirim, Mohamed Hassan,\n  Gokhan Anil, Brian Bartlett, Jimeng Sun","authorsParsed":[["Wang","Hanyin",""],["Gao","Chufan",""],["Xu","Qiping",""],["Liu","Bolun",""],["Hussein","Guleid",""],["Korsapati","Hariprasad",""],["Labban","Mohamad El",""],["Iheasirim","Kingsley",""],["Hassan","Mohamed",""],["Anil","Gokhan",""],["Bartlett","Brian",""],["Sun","Jimeng",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 06:24:34 GMT"},{"version":"v2","created":"Sat, 15 Feb 2025 16:54:49 GMT"}],"updateDate":"2025-02-18","timestamp":1734416674000,"abstract":"  Process-supervised reward models (PRMs), which verify large language model\n(LLM) outputs step-by-step, have achieved significant success in mathematical\nand coding problems. However, their application to other domains remains\nlargely unexplored. In this work, we train a PRM to provide step-level reward\nsignals for clinical notes generated by LLMs from patient-doctor dialogues.\nGuided by real-world clinician expertise, we carefully designed step\ndefinitions for clinical notes and utilized Gemini-Pro 1.5 to automatically\ngenerate process supervision data at scale. Our proposed PRM, trained on the\nLLaMA-3.1 8B instruct model, outperformed both Gemini-Pro 1.5 and the vanilla\noutcome-supervised reward model (ORM) in two key evaluations: (1) selecting\ngold-reference samples from error-containing ones, achieving 98.8% accuracy\n(versus 70.0% for the vanilla ORM and 93.8% for Gemini-Pro 1.5), and (2)\nselecting physician-preferred notes, achieving 56.2% accuracy (compared to\n37.5% for the vanilla ORM and 50.0% for Gemini-Pro 1.5). Additionally, we\nconducted ablation studies to determine optimal loss functions and data\nselection strategies, along with physician reader studies to explore predictors\nof downstream Best-of-N performance. Our promising results suggest the\npotential of PRMs to extend beyond the clinical domain, offering a scalable and\neffective solution for diverse generative tasks.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6n0V8jrDWUZAErSNUsfAqp9OsSmz9UjwAIZfCk_-2uQ","pdfSize":"653152"}