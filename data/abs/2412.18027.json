{
  "id": "2412.18027",
  "title": "LayerDropBack: A Universally Applicable Approach for Accelerating\n  Training of Deep Networks",
  "authors": "Evgeny Hershkovitch Neiterman and Gil Ben-Artzi",
  "authorsParsed": [
    [
      "Neiterman",
      "Evgeny Hershkovitch",
      ""
    ],
    [
      "Ben-Artzi",
      "Gil",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 22:39:41 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1734993581000,
  "abstract": "  Training very deep convolutional networks is challenging, requiring\nsignificant computational resources and time. Existing acceleration methods\noften depend on specific architectures or require network modifications. We\nintroduce LayerDropBack (LDB), a simple yet effective method to accelerate\ntraining across a wide range of deep networks. LDB introduces randomness only\nin the backward pass, maintaining the integrity of the forward pass,\nguaranteeing that the same network is used during both training and inference.\nLDB can be seamlessly integrated into the training process of any model without\naltering its architecture, making it suitable for various network topologies.\nOur extensive experiments across multiple architectures (ViT, Swin Transformer,\nEfficientNet, DLA) and datasets (CIFAR-100, ImageNet) show significant training\ntime reductions of 16.93\\% to 23.97\\%, while preserving or even enhancing model\naccuracy. Code is available at \\url{https://github.com/neiterman21/LDB}.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "K2tCb5V1I3xIaYBsWXOoyUSjLpqae5oo9ZtlWbBzDT0",
  "pdfSize": "472108"
}