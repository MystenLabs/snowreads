{"id":"2407.02489","title":"Magic Insert: Style-Aware Drag-and-Drop","authors":"Nataniel Ruiz, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael\n  Rubinstein, David E. Jacobs, Shlomi Fruchter","authorsParsed":[["Ruiz","Nataniel",""],["Li","Yuanzhen",""],["Wadhwa","Neal",""],["Pritch","Yael",""],["Rubinstein","Michael",""],["Jacobs","David E.",""],["Fruchter","Shlomi",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:59:50 GMT"}],"updateDate":"2024-07-03","timestamp":1719943190000,"abstract":"  We present Magic Insert, a method for dragging-and-dropping subjects from a\nuser-provided image into a target image of a different style in a physically\nplausible manner while matching the style of the target image. This work\nformalizes the problem of style-aware drag-and-drop and presents a method for\ntackling it by addressing two sub-problems: style-aware personalization and\nrealistic object insertion in stylized images. For style-aware personalization,\nour method first fine-tunes a pretrained text-to-image diffusion model using\nLoRA and learned text tokens on the subject image, and then infuses it with a\nCLIP representation of the target style. For object insertion, we use\nBootstrapped Domain Adaption to adapt a domain-specific photorealistic object\ninsertion model to the domain of diverse artistic styles. Overall, the method\nsignificantly outperforms traditional approaches such as inpainting. Finally,\nwe present a dataset, SubjectPlop, to facilitate evaluation and future progress\nin this area. Project page: https://magicinsert.github.io/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Graphics","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FiYn8pDMMooERbCyQOmHVTtg5XywxLS4jrHU_ivRHMw","pdfSize":"33105680"}