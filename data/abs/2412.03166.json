{"id":"2412.03166","title":"Are Explanations Helpful? A Comparative Analysis of Explainability\n  Methods in Skin Lesion Classifiers","authors":"Rosa Y. G. Paccotacya-Yanque, Alceu Bissoto, Sandra Avila","authorsParsed":[["Paccotacya-Yanque","Rosa Y. G.",""],["Bissoto","Alceu",""],["Avila","Sandra",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 09:46:41 GMT"}],"updateDate":"2024-12-05","timestamp":1733305601000,"abstract":"  Deep Learning has shown outstanding results in computer vision tasks;\nhealthcare is no exception. However, there is no straightforward way to expose\nthe decision-making process of DL models. Good accuracy is not enough for skin\ncancer predictions. Understanding the model's behavior is crucial for clinical\napplication and reliable outcomes. In this work, we identify desiderata for\nexplanations in skin-lesion models. We analyzed seven methods, four based on\npixel-attribution (Grad-CAM, Score-CAM, LIME, SHAP) and three on high-level\nconcepts (ACE, ICE, CME), for a deep neural network trained on the\nInternational Skin Imaging Collaboration Archive. Our findings indicate that\nwhile these techniques reveal biases, there is room for improving the\ncomprehensiveness of explanations to achieve transparency in skin-lesion\nmodels.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"jng_GLiOfNAAltjIq3SJ0d0iwhKzrW9pMpxxr8kn41k","pdfSize":"4960068"}