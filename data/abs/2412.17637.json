{
  "id": "2412.17637",
  "title": "SCBench: A Sports Commentary Benchmark for Video LLMs",
  "authors": "Kuangzhi Ge, Lingjun Chen, Kevin Zhang, Yulin Luo, Tianyu Shi,\n  Liaoyuan Fan, Xiang Li, Guanqun Wang, Shanghang Zhang",
  "authorsParsed": [
    [
      "Ge",
      "Kuangzhi",
      ""
    ],
    [
      "Chen",
      "Lingjun",
      ""
    ],
    [
      "Zhang",
      "Kevin",
      ""
    ],
    [
      "Luo",
      "Yulin",
      ""
    ],
    [
      "Shi",
      "Tianyu",
      ""
    ],
    [
      "Fan",
      "Liaoyuan",
      ""
    ],
    [
      "Li",
      "Xiang",
      ""
    ],
    [
      "Wang",
      "Guanqun",
      ""
    ],
    [
      "Zhang",
      "Shanghang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 15:13:56 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734966836000,
  "abstract": "  Recently, significant advances have been made in Video Large Language Models\n(Video LLMs) in both academia and industry. However, methods to evaluate and\nbenchmark the performance of different Video LLMs, especially their\nfine-grained, temporal visual capabilities, remain very limited. On one hand,\ncurrent benchmarks use relatively simple videos (e.g., subtitled movie clips)\nwhere the model can understand the entire video by processing just a few\nframes. On the other hand, their datasets lack diversity in task format,\ncomprising only QA or multi-choice QA, which overlooks the models' capacity for\ngenerating in-depth and precise texts. Sports videos, which feature intricate\nvisual information, sequential events, and emotionally charged commentary,\npresent a critical challenge for Video LLMs, making sports commentary an ideal\nbenchmarking task. Inspired by these challenges, we propose a novel task:\nsports video commentary generation, developed $\\textbf{SCBench}$ for Video\nLLMs. To construct such a benchmark, we introduce (1) $\\textbf{SCORES}$, a\nsix-dimensional metric specifically designed for our task, upon which we\npropose a GPT-based evaluation method, and (2) $\\textbf{CommentarySet}$, a\ndataset consisting of 5,775 annotated video clips and ground-truth labels\ntailored to our metric. Based on SCBench, we conduct comprehensive evaluations\non multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought\nbaseline methods. Our results found that InternVL-Chat-2 achieves the best\nperformance with 5.44, surpassing the second-best by 1.04. Our work provides a\nfresh perspective for future research, aiming to enhance models' overall\ncapabilities in complex visual understanding tasks. Our dataset will be\nreleased soon.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "irSuJTM1ceaOSYnXfN9d9ORdL2cav7zVFcIafDCnsEA",
  "pdfSize": "28251735"
}