{"id":"2407.15527","title":"Interpretable Concept-Based Memory Reasoning","authors":"David Debot (Department of Computer Science, KU Leuven), Pietro\n  Barbiero (Universit\\`a della Svizzera Italiana and University of Cambridge),\n  Francesco Giannini (Faculty of Sciences, Scuola Normale Superiore, Pisa),\n  Gabriele Ciravegna (Department of Control and Computer Engineering,\n  Politecnico di Torino), Michelangelo Diligenti (Universit\\`a di Siena),\n  Giuseppe Marra (Department of Computer Science, KU Leuven)","authorsParsed":[["Debot","David","","Department of Computer Science, KU Leuven"],["Barbiero","Pietro","","Università della Svizzera Italiana and University of Cambridge"],["Giannini","Francesco","","Faculty of Sciences, Scuola Normale Superiore, Pisa"],["Ciravegna","Gabriele","","Department of Control and Computer Engineering,\n  Politecnico di Torino"],["Diligenti","Michelangelo","","Università di Siena"],["Marra","Giuseppe","","Department of Computer Science, KU Leuven"]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 10:32:48 GMT"}],"updateDate":"2024-07-23","timestamp":1721644368000,"abstract":"  The lack of transparency in the decision-making processes of deep learning\nsystems presents a significant challenge in modern artificial intelligence\n(AI), as it impairs users' ability to rely on and verify these systems. To\naddress this challenge, Concept Bottleneck Models (CBMs) have made significant\nprogress by incorporating human-interpretable concepts into deep learning\narchitectures. This approach allows predictions to be traced back to specific\nconcept patterns that users can understand and potentially intervene on.\nHowever, existing CBMs' task predictors are not fully interpretable, preventing\na thorough analysis and any form of formal verification of their\ndecision-making process prior to deployment, thereby raising significant\nreliability concerns. To bridge this gap, we introduce Concept-based Memory\nReasoner (CMR), a novel CBM designed to provide a human-understandable and\nprovably-verifiable task prediction process. Our approach is to model each task\nprediction as a neural selection mechanism over a memory of learnable logic\nrules, followed by a symbolic evaluation of the selected rule. The presence of\nan explicit memory and the symbolic evaluation allow domain experts to inspect\nand formally verify the validity of certain global properties of interest for\nthe task prediction process. Experimental results demonstrate that CMR achieves\ncomparable accuracy-interpretability trade-offs to state-of-the-art CBMs,\ndiscovers logic rules consistent with ground truths, allows for rule\ninterventions, and allows pre-deployment verification.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fGEQ_XwyqKsZ0_VRqvtZKbs1kGlVr7Wlsk9LNI9xdbU","pdfSize":"8621179","objectId":"0xdb76fcdb8b5b5f15b3207ce63d939668dd0ac06f69e51f72169de9f5ff7a2070","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
