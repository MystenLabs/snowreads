{"id":"2407.13168","title":"SciCode: A Research Coding Benchmark Curated by Scientists","authors":"Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan,\n  Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu,\n  Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu,\n  Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu,\n  Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, Hao Peng","authorsParsed":[["Tian","Minyang",""],["Gao","Luyu",""],["Zhang","Shizhuo Dylan",""],["Chen","Xinan",""],["Fan","Cunwei",""],["Guo","Xuefei",""],["Haas","Roland",""],["Ji","Pan",""],["Krongchon","Kittithat",""],["Li","Yao",""],["Liu","Shengyan",""],["Luo","Di",""],["Ma","Yutao",""],["Tong","Hao",""],["Trinh","Kha",""],["Tian","Chenyu",""],["Wang","Zihan",""],["Wu","Bohao",""],["Xiong","Yanyu",""],["Yin","Shengzhu",""],["Zhu","Minhui",""],["Lieret","Kilian",""],["Lu","Yanxin",""],["Liu","Genglin",""],["Du","Yufeng",""],["Tao","Tianhua",""],["Press","Ofir",""],["Callan","Jamie",""],["Huerta","Eliu",""],["Peng","Hao",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 05:15:24 GMT"}],"updateDate":"2024-07-19","timestamp":1721279724000,"abstract":"  Since language models (LMs) now outperform average humans on many challenging\ntasks, it has become increasingly difficult to develop challenging,\nhigh-quality, and realistic evaluations. We address this issue by examining\nLMs' capabilities to generate code for solving real scientific research\nproblems. Incorporating input from scientists and AI researchers in 16 diverse\nnatural science sub-fields, including mathematics, physics, chemistry, biology,\nand materials science, we created a scientist-curated coding benchmark,\nSciCode. The problems in SciCode naturally factorize into multiple subproblems,\neach involving knowledge recall, reasoning, and code synthesis. In total,\nSciCode contains 338 subproblems decomposed from 80 challenging main problems.\nIt offers optional descriptions specifying useful scientific background\ninformation and scientist-annotated gold-standard solutions and test cases for\nevaluation. Claude3.5-Sonnet, the best-performing model among those tested, can\nsolve only 4.6% of the problems in the most realistic setting. We believe that\nSciCode demonstrates both contemporary LMs' progress towards becoming helpful\nscientific assistants and sheds light on the development and evaluation of\nscientific AI in the future.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Hx4m6iwD6s9_nx8_f2KO5P2o_gieRA2XsywDrsIsHUY","pdfSize":"1847172","objectId":"0x4086fa9dbdc45a456b58f61f82bad404da3ddcef283415bf40a28b1651191636","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
