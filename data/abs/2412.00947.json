{
  "id": "2412.00947",
  "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual\n  Perception of Geometric Information",
  "authors": "Ryo Kamoi, Yusen Zhang, Sarkar Snigdha Sarathi Das, Ranran Haoran\n  Zhang, Rui Zhang",
  "authorsParsed": [
    [
      "Kamoi",
      "Ryo",
      ""
    ],
    [
      "Zhang",
      "Yusen",
      ""
    ],
    [
      "Das",
      "Sarkar Snigdha Sarathi",
      ""
    ],
    [
      "Zhang",
      "Ranran Haoran",
      ""
    ],
    [
      "Zhang",
      "Rui",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 1 Dec 2024 19:46:22 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733082382000,
  "abstract": "  Errors in understanding visual information in images (i.e., visual perception\nerrors) remain a major source of mistakes in Large Vision Language Models\n(LVLMs). While further analysis is essential, there is a deficiency in datasets\nfor evaluating the visual perception of LVLMs. In this work, we introduce\nVisOnlyQA, a new dataset designed to directly evaluate the visual perception\ncapabilities of LVLMs on questions about geometric and numerical information in\nscientific figures. Our dataset enables us to analyze the visual perception of\nLVLMs for fine-grained visual information, independent of other capabilities\nsuch as reasoning. The evaluation set of VisOnlyQA includes 1,200\nmultiple-choice questions in 12 tasks on four categories of figures. We also\nprovide synthetic training data consisting of 70k instances. Our experiments on\nVisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including\nGPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in\nVisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on\nsynthetic training data demonstrates the potential for enhancing the visual\nperception of LVLMs, but observed improvements are limited to certain tasks and\nspecific models. (iii) Stronger language models improve the visual perception\nof LVLMs. In summary, our experiments suggest that both training data and model\narchitectures should be improved to enhance the visual perception capabilities\nof LVLMs. The datasets, code, and model responses are provided at\nhttps://github.com/psunlpgroup/VisOnlyQA.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "2uNCogzdFgaHVkL7PpoU5aWajq_xP5CVBaMMbCfCX-Y",
  "pdfSize": "13307152"
}