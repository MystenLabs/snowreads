{"id":"2407.15425","title":"Empirical Capacity Model for Self-Attention Neural Networks","authors":"Aki H\\\"arm\\\"a, Marcin Pietrasik, Anna Wilbik","authorsParsed":[["Härmä","Aki",""],["Pietrasik","Marcin",""],["Wilbik","Anna",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 07:02:15 GMT"},{"version":"v2","created":"Wed, 31 Jul 2024 10:27:37 GMT"}],"updateDate":"2024-08-01","timestamp":1721631735000,"abstract":"  Large pretrained self-attention neural networks, or transformers, have been\nvery successful in various tasks recently. The performance of a model on a\ngiven task depends on its ability to memorize and generalize the training data.\nLarge transformer models, which may have billions of parameters, in theory have\na huge capacity to memorize content. However, the current algorithms for the\noptimization fall short of the theoretical capacity, and the capacity is also\nhighly dependent on the content. In this paper, we focus on the memory capacity\nof these models obtained using common training algorithms and synthetic\ntraining data. Based on the results, we derive an empirical capacity model\n(ECM) for a generic transformer. The ECM can be used to design task-specific\ntransformer models with an optimal number of parameters in cases where the\ntarget memorization capability of the task can be defined.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_aN65FGyEJ-6BGoa6VpuDO9fO9hZDr3koKptSH5AlZ4","pdfSize":"4005289"}