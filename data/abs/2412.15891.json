{"id":"2412.15891","title":"TelcoLM: collecting data, adapting, and benchmarking language models for\n  the telecommunication domain","authors":"Camille Barboule, Viet-Phi Huynh, Adrien Bufort, Yoan Chabot,\n  G\\'eraldine Damnati, Gw\\'enol\\'e Lecorv\\'e","authorsParsed":[["Barboule","Camille",""],["Huynh","Viet-Phi",""],["Bufort","Adrien",""],["Chabot","Yoan",""],["Damnati","Géraldine",""],["Lecorvé","Gwénolé",""]],"versions":[{"version":"v1","created":"Fri, 20 Dec 2024 13:47:02 GMT"}],"updateDate":"2024-12-23","timestamp":1734702422000,"abstract":"  Despite outstanding processes in many tasks, Large Language Models (LLMs)\nstill lack accuracy when dealing with highly technical domains. Especially,\ntelecommunications (telco) is a particularly challenging domain due the large\namount of lexical, semantic and conceptual peculiarities. Yet, this domain\nholds many valuable use cases, directly linked to industrial needs. Hence, this\npaper studies how LLMs can be adapted to the telco domain. It reports our\neffort to (i) collect a massive corpus of domain-specific data (800M tokens,\n80K instructions), (ii) perform adaptation using various methodologies, and\n(iii) benchmark them against larger generalist models in downstream tasks that\nrequire extensive knowledge of telecommunications. Our experiments on\nLlama-2-7b show that domain-adapted models can challenge the large generalist\nmodels. They also suggest that adaptation can be restricted to a unique\ninstruction-tuning step, dicarding the need for any fine-tuning on raw texts\nbeforehand.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"hQdDctJzsBmy_E7jZTbw694KZjUeSjbaOO65LbIM7OY","pdfSize":"1104925"}