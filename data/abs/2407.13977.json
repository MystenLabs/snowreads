{"id":"2407.13977","title":"A Unified Confidence Sequence for Generalized Linear Models, with\n  Applications to Bandits","authors":"Junghyun Lee, Se-Young Yun, Kwang-Sung Jun","authorsParsed":[["Lee","Junghyun",""],["Yun","Se-Young",""],["Jun","Kwang-Sung",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 02:06:08 GMT"}],"updateDate":"2024-07-22","timestamp":1721354768000,"abstract":"  We present a unified likelihood ratio-based confidence sequence (CS) for any\n(self-concordant) generalized linear models (GLMs) that is guaranteed to be\nconvex and numerically tight. We show that this is on par or improves upon\nknown CSs for various GLMs, including Gaussian, Bernoulli, and Poisson. In\nparticular, for the first time, our CS for Bernoulli has a poly(S)-free radius\nwhere S is the norm of the unknown parameter. Our first technical novelty is\nits derivation, which utilizes a time-uniform PAC-Bayesian bound with a uniform\nprior/posterior, despite the latter being a rather unpopular choice for\nderiving CSs. As a direct application of our new CS, we propose a simple and\nnatural optimistic algorithm called OFUGLB applicable to any generalized linear\nbandits (GLB; Filippi et al. (2010)). Our analysis shows that the celebrated\noptimistic approach simultaneously attains state-of-the-art regrets for various\nself-concordant (not necessarily bounded) GLBs, and even poly(S)-free for\nbounded GLBs, including logistic bandits. The regret analysis, our second\ntechnical novelty, follows from combining our new CS with a new proof technique\nthat completely avoids the previously widely used self-concordant control lemma\n(Faury et al., 2020, Lemma 9). Finally, we verify numerically that OFUGLB\nsignificantly outperforms the prior state-of-the-art (Lee et al., 2024) for\nlogistic bandits.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PKvTx17l8nZtFQpW5xguMxIFG0Xx0O8UOLfKwg9Oe4M","pdfSize":"1637642"}