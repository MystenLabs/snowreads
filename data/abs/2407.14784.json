{"id":"2407.14784","title":"MedMAE: A Self-Supervised Backbone for Medical Imaging Tasks","authors":"Anubhav Gupta, Islam Osman, Mohamed S. Shehata, and John W. Braun","authorsParsed":[["Gupta","Anubhav",""],["Osman","Islam",""],["Shehata","Mohamed S.",""],["Braun","John W.",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 07:29:04 GMT"}],"updateDate":"2024-07-23","timestamp":1721460544000,"abstract":"  Medical imaging tasks are very challenging due to the lack of publicly\navailable labeled datasets. Hence, it is difficult to achieve high performance\nwith existing deep-learning models as they require a massive labeled dataset to\nbe trained effectively. An alternative solution is to use pre-trained models\nand fine-tune them using the medical imaging dataset. However, all existing\nmodels are pre-trained using natural images, which is a completely different\ndomain from that of medical imaging, which leads to poor performance due to\ndomain shift. To overcome these problems, we propose a large-scale unlabeled\ndataset of medical images and a backbone pre-trained using the proposed dataset\nwith a self-supervised learning technique called Masked autoencoder. This\nbackbone can be used as a pre-trained model for any medical imaging task, as it\nis trained to learn a visual representation of different types of medical\nimages. To evaluate the performance of the proposed backbone, we used four\ndifferent medical imaging tasks. The results are compared with existing\npre-trained models. These experiments show the superiority of our proposed\nbackbone in medical imaging tasks.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_a0n3In604wJSR8SduHdO1sts3DSTtTx7w_gTszikdE","pdfSize":"619769"}