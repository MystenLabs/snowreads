{
  "id": "2412.03293",
  "title": "Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and\n  Autoregression",
  "authors": "Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi\n  Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, Feifei Feng",
  "authorsParsed": [
    [
      "Wen",
      "Junjie",
      ""
    ],
    [
      "Zhu",
      "Minjie",
      ""
    ],
    [
      "Zhu",
      "Yichen",
      ""
    ],
    [
      "Tang",
      "Zhibin",
      ""
    ],
    [
      "Li",
      "Jinming",
      ""
    ],
    [
      "Zhou",
      "Zhongyi",
      ""
    ],
    [
      "Li",
      "Chengmeng",
      ""
    ],
    [
      "Liu",
      "Xiaoyu",
      ""
    ],
    [
      "Peng",
      "Yaxin",
      ""
    ],
    [
      "Shen",
      "Chaomin",
      ""
    ],
    [
      "Feng",
      "Feifei",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 13:11:38 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733317898000,
  "abstract": "  In this paper, we present DiffusionVLA, a novel framework that seamlessly\ncombines the autoregression model with the diffusion model for learning\nvisuomotor policy. Central to our approach is a next-token prediction\nobjective, enabling the model to reason effectively over the user's query in\nthe context of current observations. Subsequently, a diffusion model is\nattached to generate robust action outputs. To enhance policy learning through\nself-reasoning, we introduce a novel reasoning injection module that integrates\nreasoning phrases directly into the policy learning process. The whole\nframework is simple and flexible, making it easy to deploy and upgrade. We\nconduct extensive experiments using multiple real robots to validate the\neffectiveness of DiffusionVLA. Our tests include a challenging factory sorting\ntask, where DiffusionVLA successfully categorizes objects, including those not\nseen during training. We observe that the reasoning module makes the model\ninterpretable. It allows observers to understand the model thought process and\nidentify potential causes of policy failures. Additionally, we test\nDiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102\npreviously unseen objects. Our method demonstrates robustness to visual\nchanges, such as distractors and new backgrounds, and easily adapts to new\nembodiments. Furthermore, DiffusionVLA can follow novel instructions and retain\nconversational ability. Notably, DiffusionVLA is data-efficient and fast at\ninference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can\ntrain from scratch on less than 50 demonstrations for a complex task. Finally,\nwe scale the model from 2B to 72B parameters, showcasing improved\ngeneralization capabilities with increased model size.\n",
  "subjects": [
    "Computer Science/Robotics",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "WqVO8CFwNvKM07MuM93RvEC7IY4jKvFCck2H5gXQFTg",
  "pdfSize": "34843656"
}