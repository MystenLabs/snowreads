{"id":"2407.05216","title":"Large Language Model as an Assignment Evaluator: Insights, Feedback, and\n  Challenges in a 1000+ Student Course","authors":"Cheng-Han Chiang, Wei-Chih Chen, Chun-Yi Kuan, Chienchou Yang, Hung-yi\n  Lee","authorsParsed":[["Chiang","Cheng-Han",""],["Chen","Wei-Chih",""],["Kuan","Chun-Yi",""],["Yang","Chienchou",""],["Lee","Hung-yi",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 00:17:24 GMT"}],"updateDate":"2024-07-09","timestamp":1720311444000,"abstract":"  Using large language models (LLMs) for automatic evaluation has become an\nimportant evaluation method in NLP research. However, it is unclear whether\nthese LLM-based evaluators can be applied in real-world classrooms to assess\nstudent assignments. This empirical report shares how we use GPT-4 as an\nautomatic assignment evaluator in a university course with 1,028 students.\nBased on student responses, we find that LLM-based assignment evaluators are\ngenerally acceptable to students when students have free access to these\nLLM-based evaluators. However, students also noted that the LLM sometimes fails\nto adhere to the evaluation instructions. Additionally, we observe that\nstudents can easily manipulate the LLM-based evaluator to output specific\nstrings, allowing them to achieve high scores without meeting the assignment\nrubric. Based on student feedback and our experience, we provide several\nrecommendations for integrating LLM-based evaluators into future classrooms.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZGoWw_6RVhY1EWrTygL2Ooq7TouMdhrKeXzEmdCkvD8","pdfSize":"1184281"}
