{"id":"2407.16803","title":"Fusion and Cross-Modal Transfer for Zero-Shot Human Action Recognition","authors":"Abhi Kamboj, Anh Duy Nguyen, Minh Do","authorsParsed":[["Kamboj","Abhi",""],["Nguyen","Anh Duy",""],["Do","Minh",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 19:06:44 GMT"}],"updateDate":"2024-07-25","timestamp":1721761604000,"abstract":"  Despite living in a multi-sensory world, most AI models are limited to\ntextual and visual interpretations of human motion and behavior. Inertial\nmeasurement units (IMUs) provide a salient signal to understand human motion;\nhowever, they are challenging to use due to their uninterpretability and\nscarcity of their data. We investigate a method to transfer knowledge between\nvisual and inertial modalities using the structure of an informative joint\nrepresentation space designed for human action recognition (HAR). We apply the\nresulting Fusion and Cross-modal Transfer (FACT) method to a novel setup, where\nthe model does not have access to labeled IMU data during training and is able\nto perform HAR with only IMU data during testing. Extensive experiments on a\nwide range of RGB-IMU datasets demonstrate that FACT significantly outperforms\nexisting methods in zero-shot cross-modal transfer.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"7k2qsFDVR9zcRRUukKpChok1rpj9qOs8xY0CjsF4EnI","pdfSize":"994916","objectId":"0xe51e488b5aadfb77fb5c20e86f799ff6e906c5db17d7bcbe4bcca83d42c80b74","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
