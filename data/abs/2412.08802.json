{
  "id": "2412.08802",
  "title": "jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images",
  "authors": "Andreas Koukounas, Georgios Mastrapas, Bo Wang, Mohammad Kalim Akram,\n  Sedigheh Eslami, Michael G\\\"unther, Isabelle Mohr, Saba Sturua, Scott\n  Martens, Nan Wang, Han Xiao",
  "authorsParsed": [
    [
      "Koukounas",
      "Andreas",
      ""
    ],
    [
      "Mastrapas",
      "Georgios",
      ""
    ],
    [
      "Wang",
      "Bo",
      ""
    ],
    [
      "Akram",
      "Mohammad Kalim",
      ""
    ],
    [
      "Eslami",
      "Sedigheh",
      ""
    ],
    [
      "GÃ¼nther",
      "Michael",
      ""
    ],
    [
      "Mohr",
      "Isabelle",
      ""
    ],
    [
      "Sturua",
      "Saba",
      ""
    ],
    [
      "Martens",
      "Scott",
      ""
    ],
    [
      "Wang",
      "Nan",
      ""
    ],
    [
      "Xiao",
      "Han",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 22:28:12 GMT"
    }
  ],
  "updateDate": "2024-12-13",
  "timestamp": 1733956092000,
  "abstract": "  Contrastive Language-Image Pretraining (CLIP) is a highly effective method\nfor aligning images and texts in a shared embedding space. These models are\nwidely used for tasks such as cross-modal information retrieval and multi-modal\nunderstanding. However, CLIP models often struggle with text-only tasks,\nunderperforming compared to specialized text models. This performance disparity\nforces retrieval systems to rely on separate models for text-only and\nmulti-modal tasks. In this work, we build upon our previous model,\njina-clip-v1, by introducing a refined framework that utilizes multi-task,\nmulti-stage contrastive learning across multiple languages, coupled with an\nimproved training recipe to enhance text-only retrieval. The resulting model,\njina-clip-v2, outperforms its predecessor on text-only and multimodal tasks,\nwhile adding multilingual support, better understanding of complex visual\ndocuments and efficiency gains thanks to Matryoshka Representation Learning and\nvector truncation. The model performs comparably to the state-of-the-art in\nboth multilingual-multimodal and multilingual text retrieval benchmarks,\naddressing the challenge of unifying text-only and multi-modal retrieval\nsystems.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Information Retrieval"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "gZ_MAt_nfp67P-70yNlBsEiDrtypctDmMQSMJ_EFVI4",
  "pdfSize": "945607"
}