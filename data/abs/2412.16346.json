{
  "id": "2412.16346",
  "title": "SOUS VIDE: Cooking Visual Drone Navigation Policies in a Gaussian\n  Splatting Vacuum",
  "authors": "JunEn Low, Maximilian Adang, Javier Yu, Keiko Nagami, and Mac Schwager",
  "authorsParsed": [
    [
      "Low",
      "JunEn",
      ""
    ],
    [
      "Adang",
      "Maximilian",
      ""
    ],
    [
      "Yu",
      "Javier",
      ""
    ],
    [
      "Nagami",
      "Keiko",
      ""
    ],
    [
      "Schwager",
      "Mac",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 21:13:11 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734729191000,
  "abstract": "  We propose a new simulator, training approach, and policy architecture,\ncollectively called SOUS VIDE, for end-to-end visual drone navigation. Our\ntrained policies exhibit zero-shot sim-to-real transfer with robust real-world\nperformance using only on-board perception and computation. Our simulator,\ncalled FiGS, couples a computationally simple drone dynamics model with a high\nvisual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly\nsimulate drone flights producing photorealistic images at up to 130 fps. We use\nFiGS to collect 100k-300k observation-action pairs from an expert MPC with\nprivileged state and dynamics information, randomized over dynamics parameters\nand spatial disturbances. We then distill this expert MPC into an end-to-end\nvisuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net\nprocesses color image, optical flow and IMU data streams into low-level body\nrate and thrust commands at 20Hz onboard a drone. Crucially, SV-Net includes a\nRapid Motor Adaptation (RMA) module that adapts at runtime to variations in\ndrone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE\npolicies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in\nambient brightness, shifting or removing objects from the scene, and people\nmoving aggressively through the drone's visual field. Code, data, and\nexperiment videos can be found on our project page:\nhttps://stanfordmsl.github.io/SousVide/.\n",
  "subjects": [
    "Computer Science/Robotics",
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning",
    "Computer Science/Systems and Control",
    "Electrical Engineering and Systems Science/Systems and Control"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "GC3f0-_KbNydL35N8m49m5BezUvhfsRcF5MOkmjquwQ",
  "pdfSize": "10392216"
}