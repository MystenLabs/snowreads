{"id":"2412.14103","title":"Foundation Models Meet Low-Cost Sensors: Test-Time Adaptation for\n  Rescaling Disparity for Zero-Shot Metric Depth Estimation","authors":"R\\'emi Marsal, Alexandre Chapoutot, Philippe Xu and David Filliat","authorsParsed":[["Marsal","RÃ©mi",""],["Chapoutot","Alexandre",""],["Xu","Philippe",""],["Filliat","David",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 17:50:15 GMT"}],"updateDate":"2024-12-19","timestamp":1734544215000,"abstract":"  The recent development of foundation models for monocular depth estimation\nsuch as Depth Anything paved the way to zero-shot monocular depth estimation.\nSince it returns an affine-invariant disparity map, the favored technique to\nrecover the metric depth consists in fine-tuning the model. However, this stage\nis costly to perform because of the training but also due to the creation of\nthe dataset. It must contain images captured by the camera that will be used at\ntest time and the corresponding ground truth. Moreover, the fine-tuning may\nalso degrade the generalizing capacity of the original model. Instead, we\npropose in this paper a new method to rescale Depth Anything predictions using\n3D points provided by low-cost sensors or techniques such as low-resolution\nLiDAR, stereo camera, structure-from-motion where poses are given by an IMU.\nThus, this approach avoids fine-tuning and preserves the generalizing power of\nthe original depth estimation model while being robust to the noise of the\nsensor or of the depth model. Our experiments highlight improvements relative\nto other metric depth estimation methods and competitive results compared to\nfine-tuned approaches. Code available at\nhttps://gitlab.ensta.fr/ssh/monocular-depth-rescaling.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d2w1RK6EgEqwcAn3Ol_-3kMsWk2XYj32O5yCDhDH-lM","pdfSize":"2596248"}