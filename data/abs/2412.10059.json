{"id":"2412.10059","title":"Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric\n  Quantization and Energy-Saving Bit-Slice Sparsity","authors":"Dongyun Kam, Myeongji Yun, Sunwoo Yoo, Seungwoo Hong, Zhengya Zhang,\n  Youngjoo Lee","authorsParsed":[["Kam","Dongyun",""],["Yun","Myeongji",""],["Yoo","Sunwoo",""],["Hong","Seungwoo",""],["Zhang","Zhengya",""],["Lee","Youngjoo",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 11:44:09 GMT"}],"updateDate":"2024-12-16","timestamp":1734090249000,"abstract":"  Low bit-precisions and their bit-slice sparsity have recently been studied to\naccelerate general matrix-multiplications (GEMM) during large-scale deep neural\nnetwork (DNN) inferences. While the conventional symmetric quantization\nfacilitates low-resolution processing with bit-slice sparsity for both weight\nand activation, its accuracy loss caused by the activation's asymmetric\ndistributions cannot be acceptable, especially for large-scale DNNs. In efforts\nto mitigate this accuracy loss, recent studies have actively utilized\nasymmetric quantization for activations without requiring additional\noperations. However, the cutting-edge asymmetric quantization produces numerous\nnonzero slices that cannot be compressed and skipped by recent bit-slice GEMM\naccelerators, naturally consuming more processing energy to handle the\nquantized DNN models.\n  To simultaneously achieve high accuracy and hardware efficiency for\nlarge-scale DNN inferences, this paper proposes an Asymmetrically-Quantized\nbit-Slice GEMM (AQS-GEMM) for the first time. In contrast to the previous\nbit-slice computing, which only skips operations of zero slices, the AQS-GEMM\ncompresses frequent nonzero slices, generated by asymmetric quantization, and\nskips their operations. To increase the slice-level sparsity of activations, we\nalso introduce two algorithm-hardware co-optimization methods: a zero-point\nmanipulation and a distribution-based bit-slicing. To support the proposed\nAQS-GEMM and optimizations at the hardware-level, we newly introduce a DNN\naccelerator, Panacea, which efficiently handles sparse/dense workloads of the\ntiled AQS-GEMM to increase data reuse and utilization. Panacea supports a\nspecialized dataflow and run-length encoding to maximize data reuse and\nminimize external memory accesses, significantly improving its hardware\nefficiency. Our benchmark evaluations show Panacea outperforms existing DNN\naccelerators.\n","subjects":["Computer Science/Hardware Architecture","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KL2ejBC4bCaLmjBDNaJDvYu5R5MHTAw_BRYPg_QGcTU","pdfSize":"4139234"}