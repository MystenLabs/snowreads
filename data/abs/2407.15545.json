{"id":"2407.15545","title":"Inverted Activations","authors":"Georgii Novikov, Ivan Oseledets","authorsParsed":[["Novikov","Georgii",""],["Oseledets","Ivan",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 11:11:17 GMT"}],"updateDate":"2024-07-23","timestamp":1721646677000,"abstract":"  The scaling of neural networks with increasing data and model sizes\nnecessitates more efficient deep learning algorithms. This paper addresses the\nmemory footprint challenge in neural network training by proposing a\nmodification to the handling of activation tensors in pointwise nonlinearity\nlayers. Traditionally, these layers save the entire input tensor for the\nbackward pass, leading to substantial memory use. Our method involves saving\nthe output tensor instead, reducing the memory required when the subsequent\nlayer also saves its input tensor. This approach is particularly beneficial for\ntransformer-based architectures like GPT, BERT, Mistral, and Llama. Application\nof our method involves taken an inverse function of nonlinearity. To the best\nof our knowledge, that can not be done analitically and instead we buid an\naccurate approximations using simpler functions. Experimental results confirm\nthat our method significantly reduces memory usage without affecting training\naccuracy. The implementation is available at\nhttps://github.com/PgLoLo/optiacts.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qgvxxnrnCWXFRFB6mpNbUIegCgImmODEC2pMdF_mFgU","pdfSize":"355829"}