{"id":"2407.10655","title":"OVLW-DETR: Open-Vocabulary Light-Weighted Detection Transformer","authors":"Yu Wang, Xiangbo Su, Qiang Chen, Xinyu Zhang, Teng Xi, Kun Yao, Errui\n  Ding, Gang Zhang, Jingdong Wang","authorsParsed":[["Wang","Yu",""],["Su","Xiangbo",""],["Chen","Qiang",""],["Zhang","Xinyu",""],["Xi","Teng",""],["Yao","Kun",""],["Ding","Errui",""],["Zhang","Gang",""],["Wang","Jingdong",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 12:15:27 GMT"}],"updateDate":"2024-07-16","timestamp":1721045727000,"abstract":"  Open-vocabulary object detection focusing on detecting novel categories\nguided by natural language. In this report, we propose Open-Vocabulary\nLight-Weighted Detection Transformer (OVLW-DETR), a deployment friendly\nopen-vocabulary detector with strong performance and low latency. Building upon\nOVLW-DETR, we provide an end-to-end training recipe that transferring knowledge\nfrom vision-language model (VLM) to object detector with simple alignment. We\nalign detector with the text encoder from VLM by replacing the fixed\nclassification layer weights in detector with the class-name embeddings\nextracted from the text encoder. Without additional fusing module, OVLW-DETR is\nflexible and deployment friendly, making it easier to implement and modulate.\nimproving the efficiency of interleaved attention computation. Experimental\nresults demonstrate that the proposed approach is superior over existing\nreal-time open-vocabulary detectors on standard Zero-Shot LVIS benchmark.\nSource code and pre-trained models are available at\n[https://github.com/Atten4Vis/LW-DETR].\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"whPsPm7LP9KhK0CO7e9ySK7bAfqAycmIOh_Zs2IeUh8","pdfSize":"5764903","objectId":"0x93c8e7c3008772fc9e62104999cf614f13bfb82c06f2f596e4c1886b663c3a1c","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
