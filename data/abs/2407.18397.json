{"id":"2407.18397","title":"Gaussian Process Kolmogorov-Arnold Networks","authors":"Andrew Siyuan Chen","authorsParsed":[["Chen","Andrew Siyuan",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 21:09:20 GMT"},{"version":"v2","created":"Sat, 17 Aug 2024 08:45:38 GMT"}],"updateDate":"2024-08-20","timestamp":1721941760000,"abstract":"  In this paper, we introduce a probabilistic extension to Kolmogorov Arnold\nNetworks (KANs) by incorporating Gaussian Process (GP) as non-linear neurons,\nwhich we refer to as GP-KAN. A fully analytical approach to handling the output\ndistribution of one GP as an input to another GP is achieved by considering the\nfunction inner product of a GP function sample with the input distribution.\nThese GP neurons exhibit robust non-linear modelling capabilities while using\nfew parameters and can be easily and fully integrated in a feed-forward network\nstructure. They provide inherent uncertainty estimates to the model prediction\nand can be trained directly on the log-likelihood objective function, without\nneeding variational lower bounds or approximations. In the context of MNIST\nclassification, a model based on GP-KAN of 80 thousand parameters achieved\n98.5% prediction accuracy, compared to current state-of-the-art models with 1.5\nmillion parameters.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Fmr7Gt2AkeSRVuDgwEOy5sE5HwKQELXfTlmz5swgn6s","pdfSize":"798031","objectId":"0xa3fc498335be7722ca389b0df970a364b0145d06fe32caeb5f4115b75b3ea9be","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
