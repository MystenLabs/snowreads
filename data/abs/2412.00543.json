{
  "id": "2412.00543",
  "title": "Evaluating the Consistency of LLM Evaluators",
  "authors": "Noah Lee and Jiwoo Hong and James Thorne",
  "authorsParsed": [
    [
      "Lee",
      "Noah",
      ""
    ],
    [
      "Hong",
      "Jiwoo",
      ""
    ],
    [
      "Thorne",
      "James",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 30 Nov 2024 17:29:08 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1732987748000,
  "abstract": "  Large language models (LLMs) have shown potential as general evaluators along\nwith the evident benefits of speed and cost. While their correlation against\nhuman annotators has been widely studied, consistency as evaluators is still\nunderstudied, raising concerns about the reliability of LLM evaluators. In this\npaper, we conduct extensive studies on the two aspects of consistency in LLM\nevaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on\ndifferent scoring scales and criterion granularity with open-source and\nproprietary models. Our comprehensive analysis demonstrates that strong\nproprietary models are not necessarily consistent evaluators, highlighting the\nimportance of considering consistency in assessing the capability of LLM\nevaluators.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "70ckQoDyfG-W8uBtvDfeSBQk6Ih7yKLEx_iXTBf1ZPY",
  "pdfSize": "316112"
}