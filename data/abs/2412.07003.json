{
  "id": "2412.07003",
  "title": "Understanding Gradient Descent through the Training Jacobian",
  "authors": "Nora Belrose, Adam Scherlis",
  "authorsParsed": [
    [
      "Belrose",
      "Nora",
      ""
    ],
    [
      "Scherlis",
      "Adam",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 21:17:00 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 11 Dec 2024 09:32:05 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733779020000,
  "abstract": "  We examine the geometry of neural network training using the Jacobian of\ntrained network parameters with respect to their initial values. Our analysis\nreveals low-dimensional structure in the training process which is dependent on\nthe input data but largely independent of the labels. We find that the singular\nvalue spectrum of the Jacobian matrix consists of three distinctive regions: a\n\"chaotic\" region of values orders of magnitude greater than one, a large \"bulk\"\nregion of values extremely close to one, and a \"stable\" region of values less\nthan one. Along each bulk direction, the left and right singular vectors are\nnearly identical, indicating that perturbations to the initialization are\ncarried through training almost unchanged. These perturbations have virtually\nno effect on the network's output in-distribution, yet do have an effect far\nout-of-distribution. While the Jacobian applies only locally around a single\ninitialization, we find substantial overlap in bulk subspaces for different\nrandom seeds. Our code is available at\nhttps://github.com/EleutherAI/training-jacobian\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "hiHgd8M1rNFrb9a7kctg7tenpC3g2k5eMLQ2Lpnycww",
  "pdfSize": "705908"
}