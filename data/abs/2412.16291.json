{
  "id": "2412.16291",
  "title": "Benchmarking LLMs and SLMs for patient reported outcomes",
  "authors": "Matteo Marengo, Jarod L\\'evy, Jean-Emmanuel Bibault",
  "authorsParsed": [
    [
      "Marengo",
      "Matteo",
      ""
    ],
    [
      "LÃ©vy",
      "Jarod",
      ""
    ],
    [
      "Bibault",
      "Jean-Emmanuel",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 19:01:25 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734721285000,
  "abstract": "  LLMs have transformed the execution of numerous tasks, including those in the\nmedical domain. Among these, summarizing patient-reported outcomes (PROs) into\nconcise natural language reports is of particular interest to clinicians, as it\nenables them to focus on critical patient concerns and spend more time in\nmeaningful discussions. While existing work with LLMs like GPT-4 has shown\nimpressive results, real breakthroughs could arise from leveraging SLMs as they\noffer the advantage of being deployable locally, ensuring patient data privacy\nand compliance with healthcare regulations. This study benchmarks several SLMs\nagainst LLMs for summarizing patient-reported Q\\&A forms in the context of\nradiotherapy. Using various metrics, we evaluate their precision and\nreliability. The findings highlight both the promise and limitations of SLMs\nfor high-stakes medical tasks, fostering more efficient and privacy-preserving\nAI-driven healthcare solutions.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "MK8DGChln4eJ97CH1_EqngXQbd559e98mEYBHIDap9Y",
  "pdfSize": "569825"
}