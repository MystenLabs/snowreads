{
  "id": "2412.13577",
  "title": "Bridge then Begin Anew: Generating Target-relevant Intermediate Model\n  for Source-free Visual Emotion Adaptation",
  "authors": "Jiankun Zhu, Sicheng Zhao, Jing Jiang, Wenbo Tang, Zhaopan Xu,\n  Tingting Han, Pengfei Xu, Hongxun Yao",
  "authorsParsed": [
    [
      "Zhu",
      "Jiankun",
      ""
    ],
    [
      "Zhao",
      "Sicheng",
      ""
    ],
    [
      "Jiang",
      "Jing",
      ""
    ],
    [
      "Tang",
      "Wenbo",
      ""
    ],
    [
      "Xu",
      "Zhaopan",
      ""
    ],
    [
      "Han",
      "Tingting",
      ""
    ],
    [
      "Xu",
      "Pengfei",
      ""
    ],
    [
      "Yao",
      "Hongxun",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 07:51:35 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734508295000,
  "abstract": "  Visual emotion recognition (VER), which aims at understanding humans'\nemotional reactions toward different visual stimuli, has attracted increasing\nattention. Given the subjective and ambiguous characteristics of emotion,\nannotating a reliable large-scale dataset is hard. For reducing reliance on\ndata labeling, domain adaptation offers an alternative solution by adapting\nmodels trained on labeled source data to unlabeled target data. Conventional\ndomain adaptation methods require access to source data. However, due to\nprivacy concerns, source emotional data may be inaccessible. To address this\nissue, we propose an unexplored task: source-free domain adaptation (SFDA) for\nVER, which does not have access to source data during the adaptation process.\nTo achieve this, we propose a novel framework termed Bridge then Begin Anew\n(BBA), which consists of two steps: domain-bridged model generation (DMG) and\ntarget-related model adaptation (TMA). First, the DMG bridges cross-domain gaps\nby generating an intermediate model, avoiding direct alignment between two VER\ndatasets with significant differences. Then, the TMA begins training the target\nmodel anew to fit the target structure, avoiding the influence of\nsource-specific knowledge. Extensive experiments are conducted on six SFDA\nsettings for VER. The results demonstrate the effectiveness of BBA, which\nachieves remarkable performance gains compared with state-of-the-art SFDA\nmethods and outperforms representative unsupervised domain adaptation\napproaches.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "xZ6N8epKKm4qSmmWJ14rgBHwmSr6x6SDjebZyrW5Hes",
  "pdfSize": "1604113"
}