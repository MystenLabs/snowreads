{
  "id": "2412.03182",
  "title": "Quantitative convergence of trained quantum neural networks to a\n  Gaussian process",
  "authors": "Anderson Melchor Hernandez, Filippo Girardi, Davide Pastorello,\n  Giacomo De Palma",
  "authorsParsed": [
    [
      "Hernandez",
      "Anderson Melchor",
      ""
    ],
    [
      "Girardi",
      "Filippo",
      ""
    ],
    [
      "Pastorello",
      "Davide",
      ""
    ],
    [
      "De Palma",
      "Giacomo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 10:09:56 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733306996000,
  "abstract": "  We study quantum neural networks where the generated function is the\nexpectation value of the sum of single-qubit observables across all qubits. In\n[Girardi \\emph{et al.}, arXiv:2402.08726], it is proven that the probability\ndistributions of such generated functions converge in distribution to a\nGaussian process in the limit of infinite width for both untrained networks\nwith randomly initialized parameters and trained networks. In this paper, we\nprovide a quantitative proof of this convergence in terms of the Wasserstein\ndistance of order $1$. First, we establish an upper bound on the distance\nbetween the probability distribution of the function generated by any untrained\nnetwork with finite width and the Gaussian process with the same covariance.\nThis proof utilizes Stein's method to estimate the Wasserstein distance of\norder $1$. Next, we analyze the training dynamics of the network via gradient\nflow, proving an upper bound on the distance between the probability\ndistribution of the function generated by the trained network and the\ncorresponding Gaussian process. This proof is based on a quantitative upper\nbound on the maximum variation of a parameter during training. This bound\nimplies that for sufficiently large widths, training occurs in the lazy regime,\n\\emph{i.e.}, each parameter changes only by a small amount. While the\nconvergence result of [Girardi \\emph{et al.}, arXiv:2402.08726] holds at a\nfixed training time, our upper bounds are uniform in time and hold even as $t\n\\to \\infty$.\n",
  "subjects": [
    "Physics/Quantum Physics",
    "Physics/Mathematical Physics",
    "Mathematics/Mathematical Physics",
    "Mathematics/Probability"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "eFk8qR4SX6uK2yW0Pp5spovf5Isg_UEziUsasocMggI",
  "pdfSize": "577545"
}