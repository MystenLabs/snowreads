{
  "id": "2412.13862",
  "title": "Energy-Based Preference Model Offers Better Offline Alignment than the\n  Bradley-Terry Preference Model",
  "authors": "Yuzhong Hong, Hanshan Zhang, Junwei Bao, Hongfei Jiang, Yang Song",
  "authorsParsed": [
    [
      "Hong",
      "Yuzhong",
      ""
    ],
    [
      "Zhang",
      "Hanshan",
      ""
    ],
    [
      "Bao",
      "Junwei",
      ""
    ],
    [
      "Jiang",
      "Hongfei",
      ""
    ],
    [
      "Song",
      "Yang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 13:55:42 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734530142000,
  "abstract": "  Since the debut of DPO, it has been shown that aligning a target LLM with\nhuman preferences via the KL-constrained RLHF loss is mathematically equivalent\nto a special kind of reward modeling task. Concretely, the task requires: 1)\nusing the target LLM to parameterize the reward model, and 2) tuning the reward\nmodel so that it has a 1:1 linear relationship with the true reward. However,\nwe identify a significant issue: the DPO loss might have multiple minimizers,\nof which only one satisfies the required linearity condition. The problem\narises from a well-known issue of the underlying Bradley-Terry preference\nmodel: it does not always have a unique maximum likelihood estimator (MLE).\nConsequently,the minimizer of the RLHF loss might be unattainable because it is\nmerely one among many minimizers of the DPO loss. As a better alternative, we\npropose an energy-based model (EBM) that always has a unique MLE, inherently\nsatisfying the linearity requirement. To approximate the MLE in practice, we\npropose a contrastive loss named Energy Preference Alignment (EPA), wherein\neach positive sample is contrasted against one or more strong negatives as well\nas many free weak negatives. Theoretical properties of our EBM enable the\napproximation error of EPA to almost surely vanish when a sufficient number of\nnegatives are used. Empirically, we demonstrate that EPA consistently delivers\nbetter performance on open benchmarks compared to DPO, thereby showing the\nsuperiority of our EBM.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "XQSya4SGe7q0ICXxsQ5aVz3SqnhBNaVGZeHKA6h-9jw",
  "pdfSize": "1883488"
}