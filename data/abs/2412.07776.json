{
  "id": "2412.07776",
  "title": "Video Motion Transfer with Diffusion Transformers",
  "authors": "Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr,\n  Fabio Pizzati",
  "authorsParsed": [
    [
      "Pondaven",
      "Alexander",
      ""
    ],
    [
      "Siarohin",
      "Aliaksandr",
      ""
    ],
    [
      "Tulyakov",
      "Sergey",
      ""
    ],
    [
      "Torr",
      "Philip",
      ""
    ],
    [
      "Pizzati",
      "Fabio",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 18:59:58 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733857198000,
  "abstract": "  We propose DiTFlow, a method for transferring the motion of a reference video\nto a newly synthesized one, designed specifically for Diffusion Transformers\n(DiT). We first process the reference video with a pre-trained DiT to analyze\ncross-frame attention maps and extract a patch-wise motion signal called the\nAttention Motion Flow (AMF). We guide the latent denoising process in an\noptimization-based, training-free, manner by optimizing latents with our AMF\nloss to generate videos reproducing the motion of the reference one. We also\napply our optimization strategy to transformer positional embeddings, granting\nus a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow\nagainst recently published methods, outperforming all across multiple metrics\nand human evaluation.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "pl19AGpQxs13RZg64JOAmsv6KB-nZg8Mg08m4ZfLrfo",
  "pdfSize": "44002233"
}