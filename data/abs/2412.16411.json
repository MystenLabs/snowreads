{"id":"2412.16411","title":"Knowledge as a Breaking of Ergodicity","authors":"Yang He and Vassiliy Lubchenko","authorsParsed":[["He","Yang",""],["Lubchenko","Vassiliy",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 00:30:07 GMT"}],"updateDate":"2024-12-24","timestamp":1734741007000,"abstract":"  We construct a thermodynamic potential that can guide training of a\ngenerative model defined on a set of binary degrees of freedom. We argue that\nupon reduction in description, so as to make the generative model\ncomputationally-manageable, the potential develops multiple minima. This is\nmirrored by the emergence of multiple minima in the free energy proper of the\ngenerative model itself. The variety of training samples that employ N binary\ndegrees of freedom is ordinarily much lower than the size 2^N of the full phase\nspace. The non-represented configurations, we argue, should be thought of as\ncomprising a high-temperature phase separated by an extensive energy gap from\nthe configurations composing the training set. Thus, training amounts to\nsampling a free energy surface in the form of a library of distinct bound\nstates, each of which breaks ergodicity. The ergodicity breaking prevents\nescape into the near continuum of states comprising the high-temperature phase;\nthus it is necessary for proper functionality. It may however have the side\neffect of limiting access to patterns that were underrepresented in the\ntraining set. At the same time, the ergodicity breaking within the library\ncomplicates both learning and retrieval. As a remedy, one may concurrently\nemploy multiple generative models -- up to one model per free energy minimum.\n","subjects":["Computer Science/Artificial Intelligence","Condensed Matter/Disordered Systems and Neural Networks","Computer Science/Computational Complexity","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_E3ShgmznmWAj0t5KFYEZClrT84M9UMD0JhpE3QmTrk","pdfSize":"720238"}