{
  "id": "2412.17011",
  "title": "Robustness of Large Language Models Against Adversarial Attacks",
  "authors": "Yiyi Tao, Yixian Shen, Hang Zhang, Yanxin Shen, Lun Wang, Chuanqi Shi,\n  Shaoshuai Du",
  "authorsParsed": [
    [
      "Tao",
      "Yiyi",
      ""
    ],
    [
      "Shen",
      "Yixian",
      ""
    ],
    [
      "Zhang",
      "Hang",
      ""
    ],
    [
      "Shen",
      "Yanxin",
      ""
    ],
    [
      "Wang",
      "Lun",
      ""
    ],
    [
      "Shi",
      "Chuanqi",
      ""
    ],
    [
      "Du",
      "Shaoshuai",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 22 Dec 2024 13:21:15 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734873675000,
  "abstract": "  The increasing deployment of Large Language Models (LLMs) in various\napplications necessitates a rigorous evaluation of their robustness against\nadversarial attacks. In this paper, we present a comprehensive study on the\nrobustness of GPT LLM family. We employ two distinct evaluation methods to\nassess their resilience. The first method introduce character-level text attack\nin input prompts, testing the models on three sentiment classification\ndatasets: StanfordNLP/IMDB, Yelp Reviews, and SST-2. The second method involves\nusing jailbreak prompts to challenge the safety mechanisms of the LLMs. Our\nexperiments reveal significant variations in the robustness of these models,\ndemonstrating their varying degrees of vulnerability to both character-level\nand semantic-level adversarial attacks. These findings underscore the necessity\nfor improved adversarial training and enhanced safety mechanisms to bolster the\nrobustness of LLMs.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "Dgz6HL9HYjvE4XZM1VObMbebVteheGDd5bvlNHor5eo",
  "pdfSize": "479212"
}