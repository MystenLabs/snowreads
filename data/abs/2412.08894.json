{"id":"2412.08894","title":"SMMF: Square-Matricized Momentum Factorization for Memory-Efficient\n  Optimization","authors":"Kwangryeol Park and Seulki Lee","authorsParsed":[["Park","Kwangryeol",""],["Lee","Seulki",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 03:14:50 GMT"},{"version":"v2","created":"Fri, 13 Dec 2024 04:03:14 GMT"}],"updateDate":"2024-12-16","timestamp":1733973290000,"abstract":"  We propose SMMF (Square-Matricized Momentum Factorization), a\nmemory-efficient optimizer that reduces the memory requirement of the widely\nused adaptive learning rate optimizers, such as Adam, by up to 96%. SMMF\nenables flexible and efficient factorization of an arbitrary rank (shape) of\nthe first and second momentum tensors during optimization, based on the\nproposed square-matricization and one-time single matrix factorization. From\nthis, it becomes effectively applicable to any rank (shape) of momentum\ntensors, i.e., bias, matrix, and any rank-d tensors, prevalent in various deep\nmodel architectures, such as CNNs (high rank) and Transformers (low rank), in\ncontrast to existing memory-efficient optimizers that applies only to a\nparticular (rank-2) momentum tensor, e.g., linear layers. We conduct a regret\nbound analysis of SMMF, which shows that it converges similarly to\nnon-memory-efficient adaptive learning rate optimizers, such as AdamNC,\nproviding a theoretical basis for its competitive optimization capability. In\nour experiment, SMMF takes up to 96% less memory compared to state-of-the-art\nmemory efficient optimizers, e.g., Adafactor, CAME, and SM3, while achieving\ncomparable model performance on various CNN and Transformer tasks.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"ng1BKaHAub-D1fvxUcnhHtHxm2_uVArLDqx1z4M2NRA","pdfSize":"701423"}