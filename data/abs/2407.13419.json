{"id":"2407.13419","title":"From Words to Worlds: Compositionality for Cognitive Architectures","authors":"Ruchira Dhar and Anders S{\\o}gaard","authorsParsed":[["Dhar","Ruchira",""],["SÃ¸gaard","Anders",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 11:42:13 GMT"}],"updateDate":"2024-07-19","timestamp":1721302933000,"abstract":"  Large language models (LLMs) are very performant connectionist systems, but\ndo they exhibit more compositionality? More importantly, is that part of why\nthey perform so well? We present empirical analyses across four LLM families\n(12 models) and three task categories, including a novel task introduced below.\nOur findings reveal a nuanced relationship in learning of compositional\nstrategies by LLMs -- while scaling enhances compositional abilities,\ninstruction tuning often has a reverse effect. Such disparity brings forth some\nopen issues regarding the development and improvement of large language models\nin alignment with human cognitive capacities.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning","Computing Research Repository/Symbolic Computation"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"ACwRyui-PfdnY_HhRm7vvNWH3CHRGJo1MghLFsjnARo","pdfSize":"551720"}