{"id":"2412.05218","title":"Transformers Meet Relational Databases","authors":"Jakub Pele\\v{s}ka and Gustav \\v{S}\\'ir","authorsParsed":[["Peleška","Jakub",""],["Šír","Gustav",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 17:48:43 GMT"}],"updateDate":"2024-12-09","timestamp":1733507323000,"abstract":"  Transformer models have continuously expanded into all machine learning\ndomains convertible to the underlying sequence-to-sequence representation,\nincluding tabular data. However, while ubiquitous, this representation\nrestricts their extension to the more general case of relational databases. In\nthis paper, we introduce a modular neural message-passing scheme that closely\nadheres to the formal relational model, enabling direct end-to-end learning of\ntabular Transformers from database storage systems. We address the challenges\nof appropriate learning data representation and loading, which are critical in\nthe database setting, and compare our approach against a number of\nrepresentative models from various related fields across a significantly wide\nrange of datasets. Our results demonstrate a superior performance of this newly\nproposed class of neural architectures.\n","subjects":["Computer Science/Machine Learning","Computer Science/Databases"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kTiJjKoHwxculpGxxvekDn-E_LBZuMTXq32CoV3-ztY","pdfSize":"1424494"}