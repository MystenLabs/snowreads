{"id":"2412.11695","title":"CiTrus: Squeezing Extra Performance out of Low-data Bio-signal Transfer\n  Learning","authors":"Eloy Geenjaar and Lie Lu","authorsParsed":[["Geenjaar","Eloy",""],["Lu","Lie",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 12:15:16 GMT"},{"version":"v2","created":"Wed, 18 Dec 2024 17:40:34 GMT"}],"updateDate":"2024-12-19","timestamp":1734351316000,"abstract":"  Transfer learning for bio-signals has recently become an important technique\nto improve prediction performance on downstream tasks with small bio-signal\ndatasets. Recent works have shown that pre-training a neural network model on a\nlarge dataset (e.g. EEG) with a self-supervised task, replacing the\nself-supervised head with a linear classification head, and fine-tuning the\nmodel on different downstream bio-signal datasets (e.g., EMG or ECG) can\ndramatically improve the performance on those datasets. In this paper, we\npropose a new convolution-transformer hybrid model architecture with masked\nauto-encoding for low-data bio-signal transfer learning, introduce a\nfrequency-based masked auto-encoding task, employ a more comprehensive\nevaluation framework, and evaluate how much and when (multimodal) pre-training\nimproves fine-tuning performance. We also introduce a dramatically more\nperformant method of aligning a downstream dataset with a different temporal\nlength and sampling rate to the original pre-training dataset. Our findings\nindicate that the convolution-only part of our hybrid model can achieve\nstate-of-the-art performance on some low-data downstream tasks. The performance\nis often improved even further with our full model. In the case of\ntransformer-based models we find that pre-training especially improves\nperformance on downstream datasets, multimodal pre-training often increases\nthose gains further, and our frequency-based pre-training performs the best on\naverage for the lowest and highest data regimes.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"E8byp-z38p91pd9DdUQ458oTXw5Sp8NS_c_uL6yLx24","pdfSize":"625732"}