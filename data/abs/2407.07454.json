{"id":"2407.07454","title":"CM-DQN: A Value-Based Deep Reinforcement Learning Model to Simulate\n  Confirmation Bias","authors":"Jiacheng Shen, Lihan Feng","authorsParsed":[["Shen","Jiacheng",""],["Feng","Lihan",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 08:16:13 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 04:29:04 GMT"},{"version":"v3","created":"Thu, 8 Aug 2024 10:40:43 GMT"}],"updateDate":"2024-08-09","timestamp":1720599373000,"abstract":"  In human decision-making tasks, individuals learn through trials and\nprediction errors. When individuals learn the task, some are more influenced by\ngood outcomes, while others weigh bad outcomes more heavily. Such confirmation\nbias can lead to different learning effects. In this study, we propose a new\nalgorithm in Deep Reinforcement Learning, CM-DQN, which applies the idea of\ndifferent update strategies for positive or negative prediction errors, to\nsimulate the human decision-making process when the task's states are\ncontinuous while the actions are discrete. We test in Lunar Lander environment\nwith confirmatory, disconfirmatory bias and non-biased to observe the learning\neffects. Moreover, we apply the confirmation model in a multi-armed bandit\nproblem (environment in discrete states and discrete actions), which utilizes\nthe same idea as our proposed algorithm, as a contrast experiment to\nalgorithmically simulate the impact of different confirmation bias in\ndecision-making process. In both experiments, confirmatory bias indicates a\nbetter learning effect.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VzhlKVh-Y2M80v6DDXuuFhgUytC4hf5OBw-7_AJmW5k","pdfSize":"377988"}