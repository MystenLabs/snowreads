{"id":"2412.07282","title":"HARP: Hesitation-Aware Reframing in Transformer Inference Pass","authors":"Romain Stora\\\"i and Seung-won Hwang","authorsParsed":[["Stora√Ø","Romain",""],["Hwang","Seung-won",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 08:12:22 GMT"}],"updateDate":"2024-12-11","timestamp":1733818342000,"abstract":"  This paper aims to improve the performance of large language models by\naddressing the variable computational demands in inference steps, where some\ntokens require more computational resources than others. We present HARP, a\nsimple modification to \"off-the-shelf\" Transformer forward pass. Drawing from\nhesitation and the framing effect in decision-making, HARP selectively applies\nadditional computation when the model encounters uncertainty during token\ngeneration. Our method mimics human cognitive processes by pausing at difficult\ndecision points and reframing inputs for a different perspective. Unlike other\napproaches, HARP is model-agnostic, training-free, and easy to implement. We\nthoroughly evaluate our method across various downstream tasks and model sizes,\ndemonstrating performance improvements up to +5.16%. Notably, HARP achieves\nthese gains while maintaining inference times twice faster than beam search.\nSimple and yet with significant gains, HARP offers a practical solution for\nenhancing the performance of Transformer-based language models with minimal\ncomputational impact.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"lJcWGfcM7J8JjaT6V9_f0qZRvUBYG26vKHXoLhRb8G8","pdfSize":"466628"}