{"id":"2407.13928","title":"BiasDPO: Mitigating Bias in Language Models through Direct Preference\n  Optimization","authors":"Ahmed Allam","authorsParsed":[["Allam","Ahmed",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 22:32:20 GMT"}],"updateDate":"2024-07-22","timestamp":1721341940000,"abstract":"  Large Language Models (LLMs) have become pivotal in advancing natural\nlanguage processing, yet their potential to perpetuate biases poses significant\nconcerns. This paper introduces a new framework employing Direct Preference\nOptimization (DPO) to mitigate gender, racial, and religious biases in\nLLM-generated English text. By developing a loss function that favors less\nbiased over biased completions, our approach cultivates a preference for\nrespectful and non-discriminatory language in LLMs. We also contribute a\nmanually designed dataset for training LLMs to recognize and correct biases.\nThis dataset encompasses a diverse range of prompts paired with both biased and\nunbiased completions. Implementing this approach on the Microsoft Phi-2 model,\nwe demonstrate substantial reductions in biased outputs as our model\noutperforms the baseline model on almost all bias benchmarks. Our model also\nachieves better performance compared to other open-source models on most\nbenchmarks. By reducing biases in the language generated by the model, our\nstudy marks a significant step towards developing more ethical and socially\nresponsible LLMs. We publicly release BiasDPO dataset on HuggingFace.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XoxFNOIUI6EGTbZ2PTgBKFhbMToBzFKgEmtYKOTxAT4","pdfSize":"288449"}