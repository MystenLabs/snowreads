{"id":"2412.12968","title":"On Local Overfitting and Forgetting in Deep Neural Networks","authors":"Uri Stern, Tomer Yaacoby and Daphna Weinshall","authorsParsed":[["Stern","Uri",""],["Yaacoby","Tomer",""],["Weinshall","Daphna",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 14:53:38 GMT"},{"version":"v2","created":"Tue, 7 Jan 2025 14:45:04 GMT"}],"updateDate":"2025-01-08","timestamp":1734447218000,"abstract":"  The infrequent occurrence of overfitting in deep neural networks is\nperplexing: contrary to theoretical expectations, increasing model size often\nenhances performance in practice. But what if overfitting does occur, though\nrestricted to specific sub-regions of the data space? In this work, we propose\na novel score that captures the forgetting rate of deep models on validation\ndata. We posit that this score quantifies local overfitting: a decline in\nperformance confined to certain regions of the data space. We then show\nempirically that local overfitting occurs regardless of the presence of\ntraditional overfitting. Using the framework of deep over-parametrized linear\nmodels, we offer a certain theoretical characterization of forgotten knowledge,\nand show that it correlates with knowledge forgotten by real deep models.\nFinally, we devise a new ensemble method that aims to recover forgotten\nknowledge, relying solely on the training history of a single network. When\ncombined with self-distillation, this method enhances the performance of any\ntrained model without adding inference costs. Extensive empirical evaluations\ndemonstrate the efficacy of our method across multiple datasets, contemporary\nneural network architectures, and training protocols.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"Sl-cGdVAvHLn810PUIq6k-Y815-2GZI_6alM0SBbYgs","pdfSize":"2286620"}