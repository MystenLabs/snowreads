{"id":"2412.02576","title":"The Efficacy of Transfer-based No-box Attacks on Image Watermarking: A\n  Pragmatic Analysis","authors":"Qilong Wu, Varun Chandrasekaran","authorsParsed":[["Wu","Qilong",""],["Chandrasekaran","Varun",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 17:02:49 GMT"}],"updateDate":"2024-12-04","timestamp":1733245369000,"abstract":"  Watermarking approaches are widely used to identify if images being\ncirculated are authentic or AI-generated. Determining the robustness of image\nwatermarking methods in the ``no-box'' setting, where the attacker is assumed\nto have no knowledge about the watermarking model, is an interesting problem.\nOur main finding is that evading the no-box setting is challenging: the success\nof optimization-based transfer attacks (involving training surrogate models)\nproposed in prior work~\\cite{hu2024transfer} depends on impractical\nassumptions, including (i) aligning the architecture and training\nconfigurations of both the victim and attacker's surrogate watermarking models,\nas well as (ii) a large number of surrogate models with potentially large\ncomputational requirements. Relaxing these assumptions i.e., moving to a more\npragmatic threat model results in a failed attack, with an evasion rate at most\n$21.1\\%$. We show that when the configuration is mostly aligned, a simple\nnon-optimization attack we propose, OFT, with one single surrogate model can\nalready exceed the success of optimization-based efforts. Under the same\n$\\ell_\\infty$ norm perturbation budget of $0.25$, prior\nwork~\\citet{hu2024transfer} is comparable to or worse than OFT in $11$ out of\n$12$ configurations and has a limited advantage on the remaining one. The code\nused for all our experiments is available at\n\\url{https://github.com/Ardor-Wu/transfer}.\n","subjects":["Computer Science/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"13b_rAIPlamgVl2ykTfEtFVn6HbnzzRrk_7ti0B39EA","pdfSize":"3271398"}