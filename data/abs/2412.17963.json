{
  "id": "2412.17963",
  "title": "Path-of-Thoughts: Extracting and Following Paths for Robust Relational\n  Reasoning with Large Language Models",
  "authors": "Ge Zhang, Mohammad Ali Alomrani, Hongjian Gu, Jiaming Zhou, Yaochen\n  Hu, Bin Wang, Qun Liu, Mark Coates, Yingxue Zhang, Jianye Hao",
  "authorsParsed": [
    [
      "Zhang",
      "Ge",
      ""
    ],
    [
      "Alomrani",
      "Mohammad Ali",
      ""
    ],
    [
      "Gu",
      "Hongjian",
      ""
    ],
    [
      "Zhou",
      "Jiaming",
      ""
    ],
    [
      "Hu",
      "Yaochen",
      ""
    ],
    [
      "Wang",
      "Bin",
      ""
    ],
    [
      "Liu",
      "Qun",
      ""
    ],
    [
      "Coates",
      "Mark",
      ""
    ],
    [
      "Zhang",
      "Yingxue",
      ""
    ],
    [
      "Hao",
      "Jianye",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 20:27:12 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1734985632000,
  "abstract": "  Large language models (LLMs) possess vast semantic knowledge but often\nstruggle with complex reasoning tasks, particularly in relational reasoning\nproblems such as kinship or spatial reasoning. In this paper, we present\nPath-of-Thoughts (PoT), a novel framework designed to tackle relation reasoning\nby decomposing the task into three key stages: graph extraction, path\nidentification, and reasoning. Unlike previous approaches, PoT efficiently\nextracts a task-agnostic graph that identifies crucial entities, relations, and\nattributes within the problem context. Subsequently, PoT identifies relevant\nreasoning chains within the graph corresponding to the posed question,\nfacilitating inference of potential answers. Experimental evaluations on four\nbenchmark datasets, demanding long reasoning chains, demonstrate that PoT\nsurpasses state-of-the-art baselines by a significant margin (maximum 21.3%)\nwithout necessitating fine-tuning or extensive LLM calls. Furthermore, as\nopposed to prior neuro-symbolic methods, PoT exhibits improved resilience\nagainst LLM errors by leveraging the compositional nature of graphs.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "LwVYi3dX_gGvXnvMR5cASmVEsaA30PCHXxZ8_zgiDPE",
  "pdfSize": "950853"
}