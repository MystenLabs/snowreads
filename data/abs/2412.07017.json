{
  "id": "2412.07017",
  "title": "Asynchronous LLM Function Calling",
  "authors": "In Gim, Seung-seob Lee, Lin Zhong",
  "authorsParsed": [
    [
      "Gim",
      "In",
      ""
    ],
    [
      "Lee",
      "Seung-seob",
      ""
    ],
    [
      "Zhong",
      "Lin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 21:53:10 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733781190000,
  "abstract": "  Large language models (LLMs) use function calls to interface with external\ntools and data source. However, the current approach to LLM function calling is\ninherently synchronous, where each call blocks LLM inference, limiting LLM\noperation and concurrent function execution. In this work, we propose AsyncLM,\na system for asynchronous LLM function calling. AsyncLM improves LLM's\noperational efficiency by enabling LLMs to generate and execute function calls\nconcurrently. Instead of waiting for each call's completion, AsyncLM introduces\nan interrupt mechanism to asynchronously notify the LLM in-flight when function\ncalls return. We design an in-context protocol for function calls and\ninterrupts, provide fine-tuning strategy to adapt LLMs to the interrupt\nsemantics, and implement these mechanisms efficiently on LLM inference process.\nWe demonstrate that AsyncLM can reduce end-to-end task completion latency from\n1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks\nin the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss\nhow interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM\ninteractions.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "S2CIP0W-5BU3oZxL8008iD4XhqsUAkdiflDzdldTu9I",
  "pdfSize": "2511002"
}