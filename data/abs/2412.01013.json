{"id":"2412.01013","title":"Jacobian-Enforced Neural Networks (JENN) for Improved Data Assimilation\n  Consistency in Dynamical Models","authors":"Xiaoxu Tian","authorsParsed":[["Tian","Xiaoxu",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 00:12:51 GMT"}],"updateDate":"2024-12-03","timestamp":1733098371000,"abstract":"  Machine learning-based weather models have shown great promise in producing\naccurate forecasts but have struggled when applied to data assimilation tasks,\nunlike traditional numerical weather prediction (NWP) models. This study\nintroduces the Jacobian-Enforced Neural Network (JENN) framework, designed to\nenhance DA consistency in neural network (NN)-emulated dynamical systems. Using\nthe Lorenz 96 model as an example, the approach demonstrates improved\napplicability of NNs in DA through explicit enforcement of Jacobian\nrelationships. The NN architecture includes an input layer of 40 neurons, two\nhidden layers with 256 units each employing hyperbolic tangent activation\nfunctions, and an output layer of 40 neurons without activation.\n  The JENN framework employs a two-step training process: an initial phase\nusing standard prediction-label pairs to establish baseline forecast\ncapability, followed by a secondary phase incorporating a customized loss\nfunction to enforce accurate Jacobian relationships. This loss function\ncombines root mean square error (RMSE) between predicted and true state values\nwith additional RMSE terms for tangent linear (TL) and adjoint (AD) emulation\nresults, weighted to balance forecast accuracy and Jacobian sensitivity. To\nensure consistency, the secondary training phase uses additional pairs of TL/AD\ninputs and labels calculated from the physical models. Notably, this approach\ndoes not require starting from scratch or structural modifications to the NN,\nmaking it readily applicable to pretrained models such as GraphCast, NeuralGCM,\nPangu, or FuXi, facilitating their adaptation for DA tasks with minimal\nreconfiguration. Experimental results demonstrate that the JENN framework\npreserves nonlinear forecast performance while significantly reducing noise in\nthe TL and AD components, as well as in the overall Jacobian matrix.\n","subjects":["Computer Science/Machine Learning","Physics/Atmospheric and Oceanic Physics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"ipsR6Jy0XsG4pVmh-hBC3BeOSzjPK-qiU-qlqB1RmNs","pdfSize":"2165239"}