{"id":"2412.05155","title":"Multimodal Fact-Checking with Vision Language Models: A Probing\n  Classifier based Solution with Embedding Strategies","authors":"Recep Firat Cekinel, Pinar Karagoz, Cagri Coltekin","authorsParsed":[["Cekinel","Recep Firat",""],["Karagoz","Pinar",""],["Coltekin","Cagri",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 16:13:19 GMT"}],"updateDate":"2024-12-09","timestamp":1733501599000,"abstract":"  This study evaluates the effectiveness of Vision Language Models (VLMs) in\nrepresenting and utilizing multimodal content for fact-checking. To be more\nspecific, we investigate whether incorporating multimodal content improves\nperformance compared to text-only models and how well VLMs utilize text and\nimage information to enhance misinformation detection. Furthermore we propose a\nprobing classifier based solution using VLMs. Our approach extracts embeddings\nfrom the last hidden layer of selected VLMs and inputs them into a neural\nprobing classifier for multi-class veracity classification. Through a series of\nexperiments on two fact-checking datasets, we demonstrate that while\nmultimodality can enhance performance, fusing separate embeddings from text and\nimage encoders yielded superior results compared to using VLM embeddings.\nFurthermore, the proposed neural classifier significantly outperformed KNN and\nSVM baselines in leveraging extracted embeddings, highlighting its\neffectiveness for multimodal fact-checking.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"ZWCFJ3-S4KLTp46b-jTBONOJr71D2UxdFZYCZmIryCI","pdfSize":"10341943"}