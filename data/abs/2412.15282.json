{"id":"2412.15282","title":"A Systematic Examination of Preference Learning through the Lens of\n  Instruction-Following","authors":"Joongwon Kim, Anirudh Goyal, Aston Zhang, Bo Xiong, Rui Hou, Melanie\n  Kambadur, Dhruv Mahajan, Hannaneh Hajishirzi, Liang Tan","authorsParsed":[["Kim","Joongwon",""],["Goyal","Anirudh",""],["Zhang","Aston",""],["Xiong","Bo",""],["Hou","Rui",""],["Kambadur","Melanie",""],["Mahajan","Dhruv",""],["Hajishirzi","Hannaneh",""],["Tan","Liang",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 15:38:39 GMT"}],"updateDate":"2024-12-23","timestamp":1734536319000,"abstract":"  Preference learning is a widely adopted post-training technique that aligns\nlarge language models (LLMs) to human preferences and improves specific\ndownstream task capabilities. In this work we systematically investigate how\nspecific attributes of preference datasets affect the alignment and downstream\nperformance of LLMs in instruction-following tasks. We use a novel synthetic\ndata generation pipeline to generate 48,000 unique instruction-following\nprompts with combinations of 23 verifiable constraints that enable fine-grained\nand automated quality assessments of model responses. With our synthetic\nprompts, we use two preference dataset curation methods - rejection sampling\n(RS) and Monte Carlo Tree Search (MCTS) - to obtain pairs of (chosen, rejected)\nresponses. Then, we perform experiments investigating the effects of (1) the\npresence of shared prefixes between the chosen and rejected responses, (2) the\ncontrast and quality of the chosen, rejected responses and (3) the complexity\nof the training prompts. Our experiments reveal that shared prefixes in\npreference pairs, as generated by MCTS, provide marginal but consistent\nimprovements and greater stability across challenging training configurations.\nHigh-contrast preference pairs generally outperform low-contrast pairs;\nhowever, combining both often yields the best performance by balancing\ndiversity and learning efficiency. Additionally, training on prompts of\nmoderate difficulty leads to better generalization across tasks, even for more\ncomplex evaluation scenarios, compared to overly challenging prompts. Our\nfindings provide actionable insights into optimizing preference data curation\nfor instruction-following tasks, offering a scalable and effective framework\nfor enhancing LLM training and alignment.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Information Retrieval"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"ACLrxLRxrJ4j57rZE7OhpF5tEZ9ijMiY-gLSkVcZDhs","pdfSize":"1224796"}