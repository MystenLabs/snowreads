{"id":"2412.01114","title":"Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with\n  Demonstrations","authors":"Cevahir Koprulu, Po-han Li, Tianyu Qiu, Ruihan Zhao, Tyler\n  Westenbroek, David Fridovich-Keil, Sandeep Chinchali, Ufuk Topcu","authorsParsed":[["Koprulu","Cevahir",""],["Li","Po-han",""],["Qiu","Tianyu",""],["Zhao","Ruihan",""],["Westenbroek","Tyler",""],["Fridovich-Keil","David",""],["Chinchali","Sandeep",""],["Topcu","Ufuk",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 04:37:12 GMT"}],"updateDate":"2024-12-03","timestamp":1733114232000,"abstract":"  Many continuous control problems can be formulated as sparse-reward\nreinforcement learning (RL) tasks. In principle, online RL methods can\nautomatically explore the state space to solve each new task. However,\ndiscovering sequences of actions that lead to a non-zero reward becomes\nexponentially more difficult as the task horizon increases. Manually shaping\nrewards can accelerate learning for a fixed task, but it is an arduous process\nthat must be repeated for each new environment. We introduce a systematic\nreward-shaping framework that distills the information contained in 1) a\ntask-agnostic prior data set and 2) a small number of task-specific expert\ndemonstrations, and then uses these priors to synthesize dense dynamics-aware\nrewards for the given task. This supervision substantially accelerates learning\nin our experiments, and we provide analysis demonstrating how the approach can\neffectively guide online learning agents to faraway goals.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GlgsacuoImDXIgtBhbMlG-wE-AxhapAaysokyX8kVBs","pdfSize":"1439838"}