{
  "id": "2412.18040",
  "title": "Theoretical Constraints on the Expressive Power of $\\mathsf{RoPE}$-based\n  Tensor Attention Transformers",
  "authors": "Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Mingda Wan",
  "authorsParsed": [
    [
      "Li",
      "Xiaoyu",
      ""
    ],
    [
      "Liang",
      "Yingyu",
      ""
    ],
    [
      "Shi",
      "Zhenmei",
      ""
    ],
    [
      "Song",
      "Zhao",
      ""
    ],
    [
      "Wan",
      "Mingda",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 23:26:07 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1734996367000,
  "abstract": "  Tensor Attention extends traditional attention mechanisms by capturing\nhigh-order correlations across multiple modalities, addressing the limitations\nof classical matrix-based attention. Meanwhile, Rotary Position Embedding\n($\\mathsf{RoPE}$) has shown superior performance in encoding positional\ninformation in long-context scenarios, significantly enhancing transformer\nmodels' expressiveness. Despite these empirical successes, the theoretical\nlimitations of these technologies remain underexplored. In this study, we\nanalyze the circuit complexity of Tensor Attention and $\\mathsf{RoPE}$-based\nTensor Attention, showing that with polynomial precision, constant-depth\nlayers, and linear or sublinear hidden dimension, they cannot solve fixed\nmembership problems or $(A_{F,r})^*$ closure problems, under the assumption\nthat $\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$. These findings highlight a gap between\nthe empirical performance and theoretical constraints of Tensor Attention and\n$\\mathsf{RoPE}$-based Tensor Attention Transformers, offering insights that\ncould guide the development of more theoretically grounded approaches to\nTransformer model design and scaling.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computational Complexity",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "WiomWgHBr97-tUKUS0k0xImkB-lhxIOkleAdle5Ewfo",
  "pdfSize": "334521"
}