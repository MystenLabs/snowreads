{"id":"2407.11722","title":"Exploring Quantization for Efficient Pre-Training of Transformer\n  Language Models","authors":"Kamran Chitsaz, Quentin Fournier, Gon\\c{c}alo Mordido, Sarath Chandar","authorsParsed":[["Chitsaz","Kamran",""],["Fournier","Quentin",""],["Mordido","Gon√ßalo",""],["Chandar","Sarath",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 13:42:09 GMT"}],"updateDate":"2024-07-17","timestamp":1721137329000,"abstract":"  The increasing scale of Transformer models has led to an increase in their\npre-training computational requirements. While quantization has proven to be\neffective after pre-training and during fine-tuning, applying quantization in\nTransformers during pre-training has remained largely unexplored at scale for\nlanguage modeling. This study aims to explore the impact of quantization for\nefficient pre-training of Transformers, with a focus on linear layer\ncomponents. By systematically applying straightforward linear quantization to\nweights, activations, gradients, and optimizer states, we assess its effects on\nmodel efficiency, stability, and performance during training. By offering a\ncomprehensive recipe of effective quantization strategies to be applied during\nthe pre-training of Transformers, we promote high training efficiency from\nscratch while retaining language modeling ability. Code is available at\nhttps://github.com/chandar-lab/EfficientLLMs.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wng7-pzqb_jMljjpa1q1AjQqUx_O50e54E26OkqMo_E","pdfSize":"2705456"}