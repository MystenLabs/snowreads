{"id":"2412.06136","title":"AIDE: Task-Specific Fine Tuning with Attribute Guided Multi-Hop Data\n  Expansion","authors":"Jiayu Li, Xuan Zhu, Fang Liu, Yanjun Qi","authorsParsed":[["Li","Jiayu",""],["Zhu","Xuan",""],["Liu","Fang",""],["Qi","Yanjun",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 01:39:16 GMT"}],"updateDate":"2024-12-10","timestamp":1733708356000,"abstract":"  Fine-tuning large language models (LLMs) for specific tasks requires\nhigh-quality, diverse training data relevant to the task. Recent research has\nleveraged LLMs to synthesize training data, but existing approaches either\ndepend on large seed datasets or struggle to ensure both task relevance and\ndata diversity in the generated outputs. To address these challenges, we\npropose AIDE, a novel data synthesis framework that uses a multi-hop process to\nexpand 10 seed data points while ensuring diversity and task relevance. AIDE\nextracts the main topic and key knowledge attributes from the seed data to\nguide the synthesis process. In each subsequent hop, it extracts the topic and\nattributes from the newly generated data and continues guided synthesis. This\nprocess repeats for a total of K hops. To prevent irrelevant data generation as\nthe hop depth increases, AIDE incorporates a residual connection mechanism and\nuses self-reflection to improve data quality. Our empirical results demonstrate\nthat fine-tuning Mistral-7B, Llama-3.1-8B and Llama-3.2-3B with AIDE achieves\nmore than 10% accuracy improvements over the base models across 13 tasks from 5\ndifferent benchmarks, while outperforming the models fine-tuned with\nstate-of-the-art data synthesis methods like Evol-Instruct, DataTune and\nPrompt2Model.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"5RpVTf-geyR81eEp4yGigxgRNFuCgxEDdKoNrZgTfRY","pdfSize":"1023396"}