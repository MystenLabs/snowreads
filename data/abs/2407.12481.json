{"id":"2407.12481","title":"Pretraining Data and Tokenizer for Indic LLM","authors":"Rahul Kumar, Shubham Kakde, Divyansh Rajput, Daud Ibrahim, Rishabh\n  Nahata, Pidathala Sowjanya, Deepak Kumar","authorsParsed":[["Kumar","Rahul",""],["Kakde","Shubham",""],["Rajput","Divyansh",""],["Ibrahim","Daud",""],["Nahata","Rishabh",""],["Sowjanya","Pidathala",""],["Kumar","Deepak",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 11:06:27 GMT"}],"updateDate":"2024-07-18","timestamp":1721214387000,"abstract":"  We present a novel approach to data preparation for developing multilingual\nIndic large language model. Our meticulous data acquisition spans open-source\nand proprietary sources, including Common Crawl, Indic books, news articles,\nand Wikipedia, ensuring a diverse and rich linguistic representation. For each\nIndic language, we design a custom preprocessing pipeline to effectively\neliminate redundant and low-quality text content. Additionally, we perform\ndeduplication on Common Crawl data to address the redundancy present in 70% of\nthe crawled web pages. This study focuses on developing high-quality data,\noptimizing tokenization for our multilingual dataset for Indic large language\nmodels with 3B and 7B parameters, engineered for superior performance in Indic\nlanguages. We introduce a novel multilingual tokenizer training strategy,\ndemonstrating our custom-trained Indic tokenizer outperforms the\nstate-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word\nratio for Indic languages.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rL_N4pAB3ULzuxzRFIEujxkJGiuC3JdjLtqT5cLiMQ4","pdfSize":"168712"}