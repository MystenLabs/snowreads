{"id":"2407.04969","title":"EVA-Score: Evaluation of Long-form Summarization on Informativeness\n  through Extraction and Validation","authors":"Yuchen Fan, Xin Zhong, Chengsi Wang, Gaoche Wu, Bowen Zhou","authorsParsed":[["Fan","Yuchen",""],["Zhong","Xin",""],["Wang","Chengsi",""],["Wu","Gaoche",""],["Zhou","Bowen",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 06:02:38 GMT"}],"updateDate":"2024-07-09","timestamp":1720245758000,"abstract":"  Summarization is a fundamental task in natural language processing (NLP) and\nsince large language models (LLMs), such as GPT-4 and Claude, come out,\nincreasing attention has been paid to long-form summarization whose input\nsequences are much longer, indicating more information contained.\n  The current evaluation metrics either use similarity-based metrics like ROUGE\nand BERTScore which rely on similarity and fail to consider informativeness or\nLLM-based metrics, lacking quantitative analysis of information richness and\nare rather subjective.\n  In this paper, we propose a new evaluation metric called EVA-Score using\nAtomic Fact Chain Generation and Document-level Relation Extraction together to\nautomatically calculate the informativeness and give a definite number as an\ninformation score. Experiment results show that our metric shows a\nstate-of-the-art correlation with humans. We also re-evaluate the performance\nof LLMs on long-form summarization comprehensively from the information aspect,\nforecasting future ways to use LLMs for long-form summarization.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"cJLvIuXEy2v2V5VZ7ngYK8o0HHrHR8VhJDNcKVNCx3g","pdfSize":"861122"}
