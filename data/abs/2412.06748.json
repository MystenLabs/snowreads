{
  "id": "2412.06748",
  "title": "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language\n  Models",
  "authors": "Neel Jain, Aditya Shrivastava, Chenyang Zhu, Daben Liu, Alfy Samuel,\n  Ashwinee Panda, Anoop Kumar, Micah Goldblum, Tom Goldstein",
  "authorsParsed": [
    [
      "Jain",
      "Neel",
      ""
    ],
    [
      "Shrivastava",
      "Aditya",
      ""
    ],
    [
      "Zhu",
      "Chenyang",
      ""
    ],
    [
      "Liu",
      "Daben",
      ""
    ],
    [
      "Samuel",
      "Alfy",
      ""
    ],
    [
      "Panda",
      "Ashwinee",
      ""
    ],
    [
      "Kumar",
      "Anoop",
      ""
    ],
    [
      "Goldblum",
      "Micah",
      ""
    ],
    [
      "Goldstein",
      "Tom",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 18:40:44 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733769644000,
  "abstract": "  A key component of building safe and reliable language models is enabling the\nmodels to appropriately refuse to follow certain instructions or answer certain\nquestions. We may want models to output refusal messages for various categories\nof user queries, for example, ill-posed questions, instructions for committing\nillegal acts, or queries which require information past the model's knowledge\nhorizon. Engineering models that refuse to answer such questions is complicated\nby the fact that an individual may want their model to exhibit varying levels\nof sensitivity for refusing queries of various categories, and different users\nmay want different refusal rates. The current default approach involves\ntraining multiple models with varying proportions of refusal messages from each\ncategory to achieve the desired refusal rates, which is computationally\nexpensive and may require training a new model to accommodate each user's\ndesired preference over refusal rates. To address these challenges, we propose\nrefusal tokens, one such token for each refusal category or a single refusal\ntoken, which are prepended to the model's responses during training. We then\nshow how to increase or decrease the probability of generating the refusal\ntoken for each category during inference to steer the model's refusal behavior.\nRefusal tokens enable controlling a single model's refusal rates without the\nneed of any further fine-tuning, but only by selectively intervening during\ngeneration.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "tVAGCRe6S77ubE9dNZqpPbCAcInw1rq8nXGnqHzpy4E",
  "pdfSize": "460612"
}