{"id":"2412.13328","title":"Expansion Span: Combining Fading Memory and Retrieval in Hybrid State\n  Space Models","authors":"Elvis Nunez, Luca Zancato, Benjamin Bowman, Aditya Golatkar, Wei Xia,\n  Stefano Soatto","authorsParsed":[["Nunez","Elvis",""],["Zancato","Luca",""],["Bowman","Benjamin",""],["Golatkar","Aditya",""],["Xia","Wei",""],["Soatto","Stefano",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 20:55:42 GMT"}],"updateDate":"2024-12-19","timestamp":1734468942000,"abstract":"  The \"state\" of State Space Models (SSMs) represents their memory, which fades\nexponentially over an unbounded span. By contrast, Attention-based models have\n\"eidetic\" (i.e., verbatim, or photographic) memory over a finite span (context\nsize). Hybrid architectures combine State Space layers with Attention, but\nstill cannot recall the distant past and can access only the most recent tokens\neidetically. Unlike current methods of combining SSM and Attention layers, we\nallow the state to be allocated based on relevancy rather than recency. In this\nway, for every new set of query tokens, our models can \"eidetically\" access\ntokens from beyond the Attention span of current Hybrid SSMs without requiring\nextra hardware resources. We describe a method to expand the memory span of the\nhybrid state by \"reserving\" a fraction of the Attention context for tokens\nretrieved from arbitrarily distant in the past, thus expanding the eidetic\nmemory span of the overall state. We call this reserved fraction of tokens the\n\"expansion span,\" and the mechanism to retrieve and aggregate it \"Span-Expanded\nAttention\" (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose a\nnovel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows\nefficient adaptation on long spans of tokens. We show that SE-Attn enables us\nto efficiently adapt pre-trained Hybrid models on sequences of tokens up to 8\ntimes longer than the ones used for pre-training. We show that HyLoRA with\nSE-Attn is cheaper and more performant than alternatives like LongLoRA when\napplied to Hybrid models on natural language benchmarks with long-range\ndependencies, such as PG-19, RULER, and other common natural language\ndownstream tasks.\n","subjects":["Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"sM9HWr9FvhNCsZCQWC9JBD7eQ3iAwb9yTb7A2wHWvfc","pdfSize":"920806"}