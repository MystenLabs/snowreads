{"id":"2407.06887","title":"Risk-averse optimization of total rewards in Markovian models using\n  deviation measures","authors":"Christel Baier, Jakob Piribauer, Maximilian Starke","authorsParsed":[["Baier","Christel",""],["Piribauer","Jakob",""],["Starke","Maximilian",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 14:17:59 GMT"}],"updateDate":"2024-07-10","timestamp":1720534679000,"abstract":"  This paper addresses objectives tailored to the risk-averse optimization of\naccumulated rewards in Markov decision processes (MDPs). The studied objectives\nrequire maximizing the expected value of the accumulated rewards minus a\npenalty factor times a deviation measure of the resulting distribution of\nrewards. Using the variance in this penalty mechanism leads to the\nvariance-penalized expectation (VPE) for which it is known that optimal\nschedulers have to minimize future expected rewards when a high amount of\nrewards has been accumulated. This behavior is undesirable as risk-averse\nbehavior should keep the probability of particularly low outcomes low, but not\ndiscourage the accumulation of additional rewards on already good executions.\nThe paper investigates the semi-variance, which only takes outcomes below the\nexpected value into account, the mean absolute deviation (MAD), and the\nsemi-MAD as alternative deviation measures. Furthermore, a penalty mechanism\nthat penalizes outcomes below a fixed threshold is studied. For all of these\nobjectives, the properties of optimal schedulers are specified and in\nparticular the question whether these objectives overcome the problem observed\nfor the VPE is answered. Further, the resulting algorithmic problems on MDPs\nand Markov chains are investigated.\n","subjects":["Computing Research Repository/Logic in Computer Science"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"H-wEuqXnvMGpwcw-AjICAN4U98xXR_aAVjHxWjaK9UA","pdfSize":"792111"}