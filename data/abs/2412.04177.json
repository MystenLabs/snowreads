{"id":"2412.04177","title":"Fixed-Mean Gaussian Processes for Post-hoc Bayesian Deep Learning","authors":"Luis A. Ortega, Sim\\'on Rodr\\'iguez-Santana and Daniel\n  Hern\\'andez-Lobato","authorsParsed":[["Ortega","Luis A.",""],["Rodríguez-Santana","Simón",""],["Hernández-Lobato","Daniel",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 14:17:16 GMT"}],"updateDate":"2024-12-06","timestamp":1733408236000,"abstract":"  Recently, there has been an increasing interest in performing post-hoc\nuncertainty estimation about the predictions of pre-trained deep neural\nnetworks (DNNs). Given a pre-trained DNN via back-propagation, these methods\nenhance the original network by adding output confidence measures, such as\nerror bars, without compromising its initial accuracy. In this context, we\nintroduce a novel family of sparse variational Gaussian processes (GPs), where\nthe posterior mean is fixed to any continuous function when using a universal\nkernel. Specifically, we fix the mean of this GP to the output of the\npre-trained DNN, allowing our approach to effectively fit the GP's predictive\nvariances to estimate the DNN prediction uncertainty. Our approach leverages\nvariational inference (VI) for efficient stochastic optimization, with training\ncosts that remain independent of the number of training points, scaling\nefficiently to large datasets such as ImageNet. The proposed method, called\nfixed mean GP (FMGP), is architecture-agnostic, relying solely on the\npre-trained model's outputs to adjust the predictive variances. Experimental\nresults demonstrate that FMGP improves both uncertainty estimation and\ncomputational efficiency when compared to state-of-the-art methods.\n","subjects":["Computer Science/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"Xn6mQKpnkzpQj9FCiZn-OySUuwGqdY5-EZqBlEi_Av4","pdfSize":"1467112"}