{
  "id": "2412.00974",
  "title": "Optimal Algorithms for Augmented Testing of Discrete Distributions",
  "authors": "Maryam Aliakbarpour, Piotr Indyk, Ronitt Rubinfeld, Sandeep Silwal",
  "authorsParsed": [
    [
      "Aliakbarpour",
      "Maryam",
      ""
    ],
    [
      "Indyk",
      "Piotr",
      ""
    ],
    [
      "Rubinfeld",
      "Ronitt",
      ""
    ],
    [
      "Silwal",
      "Sandeep",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 1 Dec 2024 21:31:22 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733088682000,
  "abstract": "  We consider the problem of hypothesis testing for discrete distributions. In\nthe standard model, where we have sample access to an underlying distribution\n$p$, extensive research has established optimal bounds for uniformity testing,\nidentity testing (goodness of fit), and closeness testing (equivalence or\ntwo-sample testing). We explore these problems in a setting where a predicted\ndata distribution, possibly derived from historical data or predictive machine\nlearning models, is available. We demonstrate that such a predictor can indeed\nreduce the number of samples required for all three property testing tasks. The\nreduction in sample complexity depends directly on the predictor's quality,\nmeasured by its total variation distance from $p$. A key advantage of our\nalgorithms is their adaptability to the precision of the prediction.\nSpecifically, our algorithms can self-adjust their sample complexity based on\nthe accuracy of the available prediction, operating without any prior knowledge\nof the estimation's accuracy (i.e. they are consistent). Additionally, we never\nuse more samples than the standard approaches require, even if the predictions\nprovide no meaningful information (i.e. they are also robust). We provide lower\nbounds to indicate that the improvements in sample complexity achieved by our\nalgorithms are information-theoretically optimal. Furthermore, experimental\nresults show that the performance of our algorithms on real data significantly\nexceeds our worst-case guarantees for sample complexity, demonstrating the\npracticality of our approach.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Data Structures and Algorithms",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "cbAJn3QKwIR-4i8NJ9I8olzqn_9ndPq5uJ6OdQTWx-E",
  "pdfSize": "1187890"
}