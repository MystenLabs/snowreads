{
  "id": "2412.03393",
  "title": "Can neural operators always be continuously discretized?",
  "authors": "Takashi Furuya, Michael Puthawala, Maarten V. de Hoop, Matti Lassas",
  "authorsParsed": [
    [
      "Furuya",
      "Takashi",
      ""
    ],
    [
      "Puthawala",
      "Michael",
      ""
    ],
    [
      "de Hoop",
      "Maarten V.",
      ""
    ],
    [
      "Lassas",
      "Matti",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 15:22:54 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733325774000,
  "abstract": "  We consider the problem of discretization of neural operators between Hilbert\nspaces in a general framework including skip connections. We focus on bijective\nneural operators through the lens of diffeomorphisms in infinite dimensions.\nFramed using category theory, we give a no-go theorem that shows that\ndiffeomorphisms between Hilbert spaces or Hilbert manifolds may not admit any\ncontinuous approximations by diffeomorphisms on finite-dimensional spaces, even\nif the approximations are nonlinear. The natural way out is the introduction of\nstrongly monotone diffeomorphisms and layerwise strongly monotone neural\noperators which have continuous approximations by strongly monotone\ndiffeomorphisms on finite-dimensional spaces. For these, one can guarantee\ndiscretization invariance, while ensuring that finite-dimensional\napproximations converge not only as sequences of functions, but that their\nrepresentations converge in a suitable sense as well. Finally, we show that\nbilipschitz neural operators may always be written in the form of an\nalternating composition of strongly monotone neural operators, plus a simple\nisometry. Thus we realize a rigorous platform for discretization of a\ngeneralization of a neural operator. We also show that neural operators of this\ntype may be approximated through the composition of finite-rank residual neural\noperators, where each block is strongly monotone, and may be inverted locally\nvia iteration. We conclude by providing a quantitative approximation result for\nthe discretization of general bilipschitz neural operators.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "b7ckjc7NtBDJSMaobtNqVutqzvlHQX5otXB2qIM8jro",
  "pdfSize": "875690"
}