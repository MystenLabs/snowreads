{
  "id": "2412.11477",
  "title": "NoteContrast: Contrastive Language-Diagnostic Pretraining for Medical\n  Text",
  "authors": "Prajwal Kailas, Max Homilius, Rahul C. Deo, Calum A. MacRae",
  "authorsParsed": [
    [
      "Kailas",
      "Prajwal",
      ""
    ],
    [
      "Homilius",
      "Max",
      ""
    ],
    [
      "Deo",
      "Rahul C.",
      ""
    ],
    [
      "MacRae",
      "Calum A.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 06:44:39 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734331479000,
  "abstract": "  Accurate diagnostic coding of medical notes is crucial for enhancing patient\ncare, medical research, and error-free billing in healthcare organizations.\nManual coding is a time-consuming task for providers, and diagnostic codes\noften exhibit low sensitivity and specificity, whereas the free text in medical\nnotes can be a more precise description of a patients status. Thus, accurate\nautomated diagnostic coding of medical notes has become critical for a learning\nhealthcare system. Recent developments in long-document transformer\narchitectures have enabled attention-based deep-learning models to adjudicate\nmedical notes. In addition, contrastive loss functions have been used to\njointly pre-train large language and image models with noisy labels. To further\nimprove the automated adjudication of medical notes, we developed an approach\nbased on i) models for ICD-10 diagnostic code sequences using a large\nreal-world data set, ii) large language models for medical notes, and iii)\ncontrastive pre-training to build an integrated model of both ICD-10 diagnostic\ncodes and corresponding medical text. We demonstrate that a contrastive\napproach for pre-training improves performance over prior state-of-the-art\nmodels for the MIMIC-III-50, MIMIC-III-rare50, and MIMIC-III-full diagnostic\ncoding tasks.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "6c3MDvZLs1Vp3QQskaFYKOg4BRadkxsZlIXii_DdmvU",
  "pdfSize": "6513625"
}