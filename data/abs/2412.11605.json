{"id":"2412.11605","title":"SPaR: Self-Play with Tree-Search Refinement to Improve\n  Instruction-Following in Large Language Models","authors":"Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang,\n  Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang","authorsParsed":[["Cheng","Jiale",""],["Liu","Xiao",""],["Wang","Cunxiang",""],["Gu","Xiaotao",""],["Lu","Yida",""],["Zhang","Dan",""],["Dong","Yuxiao",""],["Tang","Jie",""],["Wang","Hongning",""],["Huang","Minlie",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 09:47:43 GMT"}],"updateDate":"2024-12-17","timestamp":1734342463000,"abstract":"  Instruction-following is a fundamental capability of language models,\nrequiring the model to recognize even the most subtle requirements in the\ninstructions and accurately reflect them in its output. Such an ability is\nwell-suited for and often optimized by preference learning. However, existing\nmethods often directly sample multiple independent responses from the model\nwhen creating preference pairs. Such practice can introduce content variations\nirrelevant to whether the instruction is precisely followed (e.g., different\nexpressions about the same semantic), interfering with the goal of teaching\nmodels to recognize the key differences that lead to improved instruction\nfollowing. In light of this, we introduce SPaR, a self-play framework\nintegrating tree-search self-refinement to yield valid and comparable\npreference pairs free from distractions. By playing against itself, an LLM\nemploys a tree-search strategy to refine its previous responses with respect to\nthe instruction while minimizing unnecessary variations. Our experiments show\nthat a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses\nGPT-4-Turbo on the IFEval benchmark without losing general capabilities.\nFurthermore, SPaR demonstrates promising scalability and transferability,\ngreatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how\ninference scaling in tree search would impact model performance. Our code and\ndata are publicly available at https://github.com/thu-coai/SPaR.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FIeEqtL4DUKIumZgfPFH8VujHI5jRLrZh8mezfu7eQY","pdfSize":"4128890"}