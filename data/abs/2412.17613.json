{"id":"2412.17613","title":"Can Stability be Detrimental? Better Generalization through Gradient\n  Descent Instabilities","authors":"Lawrence Wang, Stephen J. Roberts","authorsParsed":[["Wang","Lawrence",""],["Roberts","Stephen J.",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 14:32:53 GMT"}],"updateDate":"2024-12-24","timestamp":1734964373000,"abstract":"  Traditional analyses of gradient descent optimization show that, when the\nlargest eigenvalue of the loss Hessian - often referred to as the sharpness -\nis below a critical learning-rate threshold, then training is 'stable' and\ntraining loss decreases monotonically. Recent studies, however, have suggested\nthat the majority of modern deep neural networks achieve good performance\ndespite operating outside this stable regime. In this work, we demonstrate that\nsuch instabilities, induced by large learning rates, move model parameters\ntoward flatter regions of the loss landscape. Our crucial insight lies in\nnoting that, during these instabilities, the orientation of the Hessian\neigenvectors rotate. This, we conjecture, allows the model to explore regions\nof the loss landscape that display more desirable geometrical properties for\ngeneralization, such as flatness. These rotations are a consequence of network\ndepth, and we prove that for any network with depth > 1, unstable growth in\nparameters cause rotations in the principal components of the Hessian, which\npromote exploration of the parameter space away from unstable directions. Our\nempirical studies reveal an implicit regularization effect in gradient descent\nwith large learning rates operating beyond the stability threshold. We find\nthese lead to excellent generalization performance on modern benchmark\ndatasets.\n","subjects":["Computer Science/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"JtNm9GHYnjY6DE3BhbezFbPNodK4bBThqLWAV_kkYFE","pdfSize":"2180600"}