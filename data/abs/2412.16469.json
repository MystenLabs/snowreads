{"id":"2412.16469","title":"Chained Tuning Leads to Biased Forgetting","authors":"Megan Ung, Alicia Sun, Samuel J. Bell, Bhaktipriya Radharapu, Levent\n  Sagun, Adina Williams","authorsParsed":[["Ung","Megan",""],["Sun","Alicia",""],["Bell","Samuel J.",""],["Radharapu","Bhaktipriya",""],["Sagun","Levent",""],["Williams","Adina",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 03:51:58 GMT"},{"version":"v2","created":"Tue, 24 Dec 2024 19:43:57 GMT"}],"updateDate":"2024-12-30","timestamp":1734753118000,"abstract":"  Large language models (LLMs) are often fine-tuned for use on downstream\ntasks, though this can degrade capabilities learned during previous training.\nThis phenomenon, often referred to as catastrophic forgetting, has important\npotential implications for the safety of deployed models. In this work, we\nfirst show that models trained on downstream tasks forget their safety tuning\nto a greater extent than models trained in the opposite order. Second, we show\nthat forgetting disproportionately impacts safety information about certain\ngroups. To quantify this phenomenon, we define a new metric we term biased\nforgetting. We conduct a systematic evaluation of the effects of task ordering\non forgetting and apply mitigations that can help the model recover from the\nforgetting observed. We hope our findings can better inform methods for\nchaining the finetuning of LLMs in continual learning settings to enable\ntraining of safer and less toxic models.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"xBm9a0Aokm3P02qvtNGjRLL-Xoxqkg_q1noLwQv-jcM","pdfSize":"872720"}