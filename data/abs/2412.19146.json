{"id":"2412.19146","title":"AskChart: Universal Chart Understanding through Textual Enhancement","authors":"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo","authorsParsed":[["Yang","Xudong",""],["Wu","Yifan",""],["Zhu","Yizhang",""],["Tang","Nan",""],["Luo","Yuyu",""]],"versions":[{"version":"v1","created":"Thu, 26 Dec 2024 09:59:43 GMT"}],"updateDate":"2024-12-30","timestamp":1735207183000,"abstract":"  Chart understanding tasks such as ChartQA and Chart-to-Text involve\nautomatically extracting and interpreting key information from charts, enabling\nusers to query or convert visual data into structured formats. State-of-the-art\napproaches primarily focus on visual cues from chart images, failing to\nexplicitly incorporate rich textual information (e.g., data labels and axis\nlabels) embedded within the charts. This textual information is vital for\nintuitive human comprehension and interpretation of charts. Moreover, existing\nmodels are often large and computationally intensive, limiting their practical\napplicability. In this paper, we introduce AskChart, a universal model that\nexplicitly integrates both textual and visual cues from charts using a Mixture\nof Experts (MoE) architecture. AskChart facilitates the learning of enhanced\nvisual-textual representations of charts for effectively handling multiple\nchart understanding tasks, while maintaining a smaller model size. To capture\nthe synergy between visual and textual modalities, we curate a large-scale\ndataset named ChartBank with about 7.5M data samples, which helps align textual\nand visual information and facilitates the extraction of visual entities and\ntext. To effectively train AskChart, we design a three-stage training strategy\nto align visual and textual modalities for learning robust visual-textual\nrepresentations and optimizing the learning of the MoE layer. Extensive\nexperiments across five datasets demonstrate the significant performance gains\nof AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B\nparameters outperforms state-of-the-art models with 13B parameters by 68.3% in\nOpen-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable\nperformance in ChartQA and Chart-to-Table tasks.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XmUg1_rwOQFwwOJ9QLKHu3E_8ULmHWrPJcgMd-5tAok","pdfSize":"2390484"}