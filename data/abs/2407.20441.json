{"id":"2407.20441","title":"Finite-Time Analysis of Asynchronous Multi-Agent TD Learning","authors":"Nicol\\`o Dal Fabbro, Arman Adibi, Aritra Mitra and George J. Pappas","authorsParsed":[["Fabbro","Nicol√≤ Dal",""],["Adibi","Arman",""],["Mitra","Aritra",""],["Pappas","George J.",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 22:36:07 GMT"}],"updateDate":"2024-07-31","timestamp":1722292567000,"abstract":"  Recent research endeavours have theoretically shown the beneficial effect of\ncooperation in multi-agent reinforcement learning (MARL). In a setting\ninvolving $N$ agents, this beneficial effect usually comes in the form of an\n$N$-fold linear convergence speedup, i.e., a reduction - proportional to $N$ -\nin the number of iterations required to reach a certain convergence precision.\nIn this paper, we show for the first time that this speedup property also holds\nfor a MARL framework subject to asynchronous delays in the local agents'\nupdates. In particular, we consider a policy evaluation problem in which\nmultiple agents cooperate to evaluate a common policy by communicating with a\ncentral aggregator. In this setting, we study the finite-time convergence of\n\\texttt{AsyncMATD}, an asynchronous multi-agent temporal difference (TD)\nlearning algorithm in which agents' local TD update directions are subject to\nasynchronous bounded delays. Our main contribution is providing a finite-time\nanalysis of \\texttt{AsyncMATD}, for which we establish a linear convergence\nspeedup while highlighting the effect of time-varying asynchronous delays on\nthe resulting convergence rate.\n","subjects":["Computing Research Repository/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"l218qLisqi2HEa_wyvwzO5jwwkgFq286U2Fj3TmXDpw","pdfSize":"1043187"}