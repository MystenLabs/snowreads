{
  "id": "2412.02886",
  "title": "Patchfinder: Leveraging Visual Language Models for Accurate Information\n  Retrieval using Model Uncertainty",
  "authors": "Roman Colman, Minh Vu, Manish Bhattarai, Martin Ma, Hari Viswanathan,\n  Daniel O'Malley, Javier E. Santos",
  "authorsParsed": [
    [
      "Colman",
      "Roman",
      ""
    ],
    [
      "Vu",
      "Minh",
      ""
    ],
    [
      "Bhattarai",
      "Manish",
      ""
    ],
    [
      "Ma",
      "Martin",
      ""
    ],
    [
      "Viswanathan",
      "Hari",
      ""
    ],
    [
      "O'Malley",
      "Daniel",
      ""
    ],
    [
      "Santos",
      "Javier E.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 22:46:09 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 11 Dec 2024 19:28:10 GMT"
    },
    {
      "version": "v3",
      "created": "Fri, 13 Dec 2024 21:27:56 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1733265969000,
  "abstract": "  For decades, corporations and governments have relied on scanned documents to\nrecord vast amounts of information. However, extracting this information is a\nslow and tedious process due to the sheer volume and complexity of these\nrecords. The rise of Vision Language Models (VLMs) presents a way to\nefficiently and accurately extract the information out of these documents. The\ncurrent automated workflow often requires a two-step approach involving the\nextraction of information using optical character recognition software and\nsubsequent usage of large language models for processing this information.\nUnfortunately, these methods encounter significant challenges when dealing with\nnoisy scanned documents, often requiring computationally expensive language\nmodels to handle high information density effectively. In this study, we\npropose PatchFinder, an algorithm that builds upon VLMs to improve information\nextraction. First, we devise a confidence-based score, called Patch Confidence,\nbased on the Maximum Softmax Probability of the VLMs' output to measure the\nmodel's confidence in its predictions. Using this metric, PatchFinder\ndetermines a suitable patch size, partitions the input document into\noverlapping patches, and generates confidence-based predictions for the target\ninformation. Our experimental results show that PatchFinder, leveraging Phi-3v,\na 4.2-billion-parameter VLM, achieves an accuracy of 94% on our dataset of 190\nnoisy scanned documents, outperforming ChatGPT-4o by 18.5 percentage points.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "30_-602mJARG23RH2tkHC6ojIbz70UlMxK_UR0_SBHo",
  "pdfSize": "10232012"
}