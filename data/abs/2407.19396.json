{"id":"2407.19396","title":"NAVIX: Scaling MiniGrid Environments with JAX","authors":"Eduardo Pignatelli, Jarek Liesen, Robert Tjarko Lange, Chris Lu, Pablo\n  Samuel Castro, Laura Toni","authorsParsed":[["Pignatelli","Eduardo",""],["Liesen","Jarek",""],["Lange","Robert Tjarko",""],["Lu","Chris",""],["Castro","Pablo Samuel",""],["Toni","Laura",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 04:39:18 GMT"}],"updateDate":"2024-07-30","timestamp":1722141558000,"abstract":"  As Deep Reinforcement Learning (Deep RL) research moves towards solving\nlarge-scale worlds, efficient environment simulations become crucial for rapid\nexperimentation. However, most existing environments struggle to scale to high\nthroughput, setting back meaningful progress. Interactions are typically\ncomputed on the CPU, limiting training speed and throughput, due to slower\ncomputation and communication overhead when distributing the task across\nmultiple machines. Ultimately, Deep RL training is CPU-bound, and developing\nbatched, fast, and scalable environments has become a frontier for progress.\nAmong the most used Reinforcement Learning (RL) environments, MiniGrid is at\nthe foundation of several studies on exploration, curriculum learning,\nrepresentation learning, diversity, meta-learning, credit assignment, and\nlanguage-conditioned RL, and still suffers from the limitations described\nabove. In this work, we introduce NAVIX, a re-implementation of MiniGrid in\nJAX. NAVIX achieves over 200 000x speed improvements in batch mode, supporting\nup to 2048 agents in parallel on a single Nvidia A100 80 GB. This reduces\nexperiment times from one week to 15 minutes, promoting faster design\niterations and more scalable RL model development.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eVnD2qybtJVQkENbnNQiyvzf-TY-PyzJBJyoF0XFU-I","pdfSize":"964424"}