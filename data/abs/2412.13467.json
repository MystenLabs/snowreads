{"id":"2412.13467","title":"Transducer Tuning: Efficient Model Adaptation for Software Tasks Using\n  Code Property Graphs","authors":"Imam Nur Bani Yusuf and Lingxiao Jiang","authorsParsed":[["Yusuf","Imam Nur Bani",""],["Jiang","Lingxiao",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 03:25:17 GMT"}],"updateDate":"2024-12-19","timestamp":1734492317000,"abstract":"  Large language models have demonstrated promising performance across various\nsoftware engineering tasks. While fine-tuning is a common practice to adapt\nthese models for downstream tasks, it becomes challenging in\nresource-constrained environments due to increased memory requirements from\ngrowing trainable parameters in increasingly large language models. We\nintroduce \\approach, a technique to adapt large models for downstream code\ntasks using Code Property Graphs (CPGs). Our approach introduces a modular\ncomponent called \\transducer that enriches code embeddings with structural and\ndependency information from CPGs. The Transducer comprises two key components:\nGraph Vectorization Engine (GVE) and Attention-Based Fusion Layer (ABFL). GVE\nextracts CPGs from input source code and transforms them into graph feature\nvectors. ABFL then fuses those graphs feature vectors with initial code\nembeddings from a large language model. By optimizing these transducers for\ndifferent downstream tasks, our approach enhances the models without the need\nto fine-tune them for specific tasks. We have evaluated \\approach on three\ndownstream tasks: code summarization, assert generation, and code translation.\nOur results demonstrate competitive performance compared to full parameter\nfine-tuning while reducing up to 99\\% trainable parameters to save memory.\n\\approach also remains competitive against other fine-tuning approaches (e.g.,\nLoRA, Prompt-Tuning, Prefix-Tuning) while using only 1.5\\%-80\\% of their\ntrainable parameters. Our findings show that integrating structural and\ndependency information through Transducer Tuning enables more efficient model\nadaptation, making it easier for users to adapt large models in\nresource-constrained settings.\n","subjects":["Computer Science/Software Engineering","Computer Science/Artificial Intelligence","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"TH11S_vy1VGtXAVr17396LXGrg4eKCQ-ROWHObtCjoM","pdfSize":"670278"}