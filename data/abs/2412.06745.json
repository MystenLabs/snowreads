{
  "id": "2412.06745",
  "title": "ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities",
  "authors": "Adhiraj Ghosh, Sebastian Dziadzio, Ameya Prabhu, Vishaal Udandarao,\n  Samuel Albanie, Matthias Bethge",
  "authorsParsed": [
    [
      "Ghosh",
      "Adhiraj",
      ""
    ],
    [
      "Dziadzio",
      "Sebastian",
      ""
    ],
    [
      "Prabhu",
      "Ameya",
      ""
    ],
    [
      "Udandarao",
      "Vishaal",
      ""
    ],
    [
      "Albanie",
      "Samuel",
      ""
    ],
    [
      "Bethge",
      "Matthias",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 18:37:14 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733769434000,
  "abstract": "  Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Computation and Language",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "DcBNtZwGIlciXJgx4jHmnaoZfrRHqVrYW3UNXWysWc4",
  "pdfSize": "11505442"
}