{"id":"2412.00624","title":"VideoSAVi: Self-Aligned Video Language Models without Human Supervision","authors":"Yogesh Kulkarni, Pooyan Fazli","authorsParsed":[["Kulkarni","Yogesh",""],["Fazli","Pooyan",""]],"versions":[{"version":"v1","created":"Sun, 1 Dec 2024 00:33:05 GMT"}],"updateDate":"2024-12-03","timestamp":1733013185000,"abstract":"  Recent advances in vision-language models (VLMs) have significantly enhanced\nvideo understanding tasks. Instruction tuning (i.e., fine-tuning models on\ndatasets of instructions paired with desired outputs) has been key to improving\nmodel performance. However, creating diverse instruction-tuning datasets is\nchallenging due to high annotation costs and the complexity of capturing\ntemporal information in videos. Existing approaches often rely on large\nlanguage models to generate instruction-output pairs, which can limit diversity\nand lead to responses that lack grounding in the video content. To address\nthis, we propose VideoSAVi (Self-Aligned Video Language Model), a novel\nself-training pipeline that enables VLMs to generate their own training data\nwithout extensive manual annotation. The process involves three stages: (1)\ngenerating diverse video-specific questions, (2) producing multiple candidate\nanswers, and (3) evaluating these responses for alignment with the video\ncontent. This self-generated data is then used for direct preference\noptimization (DPO), allowing the model to refine its own high-quality outputs\nand improve alignment with video content. Our experiments demonstrate that even\nsmaller models (0.5B and 7B parameters) can effectively use this self-training\napproach, outperforming previous methods and achieving results comparable to\nthose trained on proprietary preference data. VideoSAVi shows significant\nimprovements across multiple benchmarks: up to 28% on multi-choice QA, 8% on\nzero-shot open-ended QA, and 12% on temporal reasoning benchmarks. These\nresults demonstrate the effectiveness of our self-training approach in\nenhancing video understanding while reducing dependence on proprietary models.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"e4sIcakJgja-KUlM8o9C35OSskNwnBlCxPG3Y4Psy-Y","pdfSize":"46582418"}