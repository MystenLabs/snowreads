{
  "id": "2412.07617",
  "title": "Swarm Behavior Cloning",
  "authors": "Jonas N\\\"u{\\ss}lein, Maximilian Zorn, Philipp Altmann and Claudia\n  Linnhoff-Popien",
  "authorsParsed": [
    [
      "Nüßlein",
      "Jonas",
      ""
    ],
    [
      "Zorn",
      "Maximilian",
      ""
    ],
    [
      "Altmann",
      "Philipp",
      ""
    ],
    [
      "Linnhoff-Popien",
      "Claudia",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 15:54:57 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733846097000,
  "abstract": "  In sequential decision-making environments, the primary approaches for\ntraining agents are Reinforcement Learning (RL) and Imitation Learning (IL).\nUnlike RL, which relies on modeling a reward function, IL leverages expert\ndemonstrations, where an expert policy $\\pi_e$ (e.g., a human) provides the\ndesired behavior. Formally, a dataset $D$ of state-action pairs is provided: $D\n= {(s, a = \\pi_e(s))}$. A common technique within IL is Behavior Cloning (BC),\nwhere a policy $\\pi(s) = a$ is learned through supervised learning on $D$.\nFurther improvements can be achieved by using an ensemble of $N$ individually\ntrained BC policies, denoted as $E = {\\pi_i(s)}{1 \\leq i \\leq N}$. The\nensemble's action $a$ for a given state $s$ is the aggregated output of the $N$\nactions: $a = \\frac{1}{N} \\sum{i} \\pi_i(s)$. This paper addresses the issue of\nincreasing action differences -- the observation that discrepancies between the\n$N$ predicted actions grow in states that are underrepresented in the training\ndata. Large action differences can result in suboptimal aggregated actions. To\naddress this, we propose a method that fosters greater alignment among the\npolicies while preserving the diversity of their computations. This approach\nreduces action differences and ensures that the ensemble retains its inherent\nstrengths, such as robustness and varied decision-making. We evaluate our\napproach across eight diverse environments, demonstrating a notable decrease in\naction differences and significant improvements in overall performance, as\nmeasured by mean episode returns.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "fmChaoaRIYn1rFS-YcOkvook3bEzHNDkJuCKpfXpy0E",
  "pdfSize": "518360"
}