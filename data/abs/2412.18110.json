{"id":"2412.18110","title":"SlimGPT: Layer-wise Structured Pruning for Large Language Models","authors":"Gui Ling, Ziyang Wang, Yuliang Yan, Qingwen Liu","authorsParsed":[["Ling","Gui",""],["Wang","Ziyang",""],["Yan","Yuliang",""],["Liu","Qingwen",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 02:49:50 GMT"}],"updateDate":"2024-12-25","timestamp":1735008590000,"abstract":"  Large language models (LLMs) have garnered significant attention for their\nremarkable capabilities across various domains, whose vast parameter scales\npresent challenges for practical deployment. Structured pruning is an effective\nmethod to balance model performance with efficiency, but performance\nrestoration under computational resource constraints is a principal challenge\nin pruning LLMs. Therefore, we present a low-cost and fast structured pruning\nmethod for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We\npropose Batched Greedy Pruning for rapid and near-optimal pruning, which\nenhances the accuracy of head-wise pruning error estimation through grouped\nCholesky decomposition and improves the pruning efficiency of FFN via Dynamic\nGroup Size, thereby achieving approximate local optimal pruning results within\none hour. Besides, we explore the limitations of layer-wise pruning from the\nperspective of error accumulation and propose Incremental Pruning Ratio, a\nnon-uniform pruning strategy to reduce performance degradation. Experimental\nresults on the LLaMA benchmark show that SlimGPT outperforms other methods and\nachieves state-of-the-art results.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"55nKBOuB5siVG0a51rK4_K00I5XaVKjtBKxNuu25Gzo","pdfSize":"591331"}