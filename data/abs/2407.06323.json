{"id":"2407.06323","title":"When in Doubt, Cascade: Towards Building Efficient and Capable\n  Guardrails","authors":"Manish Nagireddy, Inkit Padhi, Soumya Ghosh, Prasanna Sattigeri","authorsParsed":[["Nagireddy","Manish",""],["Padhi","Inkit",""],["Ghosh","Soumya",""],["Sattigeri","Prasanna",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 18:39:06 GMT"}],"updateDate":"2024-07-10","timestamp":1720463946000,"abstract":"  Large language models (LLMs) have convincing performance in a variety of\ndownstream tasks. However, these systems are prone to generating undesirable\noutputs such as harmful and biased text. In order to remedy such generations,\nthe development of guardrail (or detector) models has gained traction.\nMotivated by findings from developing a detector for social bias, we adopt the\nnotion of a use-mention distinction - which we identified as the primary source\nof under-performance in the preliminary versions of our social bias detector.\nArmed with this information, we describe a fully extensible and reproducible\nsynthetic data generation pipeline which leverages taxonomy-driven instructions\nto create targeted and labeled data. Using this pipeline, we generate over 300K\nunique contrastive samples and provide extensive experiments to systematically\nevaluate performance on a suite of open source datasets. We show that our\nmethod achieves competitive performance with a fraction of the cost in compute\nand offers insight into iteratively developing efficient and capable guardrail\nmodels.\n  Warning: This paper contains examples of text which are toxic, biased, and\npotentially harmful.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GzJWzlzc8aRosJvICHB3rSlQPRBNwNI1M8hhMcX1LrM","pdfSize":"229254"}
