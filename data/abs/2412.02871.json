{"id":"2412.02871","title":"MAGMA: Manifold Regularization for MAEs","authors":"Alin Dondera, Anuj Singh, Hadi Jamali-Rad","authorsParsed":[["Dondera","Alin",""],["Singh","Anuj",""],["Jamali-Rad","Hadi",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 22:14:10 GMT"},{"version":"v2","created":"Thu, 5 Dec 2024 19:12:43 GMT"}],"updateDate":"2024-12-09","timestamp":1733264050000,"abstract":"  Masked Autoencoders (MAEs) are an important divide in self-supervised\nlearning (SSL) due to their independence from augmentation techniques for\ngenerating positive (and/or negative) pairs as in contrastive frameworks. Their\nmasking and reconstruction strategy also nicely aligns with SSL approaches in\nnatural language processing. Most MAEs are built upon Transformer-based\narchitectures where visual features are not regularized as opposed to their\nconvolutional neural network (CNN) based counterparts, which can potentially\nhinder their performance. To address this, we introduce MAGMA, a novel\nbatch-wide layer-wise regularization loss applied to representations of\ndifferent Transformer layers. We demonstrate that by plugging in the proposed\nregularization loss, one can significantly improve the performance of MAE-based\nmodels. We further demonstrate the impact of the proposed loss on optimizing\nother generic SSL approaches (such as VICReg and SimCLR), broadening the impact\nof the proposed approach. Our code base can be found at\nhttps://github.com/adondera/magma.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"aejv3kugLXvVQUMDV966flaQbOY4lvSyUHc65GWp7FE","pdfSize":"12155928"}