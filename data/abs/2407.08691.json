{"id":"2407.08691","title":"ElasticAST: An Audio Spectrogram Transformer for All Length and\n  Resolutions","authors":"Jiu Feng, Mehmet Hamza Erol, Joon Son Chung, Arda Senocak","authorsParsed":[["Feng","Jiu",""],["Erol","Mehmet Hamza",""],["Chung","Joon Son",""],["Senocak","Arda",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:29:56 GMT"}],"updateDate":"2024-07-12","timestamp":1720718996000,"abstract":"  Transformers have rapidly overtaken CNN-based architectures as the new\nstandard in audio classification. Transformer-based models, such as the Audio\nSpectrogram Transformers (AST), also inherit the fixed-size input paradigm from\nCNNs. However, this leads to performance degradation for ASTs in the inference\nwhen input lengths vary from the training. This paper introduces an approach\nthat enables the use of variable-length audio inputs with AST models during\nboth training and inference. By employing sequence packing, our method\nElasticAST, accommodates any audio length during training, thereby offering\nflexibility across all lengths and resolutions at the inference. This\nflexibility allows ElasticAST to maintain evaluation capabilities at various\nlengths or resolutions and achieve similar performance to standard ASTs trained\nat specific lengths or resolutions. Moreover, experiments demonstrate\nElasticAST's better performance when trained and evaluated on native-length\naudio datasets.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DWEzPOGmRj4ReJyjegk2kNgDkgcgFDGEcs0p8IRrLkc","pdfSize":"4837666"}