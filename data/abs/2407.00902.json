{"id":"2407.00902","title":"From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning","authors":"Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen","authorsParsed":[["Xu","Nan",""],["Wang","Fei",""],["Zhang","Sheng",""],["Poon","Hoifung",""],["Chen","Muhao",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 01:57:21 GMT"}],"updateDate":"2024-07-02","timestamp":1719799041000,"abstract":"  Motivated by in-context learning (ICL) capabilities of Large Language models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Considering such modality impact, we further utilize\nmodality-driven demonstration strategies to boost ICL performance. We also\nidentify that demonstration selection is closely related to the models' ability\nto capture task inductive biases from multimodal ICL. Our principled analysis\nprovides a comprehensive way of understanding the role of demonstrations in\nmultimodal in-context learning, and sheds light on effectively improving\nmultimodal ICL on a wide range of tasks even if those tasks are not seen in or\neven contradict pretraining data.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CNVs5bDd1bU2qgg-dmtOe6Hl95ih69_MWvHsBKLJ2zc","pdfSize":"1220218"}