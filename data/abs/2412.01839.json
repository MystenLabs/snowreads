{
  "id": "2412.01839",
  "title": "Dynamics of Resource Allocation in O-RANs: An In-depth Exploration of\n  On-Policy and Off-Policy Deep Reinforcement Learning for Real-Time\n  Applications",
  "authors": "Manal Mehdaoui, Amine Abouaomar",
  "authorsParsed": [
    [
      "Mehdaoui",
      "Manal",
      ""
    ],
    [
      "Abouaomar",
      "Amine",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 17 Nov 2024 17:46:40 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1731865600000,
  "abstract": "  Deep Reinforcement Learning (DRL) is a powerful tool used for addressing\ncomplex challenges in mobile networks. This paper investigates the application\nof two DRL models, on-policy and off-policy, in the field of resource\nallocation for Open Radio Access Networks (O-RAN). The on-policy model is the\nProximal Policy Optimization (PPO), and the off-policy model is the Sample\nEfficient Actor-Critic with Experience Replay (ACER), which focuses on\nresolving the challenges of resource allocation associated with a Quality of\nService (QoS) application that has strict requirements. Motivated by the\noriginal work of Nessrine Hammami and Kim Khoa Nguyen, this study is a\nreplication to validate and prove the findings. Both PPO and ACER are used\nwithin the same experimental setup to assess their performance in a scenario of\nlatency-sensitive and latency-tolerant users and compare them. The aim is to\nverify the efficacy of on-policy and off-policy DRL models in the context of\nO-RAN resource allocation. Results from this replication contribute to the\nongoing scientific research and offer insights into the reproducibility and\ngeneralizability of the original research. This analysis reaffirms that both\non-policy and off-policy DRL models have better performance than greedy\nalgorithms in O-RAN settings. In addition, it confirms the original\nobservations that the on-policy model (PPO) gives a favorable balance between\nenergy consumption and user latency, while the off-policy model (ACER) shows a\nfaster convergence. These findings give good insights to optimize resource\nallocation strategies in O-RANs. Index Terms: 5G, O-RAN, resource allocation,\nML, DRL, PPO, ACER.\n",
  "subjects": [
    "Computer Science/Networking and Internet Architecture",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "DWExLMqpLiTF8PBwvKg6eMLVemk38vdAq_M1x-xqH30",
  "pdfSize": "1849807"
}