{"id":"2412.05586","title":"Towards Learning to Reason: Comparing LLMs with Neuro-Symbolic on\n  Arithmetic Relations in Abstract Reasoning","authors":"Michael Hersche, Giacomo Camposampiero, Roger Wattenhofer, Abu\n  Sebastian, Abbas Rahimi","authorsParsed":[["Hersche","Michael",""],["Camposampiero","Giacomo",""],["Wattenhofer","Roger",""],["Sebastian","Abu",""],["Rahimi","Abbas",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 08:45:39 GMT"}],"updateDate":"2024-12-10","timestamp":1733561139000,"abstract":"  This work compares large language models (LLMs) and neuro-symbolic approaches\nin solving Raven's progressive matrices (RPM), a visual abstract reasoning test\nthat involves the understanding of mathematical rules such as progression or\narithmetic addition. Providing the visual attributes directly as textual\nprompts, which assumes an oracle visual perception module, allows us to measure\nthe model's abstract reasoning capability in isolation. Despite providing such\ncompositionally structured representations from the oracle visual perception\nand advanced prompting techniques, both GPT-4 and Llama-3 70B cannot achieve\nperfect accuracy on the center constellation of the I-RAVEN dataset. Our\nanalysis reveals that the root cause lies in the LLM's weakness in\nunderstanding and executing arithmetic rules. As a potential remedy, we analyze\nthe Abductive Rule Learner with Context-awareness (ARLC), a neuro-symbolic\napproach that learns to reason with vector-symbolic architectures (VSAs). Here,\nconcepts are represented with distributed vectors s.t. dot products between\nencoded vectors define a similarity kernel, and simple element-wise operations\non the vectors perform addition/subtraction on the encoded values. We find that\nARLC achieves almost perfect accuracy on the center constellation of I-RAVEN,\ndemonstrating a high fidelity in arithmetic rules. To stress the length\ngeneralization capabilities of the models, we extend the RPM tests to larger\nmatrices (3x10 instead of typical 3x3) and larger dynamic ranges of the\nattribute values (from 10 up to 1000). We find that the LLM's accuracy of\nsolving arithmetic rules drops to sub-10%, especially as the dynamic range\nexpands, while ARLC can maintain a high accuracy due to emulating symbolic\ncomputations on top of properly distributed representations. Our code is\navailable at https://github.com/IBM/raven-large-language-models.\n","subjects":["Computer Science/Artificial Intelligence","Computer Science/Machine Learning","Computer Science/Symbolic Computation"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HIeXRXdKtpvJZ83Tc66GmK_t86sEbG3BJx7hothoc4Q","pdfSize":"636841"}