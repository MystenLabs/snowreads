{"id":"2407.21515","title":"Learning Effective Representations for Retrieval Using Self-Distillation\n  with Adaptive Relevance Margins","authors":"Lukas Gienapp, Niklas Deckers, Martin Potthast, Harrisen Scells","authorsParsed":[["Gienapp","Lukas",""],["Deckers","Niklas",""],["Potthast","Martin",""],["Scells","Harrisen",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 10:33:32 GMT"}],"updateDate":"2024-08-01","timestamp":1722422012000,"abstract":"  Representation-based retrieval models, so-called biencoders, estimate the\nrelevance of a document to a query by calculating the similarity of their\nrespective embeddings. Current state-of-the-art biencoders are trained using an\nexpensive training regime involving knowledge distillation from a teacher model\nand batch-sampling. Instead of relying on a teacher model, we contribute a\nnovel parameter-free loss function for self-supervision that exploits the\npre-trained language modeling capabilities of the encoder model as a training\nsignal, eliminating the need for batch sampling by performing implicit hard\nnegative mining. We investigate the capabilities of our proposed approach\nthrough extensive ablation studies, demonstrating that self-distillation can\nmatch the effectiveness of teacher distillation using only 13.5% of the data,\nwhile offering a speedup in training time between 3x and 15x compared to\nparametrized losses. Code and data is made openly available.\n","subjects":["Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"B0y3zceQ3rpgPRsuGWgh5tNOaoRiCC8Hw5NAloifjdc","pdfSize":"3654238"}