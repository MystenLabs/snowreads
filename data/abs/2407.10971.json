{"id":"2407.10971","title":"Walking the Values in Bayesian Inverse Reinforcement Learning","authors":"Ondrej Bajgar, Alessandro Abate, Konstantinos Gatsis and Michael A.\n  Osborne","authorsParsed":[["Bajgar","Ondrej",""],["Abate","Alessandro",""],["Gatsis","Konstantinos",""],["Osborne","Michael A.",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 17:59:52 GMT"}],"updateDate":"2024-07-16","timestamp":1721066392000,"abstract":"  The goal of Bayesian inverse reinforcement learning (IRL) is recovering a\nposterior distribution over reward functions using a set of demonstrations from\nan expert optimizing for a reward unknown to the learner. The resulting\nposterior over rewards can then be used to synthesize an apprentice policy that\nperforms well on the same or a similar task. A key challenge in Bayesian IRL is\nbridging the computational gap between the hypothesis space of possible rewards\nand the likelihood, often defined in terms of Q values: vanilla Bayesian IRL\nneeds to solve the costly forward planning problem - going from rewards to the\nQ values - at every step of the algorithm, which may need to be done thousands\nof times. We propose to solve this by a simple change: instead of focusing on\nprimarily sampling in the space of rewards, we can focus on primarily working\nin the space of Q-values, since the computation required to go from Q-values to\nreward is radically cheaper. Furthermore, this reversion of the computation\nmakes it easy to compute the gradient allowing efficient sampling using\nHamiltonian Monte Carlo. We propose ValueWalk - a new Markov chain Monte Carlo\nmethod based on this insight - and illustrate its advantages on several tasks.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"v_iVKvlrV0gjKK0UCsX9evALe9X-9ZbML8bd9VKpeCU","pdfSize":"2891448"}