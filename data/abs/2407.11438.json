{"id":"2407.11438","title":"Trust No Bot: Discovering Personal Disclosures in Human-LLM\n  Conversations in the Wild","authors":"Niloofar Mireshghallah, Maria Antoniak, Yash More, Yejin Choi,\n  Golnoosh Farnadi","authorsParsed":[["Mireshghallah","Niloofar",""],["Antoniak","Maria",""],["More","Yash",""],["Choi","Yejin",""],["Farnadi","Golnoosh",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 07:05:31 GMT"},{"version":"v2","created":"Sat, 20 Jul 2024 00:47:32 GMT"}],"updateDate":"2024-07-23","timestamp":1721113531000,"abstract":"  Measuring personal disclosures made in human-chatbot interactions can provide\na better understanding of users' AI literacy and facilitate privacy research\nfor large language models (LLMs). We run an extensive, fine-grained analysis on\nthe personal disclosures made by real users to commercial GPT models,\ninvestigating the leakage of personally identifiable and sensitive information.\nTo understand the contexts in which users disclose to chatbots, we develop a\ntaxonomy of tasks and sensitive topics, based on qualitative and quantitative\nanalysis of naturally occurring conversations. We discuss these potential\nprivacy harms and observe that: (1) personally identifiable information (PII)\nappears in unexpected contexts such as in translation or code editing (48% and\n16% of the time, respectively) and (2) PII detection alone is insufficient to\ncapture the sensitive topics that are common in human-chatbot interactions,\nsuch as detailed sexual preferences or specific drug use habits. We believe\nthat these high disclosure rates are of significant importance for researchers\nand data curators, and we call for the design of appropriate nudging mechanisms\nto help users moderate their interactions.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UTN7iStOlrjRJuDRZ2yjIkrnaMZFHTNa9naTzTdJV_A","pdfSize":"1146042"}