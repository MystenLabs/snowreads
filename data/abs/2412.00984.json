{"id":"2412.00984","title":"TGTOD: A Global Temporal Graph Transformer for Outlier Detection at\n  Scale","authors":"Kay Liu, Jiahao Ding, MohamadAli Torkamani, Philip S. Yu","authorsParsed":[["Liu","Kay",""],["Ding","Jiahao",""],["Torkamani","MohamadAli",""],["Yu","Philip S.",""]],"versions":[{"version":"v1","created":"Sun, 1 Dec 2024 22:24:55 GMT"}],"updateDate":"2024-12-03","timestamp":1733091895000,"abstract":"  While Transformers have revolutionized machine learning on various data,\nexisting Transformers for temporal graphs face limitations in (1) restricted\nreceptive fields, (2) overhead of subgraph extraction, and (3) suboptimal\ngeneralization capability beyond link prediction. In this paper, we rethink\ntemporal graph Transformers and propose TGTOD, a novel end-to-end Temporal\nGraph Transformer for Outlier Detection. TGTOD employs global attention to\nmodel both structural and temporal dependencies within temporal graphs. To\ntackle scalability, our approach divides large temporal graphs into\nspatiotemporal patches, which are then processed by a hierarchical Transformer\narchitecture comprising Patch Transformer, Cluster Transformer, and Temporal\nTransformer. We evaluate TGTOD on three public datasets under two settings,\ncomparing with a wide range of baselines. Our experimental results demonstrate\nthe effectiveness of TGTOD, achieving AP improvement of 61% on Elliptic.\nFurthermore, our efficiency evaluation shows that TGTOD reduces training time\nby 44x compared to existing Transformers for temporal graphs. To foster\nreproducibility, we make our implementation publicly available at\nhttps://github.com/kayzliu/tgtod.\n","subjects":["Computer Science/Machine Learning","Computer Science/Social and Information Networks"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2hGp4vY3sW8FfJlmkiCJGBKiLeOBszrhPPl4aIAzk1c","pdfSize":"786636"}