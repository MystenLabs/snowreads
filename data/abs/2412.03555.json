{
  "id": "2412.03555",
  "title": "PaliGemma 2: A Family of Versatile VLMs for Transfer",
  "authors": "Andreas Steiner, Andr\\'e Susano Pinto, Michael Tschannen, Daniel\n  Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer,\n  Anthony Sherbondy, Shangbang Long, Siyang Qin, Reeve Ingle, Emanuele\n  Bugliarello, Sahar Kazemzadeh, Thomas Mesnard, Ibrahim Alabdulmohsin, Lucas\n  Beyer, Xiaohua Zhai",
  "authorsParsed": [
    [
      "Steiner",
      "Andreas",
      ""
    ],
    [
      "Pinto",
      "Andr√© Susano",
      ""
    ],
    [
      "Tschannen",
      "Michael",
      ""
    ],
    [
      "Keysers",
      "Daniel",
      ""
    ],
    [
      "Wang",
      "Xiao",
      ""
    ],
    [
      "Bitton",
      "Yonatan",
      ""
    ],
    [
      "Gritsenko",
      "Alexey",
      ""
    ],
    [
      "Minderer",
      "Matthias",
      ""
    ],
    [
      "Sherbondy",
      "Anthony",
      ""
    ],
    [
      "Long",
      "Shangbang",
      ""
    ],
    [
      "Qin",
      "Siyang",
      ""
    ],
    [
      "Ingle",
      "Reeve",
      ""
    ],
    [
      "Bugliarello",
      "Emanuele",
      ""
    ],
    [
      "Kazemzadeh",
      "Sahar",
      ""
    ],
    [
      "Mesnard",
      "Thomas",
      ""
    ],
    [
      "Alabdulmohsin",
      "Ibrahim",
      ""
    ],
    [
      "Beyer",
      "Lucas",
      ""
    ],
    [
      "Zhai",
      "Xiaohua",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 18:50:42 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733338242000,
  "abstract": "  PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM)\nbased on the Gemma 2 family of language models. We combine the SigLIP-So400m\nvision encoder that was also used by PaliGemma with the whole range of Gemma 2\nmodels, from the 2B one all the way up to the 27B model. We train these models\nat three resolutions (224px, 448px, and 896px) in multiple stages to equip them\nwith broad knowledge for transfer via fine-tuning. The resulting family of base\nmodels covering different model sizes and resolutions allows us to investigate\nfactors impacting transfer performance (such as learning rate) and to analyze\nthe interplay between the type of task, model size, and resolution. We further\nincrease the number and breadth of transfer tasks beyond the scope of PaliGemma\nincluding different OCR-related tasks such as table structure recognition,\nmolecular structure recognition, music score recognition, as well as long\nfine-grained captioning and radiography report generation, on which PaliGemma 2\nobtains state-of-the-art results.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "hXKk2Rd5oA-W2NZ-OqHhhr_7sbir0fnYvdfDauzLzDs",
  "pdfSize": "2317561"
}