{"id":"2407.10241","title":"BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs","authors":"Zhiting Fan, Ruizhe Chen, Ruiling Xu, Zuozhu Liu","authorsParsed":[["Fan","Zhiting",""],["Chen","Ruizhe",""],["Xu","Ruiling",""],["Liu","Zuozhu",""]],"versions":[{"version":"v1","created":"Sun, 14 Jul 2024 15:17:02 GMT"},{"version":"v2","created":"Sat, 20 Jul 2024 15:59:46 GMT"}],"updateDate":"2024-07-23","timestamp":1720970222000,"abstract":"  Evaluating the bias in Large Language Models (LLMs) becomes increasingly\ncrucial with their rapid development. However, existing evaluation methods rely\non fixed-form outputs and cannot adapt to the flexible open-text generation\nscenarios of LLMs (e.g., sentence completion and question answering). To\naddress this, we introduce BiasAlert, a plug-and-play tool designed to detect\nsocial bias in open-text generations of LLMs. BiasAlert integrates external\nhuman knowledge with inherent reasoning capabilities to detect bias reliably.\nExtensive experiments demonstrate that BiasAlert significantly outperforms\nexisting state-of-the-art methods like GPT4-as-A-Judge in detecting bias.\nFurthermore, through application studies, we demonstrate the utility of\nBiasAlert in reliable LLM bias evaluation and bias mitigation across various\nscenarios. Model and code will be publicly released.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"oBToe7y6ZFMdD_tpKOUrUCOHMJ-Tv9A_aCj-FjgisKY","pdfSize":"2309779"}