{
  "id": "2412.07262",
  "title": "Deep Lidar-guided Image Deblurring",
  "authors": "Ziyao Yi, Diego Valsesia, Tiziano Bianchi, Enrico Magli",
  "authorsParsed": [
    [
      "Yi",
      "Ziyao",
      ""
    ],
    [
      "Valsesia",
      "Diego",
      ""
    ],
    [
      "Bianchi",
      "Tiziano",
      ""
    ],
    [
      "Magli",
      "Enrico",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 07:42:46 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733816566000,
  "abstract": "  The rise of portable Lidar instruments, including their adoption in\nsmartphones, opens the door to novel computational imaging techniques. Being an\nactive sensing instrument, Lidar can provide complementary data to passive\noptical sensors, particularly in situations like low-light imaging where motion\nblur can affect photos. In this paper, we study if the depth information\nprovided by mobile Lidar sensors is useful for the task of image deblurring and\nhow to integrate it with a general approach that transforms any\nstate-of-the-art neural deblurring model into a depth-aware one. To achieve\nthis, we developed a universal adapter structure that efficiently preprocesses\nthe depth information to modulate image features with depth features.\nAdditionally, we applied a continual learning strategy to pretrained\nencoder-decoder models, enabling them to incorporate depth information as an\nadditional input with minimal extra data requirements. We demonstrate that\nutilizing true depth information can significantly boost the effectiveness of\ndeblurring algorithms, as validated on a dataset with real-world depth data\ncaptured by a smartphone Lidar.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Electrical Engineering and Systems Science/Image and Video Processing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "WvUUxMU3wPYKewv-sr-6jLOQUwvk4GaelBoOgk1Bv-c",
  "pdfSize": "11071238"
}