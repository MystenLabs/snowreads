{"id":"2412.02919","title":"Higher Order Transformers: Efficient Attention Mechanism for Tensor\n  Structured Data","authors":"Soroush Omranpour, Guillaume Rabusseau, Reihaneh Rabbany","authorsParsed":[["Omranpour","Soroush",""],["Rabusseau","Guillaume",""],["Rabbany","Reihaneh",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 00:10:47 GMT"}],"updateDate":"2024-12-05","timestamp":1733271047000,"abstract":"  Transformers are now ubiquitous for sequence modeling tasks, but their\nextension to multi-dimensional data remains a challenge due to the quadratic\ncost of the attention mechanism. In this paper, we propose Higher-Order\nTransformers (HOT), a novel architecture designed to efficiently process data\nwith more than two axes, i.e. higher-order tensors. To address the\ncomputational challenges associated with high-order tensor attention, we\nintroduce a novel Kronecker factorized attention mechanism that reduces the\nattention cost to quadratic in each axis' dimension, rather than quadratic in\nthe total size of the input tensor. To further enhance efficiency, HOT\nleverages kernelized attention, reducing the complexity to linear. This\nstrategy maintains the model's expressiveness while enabling scalable attention\ncomputation. We validate the effectiveness of HOT on two high-dimensional\ntasks, including multivariate time series forecasting, and 3D medical image\nclassification. Experimental results demonstrate that HOT achieves competitive\nperformance while significantly improving computational efficiency, showcasing\nits potential for tackling a wide range of complex, multi-dimensional data.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"PJ3ebTm2iyFXuWXblLCv3vprYF_G1qUtJUanC6HoqUU","pdfSize":"1114428"}