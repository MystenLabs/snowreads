{"id":"2407.02466","title":"PWM: Policy Learning with Large World Models","authors":"Ignat Georgiev, Varun Giridhar, Nicklas Hansen and Animesh Garg","authorsParsed":[["Georgiev","Ignat",""],["Giridhar","Varun",""],["Hansen","Nicklas",""],["Garg","Animesh",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:47:03 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 13:24:02 GMT"}],"updateDate":"2024-07-04","timestamp":1719942423000,"abstract":"  Reinforcement Learning (RL) has achieved impressive results on complex tasks\nbut struggles in multi-task settings with different embodiments. World models\noffer scalability by learning a simulation of the environment, yet they often\nrely on inefficient gradient-free optimization methods. We introduce Policy\nlearning with large World Models (PWM), a novel model-based RL algorithm that\nlearns continuous control policies from large multi-task world models. By\npre-training the world model on offline data and using it for first-order\ngradient policy learning, PWM effectively solves tasks with up to 152 action\ndimensions and outperforms methods using ground-truth dynamics. Additionally,\nPWM scales to an 80-task setting, achieving up to 27% higher rewards than\nexisting baselines without the need for expensive online planning.\nVisualizations and code available at https://www.imgeorgiev.com/pwm\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_SzOE5qNQa6YWyFFNaImzMiD_a13nnprywRm-xvM47k","pdfSize":"14748317","objectId":"0xb7cd3c7cbe45733cfb4b0fa68b3605052acc7e5dde0226253829bb2857577b05","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
