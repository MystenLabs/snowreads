{
  "id": "2412.15536",
  "title": "The Impact of Cut Layer Selection in Split Federated Learning",
  "authors": "Justin Dachille, Chao Huang, Xin Liu",
  "authorsParsed": [
    [
      "Dachille",
      "Justin",
      ""
    ],
    [
      "Huang",
      "Chao",
      ""
    ],
    [
      "Liu",
      "Xin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 03:52:54 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734666774000,
  "abstract": "  Split Federated Learning (SFL) is a distributed machine learning paradigm\nthat combines federated learning and split learning. In SFL, a neural network\nis partitioned at a cut layer, with the initial layers deployed on clients and\nremaining layers on a training server. There are two main variants of SFL:\nSFL-V1 where the training server maintains separate server-side models for each\nclient, and SFL-V2 where the training server maintains a single shared model\nfor all clients. While existing studies have focused on algorithm development\nfor SFL, a comprehensive quantitative analysis of how the cut layer selection\naffects model performance remains unexplored. This paper addresses this gap by\nproviding numerical and theoretical analysis of SFL performance and convergence\nrelative to cut layer selection. We find that SFL-V1 is relatively invariant to\nthe choice of cut layer, which is consistent with our theoretical results.\nNumerical experiments on four datasets and two neural networks show that the\ncut layer selection significantly affects the performance of SFL-V2. Moreover,\nSFL-V2 with an appropriate cut layer selection outperforms FedAvg on\nheterogeneous data.\n",
  "subjects": [
    "Computer Science/Distributed, Parallel, and Cluster Computing",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Wd3aGZNctOM59gyLRnx_WnaAkGaQsSZuNzFe4VQpLUo",
  "pdfSize": "1098320"
}