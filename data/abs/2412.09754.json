{"id":"2412.09754","title":"ViCaS: A Dataset for Combining Holistic and Pixel-level Video\n  Understanding using Captions with Grounded Segmentation","authors":"Ali Athar, Xueqing Deng, Liang-Chieh Chen","authorsParsed":[["Athar","Ali",""],["Deng","Xueqing",""],["Chen","Liang-Chieh",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 23:10:54 GMT"},{"version":"v2","created":"Tue, 17 Dec 2024 21:14:50 GMT"}],"updateDate":"2024-12-19","timestamp":1734045054000,"abstract":"  Recent advances in multimodal large language models (MLLMs) have expanded\nresearch in video understanding, primarily focusing on high-level tasks such as\nvideo captioning and question-answering. Meanwhile, a smaller body of work\naddresses dense, pixel-precise segmentation tasks, which typically involve\ncategory-guided or referral-based object segmentation. Although both research\ndirections are essential for developing models with human-level video\ncomprehension, they have largely evolved separately, with distinct benchmarks\nand architectures. This paper aims to unify these efforts by introducing ViCaS,\na new dataset containing thousands of challenging videos, each annotated with\ndetailed, human-written captions and temporally consistent, pixel-accurate\nmasks for multiple objects with phrase grounding. Our benchmark evaluates\nmodels on both holistic/high-level understanding and language-guided,\npixel-precise segmentation. We also present carefully validated evaluation\nmeasures and propose an effective model architecture that can tackle our\nbenchmark. The project page is at https://ali2500.github.io/vicas-project/\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"S-wncpPRNIwhs4w1K_dJz0QwSqUgRgB6Spm_TPDkf9Q","pdfSize":"2159994"}