{"id":"2407.08337","title":"FedLog: Personalized Federated Classification with Less Communication\n  and More Flexibility","authors":"Haolin Yu, Guojun Zhang, Pascal Poupart","authorsParsed":[["Yu","Haolin",""],["Zhang","Guojun",""],["Poupart","Pascal",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 09:40:29 GMT"}],"updateDate":"2024-07-12","timestamp":1720690829000,"abstract":"  In federated learning (FL), the common paradigm that FedAvg proposes and most\nalgorithms follow is that clients train local models with their private data,\nand the model parameters are shared for central aggregation, mostly averaging.\nIn this paradigm, the communication cost is often a challenge, as modern\nmassive neural networks can contain millions to billions parameters. We suggest\nthat clients do not share model parameters but local data summaries, to\ndecrease the cost of sharing. We develop a new algorithm FedLog with Bayesian\ninference, which shares only sufficient statistics of local data. FedLog\ntransmits messages as small as the last layer of the original model. We\nconducted comprehensive experiments to show we outperform other FL algorithms\nthat aim at decreasing the communication cost. To provide formal privacy\nguarantees, we further extend FedLog with differential privacy and show the\ntrade-off between privacy budget and accuracy.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"vbgRGgdKHhGZ7LVuYI-bCHgMgjIYJWzdyAvmNrQqdIs","pdfSize":"1748809"}