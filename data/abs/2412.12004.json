{"id":"2412.12004","title":"The Open Source Advantage in Large Language Models (LLMs)","authors":"Jiya Manchanda, Laura Boettcher, Matheus Westphalen, Jasser Jasser","authorsParsed":[["Manchanda","Jiya",""],["Boettcher","Laura",""],["Westphalen","Matheus",""],["Jasser","Jasser",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 17:32:11 GMT"},{"version":"v2","created":"Sun, 2 Feb 2025 21:27:06 GMT"}],"updateDate":"2025-02-04","timestamp":1734370331000,"abstract":"  Large language models (LLMs) have rapidly advanced natural language\nprocessing, driving significant breakthroughs in tasks such as text generation,\nmachine translation, and domain-specific reasoning. The field now faces a\ncritical dilemma in its approach: closed-source models like GPT-4 deliver\nstate-of-the-art performance but restrict reproducibility, accessibility, and\nexternal oversight, while open-source frameworks like LLaMA and Mixtral\ndemocratize access, foster collaboration, and support diverse applications,\nachieving competitive results through techniques like instruction tuning and\nLoRA. Hybrid approaches address challenges like bias mitigation and resource\naccessibility by combining the scalability of closed-source systems with the\ntransparency and inclusivity of open-source framework. However, in this\nposition paper, we argue that open-source remains the most robust path for\nadvancing LLM research and ethical deployment.\n","subjects":["Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GTrp_lG7pCXSv8V3LwhcDO9Pe0md76PC1dOr1KGp5to","pdfSize":"303510"}