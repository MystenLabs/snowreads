{"id":"2412.19726","title":"Position: Theory of Mind Benchmarks are Broken for Large Language Models","authors":"Matthew Riemer, Zahra Ashktorab, Djallel Bouneffouf, Payel Das, Miao\n  Liu, Justin D. Weisz, and Murray Campbell","authorsParsed":[["Riemer","Matthew",""],["Ashktorab","Zahra",""],["Bouneffouf","Djallel",""],["Das","Payel",""],["Liu","Miao",""],["Weisz","Justin D.",""],["Campbell","Murray",""]],"versions":[{"version":"v1","created":"Fri, 27 Dec 2024 16:30:12 GMT"},{"version":"v2","created":"Wed, 5 Feb 2025 19:27:20 GMT"}],"updateDate":"2025-02-07","timestamp":1735317012000,"abstract":"  This position paper argues that the majority of theory of mind benchmarks are\nbroken because of their inability to directly test how large language models\n(LLMs) adapt to new partners. This problem stems from the fact that theory of\nmind benchmarks for LLMs are overwhelmingly inspired by the methods used to\ntest theory of mind in humans and fall victim to a fallacy of attributing\nhuman-like qualities to AI agents. We expect that humans will engage in a\nconsistent reasoning process across various questions about a situation, but\nthis is known to not be the case for current LLMs. Most theory of mind\nbenchmarks only measure what we call literal theory of mind: the ability to\npredict the behavior of others. Measuring this kind of reasoning is very\ninformative in testing the ability of agents with self-consistent reasoning.\nHowever, it is important to note the distinction between this and what we\nactually care about when this self-consistency cannot be taken for granted. We\ncall this functional theory of mind: the ability to adapt to agents in-context\nfollowing a rational response to predictions about their behavior. We find that\ntop performing open source LLMs may display strong capabilities in literal\ntheory of mind, depending on how they are prompted, but seem to struggle with\nfunctional theory of mind -- even when partner policies are exceedingly simple.\nSimply put, strong literal theory of mind performance does not necessarily\nimply strong functional theory of mind performance. Achieving functional theory\nof mind, particularly over long interaction horizons with a partner, is a\nsignificant challenge deserving a prominent role in any meaningful LLM theory\nof mind evaluation.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wGIzpeInBj2Qqb4cB6OAo2k4B0PB5V3Gb2M0anYqoa8","pdfSize":"8613740"}