{"id":"2407.06460","title":"MUSE: Machine Unlearning Six-Way Evaluation for Language Models","authors":"Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao,\n  Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah A. Smith, Chiyuan Zhang","authorsParsed":[["Shi","Weijia",""],["Lee","Jaechan",""],["Huang","Yangsibo",""],["Malladi","Sadhika",""],["Zhao","Jieyu",""],["Holtzman","Ari",""],["Liu","Daogao",""],["Zettlemoyer","Luke",""],["Smith","Noah A.",""],["Zhang","Chiyuan",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 23:47:29 GMT"},{"version":"v2","created":"Sun, 14 Jul 2024 20:14:02 GMT"}],"updateDate":"2024-07-16","timestamp":1720482449000,"abstract":"  Language models (LMs) are trained on vast amounts of text data, which may\ninclude private and copyrighted content. Data owners may request the removal of\ntheir data from a trained model due to privacy or copyright concerns. However,\nexactly unlearning only these datapoints (i.e., retraining with the data\nremoved) is intractable in modern-day models. This has led to the development\nof many approximate unlearning algorithms. The evaluation of the efficacy of\nthese algorithms has traditionally been narrow in scope, failing to precisely\nquantify the success and practicality of the algorithm from the perspectives of\nboth the model deployers and the data owners. We address this issue by\nproposing MUSE, a comprehensive machine unlearning evaluation benchmark that\nenumerates six diverse desirable properties for unlearned models: (1) no\nverbatim memorization, (2) no knowledge memorization, (3) no privacy leakage,\n(4) utility preservation on data not intended for removal, (5) scalability with\nrespect to the size of removal requests, and (6) sustainability over sequential\nunlearning requests. Using these criteria, we benchmark how effectively eight\npopular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter\nbooks and news articles. Our results demonstrate that most algorithms can\nprevent verbatim memorization and knowledge memorization to varying degrees,\nbut only one algorithm does not lead to severe privacy leakage. Furthermore,\nexisting algorithms fail to meet deployer's expectations because they often\ndegrade general model utility and also cannot sustainably accommodate\nsuccessive unlearning requests or large-scale content removal. Our findings\nidentify key issues with the practicality of existing unlearning algorithms on\nlanguage models, and we release our benchmark to facilitate further\nevaluations: muse-bench.github.io\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"BLeJ0RSr5Lf9zlVDC8LJ-O5wkUWY3WZd5VZLRN5-7tk","pdfSize":"1526356"}