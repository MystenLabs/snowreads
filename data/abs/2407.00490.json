{"id":"2407.00490","title":"Toward Global Convergence of Gradient EM for Over-Parameterized Gaussian\n  Mixture Models","authors":"Weihang Xu, Maryam Fazel, Simon S. Du","authorsParsed":[["Xu","Weihang",""],["Fazel","Maryam",""],["Du","Simon S.",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 16:44:29 GMT"}],"updateDate":"2024-07-02","timestamp":1719679469000,"abstract":"  We study the gradient Expectation-Maximization (EM) algorithm for Gaussian\nMixture Models (GMM) in the over-parameterized setting, where a general GMM\nwith $n>1$ components learns from data that are generated by a single ground\ntruth Gaussian distribution. While results for the special case of 2-Gaussian\nmixtures are well-known, a general global convergence analysis for arbitrary\n$n$ remains unresolved and faces several new technical barriers since the\nconvergence becomes sub-linear and non-monotonic. To address these challenges,\nwe construct a novel likelihood-based convergence analysis framework and\nrigorously prove that gradient EM converges globally with a sublinear rate\n$O(1/\\sqrt{t})$. This is the first global convergence result for Gaussian\nmixtures with more than $2$ components. The sublinear convergence rate is due\nto the algorithmic nature of learning over-parameterized GMM with gradient EM.\nWe also identify a new emerging technical challenge for learning general\nover-parameterized GMM: the existence of bad local regions that can trap\ngradient EM for an exponential number of steps.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"efWayVd40mSGr4xFudy3GwVS4R-7I36OYScop1qRZ1Q","pdfSize":"627880"}