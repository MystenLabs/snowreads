{"id":"2412.19311","title":"xSRL: Safety-Aware Explainable Reinforcement Learning -- Safety as a\n  Product of Explainability","authors":"Risal Shahriar Shefin, Md Asifur Rahman, Thai Le, Sarra Alqahtani","authorsParsed":[["Shefin","Risal Shahriar",""],["Rahman","Md Asifur",""],["Le","Thai",""],["Alqahtani","Sarra",""]],"versions":[{"version":"v1","created":"Thu, 26 Dec 2024 18:19:04 GMT"}],"updateDate":"2024-12-30","timestamp":1735237144000,"abstract":"  Reinforcement learning (RL) has shown great promise in simulated\nenvironments, such as games, where failures have minimal consequences. However,\nthe deployment of RL agents in real-world systems such as autonomous vehicles,\nrobotics, UAVs, and medical devices demands a higher level of safety and\ntransparency, particularly when facing adversarial threats. Safe RL algorithms\nhave been developed to address these concerns by optimizing both task\nperformance and safety constraints. However, errors are inevitable, and when\nthey occur, it is essential that the RL agents can also explain their actions\nto human operators. This makes trust in the safety mechanisms of RL systems\ncrucial for effective deployment. Explainability plays a key role in building\nthis trust by providing clear, actionable insights into the agent's\ndecision-making process, ensuring that safety-critical decisions are well\nunderstood. While machine learning (ML) has seen significant advances in\ninterpretability and visualization, explainability methods for RL remain\nlimited. Current tools fail to address the dynamic, sequential nature of RL and\nits needs to balance task performance with safety constraints over time. The\nre-purposing of traditional ML methods, such as saliency maps, is inadequate\nfor safety-critical RL applications where mistakes can result in severe\nconsequences. To bridge this gap, we propose xSRL, a framework that integrates\nboth local and global explanations to provide a comprehensive understanding of\nRL agents' behavior. xSRL also enables developers to identify policy\nvulnerabilities through adversarial attacks, offering tools to debug and patch\nagents without retraining. Our experiments and user studies demonstrate xSRL's\neffectiveness in increasing safety in RL systems, making them more reliable and\ntrustworthy for real-world deployment. Code is available at\nhttps://github.com/risal-shefin/xSRL.\n","subjects":["Computer Science/Artificial Intelligence","Computer Science/Human-Computer Interaction","Computer Science/Machine Learning","Computer Science/Multiagent Systems"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"d1EO-C32TjWp9QVm2mSGi5q84lCGXZpfBb40xErItg0","pdfSize":"2122615"}