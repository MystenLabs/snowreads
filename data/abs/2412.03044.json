{"id":"2412.03044","title":"Frequency-Guided Diffusion Model with Perturbation Training for\n  Skeleton-Based Video Anomaly Detection","authors":"Xiaofeng Tan, Hongsong Wang and Xin Geng","authorsParsed":[["Tan","Xiaofeng",""],["Wang","Hongsong",""],["Geng","Xin",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 05:43:53 GMT"}],"updateDate":"2024-12-05","timestamp":1733291033000,"abstract":"  Video anomaly detection is an essential yet challenging open-set task in\ncomputer vision, often addressed by leveraging reconstruction as a proxy task.\nHowever, existing reconstruction-based methods encounter challenges in two main\naspects: (1) limited model robustness for open-set scenarios, (2) and an\noveremphasis on, but restricted capacity for, detailed motion reconstruction.\nTo this end, we propose a novel frequency-guided diffusion model with\nperturbation training, which enhances the model robustness by perturbation\ntraining and emphasizes the principal motion components guided by motion\nfrequencies. Specifically, we first use a trainable generator to produce\nperturbative samples for perturbation training of the diffusion model. During\nthe perturbation training phase, the model robustness is enhanced and the\ndomain of the reconstructed model is broadened by training against this\ngenerator. Subsequently, perturbative samples are introduced for inference,\nwhich impacts the reconstruction of normal and abnormal motions differentially,\nthereby enhancing their separability. Considering that motion details originate\nfrom high-frequency information, we propose a masking method based on 2D\ndiscrete cosine transform to separate high-frequency information and\nlow-frequency information. Guided by the high-frequency information from\nobserved motion, the diffusion model can focus on generating low-frequency\ninformation, and thus reconstructing the motion accurately. Experimental\nresults on five video anomaly detection datasets, including human-related and\nopen-set benchmarks, demonstrate the effectiveness of the proposed method. Our\ncode is available at https://github.com/Xiaofeng-Tan/FGDMAD-Code.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"EmY43hT91pVDRcKiMs01Qwijpf63xXV9X16CATNFITM","pdfSize":"1270418"}