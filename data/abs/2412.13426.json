{
  "id": "2412.13426",
  "title": "Safeguarding System Prompts for LLMs",
  "authors": "Zhifeng Jiang, Zhihua Jin, Guoliang He",
  "authorsParsed": [
    [
      "Jiang",
      "Zhifeng",
      ""
    ],
    [
      "Jin",
      "Zhihua",
      ""
    ],
    [
      "He",
      "Guoliang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 01:43:25 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 9 Jan 2025 14:33:25 GMT"
    }
  ],
  "updateDate": "2025-01-10",
  "timestamp": 1734486205000,
  "abstract": "  Large language models (LLMs) are increasingly utilized in applications where\nsystem prompts, which guide model outputs, play a crucial role. These prompts\noften contain business logic and sensitive information, making their protection\nessential. However, adversarial and even regular user queries can exploit LLM\nvulnerabilities to expose these hidden prompts. To address this issue, we\npropose PromptKeeper, a robust defense mechanism designed to safeguard system\nprompts. PromptKeeper tackles two core challenges: reliably detecting prompt\nleakage and mitigating side-channel vulnerabilities when leakage occurs. By\nframing detection as a hypothesis-testing problem, PromptKeeper effectively\nidentifies both explicit and subtle leakage. Upon detection, it regenerates\nresponses using a dummy prompt, ensuring that outputs remain indistinguishable\nfrom typical interactions when no leakage is present. PromptKeeper ensures\nrobust protection against prompt extraction attacks via either adversarial or\nregular queries, while preserving conversational capability and runtime\nefficiency during benign user interactions.\n",
  "subjects": [
    "Computer Science/Cryptography and Security",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "yVTAjm6Pb3AOgGZcf1S9L_yyal6-YpXhyXdia8_Qgo8",
  "pdfSize": "1422365"
}