{"id":"2412.15584","title":"To Rely or Not to Rely? Evaluating Interventions for Appropriate\n  Reliance on Large Language Models","authors":"Jessica Y. Bo, Sophia Wan, and Ashton Anderson","authorsParsed":[["Bo","Jessica Y.",""],["Wan","Sophia",""],["Anderson","Ashton",""]],"versions":[{"version":"v1","created":"Fri, 20 Dec 2024 05:40:32 GMT"}],"updateDate":"2024-12-23","timestamp":1734673232000,"abstract":"  As Large Language Models become integral to decision-making, optimism about\ntheir power is tempered with concern over their errors. Users may over-rely on\nLLM advice that is confidently stated but wrong, or under-rely due to mistrust.\nReliance interventions have been developed to help users of LLMs, but they lack\nrigorous evaluation for appropriate reliance. We benchmark the performance of\nthree relevant interventions by conducting a randomized online experiment with\n400 participants attempting two challenging tasks: LSAT logical reasoning and\nimage-based numerical estimation. For each question, participants first\nanswered independently, then received LLM advice modified by one of three\nreliance interventions and answered the question again. Our findings indicate\nthat while interventions reduce over-reliance, they generally fail to improve\nappropriate reliance. Furthermore, people became more confident after making\nincorrect reliance decisions in certain contexts, demonstrating poor\ncalibration. Based on our findings, we discuss implications for designing\neffective reliance interventions in human-LLM collaboration.\n","subjects":["Computer Science/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DsxeBZR1KnvKJ5cFzLmDKxyMSWenCPF5QGm3afA6xEM","pdfSize":"5220288"}