{"id":"2407.17757","title":"CRASH: Crash Recognition and Anticipation System Harnessing with\n  Context-Aware and Temporal Focus Attentions","authors":"Haicheng Liao, Haoyu Sun, Huanming Shen, Chengyue Wang, Kahou Tam,\n  Chunlin Tian, Li Li, Chengzhong Xu, Zhenning Li","authorsParsed":[["Liao","Haicheng",""],["Sun","Haoyu",""],["Shen","Huanming",""],["Wang","Chengyue",""],["Tam","Kahou",""],["Tian","Chunlin",""],["Li","Li",""],["Xu","Chengzhong",""],["Li","Zhenning",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 04:12:49 GMT"}],"updateDate":"2024-07-26","timestamp":1721880769000,"abstract":"  Accurately and promptly predicting accidents among surrounding traffic agents\nfrom camera footage is crucial for the safety of autonomous vehicles (AVs).\nThis task presents substantial challenges stemming from the unpredictable\nnature of traffic accidents, their long-tail distribution, the intricacies of\ntraffic scene dynamics, and the inherently constrained field of vision of\nonboard cameras. To address these challenges, this study introduces a novel\naccident anticipation framework for AVs, termed CRASH. It seamlessly integrates\nfive components: object detector, feature extractor, object-aware module,\ncontext-aware module, and multi-layer fusion. Specifically, we develop the\nobject-aware module to prioritize high-risk objects in complex and ambiguous\nenvironments by calculating the spatial-temporal relationships between traffic\nagents. In parallel, the context-aware is also devised to extend global visual\ninformation from the temporal to the frequency domain using the Fast Fourier\nTransform (FFT) and capture fine-grained visual features of potential objects\nand broader context cues within traffic scenes. To capture a wider range of\nvisual cues, we further propose a multi-layer fusion that dynamically computes\nthe temporal dependencies between different scenes and iteratively updates the\ncorrelations between different visual features for accurate and timely accident\nprediction. Evaluated on real-world datasets--Dashcam Accident Dataset (DAD),\nCar Crash Dataset (CCD), and AnAn Accident Detection (A3D) datasets--our model\nsurpasses existing top baselines in critical evaluation metrics like Average\nPrecision (AP) and mean Time-To-Accident (mTTA). Importantly, its robustness\nand adaptability are particularly evident in challenging driving scenarios with\nmissing or limited training data, demonstrating significant potential for\napplication in real-world autonomous driving systems.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"JZjePA8KfUGmK5cAAdm9pq1XsusiwjaX8t5r7G0ewOA","pdfSize":"10527274"}