{
  "id": "2412.06195",
  "title": "Adaptive Resolution Residual Networks -- Generalizing Across Resolutions\n  Easily and Efficiently",
  "authors": "L\\'ea Demeule, Mahtab Sandhu and Glen Berseth",
  "authorsParsed": [
    [
      "Demeule",
      "LÃ©a",
      ""
    ],
    [
      "Sandhu",
      "Mahtab",
      ""
    ],
    [
      "Berseth",
      "Glen",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 04:25:37 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733718337000,
  "abstract": "  The majority of signal data captured in the real world uses numerous sensors\nwith different resolutions. In practice, however, most deep learning\narchitectures are fixed-resolution; they consider a single resolution at\ntraining time and inference time. This is convenient to implement but fails to\nfully take advantage of the diverse signal data that exists. In contrast, other\ndeep learning architectures are adaptive-resolution; they directly allow\nvarious resolutions to be processed at training time and inference time. This\nbenefits robustness and computational efficiency but introduces difficult\ndesign constraints that hinder mainstream use. In this work, we address the\nshortcomings of both fixed-resolution and adaptive-resolution methods by\nintroducing Adaptive Resolution Residual Networks (ARRNs), which inherit the\nadvantages of adaptive-resolution methods and the ease of use of\nfixed-resolution methods. We construct ARRNs from Laplacian residuals, which\nserve as generic adaptive-resolution adapters for fixed-resolution layers, and\nwhich allow casting high-resolution ARRNs into low-resolution ARRNs at\ninference time by simply omitting high-resolution Laplacian residuals, thus\nreducing computational cost on low-resolution signals without compromising\nperformance. We complement this novel component with Laplacian dropout, which\nregularizes for robustness to a distribution of lower resolutions, and which\nalso regularizes for errors that may be induced by approximate smoothing\nkernels in Laplacian residuals. We provide a solid grounding for the\nadvantageous properties of ARRNs through a theoretical analysis based on neural\noperators, and empirically show that ARRNs embrace the challenge posed by\ndiverse resolutions with greater flexibility, robustness, and computational\nefficiency.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "uwybYYyqgUl4OXEc1BoPaCb4UDaH9jEpXeJvuwiCD5A",
  "pdfSize": "29777777"
}