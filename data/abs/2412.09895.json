{"id":"2412.09895","title":"Building a Multi-modal Spatiotemporal Expert for Zero-shot Action\n  Recognition with CLIP","authors":"Yating Yu, Congqi Cao, Yueran Zhang, Qinyi Lv, Lingtong Min, Yanning\n  Zhang","authorsParsed":[["Yu","Yating",""],["Cao","Congqi",""],["Zhang","Yueran",""],["Lv","Qinyi",""],["Min","Lingtong",""],["Zhang","Yanning",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 06:30:52 GMT"},{"version":"v2","created":"Sun, 9 Feb 2025 12:42:37 GMT"}],"updateDate":"2025-02-11","timestamp":1734071452000,"abstract":"  Zero-shot action recognition (ZSAR) requires collaborative multi-modal\nspatiotemporal understanding. However, finetuning CLIP directly for ZSAR yields\nsuboptimal performance, given its inherent constraints in capturing essential\ntemporal dynamics from both vision and text perspectives, especially when\nencountering novel actions with fine-grained spatiotemporal discrepancies. In\nthis work, we propose Spatiotemporal Dynamic Duo (STDD), a novel CLIP-based\nframework to comprehend multi-modal spatiotemporal dynamics synergistically.\nFor the vision side, we propose an efficient Space-time Cross Attention, which\ncaptures spatiotemporal dynamics flexibly with simple yet effective operations\napplied before and after spatial attention, without adding additional\nparameters or increasing computational complexity. For the semantic side, we\nconduct spatiotemporal text augmentation by comprehensively constructing an\nAction Semantic Knowledge Graph (ASKG) to derive nuanced text prompts. The ASKG\nelaborates on static and dynamic concepts and their interrelations, based on\nthe idea of decomposing actions into spatial appearances and temporal motions.\nDuring the training phase, the frame-level video representations are\nmeticulously aligned with prompt-level nuanced text representations, which are\nconcurrently regulated by the video representations from the frozen CLIP to\nenhance generalizability. Extensive experiments validate the effectiveness of\nour approach, which consistently surpasses state-of-the-art approaches on\npopular video benchmarks (i.e., Kinetics-600, UCF101, and HMDB51) under\nchallenging ZSAR settings.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eI-HjHRbLD6YjhBNTtqEpSaJT1Oh1pLt-qVW2suM2lI","pdfSize":"3153503"}