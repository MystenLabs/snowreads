{"id":"2407.14111","title":"A Mirror Descent-Based Algorithm for Corruption-Tolerant Distributed\n  Gradient Descent","authors":"Shuche Wang and Vincent Y. F. Tan","authorsParsed":[["Wang","Shuche",""],["Tan","Vincent Y. F.",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 08:29:12 GMT"}],"updateDate":"2024-07-22","timestamp":1721377752000,"abstract":"  Distributed gradient descent algorithms have come to the fore in modern\nmachine learning, especially in parallelizing the handling of large datasets\nthat are distributed across several workers. However, scant attention has been\npaid to analyzing the behavior of distributed gradient descent algorithms in\nthe presence of adversarial corruptions instead of random noise. In this paper,\nwe formulate a novel problem in which adversarial corruptions are present in a\ndistributed learning system. We show how to use ideas from (lazy) mirror\ndescent to design a corruption-tolerant distributed optimization algorithm.\nExtensive convergence analysis for (strongly) convex loss functions is provided\nfor different choices of the stepsize. We carefully optimize the stepsize\nschedule to accelerate the convergence of the algorithm, while at the same time\namortizing the effect of the corruption over time. Experiments based on linear\nregression, support vector classification, and softmax classification on the\nMNIST dataset corroborate our theoretical findings.\n","subjects":["Electrical Engineering and Systems Science/Signal Processing","Computing Research Repository/Information Theory","Computing Research Repository/Machine Learning","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jEV2OyiUhi4Gp0VpA8D6_Qn70Mh3hJtvNdobCo_2MyQ","pdfSize":"1028550","objectId":"0x3497b252a51f133c5242d66936fdce7ba014282eebbcb11aa57b87ecc1f642f7","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
