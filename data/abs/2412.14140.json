{"id":"2412.14140","title":"GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking","authors":"Darshan Deshpande, Selvan Sunitha Ravi, Sky CH-Wang, Bartosz\n  Mielczarek, Anand Kannappan, Rebecca Qian","authorsParsed":[["Deshpande","Darshan",""],["Ravi","Selvan Sunitha",""],["CH-Wang","Sky",""],["Mielczarek","Bartosz",""],["Kannappan","Anand",""],["Qian","Rebecca",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 18:41:12 GMT"},{"version":"v2","created":"Fri, 20 Dec 2024 21:59:56 GMT"}],"updateDate":"2024-12-24","timestamp":1734547272000,"abstract":"  The LLM-as-judge paradigm is increasingly being adopted for automated\nevaluation of model outputs. While LLM judges have shown promise on constrained\nevaluation tasks, closed source LLMs display critical shortcomings when\ndeployed in real world applications due to challenges of fine grained metrics\nand explainability, while task specific evaluation models lack cross-domain\ngeneralization. We introduce GLIDER, a powerful 3B evaluator LLM that can score\nany text input and associated context on arbitrary user defined criteria.\nGLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly\noutperforms prior evaluation models, achieving comparable performance to LLMs\n17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,\nspan highlighting and was trained on 685 domains and 183 criteria. Extensive\nqualitative analysis shows that GLIDER scores are highly correlated with human\njudgments, with 91.3% human agreement. We have open-sourced GLIDER to\nfacilitate future research.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_q14eod3_tyoBShkDq3EOVsz4UQmHHjuJO3-Mva6vqU","pdfSize":"1521034"}