{"id":"2412.05918","title":"Block Coordinate Descent Methods for Structured Nonconvex Optimization\n  with Nonseparable Constraints: Optimality Conditions and Global Convergence","authors":"Zhijie Yuan, Ganzhao Yuan, Lei Sun","authorsParsed":[["Yuan","Zhijie",""],["Yuan","Ganzhao",""],["Sun","Lei",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 12:23:28 GMT"},{"version":"v2","created":"Mon, 16 Dec 2024 13:30:45 GMT"}],"updateDate":"2024-12-17","timestamp":1733660608000,"abstract":"  Coordinate descent algorithms are widely used in machine learning and\nlarge-scale data analysis due to their strong optimality guarantees and\nimpressive empirical performance in solving non-convex problems. In this work,\nwe introduce Block Coordinate Descent (BCD) method for structured nonconvex\noptimization with nonseparable constraints. Unlike traditional large-scale\nCoordinate Descent (CD) approaches, we do not assume the constraints are\nseparable. Instead, we account for the possibility of nonlinear coupling among\nthem. By leveraging the inherent problem structure, we propose new CD methods\nto tackle this specific challenge. Under the relatively mild condition of\nlocally bounded non-convexity, we demonstrate that achieving coordinate-wise\nstationary points offer a stronger optimality criterion compared to standard\ncritical points. Furthermore, under the Luo-Tseng error bound conditions, our\nBCD methods exhibit Q-linear convergence to coordinate-wise stationary points\nor critical points. To demonstrate the practical utility of our methods, we\napply them to various machine learning and signal processing models. We also\nprovide the geometry analysis for the models. Experiments on real-world data\nconsistently demonstrate the superior objective values of our approaches\ncompared to existing methods.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VXocQq7gHnS41hpriLJbrFVcWHpIBoHZfzS1bz5WSdk","pdfSize":"7931907"}