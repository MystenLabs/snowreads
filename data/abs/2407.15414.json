{"id":"2407.15414","title":"Weights Shuffling for Improving DPSGD in Transformer-based Models","authors":"Jungang Yang and Zhe Ji and Liyao Xiang","authorsParsed":[["Yang","Jungang",""],["Ji","Zhe",""],["Xiang","Liyao",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 06:41:59 GMT"}],"updateDate":"2024-07-23","timestamp":1721630519000,"abstract":"  Differential Privacy (DP) mechanisms, especially in high-dimensional\nsettings, often face the challenge of maintaining privacy without compromising\nthe data utility. This work introduces an innovative shuffling mechanism in\nDifferentially-Private Stochastic Gradient Descent (DPSGD) to enhance the\nutility of large models at the same privacy guarantee of the unshuffled case.\nSpecifically, we reveal that random shuffling brings additional randomness to\nthe trajectory of gradient descent while not impacting the model accuracy by\nthe permutation invariance property -- the model can be equivalently computed\nin both forward and backward propagations under permutation. We show that\npermutation indeed improves the privacy guarantee of DPSGD in theory, but\ntracking the exact privacy loss on shuffled model is particularly challenging.\nHence we exploit the approximation on sum of lognormal distributions to derive\nthe condition for the shuffled DPSGD to meet the DP guarantee. Auditing results\nshow that our condition offers a DP guarantee quite close to the audited\nprivacy level, demonstrating our approach an effective estimation in practice.\nExperimental results have verified our theoretical derivation and illustrate\nthat our mechanism improves the accuracy of DPSGD over the state-of-the-art\nbaselines on a variety of models and tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"mbbbawBV1vWKLiFvH4ZPKjneEWy_annsU_q_xJkR5QM","pdfSize":"6990030"}