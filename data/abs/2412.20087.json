{
  "id": "2412.20087",
  "title": "On the Validity of Traditional Vulnerability Scoring Systems for\n  Adversarial Attacks against LLMs",
  "authors": "Atmane Ayoub Mansour Bahar and Ahmad Samer Wazan",
  "authorsParsed": [
    [
      "Bahar",
      "Atmane Ayoub Mansour",
      ""
    ],
    [
      "Wazan",
      "Ahmad Samer",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 28 Dec 2024 09:08:37 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735376917000,
  "abstract": "  This research investigates the effectiveness of established vulnerability\nmetrics, such as the Common Vulnerability Scoring System (CVSS), in evaluating\nattacks against Large Language Models (LLMs), with a focus on Adversarial\nAttacks (AAs). The study explores the influence of both general and specific\nmetric factors in determining vulnerability scores, providing new perspectives\non potential enhancements to these metrics.\n  This study adopts a quantitative approach, calculating and comparing the\ncoefficient of variation of vulnerability scores across 56 adversarial attacks\non LLMs. The attacks, sourced from various research papers, and obtained\nthrough online databases, were evaluated using multiple vulnerability metrics.\nScores were determined by averaging the values assessed by three distinct LLMs.\nThe results indicate that existing scoring-systems yield vulnerability scores\nwith minimal variation across different attacks, suggesting that many of the\nmetric factors are inadequate for assessing adversarial attacks on LLMs. This\nis particularly true for context-specific factors or those with predefined\nvalue sets, such as those in CVSS. These findings support the hypothesis that\ncurrent vulnerability metrics, especially those with rigid values, are limited\nin evaluating AAs on LLMs, highlighting the need for the development of more\nflexible, generalized metrics tailored to such attacks.\n  This research offers a fresh analysis of the effectiveness and applicability\nof established vulnerability metrics, particularly in the context of\nAdversarial Attacks on Large Language Models, both of which have gained\nsignificant attention in recent years. Through extensive testing and\ncalculations, the study underscores the limitations of these metrics and opens\nup new avenues for improving and refining vulnerability assessment frameworks\nspecifically tailored for LLMs.\n",
  "subjects": [
    "Computer Science/Cryptography and Security",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "ze1MPiOrIMVcwqpx5zd3NcRE-eLTmKLm9S5HquF_z8g",
  "pdfSize": "970601"
}