{"id":"2412.02338","title":"Stochastic halfspace approximation method for convex optimization with\n  nonsmooth functional constraints","authors":"Nitesh Kumar Singh, Ion Necoara","authorsParsed":[["Singh","Nitesh Kumar",""],["Necoara","Ion",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 10:00:04 GMT"}],"updateDate":"2024-12-04","timestamp":1733220004000,"abstract":"  In this work, we consider convex optimization problems with smooth objective\nfunction and nonsmooth functional constraints. We propose a new stochastic\ngradient algorithm, called Stochastic Halfspace Approximation Method (SHAM), to\nsolve this problem, where at each iteration we first take a gradient step for\nthe objective function and then we perform a projection step onto one halfspace\napproximation of a randomly chosen constraint. We propose various strategies to\ncreate this stochastic halfspace approximation and we provide a unified\nconvergence analysis that yields new convergence rates for SHAM algorithm in\nboth optimality and feasibility criteria evaluated at some average point. In\nparticular, we derive convergence rates of order $\\mathcal{O} (1/\\sqrt{k})$,\nwhen the objective function is only convex, and $\\mathcal{O} (1/k)$ when the\nobjective function is strongly convex. The efficiency of SHAM is illustrated\nthrough detailed numerical simulations.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GyRb7W5ixriPpInl7OFWcE124usY73HEJa8JDyEKboY","pdfSize":"362827"}