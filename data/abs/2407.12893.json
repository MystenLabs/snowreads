{"id":"2407.12893","title":"Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference","authors":"Ghadeer Jaradat, Mohammed Tolba, Ghada Alsuhli, Hani Saleh, Mahmoud\n  Al-Qutayri, Thanos Stouraitis, Baker Mohammad","authorsParsed":[["Jaradat","Ghadeer",""],["Tolba","Mohammed",""],["Alsuhli","Ghada",""],["Saleh","Hani",""],["Al-Qutayri","Mahmoud",""],["Stouraitis","Thanos",""],["Mohammad","Baker",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 11:15:16 GMT"}],"updateDate":"2024-07-19","timestamp":1721214916000,"abstract":"  In the world of deep learning, Transformer models have become very\nsignificant, leading to improvements in many areas from understanding language\nto recognizing images, covering a wide range of applications. Despite their\nsuccess, the deployment of these models in real-time applications, particularly\non edge devices, poses significant challenges due to their quadratic\ncomputational intensity and memory demands. To overcome these challenges we\nintroduce a novel Hybrid Dynamic Pruning (HDP), an efficient\nalgorithm-architecture co-design approach that accelerates transformers using\nhead sparsity, block sparsity and approximation opportunities to reduce\ncomputations in attention and reduce memory access. With the observation of the\nhuge redundancy in attention scores and attention heads, we propose a novel\ninteger-based row-balanced block pruning to prune unimportant blocks in the\nattention matrix at run time, also propose integer-based head pruning to detect\nand prune unimportant heads at an early stage at run time. Also we propose an\napproximation method that reduces attention computations. To efficiently\nsupport these methods with lower latency and power efficiency, we propose a HDP\nco-processor architecture.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"c7WIP_q16QHWQdsyWcmpT8BwoByeXeHanDHTx-K9vo4","pdfSize":"1657714"}