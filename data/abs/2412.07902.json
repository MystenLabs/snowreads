{
  "id": "2412.07902",
  "title": "Low-Rank Correction for Quantized LLMs",
  "authors": "Meyer Scetbon, James Hensman",
  "authorsParsed": [
    [
      "Scetbon",
      "Meyer",
      ""
    ],
    [
      "Hensman",
      "James",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 20:17:46 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733861866000,
  "abstract": "  We consider the problem of model compression for Large Language Models (LLMs)\nat post-training time, where the task is to compress a well-trained model using\nonly a small set of calibration input data. In this work, we introduce a new\nlow-rank approach to correct for quantization errors of \\emph{activations} in\nLLMs: we propose to add low-rank weight matrices in full precision that act on\nthe \\emph{unquantized} activations. We then solve a joint optimization problem\nover the quantized representation of the weights and additional low-rank weight\nmatrices to quantize both weights and activations. We focus on the case of\n4-bit weight-and-activation quantization (W4A4). Using ranks equivalent to 10\\%\nof the original weight matrix size, our approach reduces the accuracy gap with\nthe original model by more than 50\\%. Using ranks equivalent to 30\\% of the\noriginal weight matrix, the accuracy gap is closed completely. We demonstrate\nour results on four recent LLMs, namely Llama-2, Llama-3, Phi-3 and Mixtral\nmodels.\n",
  "subjects": [
    "Statistics/Machine Learning",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "sZ_JF1_SiI44LAcL7csKxJjQm0vYIVEYdfpGpVNyOyQ",
  "pdfSize": "542141"
}