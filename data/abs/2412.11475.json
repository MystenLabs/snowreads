{
  "id": "2412.11475",
  "title": "OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model\n  for Efficient On-Device Inference",
  "authors": "Wei Chen, Zhiyuan Li, Shuo Xin",
  "authorsParsed": [
    [
      "Chen",
      "Wei",
      ""
    ],
    [
      "Li",
      "Zhiyuan",
      ""
    ],
    [
      "Xin",
      "Shuo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 06:38:00 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 25 Dec 2024 01:15:29 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1734331080000,
  "abstract": "  We present OmniVLM, a sub-billion-parameter vision-language model for\nefficient on-device inference. OmniVLM introduces a token compression mechanism\nthat reduces visual token sequence length from 729 to 81 tokens, significantly\nreducing computational overhead while preserving visual-semantic fidelity.\nThrough a multi-stage training pipeline of pretraining, supervised fine-tuning,\nand minimal-edit Direct Preference Optimization (DPO), OmniVLM matches the\nperformance of larger models. On multiple benchmarks including ScienceQA, POPE,\nand MMMU, OmniVLM outperforms existing baselines like nanoLLAVA within a\n968M-parameter footprint. Empirical results on the same laptop demonstrate 9.1x\nfaster time-to-first-token (0.75s vs 6.82s) and 1.5x higher decoding speed\n(29.41 vs 19.20 tokens/s) compared to nanoLLAVA, enabling efficient deployment\non edge devices. The model weights can be accessed on huggingface:\n\\url{https://huggingface.co/NexaAIDev/OmniVLM-968M}, and the inference examples\ncan be find in Appendix B.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "lUa0AkriAB0-fcrW0LhS7W5fWTxiwDW1CTMyflwUv-M",
  "pdfSize": "3317344"
}