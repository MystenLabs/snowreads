{"id":"2412.17321","title":"Assessing Human Editing Effort on LLM-Generated Texts via\n  Compression-Based Edit Distance","authors":"Nicolas Devatine and Louis Abraham","authorsParsed":[["Devatine","Nicolas",""],["Abraham","Louis",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 06:29:25 GMT"}],"updateDate":"2024-12-24","timestamp":1734935365000,"abstract":"  Assessing the extent of human edits on texts generated by Large Language\nModels (LLMs) is crucial to understanding the human-AI interactions and\nimproving the quality of automated text generation systems. Existing edit\ndistance metrics, such as Levenshtein, BLEU, ROUGE, and TER, often fail to\naccurately measure the effort required for post-editing, especially when edits\ninvolve substantial modifications, such as block operations. In this paper, we\nintroduce a novel compression-based edit distance metric grounded in the\nLempel-Ziv-77 algorithm, designed to quantify the amount of post-editing\napplied to LLM-generated texts. Our method leverages the properties of text\ncompression to measure the informational difference between the original and\nedited texts. Through experiments on real-world human edits datasets, we\ndemonstrate that our proposed metric is highly correlated with actual edit time\nand effort. We also show that LLMs exhibit an implicit understanding of editing\nspeed, that aligns well with our metric. Furthermore, we compare our metric\nwith existing ones, highlighting its advantages in capturing complex edits with\nlinear computational efficiency. Our code and data are available at:\nhttps://github.com/NDV-tiime/CompressionDistance\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"z2-TKWUoi4p-99qZsrQts5Mbv7cNfDxXZHXLEdjUTPs","pdfSize":"703958"}