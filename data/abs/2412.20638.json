{"id":"2412.20638","title":"Predicting Long Term Sequential Policy Value Using Softer Surrogates","authors":"Hyunji Nam, Allen Nie, Ge Gao, Vasilis Syrgkanis, Emma Brunskill","authorsParsed":[["Nam","Hyunji",""],["Nie","Allen",""],["Gao","Ge",""],["Syrgkanis","Vasilis",""],["Brunskill","Emma",""]],"versions":[{"version":"v1","created":"Mon, 30 Dec 2024 01:01:15 GMT"},{"version":"v2","created":"Mon, 3 Feb 2025 02:11:14 GMT"}],"updateDate":"2025-02-04","timestamp":1735520475000,"abstract":"  Off-policy policy evaluation (OPE) estimates the outcome of a new policy\nusing historical data collected from a different policy. However, existing OPE\nmethods cannot handle cases when the new policy introduces novel actions. This\nissue commonly occurs in real-world domains, like healthcare, as new drugs and\ntreatments are continuously developed. Novel actions necessitate on-policy data\ncollection, which can be burdensome and expensive if the outcome of interest\ntakes a substantial amount of time to observe--for example, in multi-year\nclinical trials. This raises a key question of how to predict the long-term\noutcome of a policy after only observing its short-term effects? Though in\ngeneral this problem is intractable, under some surrogacy conditions, the\nshort-term on-policy data can be combined with the long-term historical data to\nmake accurate predictions about the new policy's long-term value. In two\nsimulated healthcare examples--HIV and sepsis management--we show that our\nestimators can provide accurate predictions about the policy value only after\nobserving 10\\% of the full horizon data. We also provide finite sample analysis\nof our doubly robust estimators.\n","subjects":["Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zzemxwEEdPTfO4sdM4MbW9H1ISVaWwFMvJd9ab7BdLA","pdfSize":"818135"}