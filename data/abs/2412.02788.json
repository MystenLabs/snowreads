{
  "id": "2412.02788",
  "title": "Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset",
  "authors": "Tilahun Abedissa Taffa, Debayan Banerjee, Yaregal Assabie, and Ricardo\n  Usbeck",
  "authorsParsed": [
    [
      "Taffa",
      "Tilahun Abedissa",
      ""
    ],
    [
      "Banerjee",
      "Debayan",
      ""
    ],
    [
      "Assabie",
      "Yaregal",
      ""
    ],
    [
      "Usbeck",
      "Ricardo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 19:37:00 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 5 Dec 2024 10:30:56 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733254620000,
  "abstract": "  Existing Scholarly Question Answering (QA) methods typically target\nhomogeneous data sources, relying solely on either text or Knowledge Graphs\n(KGs). However, scholarly information often spans heterogeneous sources,\nnecessitating the development of QA systems that integrate information from\nmultiple heterogeneous data sources. To address this challenge, we introduce\nHybrid-SQuAD (Hybrid Scholarly Question Answering Dataset), a novel large-scale\nQA dataset designed to facilitate answering questions incorporating both text\nand KG facts. The dataset consists of 10.5K question-answer pairs generated by\na large language model, leveraging the KGs DBLP and SemOpenAlex alongside\ncorresponding text from Wikipedia. In addition, we propose a RAG-based baseline\nhybrid QA model, achieving an exact match score of 69.65 on the Hybrid-SQuAD\ntest set.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "jHlX5lUI6JMqaZiw0X9soAlLw0XI7nk4WDHQST81lpk",
  "pdfSize": "1184222"
}