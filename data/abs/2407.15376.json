{"id":"2407.15376","title":"Structure-Aware Residual-Center Representation for Self-Supervised\n  Open-Set 3D Cross-Modal Retrieval","authors":"Yang Xu, Yifan Feng, and Yu Jiang","authorsParsed":[["Xu","Yang",""],["Feng","Yifan",""],["Jiang","Yu",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 04:56:13 GMT"}],"updateDate":"2024-07-23","timestamp":1721624173000,"abstract":"  Existing methods of 3D cross-modal retrieval heavily lean on category\ndistribution priors within the training set, which diminishes their efficacy\nwhen tasked with unseen categories under open-set environments. To tackle this\nproblem, we propose the Structure-Aware Residual-Center Representation (SRCR)\nframework for self-supervised open-set 3D cross-modal retrieval. To address the\ncenter deviation due to category distribution differences, we utilize the\nResidual-Center Embedding (RCE) for each object by nested auto-encoders, rather\nthan directly mapping them to the modality or category centers. Besides, we\nperform the Hierarchical Structure Learning (HSL) approach to leverage the\nhigh-order correlations among objects for generalization, by constructing a\nheterogeneous hypergraph structure based on hierarchical inter-modality,\nintra-object, and implicit-category correlations. Extensive experiments and\nablation studies on four benchmarks demonstrate the superiority of our proposed\nframework compared to state-of-the-art methods.\n","subjects":["Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"H9EOYNPYtikAf_IWvAbpy-X9JsyQ3BdgNMnvYw3EpqM","pdfSize":"1955920"}