{
  "id": "2412.17787",
  "title": "Cross-Lingual Text-Rich Visual Comprehension: An Information Theory\n  Perspective",
  "authors": "Xinmiao Yu, Xiaocheng Feng, Yun Li, Minghui Liao, Ya-Qi Yu, Xiachong\n  Feng, Weihong Zhong, Ruihan Chen, Mengkang Hu, Jihao Wu, Dandan Tu, Duyu\n  Tang, Bing Qin",
  "authorsParsed": [
    [
      "Yu",
      "Xinmiao",
      ""
    ],
    [
      "Feng",
      "Xiaocheng",
      ""
    ],
    [
      "Li",
      "Yun",
      ""
    ],
    [
      "Liao",
      "Minghui",
      ""
    ],
    [
      "Yu",
      "Ya-Qi",
      ""
    ],
    [
      "Feng",
      "Xiachong",
      ""
    ],
    [
      "Zhong",
      "Weihong",
      ""
    ],
    [
      "Chen",
      "Ruihan",
      ""
    ],
    [
      "Hu",
      "Mengkang",
      ""
    ],
    [
      "Wu",
      "Jihao",
      ""
    ],
    [
      "Tu",
      "Dandan",
      ""
    ],
    [
      "Tang",
      "Duyu",
      ""
    ],
    [
      "Qin",
      "Bing",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 18:48:04 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734979684000,
  "abstract": "  Recent Large Vision-Language Models (LVLMs) have shown promising reasoning\ncapabilities on text-rich images from charts, tables, and documents. However,\nthe abundant text within such images may increase the model's sensitivity to\nlanguage. This raises the need to evaluate LVLM performance on cross-lingual\ntext-rich visual inputs, where the language in the image differs from the\nlanguage of the instructions. To address this, we introduce XT-VQA\n(Cross-Lingual Text-Rich Visual Question Answering), a benchmark designed to\nassess how LVLMs handle language inconsistency between image text and\nquestions. XT-VQA integrates five existing text-rich VQA datasets and a newly\ncollected dataset, XPaperQA, covering diverse scenarios that require faithful\nrecognition and comprehension of visual information despite language\ninconsistency. Our evaluation of prominent LVLMs on XT-VQA reveals a\nsignificant drop in performance for cross-lingual scenarios, even for models\nwith multilingual capabilities. A mutual information analysis suggests that\nthis performance gap stems from cross-lingual questions failing to adequately\nactivate relevant visual information. To mitigate this issue, we propose\nMVCL-MI (Maximization of Vision-Language Cross-Lingual Mutual Information),\nwhere a visual-text cross-lingual alignment is built by maximizing mutual\ninformation between the model's outputs and visual information. This is\nachieved by distilling knowledge from monolingual to cross-lingual settings\nthrough KL divergence minimization, where monolingual output logits serve as a\nteacher. Experimental results on the XT-VQA demonstrate that MVCL-MI\neffectively reduces the visual-text cross-lingual performance disparity while\npreserving the inherent capabilities of LVLMs, shedding new light on the\npotential practice for improving LVLMs. Codes are available at:\nhttps://github.com/Stardust-y/XTVQA.git\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "5-YAg9VBkP16z-FEqKNKv4koz9Ckyl4yIc9T85hJ0Vw",
  "pdfSize": "14884089"
}