{
  "id": "2412.06465",
  "title": "Agent Journey Beyond RGB: Unveiling Hybrid Semantic-Spatial\n  Environmental Representations for Vision-and-Language Navigation",
  "authors": "Xuesong Zhang and Yunbo Xu and Jia Li and Zhenzhen Hu and Richnag Hong",
  "authorsParsed": [
    [
      "Zhang",
      "Xuesong",
      ""
    ],
    [
      "Xu",
      "Yunbo",
      ""
    ],
    [
      "Li",
      "Jia",
      ""
    ],
    [
      "Hu",
      "Zhenzhen",
      ""
    ],
    [
      "Hong",
      "Richnag",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 13:10:28 GMT"
    },
    {
      "version": "v2",
      "created": "Tue, 10 Dec 2024 09:38:16 GMT"
    },
    {
      "version": "v3",
      "created": "Thu, 12 Dec 2024 03:56:01 GMT"
    }
  ],
  "updateDate": "2024-12-13",
  "timestamp": 1733749828000,
  "abstract": "  Navigating unseen environments based on natural language instructions remains\ndifficult for egocentric agents in Vision-and-Language Navigation (VLN). While\nrecent advancements have yielded promising outcomes, they primarily rely on RGB\nimages for environmental representation, often overlooking the underlying\nsemantic knowledge and spatial cues. Intuitively, humans inherently ground\ntextual semantics within the spatial layout during indoor navigation. Inspired\nby this, we propose a versatile Semantic Understanding and Spatial Awareness\n(SUSA) architecture to facilitate navigation. SUSA includes a Textual Semantic\nUnderstanding (TSU) module, which narrows the modality gap between instructions\nand environments by generating and associating the descriptions of\nenvironmental landmarks in the agent's immediate surroundings. Additionally, a\nDepth-based Spatial Perception (DSP) module incrementally constructs a depth\nexploration map, enabling a more nuanced comprehension of environmental\nlayouts. Experimental results demonstrate that SUSA hybrid semantic-spatial\nrepresentations effectively enhance navigation performance, setting new\nstate-of-the-art performance across three VLN benchmarks (REVERIE, R2R, and\nSOON). The source code will be publicly available.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Multimedia"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "fzms04OFGgaJRzaqw9dl19WNBr9xGKdv1UUBddhYZNc",
  "pdfSize": "7299673"
}