{"id":"2407.07821","title":"When to Accept Automated Predictions and When to Defer to Human\n  Judgment?","authors":"Daniel Sikar, Artur Garcez, Tillman Weyde, Robin Bloomfield and Kaleem\n  Peeroo","authorsParsed":[["Sikar","Daniel",""],["Garcez","Artur",""],["Weyde","Tillman",""],["Bloomfield","Robin",""],["Peeroo","Kaleem",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:45:52 GMT"},{"version":"v2","created":"Tue, 13 Aug 2024 09:06:08 GMT"}],"updateDate":"2024-08-14","timestamp":1720629952000,"abstract":"  Ensuring the reliability and safety of automated decision-making is crucial.\nIt is well-known that data distribution shifts in machine learning can produce\nunreliable outcomes. This paper proposes a new approach for measuring the\nreliability of predictions under distribution shifts. We analyze how the\noutputs of a trained neural network change using clustering to measure\ndistances between outputs and class centroids. We propose this distance as a\nmetric to evaluate the confidence of predictions under distribution shifts. We\nassign each prediction to a cluster with centroid representing the mean softmax\noutput for all correct predictions of a given class. We then define a safety\nthreshold for a class as the smallest distance from an incorrect prediction to\nthe given class centroid. We evaluate the approach on the MNIST and CIFAR-10\ndatasets using a Convolutional Neural Network and a Vision Transformer,\nrespectively. The results show that our approach is consistent across these\ndata sets and network models, and indicate that the proposed metric can offer\nan efficient way of determining when automated predictions are acceptable and\nwhen they should be deferred to human operators given a distribution shift.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"EsMIZUdR_hrf9Uhxgiwm01w-8-_txZNrSDivrnFh1iI","pdfSize":"2040203"}