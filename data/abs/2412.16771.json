{
  "id": "2412.16771",
  "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question\n  Answering and Object Localization",
  "authors": "Tan-Hanh Pham, Hoang-Nam Le, Phu-Vinh Nguyen, Chris Ngo, and\n  Truong-Son Hy",
  "authorsParsed": [
    [
      "Pham",
      "Tan-Hanh",
      ""
    ],
    [
      "Le",
      "Hoang-Nam",
      ""
    ],
    [
      "Nguyen",
      "Phu-Vinh",
      ""
    ],
    [
      "Ngo",
      "Chris",
      ""
    ],
    [
      "Hy",
      "Truong-Son",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 20:52:32 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734814352000,
  "abstract": "  Visual Language Models have demonstrated remarkable capabilities across\ntasks, including visual question answering and image captioning. However, most\nmodels rely on text-based instructions, limiting their effectiveness in\nhuman-machine interactions. Moreover, the quality of language models depends on\nreasoning and prompting techniques, such as COT, which remain underexplored\nwhen using speech instructions. To address these challenges, we propose SilVar,\na novel end-to-end multimodal model that uses speech instructions for reasoning\nin visual question answering. In addition, we investigate reasoning techniques\nwith levels including conversational, simple, and complex speech instruction.\nSilVar is built upon CLIP, Whisper, and LLaMA 3.1-8B, enabling intuitive\ninteractions by allowing users to provide verbal or text instructions. To this\nend, we introduce a dataset designed to challenge models with speech-based\nreasoning tasks for object localization. This dataset enhances the model\nability to process and explain visual scenes from spoken input, moving beyond\nobject recognition to reasoning-based interactions. The experiments show that\nSilVar achieves SOTA performance on the MMMU and ScienceQA benchmarks despite\nthe challenge of speech-based instructions. We believe SilVar will inspire\nnext-generation multimodal reasoning models, toward expert artificial general\nintelligence. Our code and dataset are available here.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "qeE1AvYVAsKTm4Goam6pWP_0hsYrIRe5r5xZiZ63MPc",
  "pdfSize": "3470224"
}