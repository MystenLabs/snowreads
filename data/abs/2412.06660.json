{"id":"2412.06660","title":"MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large\n  Language Models","authors":"Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, Ying Shan","authorsParsed":[["Liu","Shansong",""],["Hussain","Atin Sakkeer",""],["Wu","Qilong",""],["Sun","Chenshuo",""],["Shan","Ying",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 16:59:35 GMT"}],"updateDate":"2024-12-10","timestamp":1733763575000,"abstract":"  Research on large language models has advanced significantly across text,\nspeech, images, and videos. However, multi-modal music understanding and\ngeneration remain underexplored due to the lack of well-annotated datasets. To\naddress this, we introduce a dataset with 167.69 hours of multi-modal data,\nincluding text, images, videos, and music annotations. Based on this dataset,\nwe propose MuMu-LLaMA, a model that leverages pre-trained encoders for music,\nimages, and videos. For music generation, we integrate AudioLDM 2 and MusicGen.\nOur evaluation across four tasks--music understanding, text-to-music\ngeneration, prompt-based music editing, and multi-modal music\ngeneration--demonstrates that MuMu-LLaMA outperforms state-of-the-art models,\nshowing its potential for multi-modal music applications.\n","subjects":["Computer Science/Sound","Computer Science/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bNVv7_Iyvtv-djk3ScWC5-zOvKqgzz8nGi7mysbJYKg","pdfSize":"37162370"}