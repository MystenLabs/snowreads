{"id":"2412.17395","title":"WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models","authors":"Huawen Feng, Pu Zhao, Qingfeng Sun, Can Xu, Fangkai Yang, Lu Wang,\n  Qianli Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang","authorsParsed":[["Feng","Huawen",""],["Zhao","Pu",""],["Sun","Qingfeng",""],["Xu","Can",""],["Yang","Fangkai",""],["Wang","Lu",""],["Ma","Qianli",""],["Lin","Qingwei",""],["Rajmohan","Saravan",""],["Zhang","Dongmei",""],["Zhang","Qi",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 08:47:42 GMT"},{"version":"v2","created":"Thu, 13 Feb 2025 15:11:24 GMT"},{"version":"v3","created":"Tue, 18 Feb 2025 08:01:30 GMT"}],"updateDate":"2025-02-19","timestamp":1734943662000,"abstract":"  Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UTURDcyUGu0tlo3W4skcHE3iqX68iWunZRtuxGwn8GU","pdfSize":"453744"}