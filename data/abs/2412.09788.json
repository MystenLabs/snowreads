{
  "id": "2412.09788",
  "title": "OpenForge: Probabilistic Metadata Integration",
  "authors": "Tianji Cong, Fatemeh Nargesian, Junjie Xing, H. V. Jagadish",
  "authorsParsed": [
    [
      "Cong",
      "Tianji",
      ""
    ],
    [
      "Nargesian",
      "Fatemeh",
      ""
    ],
    [
      "Xing",
      "Junjie",
      ""
    ],
    [
      "Jagadish",
      "H. V.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 02:04:53 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734055493000,
  "abstract": "  Modern data stores increasingly rely on metadata for enabling diverse\nactivities such as data cataloging and search. However, metadata curation\nremains a labor-intensive task, and the broader challenge of metadata\nmaintenance -- ensuring its consistency, usefulness, and freshness -- has been\nlargely overlooked. In this work, we tackle the problem of resolving\nrelationships among metadata concepts from disparate sources. These\nrelationships are critical for creating clean, consistent, and up-to-date\nmetadata repositories, and a central challenge for metadata integration.\n  We propose OpenForge, a two-stage prior-posterior framework for metadata\nintegration. In the first stage, OpenForge exploits multiple methods including\nfine-tuned large language models to obtain prior beliefs about concept\nrelationships. In the second stage, OpenForge refines these predictions by\nleveraging Markov Random Field, a probabilistic graphical model. We formalize\nmetadata integration as an optimization problem, where the objective is to\nidentify the relationship assignments that maximize the joint probability of\nassignments. The MRF formulation allows OpenForge to capture prior beliefs\nwhile encoding critical relationship properties, such as transitivity, in\nprobabilistic inference. Experiments on real-world datasets demonstrate the\neffectiveness and efficiency of OpenForge. On a use case of matching two\nmetadata vocabularies, OpenForge outperforms GPT-4, the second-best method, by\n25 F1-score points.\n",
  "subjects": [
    "Computer Science/Databases"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Z2W_u9OZmiDcJaLaShc4f7y2qxREqa3blVCDC4AsYUY",
  "pdfSize": "1618876"
}