{"id":"2412.13008","title":"RCLMuFN: Relational Context Learning and Multiplex Fusion Network for\n  Multimodal Sarcasm Detection","authors":"Tongguan Wang, Junkai Li, Guixin Su, Yongcheng Zhang, Dongyu Su, Yuxue\n  Hu, Ying Sha","authorsParsed":[["Wang","Tongguan",""],["Li","Junkai",""],["Su","Guixin",""],["Zhang","Yongcheng",""],["Su","Dongyu",""],["Hu","Yuxue",""],["Sha","Ying",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 15:29:31 GMT"}],"updateDate":"2024-12-18","timestamp":1734449371000,"abstract":"  Sarcasm typically conveys emotions of contempt or criticism by expressing a\nmeaning that is contrary to the speaker's true intent. Accurate detection of\nsarcasm aids in identifying and filtering undesirable information on the\nInternet, thereby reducing malicious defamation and rumor-mongering.\nNonetheless, the task of automatic sarcasm detection remains highly challenging\nfor machines, as it critically depends on intricate factors such as relational\ncontext. Most existing multimodal sarcasm detection methods focus on\nintroducing graph structures to establish entity relationships between text and\nimages while neglecting to learn the relational context between text and\nimages, which is crucial evidence for understanding the meaning of sarcasm. In\naddition, the meaning of sarcasm changes with the evolution of different\ncontexts, but existing methods may not be accurate in modeling such dynamic\nchanges, limiting the generalization ability of the models. To address the\nabove issues, we propose a relational context learning and multiplex fusion\nnetwork (RCLMuFN) for multimodal sarcasm detection. Firstly, we employ four\nfeature extractors to comprehensively extract features from raw text and\nimages, aiming to excavate potential features that may have been previously\noverlooked. Secondly, we utilize the relational context learning module to\nlearn the contextual information of text and images and capture the dynamic\nproperties through shallow and deep interactions. Finally, we employ a\nmultiplex feature fusion module to enhance the generalization of the model by\npenetratingly integrating multimodal features derived from various interaction\ncontexts. Extensive experiments on two multimodal sarcasm detection datasets\nshow that our proposed method achieves state-of-the-art performance.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"oPBE5OiFmT9_KrHRgBh0fy_crrxLiljIwwxyyouW6I8","pdfSize":"3488521"}