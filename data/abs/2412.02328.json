{"id":"2412.02328","title":"Efficient Model Compression Techniques with FishLeg","authors":"Jamie McGowan, Wei Sheng Lai, Weibin Chen, Henry Aldridge, Jools\n  Clarke, Jezabel Garcia, Rui Xia, Yilei Liang, Guillaume Hennequin, Alberto\n  Bernacchia","authorsParsed":[["McGowan","Jamie",""],["Lai","Wei Sheng",""],["Chen","Weibin",""],["Aldridge","Henry",""],["Clarke","Jools",""],["Garcia","Jezabel",""],["Xia","Rui",""],["Liang","Yilei",""],["Hennequin","Guillaume",""],["Bernacchia","Alberto",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 09:42:16 GMT"}],"updateDate":"2024-12-04","timestamp":1733218936000,"abstract":"  In many domains, the most successful AI models tend to be the largest, indeed\noften too large to be handled by AI players with limited computational\nresources. To mitigate this, a number of compression methods have been\ndeveloped, including methods that prune the network down to high sparsity\nwhilst retaining performance. The best-performing pruning techniques are often\nthose that use second-order curvature information (such as an estimate of the\nFisher information matrix) to score the importance of each weight and to\npredict the optimal compensation for weight deletion. However, these methods\nare difficult to scale to high-dimensional parameter spaces without making\nheavy approximations. Here, we propose the FishLeg surgeon (FLS), a new\nsecond-order pruning method based on the Fisher-Legendre (FishLeg) optimizer.\nAt the heart of FishLeg is a meta-learning approach to amortising the action of\nthe inverse FIM, which brings a number of advantages. Firstly, the\nparameterisation enables the use of flexible tensor factorisation techniques to\nimprove computational and memory efficiency without sacrificing much accuracy,\nalleviating challenges associated with scalability of most second-order pruning\nmethods. Secondly, directly estimating the inverse FIM leads to less\nsensitivity to the amplification of stochasticity during inversion, thereby\nresulting in more precise estimates. Thirdly, our approach also allows for\nprogressive assimilation of the curvature into the parameterisation. In the\ngradual pruning regime, this results in a more efficient estimate refinement as\nopposed to re-estimation. We find that FishLeg achieves higher or comparable\nperformance against two common baselines in the area, most notably in the high\nsparsity regime when considering a ResNet18 model on CIFAR-10 (84% accuracy at\n95% sparsity vs 60% for OBS) and TinyIM (53% accuracy at 80% sparsity vs 48%\nfor OBS).\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"H0vrwl6xzh9rkSeokEGXc9ZIGIpu8jiskxc0md4M3DE","pdfSize":"674611"}