{
  "id": "2412.16148",
  "title": "Frequency Is What You Need: Word-frequency Masking Benefits\n  Vision-Language Model Pre-training",
  "authors": "Mingliang Liang and Martha Larson",
  "authorsParsed": [
    [
      "Liang",
      "Mingliang",
      ""
    ],
    [
      "Larson",
      "Martha",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 18:51:41 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734720701000,
  "abstract": "  Vision Language Models (VLMs) can be trained more efficiently if training\nsets can be reduced in size. Recent work has shown the benefits of masking text\nduring VLM training using a variety of approaches: truncation, random masking,\nblock masking and syntax masking. In this paper, we show that the best masking\nstrategy changes over training epochs and that, given sufficient training\nepochs, word frequency information is what you need to achieve the best\nperformance. Experiments on a large range of data sets demonstrate the\nadvantages of our approach, called Contrastive Language-Image Pre-training with\nword Frequency Masking (CLIPF). The benefits are particularly evident as the\nnumber of input tokens decreases. We analyze the impact of CLIPF vs. other\nmasking approaches on word frequency balance and discuss the apparently\ncritical contribution of CLIPF in maintaining word frequency balance across POS\ncategories.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "6BzNtIuqtIYXaM63C-6S3tYVCvu89rq8q-m_gG9Yi04",
  "pdfSize": "17405809"
}