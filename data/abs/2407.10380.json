{"id":"2407.10380","title":"NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models","authors":"Pranshu Pandya, Agney S Talwarr, Vatsal Gupta, Tushar Kataria, Vivek\n  Gupta, Dan Roth","authorsParsed":[["Pandya","Pranshu",""],["Talwarr","Agney S",""],["Gupta","Vatsal",""],["Kataria","Tushar",""],["Gupta","Vivek",""],["Roth","Dan",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 01:21:56 GMT"}],"updateDate":"2024-07-16","timestamp":1721006516000,"abstract":"  Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"sfufotZxkoYq9zJCnPvwuPdLXuW5AFPZ8wCrMnjxEqc","pdfSize":"1351767"}