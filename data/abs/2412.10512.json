{
  "id": "2412.10512",
  "title": "Differentially Private Multi-Sampling from Distributions",
  "authors": "Albert Cheu, Debanuj Nayak",
  "authorsParsed": [
    [
      "Cheu",
      "Albert",
      ""
    ],
    [
      "Nayak",
      "Debanuj",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 19:14:05 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734117245000,
  "abstract": "  Many algorithms have been developed to estimate probability distributions\nsubject to differential privacy (DP): such an algorithm takes as input\nindependent samples from a distribution and estimates the density function in a\nway that is insensitive to any one sample. A recent line of work, initiated by\nRaskhodnikova et al. (Neurips '21), explores a weaker objective: a\ndifferentially private algorithm that approximates a single sample from the\ndistribution. Raskhodnikova et al. studied the sample complexity of DP\n\\emph{single-sampling} i.e., the minimum number of samples needed to perform\nthis task. They showed that the sample complexity of DP single-sampling is less\nthan the sample complexity of DP learning for certain distribution classes. We\ndefine two variants of \\emph{multi-sampling}, where the goal is to privately\napproximate $m>1$ samples. This better models the realistic scenario where\nsynthetic data is needed for exploratory data analysis.\n  A baseline solution to \\emph{multi-sampling} is to invoke a single-sampling\nalgorithm $m$ times on independently drawn datasets of samples. When the data\ncomes from a finite domain, we improve over the baseline by a factor of $m$ in\nthe sample complexity. When the data comes from a Gaussian, Ghazi et al.\n(Neurips '23) show that \\emph{single-sampling} can be performed under\napproximate differential privacy; we show it is possible to \\emph{single- and\nmulti-sample Gaussians with known covariance subject to pure DP}. Our solution\nuses a variant of the Laplace mechanism that is of independent interest.\n  We also give sample complexity lower bounds, one for strong multi-sampling of\nfinite distributions and another for weak multi-sampling of bounded-covariance\nGaussians.\n",
  "subjects": [
    "Computer Science/Cryptography and Security",
    "Computer Science/Data Structures and Algorithms",
    "Computer Science/Machine Learning",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "zaZqZJoCjgBNGt69oTloiLRPyqKfGMZ9KhBVnlwZ4UQ",
  "pdfSize": "309661"
}