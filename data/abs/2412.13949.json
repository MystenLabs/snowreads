{
  "id": "2412.13949",
  "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence",
  "authors": "Jinghan He, Kuan Zhu, Haiyun Guo, Junfeng Fang, Zhenglin Hua, Yuheng\n  Jia, Ming Tang, Tat-Seng Chua, Jinqiao Wang",
  "authorsParsed": [
    [
      "He",
      "Jinghan",
      ""
    ],
    [
      "Zhu",
      "Kuan",
      ""
    ],
    [
      "Guo",
      "Haiyun",
      ""
    ],
    [
      "Fang",
      "Junfeng",
      ""
    ],
    [
      "Hua",
      "Zhenglin",
      ""
    ],
    [
      "Jia",
      "Yuheng",
      ""
    ],
    [
      "Tang",
      "Ming",
      ""
    ],
    [
      "Chua",
      "Tat-Seng",
      ""
    ],
    [
      "Wang",
      "Jinqiao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 15:29:30 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 27 Dec 2024 03:00:19 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1734535770000,
  "abstract": "  Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "HigOkxqEqUKw8OcKA-kGOYukJfJ8PM1z-Ihc76mOlTg",
  "pdfSize": "1569792"
}