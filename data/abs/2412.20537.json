{"id":"2412.20537","title":"Diminishing Return of Value Expansion Methods","authors":"Daniel Palenicek, Michael Lutter, Jo\\~ao Carvalho, Daniel Dennert,\n  Faran Ahmad, and Jan Peters","authorsParsed":[["Palenicek","Daniel",""],["Lutter","Michael",""],["Carvalho","Jo√£o",""],["Dennert","Daniel",""],["Ahmad","Faran",""],["Peters","Jan",""]],"versions":[{"version":"v1","created":"Sun, 29 Dec 2024 17:51:11 GMT"}],"updateDate":"2024-12-31","timestamp":1735494671000,"abstract":"  Model-based reinforcement learning aims to increase sample efficiency, but\nthe accuracy of dynamics models and the resulting compounding errors are often\nseen as key limitations. This paper empirically investigates potential sample\nefficiency gains from improved dynamics models in model-based value expansion\nmethods. Our study reveals two key findings when using oracle dynamics models\nto eliminate compounding errors. First, longer rollout horizons enhance sample\nefficiency, but the improvements quickly diminish with each additional\nexpansion step. Second, increased model accuracy only marginally improves\nsample efficiency compared to learned models with identical horizons. These\ndiminishing returns in sample efficiency are particularly noteworthy when\ncompared to model-free value expansion methods. These model-free algorithms\nachieve comparable performance without the computational overhead. Our results\nsuggest that the limitation of model-based value expansion methods cannot be\nattributed to model accuracy. Although higher accuracy is beneficial, even\nperfect models do not provide unrivaled sample efficiency. Therefore, the\nbottleneck exists elsewhere. These results challenge the common assumption that\nmodel accuracy is the primary constraint in model-based reinforcement learning.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"54t_bvWJfAWHXRZ5pmpN9lODRGZjOo98E9IdAH2QU-A","pdfSize":"15932299"}