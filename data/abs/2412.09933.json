{
  "id": "2412.09933",
  "title": "Active Poisoning: Efficient Backdoor Attacks on Transfer Learning-Based\n  Brain-Computer Interfaces",
  "authors": "X. Jiang, L. Meng, S. Li and D. Wu",
  "authorsParsed": [
    [
      "Jiang",
      "X.",
      ""
    ],
    [
      "Meng",
      "L.",
      ""
    ],
    [
      "Li",
      "S.",
      ""
    ],
    [
      "Wu",
      "D.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 07:39:26 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734075566000,
  "abstract": "  Transfer learning (TL) has been widely used in electroencephalogram\n(EEG)-based brain-computer interfaces (BCIs) for reducing calibration efforts.\nHowever, backdoor attacks could be introduced through TL. In such attacks, an\nattacker embeds a backdoor with a specific pattern into the machine learning\nmodel. As a result, the model will misclassify a test sample with the backdoor\ntrigger into a prespecified class while still maintaining good performance on\nbenign samples. Accordingly, this study explores backdoor attacks in the TL of\nEEG-based BCIs, where source-domain data are poisoned by a backdoor trigger and\nthen used in TL. We propose several active poisoning approaches to select\nsource-domain samples, which are most effective in embedding the backdoor\npattern, to improve the attack success rate and efficiency. Experiments on four\nEEG datasets and three deep learning models demonstrate the effectiveness of\nthe approaches. To our knowledge, this is the first study about backdoor\nattacks on TL models in EEG-based BCIs. It exposes a serious security risk in\nBCIs, which should be immediately addressed.\n",
  "subjects": [
    "Computer Science/Human-Computer Interaction"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Y97p09bL2D4Kub3E7hd7RZlLvLhxkO3jnbsjheWZJ6Q",
  "pdfSize": "4431369"
}