{"id":"2412.15386","title":"Systematic Evaluation of Long-Context LLMs on Financial Concepts","authors":"Lavanya Gupta, Saket Sharma, Yiyun Zhao","authorsParsed":[["Gupta","Lavanya",""],["Sharma","Saket",""],["Zhao","Yiyun",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 20:26:55 GMT"}],"updateDate":"2024-12-23","timestamp":1734640015000,"abstract":"  Long-context large language models (LC LLMs) promise to increase reliability\nof LLMs in real-world tasks requiring processing and understanding of long\ninput documents. However, this ability of LC LLMs to reliably utilize their\ngrowing context windows remains under investigation. In this work, we evaluate\nthe performance of state-of-the-art GPT-4 suite of LC LLMs in solving a series\nof progressively challenging tasks, as a function of factors such as context\nlength, task difficulty, and position of key information by creating a real\nworld financial news dataset. Our findings indicate that LC LLMs exhibit\nbrittleness at longer context lengths even for simple tasks, with performance\ndeteriorating sharply as task complexity increases. At longer context lengths,\nthese state-of-the-art models experience catastrophic failures in instruction\nfollowing resulting in degenerate outputs. Our prompt ablations also reveal\nunfortunate continued sensitivity to both the placement of the task instruction\nin the context window as well as minor markdown formatting. Finally, we\nadvocate for more rigorous evaluation of LC LLMs by employing holistic metrics\nsuch as F1 (rather than recall) and reporting confidence intervals, thereby\nensuring robust and conclusive findings.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"u-9XmSHt4CPgJGuq2MOvpCixHy8BeVpFNHm9E7CBBus","pdfSize":"5733689"}