{"id":"2412.00342","title":"Empowering the Deaf and Hard of Hearing Community: Enhancing Video\n  Captions Using Large Language Models","authors":"Nadeen Fathallah, Monika Bhole, Steffen Staab","authorsParsed":[["Fathallah","Nadeen",""],["Bhole","Monika",""],["Staab","Steffen",""]],"versions":[{"version":"v1","created":"Sat, 30 Nov 2024 03:52:08 GMT"}],"updateDate":"2024-12-03","timestamp":1732938728000,"abstract":"  In today's digital age, video content is prevalent, serving as a primary\nsource of information, education, and entertainment. However, the Deaf and Hard\nof Hearing (DHH) community often faces significant challenges in accessing\nvideo content due to the inadequacy of automatic speech recognition (ASR)\nsystems in providing accurate and reliable captions. This paper addresses the\nurgent need to improve video caption quality by leveraging Large Language\nModels (LLMs). We present a comprehensive study that explores the integration\nof LLMs to enhance the accuracy and context-awareness of captions generated by\nASR systems. Our methodology involves a novel pipeline that corrects\nASR-generated captions using advanced LLMs. It explicitly focuses on models\nlike GPT-3.5 and Llama2-13B due to their robust performance in language\ncomprehension and generation tasks. We introduce a dataset representative of\nreal-world challenges the DHH community faces to evaluate our proposed\npipeline. Our results indicate that LLM-enhanced captions significantly improve\naccuracy, as evidenced by a notably lower Word Error Rate (WER) achieved by\nChatGPT-3.5 (WER: 9.75%) compared to the original ASR captions (WER: 23.07%),\nChatGPT-3.5 shows an approximate 57.72% improvement in WER compared to the\noriginal ASR captions.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"GVKhnWGumLZbc9eTGaPiWEnMhIY3jilZqhHCzdtMTc4","pdfSize":"515086"}