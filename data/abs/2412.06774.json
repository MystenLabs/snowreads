{
  "id": "2412.06774",
  "title": "Visual Lexicon: Rich Image Features in Language Space",
  "authors": "XuDong Wang, Xingyi Zhou, Alireza Fathi, Trevor Darrell, Cordelia\n  Schmid",
  "authorsParsed": [
    [
      "Wang",
      "XuDong",
      ""
    ],
    [
      "Zhou",
      "Xingyi",
      ""
    ],
    [
      "Fathi",
      "Alireza",
      ""
    ],
    [
      "Darrell",
      "Trevor",
      ""
    ],
    [
      "Schmid",
      "Cordelia",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 18:57:24 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733770644000,
  "abstract": "  We present Visual Lexicon, a novel visual language that encodes rich image\ninformation into the text space of vocabulary tokens while retaining intricate\nvisual details that are often challenging to convey in natural language. Unlike\ntraditional methods that prioritize either high-level semantics (e.g., CLIP) or\npixel-level reconstruction (e.g., VAE), ViLex simultaneously captures rich\nsemantic content and fine visual details, enabling high-quality image\ngeneration and comprehensive visual scene understanding. Through a\nself-supervised learning pipeline, ViLex generates tokens optimized for\nreconstructing input images using a frozen text-to-image (T2I) diffusion model,\npreserving the detailed information necessary for high-fidelity semantic-level\nreconstruction. As an image embedding in the language space, ViLex tokens\nleverage the compositionality of natural languages, allowing them to be used\nindependently as \"text tokens\" or combined with natural language tokens to\nprompt pretrained T2I models with both visual and textual inputs, mirroring how\nwe interact with vision-language models (VLMs). Experiments demonstrate that\nViLex achieves higher fidelity in image reconstruction compared to text\nembeddings--even with a single ViLex token. Moreover, ViLex successfully\nperforms various DreamBooth tasks in a zero-shot, unsupervised manner without\nfine-tuning T2I models. Additionally, ViLex serves as a powerful vision\nencoder, consistently improving vision-language model performance across 15\nbenchmarks relative to a strong SigLIP baseline.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "qi9IwLE5qHNNO6Cubeg83do0hVdrTEy3tWwQqGAiTJ0",
  "pdfSize": "51045621"
}