{"id":"2407.17437","title":"Nerva: a Truly Sparse Implementation of Neural Networks","authors":"Wieger Wesselink, Bram Grooten, Qiao Xiao, Cassio de Campos, Mykola\n  Pechenizkiy","authorsParsed":[["Wesselink","Wieger",""],["Grooten","Bram",""],["Xiao","Qiao",""],["de Campos","Cassio",""],["Pechenizkiy","Mykola",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:13:31 GMT"}],"updateDate":"2024-07-25","timestamp":1721841211000,"abstract":"  We introduce Nerva, a fast neural network library under development in C++.\nIt supports sparsity by using the sparse matrix operations of Intel's Math\nKernel Library (MKL), which eliminates the need for binary masks. We show that\nNerva significantly decreases training time and memory usage while reaching\nequivalent accuracy to PyTorch. We run static sparse experiments with an MLP on\nCIFAR-10. On high sparsity levels like $99\\%$, the runtime is reduced by a\nfactor of $4\\times$ compared to a PyTorch model using masks. Similar to other\npopular frameworks such as PyTorch and Keras, Nerva offers a Python interface\nfor users to work with.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"05uV1FadyJQC4mFuIBFLWH072HVsEhvrLxDzqKaolzU","pdfSize":"616198"}