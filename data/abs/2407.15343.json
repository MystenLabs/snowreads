{"id":"2407.15343","title":"Improving Minimum Bayes Risk Decoding with Multi-Prompt","authors":"David Heineman, Yao Dou, Wei Xu","authorsParsed":[["Heineman","David",""],["Dou","Yao",""],["Xu","Wei",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 02:57:10 GMT"}],"updateDate":"2024-07-23","timestamp":1721617030000,"abstract":"  While instruction fine-tuned LLMs are effective text generators, sensitivity\nto prompt construction makes performance unstable and sub-optimal in practice.\nRelying on a single \"best\" prompt cannot capture all differing approaches to a\ngeneration problem. Using this observation, we propose multi-prompt decoding,\nwhere many candidate generations are decoded from a prompt bank at\ninference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR)\ndecoding, which selects a final output using a trained value metric. We show\nmulti-prompt improves MBR across a comprehensive set of conditional generation\ntasks, and show this is a result of estimating a more diverse and higher\nquality candidate space than that of a single prompt. Further experiments\nconfirm multi-prompt improves generation across tasks, models and metrics.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"pnwJta6X4SyaPwzAhVNwU3x_5H6FElfhFAAxOSPKD7o","pdfSize":"1249288","objectId":"0xdc560d9f6ba226591c248a3af6d9950a585f03c3fd2e57df8b899508cde73a12","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
