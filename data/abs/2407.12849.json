{"id":"2407.12849","title":"Large language models are good medical coders, if provided with tools","authors":"Keith Kwan","authorsParsed":[["Kwan","Keith",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 06:58:51 GMT"}],"updateDate":"2024-07-19","timestamp":1720249131000,"abstract":"  This study presents a novel two-stage Retrieve-Rank system for automated\nICD-10-CM medical coding, comparing its performance against a Vanilla Large\nLanguage Model (LLM) approach. Evaluating both systems on a dataset of 100\nsingle-term medical conditions, the Retrieve-Rank system achieved 100% accuracy\nin predicting correct ICD-10-CM codes, significantly outperforming the Vanilla\nLLM (GPT-3.5-turbo), which achieved only 6% accuracy. Our analysis demonstrates\nthe Retrieve-Rank system's superior precision in handling various medical terms\nacross different specialties. While these results are promising, we acknowledge\nthe limitations of using simplified inputs and the need for further testing on\nmore complex, realistic medical cases. This research contributes to the ongoing\neffort to improve the efficiency and accuracy of medical coding, highlighting\nthe importance of retrieval-based approaches.\n","subjects":["Computing Research Repository/Information Retrieval","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XE3xLh7ZtR4x2A-qPas3su_9TzrFrWmH1twemNpWAfg","pdfSize":"90940"}