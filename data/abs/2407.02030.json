{"id":"2407.02030","title":"Breaking Bias, Building Bridges: Evaluation and Mitigation of Social\n  Biases in LLMs via Contact Hypothesis","authors":"Chahat Raj, Anjishnu Mukherjee, Aylin Caliskan, Antonios\n  Anastasopoulos, Ziwei Zhu","authorsParsed":[["Raj","Chahat",""],["Mukherjee","Anjishnu",""],["Caliskan","Aylin",""],["Anastasopoulos","Antonios",""],["Zhu","Ziwei",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 07:58:46 GMT"}],"updateDate":"2024-07-03","timestamp":1719907126000,"abstract":"  Large Language Models (LLMs) perpetuate social biases, reflecting prejudices\nin their training data and reinforcing societal stereotypes and inequalities.\nOur work explores the potential of the Contact Hypothesis, a concept from\nsocial psychology for debiasing LLMs. We simulate various forms of social\ncontact through LLM prompting to measure their influence on the model's biases,\nmirroring how intergroup interactions can reduce prejudices in social contexts.\nWe create a dataset of 108,000 prompts following a principled approach\nreplicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and\nNousHermes) across 13 social bias dimensions. We propose a unique debiasing\ntechnique, Social Contact Debiasing (SCD), that instruction-tunes these models\nwith unbiased responses to prompts. Our research demonstrates that LLM\nresponses exhibit social biases when subject to contact probing, but more\nimportantly, these biases can be significantly reduced by up to 40% in 1 epoch\nof instruction tuning LLaMA 2 following our SCD strategy. Our code and data are\navailable at https://github.com/chahatraj/breakingbias.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"S71g5pcxsWVNPwmhUx5QjhDDvRlOdYBp5rh62nHwEDk","pdfSize":"546453"}