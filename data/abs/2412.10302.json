{
  "id": "2412.10302",
  "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
  "authors": "Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai\n  Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu,\n  Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu,\n  Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong\n  Wang, Chong Ruan",
  "authorsParsed": [
    [
      "Wu",
      "Zhiyu",
      ""
    ],
    [
      "Chen",
      "Xiaokang",
      ""
    ],
    [
      "Pan",
      "Zizheng",
      ""
    ],
    [
      "Liu",
      "Xingchao",
      ""
    ],
    [
      "Liu",
      "Wen",
      ""
    ],
    [
      "Dai",
      "Damai",
      ""
    ],
    [
      "Gao",
      "Huazuo",
      ""
    ],
    [
      "Ma",
      "Yiyang",
      ""
    ],
    [
      "Wu",
      "Chengyue",
      ""
    ],
    [
      "Wang",
      "Bingxuan",
      ""
    ],
    [
      "Xie",
      "Zhenda",
      ""
    ],
    [
      "Wu",
      "Yu",
      ""
    ],
    [
      "Hu",
      "Kai",
      ""
    ],
    [
      "Wang",
      "Jiawei",
      ""
    ],
    [
      "Sun",
      "Yaofeng",
      ""
    ],
    [
      "Li",
      "Yukun",
      ""
    ],
    [
      "Piao",
      "Yishi",
      ""
    ],
    [
      "Guan",
      "Kang",
      ""
    ],
    [
      "Liu",
      "Aixin",
      ""
    ],
    [
      "Xie",
      "Xin",
      ""
    ],
    [
      "You",
      "Yuxiang",
      ""
    ],
    [
      "Dong",
      "Kai",
      ""
    ],
    [
      "Yu",
      "Xingkai",
      ""
    ],
    [
      "Zhang",
      "Haowei",
      ""
    ],
    [
      "Zhao",
      "Liang",
      ""
    ],
    [
      "Wang",
      "Yisong",
      ""
    ],
    [
      "Ruan",
      "Chong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 17:37:48 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734111468000,
  "abstract": "  We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "yMJ-PLQu0czbk7R_OS2jAO20kWxsuj8nm6T87ea0Uww",
  "pdfSize": "6007241"
}