{"id":"2407.19342","title":"Parameter-Efficient Fine-Tuning via Circular Convolution","authors":"Aochuan Chen, Jiashun Cheng, Zijing Liu, Ziqi Gao, Fugee Tsung, Yu Li,\n  Jia Li","authorsParsed":[["Chen","Aochuan",""],["Cheng","Jiashun",""],["Liu","Zijing",""],["Gao","Ziqi",""],["Tsung","Fugee",""],["Li","Yu",""],["Li","Jia",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 21:12:46 GMT"},{"version":"v2","created":"Wed, 21 Aug 2024 05:44:11 GMT"}],"updateDate":"2024-08-22","timestamp":1722114766000,"abstract":"  Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large\nfoundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$\nto represent weight changes (i.e., $\\Delta \\mathbf{W} = \\mathbf{B}\n\\mathbf{A}$). This method reduces trainable parameters and mitigates heavy\nmemory consumption associated with full delta matrices by sequentially\nmultiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its\nsuccess, the intrinsic low-rank characteristic may limit its performance.\nAlthough several variants have been proposed to address this issue, they often\noverlook the crucial computational and memory efficiency brought by LoRA. In\nthis paper, we propose Circular Convolution Adaptation (C$^3$A), which not only\nachieves high-rank adaptation with enhanced performance but also excels in both\ncomputational power and memory utilization. Extensive experiments demonstrate\nthat C$^3$A consistently outperforms LoRA and its variants across various\nfine-tuning tasks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"txLRehY0CUCXsnvJAqL1xUIEp268It7JZgCjTiwx1Lg","pdfSize":"463206","objectId":"0xc2ff3a9d683b7ede95e217f35a7c8cb6361aba843f786194cc4a61c4807f980d","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
