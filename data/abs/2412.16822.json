{"id":"2412.16822","title":"Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for\n  Efficient Diffusion Transformers","authors":"Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei\n  Zhou, Lingzhi Zhang, Yotam Nitzan, Xiaoyang Liu, Zhe Lin, Eli Shechtman,\n  Sohrab Amirghodsi, Yingyan Celine Lin","authorsParsed":[["You","Haoran",""],["Barnes","Connelly",""],["Zhou","Yuqian",""],["Kang","Yan",""],["Du","Zhenbang",""],["Zhou","Wei",""],["Zhang","Lingzhi",""],["Nitzan","Yotam",""],["Liu","Xiaoyang",""],["Lin","Zhe",""],["Shechtman","Eli",""],["Amirghodsi","Sohrab",""],["Lin","Yingyan Celine",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 02:04:17 GMT"}],"updateDate":"2024-12-24","timestamp":1734833057000,"abstract":"  Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image\ngeneration quality but suffer from high latency and memory inefficiency, making\nthem difficult to deploy on resource-constrained devices. One key efficiency\nbottleneck is that existing DiTs apply equal computation across all regions of\nan image. However, not all image tokens are equally important, and certain\nlocalized areas require more computation, such as objects. To address this, we\npropose DiffRatio-MoD, a dynamic DiT inference framework with differentiable\ncompression ratios, which automatically learns to dynamically route computation\nacross layers and timesteps for each image token, resulting in\nMixture-of-Depths (MoD) efficient DiT models. Specifically, DiffRatio-MoD\nintegrates three features: (1) A token-level routing scheme where each DiT\nlayer includes a router that is jointly fine-tuned with model weights to\npredict token importance scores. In this way, unimportant tokens bypass the\nentire layer's computation; (2) A layer-wise differentiable ratio mechanism\nwhere different DiT layers automatically learn varying compression ratios from\na zero initialization, resulting in large compression ratios in redundant\nlayers while others remain less compressed or even uncompressed; (3) A\ntimestep-wise differentiable ratio mechanism where each denoising timestep\nlearns its own compression ratio. The resulting pattern shows higher ratios for\nnoisier timesteps and lower ratios as the image becomes clearer. Extensive\nexperiments on both text-to-image and inpainting tasks show that DiffRatio-MoD\neffectively captures dynamism across token, layer, and timestep axes, achieving\nsuperior trade-offs between generation quality and efficiency compared to prior\nworks.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"wMvrWAe3w7u8E1k30kHc15Rn1IJrtoqD-oJDUnKf0fM","pdfSize":"21783035"}