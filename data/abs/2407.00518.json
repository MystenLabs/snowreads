{"id":"2407.00518","title":"When Robots Get Chatty: Grounding Multimodal Human-Robot Conversation\n  and Collaboration","authors":"Philipp Allgeuer, Hassan Ali and Stefan Wermter","authorsParsed":[["Allgeuer","Philipp",""],["Ali","Hassan",""],["Wermter","Stefan",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 19:43:34 GMT"}],"updateDate":"2024-07-02","timestamp":1719690214000,"abstract":"  We investigate the use of Large Language Models (LLMs) to equip neural\nrobotic agents with human-like social and cognitive competencies, for the\npurpose of open-ended human-robot conversation and collaboration. We introduce\na modular and extensible methodology for grounding an LLM with the sensory\nperceptions and capabilities of a physical robot, and integrate multiple deep\nlearning models throughout the architecture in a form of system integration.\nThe integrated models encompass various functions such as speech recognition,\nspeech generation, open-vocabulary object detection, human pose estimation, and\ngesture detection, with the LLM serving as the central text-based coordinating\nunit. The qualitative and quantitative results demonstrate the huge potential\nof LLMs in providing emergent cognition and interactive language-oriented\ncontrol of robots in a natural and social manner.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"m2NE6u3VYvrvVQr2EMbV8U7pH3T0hvKEy07r5_PLpSs","pdfSize":"1585643"}