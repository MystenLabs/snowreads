{"id":"2407.17037","title":"Robust Comparative Statics with Misspecified Bayesian Learning","authors":"Aniruddha Ghosh","authorsParsed":[["Ghosh","Aniruddha",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 06:50:08 GMT"}],"updateDate":"2024-07-25","timestamp":1721803808000,"abstract":"  We present novel monotone comparative statics results for steady state\nbehavior in a dynamic optimization environment with misspecified Bayesian\nlearning. We consider a generalized framework, based on Esponda and Pouzo\n(2021), wherein a Bayesian learner facing a dynamic optimization problem has a\nprior on a set of parameterized transition probability functions (models) but\nis misspecified in the sense that the true process is not within this set. In\nthe steady state, the learner infers the model that best-fits the data\ngenerated by their actions, and in turn, their actions are optimally chosen\ngiven their inferred model. We characterize conditions on the primitives of the\nenvironment, and in particular, over the set of models under which the steady\nstate distribution over states and actions and inferred models exhibit\nmonotonic behavior. Further, we offer a new theorem on the existence of a\nsteady state on the basis of a monotonicity argument. Lastly, we provide an\nupper bound on the cost of misspecification, again in terms of the primitives\nof the environment. We demonstrate the utility of our results for several\nenvironments of general interest, including forecasting models, dynamic\neffort-task, and optimal consumption-savings problems.\n","subjects":["Economics/Theoretical Economics"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"sD_0w_Vl971Fax_E52ScCN7VyJlIIqhyUahXqmPMF28","pdfSize":"497933"}