{
  "id": "2412.19522",
  "title": "Exploiting Domain-Specific Parallel Data on Multilingual Language Models\n  for Low-resource Language Translation",
  "authors": "Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke\n  Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie\n  En-Shiun Lee",
  "authorsParsed": [
    [
      "Ranathungaa",
      "Surangika",
      ""
    ],
    [
      "Nayak",
      "Shravan",
      ""
    ],
    [
      "Huang",
      "Shih-Ting Cindy",
      ""
    ],
    [
      "Mao",
      "Yanke",
      ""
    ],
    [
      "Su",
      "Tong",
      ""
    ],
    [
      "Chan",
      "Yun-Hsiang Ray",
      ""
    ],
    [
      "Yuan",
      "Songchen",
      ""
    ],
    [
      "Rinaldi",
      "Anthony",
      ""
    ],
    [
      "Lee",
      "Annie En-Shiun",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 27 Dec 2024 08:25:52 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735287952000,
  "abstract": "  Neural Machine Translation (NMT) systems built on multilingual\nsequence-to-sequence Language Models (msLMs) fail to deliver expected results\nwhen the amount of parallel data for a language, as well as the language's\nrepresentation in the model are limited. This restricts the capabilities of\ndomain-specific NMT systems for low-resource languages (LRLs). As a solution,\nparallel data from auxiliary domains can be used either to fine-tune or to\nfurther pre-train the msLM. We present an evaluation of the effectiveness of\nthese two techniques in the context of domain-specific LRL-NMT. We also explore\nthe impact of domain divergence on NMT model performance. We recommend several\nstrategies for utilizing auxiliary parallel data in building domain-specific\nNMT models for LRLs.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "WwBy3ysUTUyUCaR1mhQ8JHitN0fS2LJGJte5hJrm07c",
  "pdfSize": "873364"
}