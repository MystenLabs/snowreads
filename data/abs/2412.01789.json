{"id":"2412.01789","title":"From ChebNet to ChebGibbsNet","authors":"Jie Zhang, Min-Te Sun","authorsParsed":[["Zhang","Jie",""],["Sun","Min-Te",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 18:37:45 GMT"}],"updateDate":"2024-12-03","timestamp":1733164665000,"abstract":"  Recent advancements in Spectral Graph Convolutional Networks (SpecGCNs) have\nled to state-of-the-art performance in various graph representation learning\ntasks. To exploit the potential of SpecGCNs, we analyze corresponding graph\nfilters via polynomial interpolation, the cornerstone of graph signal\nprocessing. Different polynomial bases, such as Bernstein, Chebyshev, and\nmonomial basis, have various convergence rates that will affect the error in\npolynomial interpolation. Although adopting Chebyshev basis for interpolation\ncan minimize maximum error, the performance of ChebNet is still weaker than\nGPR-GNN and BernNet. \\textbf{We point out it is caused by the Gibbs phenomenon,\nwhich occurs when the graph frequency response function approximates the target\nfunction.} It reduces the approximation ability of a truncated polynomial\ninterpolation. In order to mitigate the Gibbs phenomenon, we propose to add the\nGibbs damping factor with each term of Chebyshev polynomials on ChebNet. As a\nresult, our lightweight approach leads to a significant performance boost.\nAfterwards, we reorganize ChebNet via decoupling feature propagation and\ntransformation. We name this variant as \\textbf{ChebGibbsNet}. Our experiments\nindicate that ChebGibbsNet is superior to other advanced SpecGCNs, such as\nGPR-GNN and BernNet, in both homogeneous graphs and heterogeneous graphs.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"k6BeKZae3OWFa4IJQtu-A79kV_reKRVqaKKoEgP2E_0","pdfSize":"526654"}