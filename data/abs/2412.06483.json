{
  "id": "2412.06483",
  "title": "SafeWorld: Geo-Diverse Safety Alignment",
  "authors": "Da Yin, Haoyi Qiu, Kung-Hsiang Huang, Kai-Wei Chang, Nanyun Peng",
  "authorsParsed": [
    [
      "Yin",
      "Da",
      ""
    ],
    [
      "Qiu",
      "Haoyi",
      ""
    ],
    [
      "Huang",
      "Kung-Hsiang",
      ""
    ],
    [
      "Chang",
      "Kai-Wei",
      ""
    ],
    [
      "Peng",
      "Nanyun",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 13:31:46 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733751106000,
  "abstract": "  In the rapidly evolving field of Large Language Models (LLMs), ensuring\nsafety is a crucial and widely discussed topic. However, existing works often\noverlook the geo-diversity of cultural and legal standards across the world. To\ndemonstrate the challenges posed by geo-diverse safety standards, we introduce\nSafeWorld, a novel benchmark specifically designed to evaluate LLMs' ability to\ngenerate responses that are not only helpful but also culturally sensitive and\nlegally compliant across diverse global contexts. SafeWorld encompasses 2,342\ntest user queries, each grounded in high-quality, human-verified cultural norms\nand legal policies from 50 countries and 493 regions/races. On top of it, we\npropose a multi-dimensional automatic safety evaluation framework that assesses\nthe contextual appropriateness, accuracy, and comprehensiveness of responses.\nOur evaluations reveal that current LLMs struggle to meet these criteria. To\nenhance LLMs' alignment with geo-diverse safety standards, we synthesize\nhelpful preference pairs for Direct Preference Optimization (DPO) alignment\ntraining. The preference pair construction aims to encourage LLMs to behave\nappropriately and provide precise references to relevant cultural norms and\npolicies when necessary. Our trained SafeWorldLM outperforms all competing\nmodels, including GPT-4o on all three evaluation dimensions by a large margin.\nGlobal human evaluators also note a nearly 20% higher winning rate in\nhelpfulness and harmfulness evaluation. Our code and data can be found here:\nhttps://github.com/PlusLabNLP/SafeWorld.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "ab3XK1PMReTM8ng214NcGi6oVhYwuv131IUXX9Ep2eU",
  "pdfSize": "3526260"
}