{"id":"2412.16971","title":"Part-Of-Speech Sensitivity of Routers in Mixture of Experts Models","authors":"Elie Antoine, Fr\\'ed\\'eric B\\'echet, Philippe Langlais","authorsParsed":[["Antoine","Elie",""],["Béchet","Frédéric",""],["Langlais","Philippe",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 11:03:41 GMT"}],"updateDate":"2024-12-24","timestamp":1734865421000,"abstract":"  This study investigates the behavior of model-integrated routers in Mixture\nof Experts (MoE) models, focusing on how tokens are routed based on their\nlinguistic features, specifically Part-of-Speech (POS) tags. The goal is to\nexplore across different MoE architectures whether experts specialize in\nprocessing tokens with similar linguistic traits. By analyzing token\ntrajectories across experts and layers, we aim to uncover how MoE models handle\nlinguistic information. Findings from six popular MoE models reveal expert\nspecialization for specific POS categories, with routing paths showing high\npredictive accuracy for POS, highlighting the value of routing paths in\ncharacterizing tokens.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ifeto7b31OK5Nk55KChIgE_dcvRDbuekYXNu0WHgEPk","pdfSize":"1432087"}