{"id":"2407.03245","title":"TieBot: Learning to Knot a Tie from Visual Demonstration through a\n  Real-to-Sim-to-Real Approach","authors":"Weikun Peng, Jun Lv, Yuwei Zeng, Haonan Chen, Siheng Zhao, Jichen Sun,\n  Cewu Lu, Lin Shao","authorsParsed":[["Peng","Weikun",""],["Lv","Jun",""],["Zeng","Yuwei",""],["Chen","Haonan",""],["Zhao","Siheng",""],["Sun","Jichen",""],["Lu","Cewu",""],["Shao","Lin",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 16:16:41 GMT"},{"version":"v2","created":"Thu, 4 Jul 2024 02:03:54 GMT"}],"updateDate":"2024-07-08","timestamp":1720023401000,"abstract":"  The tie-knotting task is highly challenging due to the tie's high deformation\nand long-horizon manipulation actions. This work presents TieBot, a\nReal-to-Sim-to-Real learning from visual demonstration system for the robots to\nlearn to knot a tie. We introduce the Hierarchical Feature Matching approach to\nestimate a sequence of tie's meshes from the demonstration video. With these\nestimated meshes used as subgoals, we first learn a teacher policy using\nprivileged information. Then, we learn a student policy with point cloud\nobservation by imitating teacher policy. Lastly, our pipeline learns a residual\npolicy when the learned policy is applied to real-world execution, mitigating\nthe Sim2Real gap. We demonstrate the effectiveness of TieBot in simulation and\nthe real world. In the real-world experiment, a dual-arm robot successfully\nknots a tie, achieving 50% success rate among 10 trials. Videos can be found\nhttps://tiebots.github.io/.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ib5n7-XBq6mB1rm86UGEbewjPkc-6FB-7nLvdSQqn8o","pdfSize":"2152116"}