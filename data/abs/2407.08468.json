{"id":"2407.08468","title":"Matching-Based Policy Learning","authors":"Xuqiao Li, Ying Yan","authorsParsed":[["Li","Xuqiao",""],["Yan","Ying",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:02:10 GMT"}],"updateDate":"2024-07-12","timestamp":1720702930000,"abstract":"  Treatment heterogeneity is ubiquitous in many areas, motivating practitioners\nto search for the optimal policy that maximizes the expected outcome based on\nindividualized characteristics. However, most existing policy learning methods\nrely on weighting-based approaches, which may suffer from high instability in\nobservational studies. To enhance the robustness of the estimated policy, we\npropose a matching-based estimator of the policy improvement upon a randomized\nbaseline. After correcting the conditional bias, we learn the optimal policy by\nmaximizing the estimate over a policy class. We derive a non-asymptotic high\nprobability bound for the regret of the learned policy and show that the\nconvergence rate is almost $1/\\sqrt{n}$. The competitive finite sample\nperformance of the proposed method is demonstrated in extensive simulation\nstudies and a real data application.\n","subjects":["Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_lNLZskyOfmHnSjWrwYxYjb26CPu8OjEKi3Czo6CSoE","pdfSize":"1441629"}