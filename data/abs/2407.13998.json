{"id":"2407.13998","title":"RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval\n  Augmented Question Answering","authors":"Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu,\n  William Yang Wang, Bonan Min, Vittorio Castelli","authorsParsed":[["Han","Rujun",""],["Zhang","Yuhao",""],["Qi","Peng",""],["Xu","Yumo",""],["Wang","Jenyuan",""],["Liu","Lan",""],["Wang","William Yang",""],["Min","Bonan",""],["Castelli","Vittorio",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 03:02:51 GMT"}],"updateDate":"2024-07-22","timestamp":1721358171000,"abstract":"  Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"LoouKJMzvKJEObvOn_rqjBoz5-8it1pXWFMFAhrJ3BE","pdfSize":"5680899","objectId":"0xc4e7e6e6244c1df6f8819147e6753b7163555e0fd3a68cb4aedbee8891b7b94b","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
