{"id":"2412.01471","title":"Multi-Granularity Video Object Segmentation","authors":"Sangbeom Lim, Seongchan Kim, Seungjun An, Seokju Cho, Paul Hongsuck\n  Seo, Seungryong Kim","authorsParsed":[["Lim","Sangbeom",""],["Kim","Seongchan",""],["An","Seungjun",""],["Cho","Seokju",""],["Seo","Paul Hongsuck",""],["Kim","Seungryong",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 13:17:41 GMT"},{"version":"v2","created":"Tue, 3 Dec 2024 05:42:22 GMT"}],"updateDate":"2024-12-04","timestamp":1733145461000,"abstract":"  Current benchmarks for video segmentation are limited to annotating only\nsalient objects (i.e., foreground instances). Despite their impressive\narchitectural designs, previous works trained on these benchmarks have\nstruggled to adapt to real-world scenarios. Thus, developing a new video\nsegmentation dataset aimed at tracking multi-granularity segmentation target in\nthe video scene is necessary. In this work, we aim to generate\nmulti-granularity video segmentation dataset that is annotated for both salient\nand non-salient masks. To achieve this, we propose a large-scale, densely\nannotated multi-granularity video object segmentation (MUG-VOS) dataset that\nincludes various types and granularities of mask annotations. We automatically\ncollected a training set that assists in tracking both salient and non-salient\nobjects, and we also curated a human-annotated test set for reliable\nevaluation. In addition, we present memory-based mask propagation model (MMPM),\ntrained and evaluated on MUG-VOS dataset, which leads to the best performance\namong the existing video object segmentation methods and Segment SAM-based\nvideo segmentation methods. Project page is available at\nhttps://cvlab-kaist.github.io/MUG-VOS.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"lVBwmU2fjY1NXJjFjGGT00vYeQ189E-MmYRFmS-f7wQ","pdfSize":"32145347"}