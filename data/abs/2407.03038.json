{"id":"2407.03038","title":"On the Client Preference of LLM Fine-tuning in Federated Learning","authors":"Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Jing Gao","authorsParsed":[["Wu","Feijie",""],["Liu","Xiaoze",""],["Wang","Haoyu",""],["Wang","Xingchen",""],["Gao","Jing",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 12:02:24 GMT"}],"updateDate":"2024-07-04","timestamp":1720008144000,"abstract":"  Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained\nlarge language model (LLM) using preference datasets, enabling the LLM to\ngenerate outputs that align with human preferences. Given the sensitive nature\nof these preference datasets held by various clients, there is a need to\nimplement RLHF within a federated learning (FL) framework, where clients are\nreluctant to share their data due to privacy concerns. To address this, we\nintroduce a feasible framework in which clients collaboratively train a binary\nselector with their preference datasets using our proposed FedBis. With a\nwell-trained selector, we can further enhance the LLM that generates\nhuman-preferred completions. Meanwhile, we propose a novel algorithm,\nFedBiscuit, that trains multiple selectors by organizing clients into balanced\nand disjoint clusters based on their preferences. Compared to the FedBis,\nFedBiscuit demonstrates superior performance in simulating human preferences\nfor pairwise completions. Our extensive experiments on federated human\npreference datasets -- marking the first benchmark to address heterogeneous\ndata partitioning among clients -- demonstrate that FedBiscuit outperforms\nFedBis and even surpasses traditional centralized training.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DJdEJpCF8fo2Tv-BNpzn2suFBYbHG6Vu03qZdzAIlRs","pdfSize":"503912"}