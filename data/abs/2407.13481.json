{"id":"2407.13481","title":"Attention Overflow: Language Model Input Blur during Long-Context\n  Missing Items Recommendation","authors":"Damien Sileo","authorsParsed":[["Sileo","Damien",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 13:00:30 GMT"}],"updateDate":"2024-07-19","timestamp":1721307630000,"abstract":"  Large language models (LLMs) can suggest missing elements from items listed\nin a prompt, which can be used for list completion or recommendations based on\nusers' history. However, their performance degrades when presented with too\nmany items, as they start to suggest items already included in the input list.\nThis occurs at around 100 items for mid-2024 flagship LLMs. We evaluate this\nphenomenon on both synthetic problems (e.g., finding missing numbers in a given\nrange of shuffled integers) and realistic movie recommendation scenarios. We\nrefer to this issue as \\textit{attention overflow}, as preventing repetition\nrequires attending to all items simultaneously. Although iterative loops can\nmitigate this problem, their costs increase with the repetition rate, affecting\nthe language models' ability to derive novelty from lengthy inputs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ow1upgsENc4-OnYcBQOCJo498rNoqTG5P8q-Po1dLK8","pdfSize":"249585","objectId":"0x133477a191fa9c7470d6fcc03107b1076669e0cafc766e50897231c0bdef9c04","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
