{"id":"2407.04014","title":"Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM\n  Inference on Heterogeneous Systems","authors":"Grant Wilkins and Srinivasan Keshav and Richard Mortier","authorsParsed":[["Wilkins","Grant",""],["Keshav","Srinivasan",""],["Mortier","Richard",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 15:45:15 GMT"}],"updateDate":"2024-07-08","timestamp":1720107915000,"abstract":"  The rapid adoption of large language models (LLMs) has led to significant\nadvances in natural language processing and text generation. However, the\nenergy consumed through LLM model inference remains a major challenge for\nsustainable AI deployment. To address this problem, we model the\nworkload-dependent energy consumption and runtime of LLM inference tasks on\nheterogeneous GPU-CPU systems. By conducting an extensive characterization\nstudy of several state-of-the-art LLMs and analyzing their energy and runtime\nbehavior across different magnitudes of input prompts and output text, we\ndevelop accurate (R^2>0.96) energy and runtime models for each LLM. We employ\nthese models to explore an offline, energy-optimal LLM workload scheduling\nframework. Through a case study, we demonstrate the advantages of energy and\naccuracy aware scheduling compared to existing best practices.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AlYrHXgkfGjJJj23F1n9romiEDxPCvK3wlveahn6cDU","pdfSize":"1910667"}
