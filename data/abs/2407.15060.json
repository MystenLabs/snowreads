{"id":"2407.15060","title":"MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music\n  Generation","authors":"Yun-Han Lan, Wen-Yi Hsiao, Hao-Chung Cheng and Yi-Hsuan Yang","authorsParsed":[["Lan","Yun-Han",""],["Hsiao","Wen-Yi",""],["Cheng","Hao-Chung",""],["Yang","Yi-Hsuan",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 05:27:53 GMT"}],"updateDate":"2024-07-23","timestamp":1721539673000,"abstract":"  Existing text-to-music models can produce high-quality audio with great\ndiversity. However, textual prompts alone cannot precisely control temporal\nmusical features such as chords and rhythm of the generated music. To address\nthis challenge, we introduce MusiConGen, a temporally-conditioned\nTransformer-based text-to-music model that builds upon the pretrained MusicGen\nframework. Our innovation lies in an efficient finetuning mechanism, tailored\nfor consumer-grade GPUs, that integrates automatically-extracted rhythm and\nchords as the condition signal. During inference, the condition can either be\nmusical features extracted from a reference audio signal, or be user-defined\nsymbolic chord sequence, BPM, and textual prompts. Our performance evaluation\non two datasets -- one derived from extracted features and the other from\nuser-created inputs -- demonstrates that MusiConGen can generate realistic\nbacking track music that aligns well with the specified conditions. We\nopen-source the code and model checkpoints, and provide audio examples online,\nhttps://musicongen.github.io/musicongen_demo/.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ApwTskSoEym4hp02dyeFEwPVZ1Ynhaat_riEN32oLCk","pdfSize":"2251023"}