{"id":"2412.10995","title":"RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone","authors":"Mustafa Munir, Md Mostafijur Rahman, Radu Marculescu","authorsParsed":[["Munir","Mustafa",""],["Rahman","Md Mostafijur",""],["Marculescu","Radu",""]],"versions":[{"version":"v1","created":"Sat, 14 Dec 2024 23:39:03 GMT"}],"updateDate":"2024-12-17","timestamp":1734219543000,"abstract":"  Vision transformers (ViTs) have dominated computer vision in recent years.\nHowever, ViTs are computationally expensive and not well suited for mobile\ndevices; this led to the prevalence of convolutional neural network (CNN) and\nViT-based hybrid models for mobile vision applications. Recently, Vision GNN\n(ViG) and CNN hybrid models have also been proposed for mobile vision tasks.\nHowever, all of these methods remain slower compared to pure CNN-based models.\nIn this work, we propose Multi-Level Dilated Convolutions to devise a purely\nCNN-based mobile backbone. Using Multi-Level Dilated Convolutions allows for a\nlarger theoretical receptive field than standard convolutions. Different levels\nof dilation also allow for interactions between the short-range and long-range\nfeatures in an image. Experiments show that our proposed model outperforms\nstate-of-the-art (SOTA) mobile CNN, ViT, ViG, and hybrid architectures in terms\nof accuracy and/or speed on image classification, object detection, instance\nsegmentation, and semantic segmentation. Our fastest model, RapidNet-Ti,\nachieves 76.3\\% top-1 accuracy on ImageNet-1K with 0.9 ms inference latency on\nan iPhone 13 mini NPU, which is faster and more accurate than MobileNetV2x1.4\n(74.7\\% top-1 with 1.0 ms latency). Our work shows that pure CNN architectures\ncan beat SOTA hybrid and ViT models in terms of accuracy and speed when\ndesigned properly.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"P70eE56d1OXe7cjjO3zyNYoyHpqkKrQImAQbBaLw8Yk","pdfSize":"2099799"}