{"id":"2407.00100","title":"Enhancing In-Context Learning via Implicit Demonstration Augmentation","authors":"Xiaoling Zhou, Wei Ye, Yidong Wang, Chaoya Jiang, Zhemg Lee, Rui Xie,\n  Shikun Zhang","authorsParsed":[["Zhou","Xiaoling",""],["Ye","Wei",""],["Wang","Yidong",""],["Jiang","Chaoya",""],["Lee","Zhemg",""],["Xie","Rui",""],["Zhang","Shikun",""]],"versions":[{"version":"v1","created":"Thu, 27 Jun 2024 05:25:46 GMT"}],"updateDate":"2024-07-02","timestamp":1719465946000,"abstract":"  The emergence of in-context learning (ICL) enables large pre-trained language\nmodels (PLMs) to make predictions for unseen inputs without updating\nparameters. Despite its potential, ICL's effectiveness heavily relies on the\nquality, quantity, and permutation of demonstrations, commonly leading to\nsuboptimal and unstable performance. In this paper, we tackle this challenge\nfor the first time from the perspective of demonstration augmentation.\nSpecifically, we start with enriching representations of demonstrations by\nleveraging their deep feature distribution. We then theoretically reveal that\nwhen the number of augmented copies approaches infinity, the augmentation is\napproximately equal to a novel logit calibration mechanism integrated with\nspecific statistical properties. This insight results in a simple yet highly\nefficient method that significantly improves the average and worst-case\naccuracy across diverse PLMs and tasks. Moreover, our method effectively\nreduces performance variance among varying demonstrations, permutations, and\ntemplates, and displays the capability to address imbalanced class\ndistributions.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8i0LqcNkX4zFck5Mx6quxcwo7itrPY0-BxGnfGELnMM","pdfSize":"4401048","objectId":"0x883e86fbdda67eaca63cf82bc4d1a38b7ffabc5f5c2771c87e7bca2ce51e96ff","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
