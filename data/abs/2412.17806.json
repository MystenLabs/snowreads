{
  "id": "2412.17806",
  "title": "Reconstructing People, Places, and Cameras",
  "authors": "Lea M\\\"uller, Hongsuk Choi, Anthony Zhang, Brent Yi, Jitendra Malik,\n  Angjoo Kanazawa",
  "authorsParsed": [
    [
      "MÃ¼ller",
      "Lea",
      ""
    ],
    [
      "Choi",
      "Hongsuk",
      ""
    ],
    [
      "Zhang",
      "Anthony",
      ""
    ],
    [
      "Yi",
      "Brent",
      ""
    ],
    [
      "Malik",
      "Jitendra",
      ""
    ],
    [
      "Kanazawa",
      "Angjoo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 18:58:34 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734980314000,
  "abstract": "  We present \"Humans and Structure from Motion\" (HSfM), a method for jointly\nreconstructing multiple human meshes, scene point clouds, and camera parameters\nin a metric world coordinate system from a sparse set of uncalibrated\nmulti-view images featuring people. Our approach combines data-driven scene\nreconstruction with the traditional Structure-from-Motion (SfM) framework to\nachieve more accurate scene reconstruction and camera estimation, while\nsimultaneously recovering human meshes. In contrast to existing scene\nreconstruction and SfM methods that lack metric scale information, our method\nestimates approximate metric scale by leveraging a human statistical model.\nFurthermore, it reconstructs multiple human meshes within the same world\ncoordinate system alongside the scene point cloud, effectively capturing\nspatial relationships among individuals and their positions in the environment.\nWe initialize the reconstruction of humans, scenes, and cameras using robust\nfoundational models and jointly optimize these elements. This joint\noptimization synergistically improves the accuracy of each component. We\ncompare our method to existing approaches on two challenging benchmarks,\nEgoHumans and EgoExo4D, demonstrating significant improvements in human\nlocalization accuracy within the world coordinate frame (reducing error from\n3.51m to 1.04m in EgoHumans and from 2.9m to 0.56m in EgoExo4D). Notably, our\nresults show that incorporating human data into the SfM pipeline improves\ncamera pose estimation (e.g., increasing RRA@15 by 20.3% on EgoHumans).\nAdditionally, qualitative results show that our approach improves overall scene\nreconstruction quality. Our code is available at: muelea.github.io/hsfm.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "wJ4RPscabLg1VU2NQK4qTaQ_9MA4vX8P_lmUe_XOdrU",
  "pdfSize": "27201661"
}