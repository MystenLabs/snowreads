{
  "id": "2412.10151",
  "title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval\n  Augmented Generation",
  "authors": "Hyeonseok Lim, Dongjae Shin, Seohyun Song, Inho Won, Minjun Kim,\n  Junghun Yuk, Haneol Jang, KyungTae Lim",
  "authorsParsed": [
    [
      "Lim",
      "Hyeonseok",
      ""
    ],
    [
      "Shin",
      "Dongjae",
      ""
    ],
    [
      "Song",
      "Seohyun",
      ""
    ],
    [
      "Won",
      "Inho",
      ""
    ],
    [
      "Kim",
      "Minjun",
      ""
    ],
    [
      "Yuk",
      "Junghun",
      ""
    ],
    [
      "Jang",
      "Haneol",
      ""
    ],
    [
      "Lim",
      "KyungTae",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 14:11:26 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734099086000,
  "abstract": "  We propose the VLR-Bench, a visual question answering (VQA) benchmark for\nevaluating vision language models (VLMs) based on retrieval augmented\ngeneration (RAG). Unlike existing evaluation datasets for external\nknowledge-based VQA, the proposed VLR-Bench includes five input passages. This\nallows testing of the ability to determine which passage is useful for\nanswering a given query, a capability lacking in previous research. In this\ncontext, we constructed a dataset of 32,000 automatically generated\ninstruction-following examples, which we denote as VLR-IF. This dataset is\nspecifically designed to enhance the RAG capabilities of VLMs by enabling them\nto learn how to generate appropriate answers based on input passages. We\nevaluated the validity of the proposed benchmark and training data and verified\nits performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3\nmodel. The proposed VLR-Bench and VLR-IF datasets are publicly available\nonline.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "ytvI5A23_Mz63r8x28XcOvscVDVvs6zK4WkNyZQ6Xbw",
  "pdfSize": "5862324"
}