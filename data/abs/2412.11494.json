{
  "id": "2412.11494",
  "title": "FTP: A Fine-grained Token-wise Pruner for Large Language Models via\n  Token Routing",
  "authors": "Zekai Li, Jintu Zheng, Ji Liu, Han Liu, Haowei Zhu, Zeping Li, Fuwei\n  Yang, Haiduo Huang, Jinzhang Peng, Dong Li, Lu Tian, Emad Barsoum",
  "authorsParsed": [
    [
      "Li",
      "Zekai",
      ""
    ],
    [
      "Zheng",
      "Jintu",
      ""
    ],
    [
      "Liu",
      "Ji",
      ""
    ],
    [
      "Liu",
      "Han",
      ""
    ],
    [
      "Zhu",
      "Haowei",
      ""
    ],
    [
      "Li",
      "Zeping",
      ""
    ],
    [
      "Yang",
      "Fuwei",
      ""
    ],
    [
      "Huang",
      "Haiduo",
      ""
    ],
    [
      "Peng",
      "Jinzhang",
      ""
    ],
    [
      "Li",
      "Dong",
      ""
    ],
    [
      "Tian",
      "Lu",
      ""
    ],
    [
      "Barsoum",
      "Emad",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 07:09:46 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734332986000,
  "abstract": "  Recently, large language models (LLMs) have demonstrated superior performance\nacross various tasks by adhering to scaling laws, which significantly increase\nmodel size. However, the huge computation overhead during inference hinders the\ndeployment in industrial applications. Many works leverage traditional\ncompression approaches to boost model inference, but these always introduce\nadditional training costs to restore the performance and the pruning results\ntypically show noticeable performance drops compared to the original model when\naiming for a specific level of acceleration. To address these issues, we\npropose a fine-grained token-wise pruning approach for the LLMs, which presents\na learnable router to adaptively identify the less important tokens and skip\nthem across model blocks to reduce computational cost during inference. To\nconstruct the router efficiently, we present a search-based sparsity scheduler\nfor pruning sparsity allocation, a trainable router combined with our proposed\nfour low-dimensional factors as input and three proposed losses. We conduct\nextensive experiments across different benchmarks on different LLMs to\ndemonstrate the superiority of our method. Our approach achieves\nstate-of-the-art (SOTA) pruning results, surpassing other existing pruning\nmethods. For instance, our method outperforms BlockPruner and ShortGPT by\napproximately 10 points on both LLaMA2-7B and Qwen1.5-7B in accuracy retention\nat comparable token sparsity levels.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "AUjI-Y-XFOw7XkUMgM2_ot7pXMY0JgpIOMBvs3gWojc",
  "pdfSize": "617792"
}