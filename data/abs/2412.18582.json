{
  "id": "2412.18582",
  "title": "Exploring Embedding Priors in Prompt-Tuning for Improved\n  Interpretability and Control",
  "authors": "Sergey Sedov, Sumanth Bharadwaj Hachalli Karanam, Venu Gopal Kadamba",
  "authorsParsed": [
    [
      "Sedov",
      "Sergey",
      ""
    ],
    [
      "Karanam",
      "Sumanth Bharadwaj Hachalli",
      ""
    ],
    [
      "Kadamba",
      "Venu Gopal",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 24 Dec 2024 18:18:52 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1735064332000,
  "abstract": "  Prompt-Tuning is an efficient method for adapting pre-trained language models\nto new tasks with minimal computational overhead by modifying prompt\nembeddings. In this work, we investigate how crucial the phenomenon of\nembedding collapse, frequently observed in Prompt-Tuning, is for the final\nperformance of the model. To address this question, we designed embedding\npriors and compared them with posteriors of the converged Soft and Deep\nPrompt-Tuning methods. Our findings suggest that priors strongly affect the\nposition of the tuned embeddings, and models can effectively work with\nembeddings from different parts of activation spaces, including completely new\nregions. As the final Prompt-Tuning capabilities are limited, we hypothesize\nthat controllable Prompt-Tuning posteriors may serve as a good starting point\nfor tasks such as chain-of-thought (COT) distillation. Our experiments also\nshow that generated trajectories are not localized in the activation space of\nthe models. However, there are distinct clusters of activations for distant\ntasks (e.g., NLP and arithmetic), while activations between NLP tasks (e.g.,\nQuestion-Answering and MLM) lie in the same cluster. These observations raise\nquestions about the importance of a single activation cluster for the\ngeneralization abilities of large language models.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "NWz23lfikIE2IDvLGvxlJd5r-jb1Y56mJPh0B6aR2HE",
  "pdfSize": "2374846"
}