{"id":"2407.02397","title":"Learning to Refine with Fine-Grained Natural Language Feedback","authors":"Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, Greg Durrett","authorsParsed":[["Wadhwa","Manya",""],["Zhao","Xinyu",""],["Li","Junyi Jessy",""],["Durrett","Greg",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 16:15:01 GMT"}],"updateDate":"2024-07-03","timestamp":1719936901000,"abstract":"  Recent work has explored the capability of large language models (LLMs) to\nidentify and correct errors in LLM-generated responses. These refinement\napproaches frequently evaluate what sizes of models are able to do refinement\nfor what problems, but less attention is paid to what effective feedback for\nrefinement looks like. In this work, we propose looking at refinement with\nfeedback as a composition of three distinct LLM competencies: (1)\nidentification of bad generations; (2) fine-grained natural language feedback\ngeneration; (3) refining with fine-grained feedback. The first step can be\nimplemented with a high-performing discriminative model and steps 2 and 3 can\nbe implemented either via prompted or fine-tuned LLMs. A key property of this\napproach is that the step 2 critique model can give fine-grained feedback about\nerrors, made possible by offloading the discrimination to a separate model in\nstep 1. We show that models of different capabilities benefit from refining\nwith this approach on the task of improving factual consistency of document\ngrounded summaries. Overall, our proposed method consistently outperforms\nexisting end-to-end refinement approaches and current trained models not\nfine-tuned for factuality critiquing.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JC7BZNy3_OAspLx7WylSnsqRAF9jtw8rpuPy65WWucs","pdfSize":"989050"}