{"id":"2412.19200","title":"Personalized Dynamic Music Emotion Recognition with Dual-Scale\n  Attention-Based Meta-Learning","authors":"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen","authorsParsed":[["Zhang","Dengming",""],["You","Weitao",""],["Liu","Ziheng",""],["Sun","Lingyun",""],["Chen","Pei",""]],"versions":[{"version":"v1","created":"Thu, 26 Dec 2024 12:47:35 GMT"}],"updateDate":"2024-12-30","timestamp":1735217255000,"abstract":"  Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of\ndifferent moments in music, playing a crucial role in music information\nretrieval. The existing DMER methods struggle to capture long-term dependencies\nwhen dealing with sequence data, which limits their performance. Furthermore,\nthese methods often overlook the influence of individual differences on emotion\nperception, even though everyone has their own personalized emotional\nperception in the real world. Motivated by these issues, we explore more\neffective sequence processing methods and introduce the Personalized DMER\n(PDMER) problem, which requires models to predict emotions that align with\npersonalized perception. Specifically, we propose a Dual-Scale Attention-Based\nMeta-Learning (DSAML) method. This method fuses features from a dual-scale\nfeature extractor and captures both short and long-term dependencies using a\ndual-scale attention transformer, improving the performance in traditional\nDMER. To achieve PDMER, we design a novel task construction strategy that\ndivides tasks by annotators. Samples in a task are annotated by the same\nannotator, ensuring consistent perception. Leveraging this strategy alongside\nmeta-learning, DSAML can predict personalized perception of emotions with just\none personalized annotation sample. Our objective and subjective experiments\ndemonstrate that our method can achieve state-of-the-art performance in both\ntraditional DMER and PDMER.\n","subjects":["Computer Science/Sound","Computer Science/Information Retrieval","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qzlU9TlUGB5keeDI75qzSepdFNBjPcgdNjCGdsXmC4o","pdfSize":"2670062"}