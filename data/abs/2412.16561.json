{
  "id": "2412.16561",
  "title": "A learning-based approach to stochastic optimal control under\n  reach-avoid constraint",
  "authors": "Tingting Ni, Maryam Kamgarpour",
  "authorsParsed": [
    [
      "Ni",
      "Tingting",
      ""
    ],
    [
      "Kamgarpour",
      "Maryam",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 10:07:40 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734775660000,
  "abstract": "  We develop a model-free approach to optimally control stochastic, Markovian\nsystems subject to a reach-avoid constraint. Specifically, the state trajectory\nmust remain within a safe set while reaching a target set within a finite time\nhorizon. Due to the time-dependent nature of these constraints, we show that,\nin general, the optimal policy for this constrained stochastic control problem\nis non-Markovian, which increases the computational complexity. To address this\nchallenge, we apply the state-augmentation technique from arXiv:2402.19360,\nreformulating the problem as a constrained Markov decision process (CMDP) on an\nextended state space. This transformation allows us to search for a Markovian\npolicy, avoiding the complexity of non-Markovian policies. To learn the optimal\npolicy without a system model, and using only trajectory data, we develop a\nlog-barrier policy gradient approach. We prove that under suitable assumptions,\nthe policy parameters converge to the optimal parameters, while ensuring that\nthe system trajectories satisfy the stochastic reach-avoid constraint with high\nprobability.\n",
  "subjects": [
    "Mathematics/Optimization and Control",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "pyaZKbr1qzuBEFh6oZDJpfBgit-arDWkFENgdvOlFW4",
  "pdfSize": "325117"
}