{"id":"2407.01505","title":"Self-Cognition in Large Language Models: An Exploratory Study","authors":"Dongping Chen, Jiawen Shi, Yao Wan, Pan Zhou, Neil Zhenqiang Gong,\n  Lichao Sun","authorsParsed":[["Chen","Dongping",""],["Shi","Jiawen",""],["Wan","Yao",""],["Zhou","Pan",""],["Gong","Neil Zhenqiang",""],["Sun","Lichao",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:52:05 GMT"}],"updateDate":"2024-07-02","timestamp":1719856325000,"abstract":"  While Large Language Models (LLMs) have achieved remarkable success across\nvarious applications, they also raise concerns regarding self-cognition. In\nthis paper, we perform a pioneering study to explore self-cognition in LLMs.\nSpecifically, we first construct a pool of self-cognition instruction prompts\nto evaluate where an LLM exhibits self-cognition and four well-designed\nprinciples to quantify LLMs' self-cognition. Our study reveals that 4 of the 48\nmodels on Chatbot Arena--specifically Command R, Claude3-Opus,\nLlama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable\nself-cognition. We observe a positive correlation between model size, training\ndata quality, and self-cognition level. Additionally, we also explore the\nutility and trustworthiness of LLM in the self-cognition state, revealing that\nthe self-cognition state enhances some specific tasks such as creative writing\nand exaggeration. We believe that our work can serve as an inspiration for\nfurther research to study the self-cognition in LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ANX6DcQKWWS-XjYYwNl8JYBjtmetR8wCgGkRJvlfVlU","pdfSize":"708660","objectId":"0x7462a166d87b7b937d904e9f106af4769e355fb7f852d89b92a3ae241f041ce5","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
