{"id":"2412.17846","title":"Enhancing Knowledge Distillation for LLMs with Response-Priming\n  Prompting","authors":"Vijay Goyal, Mustafa Khan, Aprameya Tirupati, Harveer Saini, Michael\n  Lam, Kevin Zhu","authorsParsed":[["Goyal","Vijay",""],["Khan","Mustafa",""],["Tirupati","Aprameya",""],["Saini","Harveer",""],["Lam","Michael",""],["Zhu","Kevin",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 20:41:44 GMT"}],"updateDate":"2024-12-25","timestamp":1734554504000,"abstract":"  Large language models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks. However, these models\nare often difficult to deploy due to significant computational requirements and\nresource constraints. Knowledge distillation (KD) is an effective technique for\ntransferring the performance of larger LLMs to smaller models. Traditional KD\nmethods primarily focus on the direct output of the teacher model, with little\nemphasis on the role of prompting during knowledge transfer. In this paper, we\npropose a set of novel response-priming prompting strategies applied in the\nknowledge distillation pipeline to enhance the performance of student models.\nOur approach fine-tunes a smaller Llama 3.1 8B Instruct model by distilling\nknowledge from a quantized Llama 3.1 405B Instruct teacher model. We apply LoRA\noptimization and evaluate on the GSM8K benchmark. Experimental results\ndemonstrate that integrating reasoning-eliciting prompting into the proposed KD\npipeline significantly improves student model performance, offering an\nefficient way to deploy powerful models in resource-constrained environments.\nWe find that Ground Truth prompting results in a 55\\% performance increase on\nGSM8K for a distilled Llama 3.1 8B Instruct compared to the same model\ndistilled without prompting. A thorough investigation into the self-attention\nlayers of the student models indicates that the more successful prompted models\ntend to exhibit certain positive behaviors inside their attention heads which\ncan be tied to their increased accuracy. Our implementation can be found at\nhttps://github.com/alonso130r/knowledge-distillation.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yQWQMwKUMwi7sFEGAXb1JgBydT0H4KtYMscDyBH8z28","pdfSize":"635965"}