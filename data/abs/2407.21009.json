{"id":"2407.21009","title":"AI-Assisted Generation of Difficult Math Questions","authors":"Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Nan Rosemary Ke,\n  Michael Mozer, Yoshua Bengio, Sanjeev Arora, Anirudh Goyal","authorsParsed":[["Shah","Vedant",""],["Yu","Dingli",""],["Lyu","Kaifeng",""],["Park","Simon",""],["Ke","Nan Rosemary",""],["Mozer","Michael",""],["Bengio","Yoshua",""],["Arora","Sanjeev",""],["Goyal","Anirudh",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 17:55:36 GMT"},{"version":"v2","created":"Mon, 2 Sep 2024 18:01:44 GMT"}],"updateDate":"2024-09-04","timestamp":1722362136000,"abstract":"  Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"enruh08yrv_yLx6tHBqrjbnvhe8wT0WIZ3Uxk1i7j4s","pdfSize":"780487"}