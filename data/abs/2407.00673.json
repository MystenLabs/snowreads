{"id":"2407.00673","title":"TEAL: New Selection Strategy for Small Buffers in Experience Replay\n  Class Incremental Learning","authors":"Shahar Shaul-Ariel, Daphna Weinshall","authorsParsed":[["Shaul-Ariel","Shahar",""],["Weinshall","Daphna",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 12:09:08 GMT"}],"updateDate":"2024-07-02","timestamp":1719749348000,"abstract":"  Continual Learning is an unresolved challenge, whose relevance increases when\nconsidering modern applications. Unlike the human brain, trained deep neural\nnetworks suffer from a phenomenon called Catastrophic Forgetting, where they\nprogressively lose previously acquired knowledge upon learning new tasks. To\nmitigate this problem, numerous methods have been developed, many relying on\nreplaying past exemplars during new task training. However, as the memory\nallocated for replay decreases, the effectiveness of these approaches\ndiminishes. On the other hand, maintaining a large memory for the purpose of\nreplay is inefficient and often impractical. Here we introduce TEAL, a novel\napproach to populate the memory with exemplars, that can be integrated with\nvarious experience-replay methods and significantly enhance their performance\non small memory buffers. We show that TEAL improves the average accuracy of the\nSOTA method XDER as well as ER and ER-ACE on several image recognition\nbenchmarks, with a small memory buffer of 1-3 exemplars per class in the final\ntask. This confirms the hypothesis that when memory is scarce, it is best to\nprioritize the most typical data.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"EQIPzgK198DUyGGyqVZ3UWwpGvAyXgqE-XoZadHvufU","pdfSize":"1436751","objectId":"0xcd3050bc63e0d872b4ad27f817c2c6ebae93d3305debacb8f3a8cd8e434016b9","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
