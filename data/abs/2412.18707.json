{"id":"2412.18707","title":"Multiple References with Meaningful Variations Improve Literary Machine\n  Translation","authors":"Si Wu, John Wieting, David A. Smith","authorsParsed":[["Wu","Si",""],["Wieting","John",""],["Smith","David A.",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 23:49:12 GMT"},{"version":"v2","created":"Tue, 25 Feb 2025 19:31:31 GMT"}],"updateDate":"2025-02-27","timestamp":1735084152000,"abstract":"  While a source sentence can be translated in many ways, most machine\ntranslation (MT) models are trained with only a single reference. Previous work\nhas shown that using synthetic paraphrases can improve MT. This paper\ninvestigates best practices for employing multiple references by analyzing the\nsemantic similarity among different English translations of world literature in\nthe Par3 dataset. We classify the semantic similarity between paraphrases into\nthree levels: low, medium, and high, and fine-tune three different models\n(mT5-large, LLaMA-2-7B, and Opus-MT) for literary MT tasks. Across different\nmodels, holding the total training instances constant, single-reference but\nmore source texts only marginally outperforms multiple-reference with half of\nthe source texts. Moreover, when fine-tuning an LLM, using paraphrases with\nmedium and high semantic similarity outperforms an unfiltered dataset, with\nimprovements in BLEU (0.3-0.5), COMET (0.1-0.9), and chrF++ (0.17-0.32). Our\ncode is publicly available on GitHub.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AND2_YahfhkMadKHrviBkmxJ7YExmZ_KcNG5_LX4ULo","pdfSize":"564979"}