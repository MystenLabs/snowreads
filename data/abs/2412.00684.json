{"id":"2412.00684","title":"Paint Outside the Box: Synthesizing and Selecting Training Data for\n  Visual Grounding","authors":"Zilin Du, Haoxin Li, Jianfei Yu, Boyang Li","authorsParsed":[["Du","Zilin",""],["Li","Haoxin",""],["Yu","Jianfei",""],["Li","Boyang",""]],"versions":[{"version":"v1","created":"Sun, 1 Dec 2024 05:47:59 GMT"}],"updateDate":"2024-12-03","timestamp":1733032079000,"abstract":"  Visual grounding aims to localize the image regions based on a textual query.\nGiven the difficulty of large-scale data curation, we investigate how to\neffectively learn visual grounding under data-scarce settings in this paper. To\naddress data scarcity, we propose a novel framework, POBF (Paint Outside the\nBox, then Filter). POBF synthesizes images by inpainting outside the box,\ntackling a label misalignment issue encountered in previous works. Furthermore,\nPOBF leverages an innovative filtering scheme to identify the most effective\ntraining data. This scheme combines a hardness score and an overfitting score,\nbalanced by a penalty term. Experimental results show that POBF achieves\nsuperior performance across four datasets, delivering an average improvement of\n5.83% and outperforming leading baselines by 2.29% to 3.85% in accuracy.\nAdditionally, we validate the robustness and generalizability of POBF across\nvarious generative models, data ratios, and model architectures.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"5EeNCrnX0iyF5nqsKkZFnS2psPxlrJlErxmr1IkPLXo","pdfSize":"2200091"}