{"id":"2412.13190","title":"MotionBridge: Dynamic Video Inbetweening with Flexible Controls","authors":"Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang,\n  Krishna Kumar Singh, Nanxuan Zhao","authorsParsed":[["Tanveer","Maham",""],["Zhou","Yang",""],["Niklaus","Simon",""],["Amiri","Ali Mahdavi",""],["Zhang","Hao",""],["Singh","Krishna Kumar",""],["Zhao","Nanxuan",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 18:59:33 GMT"},{"version":"v2","created":"Mon, 23 Dec 2024 07:19:04 GMT"},{"version":"v3","created":"Tue, 7 Jan 2025 22:06:07 GMT"}],"updateDate":"2025-01-09","timestamp":1734461973000,"abstract":"  By generating plausible and smooth transitions between two image frames,\nvideo inbetweening is an essential tool for video editing and long video\nsynthesis. Traditional works lack the capability to generate complex large\nmotions. While recent video generation techniques are powerful in creating\nhigh-quality results, they often lack fine control over the details of\nintermediate frames, which can lead to results that do not align with the\ncreative mind. We introduce MotionBridge, a unified video inbetweening\nframework that allows flexible controls, including trajectory strokes,\nkeyframes, masks, guide pixels, and text. However, learning such multi-modal\ncontrols in a unified framework is a challenging task. We thus design two\ngenerators to extract the control signal faithfully and encode feature through\ndual-branch embedders to resolve ambiguities. We further introduce a curriculum\ntraining strategy to smoothly learn various controls. Extensive qualitative and\nquantitative experiments have demonstrated that such multi-modal controls\nenable a more dynamic, customizable, and contextually accurate visual\nnarrative.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"B0QO9G4OyopLxPyGvuteenV0mss_INinFnuSm2oa2HE","pdfSize":"3170663"}