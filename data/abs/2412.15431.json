{"id":"2412.15431","title":"Time Will Tell: Timing Side Channels via Output Token Count in Large\n  Language Models","authors":"Tianchen Zhang, Gururaj Saileshwar, David Lie","authorsParsed":[["Zhang","Tianchen",""],["Saileshwar","Gururaj",""],["Lie","David",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 22:29:58 GMT"}],"updateDate":"2024-12-23","timestamp":1734647398000,"abstract":"  This paper demonstrates a new side-channel that enables an adversary to\nextract sensitive information about inference inputs in large language models\n(LLMs) based on the number of output tokens in the LLM response. We construct\nattacks using this side-channel in two common LLM tasks: recovering the target\nlanguage in machine translation tasks and recovering the output class in\nclassification tasks. In addition, due to the auto-regressive generation\nmechanism in LLMs, an adversary can recover the output token count reliably\nusing a timing channel, even over the network against a popular closed-source\ncommercial LLM. Our experiments show that an adversary can learn the output\nlanguage in translation tasks with more than 75% precision across three\ndifferent models (Tower, M2M100, MBart50). Using this side-channel, we also\nshow the input class in text classification tasks can be leaked out with more\nthan 70% precision from open-source LLMs like Llama-3.1, Llama-3.2, Gemma2, and\nproduction models like GPT-4o. Finally, we propose tokenizer-, system-, and\nprompt-based mitigations against the output token count side-channel.\n","subjects":["Computer Science/Machine Learning","Computer Science/Computation and Language","Computer Science/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yUgWNEhgpRTiZPeySNtdE525WG64s8AH-ZexzZey8hM","pdfSize":"1667226"}