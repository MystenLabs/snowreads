{"id":"2407.02485","title":"RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in\n  LLMs","authors":"Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang,\n  Mohammad Shoeybi, Bryan Catanzaro","authorsParsed":[["Yu","Yue",""],["Ping","Wei",""],["Liu","Zihan",""],["Wang","Boxin",""],["You","Jiaxuan",""],["Zhang","Chao",""],["Shoeybi","Mohammad",""],["Catanzaro","Bryan",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:59:17 GMT"}],"updateDate":"2024-07-03","timestamp":1719943157000,"abstract":"  Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Retrieval","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fHZFI5uod3zPxOvhXGivLiPa2YLv3HBchPV_cbNVXKQ","pdfSize":"941254","objectId":"0x87242994e54c3cce507918fb622617e74b1a7ee919e4fc4a39d3541817dbd53f","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
