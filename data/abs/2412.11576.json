{"id":"2412.11576","title":"DCBM: Data-Efficient Visual Concept Bottleneck Models","authors":"Katharina Prasse, Patrick Knab, Sascha Marton, Christian Bartelt,\n  Margret Keuper","authorsParsed":[["Prasse","Katharina",""],["Knab","Patrick",""],["Marton","Sascha",""],["Bartelt","Christian",""],["Keuper","Margret",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 09:04:58 GMT"},{"version":"v2","created":"Tue, 4 Feb 2025 15:41:34 GMT"}],"updateDate":"2025-02-05","timestamp":1734339898000,"abstract":"  Concept Bottleneck Models (CBMs) enhance the interpretability of neural\nnetworks by basing predictions on human-understandable concepts. However,\ncurrent CBMs typically rely on concept sets extracted from large language\nmodels or extensive image corpora, limiting their effectiveness in data-sparse\nscenarios. We propose Data-efficient CBMs (DCBMs), which reduce the need for\nlarge sample sizes during concept generation while preserving interpretability.\nDCBMs define concepts as image regions detected by segmentation or detection\nfoundation models, allowing each image to generate multiple concepts across\ndifferent granularities. This removes reliance on textual descriptions and\nlarge-scale pre-training, making DCBMs applicable for fine-grained\nclassification and out-of-distribution tasks. Attribution analysis using\nGrad-CAM demonstrates that DCBMs deliver visual concepts that can be localized\nin test images. By leveraging dataset-specific concepts instead of predefined\nones, DCBMs enhance adaptability to new domains.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"cuwnaQ551yTofQbrVOhBOjdMi0j1pMDuf_6U-6_D9PE","pdfSize":"5465621"}