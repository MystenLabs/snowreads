{"id":"2407.11532","title":"Length-Aware Motion Synthesis via Latent Diffusion","authors":"Alessio Sampieri, Alessio Palma, Indro Spinelli, Fabio Galasso","authorsParsed":[["Sampieri","Alessio",""],["Palma","Alessio",""],["Spinelli","Indro",""],["Galasso","Fabio",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 09:30:57 GMT"}],"updateDate":"2024-07-17","timestamp":1721122257000,"abstract":"  The target duration of a synthesized human motion is a critical attribute\nthat requires modeling control over the motion dynamics and style. Speeding up\nan action performance is not merely fast-forwarding it. However,\nstate-of-the-art techniques for human behavior synthesis have limited control\nover the target sequence length.\n  We introduce the problem of generating length-aware 3D human motion sequences\nfrom textual descriptors, and we propose a novel model to synthesize motions of\nvariable target lengths, which we dub \"Length-Aware Latent Diffusion\" (LADiff).\nLADiff consists of two new modules: 1) a length-aware variational auto-encoder\nto learn motion representations with length-dependent latent codes; 2) a\nlength-conforming latent diffusion model to generate motions with a richness of\ndetails that increases with the required target sequence length. LADiff\nsignificantly improves over the state-of-the-art across most of the existing\nmotion synthesis metrics on the two established benchmarks of HumanML3D and\nKIT-ML.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jZhnUSXmhb_ibyDeLR2zukWNxmZLs2Nq8UpaAp2Lz6w","pdfSize":"2172423"}