{"id":"2412.17128","title":"Lies, Damned Lies, and Distributional Language Statistics: Persuasion\n  and Deception with Large Language Models","authors":"Cameron R. Jones and Benjamin K. Bergen","authorsParsed":[["Jones","Cameron R.",""],["Bergen","Benjamin K.",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 18:34:10 GMT"}],"updateDate":"2024-12-24","timestamp":1734892450000,"abstract":"  Large Language Models (LLMs) can generate content that is as persuasive as\nhuman-written text and appear capable of selectively producing deceptive\noutputs. These capabilities raise concerns about potential misuse and\nunintended consequences as these systems become more widely deployed. This\nreview synthesizes recent empirical work examining LLMs' capacity and\nproclivity for persuasion and deception, analyzes theoretical risks that could\narise from these capabilities, and evaluates proposed mitigations. While\ncurrent persuasive effects are relatively small, various mechanisms could\nincrease their impact, including fine-tuning, multimodality, and social\nfactors. We outline key open questions for future research, including how\npersuasive AI systems might become, whether truth enjoys an inherent advantage\nover falsehoods, and how effective different mitigation strategies may be in\npractice.\n","subjects":["Computer Science/Computation and Language","Computer Science/Computers and Society","Computer Science/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"X8R1GwyT-JbFFtV1JMgZBkvQW--mrz05Umt1TC8z-3k","pdfSize":"942662"}