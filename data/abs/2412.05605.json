{"id":"2412.05605","title":"RefSAM3D: Adapting SAM with Cross-modal Reference for 3D Medical Image\n  Segmentation","authors":"Xiang Gao, Kai Lu","authorsParsed":[["Gao","Xiang",""],["Lu","Kai",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 10:22:46 GMT"}],"updateDate":"2024-12-10","timestamp":1733566966000,"abstract":"  The Segment Anything Model (SAM), originally built on a 2D Vision Transformer\n(ViT), excels at capturing global patterns in 2D natural images but struggles\nwith 3D medical imaging modalities like CT and MRI. These modalities require\ncapturing spatial information in volumetric space for tasks such as organ\nsegmentation and tumor quantification. To address this challenge, we introduce\nRefSAM3D, which adapts SAM for 3D medical imaging by incorporating a 3D image\nadapter and cross-modal reference prompt generation. Our approach modifies the\nvisual encoder to handle 3D inputs and enhances the mask decoder for direct 3D\nmask generation. We also integrate textual prompts to improve segmentation\naccuracy and consistency in complex anatomical scenarios. By employing a\nhierarchical attention mechanism, our model effectively captures and integrates\ninformation across different scales. Extensive evaluations on multiple medical\nimaging datasets demonstrate the superior performance of RefSAM3D over\nstate-of-the-art methods. Our contributions advance the application of SAM in\naccurately segmenting complex anatomical structures in medical imaging.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"yQRaPeY7JOpsE-czgTn2fzxkasg-0Rq5DybtW_fnDPY","pdfSize":"8366542"}