{"id":"2412.11998","title":"SAMIC: Segment Anything with In-Context Spatial Prompt Engineering","authors":"Savinay Nagendra, Kashif Rashid, Chaopeng Shen, Daniel Kifer","authorsParsed":[["Nagendra","Savinay",""],["Rashid","Kashif",""],["Shen","Chaopeng",""],["Kifer","Daniel",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 17:26:06 GMT"}],"updateDate":"2024-12-17","timestamp":1734369966000,"abstract":"  Few-shot segmentation is the problem of learning to identify specific types\nof objects (e.g., airplanes) in images from a small set of labeled reference\nimages. The current state of the art is driven by resource-intensive\nconstruction of models for every new domain-specific application. Such models\nmust be trained on enormous labeled datasets of unrelated objects (e.g., cars,\ntrains, animals) so that their ``knowledge'' can be transferred to new types of\nobjects. In this paper, we show how to leverage existing vision foundation\nmodels (VFMs) to reduce the incremental cost of creating few-shot segmentation\nmodels for new domains. Specifically, we introduce SAMIC, a small network that\nlearns how to prompt VFMs in order to segment new types of objects in\ndomain-specific applications. SAMIC enables any task to be approached as a\nfew-shot learning problem. At 2.6 million parameters, it is 94% smaller than\nthe leading models (e.g., having ResNet 101 backbone with 45+ million\nparameters). Even using 1/5th of the training data provided by one-shot\nbenchmarks, SAMIC is competitive with, or sets the state of the art, on a\nvariety of few-shot and semantic segmentation datasets including COCO-$20^i$,\nPascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"7D0xHPhzodfvuwYhtVLsGkHTcSaSii2w1WYDHAFtOLg","pdfSize":"4319271"}