{"id":"2407.00487","title":"It's Morphing Time: Unleashing the Potential of Multiple LLMs via\n  Multi-objective Optimization","authors":"Bingdong Li, Zixiang Di, Yanting Yang, Hong Qian, Peng Yang, Hao Hao,\n  Ke Tang, Aimin Zhou","authorsParsed":[["Li","Bingdong",""],["Di","Zixiang",""],["Yang","Yanting",""],["Qian","Hong",""],["Yang","Peng",""],["Hao","Hao",""],["Tang","Ke",""],["Zhou","Aimin",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 16:34:23 GMT"},{"version":"v2","created":"Mon, 12 Aug 2024 14:06:48 GMT"}],"updateDate":"2024-08-13","timestamp":1719678863000,"abstract":"  In this paper, we introduce a novel approach for large language model merging\nvia black-box multi-objective optimization algorithms. The goal of model\nmerging is to combine multiple models, each excelling in different tasks, into\na single model that outperforms any of the individual source models. However,\nmodel merging faces two significant challenges: First, existing methods rely\nheavily on human intuition and customized strategies to tackle multiple tasks.\nSecond, it's difficult to search for the great model merging configuration in\nlimited evaluations. To address these challenges, we propose a multi-objective\noptimization based model merging method named MM-MO. The proposed method can\nautomatically search merging configurations for multiple tasks with\nmulti-objective optimization algorithms. Moreover, to obtain high-quality model\nmerging configurations within a limited number of evaluation iterations, we\nhave made several improvements to multi-objective Bayesian optimization\nspecifically for model merging scenarios. First, we introduced a weak-to-strong\nmethod to improve the acquisition strategy. Second, we employed Fisher\ninformation to select configurations, further increasing the chances of\ndiscovering superior model merging configurations. Third, we designed a\nsparsity metric as an additional optimization objective to enhance the model's\ngeneralization performance across different tasks. We conducted comprehensive\nexperiments with other mainstream model merging methods, demonstrating that our\nmethod consistently outperforms them. Moreover, performance improvements are\nobserved even on the tasks not explicitly targeted as optimization objectives,\nindicating that our method enhances the overall potential of the model. ...\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HPtRGLYGOgqENmOVL-wLF6e4pEySmYpNp26rcYIk8sI","pdfSize":"1419892"}