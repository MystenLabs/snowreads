{"id":"2412.00099","title":"Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference","authors":"Andrii Skliar, Ties van Rozendaal, Romain Lepert, Todor Boinovski,\n  Mart van Baalen, Markus Nagel, Paul Whatmough, Babak Ehteshami Bejnordi","authorsParsed":[["Skliar","Andrii",""],["van Rozendaal","Ties",""],["Lepert","Romain",""],["Boinovski","Todor",""],["van Baalen","Mart",""],["Nagel","Markus",""],["Whatmough","Paul",""],["Bejnordi","Babak Ehteshami",""]],"versions":[{"version":"v1","created":"Wed, 27 Nov 2024 18:59:48 GMT"}],"updateDate":"2024-12-03","timestamp":1732733988000,"abstract":"  Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Hardware Architecture"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uICCucTnEhCcNKTQ50b4cONYFtBes2WnPQAKnSXz32g","pdfSize":"1573600"}