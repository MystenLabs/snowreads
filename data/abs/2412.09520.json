{"id":"2412.09520","title":"GainAdaptor: Learning Quadrupedal Locomotion with Dual Actors for\n  Adaptable and Energy-Efficient Walking on Various Terrains","authors":"Mincheol Kim, Nahyun Kwon, Jung-Yup Kim","authorsParsed":[["Kim","Mincheol",""],["Kwon","Nahyun",""],["Kim","Jung-Yup",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 18:06:22 GMT"}],"updateDate":"2024-12-13","timestamp":1734026782000,"abstract":"  Deep reinforcement learning (DRL) has emerged as an innovative solution for\ncontrolling legged robots in challenging environments using minimalist\narchitectures. Traditional control methods for legged robots, such as inverse\ndynamics, either directly manage joint torques or use proportional-derivative\n(PD) controllers to regulate joint positions at a higher level. In case of DRL,\ndirect torque control presents significant challenges, leading to a preference\nfor joint position control. However, this approach necessitates careful\nadjustment of joint PD gains, which can limit both adaptability and efficiency.\nIn this paper, we propose GainAdaptor, an adaptive gain control framework that\nautonomously tunes joint PD gains to enhance terrain adaptability and energy\nefficiency. The framework employs a dual-actor algorithm to dynamically adjust\nthe PD gains based on varying ground conditions. By utilizing a divided action\nspace, GainAdaptor efficiently learns stable and energy-efficient locomotion.\nWe validate the effectiveness of the proposed method through experiments\nconducted on a Unitree Go1 robot, demonstrating improved locomotion performance\nacross diverse terrains.\n","subjects":["Computer Science/Robotics","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"heo8MmkFcZY-4_HOxLbeUciNzcPi38TGl34PIxTo1_Q","pdfSize":"8221729"}