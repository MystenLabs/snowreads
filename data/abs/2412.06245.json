{"id":"2412.06245","title":"A Comparative Study of Learning Paradigms in Large Language Models via\n  Intrinsic Dimension","authors":"Saahith Janapati and Yangfeng Ji","authorsParsed":[["Janapati","Saahith",""],["Ji","Yangfeng",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 06:37:35 GMT"}],"updateDate":"2024-12-10","timestamp":1733726255000,"abstract":"  The performance of Large Language Models (LLMs) on natural language tasks can\nbe improved through both supervised fine-tuning (SFT) and in-context learning\n(ICL), which operate via distinct mechanisms. Supervised fine-tuning updates\nthe model's weights by minimizing loss on training data, whereas in-context\nlearning leverages task demonstrations embedded in the prompt, without changing\nthe model's parameters. This study investigates the effects of these learning\nparadigms on the hidden representations of LLMs using Intrinsic Dimension (ID).\nWe use ID to estimate the number of degrees of freedom between representations\nextracted from LLMs as they perform specific natural language tasks. We first\nexplore how the ID of LLM representations evolves during SFT and how it varies\ndue to the number of demonstrations in ICL. We then compare the IDs induced by\nSFT and ICL and find that ICL consistently induces a higher ID compared to SFT,\nsuggesting that representations generated during ICL reside in higher\ndimensional manifolds in the embedding space.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fzdZdIzGOEYYKE0FSkiOSCddy-koCeQ4fomUuOXsO0s","pdfSize":"1530554"}