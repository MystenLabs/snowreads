{
  "id": "2412.03752",
  "title": "Beyond Local Sharpness: Communication-Efficient Global Sharpness-aware\n  Minimization for Federated Learning",
  "authors": "Debora Caldarola, Pietro Cagnasso, Barbara Caputo, Marco Ciccone",
  "authorsParsed": [
    [
      "Caldarola",
      "Debora",
      ""
    ],
    [
      "Cagnasso",
      "Pietro",
      ""
    ],
    [
      "Caputo",
      "Barbara",
      ""
    ],
    [
      "Ciccone",
      "Marco",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 22:46:06 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733352366000,
  "abstract": "  Federated learning (FL) enables collaborative model training with privacy\npreservation. Data heterogeneity across edge devices (clients) can cause models\nto converge to sharp minima, negatively impacting generalization and\nrobustness. Recent approaches use client-side sharpness-aware minimization\n(SAM) to encourage flatter minima, but the discrepancy between local and global\nloss landscapes often undermines their effectiveness, as optimizing for local\nsharpness does not ensure global flatness. This work introduces FedGloSS\n(Federated Global Server-side Sharpness), a novel FL approach that prioritizes\nthe optimization of global sharpness on the server, using SAM. To reduce\ncommunication overhead, FedGloSS cleverly approximates sharpness using the\nprevious global gradient, eliminating the need for additional client\ncommunication. Our extensive evaluations demonstrate that FedGloSS consistently\nreaches flatter minima and better performance compared to state-of-the-art FL\nmethods across various federated vision benchmarks.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "5lYhy3JdOhkYrcWp3UcyCsuV6TxU3UgfP_QlyDMUpUE",
  "pdfSize": "28041373"
}