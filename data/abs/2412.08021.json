{
  "id": "2412.08021",
  "title": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill\n  Learning",
  "authors": "Chongyi Zheng, Jens Tuyls, Joanne Peng, Benjamin Eysenbach",
  "authorsParsed": [
    [
      "Zheng",
      "Chongyi",
      ""
    ],
    [
      "Tuyls",
      "Jens",
      ""
    ],
    [
      "Peng",
      "Joanne",
      ""
    ],
    [
      "Eysenbach",
      "Benjamin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 02:00:39 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733882439000,
  "abstract": "  Self-supervised learning has the potential of lifting several of the key\nchallenges in reinforcement learning today, such as exploration, representation\nlearning, and reward design. Recent work (METRA) has effectively argued that\nmoving away from mutual information and instead optimizing a certain\nWasserstein distance is important for good performance. In this paper, we argue\nthat the benefits seen in that paper can largely be explained within the\nexisting framework of mutual information skill learning (MISL). Our analysis\nsuggests a new MISL method (contrastive successor features) that retains the\nexcellent performance of METRA with fewer moving parts, and highlights\nconnections between skill learning, contrastive representation learning, and\nsuccessor features. Finally, through careful ablation studies, we provide\nfurther insight into some of the key ingredients for both our method and METRA.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "NKlQrewtyp_zEg9hJL_2oM_i_aynNiK53O8PaaaLHPM",
  "pdfSize": "2874596"
}