{"id":"2412.09511","title":"GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency","authors":"Dongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee","authorsParsed":[["Lu","Dongyue",""],["Kong","Lingdong",""],["Huang","Tianxin",""],["Lee","Gim Hee",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 17:59:03 GMT"}],"updateDate":"2024-12-13","timestamp":1734026343000,"abstract":"  Identifying affordance regions on 3D objects from semantic cues is essential\nfor robotics and human-machine interaction. However, existing 3D affordance\nlearning methods struggle with generalization and robustness due to limited\nannotated data and a reliance on 3D backbones focused on geometric encoding,\nwhich often lack resilience to real-world noise and data corruption. We propose\nGEAL, a novel framework designed to enhance the generalization and robustness\nof 3D affordance learning by leveraging large-scale pre-trained 2D models. We\nemploy a dual-branch architecture with Gaussian splatting to establish\nconsistent mappings between 3D point clouds and 2D representations, enabling\nrealistic 2D renderings from sparse point clouds. A granularity-adaptive fusion\nmodule and a 2D-3D consistency alignment module further strengthen cross-modal\nalignment and knowledge transfer, allowing the 3D branch to benefit from the\nrich semantics and generalization capacity of 2D models. To holistically assess\nthe robustness, we introduce two new corruption-based benchmarks: PIAD-C and\nLASO-C. Extensive experiments on public datasets and our benchmarks show that\nGEAL consistently outperforms existing methods across seen and novel object\ncategories, as well as corrupted data, demonstrating robust and adaptable\naffordance prediction under diverse conditions. Code and corruption datasets\nhave been made publicly available.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Q0ZBbgrfWSgLNcmTs3q3JCH4ZQZ3oA18hQNZxIzmp24","pdfSize":"6825253"}