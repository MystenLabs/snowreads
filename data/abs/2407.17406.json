{"id":"2407.17406","title":"Dependency Transformer Grammars: Integrating Dependency Structures into\n  Transformer Language Models","authors":"Yida Zhao, Chao Lou, Kewei Tu","authorsParsed":[["Zhao","Yida",""],["Lou","Chao",""],["Tu","Kewei",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 16:38:38 GMT"}],"updateDate":"2024-07-25","timestamp":1721839118000,"abstract":"  Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"63VRyIeK09_mRog9L5QqGZfbPsOKL1XGxsdwCAjTYjw","pdfSize":"572989"}