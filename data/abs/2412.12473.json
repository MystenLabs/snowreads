{
  "id": "2412.12473",
  "title": "A Method for Enhancing Generalization of Adam by Multiple Integrations",
  "authors": "Long Jin, Han Nong, Liangming Chen, Zhenming Su",
  "authorsParsed": [
    [
      "Jin",
      "Long",
      ""
    ],
    [
      "Nong",
      "Han",
      ""
    ],
    [
      "Chen",
      "Liangming",
      ""
    ],
    [
      "Su",
      "Zhenming",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 02:15:38 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734401738000,
  "abstract": "  The insufficient generalization of adaptive moment estimation (Adam) has\nhindered its broader application. Recent studies have shown that flat minima in\nloss landscapes are highly associated with improved generalization. Inspired by\nthe filtering effect of integration operations on high-frequency signals, we\npropose multiple integral Adam (MIAdam), a novel optimizer that integrates a\nmultiple integral term into Adam. This multiple integral term effectively\nfilters out sharp minima encountered during optimization, guiding the optimizer\ntowards flatter regions and thereby enhancing generalization capability. We\nprovide a theoretical explanation for the improvement in generalization through\nthe diffusion theory framework and analyze the impact of the multiple integral\nterm on the optimizer's convergence. Experimental results demonstrate that\nMIAdam not only enhances generalization and robustness against label noise but\nalso maintains the rapid convergence characteristic of Adam, outperforming Adam\nand its variants in state-of-the-art benchmarks.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "SC_T7_Nmv_rCkuRskE7R3-9CM0AUeebPyD9pMqHEcg4",
  "pdfSize": "16901592"
}