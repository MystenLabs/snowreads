{
  "id": "2412.03205",
  "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs",
  "authors": "Konstantin Chernyshev, Vitaliy Polshkov, Ekaterina Artemova, Alex\n  Myasnikov, Vlad Stepanov, Alexei Miasnikov and Sergei Tilga",
  "authorsParsed": [
    [
      "Chernyshev",
      "Konstantin",
      ""
    ],
    [
      "Polshkov",
      "Vitaliy",
      ""
    ],
    [
      "Artemova",
      "Ekaterina",
      ""
    ],
    [
      "Myasnikov",
      "Alex",
      ""
    ],
    [
      "Stepanov",
      "Vlad",
      ""
    ],
    [
      "Miasnikov",
      "Alexei",
      ""
    ],
    [
      "Tilga",
      "Sergei",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 10:44:50 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 6 Dec 2024 08:29:43 GMT"
    },
    {
      "version": "v3",
      "created": "Tue, 14 Jan 2025 21:58:47 GMT"
    }
  ],
  "updateDate": "2025-01-16",
  "timestamp": 1733309090000,
  "abstract": "  The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "OZ74wyF0iR7Sjj9_su_RRYuOQXaxg6cMXoCsmTjtOEY",
  "pdfSize": "2119227"
}