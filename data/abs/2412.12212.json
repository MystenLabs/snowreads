{
  "id": "2412.12212",
  "title": "Finding a Wolf in Sheep's Clothing: Combating Adversarial Text-To-Image\n  Prompts with Text Summarization",
  "authors": "Portia Cooper, Harshita Narnoli, Mihai Surdeanu",
  "authorsParsed": [
    [
      "Cooper",
      "Portia",
      ""
    ],
    [
      "Narnoli",
      "Harshita",
      ""
    ],
    [
      "Surdeanu",
      "Mihai",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 15 Dec 2024 22:12:36 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734300756000,
  "abstract": "  Text-to-image models are vulnerable to the stepwise \"Divide-and-Conquer\nAttack\" (DACA) that utilize a large language model to obfuscate inappropriate\ncontent in prompts by wrapping sensitive text in a benign narrative. To\nmitigate stepwise DACA attacks, we propose a two-layer method involving text\nsummarization followed by binary classification. We assembled the Adversarial\nText-to-Image Prompt (ATTIP) dataset ($N=940$), which contained DACA-obfuscated\nand non-obfuscated prompts. From the ATTIP dataset, we created two summarized\nversions: one generated by a small encoder model and the other by a large\nlanguage model. Then, we used an encoder classifier and a GPT-4o classifier to\nperform content moderation on the summarized and unsummarized prompts. When\ncompared with a classifier that operated over the unsummarized data, our method\nimproved F1 score performance by 31%. Further, the highest recorded F1 score\nachieved (98%) was produced by the encoder classifier on a summarized ATTIP\nvariant. This study indicates that pre-classification text summarization can\ninoculate content detection models against stepwise DACA obfuscations.\n",
  "subjects": [
    "Computer Science/Cryptography and Security",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "9J_6LfCjo9UFe4bw2ql54PdBRIjklW-q71MlcUdsnZ0",
  "pdfSize": "735938"
}