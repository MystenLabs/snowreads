{"id":"2407.04549","title":"Spontaneous Reward Hacking in Iterative Self-Refinement","authors":"Jane Pan, He He, Samuel R. Bowman, Shi Feng","authorsParsed":[["Pan","Jane",""],["He","He",""],["Bowman","Samuel R.",""],["Feng","Shi",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 14:34:50 GMT"}],"updateDate":"2024-07-08","timestamp":1720190090000,"abstract":"  Language models are capable of iteratively improving their outputs based on\nnatural language feedback, thus enabling in-context optimization of user\npreference. In place of human users, a second language model can be used as an\nevaluator, providing feedback along with numerical ratings which the generator\nattempts to optimize. However, because the evaluator is an imperfect proxy of\nuser preference, this optimization can lead to reward hacking, where the\nevaluator's ratings improve while the generation quality remains stagnant or\neven decreases as judged by actual user preference. The concern of reward\nhacking is heightened in iterative self-refinement where the generator and the\nevaluator use the same underlying language model, in which case the\noptimization pressure can drive them to exploit shared vulnerabilities. Using\nan essay editing task, we show that iterative self-refinement leads to\ndeviation between the language model evaluator and human judgment,\ndemonstrating that reward hacking can occur spontaneously in-context with the\nuse of iterative self-refinement. In addition, we study conditions under which\nreward hacking occurs and observe two factors that affect reward hacking\nseverity: model size and context sharing between the generator and the\nevaluator.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"48Z9NYxoLOer3tzThH-egNJuEzsViveHybTLK9TSt3U","pdfSize":"677560"}