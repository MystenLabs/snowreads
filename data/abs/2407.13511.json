{"id":"2407.13511","title":"Can Open-Source LLMs Compete with Commercial Models? Exploring the\n  Few-Shot Performance of Current GPT Models in Biomedical Tasks","authors":"Samy Ateia, Udo Kruschwitz","authorsParsed":[["Ateia","Samy",""],["Kruschwitz","Udo",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 13:43:01 GMT"}],"updateDate":"2024-07-19","timestamp":1721310181000,"abstract":"  Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT\nand Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)\nbenchmarks across different domains. New competing Open-Source alternatives\nlike Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while\noften offering higher throughput and being less costly to use. Open-Source LLMs\ncan also be self-hosted, which makes them interesting for enterprise and\nclinical use cases where sensitive data should not be processed by third\nparties. We participated in the 12th BioASQ challenge, which is a retrieval\naugmented generation (RAG) setting, and explored the performance of current GPT\nmodels Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning\n(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional\nrelevant knowledge from Wikipedia added to the context-window of the LLM might\nimprove their performance. Mixtral 8x7b was competitive in the 10-shot setting,\nboth with and without fine-tuning, but failed to produce usable results in the\nzero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to\nmeasurable performance gains. Our results indicate that the performance gap\nbetween commercial and open-source models in RAG setups exists mainly in the\nzero-shot setting and can be closed by simply collecting few-shot examples for\ndomain-specific use cases. The code needed to rerun these experiments is\navailable through GitHub.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LJWx7WGbc8eMKJtWKjJp3V_9Vu8mFNOQRtdbgCoZ2nI","pdfSize":"781267"}