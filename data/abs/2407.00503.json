{"id":"2407.00503","title":"Toward a Diffusion-Based Generalist for Dense Vision Tasks","authors":"Yue Fan, Yongqin Xian, Xiaohua Zhai, Alexander Kolesnikov, Muhammad\n  Ferjad Naeem, Bernt Schiele, Federico Tombari","authorsParsed":[["Fan","Yue",""],["Xian","Yongqin",""],["Zhai","Xiaohua",""],["Kolesnikov","Alexander",""],["Naeem","Muhammad Ferjad",""],["Schiele","Bernt",""],["Tombari","Federico",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 17:57:22 GMT"}],"updateDate":"2024-07-02","timestamp":1719683842000,"abstract":"  Building generalized models that can solve many computer vision tasks\nsimultaneously is an intriguing direction. Recent works have shown image itself\ncan be used as a natural interface for general-purpose visual perception and\ndemonstrated inspiring results. In this paper, we explore diffusion-based\nvision generalists, where we unify different types of dense prediction tasks as\nconditional image generation and re-purpose pre-trained diffusion models for\nit. However, directly applying off-the-shelf latent diffusion models leads to a\nquantization issue. Thus, we propose to perform diffusion in pixel space and\nprovide a recipe for finetuning pre-trained text-to-image diffusion models for\ndense vision tasks. In experiments, we evaluate our method on four different\ntypes of tasks and show competitive performance to the other vision\ngeneralists.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"p95TcrRZ2sZtAStsRKjtGAZXjsdldvoyU-snFM7KRuA","pdfSize":"3994791","objectId":"0x8a3ad0dd228910e62581e1dc5821cef740df93f4fe1d1d837697ea80272af124","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
