{"id":"2412.00966","title":"From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine\n  Translation","authors":"Ali Marashian, Enora Rice, Luke Gessler, Alexis Palmer, Katharina von\n  der Wense","authorsParsed":[["Marashian","Ali",""],["Rice","Enora",""],["Gessler","Luke",""],["Palmer","Alexis",""],["von der Wense","Katharina",""]],"versions":[{"version":"v1","created":"Sun, 1 Dec 2024 21:06:08 GMT"},{"version":"v2","created":"Thu, 30 Jan 2025 20:13:25 GMT"},{"version":"v3","created":"Fri, 21 Feb 2025 16:42:44 GMT"}],"updateDate":"2025-02-24","timestamp":1733087168000,"abstract":"  Many of the world's languages have insufficient data to train high-performing\ngeneral neural machine translation (NMT) models, let alone domain-specific\nmodels, and often the only available parallel data are small amounts of\nreligious texts. Hence, domain adaptation (DA) is a crucial issue faced by\ncontemporary NMT and has, so far, been underexplored for low-resource\nlanguages. In this paper, we evaluate a set of methods from both low-resource\nNMT and DA in a realistic setting, in which we aim to translate between a\nhigh-resource and a low-resource language with access to only: a) parallel\nBible data, b) a bilingual dictionary, and c) a monolingual target-domain\ncorpus in the high-resource language. Our results show that the effectiveness\nof the tested methods varies, with the simplest one, DALI, being most\neffective. We follow up with a small human evaluation of DALI, which shows that\nthere is still a need for more careful investigation of how to accomplish DA\nfor low-resource NMT.\n","subjects":["Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"iQfhsOlxelkyayKCHhMd9Fa862id4ylmFEJZxySE-uw","pdfSize":"347303"}