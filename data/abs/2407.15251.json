{"id":"2407.15251","title":"Achieving Human Level Partial Credit Grading of Written Responses to\n  Physics Conceptual Question using GPT-3.5 with Only Prompt Engineering","authors":"Zhongzhou Chen and Tong Wan","authorsParsed":[["Chen","Zhongzhou",""],["Wan","Tong",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 19:49:18 GMT"}],"updateDate":"2024-07-23","timestamp":1721591358000,"abstract":"  Large language modules (LLMs) have great potential for auto-grading student\nwritten responses to physics problems due to their capacity to process and\ngenerate natural language. In this explorative study, we use a prompt\nengineering technique, which we name \"scaffolded chain of thought (COT)\", to\ninstruct GPT-3.5 to grade student written responses to a physics conceptual\nquestion. Compared to common COT prompting, scaffolded COT prompts GPT-3.5 to\nexplicitly compare student responses to a detailed, well-explained rubric\nbefore generating the grading outcome. We show that when compared to human\nraters, the grading accuracy of GPT-3.5 using scaffolded COT is 20% - 30%\nhigher than conventional COT. The level of agreement between AI and human\nraters can reach 70% - 80%, comparable to the level between two human raters.\nThis shows promise that an LLM-based AI grader can achieve human-level grading\naccuracy on a physics conceptual problem using prompt engineering techniques\nalone.\n","subjects":["Physics/Physics Education"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"eOBVqMsISX1px083kB9lB5KEYis0Ug372CWvaRQ16AI","pdfSize":"326545"}