{"id":"2412.13224","title":"Physics-model-guided Worst-case Sampling for Safe Reinforcement Learning","authors":"Hongpeng Cao, Yanbing Mao, Lui Sha, Marco Caccamo","authorsParsed":[["Cao","Hongpeng",""],["Mao","Yanbing",""],["Sha","Lui",""],["Caccamo","Marco",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 04:13:06 GMT"}],"updateDate":"2024-12-19","timestamp":1734408786000,"abstract":"  Real-world accidents in learning-enabled CPS frequently occur in challenging\ncorner cases. During the training of deep reinforcement learning (DRL) policy,\nthe standard setup for training conditions is either fixed at a single initial\ncondition or uniformly sampled from the admissible state space. This setup\noften overlooks the challenging but safety-critical corner cases. To bridge\nthis gap, this paper proposes a physics-model-guided worst-case sampling\nstrategy for training safe policies that can handle safety-critical cases\ntoward guaranteed safety. Furthermore, we integrate the proposed worst-case\nsampling strategy into the physics-regulated deep reinforcement learning\n(Phy-DRL) framework to build a more data-efficient and safe learning algorithm\nfor safety-critical CPS. We validate the proposed training strategy with\nPhy-DRL through extensive experiments on a simulated cart-pole system, a 2D\nquadrotor, a simulated and a real quadruped robot, showing remarkably improved\nsampling efficiency to learn more robust safe policies.\n","subjects":["Computer Science/Robotics","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JbPSJuGgLM5CnMYFapNsz00129_LfPxsqJs3aw8VNKc","pdfSize":"7841378"}