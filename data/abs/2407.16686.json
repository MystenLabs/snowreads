{"id":"2407.16686","title":"Can Large Language Models Automatically Jailbreak GPT-4V?","authors":"Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun","authorsParsed":[["Wu","Yuanwei",""],["Huang","Yue",""],["Liu","Yixin",""],["Li","Xiang",""],["Zhou","Pan",""],["Sun","Lichao",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 17:50:45 GMT"},{"version":"v2","created":"Fri, 23 Aug 2024 01:52:10 GMT"}],"updateDate":"2024-08-26","timestamp":1721757045000,"abstract":"  GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Wkj--5M5z7y7EO4znWvFvfxm0F6wlZLZPRSY2cHKdU8","pdfSize":"866037"}