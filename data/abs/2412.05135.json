{"id":"2412.05135","title":"The Polynomial Stein Discrepancy for Assessing Moment Convergence","authors":"Narayan Srinivasan, Matthew Sutton, Christopher Drovandi, and Leah F\n  South","authorsParsed":[["Srinivasan","Narayan",""],["Sutton","Matthew",""],["Drovandi","Christopher",""],["South","Leah F",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 15:51:04 GMT"}],"updateDate":"2024-12-09","timestamp":1733500264000,"abstract":"  We propose a novel method for measuring the discrepancy between a set of\nsamples and a desired posterior distribution for Bayesian inference. Classical\nmethods for assessing sample quality like the effective sample size are not\nappropriate for scalable Bayesian sampling algorithms, such as stochastic\ngradient Langevin dynamics, that are asymptotically biased. Instead, the gold\nstandard is to use the kernel Stein Discrepancy (KSD), which is itself not\nscalable given its quadratic cost in the number of samples. The KSD and its\nfaster extensions also typically suffer from the curse-of-dimensionality and\ncan require extensive tuning. To address these limitations, we develop the\npolynomial Stein discrepancy (PSD) and an associated goodness-of-fit test.\nWhile the new test is not fully convergence-determining, we prove that it\ndetects differences in the first r moments in the Bernstein-von Mises limit. We\nempirically show that the test has higher power than its competitors in several\nexamples, and at a lower computational cost. Finally, we demonstrate that the\nPSD can assist practitioners to select hyper-parameters of Bayesian sampling\nalgorithms more efficiently than competitors.\n","subjects":["Statistics/Machine Learning","Computer Science/Machine Learning","Statistics/Computation"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Mh_wM-S0gyMnVUItrp-9rxGVUECBISSsGVT9dOY9WRE","pdfSize":"1433429"}