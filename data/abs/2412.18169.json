{
  "id": "2412.18169",
  "title": "KunServe: Elastic and Efficient Large Language Model Serving with\n  Parameter-centric Memory Management",
  "authors": "Rongxin Cheng and Yifan Peng and Yuxin Lai and Xingda Wei and Rong\n  Chen and Haibo Chen",
  "authorsParsed": [
    [
      "Cheng",
      "Rongxin",
      ""
    ],
    [
      "Peng",
      "Yifan",
      ""
    ],
    [
      "Lai",
      "Yuxin",
      ""
    ],
    [
      "Wei",
      "Xingda",
      ""
    ],
    [
      "Chen",
      "Rong",
      ""
    ],
    [
      "Chen",
      "Haibo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 24 Dec 2024 05:07:46 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 26 Dec 2024 03:28:03 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735016866000,
  "abstract": "  The stateful nature of large language model (LLM) servingcan easily throttle\nprecious GPU memory under load burstor long-generation requests like\nchain-of-thought reasoning,causing latency spikes due to queuing incoming\nrequests. However, state-of-the-art KVCache centric approaches handleload\nspikes by dropping, migrating, or swapping KVCache,which faces an essential\ntradeoff between the performance ofongoing vs. incoming requests and thus still\nseverely violatesSLO.This paper makes a key observation such that model\nparam-eters are independent of the requests and are replicated acrossGPUs, and\nthus proposes a parameter-centric approach byselectively dropping replicated\nparameters to leave preciousmemory for requests. However, LLM requires KVCache\ntobe saved in bound with model parameters and thus droppingparameters can cause\neither huge computation waste or longnetwork delay, affecting all ongoing\nrequests. Based on the ob-servation that attention operators can be decoupled\nfrom otheroperators, this paper further proposes a novel remote\nattentionmechanism through pipeline parallelism so as to serve up-coming\nrequests with the additional memory borrowed fromparameters on remote GPUs.\nThis paper further addresses sev-eral other challenges including lively\nexchanging KVCachewith incomplete parameters, generating an appropriate\nplanthat balances memory requirements with cooperative exe-cution overhead, and\nseamlessly restoring parameters whenthe throttling has gone. Evaluations show\nthatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x\ncompared to the state-of-the-art.\n",
  "subjects": [
    "Computer Science/Distributed, Parallel, and Cluster Computing",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "OYam2hwfHp_U3fx3JJ2m4IJwUNuqwCC5rF8D6RtZMVs",
  "pdfSize": "3345304"
}