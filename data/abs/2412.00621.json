{"id":"2412.00621","title":"Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance","authors":"Chen-Wei Chang, Shailik Sarkar, Shutonu Mitra, Qi Zhang, Hossein\n  Salemi, Hemant Purohit, Fengxiu Zhang, Michin Hong, Jin-Hee Cho, Chang-Tien\n  Lu","authorsParsed":[["Chang","Chen-Wei",""],["Sarkar","Shailik",""],["Mitra","Shutonu",""],["Zhang","Qi",""],["Salemi","Hossein",""],["Purohit","Hemant",""],["Zhang","Fengxiu",""],["Hong","Michin",""],["Cho","Jin-Hee",""],["Lu","Chang-Tien",""]],"versions":[{"version":"v1","created":"Sun, 1 Dec 2024 00:13:28 GMT"}],"updateDate":"2024-12-03","timestamp":1733012008000,"abstract":"  Can we trust Large Language Models (LLMs) to accurately predict scam? This\npaper investigates the vulnerabilities of LLMs when facing adversarial scam\nmessages for the task of scam detection. We addressed this issue by creating a\ncomprehensive dataset with fine-grained labels of scam messages, including both\noriginal and adversarial scam messages. The dataset extended traditional binary\nclasses for the scam detection task into more nuanced scam types. Our analysis\nshowed how adversarial examples took advantage of vulnerabilities of a LLM,\nleading to high misclassification rate. We evaluated the performance of LLMs on\nthese adversarial scam messages and proposed strategies to improve their\nrobustness.\n","subjects":["Computer Science/Cryptography and Security","Computer Science/Artificial Intelligence","Computer Science/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"QhGQyeZXjVQQjoj1dFM6lLVloguJGjtwYMYhYf5jRTM","pdfSize":"333498"}