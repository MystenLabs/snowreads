{"id":"2412.04842","title":"UniMLVG: Unified Framework for Multi-view Long Video Generation with\n  Comprehensive Control Capabilities for Autonomous Driving","authors":"Rui Chen, Zehuan Wu, Yichen Liu, Yuxin Guo, Jingcheng Ni, Haifeng Xia,\n  Siyu Xia","authorsParsed":[["Chen","Rui",""],["Wu","Zehuan",""],["Liu","Yichen",""],["Guo","Yuxin",""],["Ni","Jingcheng",""],["Xia","Haifeng",""],["Xia","Siyu",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 08:27:53 GMT"},{"version":"v2","created":"Mon, 20 Jan 2025 06:32:52 GMT"}],"updateDate":"2025-01-22","timestamp":1733473673000,"abstract":"  The creation of diverse and realistic driving scenarios has become essential\nto enhance perception and planning capabilities of the autonomous driving\nsystem. However, generating long-duration, surround-view consistent driving\nvideos remains a significant challenge. To address this, we present UniMLVG, a\nunified framework designed to generate extended street multi-perspective videos\nunder precise control. By integrating single- and multi-view driving videos\ninto the training data, our approach updates cross-frame and cross-view modules\nacross three stages with different training objectives, substantially boosting\nthe diversity and quality of generated visual content. Additionally, we employ\nthe explicit viewpoint modeling in multi-view video generation to effectively\nimprove motion transition consistency. Capable of handling various input\nreference formats (e.g., text, images, or video), our UniMLVG generates\nhigh-quality multi-view videos according to the corresponding condition\nconstraints such as 3D bounding boxes or frame-level text descriptions.\nCompared to the best models with similar capabilities, our framework achieves\nimprovements of 21.4% in FID and 36.5% in FVD.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"sg1Hc9aISwSK1T-Avg52K8h7cwwEp3gMCV96u4TRN8k","pdfSize":"14006240"}