{"id":"2412.05439","title":"Statistical Mechanics of Support Vector Regression","authors":"Abdulkadir Canatar and SueYeon Chung","authorsParsed":[["Canatar","Abdulkadir",""],["Chung","SueYeon",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 21:48:00 GMT"}],"updateDate":"2024-12-10","timestamp":1733521680000,"abstract":"  A key problem in deep learning and computational neuroscience is relating the\ngeometrical properties of neural representations to task performance. Here, we\nconsider this problem for continuous decoding tasks where neural variability\nmay affect task precision. Using methods from statistical mechanics, we study\nthe average-case learning curves for $\\varepsilon$-insensitive Support Vector\nRegression ($\\varepsilon$-SVR) and discuss its capacity as a measure of linear\ndecodability. Our analysis reveals a phase transition in the training error at\na critical load, capturing the interplay between the tolerance parameter\n$\\varepsilon$ and neural variability. We uncover a double-descent phenomenon in\nthe generalization error, showing that $\\varepsilon$ acts as a regularizer,\nboth suppressing and shifting these peaks. Theoretical predictions are\nvalidated both on toy models and deep neural networks, extending the theory of\nSupport Vector Machines to continuous tasks with inherent neural variability.\n","subjects":["Condensed Matter/Disordered Systems and Neural Networks","Quantitative Biology/Neurons and Cognition","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TciQqk7b35zXAp8iunsppniVMKc19W502LKvHoo0cRw","pdfSize":"2083013"}