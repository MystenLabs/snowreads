{"id":"2407.12498","title":"Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of\n  Few-Shot Learning","authors":"Mustafa Dogan, Ilker Kesen, Iacer Calixto, Aykut Erdem, Erkut Erdem","authorsParsed":[["Dogan","Mustafa",""],["Kesen","Ilker",""],["Calixto","Iacer",""],["Erdem","Aykut",""],["Erdem","Erkut",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 11:26:47 GMT"}],"updateDate":"2024-07-18","timestamp":1721215607000,"abstract":"  The linguistic capabilities of Multimodal Large Language Models (MLLMs) are\ncritical for their effective application across diverse tasks. This study aims\nto evaluate the performance of MLLMs on the VALSE benchmark, focusing on the\nefficacy of few-shot In-Context Learning (ICL), and Chain-of-Thought (CoT)\nprompting. We conducted a comprehensive assessment of state-of-the-art MLLMs,\nvarying in model size and pretraining datasets. The experimental results reveal\nthat ICL and CoT prompting significantly boost model performance, particularly\nin tasks requiring complex reasoning and contextual understanding. Models\npretrained on captioning datasets show superior zero-shot performance, while\nthose trained on interleaved image-text data benefit from few-shot learning.\nOur findings provide valuable insights into optimizing MLLMs for better\ngrounding of language in visual contexts, highlighting the importance of the\ncomposition of pretraining data and the potential of few-shot learning\nstrategies to improve the reasoning abilities of MLLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"H0FYaHGAn3mr8SXjoJ-8i4i3lezquQOhIqG36K9Vd-E","pdfSize":"10113226"}