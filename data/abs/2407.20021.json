{"id":"2407.20021","title":"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity","authors":"Kanghyun Choi, Hye Yoon Lee, Dain Kwon, SunJong Park, Kyuyeun Kim,\n  Noseong Park, Jinho Lee","authorsParsed":[["Choi","Kanghyun",""],["Lee","Hye Yoon",""],["Kwon","Dain",""],["Park","SunJong",""],["Kim","Kyuyeun",""],["Park","Noseong",""],["Lee","Jinho",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 13:57:40 GMT"},{"version":"v2","created":"Tue, 30 Jul 2024 02:03:06 GMT"},{"version":"v3","created":"Thu, 1 Aug 2024 16:13:45 GMT"}],"updateDate":"2024-08-02","timestamp":1722261460000,"abstract":"  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we identify that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From the observation of aligned attention, we find\nthat aligning attention maps of synthetic data helps to improve the overall\nperformance of quantized ViTs. Motivated by this finding, we devise MimiQ, a\nnovel DFQ method designed for ViTs that focuses on inter-head attention\nsimilarity. First, we generate synthetic data by aligning head-wise attention\nresponses in relation to spatial query patches. Then, we apply head-wise\nstructural attention distillation to align the attention maps of the quantized\nnetwork to those of the full-precision teacher. The experimental results show\nthat the proposed method significantly outperforms baselines, setting a new\nstate-of-the-art performance for data-free ViT quantization.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"IU0a2voUg2prxQoueP6TZzlA5fX-mOnUcxH3NLQK-gQ","pdfSize":"11370350"}