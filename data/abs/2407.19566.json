{"id":"2407.19566","title":"Rouser: Robust SNN training using adaptive threshold learning","authors":"Sanaz Mahmoodi Takaghaj, Jack Sampson","authorsParsed":[["Takaghaj","Sanaz Mahmoodi",""],["Sampson","Jack",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 19:23:09 GMT"}],"updateDate":"2024-07-30","timestamp":1722194589000,"abstract":"  In Spiking Neural Networks (SNNs), learning rules are based on neuron spiking\nbehavior, that is, if and when spikes are generated due to a neuron's membrane\npotential exceeding that neuron's firing threshold, and this spike timing\nencodes vital information. However, the threshold is generally treated as a\nhyperparameter, and incorrect selection can lead to neurons that do not spike\nfor large portions of the training process, hindering the effective rate of\nlearning. Inspired by homeostatic mechanisms in biological neurons, this work\n(Rouser) presents a study to rouse training-inactive neurons and improve the\nSNN training by using an in-loop adaptive threshold learning mechanism.\nRouser's adaptive threshold allows for dynamic adjustments based on input data\nand network hyperparameters, influencing spike timing and improving training.\nThis study focuses primarily on investigating the significance of learning\nneuron thresholds alongside weights in SNNs. We evaluate the performance of\nRouser on the spatiotemporal datasets NMNIST, DVS128 and Spiking Heidelberg\nDigits (SHD), compare our results with state-of-the-art SNN training\ntechniques, and discuss the strengths and limitations of our approach. Our\nresults suggest that promoting threshold from a hyperparameter to a parameter\ncan effectively address the issue of dead neurons during training, resulting in\na more robust training algorithm that leads to improved training convergence,\nincreased test accuracy, and substantial reductions in the number of training\nepochs needed to achieve viable accuracy. Rouser achieves up to 70% lower\ntraining latency while providing up to 2% higher accuracy over state-of-the-art\nSNNs with similar network architecture on the neuromorphic datasets NMNIST,\nDVS128 and SHD.\n","subjects":["Computing Research Repository/Emerging Technologies","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"qLqBZ4nf443jOEe7d2bFJgPZrjGw7P1OaxCiCDzezpc","pdfSize":"2178663"}