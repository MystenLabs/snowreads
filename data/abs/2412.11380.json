{
  "id": "2412.11380",
  "title": "Relation-Guided Adversarial Learning for Data-free Knowledge Transfer",
  "authors": "Yingping Liang, Ying Fu",
  "authorsParsed": [
    [
      "Liang",
      "Yingping",
      ""
    ],
    [
      "Fu",
      "Ying",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 02:11:02 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734315062000,
  "abstract": "  Data-free knowledge distillation transfers knowledge by recovering training\ndata from a pre-trained model. Despite the recent success of seeking global\ndata diversity, the diversity within each class and the similarity among\ndifferent classes are largely overlooked, resulting in data homogeneity and\nlimited performance. In this paper, we introduce a novel Relation-Guided\nAdversarial Learning method with triplet losses, which solves the homogeneity\nproblem from two aspects. To be specific, our method aims to promote both\nintra-class diversity and inter-class confusion of the generated samples. To\nthis end, we design two phases, an image synthesis phase and a student training\nphase. In the image synthesis phase, we construct an optimization process to\npush away samples with the same labels and pull close samples with different\nlabels, leading to intra-class diversity and inter-class confusion,\nrespectively. Then, in the student training phase, we perform an opposite\noptimization, which adversarially attempts to reduce the distance of samples of\nthe same classes and enlarge the distance of samples of different classes. To\nmitigate the conflict of seeking high global diversity and keeping inter-class\nconfusing, we propose a focal weighted sampling strategy by selecting the\nnegative in the triplets unevenly within a finite range of distance. RGAL shows\nsignificant improvement over previous state-of-the-art methods in accuracy and\ndata efficiency. Besides, RGAL can be inserted into state-of-the-art methods on\nvarious data-free knowledge transfer applications. Experiments on various\nbenchmarks demonstrate the effectiveness and generalizability of our proposed\nmethod on various tasks, specially data-free knowledge distillation, data-free\nquantization, and non-exemplar incremental learning. Our code is available at\nhttps://github.com/Sharpiless/RGAL.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "oaIRK22IJ73uF9tT4bn58YC9HbXJSjeTRP_8wpx5u8s",
  "pdfSize": "29153872"
}