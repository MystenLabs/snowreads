{"id":"2412.18149","title":"Dense-Face: Personalized Face Generation Model via Dense Annotation\n  Prediction","authors":"Xiao Guo, Manh Tran, Jiaxin Cheng, Xiaoming Liu","authorsParsed":[["Guo","Xiao",""],["Tran","Manh",""],["Cheng","Jiaxin",""],["Liu","Xiaoming",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 04:05:21 GMT"}],"updateDate":"2024-12-25","timestamp":1735013121000,"abstract":"  The text-to-image (T2I) personalization diffusion model can generate images\nof the novel concept based on the user input text caption. However, existing\nT2I personalized methods either require test-time fine-tuning or fail to\ngenerate images that align well with the given text caption. In this work, we\npropose a new T2I personalization diffusion model, Dense-Face, which can\ngenerate face images with a consistent identity as the given reference subject\nand align well with the text caption. Specifically, we introduce a\npose-controllable adapter for the high-fidelity image generation while\nmaintaining the text-based editing ability of the pre-trained stable diffusion\n(SD). Additionally, we use internal features of the SD UNet to predict dense\nface annotations, enabling the proposed method to gain domain knowledge in face\ngeneration. Empirically, our method achieves state-of-the-art or competitive\ngeneration performance in image-text alignment, identity preservation, and pose\ncontrol.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wVpWf6fhnlNY75Obq1nBz-3AnlWUL0xWtU8OulvQYs8","pdfSize":"19428647"}