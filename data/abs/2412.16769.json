{
  "id": "2412.16769",
  "title": "Does calibration mean what they say it means; or, the reference class\n  problem rises again",
  "authors": "Lily Hu",
  "authorsParsed": [
    [
      "Hu",
      "Lily",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 20:50:31 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734814231000,
  "abstract": "  Discussions of statistical criteria for fairness commonly convey the\nnormative significance of calibration within groups by invoking what risk\nscores \"mean.\" On the Same Meaning picture, group-calibrated scores \"mean the\nsame thing\" (on average) across individuals from different groups and\naccordingly, guard against disparate treatment of individuals based on group\nmembership. My contention is that calibration guarantees no such thing. Since\nconcrete actual people belong to many groups, calibration cannot ensure the\nkind of consistent score interpretation that the Same Meaning picture implies\nmatters for fairness, unless calibration is met within every group to which an\nindividual belongs. Alas only perfect predictors may meet this bar. The Same\nMeaning picture thus commits a reference class fallacy by inferring from\ncalibration within some group to the \"meaning\" or evidential value of an\nindividual's score, because they are a member of that group. Furthermore, the\nreference class answer it presumes is almost surely wrong. I then show that the\nreference class problem besets not just calibration but all group statistical\nfacts that claim a close connection to fairness. Reflecting on the origins of\nthis error opens a wider lens onto the predominant methodology in algorithmic\nfairness based on stylized cases.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "UyqMKOtdD-imWkAxYLC5cpZOcAbb4qoYwC_eQkcxHNY",
  "pdfSize": "834414"
}