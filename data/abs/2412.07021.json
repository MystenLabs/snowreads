{"id":"2412.07021","title":"Sequential Compression Layers for Efficient Federated Learning in\n  Foundational Models","authors":"Navyansh Mahla, Sunny Gupta, Amit Sethi","authorsParsed":[["Mahla","Navyansh",""],["Gupta","Sunny",""],["Sethi","Amit",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 22:06:47 GMT"}],"updateDate":"2024-12-11","timestamp":1733782007000,"abstract":"  Federated Learning (FL) has gained popularity for fine-tuning large language\nmodels (LLMs) across multiple nodes, each with its own private data. While LoRA\nhas been widely adopted for parameter efficient federated fine-tuning, recent\ntheoretical and empirical studies highlight its suboptimal performance in the\nfederated learning context. In response, we propose a novel, simple, and more\neffective parameter-efficient fine-tuning method that does not rely on LoRA.\nOur approach introduces a small multi-layer perceptron (MLP) layer between two\nexisting MLP layers the up proj (the FFN projection layer following the\nself-attention module) and down proj within the feed forward network of the\ntransformer block. This solution addresses the bottlenecks associated with LoRA\nin federated fine tuning and outperforms recent LoRA-based approaches,\ndemonstrating superior performance for both language models and vision\nencoders.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"YhIhCXgignRzjJGbUuGzWwtw_dncjeihpsOGJ-MSbOg","pdfSize":"581590"}