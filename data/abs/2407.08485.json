{"id":"2407.08485","title":"Local logistic regression for dimension reduction in classification","authors":"Touqeer Ahmad, Fran\\c{c}ois Portier and Gilles Stupfler","authorsParsed":[["Ahmad","Touqeer",""],["Portier","Fran√ßois",""],["Stupfler","Gilles",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:19:15 GMT"}],"updateDate":"2024-07-12","timestamp":1720703955000,"abstract":"  Sufficient dimension reduction has received much interest over the past 30\nyears. Most existing approaches focus on statistical models linking the\nresponse to the covariate through a regression equation, and as such are not\nadapted to binary classification problems. We address the question of dimension\nreduction for binary classification by fitting a localized nearest-neighbor\nlogistic model with $\\ell_1$-penalty in order to estimate the gradient of the\nconditional probability of interest. Our theoretical analysis shows that the\npointwise convergence rate of the gradient estimator is optimal under very mild\nconditions. The dimension reduction subspace is estimated using an outer\nproduct of such gradient estimates at several points in the covariate space.\nOur implementation uses cross-validation on the misclassification rate to\nestimate the dimension of this subspace. We find that the proposed approach\noutperforms existing competitors in synthetic and real data applications.\n","subjects":["Mathematics/Statistics Theory","Statistics/Methodology","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NSvdmLafHYqpsxR60n2V_sJaAYZSdsOkFgni1qtR0Jg","pdfSize":"787074"}