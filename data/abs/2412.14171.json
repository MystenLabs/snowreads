{
  "id": "2412.14171",
  "title": "Thinking in Space: How Multimodal Large Language Models See, Remember,\n  and Recall Spaces",
  "authors": "Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei,\n  Saining Xie",
  "authorsParsed": [
    [
      "Yang",
      "Jihan",
      ""
    ],
    [
      "Yang",
      "Shusheng",
      ""
    ],
    [
      "Gupta",
      "Anjali W.",
      ""
    ],
    [
      "Han",
      "Rilyn",
      ""
    ],
    [
      "Fei-Fei",
      "Li",
      ""
    ],
    [
      "Xie",
      "Saining",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 18:59:54 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734548394000,
  "abstract": "  Humans possess the visual-spatial intelligence to remember spaces from\nsequential visual observations. However, can Multimodal Large Language Models\n(MLLMs) trained on million-scale video datasets also ``think in space'' from\nvideos? We present a novel video-based visual-spatial intelligence benchmark\n(VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit\ncompetitive - though subhuman - visual-spatial intelligence. We probe models to\nexpress how they think in space both linguistically and visually and find that\nwhile spatial reasoning capabilities remain the primary bottleneck for MLLMs to\nreach higher benchmark performance, local world models and spatial awareness do\nemerge within these models. Notably, prevailing linguistic reasoning techniques\n(e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve\nperformance, whereas explicitly generating cognitive maps during\nquestion-answering enhances MLLMs' spatial distance ability.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "z-sQKBpBy-WkDmiwKztcrC7tljiHsH4Zq-WB8ZgpRbc",
  "pdfSize": "10320349"
}