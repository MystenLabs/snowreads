{
  "id": "2412.07693",
  "title": "Leveraging Content and Context Cues for Low-Light Image Enhancement",
  "authors": "Igor Morawski, Kai He, Shusil Dangi and Winston H. Hsu",
  "authorsParsed": [
    [
      "Morawski",
      "Igor",
      ""
    ],
    [
      "He",
      "Kai",
      ""
    ],
    [
      "Dangi",
      "Shusil",
      ""
    ],
    [
      "Hsu",
      "Winston H.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 17:32:09 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733851929000,
  "abstract": "  Low-light conditions have an adverse impact on machine cognition, limiting\nthe performance of computer vision systems in real life. Since low-light data\nis limited and difficult to annotate, we focus on image processing to enhance\nlow-light images and improve the performance of any downstream task model,\ninstead of fine-tuning each of the models which can be prohibitively expensive.\nWe propose to improve the existing zero-reference low-light enhancement by\nleveraging the CLIP model to capture image prior and for semantic guidance.\nSpecifically, we propose a data augmentation strategy to learn an image prior\nvia prompt learning, based on image sampling, to learn the image prior without\nany need for paired or unpaired normal-light data. Next, we propose a semantic\nguidance strategy that maximally takes advantage of existing low-light\nannotation by introducing both content and context cues about the image\ntraining patches. We experimentally show, in a qualitative study, that the\nproposed prior and semantic guidance help to improve the overall image contrast\nand hue, as well as improve background-foreground discrimination, resulting in\nreduced over-saturation and noise over-amplification, common in related\nzero-reference methods. As we target machine cognition, rather than rely on\nassuming the correlation between human perception and downstream task\nperformance, we conduct and present an ablation study and comparison with\nrelated zero-reference methods in terms of task-based performance across many\nlow-light datasets, including image classification, object and face detection,\nshowing the effectiveness of our proposed method.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Electrical Engineering and Systems Science/Image and Video Processing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "OwNcsi2HX28kgcmr2RWDqJ3a9UyHblD7RKlR3A53v4Y",
  "pdfSize": "15644996"
}