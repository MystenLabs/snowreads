{"id":"2412.06534","title":"Inverting Visual Representations with Detection Transformers","authors":"Jan Rathjens, Shirin Reyhanian, David Kappel, Laurenz Wiskott","authorsParsed":[["Rathjens","Jan",""],["Reyhanian","Shirin",""],["Kappel","David",""],["Wiskott","Laurenz",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 14:43:06 GMT"}],"updateDate":"2024-12-11","timestamp":1733755386000,"abstract":"  Understanding the mechanisms underlying deep neural networks in computer\nvision remains a fundamental challenge. While many prior approaches have\nfocused on visualizing intermediate representations within deep neural\nnetworks, particularly convolutional neural networks, these techniques have yet\nto be thoroughly explored in transformer-based vision models. In this study, we\napply the approach of training inverse models to reconstruct input images from\nintermediate layers within a Detection Transformer, showing that this approach\nis efficient and feasible for transformer-based vision models. Through\nqualitative and quantitative evaluations of reconstructed images across model\nstages, we demonstrate critical properties of Detection Transformers, including\ncontextual shape preservation, inter-layer correlation, and robustness to color\nperturbations, illustrating how these characteristics emerge within the model's\narchitecture. Our findings contribute to a deeper understanding of\ntransformer-based vision models. The code for reproducing our experiments will\nbe made available at github.com/wiskott-lab/inverse-detection-transformer.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence","Computer Science/Machine Learning","Computer Science/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"km3ZpIB5WBLt9pDwMJaXgmAcNdmiReO8ml_YwEiIOWg","pdfSize":"20028007"}