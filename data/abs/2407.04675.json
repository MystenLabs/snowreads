{"id":"2407.04675","title":"Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based\n  Speech Recognition","authors":"Ye Bai, Jingping Chen, Jitong Chen, Wei Chen, Zhuo Chen, Chuang Ding,\n  Linhao Dong, Qianqian Dong, Yujiao Du, Kepan Gao, Lu Gao, Yi Guo, Minglun\n  Han, Ting Han, Wenchao Hu, Xinying Hu, Yuxiang Hu, Deyu Hua, Lu Huang,\n  Mingkun Huang, Youjia Huang, Jishuo Jin, Fanliu Kong, Zongwei Lan, Tianyu Li,\n  Xiaoyang Li, Zeyang Li, Zehua Lin, Rui Liu, Shouda Liu, Lu Lu, Yizhou Lu,\n  Jingting Ma, Shengtao Ma, Yulin Pei, Chen Shen, Tian Tan, Xiaogang Tian, Ming\n  Tu, Bo Wang, Hao Wang, Yuping Wang, Yuxuan Wang, Hanzhang Xia, Rui Xia,\n  Shuangyi Xie, Hongmin Xu, Meng Yang, Bihong Zhang, Jun Zhang, Wanyi Zhang,\n  Yang Zhang, Yawei Zhang, Yijie Zheng, Ming Zou","authorsParsed":[["Bai","Ye",""],["Chen","Jingping",""],["Chen","Jitong",""],["Chen","Wei",""],["Chen","Zhuo",""],["Ding","Chuang",""],["Dong","Linhao",""],["Dong","Qianqian",""],["Du","Yujiao",""],["Gao","Kepan",""],["Gao","Lu",""],["Guo","Yi",""],["Han","Minglun",""],["Han","Ting",""],["Hu","Wenchao",""],["Hu","Xinying",""],["Hu","Yuxiang",""],["Hua","Deyu",""],["Huang","Lu",""],["Huang","Mingkun",""],["Huang","Youjia",""],["Jin","Jishuo",""],["Kong","Fanliu",""],["Lan","Zongwei",""],["Li","Tianyu",""],["Li","Xiaoyang",""],["Li","Zeyang",""],["Lin","Zehua",""],["Liu","Rui",""],["Liu","Shouda",""],["Lu","Lu",""],["Lu","Yizhou",""],["Ma","Jingting",""],["Ma","Shengtao",""],["Pei","Yulin",""],["Shen","Chen",""],["Tan","Tian",""],["Tian","Xiaogang",""],["Tu","Ming",""],["Wang","Bo",""],["Wang","Hao",""],["Wang","Yuping",""],["Wang","Yuxuan",""],["Xia","Hanzhang",""],["Xia","Rui",""],["Xie","Shuangyi",""],["Xu","Hongmin",""],["Yang","Meng",""],["Zhang","Bihong",""],["Zhang","Jun",""],["Zhang","Wanyi",""],["Zhang","Yang",""],["Zhang","Yawei",""],["Zheng","Yijie",""],["Zou","Ming",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 17:38:03 GMT"},{"version":"v2","created":"Wed, 10 Jul 2024 09:01:17 GMT"}],"updateDate":"2024-07-11","timestamp":1720201083000,"abstract":"  Modern automatic speech recognition (ASR) model is required to accurately\ntranscribe diverse speech signals (from different domains, languages, accents,\netc) given the specific contextual information in various application\nscenarios. Classic end-to-end models fused with extra language models perform\nwell, but mainly in data matching scenarios and are gradually approaching a\nbottleneck. In this work, we introduce Seed-ASR, a large language model (LLM)\nbased speech recognition model. Seed-ASR is developed based on the framework of\naudio conditioned LLM (AcLLM), leveraging the capabilities of LLMs by inputting\ncontinuous speech representations together with contextual information into the\nLLM. Through stage-wise large-scale training and the elicitation of\ncontext-aware capabilities in LLM, Seed-ASR demonstrates significant\nimprovement over end-to-end models on comprehensive evaluation sets, including\nmultiple domains, accents/dialects and languages. Additionally, Seed-ASR can be\nfurther deployed to support specific needs in various scenarios without\nrequiring extra language models. Compared to recently released large ASR\nmodels, Seed-ASR achieves 10%-40% reduction in word (or character, for Chinese)\nerror rates on Chinese and English public test sets, further demonstrating its\npowerful performance.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"fEE7JQCn4ip7LOg8rxN2dwB1-eHv3NbtQPLbazvY4hE","pdfSize":"2174691"}
