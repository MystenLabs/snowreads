{"id":"2412.03471","title":"Cluster Specific Representation Learning","authors":"Mahalakshmi Sabanayagam and Omar Al-Dabooni and Pascal Esser","authorsParsed":[["Sabanayagam","Mahalakshmi",""],["Al-Dabooni","Omar",""],["Esser","Pascal",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 16:59:37 GMT"}],"updateDate":"2024-12-05","timestamp":1733331577000,"abstract":"  Representation learning aims to extract meaningful lower-dimensional\nembeddings from data, known as representations. Despite its widespread\napplication, there is no established definition of a ``good'' representation.\nTypically, the representation quality is evaluated based on its performance in\ndownstream tasks such as clustering, de-noising, etc. However, this\ntask-specific approach has a limitation where a representation that performs\nwell for one task may not necessarily be effective for another. This highlights\nthe need for a more agnostic formulation, which is the focus of our work. We\npropose a downstream-agnostic formulation: when inherent clusters exist in the\ndata, the representations should be specific to each cluster. Under this idea,\nwe develop a meta-algorithm that jointly learns cluster-specific\nrepresentations and cluster assignments. As our approach is easy to integrate\nwith any representation learning framework, we demonstrate its effectiveness in\nvarious setups, including Autoencoders, Variational Autoencoders, Contrastive\nlearning models, and Restricted Boltzmann Machines. We qualitatively compare\nour cluster-specific embeddings to standard embeddings and downstream tasks\nsuch as de-noising and clustering. While our method slightly increases runtime\nand parameters compared to the standard model, the experiments clearly show\nthat it extracts the inherent cluster structures in the data, resulting in\nimproved performance in relevant applications.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Q2C9YpFdHcCrT_v296UWSQQTdLw02fLOv-FKa8Tjyes","pdfSize":"3887634"}