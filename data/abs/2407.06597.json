{"id":"2407.06597","title":"TVR-Ranking: A Dataset for Ranked Video Moment Retrieval with Imprecise\n  Queries","authors":"Renjie Liang, Li Li, Chongzhi Zhang, Jing Wang, Xizhou Zhu, Aixin Sun","authorsParsed":[["Liang","Renjie",""],["Li","Li",""],["Zhang","Chongzhi",""],["Wang","Jing",""],["Zhu","Xizhou",""],["Sun","Aixin",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 06:57:30 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 03:54:53 GMT"}],"updateDate":"2024-07-25","timestamp":1720508250000,"abstract":"  In this paper, we propose the task of \\textit{Ranked Video Moment Retrieval}\n(RVMR) to locate a ranked list of matching moments from a collection of videos,\nthrough queries in natural language. Although a few related tasks have been\nproposed and studied by CV, NLP, and IR communities, RVMR is the task that best\nreflects the practical setting of moment search. To facilitate research in\nRVMR, we develop the TVR-Ranking dataset, based on the raw videos and existing\nmoment annotations provided in the TVR dataset. Our key contribution is the\nmanual annotation of relevance levels for 94,442 query-moment pairs. We then\ndevelop the $NDCG@K, IoU\\geq \\mu$ evaluation metric for this new task and\nconduct experiments to evaluate three baseline models. Our experiments show\nthat the new RVMR task brings new challenges to existing models and we believe\nthis new dataset contributes to the research on multi-modality search. The\ndataset is available at \\url{https://github.com/Ranking-VMR/TVR-Ranking}\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"scx1aNp-avs92q-c07Z1doS6N-iVFgU4-dOiLhU-ZXo","pdfSize":"9493361"}
