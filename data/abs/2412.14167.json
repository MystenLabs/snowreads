{"id":"2412.14167","title":"VideoDPO: Omni-Preference Alignment for Video Diffusion Generation","authors":"Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi,\n  Qifeng Chen","authorsParsed":[["Liu","Runtao",""],["Wu","Haoyu",""],["Ziqiang","Zheng",""],["Wei","Chen",""],["He","Yingqing",""],["Pi","Renjie",""],["Chen","Qifeng",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 18:59:49 GMT"}],"updateDate":"2024-12-19","timestamp":1734548389000,"abstract":"  Recent progress in generative diffusion models has greatly advanced\ntext-to-video generation. While text-to-video models trained on large-scale,\ndiverse datasets can produce varied outputs, these generations often deviate\nfrom user preferences, highlighting the need for preference alignment on\npre-trained models. Although Direct Preference Optimization (DPO) has\ndemonstrated significant improvements in language and image generation, we\npioneer its adaptation to video diffusion models and propose a VideoDPO\npipeline by making several key adjustments. Unlike previous image alignment\nmethods that focus solely on either (i) visual quality or (ii) semantic\nalignment between text and videos, we comprehensively consider both dimensions\nand construct a preference score accordingly, which we term the OmniScore. We\ndesign a pipeline to automatically collect preference pair data based on the\nproposed OmniScore and discover that re-weighting these pairs based on the\nscore significantly impacts overall preference alignment. Our experiments\ndemonstrate substantial improvements in both visual quality and semantic\nalignment, ensuring that no preference aspect is neglected. Code and data will\nbe shared at https://videodpo.github.io/.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dvZsy5WvPftnKklIfrijSwLwp9_DQYwjMSWt0RAQNso","pdfSize":"24078927"}