{"id":"2412.17397","title":"Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search\n  Boosted Reasoning via Iterative Preference Learning","authors":"Huchen Jiang, Yangyang Ma, Chaofan Ding, Kexin Luan and Xinhan Di","authorsParsed":[["Jiang","Huchen",""],["Ma","Yangyang",""],["Ding","Chaofan",""],["Luan","Kexin",""],["Di","Xinhan",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 08:51:48 GMT"}],"updateDate":"2024-12-24","timestamp":1734943908000,"abstract":"  With current state-of-the-art approaches aimed at enhancing the reasoning\ncapabilities of Large Language Models(LLMs) through iterative preference\nlearning inspired by AlphaZero, we propose to further enhance the step-wise\nreasoning capabilities through intrinsic self-correction to some extent. Our\nwork leverages step-wise preference learning to enhance self-verification via\nreinforcement learning. We initially conduct our work through a two-stage\ntraining procedure. At the first stage, the self-correction reasoning ability\nof an LLM is enhanced through its own predictions, relying entirely on\nself-generated data within the intrinsic self-correction to some extent. At the\nsecond stage, the baseline step-wise preference learning is leveraged via the\napplication of the enhanced self-correct policy achieved at the first stage. In\nthe evaluation of arithmetic reasoning tasks, our approach outperforms\nOpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in\naccuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct,\nMistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%)\nand 38.06%(+2.28%).\n","subjects":["Computer Science/Machine Learning","Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"WY67nlR3kSW1jxgzPqr3AyyhC2HhpHQG4VJSozYgZu0","pdfSize":"796677"}