{
  "id": "2412.11231",
  "title": "Smaller Language Models Are Better Instruction Evolvers",
  "authors": "Tingfeng Hui and Lulu Zhao and Guanting Dong and Yaqi Zhang and Hua\n  Zhou and Sen Su",
  "authorsParsed": [
    [
      "Hui",
      "Tingfeng",
      ""
    ],
    [
      "Zhao",
      "Lulu",
      ""
    ],
    [
      "Dong",
      "Guanting",
      ""
    ],
    [
      "Zhang",
      "Yaqi",
      ""
    ],
    [
      "Zhou",
      "Hua",
      ""
    ],
    [
      "Su",
      "Sen",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 15 Dec 2024 16:07:48 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734278868000,
  "abstract": "  Instruction tuning has been widely used to unleash the complete potential of\nlarge language models. Notably, complex and diverse instructions are of\nsignificant importance as they can effectively align models with various\ndownstream tasks. However, current approaches to constructing large-scale\ninstructions predominantly favour powerful models such as GPT-4 or those with\nover 70 billion parameters, under the empirical presumption that such larger\nlanguage models (LLMs) inherently possess enhanced capabilities. In this study,\nwe question this prevalent assumption and conduct an in-depth exploration into\nthe potential of smaller language models (SLMs) in the context of instruction\nevolution. Extensive experiments across three scenarios of instruction\nevolution reveal that smaller language models (SLMs) can synthesize more\neffective instructions than LLMs. Further analysis demonstrates that SLMs\npossess a broader output space during instruction evolution, resulting in more\ncomplex and diverse variants. We also observe that the existing metrics fail to\nfocus on the impact of the instructions. Thus, we propose Instruction\nComplex-Aware IFD (IC-IFD), which introduces instruction complexity in the\noriginal IFD score to evaluate the effectiveness of instruction data more\naccurately. Our source code is available at:\n\\href{https://github.com/HypherX/Evolution-Analysis}{https://github.com/HypherX/Evolution-Analysis}\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "lq76FliI1dwt1U2GXntICvP3bSKu8-pMjyXfnlUUqBE",
  "pdfSize": "1387682"
}