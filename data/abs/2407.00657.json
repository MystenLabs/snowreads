{"id":"2407.00657","title":"Improving Real-Time Music Accompaniment Separation with MMDenseNet","authors":"Chun-Hsiang Wang, Chung-Che Wang, Jun-You Wang, Jyh-Shing Roger Jang,\n  Yen-Hsun Chu","authorsParsed":[["Wang","Chun-Hsiang",""],["Wang","Chung-Che",""],["Wang","Jun-You",""],["Jang","Jyh-Shing Roger",""],["Chu","Yen-Hsun",""]],"versions":[{"version":"v1","created":"Sun, 30 Jun 2024 11:00:09 GMT"}],"updateDate":"2024-07-02","timestamp":1719745209000,"abstract":"  Music source separation aims to separate polyphonic music into different\ntypes of sources. Most existing methods focus on enhancing the quality of\nseparated results by using a larger model structure, rendering them unsuitable\nfor deployment on edge devices. Moreover, these methods may produce low-quality\noutput when the input duration is short, making them impractical for real-time\napplications. Therefore, the goal of this paper is to enhance a lightweight\nmodel, MMDenstNet, to strike a balance between separation quality and latency\nfor real-time applications. Different directions of improvement are explored or\nproposed in this paper, including complex ideal ratio mask, self-attention,\nband-merge-split method, and feature look back. Source-to-distortion ratio,\nreal-time factor, and optimal latency are employed to evaluate the performance.\nTo align with our application requirements, the evaluation process in this\npaper focuses on the separation performance of the accompaniment part.\nExperimental results demonstrate that our improvement achieves low real-time\nfactor and optimal latency while maintaining acceptable separation quality.\n","subjects":["Computing Research Repository/Sound","Computing Research Repository/Machine Learning","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"iMXxI28Skpo_-jnejQ5w8QpkWY26B9f7SDQ1C_YBdic","pdfSize":"540481"}