{"id":"2407.02552","title":"RLHF Can Speak Many Languages: Unlocking Multilingual Preference\n  Optimization for LLMs","authors":"John Dang, Arash Ahmadian, Kelly Marchisio, Julia Kreutzer, Ahmet\n  \\\"Ust\\\"un, Sara Hooker","authorsParsed":[["Dang","John",""],["Ahmadian","Arash",""],["Marchisio","Kelly",""],["Kreutzer","Julia",""],["Üstün","Ahmet",""],["Hooker","Sara",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 17:42:30 GMT"}],"updateDate":"2024-07-04","timestamp":1719942150000,"abstract":"  Preference optimization techniques have become a standard final stage for\ntraining state-of-art large language models (LLMs). However, despite widespread\nadoption, the vast majority of work to-date has focused on first-class citizen\nlanguages like English and Chinese. This captures a small fraction of the\nlanguages in the world, but also makes it unclear which aspects of current\nstate-of-the-art research transfer to a multilingual setting. In this work, we\nperform an exhaustive study to achieve a new state-of-the-art in aligning\nmultilingual LLMs. We introduce a novel, scalable method for generating\nhigh-quality multilingual feedback data to balance data coverage. We establish\nthe benefits of cross-lingual transfer and increased dataset size in preference\ntraining. Our preference-trained model achieves a 54.4% win-rate against Aya 23\n8B, the current state-of-the-art multilingual LLM in its parameter class, and a\n69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it,\nLlama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we\nexpand the frontier of alignment techniques to 23 languages covering half of\nthe world's population.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Rajvt20iLM4MtCkXy0_r5a4sHSyIT44_El6xbSq0oro","pdfSize":"986679"}