{"id":"2407.04153","title":"Mixture of A Million Experts","authors":"Xu Owen He","authorsParsed":[["He","Xu Owen",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 20:59:20 GMT"}],"updateDate":"2024-07-08","timestamp":1720126760000,"abstract":"  The feedforward (FFW) layers in standard transformer architectures incur a\nlinear increase in computational costs and activation memory as the hidden\nlayer width grows. Sparse mixture-of-experts (MoE) architectures have emerged\nas a viable approach to address this issue by decoupling model size from\ncomputational cost. The recent discovery of the fine-grained MoE scaling law\nshows that higher granularity leads to better performance. However, existing\nMoE models are limited to a small number of experts due to computational and\noptimization challenges. This paper introduces PEER (parameter efficient expert\nretrieval), a novel layer design that utilizes the product key technique for\nsparse retrieval from a vast pool of tiny experts (over a million). Experiments\non language modeling tasks demonstrate that PEER layers outperform dense FFWs\nand coarse-grained MoEs in terms of performance-compute trade-off. By enabling\nefficient utilization of a massive number of experts, PEER unlocks the\npotential for further scaling of transformer models while maintaining\ncomputational efficiency.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0JvZFGUWVCUYRO2NE2ZbCAStV6Mb1DdhuHS43XMhR1w","pdfSize":"935664"}