{"id":"2407.05781","title":"Regret Analysis of Multi-task Representation Learning for\n  Linear-Quadratic Adaptive Control","authors":"Bruce D. Lee, Leonardo F. Toso, Thomas T. Zhang, James Anderson,\n  Nikolai Matni","authorsParsed":[["Lee","Bruce D.",""],["Toso","Leonardo F.",""],["Zhang","Thomas T.",""],["Anderson","James",""],["Matni","Nikolai",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 09:41:42 GMT"},{"version":"v2","created":"Sat, 27 Jul 2024 13:20:35 GMT"}],"updateDate":"2024-07-30","timestamp":1720431702000,"abstract":"  Representation learning is a powerful tool that enables learning over large\nmultitudes of agents or domains by enforcing that all agents operate on a\nshared set of learned features. However, many robotics or controls applications\nthat would benefit from collaboration operate in settings with changing\nenvironments and goals, whereas most guarantees for representation learning are\nstated for static settings. Toward rigorously establishing the benefit of\nrepresentation learning in dynamic settings, we analyze the regret of\nmulti-task representation learning for linear-quadratic control. This setting\nintroduces unique challenges. Firstly, we must account for and balance the\n$\\textit{misspecification}$ introduced by an approximate representation.\nSecondly, we cannot rely on the parameter update schemes of single-task online\nLQR, for which least-squares often suffices, and must devise a novel scheme to\nensure sufficient improvement. We demonstrate that for settings where\nexploration is \"benign\", the regret of any agent after $T$ timesteps scales as\n$\\tilde O(\\sqrt{T/H})$, where $H$ is the number of agents. In settings with\n\"difficult\" exploration, the regret scales as $\\tilde O(\\sqrt{d_u d_\\theta}\n\\sqrt{T} + T^{3/4}/H^{1/5})$, where $d_x$ is the state-space dimension, $d_u$\nis the input dimension, and $d_\\theta$ is the task-specific parameter count. In\nboth cases, by comparing to the minimax single-task regret $O(\\sqrt{d_x\nd_u^2}\\sqrt{T})$, we see a benefit of a large number of agents. Notably, in the\ndifficult exploration case, by sharing a representation across tasks, the\neffective task-specific parameter count can often be small $d_\\theta < d_x\nd_u$. Lastly, we provide numerical validation of the trends we predict.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Systems and Control","Electrical Engineering and Systems Science/Systems and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2LafB3JLforYa52249xd9iys5ZHCmP8IPIjYZswtIZg","pdfSize":"734094"}
