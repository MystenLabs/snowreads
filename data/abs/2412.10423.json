{
  "id": "2412.10423",
  "title": "Look Before You Leap: Enhancing Attention and Vigilance Regarding\n  Harmful Content with GuidelineLLM",
  "authors": "Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Rongxiang Weng, Muyun\n  Yang, Tiejun Zhao, Min Zhang",
  "authorsParsed": [
    [
      "Zhang",
      "Shaoqing",
      ""
    ],
    [
      "Zhang",
      "Zhuosheng",
      ""
    ],
    [
      "Chen",
      "Kehai",
      ""
    ],
    [
      "Weng",
      "Rongxiang",
      ""
    ],
    [
      "Yang",
      "Muyun",
      ""
    ],
    [
      "Zhao",
      "Tiejun",
      ""
    ],
    [
      "Zhang",
      "Min",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 12:42:33 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1733834553000,
  "abstract": "  Despite being empowered with alignment mechanisms, large language models\n(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can\ncompromise their alignment mechanisms. This vulnerability poses significant\nrisks to the real-world applications. Existing work faces challenges in both\ntraining efficiency and generalization capabilities (i.e., Reinforcement\nLearning from Human Feedback and Red-Teaming). Developing effective strategies\nto enable LLMs to resist continuously evolving jailbreak attempts represents a\nsignificant challenge. To address this challenge, we propose a novel defensive\nparadigm called GuidelineLLM, which assists LLMs in recognizing queries that\nmay have harmful content. Before LLMs respond to a query, GuidelineLLM first\nidentifies potential risks associated with the query, summarizes these risks\ninto guideline suggestions, and then feeds these guidelines to the responding\nLLMs. Importantly, our approach eliminates the necessity for additional safety\nfine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.\nThis characteristic enhances the general applicability of GuidelineLLM across\nvarious LLMs. Experimental results demonstrate that GuidelineLLM can\nsignificantly reduce the attack success rate (ASR) against the LLMs (an average\nreduction of 34.17\\% ASR) while maintaining the helpfulness of the LLMs in\nhandling benign queries. Code is available at\nhttps://github.com/sqzhang-lazy/GuidelineLLM.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "WdMee7YhfAHkH-J9CpQ_REWnhYFAgZ8qrafceOU51ZQ",
  "pdfSize": "500308"
}