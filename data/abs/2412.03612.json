{
  "id": "2412.03612",
  "title": "Chatting with Logs: An exploratory study on Finetuning LLMs for LogQL",
  "authors": "Vishwanath Seshagiri, Siddharth Balyan, Vaastav Anand, Kaustubh Dhole,\n  Ishan Sharma, Avani Wildani, Jos\\'e Cambronero, Andreas Z\\\"ufle",
  "authorsParsed": [
    [
      "Seshagiri",
      "Vishwanath",
      ""
    ],
    [
      "Balyan",
      "Siddharth",
      ""
    ],
    [
      "Anand",
      "Vaastav",
      ""
    ],
    [
      "Dhole",
      "Kaustubh",
      ""
    ],
    [
      "Sharma",
      "Ishan",
      ""
    ],
    [
      "Wildani",
      "Avani",
      ""
    ],
    [
      "Cambronero",
      "José",
      ""
    ],
    [
      "Züfle",
      "Andreas",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 14:06:24 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733321184000,
  "abstract": "  Logging is a critical function in modern distributed applications, but the\nlack of standardization in log query languages and formats creates significant\nchallenges. Developers currently must write ad hoc queries in platform-specific\nlanguages, requiring expertise in both the query language and\napplication-specific log details -- an impractical expectation given the\nvariety of platforms and volume of logs and applications. While generating\nthese queries with large language models (LLMs) seems intuitive, we show that\ncurrent LLMs struggle with log-specific query generation due to the lack of\nexposure to domain-specific knowledge. We propose a novel natural language (NL)\ninterface to address these inconsistencies and aide log query generation,\nenabling developers to create queries in a target log query language by\nproviding NL inputs. We further introduce ~\\textbf{NL2QL}, a manually\nannotated, real-world dataset of natural language questions paired with\ncorresponding LogQL queries spread across three log formats, to promote the\ntraining and evaluation of NL-to-loq query systems. Using NL2QL, we\nsubsequently fine-tune and evaluate several state of the art LLMs, and\ndemonstrate their improved capability to generate accurate LogQL queries. We\nperform further ablation studies to demonstrate the effect of additional\ntraining data, and the transferability across different log formats. In our\nexperiments, we find up to 75\\% improvement of finetuned models to generate\nLogQL queries compared to non finetuned models.\n",
  "subjects": [
    "Computer Science/Databases",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Programming Languages"
  ],
  "license": "http://creativecommons.org/publicdomain/zero/1.0/",
  "blobId": "9ABzL_QYZEzayuTIX2YNxjUgyz81g7Q-ZHQbST9pVsM",
  "pdfSize": "1021493"
}