{
  "id": "2412.12527",
  "title": "When to Speak, When to Abstain: Contrastive Decoding with Abstention",
  "authors": "Hyuhng Joon Kim, Youna Kim, Sang-goo Lee, Taeuk Kim",
  "authorsParsed": [
    [
      "Kim",
      "Hyuhng Joon",
      ""
    ],
    [
      "Kim",
      "Youna",
      ""
    ],
    [
      "Lee",
      "Sang-goo",
      ""
    ],
    [
      "Kim",
      "Taeuk",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 04:38:08 GMT"
    },
    {
      "version": "v2",
      "created": "Sun, 16 Feb 2025 14:09:46 GMT"
    }
  ],
  "updateDate": "2025-02-18",
  "timestamp": 1734410288000,
  "abstract": "  Large Language Models (LLMs) demonstrate exceptional performance across\ndiverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e.,\ncontextual) knowledge. While substantial efforts have been made to enhance the\nutilization of both forms of knowledge, situations in which models lack\nrelevant information remain underexplored. To investigate this challenge, we\nfirst present a controlled testbed featuring four distinct knowledge access\nscenarios, including the aforementioned edge case, revealing that conventional\nLLM usage exhibits insufficient robustness in handling all instances.\nAddressing this limitation, we propose Contrastive Decoding with Abstention\n(CDA), a novel training-free decoding method that allows LLMs to generate\nresponses when relevant knowledge is available and to abstain otherwise. CDA\nestimates the relevance of both knowledge sources for a given input, adaptively\ndeciding which type of information to prioritize and which to exclude. Through\nextensive experiments, we demonstrate that CDA can effectively perform accurate\ngeneration and abstention simultaneously, enhancing reliability and preserving\nuser trust.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "gj6P7WLWUilcjO820C8zkBAghN4vZAj02hA-8jZQuBs",
  "pdfSize": "1637490"
}