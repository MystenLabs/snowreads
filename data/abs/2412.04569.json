{"id":"2412.04569","title":"Towards Performance-Aware Allocation for Accelerated Machine Learning on\n  GPU-SSD Systems","authors":"Ayush Gundawar and Euijun Chung and Hyesoon Kim","authorsParsed":[["Gundawar","Ayush",""],["Chung","Euijun",""],["Kim","Hyesoon",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 19:26:30 GMT"},{"version":"v2","created":"Mon, 9 Dec 2024 02:17:00 GMT"}],"updateDate":"2024-12-10","timestamp":1733426790000,"abstract":"  The exponential growth of data-intensive machine learning workloads has\nexposed significant limitations in conventional GPU-accelerated systems,\nespecially when processing datasets exceeding GPU DRAM capacity. We propose\nMQMS, an augmented in-storage GPU architecture and simulator that is aware of\ninternal SSD states and operations, enabling intelligent scheduling and address\nallocation to overcome performance bottlenecks caused by CPU-mediated data\naccess patterns. MQMS introduces dynamic address allocation to maximize\ninternal parallelism and fine-grained address mapping to efficiently handle\nsmall I/O requests without incurring read-modify-write overheads. Through\nextensive evaluations on workloads ranging from large language model inference\nto classical machine learning algorithms, MQMS demonstrates orders-of-magnitude\nimprovements in I/O request throughput, device response time, and simulation\nend time compared to existing simulators.\n","subjects":["Computer Science/Hardware Architecture"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TM-OgFoUEgTaJ8gonnRbEZ1-yoh8fbwop_nHAd48jSw","pdfSize":"1372339"}