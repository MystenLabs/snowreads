{
  "id": "2412.08842",
  "title": "Kajal: Extracting Grammar of a Source Code Using Large Language Models",
  "authors": "Mohammad Jalili Torkamani",
  "authorsParsed": [
    [
      "Torkamani",
      "Mohammad Jalili",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 12 Dec 2024 00:40:54 GMT"
    }
  ],
  "updateDate": "2024-12-13",
  "timestamp": 1733964054000,
  "abstract": "  Understanding and extracting the grammar of a domain-specific language (DSL)\nis crucial for various software engineering tasks; however, manually creating\nthese grammars is time-intensive and error-prone. This paper presents Kajal, a\nnovel approach that automatically infers grammar from DSL code snippets by\nleveraging Large Language Models (LLMs) through prompt engineering and few-shot\nlearning. Kajal dynamically constructs input prompts, using contextual\ninformation to guide the LLM in generating the corresponding grammars, which\nare iteratively refined through a feedback-driven approach. Our experiments\nshow that Kajal achieves 60% accuracy with few-shot learning and 45% without\nit, demonstrating the significant impact of few-shot learning on the tool's\neffectiveness. This approach offers a promising solution for automating DSL\ngrammar extraction, and future work will explore using smaller, open-source\nLLMs and testing on larger datasets to further validate Kajal's performance.\n",
  "subjects": [
    "Computer Science/Software Engineering",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "46EGbPUcCG3o-X75WYepQVUcGT9mmKp__8qtLG2JshY",
  "pdfSize": "830826"
}