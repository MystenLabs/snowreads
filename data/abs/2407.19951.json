{"id":"2407.19951","title":"Can I trust my anomaly detection system? A case study based on\n  explainable AI","authors":"Muhammad Rashid, Elvio Amparore, Enrico Ferrari, Damiano Verda","authorsParsed":[["Rashid","Muhammad",""],["Amparore","Elvio",""],["Ferrari","Enrico",""],["Verda","Damiano",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 12:39:07 GMT"}],"updateDate":"2024-07-30","timestamp":1722256747000,"abstract":"  Generative models based on variational autoencoders are a popular technique\nfor detecting anomalies in images in a semi-supervised context. A common\napproach employs the anomaly score to detect the presence of anomalies, and it\nis known to reach high level of accuracy on benchmark datasets. However, since\nanomaly scores are computed from reconstruction disparities, they often obscure\nthe detection of various spurious features, raising concerns regarding their\nactual efficacy. This case study explores the robustness of an anomaly\ndetection system based on variational autoencoder generative models through the\nuse of eXplainable AI methods. The goal is to get a different perspective on\nthe real performances of anomaly detectors that use reconstruction differences.\nIn our case study we discovered that, in many cases, samples are detected as\nanomalous for the wrong or misleading factors.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"72c1_R6sLYvBt_zKk9bK6FYY1ObAnxMkM_ktatym2Eo","pdfSize":"4307963"}