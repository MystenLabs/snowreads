{"id":"2407.10490","title":"Learning Dynamics of LLM Finetuning","authors":"Yi Ren, Danica J. Sutherland","authorsParsed":[["Ren","Yi",""],["Sutherland","Danica J.",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 07:30:28 GMT"}],"updateDate":"2024-07-16","timestamp":1721028628000,"abstract":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's prediction of other examples, give us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during finetuning, by analyzing\nthe step-wise decomposition and accumulated influence among different\nresponses. Our framework allows a uniform interpretation of many interesting\nobservations about the training of popular algorithms for both instruction\ntuning and preference tuning. The analysis not only explains where the benefits\nof these methods come from but also inspires a simple, effective method to\nfurther improve the alignment performance. Code for experiments is available at\nhttps://github.com/Joshua-Ren/Learning_dynamics_LLM.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"8iSkyciUrtxeYfIbNHI5kSC14KwERjk7iYphUCfeHGw","pdfSize":"5336424","objectId":"0x13a61a66534a88fe98d8099c0820027f9020ccd3bff799e182145d734dcb4083","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
