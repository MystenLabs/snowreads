{"id":"2412.12417","title":"Bridging the Gap: Enhancing LLM Performance for Low-Resource African\n  Languages with New Benchmarks, Fine-Tuning, and Cultural Adjustments","authors":"Tuka Alhanai, Adam Kasumovic, Mohammad Ghassemi, Aven Zitzelberger,\n  Jessica Lundin, Guillaume Chabot-Couture","authorsParsed":[["Alhanai","Tuka",""],["Kasumovic","Adam",""],["Ghassemi","Mohammad",""],["Zitzelberger","Aven",""],["Lundin","Jessica",""],["Chabot-Couture","Guillaume",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 23:50:21 GMT"}],"updateDate":"2024-12-18","timestamp":1734393021000,"abstract":"  Large Language Models (LLMs) have shown remarkable performance across various\ntasks, yet significant disparities remain for non-English languages, and\nespecially native African languages. This paper addresses these disparities by\ncreating approximately 1 million human-translated words of new benchmark data\nin 8 low-resource African languages, covering a population of over 160 million\nspeakers of: Amharic, Bambara, Igbo, Sepedi (Northern Sotho), Shona, Sesotho\n(Southern Sotho), Setswana, and Tsonga. Our benchmarks are translations of\nWinogrande and three sections of MMLU: college medicine, clinical knowledge,\nand virology. Using the translated benchmarks, we report previously unknown\nperformance gaps between state-of-the-art (SOTA) LLMs in English and African\nlanguages. Finally, using results from over 400 fine-tuned models, we explore\nseveral methods to reduce the LLM performance gap, including high-quality\ndataset fine-tuning (using an LLM-as-an-Annotator), cross-lingual transfer, and\ncultural appropriateness adjustments. Key findings include average mono-lingual\nimprovements of 5.6% with fine-tuning (with 5.4% average mono-lingual\nimprovements when using high-quality data over low-quality data), 2.9% average\ngains from cross-lingual transfer, and a 3.0% out-of-the-box performance boost\non culturally appropriate questions. The publicly available benchmarks,\ntranslations, and code from this study support further research and development\naimed at creating more inclusive and effective language technologies.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DNy6vDocKa7GDmMQ4qGcbscz3idt0DMHxx_4PQ0snjc","pdfSize":"1574652"}