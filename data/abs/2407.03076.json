{"id":"2407.03076","title":"A Case Study on Context-Aware Neural Machine Translation with Multi-Task\n  Learning","authors":"Ramakrishna Appicharla, Baban Gain, Santanu Pal, Asif Ekbal, Pushpak\n  Bhattacharyya","authorsParsed":[["Appicharla","Ramakrishna",""],["Gain","Baban",""],["Pal","Santanu",""],["Ekbal","Asif",""],["Bhattacharyya","Pushpak",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 12:50:49 GMT"}],"updateDate":"2024-07-04","timestamp":1720011049000,"abstract":"  In document-level neural machine translation (DocNMT), multi-encoder\napproaches are common in encoding context and source sentences. Recent studies\n\\cite{li-etal-2020-multi-encoder} have shown that the context encoder generates\nnoise and makes the model robust to the choice of context. This paper further\ninvestigates this observation by explicitly modelling context encoding through\nmulti-task learning (MTL) to make the model sensitive to the choice of context.\nWe conduct experiments on cascade MTL architecture, which consists of one\nencoder and two decoders. Generation of the source from the context is\nconsidered an auxiliary task, and generation of the target from the source is\nthe main task. We experimented with German--English language pairs on News,\nTED, and Europarl corpora. Evaluation results show that the proposed MTL\napproach performs better than concatenation-based and multi-encoder DocNMT\nmodels in low-resource settings and is sensitive to the choice of context.\nHowever, we observe that the MTL models are failing to generate the source from\nthe context. These observations align with the previous studies, and this might\nsuggest that the available document-level parallel corpora are not\ncontext-aware, and a robust sentence-level model can outperform the\ncontext-aware models.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"hRBSALL_ZKfIn6A7a6KdF_lifC6NUItBPGwcX4C4p2w","pdfSize":"421949"}