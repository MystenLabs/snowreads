{"id":"2407.15195","title":"Exact Convergence rate of the subgradient method by using Polyak step\n  size","authors":"Moslem Zamani, Fran\\c{c}ois Glineur","authorsParsed":[["Zamani","Moslem",""],["Glineur","Fran√ßois",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 15:22:55 GMT"}],"updateDate":"2024-07-23","timestamp":1721575375000,"abstract":"  This paper studies the last iterate of subgradient method with Polyak step\nsize when applied to the minimization of a nonsmooth convex function with\nbounded subgradients. We show that the subgradient method with Polyak step size\nachieves a convergence rate $\\mathcal{O}\\left(\\tfrac{1}{\\sqrt[4]{N}}\\right)$ in\nterms of the final iterate. An example is provided to show that this rate is\nexact and cannot be improved. We introduce an adaptive Polyak step size for\nwhich the subgradient method enjoys a convergence rate\n$\\mathcal{O}\\left(\\tfrac{1}{\\sqrt{N}}\\right)$ for the last iterate. Its\nconvergence rate matches exactly the lower bound on the performance of any\nblack-box method on the considered problem class. Additionally, we propose an\nadaptive Polyak method with a momentum term, where the step sizes are\nindependent of the number of iterates. We establish that the algorithm also\nattains the optimal convergence rate. We investigate the alternating projection\nmethod. We derive a convergence rate $\\left( \\frac{2N }{ 2N+1 }\n\\right)^N\\tfrac{R}{\\sqrt{2N+1}}$ for the last iterate, where $R$ is a bound on\nthe distance between the initial iterate and a solution. An example is also\nprovided to illustrate the exactness of the rate.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"APsj3vC6Uu0kceyz4ASA-1jGNVvqQKIeVDfhE8WdpyU","pdfSize":"244070","objectId":"0x87d9116bd520d30799ff79f989e60cf63cbd61b793daa3a6c24581ae07f1d745","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
