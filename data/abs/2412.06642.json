{
  "id": "2412.06642",
  "title": "Class Balance Matters to Active Class-Incremental Learning",
  "authors": "Zitong Huang, Ze Chen, Yuanze Li, Bowen Dong, Erjin Zhou, Yong Liu,\n  Rick Siow Mong Goh, Chun-Mei Feng, Wangmeng Zuo",
  "authorsParsed": [
    [
      "Huang",
      "Zitong",
      ""
    ],
    [
      "Chen",
      "Ze",
      ""
    ],
    [
      "Li",
      "Yuanze",
      ""
    ],
    [
      "Dong",
      "Bowen",
      ""
    ],
    [
      "Zhou",
      "Erjin",
      ""
    ],
    [
      "Liu",
      "Yong",
      ""
    ],
    [
      "Goh",
      "Rick Siow Mong",
      ""
    ],
    [
      "Feng",
      "Chun-Mei",
      ""
    ],
    [
      "Zuo",
      "Wangmeng",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 16:37:27 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733762247000,
  "abstract": "  Few-Shot Class-Incremental Learning has shown remarkable efficacy in\nefficient learning new concepts with limited annotations. Nevertheless, the\nheuristic few-shot annotations may not always cover the most informative\nsamples, which largely restricts the capability of incremental learner. We aim\nto start from a pool of large-scale unlabeled data and then annotate the most\ninformative samples for incremental learning. Based on this premise, this paper\nintroduces the Active Class-Incremental Learning (ACIL). The objective of ACIL\nis to select the most informative samples from the unlabeled pool to\neffectively train an incremental learner, aiming to maximize the performance of\nthe resulting model. Note that vanilla active learning algorithms suffer from\nclass-imbalanced distribution among annotated samples, which restricts the\nability of incremental learning. To achieve both class balance and\ninformativeness in chosen samples, we propose Class-Balanced Selection (CBS)\nstrategy. Specifically, we first cluster the features of all unlabeled images\ninto multiple groups. Then for each cluster, we employ greedy selection\nstrategy to ensure that the Gaussian distribution of the sampled features\nclosely matches the Gaussian distribution of all unlabeled features within the\ncluster. Our CBS can be plugged and played into those CIL methods which are\nbased on pretrained models with prompts tunning technique. Extensive\nexperiments under ACIL protocol across five diverse datasets demonstrate that\nCBS outperforms both random selection and other SOTA active learning\napproaches. Code is publicly available at https://github.com/1170300714/CBS.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "mbO2MziDzXd55IPM67hNFImMS9L78G9TWnwo1ONSj1I",
  "pdfSize": "6129544"
}