{
  "id": "2412.03035",
  "title": "A Granger-Causal Perspective on Gradient Descent with Application to\n  Pruning",
  "authors": "Aditya Shah, Aditya Challa, Sravan Danda, Archana Mathur, Snehanshu\n  Saha",
  "authorsParsed": [
    [
      "Shah",
      "Aditya",
      ""
    ],
    [
      "Challa",
      "Aditya",
      ""
    ],
    [
      "Danda",
      "Sravan",
      ""
    ],
    [
      "Mathur",
      "Archana",
      ""
    ],
    [
      "Saha",
      "Snehanshu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 05:16:48 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733289408000,
  "abstract": "  Stochastic Gradient Descent (SGD) is the main approach to optimizing neural\nnetworks. Several generalization properties of deep networks, such as\nconvergence to a flatter minima, are believed to arise from SGD. This article\nexplores the causality aspect of gradient descent. Specifically, we show that\nthe gradient descent procedure has an implicit granger-causal relationship\nbetween the reduction in loss and a change in parameters. By suitable\nmodifications, we make this causal relationship explicit. A causal approach to\ngradient descent has many significant applications which allow greater control.\nIn this article, we illustrate the significance of the causal approach using\nthe application of Pruning. The causal approach to pruning has several\ninteresting properties - (i) We observe a phase shift as the percentage of\npruned parameters increase. Such phase shift is indicative of an optimal\npruning strategy. (ii) After pruning, we see that minima becomes \"flatter\",\nexplaining the increase in accuracy after pruning weights.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "hV1WTnK4jm3A6WvsqwxAg2jP6Ct7_qbgsg7L5RYG4N4",
  "pdfSize": "1005099"
}