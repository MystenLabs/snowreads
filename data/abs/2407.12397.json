{"id":"2407.12397","title":"Mamba-PTQ: Outlier Channels in Recurrent Large Language Models","authors":"Alessandro Pierro, Steven Abreu","authorsParsed":[["Pierro","Alessandro",""],["Abreu","Steven",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 08:21:06 GMT"}],"updateDate":"2024-07-18","timestamp":1721204466000,"abstract":"  Modern recurrent layers are emerging as a promising path toward edge\ndeployment of foundation models, especially in the context of large language\nmodels (LLMs). Compressing the whole input sequence in a finite-dimensional\nrepresentation enables recurrent layers to model long-range dependencies while\nmaintaining a constant inference cost for each token and a fixed memory\nrequirement. However, the practical deployment of LLMs in resource-limited\nenvironments often requires further model compression, such as quantization and\npruning. While these techniques are well-established for attention-based\nmodels, their effects on recurrent layers remain underexplored.\n  In this preliminary work, we focus on post-training quantization for\nrecurrent LLMs and show that Mamba models exhibit the same pattern of outlier\nchannels observed in attention-based LLMs. We show that the reason for the\ndifficulty of quantizing SSMs is caused by activation outliers, similar to\nthose observed in transformer-based LLMs. We report baseline results for\npost-training quantization of Mamba that do not take into account the\nactivation outliers and suggest first steps for outlier-aware quantization.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"5ny1yuI21B5BS9g6uaruHBZkLnJbY69Bz-KWNS5BKbw","pdfSize":"704686","objectId":"0x5f3236c5c3f7d283f1b571d30db0d76ba10a002be556c63b915509d2289e72fc","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
