{"id":"2407.04003","title":"Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners","authors":"Mushui Liu, Bozheng Li, Yunlong Yu","authorsParsed":[["Liu","Mushui",""],["Li","Bozheng",""],["Yu","Yunlong",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 15:22:54 GMT"}],"updateDate":"2024-07-08","timestamp":1720106574000,"abstract":"  Prompt tuning, which involves training a small set of parameters, effectively\nenhances the pre-trained Vision-Language Models (VLMs) to downstream tasks.\nHowever, they often come at the cost of flexibility and adaptability when the\ntuned models are applied to different datasets or domains. In this paper, we\nexplore capturing the task-specific information via meticulous refinement of\nentire VLMs, with minimal parameter adjustments. When fine-tuning the entire\nVLMs for specific tasks under limited supervision, overfitting and catastrophic\nforgetting become the defacto factors. To mitigate these issues, we propose a\nframework named CLIP-CITE via designing a discriminative visual-text task,\nfurther aligning the visual-text semantics in a supervision manner, and\nintegrating knowledge distillation techniques to preserve the gained knowledge.\nExtensive experimental results under few-shot learning, base-to-new\ngeneralization, domain generalization, and cross-domain generalization\nsettings, demonstrate that our method effectively enhances the performance on\nspecific tasks under limited supervision while preserving the versatility of\nthe VLMs on other datasets.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"QR84SFuzkIR97FYS9iTq9p9BK1UJf6NWZMJMtM0l344","pdfSize":"1507829"}