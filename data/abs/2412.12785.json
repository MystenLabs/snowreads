{
  "id": "2412.12785",
  "title": "Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference",
  "authors": "Siyuan Wang, Dianyi Wang, Chengxing Zhou, Zejun Li, Zhihao Fan,\n  Xuanjing Huang, Zhongyu Wei",
  "authorsParsed": [
    [
      "Wang",
      "Siyuan",
      ""
    ],
    [
      "Wang",
      "Dianyi",
      ""
    ],
    [
      "Zhou",
      "Chengxing",
      ""
    ],
    [
      "Li",
      "Zejun",
      ""
    ],
    [
      "Fan",
      "Zhihao",
      ""
    ],
    [
      "Huang",
      "Xuanjing",
      ""
    ],
    [
      "Wei",
      "Zhongyu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 10:44:47 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734432287000,
  "abstract": "  Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Drawing inspiration from the concept of visual region in the human\nbrain, we investigate the existence of an analogous \\textit{visual region}\nwithin LLMs that functions as a cognitive core, and explore the possibility of\nefficient training of LVLMs via selective layers tuning. We use\nBunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B\nfor validation across a range of visual and textual tasks. Our findings reveal\nthat selectively updating 25\\% of LLMs layers, when sparsely and uniformly\ndistributed, can preserve nearly 99\\% of visual performance while maintaining\nor enhancing textual task results, and also effectively reducing training time.\nBased on this targeted training approach, we further propose a novel visual\nregion-based pruning paradigm, removing non-critical layers outside the visual\nregion, which can achieve minimal performance loss. This study offers an\neffective and efficient strategy for LVLM training and inference by activating\na layer-wise visual region within LLMs, which is consistently effective across\ndifferent models and parameter scales.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "S52ytbF2VDHBeOfdwRpETZENoxCdmvma5KGrDK1Q7SU",
  "pdfSize": "1073505"
}