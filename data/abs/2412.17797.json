{
  "id": "2412.17797",
  "title": "Observation Interference in Partially Observable Assistance Games",
  "authors": "Scott Emmons, Caspar Oesterheld, Vincent Conitzer, Stuart Russell",
  "authorsParsed": [
    [
      "Emmons",
      "Scott",
      ""
    ],
    [
      "Oesterheld",
      "Caspar",
      ""
    ],
    [
      "Conitzer",
      "Vincent",
      ""
    ],
    [
      "Russell",
      "Stuart",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 18:53:33 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734980013000,
  "abstract": "  We study partially observable assistance games (POAGs), a model of the\nhuman-AI value alignment problem which allows the human and the AI assistant to\nhave partial observations. Motivated by concerns of AI deception, we study a\nqualitatively new phenomenon made possible by partial observability: would an\nAI assistant ever have an incentive to interfere with the human's observations?\nFirst, we prove that sometimes an optimal assistant must take\nobservation-interfering actions, even when the human is playing optimally, and\neven when there are otherwise-equivalent actions available that do not\ninterfere with observations. Though this result seems to contradict the classic\ntheorem from single-agent decision making that the value of perfect information\nis nonnegative, we resolve this seeming contradiction by developing a notion of\ninterference defined on entire policies. This can be viewed as an extension of\nthe classic result that the value of perfect information is nonnegative into\nthe cooperative multiagent setting. Second, we prove that if the human is\nsimply making decisions based on their immediate outcomes, the assistant might\nneed to interfere with observations as a way to query the human's preferences.\nWe show that this incentive for interference goes away if the human is playing\noptimally, or if we introduce a communication channel for the human to\ncommunicate their preferences to the assistant. Third, we show that if the\nhuman acts according to the Boltzmann model of irrationality, this can create\nan incentive for the assistant to interfere with observations. Finally, we use\nan experimental model to analyze tradeoffs faced by the AI assistant in\npractice when considering whether or not to take observation-interfering\nactions.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computer Science and Game Theory",
    "Computer Science/Machine Learning",
    "Computer Science/Multiagent Systems"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "ag46Ou7AAgQR4kFLqlKJeX8H-xtGaV9ZmEM26huV2mo",
  "pdfSize": "607586"
}