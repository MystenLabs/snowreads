{"id":"2412.16956","title":"Semantic Hierarchical Prompt Tuning for Parameter-Efficient Fine-Tuning","authors":"Haowei Zhu, Fangyuan Zhang, Rui Qin, Tianxiang Pan, Junhai Yong, Bin\n  Wang","authorsParsed":[["Zhu","Haowei",""],["Zhang","Fangyuan",""],["Qin","Rui",""],["Pan","Tianxiang",""],["Yong","Junhai",""],["Wang","Bin",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 10:28:52 GMT"},{"version":"v2","created":"Tue, 24 Dec 2024 09:07:26 GMT"}],"updateDate":"2024-12-25","timestamp":1734863332000,"abstract":"  As the scale of vision models continues to grow, Visual Prompt Tuning (VPT)\nhas emerged as a parameter-efficient transfer learning technique, noted for its\nsuperior performance compared to full fine-tuning. However, indiscriminately\napplying prompts to every layer without considering their inherent\ncorrelations, can cause significant disturbances, leading to suboptimal\ntransferability. Additionally, VPT disrupts the original self-attention\nstructure, affecting the aggregation of visual features, and lacks a mechanism\nfor explicitly mining discriminative visual features, which are crucial for\nclassification. To address these issues, we propose a Semantic Hierarchical\nPrompt (SHIP) fine-tuning strategy. We adaptively construct semantic\nhierarchies and use semantic-independent and semantic-shared prompts to learn\nhierarchical representations. We also integrate attribute prompts and a prompt\nmatching loss to enhance feature discrimination and employ decoupled attention\nfor robustness and reduced inference costs. SHIP significantly improves\nperformance, achieving a 4.9% gain in accuracy over VPT with a ViT-B/16\nbackbone on VTAB-1k tasks. Our code is available at\nhttps://github.com/haoweiz23/SHIP.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"91FSufpLh9d1lp2mjel5VmRqTV2QcSF_A1dgYxXwjXQ","pdfSize":"540813"}