{"id":"2412.12048","title":"A LoRA is Worth a Thousand Pictures","authors":"Chenxi Liu, Towaki Takikawa, Alec Jacobson","authorsParsed":[["Liu","Chenxi",""],["Takikawa","Towaki",""],["Jacobson","Alec",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 18:18:17 GMT"}],"updateDate":"2024-12-17","timestamp":1734373097000,"abstract":"  Recent advances in diffusion models and parameter-efficient fine-tuning\n(PEFT) have made text-to-image generation and customization widely accessible,\nwith Low Rank Adaptation (LoRA) able to replicate an artist's style or subject\nusing minimal data and computation. In this paper, we examine the relationship\nbetween LoRA weights and artistic styles, demonstrating that LoRA weights alone\ncan serve as an effective descriptor of style, without the need for additional\nimage generation or knowledge of the original training set. Our findings show\nthat LoRA weights yield better performance in clustering of artistic styles\ncompared to traditional pre-trained features, such as CLIP and DINO, with\nstrong structural similarities between LoRA-based and conventional image-based\nembeddings observed both qualitatively and quantitatively. We identify various\nretrieval scenarios for the growing collection of customized models and show\nthat our approach enables more accurate retrieval in real-world settings where\nknowledge of the training images is unavailable and additional generation is\nrequired. We conclude with a discussion on potential future applications, such\nas zero-shot LoRA fine-tuning and model attribution.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ARri8RTZ0_0g-JTZ_q4Y2Qd5XSuk-lOyLU1xiObNQiQ","pdfSize":"44754288"}