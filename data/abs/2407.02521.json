{"id":"2407.02521","title":"Performance Comparison of Deep RL Algorithms for Mixed Traffic\n  Cooperative Lane-Changing","authors":"Xue Yao, Shengren Hou, Serge P. Hoogendoorn, and Simeon C. Calvert","authorsParsed":[["Yao","Xue",""],["Hou","Shengren",""],["Hoogendoorn","Serge P.",""],["Calvert","Simeon C.",""]],"versions":[{"version":"v1","created":"Tue, 25 Jun 2024 07:49:25 GMT"}],"updateDate":"2024-07-04","timestamp":1719301765000,"abstract":"  Lane-changing (LC) is a challenging scenario for connected and automated\nvehicles (CAVs) because of the complex dynamics and high uncertainty of the\ntraffic environment. This challenge can be handled by deep reinforcement\nlearning (DRL) approaches, leveraging their data-driven and model-free nature.\nOur previous work proposed a cooperative lane-changing in mixed traffic (CLCMT)\nmechanism based on TD3 to facilitate an optimal lane-changing strategy. This\nstudy enhances the current CLCMT mechanism by considering both the uncertainty\nof the human-driven vehicles (HVs) and the microscopic interactions between HVs\nand CAVs. The state-of-the-art (SOTA) DRL algorithms including DDPG, TD3, SAC,\nand PPO are utilized to deal with the formulated MDP with continuous actions.\nPerformance comparison among the four DRL algorithms demonstrates that DDPG,\nTD3, and PPO algorithms can deal with uncertainty in traffic environments and\nlearn well-performed LC strategies in terms of safety, efficiency, comfort, and\necology. The PPO algorithm outperforms the other three algorithms, regarding a\nhigher reward, fewer exploration mistakes and crashes, and a more comfortable\nand ecology LC strategy. The improvements promise CLCMT mechanism greater\nadvantages in the LC motion planning of CAVs.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"N-w07Xxa7Avi_tvXiGVRVOFZil1_822JXba9izZA7RI","pdfSize":"3916386"}