{"id":"2412.15726","title":"Fine-tuning Whisper on Low-Resource Languages for Real-World\n  Applications","authors":"Vincenzo Timmel, Claudio Paonessa, Reza Kakooee, Manfred Vogel and\n  Daniel Perruchoud","authorsParsed":[["Timmel","Vincenzo",""],["Paonessa","Claudio",""],["Kakooee","Reza",""],["Vogel","Manfred",""],["Perruchoud","Daniel",""]],"versions":[{"version":"v1","created":"Fri, 20 Dec 2024 09:49:02 GMT"}],"updateDate":"2024-12-23","timestamp":1734688142000,"abstract":"  This paper presents a new approach to fine-tuning OpenAI's Whisper model for\nlow-resource languages by introducing a novel data generation method that\nconverts sentence-level data into a long-form corpus, using Swiss German as a\ncase study. Non-sentence-level data, which could improve the performance of\nlong-form audio, is difficult to obtain and often restricted by copyright laws.\nOur method bridges this gap by transforming more accessible sentence-level data\ninto a format that preserves the model's ability to handle long-form audio and\nperform segmentation without requiring non-sentence-level data. Our data\ngeneration process improves performance in several real-world applications and\nleads to the development of a new state-of-the-art speech-to-text (STT) model\nfor Swiss German. We compare our model with a non-fine-tuned Whisper and our\nprevious state-of-the-art Swiss German STT models, where our new model achieves\nhigher BLEU scores. Our results also indicate that the proposed method is\nadaptable to other low-resource languages, supported by written guidance and\ncode that allows the creation of fine-tuned Whisper models, which keep\nsegmentation capabilities and allow the transcription of longer audio files\nusing only sentence-level data with high quality.\n","subjects":["Computer Science/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6SxF43YHXWSP8FOxneaW1PnjoXuFFP_phwLOdgSZD_g","pdfSize":"551489"}