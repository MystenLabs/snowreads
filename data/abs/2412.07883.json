{"id":"2412.07883","title":"On Faster Marginalization with Squared Circuits via Orthonormalization","authors":"Lorenzo Loconte, Antonio Vergari","authorsParsed":[["Loconte","Lorenzo",""],["Vergari","Antonio",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 19:37:03 GMT"},{"version":"v2","created":"Sun, 19 Jan 2025 10:48:35 GMT"}],"updateDate":"2025-01-22","timestamp":1733859423000,"abstract":"  Squared tensor networks (TNs) and their generalization as parameterized\ncomputational graphs -- squared circuits -- have been recently used as\nexpressive distribution estimators in high dimensions. However, the squaring\noperation introduces additional complexity when marginalizing variables or\ncomputing the partition function, which hinders their usage in machine learning\napplications. Canonical forms of popular TNs are parameterized via unitary\nmatrices as to simplify the computation of particular marginals, but cannot be\nmapped to general circuits since these might not correspond to a known TN.\nInspired by TN canonical forms, we show how to parameterize squared circuits to\nensure they encode already normalized distributions. We then use this\nparameterization to devise an algorithm to compute any marginal of squared\ncircuits that is more efficient than a previously known one. We conclude by\nformally showing the proposed parameterization comes with no expressiveness\nloss for many circuit classes.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"WIQgHyfc07lFksdh20NEdTijd_WUk_DJsvihKqCfUtc","pdfSize":"552581"}