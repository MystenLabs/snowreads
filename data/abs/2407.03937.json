{"id":"2407.03937","title":"TongGu: Mastering Classical Chinese Understanding with\n  Knowledge-Grounded Large Language Models","authors":"Jiahuan Cao, Dezhi Peng, Peirong Zhang, Yongxin Shi, Yang Liu, Kai\n  Ding, Lianwen Jin","authorsParsed":[["Cao","Jiahuan",""],["Peng","Dezhi",""],["Zhang","Peirong",""],["Shi","Yongxin",""],["Liu","Yang",""],["Ding","Kai",""],["Jin","Lianwen",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 13:52:23 GMT"}],"updateDate":"2024-07-08","timestamp":1720101143000,"abstract":"  Classical Chinese is a gateway to the rich heritage and wisdom of ancient\nChina, yet its complexities pose formidable comprehension barriers for most\nmodern people without specialized knowledge. While Large Language Models (LLMs)\nhave shown remarkable capabilities in Natural Language Processing (NLP), they\nstruggle with Classical Chinese Understanding (CCU), especially in\ndata-demanding and knowledge-intensive tasks. In response to this dilemma, we\npropose \\textbf{TongGu} (mean understanding ancient and modern), the first\nCCU-specific LLM, underpinned by three core contributions. First, we construct\na two-stage instruction-tuning dataset ACCN-INS derived from rich classical\nChinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we\npropose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,\nenabling TongGu to acquire new capabilities while preserving its foundational\nknowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)\ntechnique to reduce hallucinations based on knowledge-grounding. Extensive\nexperiments across 24 diverse CCU tasks validate TongGu's superior ability,\nunderscoring the effectiveness of RAT and CCU-RAG. The model and dataset will\nbe public available.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"4UW6yepdWjaDPOZ7RaGssdavUsnedLj1_sq5yjmTmG4","pdfSize":"3026609"}