{
  "id": "2412.06340",
  "title": "UniPaint: Unified Space-time Video Inpainting via Mixture-of-Experts",
  "authors": "Zhen Wan, Yue Ma, Chenyang Qi, Zhiheng Liu, Tao Gui",
  "authorsParsed": [
    [
      "Wan",
      "Zhen",
      ""
    ],
    [
      "Ma",
      "Yue",
      ""
    ],
    [
      "Qi",
      "Chenyang",
      ""
    ],
    [
      "Liu",
      "Zhiheng",
      ""
    ],
    [
      "Gui",
      "Tao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 09:45:14 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733737514000,
  "abstract": "  In this paper, we present UniPaint, a unified generative space-time video\ninpainting framework that enables spatial-temporal inpainting and\ninterpolation. Different from existing methods that treat video inpainting and\nvideo interpolation as two distinct tasks, we leverage a unified inpainting\nframework to tackle them and observe that these two tasks can mutually enhance\nsynthesis performance. Specifically, we first introduce a plug-and-play\nspace-time video inpainting adapter, which can be employed in various\npersonalized models. The key insight is to propose a Mixture of Experts (MoE)\nattention to cover various tasks. Then, we design a spatial-temporal masking\nstrategy during the training stage to mutually enhance each other and improve\nperformance. UniPaint produces high-quality and aesthetically pleasing results,\nachieving the best quantitative results across various tasks and scale setups.\nThe code and checkpoints will be available soon.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Tnnogb1ONw3WrZ-yPZmAAkduJbgQMjGcd4nxcpDJplg",
  "pdfSize": "30475761"
}