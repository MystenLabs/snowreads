{
  "id": "2412.07391",
  "title": "Post-Training Non-Uniform Quantization for Convolutional Neural Networks",
  "authors": "Ahmed Luqman, Khuzemah Qazi, Imdadullah Khan",
  "authorsParsed": [
    [
      "Luqman",
      "Ahmed",
      ""
    ],
    [
      "Qazi",
      "Khuzemah",
      ""
    ],
    [
      "Khan",
      "Imdadullah",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 10:33:58 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733826838000,
  "abstract": "  Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on Real World Datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "dHLJ6YaGWDMBTXONQX5HJZdmMOKLwk1G3_EZ_baVt9A",
  "pdfSize": "643838"
}