{"id":"2412.05951","title":"When Vision Models Meet Parameter Efficient Look-Aside Adapters Without\n  Large-Scale Audio Pretraining","authors":"Juan Yeo, Jinkwan Jang, Kyubyung Chae, Seongkyu Mun, Taesup Kim","authorsParsed":[["Yeo","Juan",""],["Jang","Jinkwan",""],["Chae","Kyubyung",""],["Mun","Seongkyu",""],["Kim","Taesup",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 14:14:30 GMT"}],"updateDate":"2024-12-10","timestamp":1733667270000,"abstract":"  Recent studies show that pretrained vision models can boost performance in\naudio downstream tasks. To enhance the performance further, an additional\npretraining stage with large scale audio data is typically required to infuse\naudio specific knowledge into the vision model. However, such approaches\nrequire extensive audio data and a carefully designed objective function. In\nthis work, we propose bypassing the pretraining stage by directly fine-tuning\nthe vision model with our Look Aside Adapter (LoAA) designed for efficient\naudio understanding. Audio spectrum data is represented across two\nheterogeneous dimensions time and frequency and we refine adapters to\nfacilitate interactions between tokens across these dimensions. Our experiments\ndemonstrate that our adapters allow vision models to reach or surpass the\nperformance of pretrained audio models in various audio and speech tasks,\noffering a resource efficient and effective solution for leveraging vision\nmodels in audio applications.\n","subjects":["Computer Science/Sound","Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"snkY50qi8QTjpHRatE8KjVTtwR2BjG7SQ0L6fzu2aqI","pdfSize":"1378047"}