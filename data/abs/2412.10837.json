{"id":"2412.10837","title":"A Diagrammatic Approach to Improve Computational Efficiency in Group\n  Equivariant Neural Networks","authors":"Edward Pearce-Crump, William J. Knottenbelt","authorsParsed":[["Pearce-Crump","Edward",""],["Knottenbelt","William J.",""]],"versions":[{"version":"v1","created":"Sat, 14 Dec 2024 14:08:06 GMT"}],"updateDate":"2024-12-17","timestamp":1734185286000,"abstract":"  Group equivariant neural networks are growing in importance owing to their\nability to generalise well in applications where the data has known underlying\nsymmetries. Recent characterisations of a class of these networks that use\nhigh-order tensor power spaces as their layers suggest that they have\nsignificant potential; however, their implementation remains challenging owing\nto the prohibitively expensive nature of the computations that are involved. In\nthis work, we present a fast matrix multiplication algorithm for any\nequivariant weight matrix that maps between tensor power layer spaces in these\nnetworks for four groups: the symmetric, orthogonal, special orthogonal, and\nsymplectic groups. We obtain this algorithm by developing a diagrammatic\nframework based on category theory that enables us to not only express each\nweight matrix as a linear combination of diagrams but also makes it possible\nfor us to use these diagrams to factor the original computation into a series\nof steps that are optimal. We show that this algorithm improves the Big-$O$\ntime complexity exponentially in comparison to a na\\\"{i}ve matrix\nmultiplication.\n","subjects":["Computer Science/Machine Learning","Mathematics/Combinatorics","Mathematics/Representation Theory","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qjgkqJbQ1fbX44IOLxpB-bzVT0l_fCdrPghsPZND8A0","pdfSize":"526884"}