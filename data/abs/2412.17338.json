{"id":"2412.17338","title":"Enhancing Topic Interpretability for Neural Topic Modeling through\n  Topic-wise Contrastive Learning","authors":"Xin Gao, Yang Lin, Ruiqing Li, Yasha Wang, Xu Chu, Xinyu Ma, Hailong\n  Yu","authorsParsed":[["Gao","Xin",""],["Lin","Yang",""],["Li","Ruiqing",""],["Wang","Yasha",""],["Chu","Xu",""],["Ma","Xinyu",""],["Yu","Hailong",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 07:07:06 GMT"}],"updateDate":"2024-12-24","timestamp":1734937626000,"abstract":"  Data mining and knowledge discovery are essential aspects of extracting\nvaluable insights from vast datasets. Neural topic models (NTMs) have emerged\nas a valuable unsupervised tool in this field. However, the predominant\nobjective in NTMs, which aims to discover topics maximizing data likelihood,\noften lacks alignment with the central goals of data mining and knowledge\ndiscovery which is to reveal interpretable insights from large data\nrepositories. Overemphasizing likelihood maximization without incorporating\ntopic regularization can lead to an overly expansive latent space for topic\nmodeling. In this paper, we present an innovative approach to NTMs that\naddresses this misalignment by introducing contrastive learning measures to\nassess topic interpretability. We propose a novel NTM framework, named\nContraTopic, that integrates a differentiable regularizer capable of evaluating\nmultiple facets of topic interpretability throughout the training process. Our\nregularizer adopts a unique topic-wise contrastive methodology, fostering both\ninternal coherence within topics and clear external distinctions among them.\nComprehensive experiments conducted on three diverse datasets demonstrate that\nour approach consistently produces topics with superior interpretability\ncompared to state-of-the-art NTMs.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qJNcfOwRblsvnKshG1Qcne2y9WPZCd6kr3I_KEUxNs0","pdfSize":"5812773"}