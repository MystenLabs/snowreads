{
  "id": "2412.07942",
  "title": "Neural Scaling Laws Rooted in the Data Distribution",
  "authors": "Ari Brill",
  "authorsParsed": [
    [
      "Brill",
      "Ari",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 22:01:38 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733868098000,
  "abstract": "  Deep neural networks exhibit empirical neural scaling laws, with error\ndecreasing as a power law with increasing model or data size, across a wide\nvariety of architectures, tasks, and datasets. This universality suggests that\nscaling laws may result from general properties of natural learning tasks. We\ndevelop a mathematical model intended to describe natural datasets using\npercolation theory. Two distinct criticality regimes emerge, each yielding\noptimal power-law neural scaling laws. These regimes, corresponding to\npower-law-distributed discrete subtasks and a dominant data manifold, can be\nassociated with previously proposed theories of neural scaling, thereby\ngrounding and unifying prior works. We test the theory by training regression\nmodels on toy datasets derived from percolation theory simulations. We suggest\ndirections for quantitatively predicting language model scaling.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Condensed Matter/Disordered Systems and Neural Networks"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "y_uJX1-5zyU2RNIHoyxHaVaobgbPQsPQTzHl_TDfwDM",
  "pdfSize": "1174570"
}