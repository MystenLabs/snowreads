{"id":"2412.16906","title":"Self-Corrected Flow Distillation for Consistent One-Step and Few-Step\n  Text-to-Image Generation","authors":"Quan Dao, Hao Phung, Trung Dao, Dimitris Metaxas, Anh Tran","authorsParsed":[["Dao","Quan",""],["Phung","Hao",""],["Dao","Trung",""],["Metaxas","Dimitris",""],["Tran","Anh",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 07:48:49 GMT"}],"updateDate":"2024-12-24","timestamp":1734853729000,"abstract":"  Flow matching has emerged as a promising framework for training generative\nmodels, demonstrating impressive empirical performance while offering relative\nease of training compared to diffusion-based models. However, this method still\nrequires numerous function evaluations in the sampling process. To address\nthese limitations, we introduce a self-corrected flow distillation method that\neffectively integrates consistency models and adversarial training within the\nflow-matching framework. This work is a pioneer in achieving consistent\ngeneration quality in both few-step and one-step sampling. Our extensive\nexperiments validate the effectiveness of our method, yielding superior results\nboth quantitatively and qualitatively on CelebA-HQ and zero-shot benchmarks on\nthe COCO dataset. Our implementation is released at\nhttps://github.com/VinAIResearch/SCFlow\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4X0vTHvmVOe2QEu-j_7y-TYE2fV50O96Np4YcUkPJZ0","pdfSize":"18590972"}