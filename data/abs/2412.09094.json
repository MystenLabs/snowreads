{
  "id": "2412.09094",
  "title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion",
  "authors": "Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng",
  "authorsParsed": [
    [
      "Liu",
      "Ben",
      ""
    ],
    [
      "Zhang",
      "Jihai",
      ""
    ],
    [
      "Lin",
      "Fangquan",
      ""
    ],
    [
      "Yang",
      "Cheng",
      ""
    ],
    [
      "Peng",
      "Min",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 12 Dec 2024 09:22:04 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 9 Jan 2025 12:38:37 GMT"
    },
    {
      "version": "v3",
      "created": "Sat, 8 Feb 2025 13:40:23 GMT"
    }
  ],
  "updateDate": "2025-02-11",
  "timestamp": 1733995324000,
  "abstract": "  Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a filter-then-generate paradigm and formulate the KGC task into a\nmultiple-choice question format. In this way, we can harness the capability of\nLLMs while mitigating the issue casused by hallucinations. Moreover, we devise\na flexible ego-graph serialization prompt and employ a structure-text adapter\nto couple structure and text information in a contextualized manner.\nExperimental results demonstrate that FtG achieves substantial performance gain\ncompared to existing state-of-the-art methods. The instruction dataset and code\nare available at https://github.com/LB0828/FtG.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "aVzCetWYWhdvb1ajJYc2iDC-9kmbqjsA2HAua4zGKd8",
  "pdfSize": "1143628"
}