{
  "id": "2412.12145",
  "title": "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars",
  "authors": "Yu Yan, Sheng Sun, Junqi Tong, Min Liu, and Qi Li",
  "authorsParsed": [
    [
      "Yan",
      "Yu",
      ""
    ],
    [
      "Sun",
      "Sheng",
      ""
    ],
    [
      "Tong",
      "Junqi",
      ""
    ],
    [
      "Liu",
      "Min",
      ""
    ],
    [
      "Li",
      "Qi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 10:14:03 GMT"
    },
    {
      "version": "v2",
      "created": "Sun, 16 Feb 2025 11:19:14 GMT"
    },
    {
      "version": "v3",
      "created": "Wed, 19 Feb 2025 11:09:15 GMT"
    },
    {
      "version": "v4",
      "created": "Sat, 22 Feb 2025 07:36:26 GMT"
    }
  ],
  "updateDate": "2025-02-25",
  "timestamp": 1733825643000,
  "abstract": "  Metaphor serves as an implicit approach to convey information, while enabling\nthe generalized comprehension of complex subjects. However, metaphor can\npotentially be exploited to bypass the safety alignment mechanisms of Large\nLanguage Models (LLMs), leading to the theft of harmful knowledge. In our\nstudy, we introduce a novel attack framework that exploits the imaginative\ncapacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak\n\\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial\nMe\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}).\nSpecifically, to elicit the harmful response, AVATAR extracts harmful entities\nfrom a given harmful target and maps them to innocuous adversarial entities\nbased on LLM's imagination. Then, according to these metaphors, the harmful\ntarget is nested within human-like interaction for jailbreaking adaptively.\nExperimental results demonstrate that AVATAR can effectively and transferablly\njailbreak LLMs and achieve a state-of-the-art attack success rate across\nmultiple advanced LLMs. Our study exposes a security risk in LLMs from their\nendogenous imaginative capabilities. Furthermore, the analytical study reveals\nthe vulnerability of LLM to adversarial metaphors and the necessity of\ndeveloping defense methods against jailbreaking caused by the adversarial\nmetaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially\nharmful content from LLMs.}}\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "P1x4ZwNwtjjRHjjipU142e_Q_NSoyWxjxfj5VO_eYcY",
  "pdfSize": "14576710"
}