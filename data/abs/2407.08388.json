{"id":"2407.08388","title":"On the attribution of confidence to large language models","authors":"Geoff Keeling, Winnie Street","authorsParsed":[["Keeling","Geoff",""],["Street","Winnie",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 10:51:06 GMT"}],"updateDate":"2024-07-12","timestamp":1720695066000,"abstract":"  Credences are mental states corresponding to degrees of confidence in\npropositions. Attribution of credences to Large Language Models (LLMs) is\ncommonplace in the empirical literature on LLM evaluation. Yet the theoretical\nbasis for LLM credence attribution is unclear. We defend three claims. First,\nour semantic claim is that LLM credence attributions are (at least in general)\ncorrectly interpreted literally, as expressing truth-apt beliefs on the part of\nscientists that purport to describe facts about LLM credences. Second, our\nmetaphysical claim is that the existence of LLM credences is at least\nplausible, although current evidence is inconclusive. Third, our epistemic\nclaim is that LLM credence attributions made in the empirical literature on LLM\nevaluation are subject to non-trivial sceptical concerns. It is a distinct\npossibility that even if LLMs have credences, LLM credence attributions are\ngenerally false because the experimental techniques used to assess LLM\ncredences are not truth-tracking.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Icy7rVcEp8uLqK6W56Esvctn76VsclIOl2Z7xwJW3io","pdfSize":"287928"}