{"id":"2412.05520","title":"More than Marketing? On the Information Value of AI Benchmarks for\n  Practitioners","authors":"Amelia Hardy, Anka Reuel, Kiana Jafari Meimandi, Lisa Soder, Allie\n  Griffith, Dylan M. Asmar, Sanmi Koyejo, Michael S. Bernstein, Mykel J.\n  Kochenderfer","authorsParsed":[["Hardy","Amelia",""],["Reuel","Anka",""],["Meimandi","Kiana Jafari",""],["Soder","Lisa",""],["Griffith","Allie",""],["Asmar","Dylan M.",""],["Koyejo","Sanmi",""],["Bernstein","Michael S.",""],["Kochenderfer","Mykel J.",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 03:35:39 GMT"}],"updateDate":"2024-12-10","timestamp":1733542539000,"abstract":"  Public AI benchmark results are widely broadcast by model developers as\nindicators of model quality within a growing and competitive market. However,\nthese advertised scores do not necessarily reflect the traits of interest to\nthose who will ultimately apply AI models. In this paper, we seek to understand\nif and how AI benchmarks are used to inform decision-making. Based on the\nanalyses of interviews with 19 individuals who have used, or decided against\nusing, benchmarks in their day-to-day work, we find that across these settings,\nparticipants use benchmarks as a signal of relative performance difference\nbetween models. However, whether this signal was considered a definitive sign\nof model superiority, sufficient for downstream decisions, varied. In academia,\npublic benchmarks were generally viewed as suitable measures for capturing\nresearch progress. By contrast, in both product and policy, benchmarks -- even\nthose developed internally for specific tasks -- were often found to be\ninadequate for informing substantive decisions. Of the benchmarks deemed\nunsatisfactory, respondents reported that their goals were neither well-defined\nnor reflective of real-world use. Based on the study results, we conclude that\neffective benchmarks should provide meaningful, real-world evaluations,\nincorporate domain expertise, and maintain transparency in scope and goals.\nThey must capture diverse, task-relevant capabilities, be challenging enough to\navoid quick saturation, and account for trade-offs in model performance rather\nthan relying on a single score. Additionally, proprietary data collection and\ncontamination prevention are critical for producing reliable and actionable\nresults. By adhering to these criteria, benchmarks can move beyond mere\nmarketing tricks into robust evaluative frameworks.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"jtwg5SLYORhQ_luAyNAoFmHKY3ly89L5N1zNnaoZ8M8","pdfSize":"282485"}