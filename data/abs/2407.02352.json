{"id":"2407.02352","title":"Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition\n  and Program of Thought Verification","authors":"Pritish Sahu and Karan Sikka and Ajay Divakaran","authorsParsed":[["Sahu","Pritish",""],["Sikka","Karan",""],["Divakaran","Ajay",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 15:17:44 GMT"}],"updateDate":"2024-07-03","timestamp":1719933464000,"abstract":"  Large Visual Language Models (LVLMs) struggle with hallucinations in visual\ninstruction following task(s), limiting their trustworthiness and real-world\napplicability. We propose Pelican -- a novel framework designed to detect and\nmitigate hallucinations through claim verification. Pelican first decomposes\nthe visual claim into a chain of sub-claims based on first-order predicates.\nThese sub-claims consist of (predicate, question) pairs and can be\nconceptualized as nodes of a computational graph. We then use\nProgram-of-Thought prompting to generate Python code for answering these\nquestions through flexible composition of external tools. Pelican improves over\nprior work by introducing (1) intermediate variables for precise grounding of\nobject instances, and (2) shared computation for answering the sub-question to\nenable adaptive corrections and inconsistency identification. We finally use\nreasoning abilities of LLM to verify the correctness of the the claim by\nconsidering the consistency and confidence of the (question, answer) pairs from\neach sub-claim. Our experiments reveal a drop in hallucination rate by\n$\\sim$8%-32% across various baseline LVLMs and a 27% drop compared to\napproaches proposed for hallucination mitigation on MMHal-Bench. Results on two\nother benchmarks further corroborate our results.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"MVsNlqqOIapEYdfz4x__abAJ80xh2duP4qAewnkG6ec","pdfSize":"24622087"}