{"id":"2412.11060","title":"Making Bias Amplification in Balanced Datasets Directional and\n  Interpretable","authors":"Bhanu Tokas, Rahul Nair, Hannah Kerner","authorsParsed":[["Tokas","Bhanu",""],["Nair","Rahul",""],["Kerner","Hannah",""]],"versions":[{"version":"v1","created":"Sun, 15 Dec 2024 05:32:54 GMT"}],"updateDate":"2024-12-17","timestamp":1734240774000,"abstract":"  Most of the ML datasets we use today are biased. When we train models on\nthese biased datasets, they often not only learn dataset biases but can also\namplify them -- a phenomenon known as bias amplification. Several\nco-occurrence-based metrics have been proposed to measure bias amplification\nbetween a protected attribute A (e.g., gender) and a task T (e.g., cooking).\nHowever, these metrics fail to measure biases when A is balanced with T. To\nmeasure bias amplification in balanced datasets, recent work proposed a\npredictability-based metric called leakage amplification. However, leakage\namplification cannot identify the direction in which biases are amplified. In\nthis work, we propose a new predictability-based metric called directional\npredictability amplification (DPA). DPA measures directional bias\namplification, even for balanced datasets. Unlike leakage amplification, DPA is\neasier to interpret and less sensitive to attacker models (a hyperparameter in\npredictability-based metrics). Our experiments on tabular and image datasets\nshow that DPA is an effective metric for measuring directional bias\namplification. The code will be available soon.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cQ9p2YGAZwmc4A5N0krawnz3lPPZeue3LrgtVHPdeTc","pdfSize":"1642747"}