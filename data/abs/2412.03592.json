{
  "id": "2412.03592",
  "title": "Using Images to Find Context-Independent Word Representations in Vector\n  Space",
  "authors": "Harsh Kumar",
  "authorsParsed": [
    [
      "Kumar",
      "Harsh",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 28 Nov 2024 08:44:10 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1732783450000,
  "abstract": "  Many methods have been proposed to find vector representation for words, but\nmost rely on capturing context from the text to find semantic relationships\nbetween these vectors. We propose a novel method of using dictionary meanings\nand image depictions to find word vectors independent of any context. We use\nauto-encoder on the word images to find meaningful representations and use them\nto calculate the word vectors. We finally evaluate our method on word\nsimilarity, concept categorization and outlier detection tasks. Our method\nperforms comparably to context-based methods while taking much less training\ntime.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "J95Ap0wnaM_HFxL5hjEUShW1f6oVjnflQC9-B5jMYcc",
  "pdfSize": "218490"
}