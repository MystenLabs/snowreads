{
  "id": "2412.17614",
  "title": "Emerging Security Challenges of Large Language Models",
  "authors": "Herve Debar, Sven Dietrich, Pavel Laskov, Emil C. Lupu, Eirini Ntoutsi",
  "authorsParsed": [
    [
      "Debar",
      "Herve",
      ""
    ],
    [
      "Dietrich",
      "Sven",
      ""
    ],
    [
      "Laskov",
      "Pavel",
      ""
    ],
    [
      "Lupu",
      "Emil C.",
      ""
    ],
    [
      "Ntoutsi",
      "Eirini",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 14:36:37 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734964597000,
  "abstract": "  Large language models (LLMs) have achieved record adoption in a short period\nof time across many different sectors including high importance areas such as\neducation [4] and healthcare [23]. LLMs are open-ended models trained on\ndiverse data without being tailored for specific downstream tasks, enabling\nbroad applicability across various domains. They are commonly used for text\ngeneration, but also widely used to assist with code generation [3], and even\nanalysis of security information, as Microsoft Security Copilot demonstrates\n[18]. Traditional Machine Learning (ML) models are vulnerable to adversarial\nattacks [9]. So the concerns on the potential security implications of such\nwide scale adoption of LLMs have led to the creation of this working group on\nthe security of LLMs. During the Dagstuhl seminar on \"Network Attack Detection\nand Defense - AI-Powered Threats and Responses\", the working group discussions\nfocused on the vulnerability of LLMs to adversarial attacks, rather than their\npotential use in generating malware or enabling cyberattacks. Although we note\nthe potential threat represented by the latter, the role of the LLMs in such\nuses is mostly as an accelerator for development, similar to what it is in\nbenign use. To make the analysis more specific, the working group employed\nChatGPT as a concrete example of an LLM and addressed the following points,\nwhich also form the structure of this report: 1. How do LLMs differ in\nvulnerabilities from traditional ML models? 2. What are the attack objectives\nin LLMs? 3. How complex it is to assess the risks posed by the vulnerabilities\nof LLMs? 4. What is the supply chain in LLMs, how data flow in and out of\nsystems and what are the security implications? We conclude with an overview of\nopen challenges and outlook.\n",
  "subjects": [
    "Computer Science/Cryptography and Security",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "rd6m9GgDO4cNVVuoaCWTW9jmqSnA_6Ks-XdQEJvrsrQ",
  "pdfSize": "315284"
}