{"id":"2412.18450","title":"3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D\n  Scene Understanding","authors":"Tatiana Zemskova and Dmitry Yudin","authorsParsed":[["Zemskova","Tatiana",""],["Yudin","Dmitry",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 14:21:58 GMT"},{"version":"v2","created":"Wed, 25 Dec 2024 11:13:41 GMT"}],"updateDate":"2024-12-30","timestamp":1735050118000,"abstract":"  A 3D scene graph represents a compact scene model, storing information about\nthe objects and the semantic relationships between them, making its use\npromising for robotic tasks. When interacting with a user, an embodied\nintelligent agent should be capable of responding to various queries about the\nscene formulated in natural language. Large Language Models (LLMs) are\nbeneficial solutions for user-robot interaction due to their natural language\nunderstanding and reasoning abilities. Recent methods for creating learnable\nrepresentations of 3D scenes have demonstrated the potential to improve the\nquality of LLMs responses by adapting to the 3D world. However, the existing\nmethods do not explicitly utilize information about the semantic relationships\nbetween objects, limiting themselves to information about their coordinates. In\nthis work, we propose a method 3DGraphLLM for constructing a learnable\nrepresentation of a 3D scene graph. The learnable representation is used as\ninput for LLMs to perform 3D vision-language tasks. In our experiments on\npopular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap\ndatasets, we demonstrate the advantage of this approach over baseline methods\nthat do not use information about the semantic relationships between objects.\nThe code is publicly available at\nhttps://github.com/CognitiveAISystems/3DGraphLLM.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"P60LqI9POVRIkcVM4SHmDKeemR8R309v_pYcJEAbfXY","pdfSize":"8447384"}