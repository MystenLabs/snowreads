{"id":"2407.03310","title":"Universal Length Generalization with Turing Programs","authors":"Kaiying Hou, David Brandfonbrener, Sham Kakade, Samy Jelassi, Eran\n  Malach","authorsParsed":[["Hou","Kaiying",""],["Brandfonbrener","David",""],["Kakade","Sham",""],["Jelassi","Samy",""],["Malach","Eran",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 17:53:44 GMT"}],"updateDate":"2024-07-04","timestamp":1720029224000,"abstract":"  Length generalization refers to the ability to extrapolate from short\ntraining sequences to long test sequences and is a challenge for current large\nlanguage models. While prior work has proposed some architecture or data format\nchanges to achieve length generalization, these proposals typically apply to a\nlimited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT)\ntechniques, we propose Turing Programs, a novel CoT strategy that decomposes an\nalgorithmic task into steps mimicking the computation of a Turing Machine. This\nframework is both universal, as it can accommodate any algorithmic task, and\nsimple, requiring only copying text from the context with small modifications.\nWe show that by using Turing Programs, we obtain robust length generalization\non a range of algorithmic tasks: addition, multiplication and in-context SGD.\nWe then demonstrate that transformers achieve length generalization on random\nTuring Programs, suggesting that length generalization is possible for any\nalgorithmic task. Finally, we theoretically prove that transformers can\nimplement Turing Programs, constructing a simple RASP (Weiss et al.) program\nthat simulates an arbitrary Turing machine.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rIrxy8NaoY5WiBDVIkYDRQsb6crp8v6f1v7-Y6KXysg","pdfSize":"798848"}
