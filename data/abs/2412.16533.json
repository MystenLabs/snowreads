{"id":"2412.16533","title":"Self-guided Knowledgeable Network of Thoughts: Amplifying Reasoning with\n  Large Language Models","authors":"Chao-Chi Chen, Chin-Yuan Yeh, Hsi-Wen Chen, De-Nian Yang, Ming-Syan\n  Chen","authorsParsed":[["Chen","Chao-Chi",""],["Yeh","Chin-Yuan",""],["Chen","Hsi-Wen",""],["Yang","De-Nian",""],["Chen","Ming-Syan",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 08:19:42 GMT"}],"updateDate":"2024-12-24","timestamp":1734769182000,"abstract":"  We introduce Knowledgeable Network of Thoughts (kNoT): a prompt scheme that\nadvances the capabilities of large language models (LLMs) beyond existing\nparadigms like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of\nThoughts (GoT). The key innovation of kNoT is the LLM Workflow Template (LWT),\nwhich allows for an executable plan to be specified by LLMs for LLMs. LWT\nallows these plans to be arbitrary networks, where single-step LLM operations\nare nodes, and edges correspond to message passing between these steps.\nFurthermore, LWT supports selection of individual elements through indexing,\nfacilitating kNoT to produce intricate plans where each LLM operation can be\nlimited to elementary operations, greatly enhancing reliability over extended\ntask sequences. We demonstrate that kNoT significantly outperforms the state of\nthe art on six use cases, while reducing the need for extensive prompt\nengineering. For instance, kNoT finds 92% accuracy for sorting 32 numbers over\n12% and 31% for ToT and GoT, while utilizing up to 84.4% and 87.3% less\ntask-specific prompts, respectively.\n","subjects":["Computer Science/Multiagent Systems","Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"vJuahpwJArq7HLN5_kOvat461YlXQBRNxhP0rGeuF9w","pdfSize":"1053166"}