{"id":"2412.08871","title":"Inference-Time Diffusion Model Distillation","authors":"Geon Yeong Park, Sang Wan Lee, Jong Chul Ye","authorsParsed":[["Park","Geon Yeong",""],["Lee","Sang Wan",""],["Ye","Jong Chul",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 02:07:17 GMT"}],"updateDate":"2024-12-13","timestamp":1733969237000,"abstract":"  Diffusion distillation models effectively accelerate reverse sampling by\ncompressing the process into fewer steps. However, these models still exhibit a\nperformance gap compared to their pre-trained diffusion model counterparts,\nexacerbated by distribution shifts and accumulated errors during multi-step\nsampling. To address this, we introduce Distillation++, a novel inference-time\ndistillation framework that reduces this gap by incorporating teacher-guided\nrefinement during sampling. Inspired by recent advances in conditional\nsampling, our approach recasts student model sampling as a proximal\noptimization problem with a score distillation sampling loss (SDS). To this\nend, we integrate distillation optimization during reverse sampling, which can\nbe viewed as teacher guidance that drives student sampling trajectory towards\nthe clean manifold using pre-trained diffusion models. Thus, Distillation++\nimproves the denoising process in real-time without additional source data or\nfine-tuning. Distillation++ demonstrates substantial improvements over\nstate-of-the-art distillation baselines, particularly in early sampling stages,\npositioning itself as a robust guided sampling process crafted for diffusion\ndistillation models. Code:\nhttps://github.com/geonyeong-park/inference_distillation.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"LHuqTqaVLWNCe5sUbimmgRLUHMTdRQ7Sns7inv2R4ho","pdfSize":"12516765"}