{"id":"2407.02891","title":"GPTQT: Quantize Large Language Models Twice to Push the Efficiency","authors":"Yipin Guo, Yilin Lang, Qinyuan Ren","authorsParsed":[["Guo","Yipin",""],["Lang","Yilin",""],["Ren","Qinyuan",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 08:08:01 GMT"}],"updateDate":"2024-07-04","timestamp":1719994081000,"abstract":"  Due to their large size, generative Large Language Models (LLMs) require\nsignificant computing and storage resources. This paper introduces a new\npost-training quantization method, GPTQT, to reduce memory usage and enhance\nprocessing speed by expressing the weight of LLM in 3bit/2bit. Practice has\nshown that minimizing the quantization error of weights is ineffective, leading\nto overfitting. Therefore, GPTQT employs a progressive two-step approach:\ninitially quantizing weights using Linear quantization to a relatively high\nbit, followed by converting obtained int weight to lower bit binary coding. A\nre-explore strategy is proposed to optimize initial scaling factor. During\ninference, these steps are merged into pure binary coding, enabling efficient\ncomputation. Testing across various models and datasets confirms GPTQT's\neffectiveness. Compared to the strong 3-bit quantization baseline, GPTQT\nfurther reduces perplexity by 4.01 on opt-66B and increases speed by 1.24 times\non opt-30b. The results on Llama2 show that GPTQT is currently the best binary\ncoding quantization method for such kind of LLMs.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"a6Tec4MzgaJR5e-9tHJ6Qx3QnRyvCV_of_ONjn1HDeE","pdfSize":"623970"}