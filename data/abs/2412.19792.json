{"id":"2412.19792","title":"InfAlign: Inference-aware language model alignment","authors":"Ananth Balashankar and Ziteng Sun and Jonathan Berant and Jacob\n  Eisenstein and Michael Collins and Adrian Hutter and Jong Lee and Chirag\n  Nagpal and Flavien Prost and Aradhana Sinha and Ananda Theertha Suresh and\n  Ahmad Beirami","authorsParsed":[["Balashankar","Ananth",""],["Sun","Ziteng",""],["Berant","Jonathan",""],["Eisenstein","Jacob",""],["Collins","Michael",""],["Hutter","Adrian",""],["Lee","Jong",""],["Nagpal","Chirag",""],["Prost","Flavien",""],["Sinha","Aradhana",""],["Suresh","Ananda Theertha",""],["Beirami","Ahmad",""]],"versions":[{"version":"v1","created":"Fri, 27 Dec 2024 18:45:36 GMT"},{"version":"v2","created":"Mon, 30 Dec 2024 09:37:33 GMT"},{"version":"v3","created":"Thu, 6 Feb 2025 18:15:48 GMT"}],"updateDate":"2025-02-07","timestamp":1735325136000,"abstract":"  Language model alignment is a critical step in training modern generative\nlanguage models. Alignment targets to improve win rate of a sample from the\naligned model against the base model. Today, we are increasingly using\ninference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)\nto decode from language models rather than standard sampling. We show that this\ntrain/test mismatch makes standard RLHF framework sub-optimal in view of such\ninference-time methods. To this end, we propose a framework for inference-aware\nalignment (InfAlign), which aims to optimize inference-time win rate of the\naligned policy against the base model. We prove that for any inference-time\ndecoding procedure, the optimal aligned policy is the solution to the standard\nRLHF problem with a transformation of the reward. This motivates us to provide\nthe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,\nwhich involves a reward calibration step and a KL-regularized reward\nmaximization step with a transformation of the calibrated reward. For best-of-N\nsampling and best-of-N jailbreaking, we propose specific transformations\noffering up to 3-8% improvement on inference-time win rates. Finally, we also\nshow that our proposed reward calibration method is a strong baseline for\noptimizing standard win rate.\n","subjects":["Computer Science/Machine Learning","Computer Science/Computation and Language","Computer Science/Information Theory","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8V7eb-dm8uRF3YesIiQSLMP0qRw_tRA_QkQU3shk6JA","pdfSize":"3790165"}