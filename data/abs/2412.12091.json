{
  "id": "2412.12091",
  "title": "Wonderland: Navigating 3D Scenes from a Single Image",
  "authors": "Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev,\n  Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren",
  "authorsParsed": [
    [
      "Liang",
      "Hanwen",
      ""
    ],
    [
      "Cao",
      "Junli",
      ""
    ],
    [
      "Goel",
      "Vidit",
      ""
    ],
    [
      "Qian",
      "Guocheng",
      ""
    ],
    [
      "Korolev",
      "Sergei",
      ""
    ],
    [
      "Terzopoulos",
      "Demetri",
      ""
    ],
    [
      "Plataniotis",
      "Konstantinos N.",
      ""
    ],
    [
      "Tulyakov",
      "Sergey",
      ""
    ],
    [
      "Ren",
      "Jian",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 18:58:17 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734375497000,
  "abstract": "  This paper addresses a challenging question: How can we efficiently create\nhigh-quality, wide-scope 3D scenes from a single arbitrary image? Existing\nmethods face several constraints, such as requiring multi-view data,\ntime-consuming per-scene optimization, low visual quality in backgrounds, and\ndistorted reconstructions in unseen areas. We propose a novel pipeline to\novercome these limitations. Specifically, we introduce a large-scale\nreconstruction model that uses latents from a video diffusion model to predict\n3D Gaussian Splattings for the scenes in a feed-forward manner. The video\ndiffusion model is designed to create videos precisely following specified\ncamera trajectories, allowing it to generate compressed video latents that\ncontain multi-view information while maintaining 3D consistency. We train the\n3D reconstruction model to operate on the video latent space with a progressive\ntraining strategy, enabling the efficient generation of high-quality,\nwide-scope, and generic 3D scenes. Extensive evaluations across various\ndatasets demonstrate that our model significantly outperforms existing methods\nfor single-view 3D scene generation, particularly with out-of-domain images.\nFor the first time, we demonstrate that a 3D reconstruction model can be\neffectively built upon the latent space of a diffusion model to realize\nefficient 3D scene generation.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "MxxFoBPvZ0FWtuTEDLKNGcyCvJRc73Rht3CtAvA-SMo",
  "pdfSize": "9329992"
}