{
  "id": "2412.11970",
  "title": "DARWIN 1.5: Large Language Models as Materials Science Adapted Learners",
  "authors": "Tong Xie, Yuwei Wan, Yixuan Liu, Yuchen Zeng, Shaozhou Wang, Wenjie\n  Zhang, Clara Grazian, Chunyu Kit, Wanli Ouyang, Dongzhan Zhou, Bram Hoex",
  "authorsParsed": [
    [
      "Xie",
      "Tong",
      ""
    ],
    [
      "Wan",
      "Yuwei",
      ""
    ],
    [
      "Liu",
      "Yixuan",
      ""
    ],
    [
      "Zeng",
      "Yuchen",
      ""
    ],
    [
      "Wang",
      "Shaozhou",
      ""
    ],
    [
      "Zhang",
      "Wenjie",
      ""
    ],
    [
      "Grazian",
      "Clara",
      ""
    ],
    [
      "Kit",
      "Chunyu",
      ""
    ],
    [
      "Ouyang",
      "Wanli",
      ""
    ],
    [
      "Zhou",
      "Dongzhan",
      ""
    ],
    [
      "Hoex",
      "Bram",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 16:51:27 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 23 Jan 2025 08:07:41 GMT"
    }
  ],
  "updateDate": "2025-01-24",
  "timestamp": 1734367887000,
  "abstract": "  Materials discovery and design aim to find compositions and structures with\ndesirable properties over highly complex and diverse physical spaces.\nTraditional solutions, such as high-throughput simulations or machine learning,\noften rely on complex descriptors, which hinder generalizability and\ntransferability across different material systems. Moreover, These descriptors\nmay inadequately represent macro-scale material properties, which are\ninfluenced by structural imperfections and compositional variations in\nreal-world samples, thus limiting their practical applicability. To address\nthese challenges, we propose DARWIN 1.5, the largest open-source large language\nmodel tailored for materials science. By leveraging natural language as input,\nDARWIN eliminates the need for task-specific descriptors and enables a\nflexible, unified approach to material property prediction and discovery. Our\napproach integrates 6M material domain papers and 21 experimental datasets from\n49,256 materials across modalities while enabling cross-task knowledge\ntransfer. The enhanced model achieves up to 59.1% improvement in prediction\naccuracy over the base LLaMA-7B architecture and outperforms SOTA machine\nlearning approaches across 8 materials design tasks. These results establish\nLLMs as a promising foundation for developing versatile and scalable models in\nmaterials science.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "BCI2bEliGJDiEsKZmDnhWn088uXfP65w376nUKwsr08",
  "pdfSize": "3706956"
}