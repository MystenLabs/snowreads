{"id":"2407.04191","title":"GazeFusion: Saliency-guided Image Generation","authors":"Yunxiang Zhang, Nan Wu, Connor Z. Lin, Gordon Wetzstein, Qi Sun","authorsParsed":[["Zhang","Yunxiang",""],["Wu","Nan",""],["Lin","Connor Z.",""],["Wetzstein","Gordon",""],["Sun","Qi",""]],"versions":[{"version":"v1","created":"Sat, 16 Mar 2024 21:01:35 GMT"}],"updateDate":"2024-07-08","timestamp":1710622895000,"abstract":"  Diffusion models offer unprecedented image generation capabilities given just\na text prompt. While emerging control mechanisms have enabled users to specify\nthe desired spatial arrangements of the generated content, they cannot predict\nor control where viewers will pay more attention due to the complexity of human\nvision. Recognizing the critical necessity of attention-controllable image\ngeneration in practical applications, we present a saliency-guided framework to\nincorporate the data priors of human visual attention into the generation\nprocess. Given a desired viewer attention distribution, our control module\nconditions a diffusion model to generate images that attract viewers' attention\ntoward desired areas. To assess the efficacy of our approach, we performed an\neye-tracked user study and a large-scale model-based saliency analysis. The\nresults evidence that both the cross-user eye gaze distributions and the\nsaliency model predictions align with the desired attention distributions.\nLastly, we outline several applications, including interactive design of\nsaliency guidance, attention suppression in unwanted regions, and adaptive\ngeneration for varied display/viewing conditions.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Graphics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Or8xzIAVieBoUBpqL8GWHNfOmKPZVhFStnvmMNFfkwE","pdfSize":"12447297"}