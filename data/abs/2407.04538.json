{"id":"2407.04538","title":"PDiscoFormer: Relaxing Part Discovery Constraints with Vision\n  Transformers","authors":"Ananthu Aniraj, Cassio F.Dantas, Dino Ienco, Diego Marcos","authorsParsed":[["Aniraj","Ananthu",""],["Dantas","Cassio F.",""],["Ienco","Dino",""],["Marcos","Diego",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 14:24:37 GMT"},{"version":"v2","created":"Mon, 8 Jul 2024 14:44:06 GMT"},{"version":"v3","created":"Mon, 22 Jul 2024 09:41:39 GMT"}],"updateDate":"2024-07-23","timestamp":1720189477000,"abstract":"  Computer vision methods that explicitly detect object parts and reason on\nthem are a step towards inherently interpretable models. Existing approaches\nthat perform part discovery driven by a fine-grained classification task make\nvery restrictive assumptions on the geometric properties of the discovered\nparts; they should be small and compact. Although this prior is useful in some\ncases, in this paper we show that pre-trained transformer-based vision models,\nsuch as self-supervised DINOv2 ViT, enable the relaxation of these constraints.\nIn particular, we find that a total variation (TV) prior, which allows for\nmultiple connected components of any size, substantially outperforms previous\nwork. We test our approach on three fine-grained classification benchmarks:\nCUB, PartImageNet and Oxford Flowers, and compare our results to previously\npublished methods as well as a re-implementation of the state-of-the-art method\nPDiscoNet with a transformer-based backbone. We consistently obtain substantial\nimprovements across the board, both on part discovery metrics and the\ndownstream classification task, showing that the strong inductive biases in\nself-supervised ViT models require to rethink the geometric priors that can be\nused for unsupervised part discovery.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HKIZqHmLsOEw04yMKqIX1b3vL8aF2Ghv6lyu2rOFFRI","pdfSize":"10037515"}