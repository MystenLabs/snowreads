{"id":"2407.04215","title":"T2IShield: Defending Against Backdoors on Text-to-Image Diffusion Models","authors":"Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen","authorsParsed":[["Wang","Zhongqi",""],["Zhang","Jie",""],["Shan","Shiguang",""],["Chen","Xilin",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 01:53:21 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 11:19:16 GMT"}],"updateDate":"2024-07-18","timestamp":1720144401000,"abstract":"  While text-to-image diffusion models demonstrate impressive generation\ncapabilities, they also exhibit vulnerability to backdoor attacks, which\ninvolve the manipulation of model outputs through malicious triggers. In this\npaper, for the first time, we propose a comprehensive defense method named\nT2IShield to detect, localize, and mitigate such attacks. Specifically, we find\nthe \"Assimilation Phenomenon\" on the cross-attention maps caused by the\nbackdoor trigger. Based on this key insight, we propose two effective backdoor\ndetection methods: Frobenius Norm Threshold Truncation and Covariance\nDiscriminant Analysis. Besides, we introduce a binary-search approach to\nlocalize the trigger within a backdoor sample and assess the efficacy of\nexisting concept editing methods in mitigating backdoor attacks. Empirical\nevaluations on two advanced backdoor attack scenarios show the effectiveness of\nour proposed defense method. For backdoor sample detection, T2IShield achieves\na detection F1 score of 88.9$\\%$ with low computational cost. Furthermore,\nT2IShield achieves a localization F1 score of 86.4$\\%$ and invalidates 99$\\%$\npoisoned samples. Codes are released at https://github.com/Robin-WZQ/T2IShield.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wwHRyc489zScAsYeJA8neuy0LAr7AbJjJ27Frwy7K1E","pdfSize":"9430877"}