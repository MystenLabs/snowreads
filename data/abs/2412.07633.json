{"id":"2412.07633","title":"ChocoLlama: Lessons Learned From Teaching Llamas Dutch","authors":"Matthieu Meeus, Anthony Rath\\'e, Fran\\c{c}ois Remy, Pieter Delobelle,\n  Jens-Joris Decorte, Thomas Demeester","authorsParsed":[["Meeus","Matthieu",""],["Rathé","Anthony",""],["Remy","François",""],["Delobelle","Pieter",""],["Decorte","Jens-Joris",""],["Demeester","Thomas",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 16:13:58 GMT"}],"updateDate":"2024-12-11","timestamp":1733847238000,"abstract":"  While Large Language Models (LLMs) have shown remarkable capabilities in\nnatural language understanding and generation, their performance often lags in\nlower-resource, non-English languages due to biases in the training data. In\nthis work, we explore strategies for adapting the primarily English LLMs\n(Llama-2 and Llama-3) to Dutch, a language spoken by 30 million people\nworldwide yet often underrepresented in LLM development. We collect 104GB of\nDutch text ($32$B tokens) from various sources to first apply continued\npretraining using low-rank adaptation (LoRA), complemented with Dutch\nposttraining strategies provided by prior work. For Llama-2, we consider using\n(i) the tokenizer of the original model, and (ii) training a new,\nDutch-specific tokenizer combined with embedding reinitialization. We evaluate\nour adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutch\nbenchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectively\nscale for language adaptation, and that tokenizer modification with careful\nweight reinitialization can improve performance. Notably, Llama-3 was released\nduring the course of this project and, upon evaluation, demonstrated superior\nDutch capabilities compared to our Dutch-adapted versions of Llama-2. We hence\napply the same adaptation technique to Llama-3, using its original tokenizer.\nWhile our adaptation methods enhanced Llama-2's Dutch capabilities, we found\nlimited gains when applying the same techniques to Llama-3. This suggests that\nfor ever improving, multilingual foundation models, language adaptation\ntechniques may benefit more from focusing on language-specific posttraining\nrather than on continued pretraining. We hope this work contributes to the\nbroader understanding of adapting LLMs to lower-resource languages, and to the\ndevelopment of Dutch LLMs in particular.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2vUx7ifBGbbWnrCZaqfIzrbWJdizQgrTU9sJpyM4DjY","pdfSize":"1100405"}