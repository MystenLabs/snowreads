{"id":"2407.21368","title":"Prompting Medical Large Vision-Language Models to Diagnose Pathologies\n  by Visual Question Answering","authors":"Danfeng Guo and Demetri Terzopoulos","authorsParsed":[["Guo","Danfeng",""],["Terzopoulos","Demetri",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 06:34:38 GMT"}],"updateDate":"2024-08-01","timestamp":1722407678000,"abstract":"  Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Vpu4ptm1FdV0Vo4FOJl0r1ECZS60y4dMoTmDCtx9SgY","pdfSize":"5046299","objectId":"0x54c3d07a0f44bb77a38b896895f79b4361a9dd00b7bbdf8d1b0bea27c0f2c954","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
