{"id":"2407.06329","title":"Solving Multi-Model MDPs by Coordinate Ascent and Dynamic Programming","authors":"Xihong Su, Marek Petrik","authorsParsed":[["Su","Xihong",""],["Petrik","Marek",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 18:47:59 GMT"}],"updateDate":"2024-07-10","timestamp":1720464479000,"abstract":"  Multi-model Markov decision process (MMDP) is a promising framework for\ncomputing policies that are robust to parameter uncertainty in MDPs. MMDPs aim\nto find a policy that maximizes the expected return over a distribution of MDP\nmodels. Because MMDPs are NP-hard to solve, most methods resort to\napproximations. In this paper, we derive the policy gradient of MMDPs and\npropose CADP, which combines a coordinate ascent method and a dynamic\nprogramming algorithm for solving MMDPs. The main innovation of CADP compared\nwith earlier algorithms is to take the coordinate ascent perspective to adjust\nmodel weights iteratively to guarantee monotone policy improvements to a local\nmaximum. A theoretical analysis of CADP proves that it never performs worse\nthan previous dynamic programming algorithms like WSU. Our numerical results\nindicate that CADP substantially outperforms existing methods on several\nbenchmark problems.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"C1rLDz4xtUuMs1vXn4CDHKE_MIuBNiFdMzVlGK4pQb0","pdfSize":"1042915"}