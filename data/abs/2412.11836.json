{"id":"2412.11836","title":"UnMA-CapSumT: Unified and Multi-Head Attention-driven Caption\n  Summarization Transformer","authors":"Dhruv Sharma, Chhavi Dhiman, Dinesh Kumar","authorsParsed":[["Sharma","Dhruv",""],["Dhiman","Chhavi",""],["Kumar","Dinesh",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 14:57:40 GMT"}],"updateDate":"2024-12-17","timestamp":1734361060000,"abstract":"  Image captioning is the generation of natural language descriptions of images\nwhich have increased immense popularity in the recent past. With this different\ndeep-learning techniques are devised for the development of factual and\nstylized image captioning models. Previous models focused more on the\ngeneration of factual and stylized captions separately providing more than one\ncaption for a single image. The descriptions generated from these suffer from\nout-of-vocabulary and repetition issues. To the best of our knowledge, no such\nwork exists that provided a description that integrates different captioning\nmethods to describe the contents of an image with factual and stylized\n(romantic and humorous) elements. To overcome these limitations, this paper\npresents a novel Unified Attention and Multi-Head Attention-driven Caption\nSummarization Transformer (UnMA-CapSumT) based Captioning Framework. It\nutilizes both factual captions and stylized captions generated by the Modified\nAdaptive Attention-based factual image captioning model (MAA-FIC) and Style\nFactored Bi-LSTM with attention (SF-Bi-ALSTM) driven stylized image captioning\nmodel respectively. SF-Bi-ALSTM-based stylized IC model generates two prominent\nstyles of expression- {romance, and humor}. The proposed summarizer UnMHA-ST\ncombines both factual and stylized descriptions of an input image to generate\nstyled rich coherent summarized captions. The proposed UnMHA-ST transformer\nlearns and summarizes different linguistic styles efficiently by incorporating\nproposed word embedding fastText with Attention Word Embedding (fTA-WE) and\npointer-generator network with coverage mechanism concept to solve the\nout-of-vocabulary issues and repetition problem. Extensive experiments are\nconducted on Flickr8K and a subset of FlickrStyle10K with supporting ablation\nstudies to prove the efficiency and efficacy of the proposed framework.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"BQniqcAZXlTHCZPRQOWau5lwTQGpQef2Q6XGqCAuV5o","pdfSize":"772955"}