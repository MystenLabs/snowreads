{"id":"2407.17468","title":"WildHallucinations: Evaluating Long-form Factuality in LLMs with\n  Real-World Entity Queries","authors":"Wenting Zhao, Tanya Goyal, Yu Ying Chiu, Liwei Jiang, Benjamin Newman,\n  Abhilasha Ravichander, Khyathi Chandu, Ronan Le Bras, Claire Cardie, Yuntian\n  Deng, Yejin Choi","authorsParsed":[["Zhao","Wenting",""],["Goyal","Tanya",""],["Chiu","Yu Ying",""],["Jiang","Liwei",""],["Newman","Benjamin",""],["Ravichander","Abhilasha",""],["Chandu","Khyathi",""],["Bras","Ronan Le",""],["Cardie","Claire",""],["Deng","Yuntian",""],["Choi","Yejin",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 17:59:05 GMT"}],"updateDate":"2024-07-25","timestamp":1721843945000,"abstract":"  While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"v-dYvwLYcgJj3k0GPB9J8rokH_4NLpgokNmmzAXKtPs","pdfSize":"1121221"}