{"id":"2407.09702","title":"Investigating the Interplay of Prioritized Replay and Generalization","authors":"Parham Mohammad Panahi, Andrew Patterson, Martha White, Adam White","authorsParsed":[["Panahi","Parham Mohammad",""],["Patterson","Andrew",""],["White","Martha",""],["White","Adam",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 21:56:24 GMT"}],"updateDate":"2024-07-16","timestamp":1720821384000,"abstract":"  Experience replay is ubiquitous in reinforcement learning, to reuse past data\nand improve sample efficiency. Though a variety of smart sampling schemes have\nbeen introduced to improve performance, uniform sampling by far remains the\nmost common approach. One exception is Prioritized Experience Replay (PER),\nwhere sampling is done proportionally to TD errors, inspired by the success of\nprioritized sweeping in dynamic programming. The original work on PER showed\nimprovements in Atari, but follow-up results are mixed. In this paper, we\ninvestigate several variations on PER, to attempt to understand where and when\nPER may be useful. Our findings in prediction tasks reveal that while PER can\nimprove value propagation in tabular settings, behavior is significantly\ndifferent when combined with neural networks. Certain mitigations -- like\ndelaying target network updates to control generalization and using estimates\nof expected TD errors in PER to avoid chasing stochasticity -- can avoid large\nspikes in error with PER and neural networks, but nonetheless generally do not\noutperform uniform replay. In control tasks, none of the prioritized variants\nconsistently outperform uniform replay.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wbreljMmt1h6uyF5H8WBFVA6Ooc0w449FgjOWb7XlZs","pdfSize":"9947984"}