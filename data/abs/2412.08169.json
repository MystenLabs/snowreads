{
  "id": "2412.08169",
  "title": "Illusory VQA: Benchmarking and Enhancing Multimodal Models on Visual\n  Illusions",
  "authors": "Mohammadmostafa Rostamkhani, Baktash Ansari, Hoorieh Sabzevari, Farzan\n  Rahmani, Sauleh Eetemadi",
  "authorsParsed": [
    [
      "Rostamkhani",
      "Mohammadmostafa",
      ""
    ],
    [
      "Ansari",
      "Baktash",
      ""
    ],
    [
      "Sabzevari",
      "Hoorieh",
      ""
    ],
    [
      "Rahmani",
      "Farzan",
      ""
    ],
    [
      "Eetemadi",
      "Sauleh",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 07:51:18 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733903478000,
  "abstract": "  In recent years, Visual Question Answering (VQA) has made significant\nstrides, particularly with the advent of multimodal models that integrate\nvision and language understanding. However, existing VQA datasets often\noverlook the complexities introduced by image illusions, which pose unique\nchallenges for both human perception and model interpretation. In this study,\nwe introduce a novel task called Illusory VQA, along with four specialized\ndatasets: IllusionMNIST, IllusionFashionMNIST, IllusionAnimals, and\nIllusionChar. These datasets are designed to evaluate the performance of\nstate-of-the-art multimodal models in recognizing and interpreting visual\nillusions. We assess the zero-shot performance of various models, fine-tune\nselected models on our datasets, and propose a simple yet effective solution\nfor illusion detection using Gaussian and blur low-pass filters. We show that\nthis method increases the performance of models significantly and in the case\nof BLIP-2 on IllusionAnimals without any fine-tuning, it outperforms humans.\nOur findings highlight the disparity between human and model perception of\nillusions and demonstrate that fine-tuning and specific preprocessing\ntechniques can significantly enhance model robustness. This work contributes to\nthe development of more human-like visual understanding in multimodal models\nand suggests future directions for adapting filters using learnable parameters.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "NdP9ui0a0Uc_T9vsutSa3pY_zlOJPQopyuOBVU_n-BI",
  "pdfSize": "7818966"
}