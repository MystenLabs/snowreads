{"id":"2407.20601","title":"Investigating Sparsity in Recurrent Neural Networks","authors":"Harshil Darji","authorsParsed":[["Darji","Harshil",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 07:24:58 GMT"}],"updateDate":"2024-07-31","timestamp":1722324298000,"abstract":"  In the past few years, neural networks have evolved from simple Feedforward\nNeural Networks to more complex neural networks, such as Convolutional Neural\nNetworks and Recurrent Neural Networks. Where CNNs are a perfect fit for tasks\nwhere the sequence is not important such as image recognition, RNNs are useful\nwhen order is important such as machine translation. An increasing number of\nlayers in a neural network is one way to improve its performance, but it also\nincreases its complexity making it much more time and power-consuming to train.\nOne way to tackle this problem is to introduce sparsity in the architecture of\nthe neural network. Pruning is one of the many methods to make a neural network\narchitecture sparse by clipping out weights below a certain threshold while\nkeeping the performance near to the original. Another way is to generate\narbitrary structures using random graphs and embed them between an input and\noutput layer of an Artificial Neural Network. Many researchers in past years\nhave focused on pruning mainly CNNs, while hardly any research is done for the\nsame in RNNs. The same also holds in creating sparse architectures for RNNs by\ngenerating and embedding arbitrary structures. Therefore, this thesis focuses\non investigating the effects of the before-mentioned two techniques on the\nperformance of RNNs. We first describe the pruning of RNNs, its impact on the\nperformance of RNNs, and the number of training epochs required to regain\naccuracy after the pruning is performed. Next, we continue with the creation\nand training of Sparse Recurrent Neural Networks and identify the relation\nbetween the performance and the graph properties of its underlying arbitrary\nstructure. We perform these experiments on RNN with Tanh nonlinearity\n(RNN-Tanh), RNN with ReLU nonlinearity (RNN-ReLU), GRU, and LSTM. Finally, we\nanalyze and discuss the results achieved from both the experiments.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_QiMT864OgbhJG9djBbiU300rRCxTInTawPU1-wvjpc","pdfSize":"5096116","objectId":"0x136300f47b6063e35e88a16b03609c01d7c947fca7cd9543339ce3b8664db95c","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
