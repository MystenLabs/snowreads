{"id":"2407.02218","title":"Multi-Modal Video Dialog State Tracking in the Wild","authors":"Adnen Abdessaied, Lei Shi, Andreas Bulling","authorsParsed":[["Abdessaied","Adnen",""],["Shi","Lei",""],["Bulling","Andreas",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 12:34:17 GMT"},{"version":"v2","created":"Fri, 5 Jul 2024 08:55:00 GMT"}],"updateDate":"2024-07-08","timestamp":1719923657000,"abstract":"  We present MST-MIXER - a novel video dialog model operating over a generic\nmulti-modal state tracking scheme. Current models that claim to perform\nmulti-modal state tracking fall short of two major aspects: (1) They either\ntrack only one modality (mostly the visual input) or (2) they target synthetic\ndatasets that do not reflect the complexity of real-world in the wild\nscenarios. Our model addresses these two limitations in an attempt to close\nthis crucial research gap. Specifically, MST-MIXER first tracks the most\nimportant constituents of each input modality. Then, it predicts the missing\nunderlying structure of the selected constituents of each modality by learning\nlocal latent graphs using a novel multi-modal graph structure learning method.\nSubsequently, the learned local graphs and features are parsed together to form\na global graph operating on the mix of all modalities which further refines its\nstructure and node embeddings. Finally, the fine-grained graph node features\nare used to enhance the hidden states of the backbone Vision-Language Model\n(VLM). MST-MIXER achieves new state-of-the-art results on five challenging\nbenchmarks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"YuB7xNzRnAu6U5Rp-fJ2nmkU_V38xtNNsxlWT3uLFPE","pdfSize":"16254286"}