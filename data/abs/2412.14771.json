{
  "id": "2412.14771",
  "title": "ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine",
  "authors": "Rabee Qasem, Mohannad Hendi, Banan Tantour",
  "authorsParsed": [
    [
      "Qasem",
      "Rabee",
      ""
    ],
    [
      "Hendi",
      "Mohannad",
      ""
    ],
    [
      "Tantour",
      "Banan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 19 Dec 2024 11:55:51 GMT"
    }
  ],
  "updateDate": "2024-12-20",
  "timestamp": 1734609351000,
  "abstract": "  Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "59HvpdFRWJEAl4EcDQ3lFIovosHKbnv3ahj333VRpkA",
  "pdfSize": "1457059"
}