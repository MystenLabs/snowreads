{"id":"2412.12194","title":"BlockDoor: Blocking Backdoor Based Watermarks in Deep Neural Networks","authors":"Yi Hao Puah, Anh Tu Ngo, Nandish Chattopadhyay, Anupam Chattopadhyay","authorsParsed":[["Puah","Yi Hao",""],["Ngo","Anh Tu",""],["Chattopadhyay","Nandish",""],["Chattopadhyay","Anupam",""]],"versions":[{"version":"v1","created":"Sat, 14 Dec 2024 06:38:01 GMT"}],"updateDate":"2025-01-07","timestamp":1734158281000,"abstract":"  Adoption of machine learning models across industries have turned Neural\nNetworks (DNNs) into a prized Intellectual Property (IP), which needs to be\nprotected from being stolen or being used without authorization. This topic\ngave rise to multiple watermarking schemes, through which, one can establish\nthe ownership of a model. Watermarking using backdooring is the most well\nestablished method available in the literature, with specific works\ndemonstrating the difficulty in removing the watermarks, embedded as backdoors\nwithin the weights of the network. However, in our work, we have identified a\ncritical flaw in the design of the watermark verification with backdoors,\npertaining to the behaviour of the samples of the Trigger Set, which acts as\nthe secret key. In this paper, we present BlockDoor, which is a comprehensive\npackage of techniques that is used as a wrapper to block all three different\nkinds of Trigger samples, which are used in the literature as means to embed\nwatermarks within the trained neural networks as backdoors. The framework\nimplemented through BlockDoor is able to detect potential Trigger samples,\nthrough separate functions for adversarial noise based triggers,\nout-of-distribution triggers and random label based triggers. Apart from a\nsimple Denial-of-Service for a potential Trigger sample, our approach is also\nable to modify the Trigger samples for correct machine learning functionality.\nExtensive evaluation of BlockDoor establishes that it is able to significantly\nreduce the watermark validation accuracy of the Trigger set by up to $98\\%$\nwithout compromising on functionality, delivering up to a less than $1\\%$ drop\non the clean samples. BlockDoor has been tested on multiple datasets and neural\narchitectures.\n","subjects":["Computer Science/Cryptography and Security","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Nx6R3rPgGi6EGAEUC2jYT_zSMWfOQoRukopc8e4utYg","pdfSize":"1288773"}