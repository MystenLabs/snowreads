{"id":"2407.12029","title":"A Quality-Aware Voltage Overscaling Framework to Improve the Energy\n  Efficiency and Lifetime of TPUs based on Statistical Error Modeling","authors":"Alireza Senobari, Jafar Vafaei, Omid Akbari, Christian Hochberger,\n  Muhammad Shafique","authorsParsed":[["Senobari","Alireza",""],["Vafaei","Jafar",""],["Akbari","Omid",""],["Hochberger","Christian",""],["Shafique","Muhammad",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 06:22:39 GMT"}],"updateDate":"2024-07-18","timestamp":1719642159000,"abstract":"  Deep neural networks (DNNs) are a type of artificial intelligence models that\nare inspired by the structure and function of the human brain, designed to\nprocess and learn from large amounts of data, making them particularly\nwell-suited for tasks such as image and speech recognition. However,\napplications of DNNs are experiencing emerging growth due to the deployment of\nspecialized accelerators such as the Google Tensor Processing Units (TPUs). In\nlarge-scale deployments, the energy efficiency of such accelerators may become\na critical concern. In the voltage overscaling (VOS) technique, the operating\nvoltage of the system is scaled down beyond the nominal operating voltage,\nwhich increases the energy efficiency and lifetime of digital circuits. The VOS\ntechnique is usually performed without changing the frequency resulting in\ntiming errors. However, some applications such as multimedia processing,\nincluding DNNs, have intrinsic resilience against errors and noise. In this\npaper, we exploit the inherent resilience of DNNs to propose a quality-aware\nvoltage overscaling framework for TPUs, named X-TPU, which offers higher energy\nefficiency and lifetime compared to conventional TPUs. The X-TPU framework is\ncomposed of two main parts, a modified TPU architecture that supports a runtime\nvoltage overscaling, and a statistical error modeling-based algorithm to\ndetermine the voltage of neurons such that the output quality is retained above\na given user-defined quality threshold. We synthesized a single-neuron\narchitecture using a 15-nm FinFET technology under various operating voltage\nlevels. Then, we extracted different statistical error models for a neuron\ncorresponding to those voltage levels. Using these models and the proposed\nalgorithm, we determined the appropriate voltage of each neuron. Results show\nthat running a DNN on X-TPU can achieve 32% energy saving for only 0.6%\naccuracy loss.\n","subjects":["Computing Research Repository/Hardware Architecture"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"8lONzyDQembKYP8njmNjL-IKH27GNWxk4LlB7jP33nQ","pdfSize":"1659556","objectId":"0x81786316561f2786b8684dcbe21f7ac494a1d4d6b40dc27de3eb61fa24fc860d","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
