{"id":"2407.04105","title":"Can Pre-trained Language Models Understand Chinese Humor?","authors":"Yuyan Chen, Zhixu Li, Jiaqing Liang, Yanghua Xiao, Bang Liu, Yunwen\n  Chen","authorsParsed":[["Chen","Yuyan",""],["Li","Zhixu",""],["Liang","Jiaqing",""],["Xiao","Yanghua",""],["Liu","Bang",""],["Chen","Yunwen",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 18:13:38 GMT"}],"updateDate":"2024-07-08","timestamp":1720116818000,"abstract":"  Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cxZdsmCNv_xETt_YmSOs9cMIPbSOolRgCXT2Aja8Bkc","pdfSize":"6089360"}
