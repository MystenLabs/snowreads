{
  "id": "2412.10348",
  "title": "A dual contrastive framework",
  "authors": "Yuan Sun, Zhao Zhang, Jorge Ortiz",
  "authorsParsed": [
    [
      "Sun",
      "Yuan",
      ""
    ],
    [
      "Zhang",
      "Zhao",
      ""
    ],
    [
      "Ortiz",
      "Jorge",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 18:45:18 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734115518000,
  "abstract": "  In current multimodal tasks, models typically freeze the encoder and decoder\nwhile adapting intermediate layers to task-specific goals, such as region\ncaptioning. Region-level visual understanding presents significant challenges\nfor large-scale vision-language models. While limited spatial awareness is a\nknown issue, coarse-grained pretraining, in particular, exacerbates the\ndifficulty of optimizing latent representations for effective encoder-decoder\nalignment. We propose AlignCap, a framework designed to enhance region-level\nunderstanding through fine-grained alignment of latent spaces. Our approach\nintroduces a novel latent feature refinement module that enhances conditioned\nlatent space representations to improve region-level captioning performance. We\nalso propose an innovative alignment strategy, the semantic space alignment\nmodule, which boosts the quality of multimodal representations. Additionally,\nwe incorporate contrastive learning in a novel manner within both modules to\nfurther enhance region-level captioning performance. To address spatial\nlimitations, we employ a General Object Detection (GOD) method as a data\npreprocessing pipeline that enhances spatial reasoning at the regional level.\nExtensive experiments demonstrate that our approach significantly improves\nregion-level captioning performance across various tasks\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "KPs_l1NkZO3ZI6tm6GggdKqKpbid0U8ozaYhBIS_OMg",
  "pdfSize": "1365566"
}