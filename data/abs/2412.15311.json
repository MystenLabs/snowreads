{"id":"2412.15311","title":"Re-evaluating Group Robustness via Adaptive Class-Specific Scaling","authors":"Seonguk Seo, Bohyung Han","authorsParsed":[["Seo","Seonguk",""],["Han","Bohyung",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 16:01:51 GMT"}],"updateDate":"2024-12-23","timestamp":1734624111000,"abstract":"  Group distributionally robust optimization, which aims to improve robust\naccuracies -- worst-group and unbiased accuracies -- is a prominent algorithm\nused to mitigate spurious correlations and address dataset bias. Although\nexisting approaches have reported improvements in robust accuracies, these\ngains often come at the cost of average accuracy due to inherent trade-offs. To\ncontrol this trade-off flexibly and efficiently, we propose a simple\nclass-specific scaling strategy, directly applicable to existing debiasing\nalgorithms with no additional training. We further develop an instance-wise\nadaptive scaling technique to alleviate this trade-off, even leading to\nimprovements in both robust and average accuracies. Our approach reveals that a\nna\\\"ive ERM baseline matches or even outperforms the recent debiasing methods\nby simply adopting the class-specific scaling technique. Additionally, we\nintroduce a novel unified metric that quantifies the trade-off between the two\naccuracies as a scalar value, allowing for a comprehensive evaluation of\nexisting algorithms. By tackling the inherent trade-off and offering a\nperformance landscape, our approach provides valuable insights into robust\ntechniques beyond just robust accuracy. We validate the effectiveness of our\nframework through experiments across datasets in computer vision and natural\nlanguage processing domains.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LSfN5n4mvhU1rpwETpFPcbHiGCAEPQglF6srakU2b-g","pdfSize":"1120930"}