{"id":"2412.02101","title":"Improving Language Transfer Capability of Decoder-only Architecture in\n  Multilingual Neural Machine Translation","authors":"Zhi Qu, Yiran Wang, Chenchen Ding, Hideki Tanaka, Masao Utiyama, Taro\n  Watanabe","authorsParsed":[["Qu","Zhi",""],["Wang","Yiran",""],["Ding","Chenchen",""],["Tanaka","Hideki",""],["Utiyama","Masao",""],["Watanabe","Taro",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 02:52:14 GMT"}],"updateDate":"2024-12-04","timestamp":1733194334000,"abstract":"  Existing multilingual neural machine translation (MNMT) approaches mainly\nfocus on improving models with the encoder-decoder architecture to translate\nmultiple languages. However, decoder-only architecture has been explored less\nin MNMT due to its underperformance when trained on parallel data solely. In\nthis work, we attribute the issue of the decoder-only architecture to its lack\nof language transfer capability. Specifically, the decoder-only architecture is\ninsufficient in encoding source tokens with the target language features. We\npropose dividing the decoding process into two stages so that target tokens are\nexplicitly excluded in the first stage to implicitly boost the transfer\ncapability across languages. Additionally, we impose contrastive learning on\ntranslation instructions, resulting in improved performance in zero-shot\ntranslation. We conduct experiments on TED-19 and OPUS-100 datasets,\nconsidering both training from scratch and fine-tuning scenarios. Experimental\nresults show that, compared to the encoder-decoder architecture, our methods\nnot only perform competitively in supervised translations but also achieve\nimprovements of up to 3.39 BLEU, 6.99 chrF++, 3.22 BERTScore, and 4.81 COMET in\nzero-shot translations.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FC3JSO5AilCA2ilte1_w_9l6ASkKXmSAxKdZLVH4U4E","pdfSize":"845361"}