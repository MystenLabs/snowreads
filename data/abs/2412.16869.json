{
  "id": "2412.16869",
  "title": "CoF: Coarse to Fine-Grained Image Understanding for Multi-modal Large\n  Language Models",
  "authors": "Yeyuan Wang, Dehong Gao, Bin Li, Rujiao Long, Lei Yi, Xiaoyan Cai,\n  Libin Yang, Jinxia Zhang, Shanqing Yu, Qi Xuan",
  "authorsParsed": [
    [
      "Wang",
      "Yeyuan",
      ""
    ],
    [
      "Gao",
      "Dehong",
      ""
    ],
    [
      "Li",
      "Bin",
      ""
    ],
    [
      "Long",
      "Rujiao",
      ""
    ],
    [
      "Yi",
      "Lei",
      ""
    ],
    [
      "Cai",
      "Xiaoyan",
      ""
    ],
    [
      "Yang",
      "Libin",
      ""
    ],
    [
      "Zhang",
      "Jinxia",
      ""
    ],
    [
      "Yu",
      "Shanqing",
      ""
    ],
    [
      "Xuan",
      "Qi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 22 Dec 2024 05:42:40 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734846160000,
  "abstract": "  The impressive performance of Large Language Model (LLM) has prompted\nresearchers to develop Multi-modal LLM (MLLM), which has shown great potential\nfor various multi-modal tasks. However, current MLLM often struggles to\neffectively address fine-grained multi-modal challenges. We argue that this\nlimitation is closely linked to the models' visual grounding capabilities. The\nrestricted spatial awareness and perceptual acuity of visual encoders\nfrequently lead to interference from irrelevant background information in\nimages, causing the models to overlook subtle but crucial details. As a result,\nachieving fine-grained regional visual comprehension becomes difficult. In this\npaper, we break down multi-modal understanding into two stages, from Coarse to\nFine (CoF). In the first stage, we prompt the MLLM to locate the approximate\narea of the answer. In the second stage, we further enhance the model's focus\non relevant areas within the image through visual prompt engineering, adjusting\nattention weights of pertinent regions. This, in turn, improves both visual\ngrounding and overall performance in downstream tasks. Our experiments show\nthat this approach significantly boosts the performance of baseline models,\ndemonstrating notable generalization and effectiveness. Our CoF approach is\navailable online at https://github.com/Gavin001201/CoF.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "PWh7lat_3BPbSsqIUNFOV6k-Kpg4XnjVCWxlUcRYrZw",
  "pdfSize": "923416"
}