{"id":"2412.07334","title":"Frame Representation Hypothesis: Multi-Token LLM Interpretability and\n  Concept-Guided Text Generation","authors":"Pedro H. V. Valois, Lincon S. Souza, Erica K. Shimomoto, Kazuhiro\n  Fukui","authorsParsed":[["Valois","Pedro H. V.",""],["Souza","Lincon S.",""],["Shimomoto","Erica K.",""],["Fukui","Kazuhiro",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 09:25:39 GMT"},{"version":"v2","created":"Thu, 12 Dec 2024 05:01:04 GMT"}],"updateDate":"2024-12-13","timestamp":1733822739000,"abstract":"  Interpretability is a key challenge in fostering trust for Large Language\nModels (LLMs), which stems from the complexity of extracting reasoning from\nmodel's parameters. We present the Frame Representation Hypothesis, a\ntheoretically robust framework grounded in the Linear Representation Hypothesis\n(LRH) to interpret and control LLMs by modeling multi-token words. Prior\nresearch explored LRH to connect LLM representations with linguistic concepts,\nbut was limited to single token analysis. As most words are composed of several\ntokens, we extend LRH to multi-token words, thereby enabling usage on any\ntextual data with thousands of concepts. To this end, we propose words can be\ninterpreted as frames, ordered sequences of vectors that better capture\ntoken-word relationships. Then, concepts can be represented as the average of\nword frames sharing a common concept. We showcase these tools through Top-k\nConcept-Guided Decoding, which can intuitively steer text generation using\nconcepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3\nfamilies, demonstrating gender and language biases, exposing harmful content,\nbut also potential to remediate them, leading to safer and more transparent\nLLMs. Code is available at\nhttps://github.com/phvv-me/frame-representation-hypothesis.git\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"___4u96sNVifgkOfCT-fMd3HoRnEINBRrj1jsONdP8U","pdfSize":"2208973"}