{
  "id": "2412.15021",
  "title": "Event-based backpropagation on the neuromorphic platform SpiNNaker2",
  "authors": "Gabriel B\\'ena, Timo Wunderlich, Mahmoud Akl, Bernhard Vogginger,\n  Christian Mayr, Hector Andres Gonzales",
  "authorsParsed": [
    [
      "BÃ©na",
      "Gabriel",
      ""
    ],
    [
      "Wunderlich",
      "Timo",
      ""
    ],
    [
      "Akl",
      "Mahmoud",
      ""
    ],
    [
      "Vogginger",
      "Bernhard",
      ""
    ],
    [
      "Mayr",
      "Christian",
      ""
    ],
    [
      "Gonzales",
      "Hector Andres",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 19 Dec 2024 16:31:42 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 3 Jan 2025 10:07:41 GMT"
    },
    {
      "version": "v3",
      "created": "Tue, 28 Jan 2025 19:40:21 GMT"
    }
  ],
  "updateDate": "2025-01-30",
  "timestamp": 1734625902000,
  "abstract": "  Neuromorphic computing aims to replicate the brain's capabilities for energy\nefficient and parallel information processing, promising a solution to the\nincreasing demand for faster and more efficient computational systems.\nEfficient training of neural networks on neuromorphic hardware requires the\ndevelopment of training algorithms that retain the sparsity of spike-based\ncommunication during training. Here, we report on the first implementation of\nevent-based backpropagation on the SpiNNaker2 neuromorphic hardware platform.\nWe use EventProp, an algorithm for event-based backpropagation in spiking\nneural networks (SNNs), to compute exact gradients using sparse communication\nof error signals between neurons. Our implementation computes multi-layer\nnetworks of leaky integrate-and-fire neurons using discretized versions of the\ndifferential equations and their adjoints, and uses event packets to transmit\nspikes and error signals between network layers. We demonstrate a\nproof-of-concept of batch-parallelized, on-chip training of SNNs using the Yin\nYang dataset, and provide an off-chip implementation for efficient prototyping,\nhyper-parameter search, and hybrid training methods.\n",
  "subjects": [
    "Computer Science/Neural and Evolutionary Computing",
    "Computer Science/Hardware Architecture",
    "Computer Science/Emerging Technologies"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "N8byqEJ5M6QCIb2yjdAvAwdDKewfx5tis_wmSWV1_3Y",
  "pdfSize": "1029958"
}