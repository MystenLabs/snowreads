{"id":"2412.12500","title":"Beyond Data Quantity: Key Factors Driving Performance in Multilingual\n  Language Models","authors":"Sina Bagheri Nezhad, Ameeta Agrawal, Rhitabrat Pokharel","authorsParsed":[["Nezhad","Sina Bagheri",""],["Agrawal","Ameeta",""],["Pokharel","Rhitabrat",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 03:05:26 GMT"}],"updateDate":"2024-12-18","timestamp":1734404726000,"abstract":"  Multilingual language models (MLLMs) are crucial for handling text across\nvarious languages, yet they often show performance disparities due to\ndifferences in resource availability and linguistic characteristics. While the\nimpact of pre-train data percentage and model size on performance is\nwell-known, our study reveals additional critical factors that significantly\ninfluence MLLM effectiveness. Analyzing a wide range of features, including\ngeographical, linguistic, and resource-related aspects, we focus on the SIB-200\ndataset for classification and the Flores-200 dataset for machine translation,\nusing regression models and SHAP values across 204 languages. Our findings\nidentify token similarity and country similarity as pivotal factors, alongside\npre-train data and model size, in enhancing model performance. Token similarity\nfacilitates cross-lingual transfer, while country similarity highlights the\nimportance of shared cultural and linguistic contexts. These insights offer\nvaluable guidance for developing more equitable and effective multilingual\nlanguage models, particularly for underrepresented languages.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"87O0EX-mK8w7Q-lnO3EHwJQv3ClWDQUm7qoGGuFxpZk","pdfSize":"1592063"}