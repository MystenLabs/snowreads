{"id":"2412.20646","title":"Enhancing Visual Representation for Text-based Person Searching","authors":"Wei Shen and Ming Fang and Yuxia Wang and Jiafeng Xiao and Diping Li\n  and Huangqun Chen and Ling Xu and Weifeng Zhang","authorsParsed":[["Shen","Wei",""],["Fang","Ming",""],["Wang","Yuxia",""],["Xiao","Jiafeng",""],["Li","Diping",""],["Chen","Huangqun",""],["Xu","Ling",""],["Zhang","Weifeng",""]],"versions":[{"version":"v1","created":"Mon, 30 Dec 2024 01:38:14 GMT"}],"updateDate":"2024-12-31","timestamp":1735522694000,"abstract":"  Text-based person search aims to retrieve the matched pedestrians from a\nlarge-scale image database according to the text description. The core\ndifficulty of this task is how to extract effective details from pedestrian\nimages and texts, and achieve cross-modal alignment in a common latent space.\nPrior works adopt image and text encoders pre-trained on unimodal data to\nextract global and local features from image and text respectively, and then\nglobal-local alignment is achieved explicitly. However, these approaches still\nlack the ability of understanding visual details, and the retrieval accuracy is\nstill limited by identity confusion. In order to alleviate the above problems,\nwe rethink the importance of visual features for text-based person search, and\npropose VFE-TPS, a Visual Feature Enhanced Text-based Person Search model. It\nintroduces a pre-trained multimodal backbone CLIP to learn basic multimodal\nfeatures and constructs Text Guided Masked Image Modeling task to enhance the\nmodel's ability of learning local visual details without explicit annotation.\nIn addition, we design Identity Supervised Global Visual Feature Calibration\ntask to guide the model learn identity-aware global visual features. The key\nfinding of our study is that, with the help of our proposed auxiliary tasks,\nthe knowledge embedded in the pre-trained CLIP model can be successfully\nadapted to text-based person search task, and the model's visual understanding\nability is significantly enhanced. Experimental results on three benchmarks\ndemonstrate that our proposed model exceeds the existing approaches, and the\nRank-1 accuracy is significantly improved with a notable margin of about\n$1\\%\\sim9\\%$. Our code can be found at\nhttps://github.com/zhangweifeng1218/VFE_TPS.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"xtuLMpgTq7MJngNL-cjauDlCMD5OxQw5Iqd7CYH1Kcs","pdfSize":"1422943"}