{"id":"2412.14695","title":"Lorentzian Residual Neural Networks","authors":"Neil He, Menglin Yang, Rex Ying","authorsParsed":[["He","Neil",""],["Yang","Menglin",""],["Ying","Rex",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 09:56:01 GMT"},{"version":"v2","created":"Sun, 12 Jan 2025 05:47:25 GMT"}],"updateDate":"2025-01-14","timestamp":1734602161000,"abstract":"  Hyperbolic neural networks have emerged as a powerful tool for modeling\nhierarchical data structures prevalent in real-world datasets. Notably,\nresidual connections, which facilitate the direct flow of information across\nlayers, have been instrumental in the success of deep neural networks. However,\ncurrent methods for constructing hyperbolic residual networks suffer from\nlimitations such as increased model complexity, numerical instability, and\nerrors due to multiple mappings to and from the tangent space. To address these\nlimitations, we introduce LResNet, a novel Lorentzian residual neural network\nbased on the weighted Lorentzian centroid in the Lorentz model of hyperbolic\ngeometry. Our method enables the efficient integration of residual connections\nin Lorentz hyperbolic neural networks while preserving their hierarchical\nrepresentation capabilities. We demonstrate that our method can theoretically\nderive previous methods while offering improved stability, efficiency, and\neffectiveness. Extensive experiments on both graph and vision tasks showcase\nthe superior performance and robustness of our method compared to\nstate-of-the-art Euclidean and hyperbolic alternatives. Our findings highlight\nthe potential of LResNet for building more expressive neural networks in\nhyperbolic embedding space as a generally applicable method to multiple\narchitectures, including CNNs, GNNs, and graph Transformers.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"fqhomi4xvnW_XTRzbqs1qrxQpSy0kmkAWiGG9B2iJx0","pdfSize":"1253271"}