{"id":"2412.16620","title":"A Large-scale Empirical Study on Fine-tuning Large Language Models for\n  Unit Testing","authors":"Ye Shang, Quanjun Zhang, Chunrong Fang, Siqi Gu, Jianyi Zhou, Zhenyu\n  Chen","authorsParsed":[["Shang","Ye",""],["Zhang","Quanjun",""],["Fang","Chunrong",""],["Gu","Siqi",""],["Zhou","Jianyi",""],["Chen","Zhenyu",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 13:28:11 GMT"}],"updateDate":"2024-12-24","timestamp":1734787691000,"abstract":"  Unit testing plays a pivotal role in software development, improving software\nquality and reliability. However, generating effective test cases manually is\ntime-consuming, prompting interest in unit testing research. Recently, Large\nLanguage Models (LLMs) have shown potential in various unit testing tasks,\nincluding test generation, assertion generation, and test evolution, but\nexisting studies are limited in scope and lack a systematic evaluation of the\neffectiveness of LLMs.\n  To bridge this gap, we present a large-scale empirical study on fine-tuning\nLLMs for unit testing. Our study involves three unit testing tasks, five\nbenchmarks, eight evaluation metrics, and 37 popular LLMs across various\narchitectures and sizes, consuming over 3,000 NVIDIA A100 GPU hours. We focus\non three key research questions: (1) the performance of LLMs compared to\nstate-of-the-art methods, (2) the impact of different factors on LLM\nperformance, and (3) the effectiveness of fine-tuning versus prompt\nengineering. Our findings reveal that LLMs outperform existing state-of-the-art\napproaches on all three unit testing tasks across nearly all metrics,\nhighlighting the potential of fine-tuning LLMs in unit testing tasks.\nFurthermore, large-scale, decoder-only models achieve the best results across\ntasks, while encoder-decoder models perform better under the same parameter\nscale. Additionally, the comparison of the performance between fine-tuning and\nprompt engineering approaches reveals the considerable potential capability of\nthe prompt engineering approach in unit testing tasks. We then discuss the\nconcerned issues on the test generation task, including data leakage issues,\nbug detection capabilities, and metrics comparisons. Finally, we further\npinpoint carious practical guidelines for LLM-based approaches to unit testing\ntasks in the near future.\n","subjects":["Computer Science/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bJjw8NjX24ENRkNRV3PsmLw9So-xBadfmSlfamaMvFA","pdfSize":"864325"}