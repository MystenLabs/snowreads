{"id":"2407.04449","title":"Multi-modal Masked Siamese Network Improves Chest X-Ray Representation\n  Learning","authors":"Saeed Shurrab, Alejandro Guerra-Manzanares, Farah E. Shamout","authorsParsed":[["Shurrab","Saeed",""],["Guerra-Manzanares","Alejandro",""],["Shamout","Farah E.",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 12:04:12 GMT"}],"updateDate":"2024-07-08","timestamp":1720181052000,"abstract":"  Self-supervised learning methods for medical images primarily rely on the\nimaging modality during pretraining. While such approaches deliver promising\nresults, they do not leverage associated patient or scan information collected\nwithin Electronic Health Records (EHR). Here, we propose to incorporate EHR\ndata during self-supervised pretraining with a Masked Siamese Network (MSN) to\nenhance the quality of chest X-ray representations. We investigate three types\nof EHR data, including demographic, scan metadata, and inpatient stay\ninformation. We evaluate our approach on three publicly available chest X-ray\ndatasets, MIMIC-CXR, CheXpert, and NIH-14, using two vision transformer (ViT)\nbackbones, specifically ViT-Tiny and ViT-Small. In assessing the quality of the\nrepresentations via linear evaluation, our proposed method demonstrates\nsignificant improvement compared to vanilla MSN and state-of-the-art\nself-supervised learning baselines. Our work highlights the potential of\nEHR-enhanced self-supervised pre-training for medical imaging. The code is\npublicly available at: https://github.com/nyuad-cai/CXR-EHR-MSN\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1Gb_qvhNAyY0uMbHmxU4h0SEG93CD8aVunPwx4RQTxg","pdfSize":"12803312"}
