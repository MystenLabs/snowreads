{
  "id": "2412.11501",
  "title": "Explicit and Implicit Graduated Optimization in Deep Neural Networks",
  "authors": "Naoki Sato, Hideaki Iiduka",
  "authorsParsed": [
    [
      "Sato",
      "Naoki",
      ""
    ],
    [
      "Iiduka",
      "Hideaki",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 07:23:22 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734333802000,
  "abstract": "  Graduated optimization is a global optimization technique that is used to\nminimize a multimodal nonconvex function by smoothing the objective function\nwith noise and gradually refining the solution. This paper experimentally\nevaluates the performance of the explicit graduated optimization algorithm with\nan optimal noise scheduling derived from a previous study and discusses its\nlimitations. It uses traditional benchmark functions and empirical loss\nfunctions for modern neural network architectures for evaluating. In addition,\nthis paper extends the implicit graduated optimization algorithm, which is\nbased on the fact that stochastic noise in the optimization process of SGD\nimplicitly smooths the objective function, to SGD with momentum, analyzes its\nconvergence, and demonstrates its effectiveness through experiments on image\nclassification tasks with ResNet architectures.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "v6iHs-4I6TbJ2JqwGT9d18FKf9qhz7PD8lOLaAN3r54",
  "pdfSize": "4463558"
}