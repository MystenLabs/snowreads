{
  "id": "2412.17251",
  "title": "GCS-M3VLT: Guided Context Self-Attention based Multi-modal Medical\n  Vision Language Transformer for Retinal Image Captioning",
  "authors": "Teja Krishna Cherukuri, Nagur Shareef Shaik, Jyostna Devi Bodapati,\n  Dong Hye Ye",
  "authorsParsed": [
    [
      "Cherukuri",
      "Teja Krishna",
      ""
    ],
    [
      "Shaik",
      "Nagur Shareef",
      ""
    ],
    [
      "Bodapati",
      "Jyostna Devi",
      ""
    ],
    [
      "Ye",
      "Dong Hye",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 03:49:29 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734925769000,
  "abstract": "  Retinal image analysis is crucial for diagnosing and treating eye diseases,\nyet generating accurate medical reports from images remains challenging due to\nvariability in image quality and pathology, especially with limited labeled\ndata. Previous Transformer-based models struggled to integrate visual and\ntextual information under limited supervision. In response, we propose a novel\nvision-language model for retinal image captioning that combines visual and\ntextual features through a guided context self-attention mechanism. This\napproach captures both intricate details and the global clinical context, even\nin data-scarce scenarios. Extensive experiments on the DeepEyeNet dataset\ndemonstrate a 0.023 BLEU@4 improvement, along with significant qualitative\nadvancements, highlighting the effectiveness of our model in generating\ncomprehensive medical captions.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning",
    "Electrical Engineering and Systems Science/Image and Video Processing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "MxLrb944dBj3ARPF4e2fOWTHqy_jWyYaf1Pzu41xln0",
  "pdfSize": "7825607"
}