{
  "id": "2412.05718",
  "title": "RL Zero: Zero-Shot Language to Behaviors without any Supervision",
  "authors": "Harshit Sikchi, Siddhant Agarwal, Pranaya Jajoo, Samyak Parajuli,\n  Caleb Chuck, Max Rudolph, Peter Stone, Amy Zhang, Scott Niekum",
  "authorsParsed": [
    [
      "Sikchi",
      "Harshit",
      ""
    ],
    [
      "Agarwal",
      "Siddhant",
      ""
    ],
    [
      "Jajoo",
      "Pranaya",
      ""
    ],
    [
      "Parajuli",
      "Samyak",
      ""
    ],
    [
      "Chuck",
      "Caleb",
      ""
    ],
    [
      "Rudolph",
      "Max",
      ""
    ],
    [
      "Stone",
      "Peter",
      ""
    ],
    [
      "Zhang",
      "Amy",
      ""
    ],
    [
      "Niekum",
      "Scott",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 7 Dec 2024 18:31:16 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733596276000,
  "abstract": "  Rewards remain an uninterpretable way to specify tasks for Reinforcement\nLearning, as humans are often unable to predict the optimal behavior of any\ngiven reward function, leading to poor reward design and reward hacking.\nLanguage presents an appealing way to communicate intent to agents and bypass\nreward design, but prior efforts to do so have been limited by costly and\nunscalable labeling efforts. In this work, we propose a method for a completely\nunsupervised alternative to grounding language instructions in a zero-shot\nmanner to obtain policies. We present a solution that takes the form of\nimagine, project, and imitate: The agent imagines the observation sequence\ncorresponding to the language description of a task, projects the imagined\nsequence to our target domain, and grounds it to a policy. Video-language\nmodels allow us to imagine task descriptions that leverage knowledge of tasks\nlearned from internet-scale video-text mappings. The challenge remains to\nground these generations to a policy. In this work, we show that we can achieve\na zero-shot language-to-behavior policy by first grounding the imagined\nsequences in real observations of an unsupervised RL agent and using a\nclosed-form solution to imitation learning that allows the RL agent to mimic\nthe grounded observations. Our method, RLZero, is the first to our knowledge to\nshow zero-shot language to behavior generation abilities without any\nsupervision on a variety of tasks on simulated domains. We further show that\nRLZero can also generate policies zero-shot from cross-embodied videos such as\nthose scraped from YouTube.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Graphics",
    "Computer Science/Machine Learning",
    "Computer Science/Robotics"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "GGezbL_9fxsZCEHANFHYFEfLdbTEtkWd8jOVDveUT7Q",
  "pdfSize": "1628555"
}