{"id":"2412.16878","title":"Online Preference-based Reinforcement Learning with Self-augmented\n  Feedback from Large Language Model","authors":"Songjun Tu, Jingbo Sun, Qichao Zhang, Xiangyuan Lan, Dongbin Zhao","authorsParsed":[["Tu","Songjun",""],["Sun","Jingbo",""],["Zhang","Qichao",""],["Lan","Xiangyuan",""],["Zhao","Dongbin",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 06:15:25 GMT"}],"updateDate":"2024-12-24","timestamp":1734848125000,"abstract":"  Preference-based reinforcement learning (PbRL) provides a powerful paradigm\nto avoid meticulous reward engineering by learning rewards based on human\npreferences. However, real-time human feedback is hard to obtain in online\ntasks. Most work suppose there is a \"scripted teacher\" that utilizes privileged\npredefined reward to provide preference feedback. In this paper, we propose a\nRL Self-augmented Large Language Model Feedback (RL-SaLLM-F) technique that\ndoes not rely on privileged information for online PbRL. RL-SaLLM-F leverages\nthe reflective and discriminative capabilities of LLM to generate\nself-augmented trajectories and provide preference labels for reward learning.\nFirst, we identify an failure issue in LLM-based preference discrimination,\nspecifically \"query ambiguity\", in online PbRL. Then LLM is employed to provide\npreference labels and generate self-augmented imagined trajectories that better\nachieve the task goal, thereby enhancing the quality and efficiency of\nfeedback. Additionally, a double-check mechanism is introduced to mitigate\nrandomness in the preference labels, improving the reliability of LLM feedback.\nThe experiment across multiple tasks in the MetaWorld benchmark demonstrates\nthe specific contributions of each proposed module in RL-SaLLM-F, and shows\nthat self-augmented LLM feedback can effectively replace the impractical\n\"scripted teacher\" feedback. In summary, RL-SaLLM-F introduces a new direction\nof feedback acquisition in online PbRL that does not rely on any online\nprivileged information, offering an efficient and lightweight solution with\nLLM-driven feedback.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LYfILPNrsyeWC1RHfmiv-d-wx6DZFnoYlcBqOFlzOIU","pdfSize":"3189959"}