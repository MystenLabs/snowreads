{
  "id": "2412.05738",
  "title": "Exploring the Impact of Emotional Voice Integration in Sign-to-Speech\n  Translators for Deaf-to-Hearing Communication",
  "authors": "Hyunchul Lim, Minghan Gao, Franklin Mingzhe Li, Nam Anh Dang, Ianip\n  Sit, Michelle M Olson, and Cheng Zhang",
  "authorsParsed": [
    [
      "Lim",
      "Hyunchul",
      ""
    ],
    [
      "Gao",
      "Minghan",
      ""
    ],
    [
      "Li",
      "Franklin Mingzhe",
      ""
    ],
    [
      "Dang",
      "Nam Anh",
      ""
    ],
    [
      "Sit",
      "Ianip",
      ""
    ],
    [
      "Olson",
      "Michelle M",
      ""
    ],
    [
      "Zhang",
      "Cheng",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 7 Dec 2024 20:18:46 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733602726000,
  "abstract": "  Emotional voice communication plays a crucial role in effective daily\ninteractions. Deaf and hard-of-hearing (DHH) individuals often rely on facial\nexpressions to supplement sign language to convey emotions, as the use of voice\nis limited. However, in American Sign Language (ASL), these facial expressions\nserve not only emotional purposes but also as linguistic markers, altering sign\nmeanings and often confusing non-signers when interpreting a signer's emotional\nstate. Most existing ASL translation technologies focus solely on signs,\nneglecting the role of emotional facial expressions in the translated output\n(e.g., text, voice). This paper present studies which 1) confirmed the\nchallenges for non-signers of interpreting emotions from facial expressions in\nASL communication, of facial expressions, and 2) how integrating emotional\nvoices into translation systems can enhance hearing individuals' comprehension\nof a signer's emotions. An online survey conducted with 45 hearing participants\n(Non-ASL Signers) revealed that they frequently misinterpret signers' emotions\nwhen emotional and linguistic facial expressions are used simultaneously. The\nfindings indicate that incorporating emotional voice into translation systems\nsignificantly improves the recognition of signers' emotions by 32%.\nAdditionally, further research involving 6 DHH participants discusses design\nconsiderations for the emotional voice feature from both perspectives,\nemphasizing the importance of integrating emotional voices in translation\nsystems to bridge communication gaps between DHH and hearing communities.\n",
  "subjects": [
    "Computer Science/Human-Computer Interaction"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "buBgkPlzB-buJWxb2dPrvpvcxJeXn9TgLDeLBNjBiuw",
  "pdfSize": "5621544"
}