{"id":"2412.07583","title":"Mobile Video Diffusion","authors":"Haitam Ben Yahia and Denis Korzhenkov and Ioannis Lelekas and Amir\n  Ghodrati and Amirhossein Habibian","authorsParsed":[["Yahia","Haitam Ben",""],["Korzhenkov","Denis",""],["Lelekas","Ioannis",""],["Ghodrati","Amir",""],["Habibian","Amirhossein",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 15:19:10 GMT"}],"updateDate":"2024-12-11","timestamp":1733843950000,"abstract":"  Video diffusion models have achieved impressive realism and controllability\nbut are limited by high computational demands, restricting their use on mobile\ndevices. This paper introduces the first mobile-optimized video diffusion\nmodel. Starting from a spatio-temporal UNet from Stable Video Diffusion (SVD),\nwe reduce memory and computational cost by reducing the frame resolution,\nincorporating multi-scale temporal representations, and introducing two novel\npruning schema to reduce the number of channels and temporal blocks.\nFurthermore, we employ adversarial finetuning to reduce the denoising to a\nsingle step. Our model, coined as MobileVD, is 523x more efficient (1817.2 vs.\n4.34 TFLOPs) with a slight quality drop (FVD 149 vs. 171), generating latents\nfor a 14x512x256 px clip in 1.7 seconds on a Xiaomi-14 Pro. Our results are\navailable at https://qualcomm-ai-research.github.io/mobile-video-diffusion/\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"h_HoSyB59jt4Mrhdbo4bwnrSJgWdSSYWMwjIR26TMMc","pdfSize":"16195274"}