{"id":"2412.13571","title":"PowerMLP: An Efficient Version of KAN","authors":"Ruichen Qiu and Yibo Miao and Shiwen Wang and Lijia Yu and Yifan Zhu\n  and Xiao-Shan Gao","authorsParsed":[["Qiu","Ruichen",""],["Miao","Yibo",""],["Wang","Shiwen",""],["Yu","Lijia",""],["Zhu","Yifan",""],["Gao","Xiao-Shan",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 07:42:34 GMT"}],"updateDate":"2024-12-19","timestamp":1734507754000,"abstract":"  The Kolmogorov-Arnold Network (KAN) is a new network architecture known for\nits high accuracy in several tasks such as function fitting and PDE solving.\nThe superior expressive capability of KAN arises from the Kolmogorov-Arnold\nrepresentation theorem and learnable spline functions. However, the computation\nof spline functions involves multiple iterations, which renders KAN\nsignificantly slower than MLP, thereby increasing the cost associated with\nmodel training and deployment. The authors of KAN have also noted that ``the\nbiggest bottleneck of KANs lies in its slow training. KANs are usually 10x\nslower than MLPs, given the same number of parameters.'' To address this issue,\nwe propose a novel MLP-type neural network PowerMLP that employs simpler\nnon-iterative spline function representation, offering approximately the same\ntraining time as MLP while theoretically demonstrating stronger expressive\npower than KAN. Furthermore, we compare the FLOPs of KAN and PowerMLP,\nquantifying the faster computation speed of PowerMLP. Our comprehensive\nexperiments demonstrate that PowerMLP generally achieves higher accuracy and a\ntraining speed about 40 times faster than KAN in various tasks.\n","subjects":["Computer Science/Machine Learning","Computer Science/Numerical Analysis","Mathematics/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PpQlwkifmFaIaQIkvewYOu4WgwSqvev62JHDTvxhwSg","pdfSize":"531290"}