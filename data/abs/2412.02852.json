{"id":"2412.02852","title":"Effortless Efficiency: Low-Cost Pruning of Diffusion Models","authors":"Yang Zhang, Er Jin, Yanfei Dong, Ashkan Khakzar, Philip Torr, Johannes\n  Stegmaier, Kenji Kawaguchi","authorsParsed":[["Zhang","Yang",""],["Jin","Er",""],["Dong","Yanfei",""],["Khakzar","Ashkan",""],["Torr","Philip",""],["Stegmaier","Johannes",""],["Kawaguchi","Kenji",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 21:37:50 GMT"}],"updateDate":"2024-12-05","timestamp":1733261870000,"abstract":"  Diffusion models have achieved impressive advancements in various vision\ntasks. However, these gains often rely on increasing model size, which\nescalates computational complexity and memory demands, complicating deployment,\nraising inference costs, and causing environmental impact. While some studies\nhave explored pruning techniques to improve the memory efficiency of diffusion\nmodels, most existing methods require extensive retraining to retain the model\nperformance. Retraining a modern large diffusion model is extremely costly and\nresource-intensive, which limits the practicality of these methods. In this\nwork, we achieve low-cost diffusion pruning without retraining by proposing a\nmodel-agnostic structural pruning framework for diffusion models that learns a\ndifferentiable mask to sparsify the model. To ensure effective pruning that\npreserves the quality of the final denoised latent, we design a novel\nend-to-end pruning objective that spans the entire diffusion process. As\nend-to-end pruning is memory-intensive, we further propose time step gradient\ncheckpointing, a technique that significantly reduces memory usage during\noptimization, enabling end-to-end pruning within a limited memory budget.\nResults on state-of-the-art U-Net diffusion models SDXL and diffusion\ntransformers (FLUX) demonstrate that our method can effectively prune up to 20%\nparameters with minimal perceptible performance degradation, and notably,\nwithout the need for model retraining. We also showcase that our method can\nstill prune on top of time step distilled diffusion models.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uogWICGfWipEQxmWQK2y4ccDfv1uH9W7jYSX84kKj4o","pdfSize":"10705997"}