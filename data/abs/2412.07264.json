{
  "id": "2412.07264",
  "title": "QuantFormer: Learning to Quantize for Neural Activity Forecasting in\n  Mouse Visual Cortex",
  "authors": "Salvatore Calcagno, Isaak Kavasidis, Simone Palazzo, Marco Brondi,\n  Luca Sit\\`a, Giacomo Turri, Daniela Giordano, Vladimir R. Kostic, Tommaso\n  Fellin, Massimiliano Pontil, Concetto Spampinato",
  "authorsParsed": [
    [
      "Calcagno",
      "Salvatore",
      ""
    ],
    [
      "Kavasidis",
      "Isaak",
      ""
    ],
    [
      "Palazzo",
      "Simone",
      ""
    ],
    [
      "Brondi",
      "Marco",
      ""
    ],
    [
      "Sit√†",
      "Luca",
      ""
    ],
    [
      "Turri",
      "Giacomo",
      ""
    ],
    [
      "Giordano",
      "Daniela",
      ""
    ],
    [
      "Kostic",
      "Vladimir R.",
      ""
    ],
    [
      "Fellin",
      "Tommaso",
      ""
    ],
    [
      "Pontil",
      "Massimiliano",
      ""
    ],
    [
      "Spampinato",
      "Concetto",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 07:44:35 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733816675000,
  "abstract": "  Understanding complex animal behaviors hinges on deciphering the neural\nactivity patterns within brain circuits, making the ability to forecast neural\nactivity crucial for developing predictive models of brain dynamics. This\ncapability holds immense value for neuroscience, particularly in applications\nsuch as real-time optogenetic interventions. While traditional encoding and\ndecoding methods have been used to map external variables to neural activity\nand vice versa, they focus on interpreting past data. In contrast, neural\nforecasting aims to predict future neural activity, presenting a unique and\nchallenging task due to the spatiotemporal sparsity and complex dependencies of\nneural signals. Existing transformer-based forecasting methods, while effective\nin many domains, struggle to capture the distinctiveness of neural signals\ncharacterized by spatiotemporal sparsity and intricate dependencies. To address\nthis challenge, we here introduce QuantFormer, a transformer-based model\nspecifically designed for forecasting neural activity from two-photon calcium\nimaging data. Unlike conventional regression-based approaches,\nQuantFormerreframes the forecasting task as a classification problem via\ndynamic signal quantization, enabling more effective learning of sparse neural\nactivation patterns. Additionally, QuantFormer tackles the challenge of\nanalyzing multivariate signals from an arbitrary number of neurons by\nincorporating neuron-specific tokens, allowing scalability across diverse\nneuronal populations. Trained with unsupervised quantization on the Allen\ndataset, QuantFormer sets a new benchmark in forecasting mouse visual cortex\nactivity. It demonstrates robust performance and generalization across various\nstimuli and individuals, paving the way for a foundational model in neural\nsignal prediction.\n",
  "subjects": [
    "Quantitative Biology/Neurons and Cognition",
    "Computer Science/Computer Vision and Pattern Recognition",
    "Electrical Engineering and Systems Science/Image and Video Processing",
    "Electrical Engineering and Systems Science/Signal Processing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "p3gSHEcIzwNEwJ6CMMz5LZYV2iiN15kLer99GFZzv4A",
  "pdfSize": "4718468"
}