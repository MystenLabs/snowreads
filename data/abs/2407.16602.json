{"id":"2407.16602","title":"Functional Acceleration for Policy Mirror Descent","authors":"Veronica Chelu and Doina Precup","authorsParsed":[["Chelu","Veronica",""],["Precup","Doina",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 16:04:55 GMT"}],"updateDate":"2024-07-24","timestamp":1721750695000,"abstract":"  We apply functional acceleration to the Policy Mirror Descent (PMD) general\nfamily of algorithms, which cover a wide range of novel and fundamental methods\nin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based\nPMD update. By taking the functional route, our approach is independent of the\npolicy parametrization and applicable to large-scale optimization, covering\nprevious applications of momentum at the level of policy parameters as a\nspecial case. We theoretically analyze several properties of this approach and\ncomplement with a numerical ablation study, which serves to illustrate the\npolicy optimization dynamics on the value polytope, relative to different\nalgorithmic design choices in this space. We further characterize numerically\nseveral features of the problem setting relevant for functional acceleration,\nand lastly, we investigate the impact of approximation on their learning\nmechanics.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"k7pFqjYb2829vzRcuRVfxOTwF7knsGYBg-9kTAEmz9U","pdfSize":"16104636","objectId":"0x0ac67fc4313a63af26cb53498dae4771e00f93181bcebe815fe8b7b1db3b1c65","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
