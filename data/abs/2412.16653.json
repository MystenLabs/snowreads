{
  "id": "2412.16653",
  "title": "Internalized Self-Correction for Large Language Models",
  "authors": "Nishanth Upadhyaya and Raghavendra Sridharamurthy",
  "authorsParsed": [
    [
      "Upadhyaya",
      "Nishanth",
      ""
    ],
    [
      "Sridharamurthy",
      "Raghavendra",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 14:53:13 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734792793000,
  "abstract": "  In this article, we introduce 'Internalized Self-Correction' (InSeC) for\nlarge language models (LLMs). While many approaches exist for self-reflection\nat inference time, we propose a novel method that combines ideas from negative\nsampling, self-reflection during training, and inference time. InSeC allows\nLLMs to correct themselves by introducing mistakes and their corresponding\ncorrections during training, thereby converting the learning process into a\ntrue supervised learning task with both positive and negative examples. This\napproach can be extended to improve instruction following and correct\nhallucinations or incorrect sentences generated by LLMs.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "kQU-ElfntGpqYqy_DFI_rftpwrPxJx3baON4_fP79mU",
  "pdfSize": "78054"
}