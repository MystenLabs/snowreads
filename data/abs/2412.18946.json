{
  "id": "2412.18946",
  "title": "Constraint-Adaptive Policy Switching for Offline Safe Reinforcement\n  Learning",
  "authors": "Yassine Chemingui, Aryan Deshwal, Honghao Wei, Alan Fern, Janardhan\n  Rao Doppa",
  "authorsParsed": [
    [
      "Chemingui",
      "Yassine",
      ""
    ],
    [
      "Deshwal",
      "Aryan",
      ""
    ],
    [
      "Wei",
      "Honghao",
      ""
    ],
    [
      "Fern",
      "Alan",
      ""
    ],
    [
      "Doppa",
      "Janardhan Rao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 25 Dec 2024 16:42:27 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735144947000,
  "abstract": "  Offline safe reinforcement learning (OSRL) involves learning a\ndecision-making policy to maximize rewards from a fixed batch of training data\nto satisfy pre-defined safety constraints. However, adapting to varying safety\nconstraints during deployment without retraining remains an under-explored\nchallenge. To address this challenge, we introduce constraint-adaptive policy\nswitching (CAPS), a wrapper framework around existing offline RL algorithms.\nDuring training, CAPS uses offline data to learn multiple policies with a\nshared representation that optimize different reward and cost trade-offs.\nDuring testing, CAPS switches between those policies by selecting at each state\nthe policy that maximizes future rewards among those that satisfy the current\ncost constraint. Our experiments on 38 tasks from the DSRL benchmark\ndemonstrate that CAPS consistently outperforms existing methods, establishing a\nstrong wrapper-based baseline for OSRL. The code is publicly available at\nhttps://github.com/yassineCh/CAPS.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "SNNu-jjtDUDX8F4TuF0kh28Zaw9wKtyTmqph01bK1X8",
  "pdfSize": "1830249"
}