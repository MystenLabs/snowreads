{"id":"2407.09943","title":"Minimizing PLM-Based Few-Shot Intent Detectors","authors":"Haode Zhang, Albert Y.S. Lam, Xiao-Ming Wu","authorsParsed":[["Zhang","Haode",""],["Lam","Albert Y. S.",""],["Wu","Xiao-Ming",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 16:47:20 GMT"},{"version":"v2","created":"Sun, 15 Sep 2024 15:45:50 GMT"}],"updateDate":"2024-09-17","timestamp":1720889240000,"abstract":"  Recent research has demonstrated the feasibility of training efficient intent\ndetectors based on pre-trained language model~(PLM) with limited labeled data.\nHowever, deploying these detectors in resource-constrained environments such as\nmobile devices poses challenges due to their large sizes. In this work, we aim\nto address this issue by exploring techniques to minimize the size of PLM-based\nintent detectors trained with few-shot data. Specifically, we utilize large\nlanguage models (LLMs) for data augmentation, employ a cutting-edge model\ncompression method for knowledge distillation, and devise a vocabulary pruning\nmechanism called V-Prune. Through these approaches, we successfully achieve a\ncompression ratio of 21 in model memory usage, including both Transformer and\nthe vocabulary, while maintaining almost identical performance levels on four\nreal-world benchmarks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rj-1JqnnuTtraUawb8xVOCOda1X8pGrrXis995ThWGA","pdfSize":"485731"}