{"id":"2412.07520","title":"Quantifying the Prediction Uncertainty of Machine Learning Models for\n  Individual Data","authors":"Koby Bibas","authorsParsed":[["Bibas","Koby",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 13:58:19 GMT"}],"updateDate":"2024-12-11","timestamp":1733839099000,"abstract":"  Machine learning models have exhibited exceptional results in various\ndomains. The most prevalent approach for learning is the empirical risk\nminimizer (ERM), which adapts the model's weights to reduce the loss on a\ntraining set and subsequently leverages these weights to predict the label for\nnew test data. Nonetheless, ERM makes the assumption that the test distribution\nis similar to the training distribution, which may not always hold in\nreal-world situations. In contrast, the predictive normalized maximum\nlikelihood (pNML) was proposed as a min-max solution for the individual setting\nwhere no assumptions are made on the distribution of the tested input. This\nstudy investigates pNML's learnability for linear regression and neural\nnetworks, and demonstrates that pNML can improve the performance and robustness\nof these models on various tasks. Moreover, the pNML provides an accurate\nconfidence measure for its output, showcasing state-of-the-art results for\nout-of-distribution detection, resistance to adversarial attacks, and active\nlearning.\n","subjects":["Computer Science/Machine Learning","Computer Science/Information Theory","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ux9kDusHCVdJPEyKJeM5t6dwtD8jamRUOKMtqXTXev4","pdfSize":"3328426"}