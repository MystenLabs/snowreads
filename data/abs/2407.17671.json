{"id":"2407.17671","title":"Unsqueeze [CLS] Bottleneck to Learn Rich Representations","authors":"Qing Su, Shihao Ji","authorsParsed":[["Su","Qing",""],["Ji","Shihao",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 23:23:38 GMT"},{"version":"v2","created":"Fri, 26 Jul 2024 14:09:08 GMT"}],"updateDate":"2024-07-29","timestamp":1721863418000,"abstract":"  Distillation-based self-supervised learning typically leads to more\ncompressed representations due to its radical clustering process and the\nimplementation of a sharper target distribution. To overcome this limitation\nand preserve more information from input, we introduce UDI, conceptualized as\nUnsqueezed Distillation-based self-supervised learning (SSL). UDI enriches the\nlearned representation by encouraging multimodal prediction distilled from a\nconsolidated profile of local predictions that are derived via stratified\nsampling. Our evaluations show that UDI not only promotes semantically\nmeaningful representations at instance level, delivering superior or\ncompetitive results to state-of-the-art SSL methods in image classification,\nbut also effectively preserves the nuisance of input, which yields significant\nimprovement in dense prediction tasks, including object detection and\nsegmentation. Additionally, UDI performs competitively in low-shot image\nclassification, improving the scalability of joint-embedding pipelines. Various\nvisualizations and ablation studies are presented to further elucidate the\nmechanisms behind UDI. Our source code is available at\nhttps://github.com/ISL-CV/udi.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YOoxgwvGQl5hfCClrXPvOJV4Bna2XlOkT1QyIuW048o","pdfSize":"7675645","objectId":"0x5598cd2d65872561e63d2b305f651a8c779a1162e29f83806e28c4048e901403","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
