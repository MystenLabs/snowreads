{"id":"2412.18775","title":"ObitoNet: Multimodal High-Resolution Point Cloud Reconstruction","authors":"Apoorv Thapliyal, Vinay Lanka, Swathi Baskaran","authorsParsed":[["Thapliyal","Apoorv",""],["Lanka","Vinay",""],["Baskaran","Swathi",""]],"versions":[{"version":"v1","created":"Wed, 25 Dec 2024 04:34:22 GMT"}],"updateDate":"2024-12-30","timestamp":1735101262000,"abstract":"  ObitoNet employs a Cross Attention mechanism to integrate multimodal inputs,\nwhere Vision Transformers (ViT) extract semantic features from images and a\npoint cloud tokenizer processes geometric information using Farthest Point\nSampling (FPS) and K Nearest Neighbors (KNN) for spatial structure capture. The\nlearned multimodal features are fed into a transformer-based decoder for\nhigh-resolution point cloud reconstruction. This approach leverages the\ncomplementary strengths of both modalities rich image features and precise\ngeometric details ensuring robust point cloud generation even in challenging\nconditions such as sparse or noisy data.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"SSNz1tzXyk7KdSvqPKF7AInG3y7ghqFoMXrXbq1esyU","pdfSize":"2780266"}