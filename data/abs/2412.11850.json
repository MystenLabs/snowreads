{
  "id": "2412.11850",
  "title": "Causal Invariance Learning via Efficient Optimization of a Nonconvex\n  Objective",
  "authors": "Zhenyu Wang, Yifan Hu, Peter B\\\"uhlmann, Zijian Guo",
  "authorsParsed": [
    [
      "Wang",
      "Zhenyu",
      ""
    ],
    [
      "Hu",
      "Yifan",
      ""
    ],
    [
      "BÃ¼hlmann",
      "Peter",
      ""
    ],
    [
      "Guo",
      "Zijian",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 15:11:02 GMT"
    },
    {
      "version": "v2",
      "created": "Tue, 17 Dec 2024 15:44:30 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734361862000,
  "abstract": "  Data from multiple environments offer valuable opportunities to uncover\ncausal relationships among variables. Leveraging the assumption that the causal\noutcome model remains invariant across heterogeneous environments,\nstate-of-the-art methods attempt to identify causal outcome models by learning\ninvariant prediction models and rely on exhaustive searches over all\n(exponentially many) covariate subsets. These approaches present two major\nchallenges: 1) determining the conditions under which the invariant prediction\nmodel aligns with the causal outcome model, and 2) devising computationally\nefficient causal discovery algorithms that scale polynomially, instead of\nexponentially, with the number of covariates. To address both challenges, we\nfocus on the additive intervention regime and propose nearly necessary and\nsufficient conditions for ensuring that the invariant prediction model matches\nthe causal outcome model. Exploiting the essentially necessary identifiability\nconditions, we introduce Negative Weight Distributionally Robust Optimization\n(NegDRO), a nonconvex continuous minimax optimization whose global optimizer\nrecovers the causal outcome model. Unlike standard group DRO problems that\nmaximize over the simplex, NegDRO allows negative weights on environment\nlosses, which break the convexity. Despite its nonconvexity, we demonstrate\nthat a standard gradient method converges to the causal outcome model, and we\nestablish the convergence rate with respect to the sample size and the number\nof iterations. Our algorithm avoids exhaustive search, making it scalable\nespecially when the number of covariates is large. The numerical results\nfurther validate the efficiency of the proposed method.\n",
  "subjects": [
    "Statistics/Methodology",
    "Computer Science/Machine Learning",
    "Mathematics/Optimization and Control",
    "Mathematics/Statistics Theory",
    "Statistics/Statistics Theory"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "rDRKWjcIeLpGjoloBhh1t3Y2eWan6MtD8rcBLmbn0mM",
  "pdfSize": "1556568"
}