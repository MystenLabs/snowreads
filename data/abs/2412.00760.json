{
  "id": "2412.00760",
  "title": "Automating Feedback Analysis in Surgical Training: Detection,\n  Categorization, and Assessment",
  "authors": "Firdavs Nasriddinov, Rafal Kocielnik, Arushi Gupta, Cherine Yang,\n  Elyssa Wong, Anima Anandkumar, Andrew Hung",
  "authorsParsed": [
    [
      "Nasriddinov",
      "Firdavs",
      ""
    ],
    [
      "Kocielnik",
      "Rafal",
      ""
    ],
    [
      "Gupta",
      "Arushi",
      ""
    ],
    [
      "Yang",
      "Cherine",
      ""
    ],
    [
      "Wong",
      "Elyssa",
      ""
    ],
    [
      "Anandkumar",
      "Anima",
      ""
    ],
    [
      "Hung",
      "Andrew",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 1 Dec 2024 10:35:12 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733049312000,
  "abstract": "  This work introduces the first framework for reconstructing surgical dialogue\nfrom unstructured real-world recordings, which is crucial for characterizing\nteaching tasks. In surgical training, the formative verbal feedback that\ntrainers provide to trainees during live surgeries is crucial for ensuring\nsafety, correcting behavior immediately, and facilitating long-term skill\nacquisition. However, analyzing and quantifying this feedback is challenging\ndue to its unstructured and specialized nature. Automated systems are essential\nto manage these complexities at scale, allowing for the creation of structured\ndatasets that enhance feedback analysis and improve surgical education. Our\nframework integrates voice activity detection, speaker diarization, and\nautomated speech recaognition, with a novel enhancement that 1) removes\nhallucinations (non-existent utterances generated during speech recognition\nfueled by noise in the operating room) and 2) separates speech from trainers\nand trainees using few-shot voice samples. These aspects are vital for\nreconstructing accurate surgical dialogues and understanding the roles of\noperating room participants. Using data from 33 real-world surgeries, we\ndemonstrated the system's capability to reconstruct surgical teaching dialogues\nand detect feedback instances effectively (F1 score of 0.79+/-0.07). Moreover,\nour hallucination removal step improves feedback detection performance by ~14%.\nEvaluation on downstream clinically relevant tasks of predicting Behavioral\nAdjustment of trainees and classifying Technical feedback, showed performances\ncomparable to manual annotations with F1 scores of 0.82+/0.03 and 0.81+/0.03\nrespectively. These results highlight the effectiveness of our framework in\nsupporting clinically relevant tasks and improving over manual methods.\n",
  "subjects": [
    "Electrical Engineering and Systems Science/Audio and Speech Processing",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language",
    "Computer Science/Emerging Technologies",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "BAMcsrlSIH7ugcJKwwKxBV5hRRhiuMU4kMPfGB36KM8",
  "pdfSize": "695236"
}