{"id":"2407.08536","title":"Exemplar-free Continual Representation Learning via Learnable Drift\n  Compensation","authors":"Alex Gomez-Villa, Dipam Goswami, Kai Wang, Andrew D. Bagdanov,\n  Bartlomiej Twardowski, Joost van de Weijer","authorsParsed":[["Gomez-Villa","Alex",""],["Goswami","Dipam",""],["Wang","Kai",""],["Bagdanov","Andrew D.",""],["Twardowski","Bartlomiej",""],["van de Weijer","Joost",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 14:23:08 GMT"}],"updateDate":"2024-07-12","timestamp":1720707788000,"abstract":"  Exemplar-free class-incremental learning using a backbone trained from\nscratch and starting from a small first task presents a significant challenge\nfor continual representation learning. Prototype-based approaches, when\ncontinually updated, face the critical issue of semantic drift due to which the\nold class prototypes drift to different positions in the new feature space.\nThrough an analysis of prototype-based continual learning, we show that\nforgetting is not due to diminished discriminative power of the feature\nextractor, and can potentially be corrected by drift compensation. To address\nthis, we propose Learnable Drift Compensation (LDC), which can effectively\nmitigate drift in any moving backbone, whether supervised or unsupervised. LDC\nis fast and straightforward to integrate on top of existing continual learning\napproaches. Furthermore, we showcase how LDC can be applied in combination with\nself-supervised CL methods, resulting in the first exemplar-free\nsemi-supervised continual learning approach. We achieve state-of-the-art\nperformance in both supervised and semi-supervised settings across multiple\ndatasets. Code is available at \\url{https://github.com/alviur/ldc}.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4n1jqMVe4qbXedwwtj6Ho7VNKeoqVP9nOROzhDW9rfA","pdfSize":"996084"}