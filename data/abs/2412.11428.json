{
  "id": "2412.11428",
  "title": "View Transformation Robustness for Multi-View 3D Object Reconstruction\n  with Reconstruction Error-Guided View Selection",
  "authors": "Qi Zhang and Zhouhang Luo and Tao Yu and Hui Huang",
  "authorsParsed": [
    [
      "Zhang",
      "Qi",
      ""
    ],
    [
      "Luo",
      "Zhouhang",
      ""
    ],
    [
      "Yu",
      "Tao",
      ""
    ],
    [
      "Huang",
      "Hui",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 03:54:08 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734321248000,
  "abstract": "  View transformation robustness (VTR) is critical for deep-learning-based\nmulti-view 3D object reconstruction models, which indicates the methods'\nstability under inputs with various view transformations. However, existing\nresearch seldom focused on view transformation robustness in multi-view 3D\nobject reconstruction. One direct way to improve the models' VTR is to produce\ndata with more view transformations and add them to model training. Recent\nprogress on large vision models, particularly Stable Diffusion models, has\nprovided great potential for generating 3D models or synthesizing novel view\nimages with only a single image input. Directly deploying these models at\ninference consumes heavy computation resources and their robustness to view\ntransformations is not guaranteed either. To fully utilize the power of Stable\nDiffusion models without extra inference computation burdens, we propose to\ngenerate novel views with Stable Diffusion models for better view\ntransformation robustness. Instead of synthesizing random views, we propose a\nreconstruction error-guided view selection method, which considers the\nreconstruction errors' spatial distribution of the 3D predictions and chooses\nthe views that could cover the reconstruction errors as much as possible. The\nmethods are trained and tested on sets with large view transformations to\nvalidate the 3D reconstruction models' robustness to view transformations.\nExtensive experiments demonstrate that the proposed method can outperform\nstate-of-the-art 3D reconstruction methods and other view transformation\nrobustness comparison methods.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "Id7AN98FdRbE_cAB9hU46lrKE6TkQKPgPAQ3OVvKUgA",
  "pdfSize": "5930899"
}