{
  "id": "2412.16751",
  "title": "The Master Key Filters Hypothesis: Deep Filters Are General",
  "authors": "Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu",
  "authorsParsed": [
    [
      "Babaiee",
      "Zahra",
      ""
    ],
    [
      "Kiasari",
      "Peyman M.",
      ""
    ],
    [
      "Rus",
      "Daniela",
      ""
    ],
    [
      "Grosu",
      "Radu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 20:04:23 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 3 Feb 2025 16:58:12 GMT"
    }
  ],
  "updateDate": "2025-02-04",
  "timestamp": 1734811463000,
  "abstract": "  This paper challenges the prevailing view that convolutional neural network\n(CNN) filters become increasingly specialized in deeper layers. Motivated by\nrecent observations of clusterable repeating patterns in depthwise separable\nCNNs (DS-CNNs) trained on ImageNet, we extend this investigation across various\ndomains and datasets. Our analysis of DS-CNNs reveals that deep filters\nmaintain generality, contradicting the expected transition to class-specific\nfilters. We demonstrate the generalizability of these filters through transfer\nlearning experiments, showing that frozen filters from models trained on\ndifferent datasets perform well and can be further improved when sourced from\nlarger datasets. Our findings indicate that spatial features learned by\ndepthwise separable convolutions remain generic across all layers, domains, and\narchitectures. This research provides new insights into the nature of\ngeneralization in neural networks, particularly in DS-CNNs, and has significant\nimplications for transfer learning and model design.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "DVkcGYsZ2kfQ5ZY4yCszgoUFeZsJH06zqCJ9csPsA5Y",
  "pdfSize": "593932"
}