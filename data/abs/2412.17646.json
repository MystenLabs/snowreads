{"id":"2412.17646","title":"Rate of Model Collapse in Recursive Training","authors":"Ananda Theertha Suresh and Andrew Thangaraj and Aditya Nanda Kishore\n  Khandavally","authorsParsed":[["Suresh","Ananda Theertha",""],["Thangaraj","Andrew",""],["Khandavally","Aditya Nanda Kishore",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 15:21:50 GMT"}],"updateDate":"2024-12-24","timestamp":1734967310000,"abstract":"  Given the ease of creating synthetic data from machine learning models, new\nmodels can be potentially trained on synthetic data generated by previous\nmodels. This recursive training process raises concerns about the long-term\nimpact on model quality. As models are recursively trained on generated data\nfrom previous rounds, their ability to capture the nuances of the original\nhuman-generated data may degrade. This is often referred to as \\emph{model\ncollapse}. In this work, we ask how fast model collapse occurs for some\nwell-studied distribution families under maximum likelihood (ML or near ML)\nestimation during recursive training. Surprisingly, even for fundamental\ndistributions such as discrete and Gaussian distributions, the exact rate of\nmodel collapse is unknown. In this work, we theoretically characterize the rate\nof collapse in these fundamental settings and complement it with experimental\nevaluations. Our results show that for discrete distributions, the time to\nforget a word is approximately linearly dependent on the number of times it\noccurred in the original corpus, and for Gaussian models, the standard\ndeviation reduces to zero roughly at $n$ iterations, where $n$ is the number of\nsamples at each iteration. Both of these findings imply that model forgetting,\nat least in these simple distributions under near ML estimation with many\nsamples, takes a long time.\n","subjects":["Computer Science/Machine Learning","Computer Science/Information Theory","Mathematics/Information Theory","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"01dJU1wlaGh_9emB1l8eQfRwlt9CnG543QnPHC_-VUA","pdfSize":"1734504"}