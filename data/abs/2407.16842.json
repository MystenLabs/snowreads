{"id":"2407.16842","title":"Adapting Image-based RL Policies via Predicted Rewards","authors":"Weiyao Wang, Xinyuan Fang and Gregory D. Hager","authorsParsed":[["Wang","Weiyao",""],["Fang","Xinyuan",""],["Hager","Gregory D.",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 21:08:04 GMT"}],"updateDate":"2024-07-25","timestamp":1721768884000,"abstract":"  Image-based reinforcement learning (RL) faces significant challenges in\ngeneralization when the visual environment undergoes substantial changes\nbetween training and deployment. Under such circumstances, learned policies may\nnot perform well leading to degraded results. Previous approaches to this\nproblem have largely focused on broadening the training observation\ndistribution, employing techniques like data augmentation and domain\nrandomization. However, given the sequential nature of the RL decision-making\nproblem, it is often the case that residual errors are propagated by the\nlearned policy model and accumulate throughout the trajectory, resulting in\nhighly degraded performance. In this paper, we leverage the observation that\npredicted rewards under domain shift, even though imperfect, can still be a\nuseful signal to guide fine-tuning. We exploit this property to fine-tune a\npolicy using reward prediction in the target domain. We have found that, even\nunder significant domain shift, the predicted reward can still provide\nmeaningful signal and fine-tuning substantially improves the original policy.\nOur approach, termed Predicted Reward Fine-tuning (PRFT), improves performance\nacross diverse tasks in both simulated benchmarks and real-world experiments.\nMore information is available at project web page:\nhttps://sites.google.com/view/prft.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dJiiRlvqASVpl_L7jnKKDRhLZviK6jBzxFmZfHIPNQ4","pdfSize":"2862344"}