{
  "id": "2412.14435",
  "title": "Cherry-Picking in Time Series Forecasting: How to Select Datasets to\n  Make Your Model Shine",
  "authors": "Luis Roque, Carlos Soares, Vitor Cerqueira, Luis Torgo",
  "authorsParsed": [
    [
      "Roque",
      "Luis",
      ""
    ],
    [
      "Soares",
      "Carlos",
      ""
    ],
    [
      "Cerqueira",
      "Vitor",
      ""
    ],
    [
      "Torgo",
      "Luis",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 19 Dec 2024 01:34:17 GMT"
    }
  ],
  "updateDate": "2024-12-20",
  "timestamp": 1734572057000,
  "abstract": "  The importance of time series forecasting drives continuous research and the\ndevelopment of new approaches to tackle this problem. Typically, these methods\nare introduced through empirical studies that frequently claim superior\naccuracy for the proposed approaches. Nevertheless, concerns are rising about\nthe reliability and generalizability of these results due to limitations in\nexperimental setups. This paper addresses a critical limitation: the number and\nrepresentativeness of the datasets used. We investigate the impact of dataset\nselection bias, particularly the practice of cherry-picking datasets, on the\nperformance evaluation of forecasting methods. Through empirical analysis with\na diverse set of benchmark datasets, our findings reveal that cherry-picking\ndatasets can significantly distort the perceived performance of methods, often\nexaggerating their effectiveness. Furthermore, our results demonstrate that by\nselectively choosing just four datasets - what most studies report - 46% of\nmethods could be deemed best in class, and 77% could rank within the top three.\nAdditionally, recent deep learning-based approaches show high sensitivity to\ndataset selection, whereas classical methods exhibit greater robustness.\nFinally, our results indicate that, when empirically validating forecasting\nalgorithms on a subset of the benchmarks, increasing the number of datasets\ntested from 3 to 6 reduces the risk of incorrectly identifying an algorithm as\nthe best one by approximately 40%. Our study highlights the critical need for\ncomprehensive evaluation frameworks that more accurately reflect real-world\nscenarios. Adopting such frameworks will ensure the development of robust and\nreliable forecasting methods.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "tZ31MWPC7vfK0OIe4GFqdA1BMSLBLzZ0Y-3ZYUpQT24",
  "pdfSize": "326802"
}