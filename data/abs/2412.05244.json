{
  "id": "2412.05244",
  "title": "Enhancing Foundation Models for Time Series Forecasting via\n  Wavelet-based Tokenization",
  "authors": "Luca Masserano, Abdul Fatir Ansari, Boran Han, Xiyuan Zhang, Christos\n  Faloutsos, Michael W. Mahoney, Andrew Gordon Wilson, Youngsuk Park, Syama\n  Rangapuram, Danielle C. Maddix, Yuyang Wang",
  "authorsParsed": [
    [
      "Masserano",
      "Luca",
      ""
    ],
    [
      "Ansari",
      "Abdul Fatir",
      ""
    ],
    [
      "Han",
      "Boran",
      ""
    ],
    [
      "Zhang",
      "Xiyuan",
      ""
    ],
    [
      "Faloutsos",
      "Christos",
      ""
    ],
    [
      "Mahoney",
      "Michael W.",
      ""
    ],
    [
      "Wilson",
      "Andrew Gordon",
      ""
    ],
    [
      "Park",
      "Youngsuk",
      ""
    ],
    [
      "Rangapuram",
      "Syama",
      ""
    ],
    [
      "Maddix",
      "Danielle C.",
      ""
    ],
    [
      "Wang",
      "Yuyang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 6 Dec 2024 18:22:59 GMT"
    }
  ],
  "updateDate": "2024-12-09",
  "timestamp": 1733509379000,
  "abstract": "  How to best develop foundational models for time series forecasting remains\nan important open question. Tokenization is a crucial consideration in this\neffort: what is an effective discrete vocabulary for a real-valued sequential\ninput? To address this question, we develop WaveToken, a wavelet-based\ntokenizer that allows models to learn complex representations directly in the\nspace of time-localized frequencies. Our method first scales and decomposes the\ninput time series, then thresholds and quantizes the wavelet coefficients, and\nfinally pre-trains an autoregressive model to forecast coefficients for the\nforecast horizon. By decomposing coarse and fine structures in the inputs,\nwavelets provide an eloquent and compact language for time series forecasting\nthat simplifies learning. Empirical results on a comprehensive benchmark,\nincluding 42 datasets for both in-domain and zero-shot settings, show that\nWaveToken: i) provides better accuracy than recently proposed foundation models\nfor forecasting while using a much smaller vocabulary (1024 tokens), and\nperforms on par or better than modern deep learning models trained specifically\non each dataset; and ii) exhibits superior generalization capabilities,\nachieving the best average rank across all datasets for three complementary\nmetrics. In addition, we show that our method can easily capture complex\ntemporal patterns of practical relevance that are challenging for other recent\npre-trained models, including trends, sparse spikes, and non-stationary time\nseries with varying frequencies evolving over time.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "o0Psp3BHyF6ES2KQDEwC_xheDQDFZ98tM8h4Rpl43vA",
  "pdfSize": "21412391"
}