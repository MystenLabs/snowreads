{"id":"2412.21044","title":"E2EDiff: Direct Mapping from Noise to Data for Enhanced Diffusion Models","authors":"Zhiyu Tan, WenXu Qian, Hesen Chen, Mengping Yang, Lei Chen, Hao Li","authorsParsed":[["Tan","Zhiyu",""],["Qian","WenXu",""],["Chen","Hesen",""],["Yang","Mengping",""],["Chen","Lei",""],["Li","Hao",""]],"versions":[{"version":"v1","created":"Mon, 30 Dec 2024 16:06:31 GMT"}],"updateDate":"2024-12-31","timestamp":1735574791000,"abstract":"  Diffusion models have emerged as a powerful framework for generative\nmodeling, achieving state-of-the-art performance across various tasks. However,\nthey face several inherent limitations, including a training-sampling gap,\ninformation leakage in the progressive noising process, and the inability to\nincorporate advanced loss functions like perceptual and adversarial losses\nduring training. To address these challenges, we propose an innovative\nend-to-end training framework that aligns the training and sampling processes\nby directly optimizing the final reconstruction output. Our method eliminates\nthe training-sampling gap, mitigates information leakage by treating the\ntraining process as a direct mapping from pure noise to the target data\ndistribution, and enables the integration of perceptual and adversarial losses\ninto the objective. Extensive experiments on benchmarks such as COCO30K and\nHW30K demonstrate that our approach consistently outperforms traditional\ndiffusion models, achieving superior results in terms of FID and CLIP score,\neven with reduced sampling steps. These findings highlight the potential of\nend-to-end training to advance diffusion-based generative models toward more\nrobust and efficient solutions.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OiG6UpY6Eej6eDlNzv4IBHYZWCW1QgZPi2nILgPtgD4","pdfSize":"4587723"}