{"id":"2412.12940","title":"Improving Fine-grained Visual Understanding in VLMs through Text-Only\n  Training","authors":"Dasol Choi, Guijin Son, Soo Yong Kim, Gio Paik, Seunghyeok Hong","authorsParsed":[["Choi","Dasol",""],["Son","Guijin",""],["Kim","Soo Yong",""],["Paik","Gio",""],["Hong","Seunghyeok",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 14:18:50 GMT"}],"updateDate":"2024-12-18","timestamp":1734445130000,"abstract":"  Visual-Language Models (VLMs) have become a powerful tool for bridging the\ngap between visual and linguistic understanding. However, the conventional\nlearning approaches for VLMs often suffer from limitations, such as the high\nresource requirements of collecting and training image-text paired data. Recent\nresearch has suggested that language understanding plays a crucial role in the\nperformance of VLMs, potentially indicating that text-only training could be a\nviable approach. In this work, we investigate the feasibility of enhancing\nfine-grained visual understanding in VLMs through text-only training. Inspired\nby how humans develop visual concept understanding, where rich textual\ndescriptions can guide visual recognition, we hypothesize that VLMs can also\nbenefit from leveraging text-based representations to improve their visual\nrecognition abilities. We conduct comprehensive experiments on two distinct\ndomains: fine-grained species classification and cultural visual understanding\ntasks. Our findings demonstrate that text-only training can be comparable to\nconventional image-text training while significantly reducing computational\ncosts. This suggests a more efficient and cost-effective pathway for advancing\nVLM capabilities, particularly valuable in resource-constrained environments.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"Nb_MAfjmmCFb5LOMgTkRoIerTd9lfZhbYi9TfGxiXQE","pdfSize":"257984"}