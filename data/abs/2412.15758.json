{
  "id": "2412.15758",
  "title": "Function Space Diversity for Uncertainty Prediction via Repulsive\n  Last-Layer Ensembles",
  "authors": "Sophie Steger, Christian Knoll, Bernhard Klein, Holger Fr\\\"oning,\n  Franz Pernkopf",
  "authorsParsed": [
    [
      "Steger",
      "Sophie",
      ""
    ],
    [
      "Knoll",
      "Christian",
      ""
    ],
    [
      "Klein",
      "Bernhard",
      ""
    ],
    [
      "Fr√∂ning",
      "Holger",
      ""
    ],
    [
      "Pernkopf",
      "Franz",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 10:24:08 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734690248000,
  "abstract": "  Bayesian inference in function space has gained attention due to its\nrobustness against overparameterization in neural networks. However,\napproximating the infinite-dimensional function space introduces several\nchallenges. In this work, we discuss function space inference via particle\noptimization and present practical modifications that improve uncertainty\nestimation and, most importantly, make it applicable for large and pretrained\nnetworks. First, we demonstrate that the input samples, where particle\npredictions are enforced to be diverse, are detrimental to the model\nperformance. While diversity on training data itself can lead to underfitting,\nthe use of label-destroying data augmentation, or unlabeled out-of-distribution\ndata can improve prediction diversity and uncertainty estimates. Furthermore,\nwe take advantage of the function space formulation, which imposes no\nrestrictions on network parameterization other than sufficient flexibility.\nInstead of using full deep ensembles to represent particles, we propose a\nsingle multi-headed network that introduces a minimal increase in parameters\nand computation. This allows seamless integration to pretrained networks, where\nthis repulsive last-layer ensemble can be used for uncertainty aware\nfine-tuning at minimal additional cost. We achieve competitive results in\ndisentangling aleatoric and epistemic uncertainty for active learning,\ndetecting out-of-domain data, and providing calibrated uncertainty estimates\nunder distribution shifts with minimal compute and memory.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "ruJULiHekucYQgUBmAFV-h_C03kRrx-9wiSUsX4fcKo",
  "pdfSize": "3466240"
}