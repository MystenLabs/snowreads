{"id":"2407.02402","title":"Assessing the Code Clone Detection Capability of Large Language Models","authors":"Zixian Zhang and Takfarinas Saber","authorsParsed":[["Zhang","Zixian",""],["Saber","Takfarinas",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 16:20:44 GMT"}],"updateDate":"2024-07-03","timestamp":1719937244000,"abstract":"  This study aims to assess the performance of two advanced Large Language\nModels (LLMs), GPT-3.5 and GPT-4, in the task of code clone detection. The\nevaluation involves testing the models on a variety of code pairs of different\nclone types and levels of similarity, sourced from two datasets: BigCloneBench\n(human-made) and GPTCloneBench (LLM-generated). Findings from the study\nindicate that GPT-4 consistently surpasses GPT-3.5 across all clone types. A\ncorrelation was observed between the GPTs' accuracy at identifying code clones\nand code similarity, with both GPT models exhibiting low effectiveness in\ndetecting the most complex Type-4 code clones. Additionally, GPT models\ndemonstrate a higher performance identifying code clones in LLM-generated code\ncompared to humans-generated code. However, they do not reach impressive\naccuracy. These results emphasize the imperative for ongoing enhancements in\nLLM capabilities, particularly in the recognition of code clones and in\nmitigating their predisposition towards self-generated code clones--which is\nlikely to become an issue as software engineers are more numerous to leverage\nLLM-enabled code generation and code refactoring tools.\n","subjects":["Computing Research Repository/Software Engineering","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pF--lHNWWSpJdc3ArP4AATq6uxGUro3MDf-QNAV0ntk","pdfSize":"835894","objectId":"0x3db3af60eb28b50c9421418873a85472b3917490dd5f7f8cb4f4406457b6b951","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
