{
  "id": "2412.15739",
  "title": "VORD: Visual Ordinal Calibration for Mitigating Object Hallucinations in\n  Large Vision-Language Models",
  "authors": "Dexter Neo, Tsuhan Chen",
  "authorsParsed": [
    [
      "Neo",
      "Dexter",
      ""
    ],
    [
      "Chen",
      "Tsuhan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 10:00:26 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734688826000,
  "abstract": "  Large Vision-Language Models (LVLMs) have made remarkable developments along\nwith the recent surge of large language models. Despite their advancements,\nLVLMs have a tendency to generate plausible yet inaccurate or inconsistent\ninformation based on the provided source content. This phenomenon, also known\nas ``hallucinations\" can have serious downstream implications during the\ndeployment of LVLMs. To address this, we present VORD a simple and effective\nmethod that alleviates hallucinations by calibrating token predictions based on\nordinal relationships between modified image pairs. VORD is presented in two\nforms: 1.) a minimalist training-free variant which eliminates implausible\ntokens from modified image pairs, and 2.) a trainable objective function that\npenalizes unlikely tokens. Our experiments demonstrate that VORD delivers\nbetter calibration and effectively mitigates object hallucinations on a\nwide-range of LVLM benchmarks.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "HEJBWZyHDvE6n1CFlSqgwIj5g4A4_hEpGZaLpYTmz9I",
  "pdfSize": "18305196"
}