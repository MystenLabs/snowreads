{
  "id": "2412.09919",
  "title": "B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal\n  Tokens",
  "authors": "Zhuqiang Lu, Zhenfei Yin, Mengwei He, Zhihui Wang, Zicheng Liu,\n  Zhiyong Wang and Kun Hu",
  "authorsParsed": [
    [
      "Lu",
      "Zhuqiang",
      ""
    ],
    [
      "Yin",
      "Zhenfei",
      ""
    ],
    [
      "He",
      "Mengwei",
      ""
    ],
    [
      "Wang",
      "Zhihui",
      ""
    ],
    [
      "Liu",
      "Zicheng",
      ""
    ],
    [
      "Wang",
      "Zhiyong",
      ""
    ],
    [
      "Hu",
      "Kun",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 07:13:40 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734074020000,
  "abstract": "  Recently, Vision Large Language Models (VLLMs) integrated with vision\nencoders have shown promising performance in vision understanding. The key of\nVLLMs is to encode visual content into sequences of visual tokens, enabling\nVLLMs to simultaneously process both visual and textual content. However,\nunderstanding videos, especially long videos, remain a challenge to VLLMs as\nthe number of visual tokens grows rapidly when encoding videos, resulting in\nthe risk of exceeding the context window of VLLMs and introducing heavy\ncomputation burden. To restrict the number of visual tokens, existing VLLMs\neither: (1) uniformly downsample videos into a fixed number of frames or (2)\nreducing the number of visual tokens encoded from each frame. We argue the\nformer solution neglects the rich temporal cue in videos and the later\noverlooks the spatial details in each frame. In this work, we present\nBalanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively\nleverage task relevant spatio-temporal cues while restricting the number of\nvisual tokens under the VLLM context window length. At the core of our method,\nwe devise a text-conditioned adaptive frame selection module to identify frames\nrelevant to the visual understanding task. The selected frames are then\nde-duplicated using a temporal frame token merging technique. The visual tokens\nof the selected frames are processed through a spatial token sampling module\nand an optional spatial token merging strategy to achieve precise control over\nthe token count. Experimental results show that B-VLLM is effective in\nbalancing the number of frames and visual tokens in video understanding,\nyielding superior performance on various video understanding benchmarks. Our\ncode is available at https://github.com/zhuqiangLu/B-VLLM.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "E9yB0VU8kfbFbc6EP2-6ijYXFmamooNtisSe-eKCJO8",
  "pdfSize": "1445528"
}