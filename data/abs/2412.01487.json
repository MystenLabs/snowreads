{"id":"2412.01487","title":"FastRM: An efficient and automatic explainability framework for\n  multimodal generative models","authors":"Gabriela Ben-Melech Stan, Estelle Aflalo, Man Luo, Shachar Rosenman,\n  Tiep Le, Sayak Paul, Shao-Yen Tseng, Vasudev Lal","authorsParsed":[["Stan","Gabriela Ben-Melech",""],["Aflalo","Estelle",""],["Luo","Man",""],["Rosenman","Shachar",""],["Le","Tiep",""],["Paul","Sayak",""],["Tseng","Shao-Yen",""],["Lal","Vasudev",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 13:39:29 GMT"},{"version":"v2","created":"Sun, 23 Feb 2025 14:35:56 GMT"}],"updateDate":"2025-02-25","timestamp":1733146769000,"abstract":"  Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning\ncapabilities over textual and visual inputs. However, these models remain prone\nto generating misinformation. Identifying and mitigating ungrounded responses\nis crucial for developing trustworthy AI. Traditional explainability methods\nsuch as gradient-based relevancy maps, offer insight into the decision process\nof models, but are often computationally expensive and unsuitable for real-time\noutput validation. In this work, we introduce FastRM, an efficient method for\npredicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides\nboth quantitative and qualitative assessment of model confidence. Experimental\nresults demonstrate that FastRM achieves a 99.8% reduction in computation time\nand a 44.4% reduction in memory footprint compared to traditional relevancy map\ngeneration. FastRM allows explainable AI to be more practical and scalable,\nthereby promoting its deployment in real-world applications and enabling users\nto more effectively evaluate the reliability of model outputs.\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Zw5lZBkybCKLzDh0OFQ1keUJMMnJziCXk1kvVKl7GsI","pdfSize":"9978391"}