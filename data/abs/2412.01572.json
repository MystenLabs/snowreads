{"id":"2412.01572","title":"MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation\n  through Question Complexity","authors":"Xiaqiang Tang, Qiang Gao, Jian Li, Nan Du, Qi Li, Sihong Xie","authorsParsed":[["Tang","Xiaqiang",""],["Gao","Qiang",""],["Li","Jian",""],["Du","Nan",""],["Li","Qi",""],["Xie","Sihong",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 14:55:02 GMT"},{"version":"v2","created":"Tue, 3 Dec 2024 06:58:06 GMT"},{"version":"v3","created":"Sat, 7 Dec 2024 07:17:46 GMT"},{"version":"v4","created":"Wed, 1 Jan 2025 08:52:20 GMT"}],"updateDate":"2025-01-03","timestamp":1733151302000,"abstract":"  Retrieval Augmented Generation (RAG) has proven to be highly effective in\nboosting the generative performance of language model in knowledge-intensive\ntasks. However, existing RAG framework either indiscriminately perform\nretrieval or rely on rigid single-class classifiers to select retrieval\nmethods, leading to inefficiencies and suboptimal performance across queries of\nvarying complexity. To address these challenges, we propose a reinforcement\nlearning-based framework that dynamically selects the most suitable retrieval\nstrategy based on query complexity. % our solution Our approach leverages a\nmulti-armed bandit algorithm, which treats each retrieval method as a distinct\n``arm'' and adapts the selection process by balancing exploration and\nexploitation. Additionally, we introduce a dynamic reward function that\nbalances accuracy and efficiency, penalizing methods that require more\nretrieval steps, even if they lead to a correct result. Our method achieves new\nstate of the art results on multiple single-hop and multi-hop datasets while\nreducing retrieval costs. Our code are available at\nhttps://github.com/FUTUREEEEEE/MBA .\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"qUCPkPHc63S1yrAdGCD53uv2RhfX19SF5NgQous-Unw","pdfSize":"409498"}