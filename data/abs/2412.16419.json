{
  "id": "2412.16419",
  "title": "Amortising Variational Bayesian Inference over prior hyperparameters\n  with a Normalising Flow",
  "authors": "Laura Battaglia and Geoff Nicholls",
  "authorsParsed": [
    [
      "Battaglia",
      "Laura",
      ""
    ],
    [
      "Nicholls",
      "Geoff",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 00:48:43 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734742123000,
  "abstract": "  In Bayesian inference prior hyperparameters are chosen subjectively or\nestimated using empirical Bayes methods. Generalised Bayesian Inference also\nhas hyperparameters (the learning rate, and parameters of the loss). As part of\nthe Generalised-Bayes workflow it is necessary to check sensitivity to the\nchoice of hyperparameters, but running MCMC or fitting a variational\napproximation at each hyperparameter setting is impractical when there are more\nthan a few hyperparameters. Simulation Based Inference has been used to\namortise over data and hyperparameters and can be useful for Bayesian problems.\nHowever, there is no Simulation Based Inference for Generalised Bayes\nposteriors, as there is no generative model for the data. Working with a\nvariational family parameterised by a normalising flow, we show how to fit a\nvariational Generalised Bayes posterior, amortised over all hyperparameters.\nThis may be sampled very efficiently at different hyperparameter values without\nrefitting, and supports efficient robustness checks and hyperparameter\nselection. We show that there exist amortised normalising-flow architectures\nwhich are universal approximators. We test our approach on a relatively\nlarge-scale application of Generalised Bayesian Inference. The code is\navailable online.\n",
  "subjects": [
    "Statistics/Computation"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "g5QZaj9SuYMsUCUAK5cIgichb6bvF_4Uizvs7GgwXgE",
  "pdfSize": "5302425"
}