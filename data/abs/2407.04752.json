{"id":"2407.04752","title":"SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking","authors":"Xingrun Xing, Boyan Gao, Zheng Zhang, David A. Clifton, Shitao Xiao,\n  Li Du, Guoqi Li, Jiajun Zhang","authorsParsed":[["Xing","Xingrun",""],["Gao","Boyan",""],["Zhang","Zheng",""],["Clifton","David A.",""],["Xiao","Shitao",""],["Du","Li",""],["Li","Guoqi",""],["Zhang","Jiajun",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 08:37:17 GMT"}],"updateDate":"2024-07-09","timestamp":1720168637000,"abstract":"  The recent advancements in large language models (LLMs) with billions of\nparameters have significantly boosted their performance across various\nreal-world applications. However, the inference processes for these models\nrequire substantial energy and computational resources, presenting considerable\ndeployment challenges. In contrast, human brains, which contain approximately\n86 billion biological neurons, exhibit significantly greater energy efficiency\ncompared to LLMs with a similar number of parameters. Inspired by this, we\nredesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms,\nemulating the efficient behavior of the human brain. We propose the first\nspiking large language model as recent LLMs termed SpikeLLM. Coupled with the\nproposed model, a novel spike-driven quantization framework named Optimal Brain\nSpiking is introduced to reduce the energy cost and accelerate inference speed\nvia two essential approaches: first (second)-order differentiation-based\nsalient channel detection, and per-channel salient outlier expansion with\nGeneralized Integrate-and-Fire neurons. Our proposed spike-driven quantization\ncan plug in main streams of quantization training methods. In the OmniQuant\npipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and\nimproves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W\nmodel. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization,\nwhich achieves additive in all linear layers. Compared with PB-LLM with similar\noperations, SpikeLLM also exceeds significantly. We will release our code on\nGitHub.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"RYq62wzs-NhD7ZWRpZcipR2BV8DS7A2zqBlrDo05uMo","pdfSize":"2185498"}