{
  "id": "2412.18626",
  "title": "Why Do Large Language Models (LLMs) Struggle to Count Letters?",
  "authors": "Tairan Fu, Raquel Ferrando, Javier Conde, Carlos Arriaga, and Pedro\n  Reviriego",
  "authorsParsed": [
    [
      "Fu",
      "Tairan",
      ""
    ],
    [
      "Ferrando",
      "Raquel",
      ""
    ],
    [
      "Conde",
      "Javier",
      ""
    ],
    [
      "Arriaga",
      "Carlos",
      ""
    ],
    [
      "Reviriego",
      "Pedro",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 19 Dec 2024 22:47:08 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1734648428000,
  "abstract": "  Large Language Models (LLMs) have achieved unprecedented performance on many\ncomplex tasks, being able, for example, to answer questions on almost any\ntopic. However, they struggle with other simple tasks, such as counting the\noccurrences of letters in a word, as illustrated by the inability of many LLMs\nto count the number of \"r\" letters in \"strawberry\". Several works have studied\nthis problem and linked it to the tokenization used by LLMs, to the intrinsic\nlimitations of the attention mechanism, or to the lack of character-level\ntraining data. In this paper, we conduct an experimental study to evaluate the\nrelations between the LLM errors when counting letters with 1) the frequency of\nthe word and its components in the training dataset and 2) the complexity of\nthe counting operation. We present a comprehensive analysis of the errors of\nLLMs when counting letter occurrences by evaluating a representative group of\nmodels over a large number of words. The results show a number of consistent\ntrends in the models evaluated: 1) models are capable of recognizing the\nletters but not counting them; 2) the frequency of the word and tokens in the\nword does not have a significant impact on the LLM errors; 3) there is a\npositive correlation of letter frequency with errors, more frequent letters\ntend to have more counting errors, 4) the errors show a strong correlation with\nthe number of letters or tokens in a word and 5) the strongest correlation\noccurs with the number of letters with counts larger than one, with most models\nbeing unable to correctly count words in which letters appear more than twice.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "_cdonxtOvXXdrmTZ3EC6GuzeR8y6t7KyMQml2OunCUw",
  "pdfSize": "2284077"
}