{"id":"2412.09530","title":"Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM","authors":"Han Wang, Yuxiang Nie, Yongjie Ye, Deng GuanYu, Yanjie Wang, Shuai Li,\n  Haiyang Yu, Jinghui Lu, Can Huang","authorsParsed":[["Wang","Han",""],["Nie","Yuxiang",""],["Ye","Yongjie",""],["GuanYu","Deng",""],["Wang","Yanjie",""],["Li","Shuai",""],["Yu","Haiyang",""],["Lu","Jinghui",""],["Huang","Can",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 18:20:41 GMT"}],"updateDate":"2024-12-13","timestamp":1734027641000,"abstract":"  The application of Large Vision-Language Models (LVLMs) for analyzing images\nand videos is an exciting and rapidly evolving field. In recent years, we've\nseen significant growth in high-quality image-text datasets for fine-tuning\nimage understanding, but there is still a lack of comparable datasets for\nvideos. Additionally, many VideoLLMs are extensions of single-image VLMs, which\nmay not efficiently handle the complexities of longer videos. In this study, we\nintroduce a large-scale synthetic dataset created from proprietary models,\nusing carefully designed prompts to tackle a wide range of questions. We also\nexplore a dynamic visual token compression architecture that strikes a balance\nbetween computational efficiency and performance. Our proposed \\model{}\nachieves state-of-the-art results across various video tasks and shows\nimpressive generalization, setting new baselines in multi-image understanding.\nNotably, \\model{} delivers an absolute improvement of 2.7\\% over\nLLaVA-OneVision on VideoMME and 10.7\\% on MuirBench. Codes are available at\nhttps://github.com/Hon-Wong/ByteVideoLLM\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Juy0SizY9F2R8pCTqAWSqfV8C-LkC5noB0GWUXAC2_A","pdfSize":"6619514"}