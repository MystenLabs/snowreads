{"id":"2412.02399","title":"OMENN: One Matrix to Explain Neural Networks","authors":"Adam Wr\\'obel, Miko{\\l}aj Janusz, Bartosz Zieli\\'nski, Dawid Rymarczyk","authorsParsed":[["Wróbel","Adam",""],["Janusz","Mikołaj",""],["Zieliński","Bartosz",""],["Rymarczyk","Dawid",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 11:49:01 GMT"}],"updateDate":"2024-12-04","timestamp":1733226541000,"abstract":"  Deep Learning (DL) models are often black boxes, making their decision-making\nprocesses difficult to interpret. This lack of transparency has driven\nadvancements in eXplainable Artificial Intelligence (XAI), a field dedicated to\nclarifying the reasoning behind DL model predictions. Among these,\nattribution-based methods such as LRP and GradCAM are widely used, though they\nrely on approximations that can be imprecise.\n  To address these limitations, we introduce One Matrix to Explain Neural\nNetworks (OMENN), a novel post-hoc method that represents a neural network as a\nsingle, interpretable matrix for each specific input. This matrix is\nconstructed through a series of linear transformations that represent the\nprocessing of the input by each successive layer in the neural network. As a\nresult, OMENN provides locally precise, attribution-based explanations of the\ninput across various modern models, including ViTs and CNNs. We present a\ntheoretical analysis of OMENN based on dynamic linearity property and validate\nits effectiveness with extensive tests on two XAI benchmarks, demonstrating\nthat OMENN is competitive with state-of-the-art methods.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"g7TYYFKy583iZlSS3LvtIyA8UHAQmaUsNGlLRC4b5bs","pdfSize":"41030103"}