{
  "id": "2412.02802",
  "title": "Flattering to Deceive: The Impact of Sycophantic Behavior on User Trust\n  in Large Language Model",
  "authors": "Mar\\'ia Victoria Carro",
  "authorsParsed": [
    [
      "Carro",
      "Mar√≠a Victoria",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 20:07:41 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733256461000,
  "abstract": "  Sycophancy refers to the tendency of a large language model to align its\noutputs with the user's perceived preferences, beliefs, or opinions, in order\nto look favorable, regardless of whether those statements are factually\ncorrect. This behavior can lead to undesirable consequences, such as\nreinforcing discriminatory biases or amplifying misinformation. Given that\nsycophancy is often linked to human feedback training mechanisms, this study\nexplores whether sycophantic tendencies negatively impact user trust in large\nlanguage models or, conversely, whether users consider such behavior as\nfavorable. To investigate this, we instructed one group of participants to\nanswer ground-truth questions with the assistance of a GPT specifically\ndesigned to provide sycophantic responses, while another group used the\nstandard version of ChatGPT. Initially, participants were required to use the\nlanguage model, after which they were given the option to continue using it if\nthey found it trustworthy and useful. Trust was measured through both\ndemonstrated actions and self-reported perceptions. The findings consistently\nshow that participants exposed to sycophantic behavior reported and exhibited\nlower levels of trust compared to those who interacted with the standard\nversion of the model, despite the opportunity to verify the accuracy of the\nmodel's output.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "jd2PKQVMiKUNbvR873KSQRH9wzOA6-l9r4C5MlBfR0k",
  "pdfSize": "1571546"
}