{"id":"2407.05767","title":"Nonrigid Reconstruction of Freehand Ultrasound without a Tracker","authors":"Qi Li, Ziyi Shen, Qianye Yang, Dean C. Barratt, Matthew J. Clarkson,\n  Tom Vercauteren, Yipeng Hu","authorsParsed":[["Li","Qi",""],["Shen","Ziyi",""],["Yang","Qianye",""],["Barratt","Dean C.",""],["Clarkson","Matthew J.",""],["Vercauteren","Tom",""],["Hu","Yipeng",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 09:19:40 GMT"},{"version":"v2","created":"Sun, 14 Jul 2024 10:36:37 GMT"}],"updateDate":"2024-07-16","timestamp":1720430380000,"abstract":"  Reconstructing 2D freehand Ultrasound (US) frames into 3D space without using\na tracker has recently seen advances with deep learning. Predicting good\nframe-to-frame rigid transformations is often accepted as the learning\nobjective, especially when the ground-truth labels from spatial tracking\ndevices are inherently rigid transformations. Motivated by a) the observed\nnonrigid deformation due to soft tissue motion during scanning, and b) the\nhighly sensitive prediction of rigid transformation, this study investigates\nthe methods and their benefits in predicting nonrigid transformations for\nreconstructing 3D US. We propose a novel co-optimisation algorithm for\nsimultaneously estimating rigid transformations among US frames, supervised by\nground-truth from a tracker, and a nonrigid deformation, optimised by a\nregularised registration network. We show that these two objectives can be\neither optimised using meta-learning or combined by weighting. A fast scattered\ndata interpolation is also developed for enabling frequent reconstruction and\nregistration of non-parallel US frames, during training. With a new data set\ncontaining over 357,000 frames in 720 scans, acquired from 60 subjects, the\nexperiments demonstrate that, due to an expanded thus easier-to-optimise\nsolution space, the generalisation is improved with the added deformation\nestimation, with respect to the rigid ground-truth. The global pixel\nreconstruction error (assessing accumulative prediction) is lowered from 18.48\nto 16.51 mm, compared with baseline rigid-transformation-predicting methods.\nUsing manually identified landmarks, the proposed co-optimisation also shows\npotentials in compensating nonrigid tissue motion at inference, which is not\nmeasurable by tracker-provided ground-truth. The code and data used in this\npaper are made publicly available at https://github.com/QiLi111/NR-Rec-FUS.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"du7sivIeHOqVwlD44ogGnYt3ONKQ0Q93tAb0w5twsQc","pdfSize":"2551255"}