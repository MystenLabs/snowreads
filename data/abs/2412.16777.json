{"id":"2412.16777","title":"HyperCLIP: Adapting Vision-Language models with Hypernetworks","authors":"Victor Akinwande, Mohammad Sadegh Norouzzadeh, Devin Willmott, Anna\n  Bair, Madan Ravi Ganesh, J. Zico Kolter","authorsParsed":[["Akinwande","Victor",""],["Norouzzadeh","Mohammad Sadegh",""],["Willmott","Devin",""],["Bair","Anna",""],["Ganesh","Madan Ravi",""],["Kolter","J. Zico",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 21:19:08 GMT"}],"updateDate":"2024-12-24","timestamp":1734815948000,"abstract":"  Self-supervised vision-language models trained with contrastive objectives\nform the basis of current state-of-the-art methods in AI vision tasks. The\nsuccess of these models is a direct consequence of the huge web-scale datasets\nused to train them, but they require correspondingly large vision components to\nproperly learn powerful and general representations from such a broad data\ndomain. This poses a challenge for deploying large vision-language models,\nespecially in resource-constrained environments. To address this, we propose an\nalternate vision-language architecture, called HyperCLIP, that uses a small\nimage encoder along with a hypernetwork that dynamically adapts image encoder\nweights to each new set of text inputs. All three components of the model\n(hypernetwork, image encoder, and text encoder) are pre-trained jointly\nend-to-end, and with a trained HyperCLIP model, we can generate new zero-shot\ndeployment-friendly image classifiers for any task with a single forward pass\nthrough the text encoder and hypernetwork. HyperCLIP increases the zero-shot\naccuracy of SigLIP trained models with small image encoders by up to 3% on\nImageNet and 5% on CIFAR-100 with minimal training throughput overhead.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PR200XMdQzJZ65c3k1xxyCHwD5fUzZoGcJ8WQelAgA0","pdfSize":"600170"}