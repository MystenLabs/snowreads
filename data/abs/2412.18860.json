{
  "id": "2412.18860",
  "title": "Bootstrap Your Own Context Length",
  "authors": "Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei",
  "authorsParsed": [
    [
      "Wang",
      "Liang",
      ""
    ],
    [
      "Yang",
      "Nan",
      ""
    ],
    [
      "Zhang",
      "Xingxing",
      ""
    ],
    [
      "Huang",
      "Xiaolong",
      ""
    ],
    [
      "Wei",
      "Furu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 25 Dec 2024 10:08:54 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735121334000,
  "abstract": "  We introduce a bootstrapping approach to train long-context language models\nby exploiting their short-context capabilities only. Our method utilizes a\nsimple agent workflow to synthesize diverse long-context instruction tuning\ndata, thereby eliminating the necessity for manual data collection and\nannotation. The proposed data synthesis workflow requires only a short-context\nlanguage model, a text retriever, and a document collection, all of which are\nreadily accessible within the open-source ecosystem. Subsequently, language\nmodels are fine-tuned using the synthesized data to extend their context\nlengths. In this manner, we effectively transfer the short-context capabilities\nof language models to long-context scenarios through a bootstrapping process.\nWe conduct experiments with the open-source Llama-3 family of models and\ndemonstrate that our method can successfully extend the context length to up to\n1M tokens, achieving superior performance across various benchmarks.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Information Retrieval"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "B0liySO1KzdpEiQf4L7jECt4aY4Jsc5oko9AkHgbdC8",
  "pdfSize": "559074"
}