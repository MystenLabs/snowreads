{"id":"2412.08347","title":"SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better\n  Reasoning in SLMs","authors":"Sultan Alrashed","authorsParsed":[["Alrashed","Sultan",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 12:41:36 GMT"}],"updateDate":"2024-12-12","timestamp":1733920896000,"abstract":"  We present SmolTulu-1.7b-Instruct, referenced in this report as\nSmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAI's\nTulu 3 post-training pipeline to enhance Huggingface's SmolLM2-1.7B base model.\nThrough comprehensive empirical analysis using a 135M parameter model, we\ndemonstrate that the relationship between learning rate and batch size\nsignificantly impacts model performance in a task-dependent manner. Our\nfindings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from\nhigher learning rate to batch size ratios, while pattern recognition tasks such\nas HellaSwag and IFEval show optimal performance with lower ratios. These\ninsights informed the development of SmolTulu, which achieves state-of-the-art\nperformance among sub-2B parameter models on instruction following, scoring\n67.7% on IFEval ($\\Delta$11%), and mathematical reasoning with 51.6% on GSM8K\n($\\Delta$3.4%), with an alternate version achieving scoring 57.1% on ARC\n($\\Delta5.4%$). We release our model, training recipes, and ablation studies to\nfacilitate further research in efficient model alignment, demonstrating that\ncareful adaptation of optimization dynamics can help bridge the capability gap\nbetween small and large language models.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eafxSrF9TTJHJsN3-K1jkbZafd-LhWtdBqrgWL0hWoQ","pdfSize":"998907"}