{"id":"2412.16178","title":"Context Clues: Evaluating Long Context Models for Clinical Prediction\n  Tasks on EHRs","authors":"Michael Wornow, Suhana Bedi, Miguel Angel Fuentes Hernandez, Ethan\n  Steinberg, Jason Alan Fries, Christopher R\\'e, Sanmi Koyejo, Nigam H. Shah","authorsParsed":[["Wornow","Michael",""],["Bedi","Suhana",""],["Hernandez","Miguel Angel Fuentes",""],["Steinberg","Ethan",""],["Fries","Jason Alan",""],["RÃ©","Christopher",""],["Koyejo","Sanmi",""],["Shah","Nigam H.",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 21:58:27 GMT"}],"updateDate":"2024-12-24","timestamp":1733781507000,"abstract":"  Foundation Models (FMs) trained on Electronic Health Records (EHRs) have\nachieved state-of-the-art results on numerous clinical prediction tasks.\nHowever, most existing EHR FMs have context windows of <1k tokens. This\nprevents them from modeling full patient EHRs which can exceed 10k's of events.\nRecent advancements in subquadratic long-context architectures (e.g., Mamba)\noffer a promising solution. However, their application to EHR data has not been\nwell-studied. We address this gap by presenting the first systematic evaluation\nof the effect of context length on modeling EHR data. We find that longer\ncontext models improve predictive performance -- our Mamba-based model\nsurpasses the prior state-of-the-art on 9/14 tasks on the EHRSHOT prediction\nbenchmark. For clinical applications, however, model performance alone is\ninsufficient -- robustness to the unique properties of EHR is crucial. Thus, we\nalso evaluate models across three previously underexplored properties of EHR\ndata: (1) the prevalence of \"copy-forwarded\" diagnoses which creates artificial\nrepetition of tokens within EHR sequences; (2) the irregular time intervals\nbetween EHR events which can lead to a wide range of timespans within a context\nwindow; and (3) the natural increase in disease complexity over time which\nmakes later tokens in the EHR harder to predict than earlier ones. Stratifying\nour EHRSHOT results, we find that higher levels of each property correlate\nnegatively with model performance, but that longer context models are more\nrobust to more extreme levels of these properties. Our work highlights the\npotential for using long-context architectures to model EHR data, and offers a\ncase study for identifying new challenges in modeling sequential data motivated\nby domains outside of natural language. We release our models and code at:\nhttps://github.com/som-shahlab/long_context_clues\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Computational Engineering, Finance, and Science"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fA7RC-7GKwrfoEuM875SHLo408HYorB7c26PK2klvvw","pdfSize":"7307130"}