{
  "id": "2412.11912",
  "title": "CharacterBench: Benchmarking Character Customization of Large Language\n  Models",
  "authors": "Jinfeng Zhou, Yongkang Huang, Bosi Wen, Guanqun Bi, Yuxuan Chen, Pei\n  Ke, Zhuang Chen, Xiyao Xiao, Libiao Peng, Kuntian Tang, Rongsheng Zhang, Le\n  Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang",
  "authorsParsed": [
    [
      "Zhou",
      "Jinfeng",
      ""
    ],
    [
      "Huang",
      "Yongkang",
      ""
    ],
    [
      "Wen",
      "Bosi",
      ""
    ],
    [
      "Bi",
      "Guanqun",
      ""
    ],
    [
      "Chen",
      "Yuxuan",
      ""
    ],
    [
      "Ke",
      "Pei",
      ""
    ],
    [
      "Chen",
      "Zhuang",
      ""
    ],
    [
      "Xiao",
      "Xiyao",
      ""
    ],
    [
      "Peng",
      "Libiao",
      ""
    ],
    [
      "Tang",
      "Kuntian",
      ""
    ],
    [
      "Zhang",
      "Rongsheng",
      ""
    ],
    [
      "Zhang",
      "Le",
      ""
    ],
    [
      "Lv",
      "Tangjie",
      ""
    ],
    [
      "Hu",
      "Zhipeng",
      ""
    ],
    [
      "Wang",
      "Hongning",
      ""
    ],
    [
      "Huang",
      "Minlie",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 15:55:34 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734364534000,
  "abstract": "  Character-based dialogue (aka role-playing) enables users to freely customize\ncharacters for interaction, which often relies on LLMs, raising the need to\nevaluate LLMs' character customization capability. However, existing benchmarks\nfail to ensure a robust evaluation as they often only involve a single\ncharacter category or evaluate limited dimensions. Moreover, the sparsity of\ncharacter features in responses makes feature-focused generative evaluation\nboth ineffective and inefficient. To address these issues, we propose\nCharacterBench, the largest bilingual generative benchmark, with 22,859\nhuman-annotated samples covering 3,956 characters from 25 detailed character\ncategories. We define 11 dimensions of 6 aspects, classified as sparse and\ndense dimensions based on whether character features evaluated by specific\ndimensions manifest in each response. We enable effective and efficient\nevaluation by crafting tailored queries for each dimension to induce\ncharacters' responses related to specific dimensions. Further, we develop\nCharacterJudge model for cost-effective and stable evaluations. Experiments\nshow its superiority over SOTA automatic judges (e.g., GPT-4) and our\nbenchmark's potential to optimize LLMs' character customization. Our repository\nis at https://github.com/thu-coai/CharacterBench.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "b4avQUEHp-ubrF4qcDQnwBTYqvdEGUhzEUkKdBnDDRQ",
  "pdfSize": "7168289"
}