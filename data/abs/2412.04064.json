{"id":"2412.04064","title":"Graph Neural Networks Need Cluster-Normalize-Activate Modules","authors":"Arseny Skryagin, Felix Divo, Mohammad Amin Ali, Devendra Singh Dhami,\n  Kristian Kersting","authorsParsed":[["Skryagin","Arseny",""],["Divo","Felix",""],["Ali","Mohammad Amin",""],["Dhami","Devendra Singh",""],["Kersting","Kristian",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 10:59:20 GMT"}],"updateDate":"2024-12-06","timestamp":1733396360000,"abstract":"  Graph Neural Networks (GNNs) are non-Euclidean deep learning models for\ngraph-structured data. Despite their successful and diverse applications,\noversmoothing prohibits deep architectures due to node features converging to a\nsingle fixed point. This severely limits their potential to solve complex\ntasks. To counteract this tendency, we propose a plug-and-play module\nconsisting of three steps: Cluster-Normalize-Activate (CNA). By applying CNA\nmodules, GNNs search and form super nodes in each layer, which are normalized\nand activated individually. We demonstrate in node classification and property\nprediction tasks that CNA significantly improves the accuracy over the\nstate-of-the-art. Particularly, CNA reaches 94.18% and 95.75% accuracy on Cora\nand CiteSeer, respectively. It further benefits GNNs in regression tasks as\nwell, reducing the mean squared error compared to all baselines. At the same\ntime, GNNs with CNA require substantially fewer learnable parameters than\ncompeting architectures.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9DoSmMB9flHHLXa0JCCTd8wbladPRgZwO3EX80CAFsI","pdfSize":"6547018"}