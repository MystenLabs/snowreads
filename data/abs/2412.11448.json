{"id":"2412.11448","title":"TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated\n  Learning","authors":"Gangqiang Hu, Jianfeng Lu, Jianmin Han, Shuqin Cao, Jing Liu, Hao Fu","authorsParsed":[["Hu","Gangqiang",""],["Lu","Jianfeng",""],["Han","Jianmin",""],["Cao","Shuqin",""],["Liu","Jing",""],["Fu","Hao",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 05:02:50 GMT"},{"version":"v2","created":"Tue, 17 Dec 2024 13:15:24 GMT"},{"version":"v3","created":"Thu, 19 Dec 2024 12:46:27 GMT"}],"updateDate":"2024-12-20","timestamp":1734325370000,"abstract":"  Due to the sensitivity of data, Federated Learning (FL) is employed to enable\ndistributed machine learning while safeguarding data privacy and accommodating\nthe requirements of various devices. However, in the context of\nsemi-decentralized FL, clients' communication and training states are dynamic.\nThis variability arises from local training fluctuations, heterogeneous data\ndistributions, and intermittent client participation. Most existing studies\nprimarily focus on stable client states, neglecting the dynamic challenges\ninherent in real-world scenarios. To tackle this issue, we propose a\nTRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client\nstates and contributions, enhancing model training efficiency through selective\nclient participation. We focus on a semi-decentralized FL framework where edge\nservers and clients train a shared global model using unreliable intra-cluster\nmodel aggregation and inter-cluster model consensus. First, we propose an\nadaptive hidden semi-Markov model to estimate clients' communication states and\ncontributions. Next, we address a client-server association optimization\nproblem to minimize global training loss. Using convergence analysis, we\npropose a greedy client scheduling algorithm. Finally, our experiments\nconducted on real-world datasets demonstrate that TRAIL outperforms\nstate-of-the-art baselines, achieving an improvement of 8.7% in test accuracy\nand a reduction of 15.3% in training loss.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"n1vDucboZDM24PpI5HUjvf1HXAu5xbev1Ep410dfHb4","pdfSize":"558773"}