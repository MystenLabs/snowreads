{"id":"2407.07263","title":"Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language\n  Models","authors":"Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi,\n  Bryan Catanzaro","authorsParsed":[["Parmar","Jupinder",""],["Satheesh","Sanjev",""],["Patwary","Mostofa",""],["Shoeybi","Mohammad",""],["Catanzaro","Bryan",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 22:37:59 GMT"}],"updateDate":"2024-07-11","timestamp":1720564679000,"abstract":"  As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2ozriFDiAKAtMYcDLTKfvQlsLCnSbVMhod35QJ7omQo","pdfSize":"540438"}