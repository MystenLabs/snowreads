{"id":"2407.04871","title":"Improving Knowledge Distillation in Transfer Learning with Layer-wise\n  Learning Rates","authors":"Shirley Kokane, Mostofa Rafid Uddin and Min Xu","authorsParsed":[["Kokane","Shirley",""],["Uddin","Mostofa Rafid",""],["Xu","Min",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 21:35:17 GMT"}],"updateDate":"2024-07-09","timestamp":1720215317000,"abstract":"  Transfer learning methods start performing poorly when the complexity of the\nlearning task is increased. Most of these methods calculate the cumulative\ndifferences of all the matched features and then use them to back-propagate\nthat loss through all the layers. Contrary to these methods, in this work, we\npropose a novel layer-wise learning scheme that adjusts learning parameters per\nlayer as a function of the differences in the Jacobian/Attention/Hessian of the\noutput activations w.r.t. the network parameters. We applied this novel scheme\nfor attention map-based and derivative-based (first and second order) transfer\nlearning methods. We received improved learning performance and stability\nagainst a wide range of datasets. From extensive experimental evaluation, we\nobserved that the performance boost achieved by our method becomes more\nsignificant with the increasing difficulty of the learning task.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0xgTw5aEkkPbXzqwb7pebfsppfsIiyzfXc33YXKRfcg","pdfSize":"870932"}