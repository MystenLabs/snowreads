{"id":"2407.12869","title":"Bilingual Adaptation of Monolingual Foundation Models","authors":"Gurpreet Gosal, Yishi Xu, Gokul Ramakrishnan, Rituraj Joshi, Avraham\n  Sheinin, Zhiming (Charles) Chen, Biswajit Mishra, Natalia Vassilieva, Joel\n  Hestness, Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Onkar Pandit, Satheesh\n  Katipomu, Samta Kamboj, Samujjwal Ghosh, Rahul Pal, Parvez Mullah, Soundar\n  Doraiswamy, Mohamed El Karim Chami, Preslav Nakov","authorsParsed":[["Gosal","Gurpreet","","Charles"],["Xu","Yishi","","Charles"],["Ramakrishnan","Gokul","","Charles"],["Joshi","Rituraj","","Charles"],["Sheinin","Avraham","","Charles"],["Zhiming","","","Charles"],["Chen","",""],["Mishra","Biswajit",""],["Vassilieva","Natalia",""],["Hestness","Joel",""],["Sengupta","Neha",""],["Sahu","Sunil Kumar",""],["Jia","Bokang",""],["Pandit","Onkar",""],["Katipomu","Satheesh",""],["Kamboj","Samta",""],["Ghosh","Samujjwal",""],["Pal","Rahul",""],["Mullah","Parvez",""],["Doraiswamy","Soundar",""],["Chami","Mohamed El Karim",""],["Nakov","Preslav",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 21:09:38 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 22:51:39 GMT"}],"updateDate":"2024-07-29","timestamp":1720904978000,"abstract":"  We present an efficient method for adapting a monolingual Large Language\nModel (LLM) to another language, addressing challenges of catastrophic\nforgetting and tokenizer limitations. We focus this study on adapting Llama 2\nto Arabic. Our two-stage approach begins with expanding the vocabulary and\ntraining only the embeddings matrix, followed by full model continual\npre-training on a bilingual corpus. By continually pre-training on a mix of\nArabic and English corpora, the model retains its proficiency in English while\nacquiring capabilities in Arabic. Our approach results in significant\nimprovements in Arabic and slight enhancements in English, demonstrating\ncost-effective cross-lingual transfer. We perform ablations on embedding\ninitialization techniques, data mix ratios, and learning rates and release a\ndetailed training recipe. To demonstrate generalizability of this approach we\nalso adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"46yQfaN7F5q5fiB8qm_9IpfBrRkTZ1YSeLZM4G9btxM","pdfSize":"216051"}