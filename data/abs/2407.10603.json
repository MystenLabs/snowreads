{"id":"2407.10603","title":"Leave No Knowledge Behind During Knowledge Distillation: Towards\n  Practical and Effective Knowledge Distillation for Code-Switching ASR Using\n  Realistic Data","authors":"Liang-Hsuan Tseng, Zih-Ching Chen, Wei-Shun Chang, Cheng-Kuang Lee,\n  Tsung-Ren Huang, Hung-yi Lee","authorsParsed":[["Tseng","Liang-Hsuan",""],["Chen","Zih-Ching",""],["Chang","Wei-Shun",""],["Lee","Cheng-Kuang",""],["Huang","Tsung-Ren",""],["Lee","Hung-yi",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 10:25:14 GMT"}],"updateDate":"2024-07-16","timestamp":1721039114000,"abstract":"  Recent advances in automatic speech recognition (ASR) often rely on large\nspeech foundation models for generating high-quality transcriptions. However,\nthese models can be impractical due to limited computing resources. The\nsituation is even more severe in terms of more realistic or difficult\nscenarios, such as code-switching ASR (CS-ASR). To address this, we present a\nframework for developing more efficient models for CS-ASR through knowledge\ndistillation using realistic speech-only data. Our proposed method, Leave No\nKnowledge Behind During Knowledge Distillation (K$^2$D), leverages both the\nteacher model's knowledge and additional insights from a small auxiliary model.\nWe evaluate our approach on two in-domain and two out-domain datasets,\ndemonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled\nrealistic data, we have successfully obtained a 2-time smaller model with\n5-time faster generation speed while outperforming the baseline methods and the\nteacher model on all the testing sets. We have made our model publicly\navailable on Hugging Face\n(https://huggingface.co/andybi7676/k2d-whisper.zh-en).\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"okYdZ1bh_IBgMLnNrDn-GyhKDW4FjHDU8KMYQNFeafw","pdfSize":"1181949"}