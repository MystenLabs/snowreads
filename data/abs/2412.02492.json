{
  "id": "2412.02492",
  "title": "The Cost of Consistency: Submodular Maximization with Constant Recourse",
  "authors": "Paul D\\\"utting, Federico Fusco, Silvio Lattanzi, Ashkan Norouzi-Fard,\n  Ola Svensson, Morteza Zadimoghaddam",
  "authorsParsed": [
    [
      "DÃ¼tting",
      "Paul",
      ""
    ],
    [
      "Fusco",
      "Federico",
      ""
    ],
    [
      "Lattanzi",
      "Silvio",
      ""
    ],
    [
      "Norouzi-Fard",
      "Ashkan",
      ""
    ],
    [
      "Svensson",
      "Ola",
      ""
    ],
    [
      "Zadimoghaddam",
      "Morteza",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 15:06:07 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733238367000,
  "abstract": "  In this work, we study online submodular maximization, and how the\nrequirement of maintaining a stable solution impacts the approximation. In\nparticular, we seek bounds on the best-possible approximation ratio that is\nattainable when the algorithm is allowed to make at most a constant number of\nupdates per step. We show a tight information-theoretic bound of $\\tfrac{2}{3}$\nfor general monotone submodular functions, and an improved (also tight) bound\nof $\\tfrac{3}{4}$ for coverage functions. Since both these bounds are attained\nby non poly-time algorithms, we also give a poly-time randomized algorithm that\nachieves a $0.51$-approximation. Combined with an information-theoretic\nhardness of $\\tfrac{1}{2}$ for deterministic algorithms from prior work, our\nwork thus shows a separation between deterministic and randomized algorithms,\nboth information theoretically and for poly-time algorithms.\n",
  "subjects": [
    "Computer Science/Data Structures and Algorithms",
    "Computer Science/Machine Learning",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Nqyhw0TQ0MoQUtZF2VqSFHo8_KvvGJYUN9MSHU0Iv_w",
  "pdfSize": "2773948"
}