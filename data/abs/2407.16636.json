{"id":"2407.16636","title":"Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models\n  for Autonomous Vehicles","authors":"Seamie Hayes, Sushil Sharma, Ciar\\'an Eising","authorsParsed":[["Hayes","Seamie",""],["Sharma","Sushil",""],["Eising","Ciar√°n",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 16:52:42 GMT"},{"version":"v2","created":"Wed, 24 Jul 2024 13:04:19 GMT"}],"updateDate":"2024-07-25","timestamp":1721753562000,"abstract":"  Fusing different sensor modalities can be a difficult task, particularly if\nthey are asynchronous. Asynchronisation may arise due to long processing times\nor improper synchronisation during calibration, and there must exist a way to\nstill utilise this previous information for the purpose of safe driving, and\nobject detection in ego vehicle/ multi-agent trajectory prediction.\nDifficulties arise in the fact that the sensor modalities have captured\ninformation at different times and also at different positions in space.\nTherefore, they are not spatially nor temporally aligned. This paper will\ninvestigate the challenge of radar and LiDAR sensors being asynchronous\nrelative to the camera sensors, for various time latencies. The spatial\nalignment will be resolved before lifting into BEV space via the transformation\nof the radar/LiDAR point clouds into the new ego frame coordinate system. Only\nafter this can we concatenate the radar/LiDAR point cloud and lifted camera\nfeatures. Temporal alignment will be remedied for radar data only, we will\nimplement a novel method of inferring the future radar point positions using\nthe velocity information. Our approach to resolving the issue of sensor\nasynchrony yields promising results. We demonstrate velocity information can\ndrastically improve IoU for asynchronous datasets, as for a time latency of 360\nmilliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time\nlatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR\n(C+L) model by 0.18 IoU. This is an advancement in utilising the\noften-neglected radar sensor modality, which is less favoured than LiDAR for\nautonomous driving purposes.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ocyF9KgmQmWbh1QT-HEtPL9fX18u_LfyNtabXtBbNeA","pdfSize":"9303896"}