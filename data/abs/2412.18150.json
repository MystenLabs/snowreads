{"id":"2412.18150","title":"EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive\n  Human Annotations for Text-to-Image Generation Model Evaluation","authors":"Shuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui,\n  Yunqiu Wang, Yang Tai, Jingwei Sun, Chunle Guo, Chongyi Li","authorsParsed":[["Han","Shuhao",""],["Fan","Haotian",""],["Fu","Jiachen",""],["Li","Liang",""],["Li","Tao",""],["Cui","Junhui",""],["Wang","Yunqiu",""],["Tai","Yang",""],["Sun","Jingwei",""],["Guo","Chunle",""],["Li","Chongyi",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 04:08:25 GMT"},{"version":"v2","created":"Wed, 25 Dec 2024 15:00:36 GMT"}],"updateDate":"2024-12-30","timestamp":1735013305000,"abstract":"  Recently, Text-to-Image (T2I) generation models have achieved significant\nadvancements. Correspondingly, many automated metrics have emerged to evaluate\nthe image-text alignment capabilities of generative models. However, the\nperformance comparison among these automated metrics is limited by existing\nsmall datasets. Additionally, these datasets lack the capacity to assess the\nperformance of automated metrics at a fine-grained level. In this study, we\ncontribute an EvalMuse-40K benchmark, gathering 40K image-text pairs with\nfine-grained human annotations for image-text alignment-related tasks. In the\nconstruction process, we employ various strategies such as balanced prompt\nsampling and data re-annotation to ensure the diversity and reliability of our\nbenchmark. This allows us to comprehensively evaluate the effectiveness of\nimage-text alignment metrics for T2I models. Meanwhile, we introduce two new\nmethods to evaluate the image-text alignment capabilities of T2I models:\nFGA-BLIP2 which involves end-to-end fine-tuning of a vision-language model to\nproduce fine-grained image-text alignment scores and PN-VQA which adopts a\nnovel positive-negative VQA manner in VQA models for zero-shot fine-grained\nevaluation. Both methods achieve impressive performance in image-text alignment\nevaluations. We also use our methods to rank current AIGC models, in which the\nresults can serve as a reference source for future study and promote the\ndevelopment of T2I generation. The data and code will be made publicly\navailable.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6TRwjF5LaaimNoXWWp83EoBZ5rSSBSvmFIYsjFMkR_M","pdfSize":"4847299"}