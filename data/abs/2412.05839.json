{"id":"2412.05839","title":"DiTer++: Diverse Terrain and Multi-modal Dataset for Multi-Robot SLAM in\n  Multi-session Environments","authors":"Juwon Kim, Hogyun Kim, Seokhwan Jeong, Youngsik Shin, and Younggun Cho","authorsParsed":[["Kim","Juwon",""],["Kim","Hogyun",""],["Jeong","Seokhwan",""],["Shin","Youngsik",""],["Cho","Younggun",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 07:21:21 GMT"}],"updateDate":"2024-12-10","timestamp":1733642481000,"abstract":"  We encounter large-scale environments where both structured and unstructured\nspaces coexist, such as on campuses. In this environment, lighting conditions\nand dynamic objects change constantly. To tackle the challenges of large-scale\nmapping under such conditions, we introduce DiTer++, a diverse terrain and\nmulti-modal dataset designed for multi-robot SLAM in multi-session\nenvironments. According to our datasets' scenarios, Agent-A and Agent-B scan\nthe area designated for efficient large-scale mapping day and night,\nrespectively. Also, we utilize legged robots for terrain-agnostic traversing.\nTo generate the ground-truth of each robot, we first build the survey-grade\nprior map. Then, we remove the dynamic objects and outliers from the prior map\nand extract the trajectory through scan-to-map matching. Our dataset and\nsupplement materials are available at\nhttps://sites.google.com/view/diter-plusplus/.\n","subjects":["Computer Science/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"AmigAudvBVqiaw3BeuI86yI9-S2SN9SNU9zRkkArxy4","pdfSize":"12955055"}