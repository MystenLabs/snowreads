{"id":"2412.07430","title":"Knowledge Graph Guided Evaluation of Abstention Techniques","authors":"Kinshuk Vasisht, Navreet Kaur, Danish Pruthi","authorsParsed":[["Vasisht","Kinshuk",""],["Kaur","Navreet",""],["Pruthi","Danish",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 11:40:47 GMT"},{"version":"v2","created":"Sat, 8 Feb 2025 19:36:28 GMT"}],"updateDate":"2025-02-11","timestamp":1733830847000,"abstract":"  To deploy language models safely, it is crucial that they abstain from\nresponding to inappropriate requests. Several prior studies test the safety\npromises of models based on their effectiveness in blocking malicious requests.\nIn this work, we focus on evaluating the underlying techniques that cause\nmodels to abstain. We create SELECT, a benchmark derived from a set of benign\nconcepts (e.g., \"rivers\") from a knowledge graph. Focusing on benign concepts\nisolates the effect of safety training, and grounding these concepts in a\nknowledge graph allows us to study the generalization and specificity of\nabstention techniques. Using SELECT, we benchmark different abstention\ntechniques over six open-weight and closed-source models. We find that the\nexamined techniques indeed cause models to abstain with over $80\\%$ abstention\nrates. However, these techniques are not as effective for descendants of the\ntarget concepts, where abstention rates drop by $19\\%$. We also characterize\nthe generalization-specificity trade-offs for different techniques. Overall, no\nsingle technique is invariably better than others, and our findings inform\npractitioners of the various trade-offs involved.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"T-QiMbvn1hq7iENWOTLcUN_ZhDWO1MJKJLmgzHOOAeo","pdfSize":"4174434"}