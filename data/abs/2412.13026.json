{
  "id": "2412.13026",
  "title": "NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation",
  "authors": "Karan Wanchoo, Xiaoye Zuo, Hannah Gonzalez, Soham Dan, Georgios\n  Georgakis, Dan Roth, Kostas Daniilidis, Eleni Miltsakaki",
  "authorsParsed": [
    [
      "Wanchoo",
      "Karan",
      ""
    ],
    [
      "Zuo",
      "Xiaoye",
      ""
    ],
    [
      "Gonzalez",
      "Hannah",
      ""
    ],
    [
      "Dan",
      "Soham",
      ""
    ],
    [
      "Georgakis",
      "Georgios",
      ""
    ],
    [
      "Roth",
      "Dan",
      ""
    ],
    [
      "Daniilidis",
      "Kostas",
      ""
    ],
    [
      "Miltsakaki",
      "Eleni",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 15:48:25 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 18 Dec 2024 03:05:45 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734450505000,
  "abstract": "  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "KUTuP3XkckTVmTTODpOVyRzgJY4ieXUrTZclvE5c9-0",
  "pdfSize": "3522394"
}