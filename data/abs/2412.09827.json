{
  "id": "2412.09827",
  "title": "Low-Rank Adaptation with Task-Relevant Feature Enhancement for\n  Fine-tuning Language Models",
  "authors": "Changqun Li, Chaofan Ding, Kexin Luan, Xinhan Di",
  "authorsParsed": [
    [
      "Li",
      "Changqun",
      ""
    ],
    [
      "Ding",
      "Chaofan",
      ""
    ],
    [
      "Luan",
      "Kexin",
      ""
    ],
    [
      "Di",
      "Xinhan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 03:38:49 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734061129000,
  "abstract": "  Fine-tuning pre-trained large language models in a parameter-efficient manner\nis widely studied for its effectiveness and efficiency. LoRA is one of the most\nwidely used methods, which assumes that the optimization process is essentially\nlow dimensional. Although LoRA has demonstrated commendable performance, there\nremains a significant performance gap between LoRA and full fine-tuning when\nlearning new tasks. In this work, we propose Low-Rank Adaptation with\nTask-Relevant Feature Enhancement(LoRATRF) for enhancing task-relevant features\nfrom the perspective of editing neural network representations. To prioritize\ntask-relevant features, a task-aware filter that selectively extracts valuable\nknowledge from hidden representations for the target or current task is\ndesigned. As the experiments on a vareity of datasets including NLU,\ncommonsense reasoning and mathematical reasoning tasks demonstrates, our method\nreduces 33.71% parameters and achieves better performance on a variety of\ndatasets in comparison with SOTA low-rank methods.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "fvQ3B6BE4aOrnCv7yeT82BWnYEP_S3KvA3s54iuJGzg",
  "pdfSize": "275337"
}