{"id":"2412.09694","title":"Omni-ID: Holistic Identity Representation Designed for Generative Tasks","authors":"Guocheng Qian, Kuan-Chieh Wang, Or Patashnik, Negin Heravi, Daniil\n  Ostashev, Sergey Tulyakov, Daniel Cohen-Or, Kfir Aberman","authorsParsed":[["Qian","Guocheng",""],["Wang","Kuan-Chieh",""],["Patashnik","Or",""],["Heravi","Negin",""],["Ostashev","Daniil",""],["Tulyakov","Sergey",""],["Cohen-Or","Daniel",""],["Aberman","Kfir",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 19:21:20 GMT"}],"updateDate":"2024-12-16","timestamp":1734031280000,"abstract":"  We introduce Omni-ID, a novel facial representation designed specifically for\ngenerative tasks. Omni-ID encodes holistic information about an individual's\nappearance across diverse expressions and poses within a fixed-size\nrepresentation. It consolidates information from a varied number of\nunstructured input images into a structured representation, where each entry\nrepresents certain global or local identity features. Our approach uses a\nfew-to-many identity reconstruction training paradigm, where a limited set of\ninput images is used to reconstruct multiple target images of the same\nindividual in various poses and expressions. A multi-decoder framework is\nfurther employed to leverage the complementary strengths of diverse decoders\nduring training. Unlike conventional representations, such as CLIP and ArcFace,\nwhich are typically learned through discriminative or contrastive objectives,\nOmni-ID is optimized with a generative objective, resulting in a more\ncomprehensive and nuanced identity capture for generative tasks. Trained on our\nMFHQ dataset -- a multi-view facial image collection, Omni-ID demonstrates\nsubstantial improvements over conventional representations across various\ngenerative tasks.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"5uzqoagkKEKKmD4V6gq9fSDcjzPmEvEazekb9kA6pE8","pdfSize":"18539854"}