{"id":"2412.08536","title":"SenCLIP: Enhancing zero-shot land-use mapping for Sentinel-2 with\n  ground-level prompting","authors":"Pallavi Jain, Dino Ienco, Roberto Interdonato, Tristan Berchoux and\n  Diego Marcos","authorsParsed":[["Jain","Pallavi",""],["Ienco","Dino",""],["Interdonato","Roberto",""],["Berchoux","Tristan",""],["Marcos","Diego",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 16:52:14 GMT"}],"updateDate":"2024-12-12","timestamp":1733935934000,"abstract":"  Pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive zero-shot classification capabilities with free-form prompts and\neven show some generalization in specialized domains. However, their\nperformance on satellite imagery is limited due to the underrepresentation of\nsuch data in their training sets, which predominantly consist of ground-level\nimages. Existing prompting techniques for satellite imagery are often\nrestricted to generic phrases like a satellite image of ..., limiting their\neffectiveness for zero-shot land-use and land-cover (LULC) mapping. To address\nthese challenges, we introduce SenCLIP, which transfers CLIPs representation to\nSentinel-2 imagery by leveraging a large dataset of Sentinel-2 images paired\nwith geotagged ground-level photos from across Europe. We evaluate SenCLIP\nalongside other SOTA remote sensing VLMs on zero-shot LULC mapping tasks using\nthe EuroSAT and BigEarthNet datasets with both aerial and ground-level\nprompting styles. Our approach, which aligns ground-level representations with\nsatellite imagery, demonstrates significant improvements in classification\naccuracy across both prompt styles, opening new possibilities for applying\nfree-form textual descriptions in zero-shot LULC mapping.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"m_6u4cOnDjL5GFZ9prB0lYo60y4zD8HHnDTl2ldpExU","pdfSize":"5044582"}