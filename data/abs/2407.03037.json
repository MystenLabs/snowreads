{"id":"2407.03037","title":"Vision-driven Automated Mobile GUI Testing via Multimodal Large Language\n  Model","authors":"Zhe Liu, Cheng Li, Chunyang Chen, Junjie Wang, Boyu Wu, Yawen Wang,\n  Jun Hu, Qing Wang","authorsParsed":[["Liu","Zhe",""],["Li","Cheng",""],["Chen","Chunyang",""],["Wang","Junjie",""],["Wu","Boyu",""],["Wang","Yawen",""],["Hu","Jun",""],["Wang","Qing",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 11:58:09 GMT"}],"updateDate":"2024-07-04","timestamp":1720007889000,"abstract":"  With the advancement of software rendering techniques, GUI pages in mobile\napps now encompass a wealth of visual information, where the visual semantics\nof each page contribute to the overall app logic, presenting new challenges to\nsoftware testing. Despite the progress in automated Graphical User Interface\n(GUI) testing, the absence of testing oracles has constrained its efficacy to\nidentify only crash bugs with evident abnormal signals. Nonetheless, there are\nstill a considerable number of non-crash bugs, ranging from unexpected\nbehaviors to misalignments, often evading detection by existing techniques.\nWhile these bugs can exhibit visual cues that serve as potential testing\noracles, they often entail a sequence of screenshots, and detecting them\nnecessitates an understanding of the operational logic among GUI page\ntransitions, which is challenging traditional techniques. Considering the\nremarkable performance of Multimodal Large Language Models (MLLM) in visual and\nlanguage understanding, this paper proposes a vision-driven automated GUI\ntesting approach VisionDroid to detect non-crash functional bugs with MLLM. It\nbegins by extracting GUI text information and aligning it with screenshots to\nform a vision prompt, enabling MLLM to understand GUI context. The\nfunction-aware explorer then employs MLLM for deeper and function-oriented GUI\npage exploration, while the logic-aware bug detector segments the entire\nexploration history into logically cohesive parts and prompts the MLLM for bug\ndetection. We evaluate VisionDroid on three datasets and compare it with 10\nbaselines, demonstrating its excellent performance. The ablation study further\nproves the contribution of each module. Moreover, VisionDroid identifies 29 new\nbugs on Google Play, of which 19 have been confirmed and fixed.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"OfheguNW93mfn9o-e2WfgPSJHC_DIZjrgWVZmj-dbxE","pdfSize":"4016096"}