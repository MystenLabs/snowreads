{"id":"2407.16951","title":"Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias\n  Mitigation","authors":"Huimin Lu, Masaru Isonuma, Junichiro Mori, Ichiro Sakata","authorsParsed":[["Lu","Huimin",""],["Isonuma","Masaru",""],["Mori","Junichiro",""],["Sakata","Ichiro",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 02:37:42 GMT"}],"updateDate":"2024-07-25","timestamp":1721788662000,"abstract":"  Large language models (LLMs) often inherit biases from vast amounts of\ntraining corpora. Traditional debiasing methods, while effective to some\nextent, do not completely eliminate memorized biases and toxicity in LLMs. In\nthis paper, we study an unlearning-based approach to debiasing in LLMs by\nperforming gradient ascent on hate speech against minority groups, i.e.,\nminimizing the likelihood of biased or toxic content. Specifically, we propose\na mask language modeling unlearning technique, which unlearns the harmful part\nof the text. This method enables LLMs to selectively forget and disassociate\nfrom biased and harmful content. Experimental results demonstrate the\neffectiveness of our approach in diminishing bias while maintaining the\nlanguage modeling abilities. Surprisingly, the results also unveil an\nunexpected potential for cross-domain transfer unlearning: debiasing in one\nbias form (e.g. gender) may contribute to mitigating others (e.g. race and\nreligion).\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DpEMdqJ3ufkdF8yMSbQU0U67-uT_7tgcmSiIjj3jsSU","pdfSize":"234133"}