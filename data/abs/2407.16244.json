{"id":"2407.16244","title":"HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for\n  Multi-Label Image Classification","authors":"Shuyi Ouyang, Hongyi Wang, Ziwei Niu, Zhenjia Bai, Shiao Xie, Yingying\n  Xu, Ruofeng Tong, Yen-Wei Chen, Lanfen Lin","authorsParsed":[["Ouyang","Shuyi",""],["Wang","Hongyi",""],["Niu","Ziwei",""],["Bai","Zhenjia",""],["Xie","Shiao",""],["Xu","Yingying",""],["Tong","Ruofeng",""],["Chen","Yen-Wei",""],["Lin","Lanfen",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 07:31:42 GMT"}],"updateDate":"2024-07-24","timestamp":1721719902000,"abstract":"  The task of multi-label image classification involves recognizing multiple\nobjects within a single image. Considering both valuable semantic information\ncontained in the labels and essential visual features presented in the image,\ntight visual-linguistic interactions play a vital role in improving\nclassification performance. Moreover, given the potential variance in object\nsize and appearance within a single image, attention to features of different\nscales can help to discover possible objects in the image. Recently,\nTransformer-based methods have achieved great success in multi-label image\nclassification by leveraging the advantage of modeling long-range dependencies,\nbut they have several limitations. Firstly, existing methods treat visual\nfeature extraction and cross-modal fusion as separate steps, resulting in\ninsufficient visual-linguistic alignment in the joint semantic space.\nAdditionally, they only extract visual features and perform cross-modal fusion\nat a single scale, neglecting objects with different characteristics. To\naddress these issues, we propose a Hierarchical Scale-Aware Vision-Language\nTransformer (HSVLT) with two appealing designs: (1)~A hierarchical multi-scale\narchitecture that involves a Cross-Scale Aggregation module, which leverages\njoint multi-modal features extracted from multiple scales to recognize objects\nof varying sizes and appearances in images. (2)~Interactive Visual-Linguistic\nAttention, a novel attention mechanism module that tightly integrates\ncross-modal interaction, enabling the joint updating of visual, linguistic and\nmulti-modal features. We have evaluated our method on three benchmark datasets.\nThe experimental results demonstrate that HSVLT surpasses state-of-the-art\nmethods with lower computational cost.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Multimedia"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"OOSsq3XkTPLfaBTuVBCDDXwJTWzAEo7Nk0UT0TbxzRc","pdfSize":"1632567"}