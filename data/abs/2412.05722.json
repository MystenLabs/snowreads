{
  "id": "2412.05722",
  "title": "Evaluating Hallucination in Text-to-Image Diffusion Models with\n  Scene-Graph based Question-Answering Agent",
  "authors": "Ziyuan Qin, Dongjie Cheng, Haoyu Wang, Huahui Yi, Yuting Shao, Zhiyuan\n  Fan, Kang Li, Qicheng Lao",
  "authorsParsed": [
    [
      "Qin",
      "Ziyuan",
      ""
    ],
    [
      "Cheng",
      "Dongjie",
      ""
    ],
    [
      "Wang",
      "Haoyu",
      ""
    ],
    [
      "Yi",
      "Huahui",
      ""
    ],
    [
      "Shao",
      "Yuting",
      ""
    ],
    [
      "Fan",
      "Zhiyuan",
      ""
    ],
    [
      "Li",
      "Kang",
      ""
    ],
    [
      "Lao",
      "Qicheng",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 7 Dec 2024 18:44:38 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733597078000,
  "abstract": "  Contemporary Text-to-Image (T2I) models frequently depend on qualitative\nhuman evaluations to assess the consistency between synthesized images and the\ntext prompts. There is a demand for quantitative and automatic evaluation\ntools, given that human evaluation lacks reproducibility. We believe that an\neffective T2I evaluation metric should accomplish the following: detect\ninstances where the generated images do not align with the textual prompts, a\ndiscrepancy we define as the `hallucination problem' in T2I tasks; record the\ntypes and frequency of hallucination issues, aiding users in understanding the\ncauses of errors; and provide a comprehensive and intuitive scoring that close\nto human standard. To achieve these objectives, we propose a method based on\nlarge language models (LLMs) for conducting question-answering with an\nextracted scene-graph and created a dataset with human-rated scores for\ngenerated images. From the methodology perspective, we combine\nknowledge-enhanced question-answering tasks with image evaluation tasks, making\nthe evaluation metrics more controllable and easier to interpret. For the\ncontribution on the dataset side, we generated 12,000 synthesized images based\non 1,000 composited prompts using three advanced T2I models. Subsequently, we\nconduct human scoring on all synthesized images and prompt pairs to validate\nthe accuracy and effectiveness of our method as an evaluation metric. All\ngenerated images and the human-labeled scores will be made publicly available\nin the future to facilitate ongoing research on this crucial issue. Extensive\nexperiments show that our method aligns more closely with human scoring\npatterns than other evaluation metrics.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Rg12v3zqsEbKzinxJ735pw7mAzWCqAsXb0S3QDu3HSk",
  "pdfSize": "860119"
}