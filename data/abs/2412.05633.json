{
  "id": "2412.05633",
  "title": "Efficient Continuous Video Flow Model for Video Prediction",
  "authors": "Gaurav Shrivastava, Abhinav Shrivastava",
  "authorsParsed": [
    [
      "Shrivastava",
      "Gaurav",
      ""
    ],
    [
      "Shrivastava",
      "Abhinav",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 7 Dec 2024 12:11:25 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733573485000,
  "abstract": "  Multi-step prediction models, such as diffusion and rectified flow models,\nhave emerged as state-of-the-art solutions for generation tasks. However, these\nmodels exhibit higher latency in sampling new frames compared to single-step\nmethods. This latency issue becomes a significant bottleneck when adapting such\nmethods for video prediction tasks, given that a typical 60-second video\ncomprises approximately 1.5K frames. In this paper, we propose a novel approach\nto modeling the multi-step process, aimed at alleviating latency constraints\nand facilitating the adaptation of such processes for video prediction tasks.\nOur approach not only reduces the number of sample steps required to predict\nthe next frame but also minimizes computational demands by reducing the model\nsize to one-third of the original size. We evaluate our method on standard\nvideo prediction datasets, including KTH, BAIR action robot, Human3.6M and\nUCF101, demonstrating its efficacy in achieving state-of-the-art performance on\nthese benchmarks.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "dvbmQtveE8OEEk-WOYgw-LRFsKdAMv-Kc6CktIIcWyA",
  "pdfSize": "26436459"
}