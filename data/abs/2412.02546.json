{
  "id": "2412.02546",
  "title": "Fractional Order Distributed Optimization",
  "authors": "Andrei Lixandru, Marcel van Gerven, Sergio Pequito",
  "authorsParsed": [
    [
      "Lixandru",
      "Andrei",
      ""
    ],
    [
      "van Gerven",
      "Marcel",
      ""
    ],
    [
      "Pequito",
      "Sergio",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 16:39:01 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733243941000,
  "abstract": "  Distributed optimization is fundamental to modern machine learning\napplications like federated learning, but existing methods often struggle with\nill-conditioned problems and face stability-versus-speed tradeoffs. We\nintroduce fractional order distributed optimization (FrODO); a\ntheoretically-grounded framework that incorporates fractional-order memory\nterms to enhance convergence properties in challenging optimization landscapes.\nOur approach achieves provable linear convergence for any strongly connected\nnetwork. Through empirical validation, our results suggest that FrODO achieves\nup to 4 times faster convergence versus baselines on ill-conditioned problems\nand 2-3 times speedup in federated neural network training, while maintaining\nstability and theoretical guarantees.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "9zAIA4gzsFjF3Hxew55J39DSMTV8cpSnYYre5MhM0ok",
  "pdfSize": "890837"
}