{
  "id": "2412.17019",
  "title": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT",
  "authors": "Shahar Katz, Lior Wolf",
  "authorsParsed": [
    [
      "Katz",
      "Shahar",
      ""
    ],
    [
      "Wolf",
      "Lior",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 22 Dec 2024 13:48:04 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734875284000,
  "abstract": "  The success of Transformer-based Language Models (LMs) stems from their\nattention mechanism. While this mechanism has been extensively studied in\nexplainability research, particularly through the attention values obtained\nduring the forward pass of LMs, the backward pass of attention has been largely\noverlooked. In this work, we study the mathematics of the backward pass of\nattention, revealing that it implicitly calculates an attention matrix we refer\nto as \"Reversed Attention\". We examine the properties of Reversed Attention and\ndemonstrate its ability to elucidate the models' behavior and edit dynamics. In\nan experimental setup, we showcase the ability of Reversed Attention to\ndirectly alter the forward pass of attention, without modifying the model's\nweights, using a novel method called \"attention patching\". In addition to\nenhancing the comprehension of how LM configure attention layers during\nbackpropagation, Reversed Attention maps contribute to a more interpretable\nbackward pass.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "YjSwwfhTjKUfIVoM8MNltWo8nNmqs0NVeTCSXjgCiiQ",
  "pdfSize": "1155531"
}