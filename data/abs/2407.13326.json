{"id":"2407.13326","title":"RISC-V RVV efficiency for ANN algorithms","authors":"Konstantin Rumyantsev, Pavel Yakovlev, Andrey Gorshkov, Andrey P.\n  Sokolov","authorsParsed":[["Rumyantsev","Konstantin",""],["Yakovlev","Pavel",""],["Gorshkov","Andrey",""],["Sokolov","Andrey P.",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 09:26:07 GMT"}],"updateDate":"2024-07-19","timestamp":1721294767000,"abstract":"  Handling vast amounts of data is crucial in today's world. The growth of\nhigh-performance computing has created a need for parallelization, particularly\nin the area of machine learning algorithms such as ANN (Approximate Nearest\nNeighbors). To improve the speed of these algorithms, it is important to\noptimize them for specific processor architectures. RISC-V (Reduced Instruction\nSet Computer Five) is one of the modern processor architectures, which features\na vector instruction set called RVV (RISC-V Vector Extension). In machine\nlearning algorithms, vector extensions are widely utilized to improve the\nprocessing of voluminous data. This study examines the effectiveness of\napplying RVV to commonly used ANN algorithms. The algorithms were adapted for\nRISC-V and optimized using RVV after identifying the primary bottlenecks.\nAdditionally, we developed a theoretical model of a parameterized vector block\nand identified the best on average configuration that demonstrates the highest\ntheoretical performance of the studied ANN algorithms when the other CPU\nparameters are fixed.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"dJlSA-VlwbDzAcingr9m9OBkdFXydoVNy8H2K8fWIVk","pdfSize":"596978","objectId":"0x1aacb8c63e04f9c8d739ef5635fc59f45c738566d902527bf85f5ea881a9c46e","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
