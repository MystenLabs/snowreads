{"id":"2412.01519","title":"ReHub: Linear Complexity Graph Transformers with Adaptive Hub-Spoke\n  Reassignment","authors":"Tomer Borreda, Daniel Freedman, Or Litany","authorsParsed":[["Borreda","Tomer",""],["Freedman","Daniel",""],["Litany","Or",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 14:10:23 GMT"}],"updateDate":"2024-12-03","timestamp":1733148623000,"abstract":"  We present ReHub, a novel graph transformer architecture that achieves linear\ncomplexity through an efficient reassignment technique between nodes and\nvirtual nodes. Graph transformers have become increasingly important in graph\nlearning for their ability to utilize long-range node communication explicitly,\naddressing limitations such as oversmoothing and oversquashing found in\nmessage-passing graph networks. However, their dense attention mechanism scales\nquadratically with the number of nodes, limiting their applicability to\nlarge-scale graphs. ReHub draws inspiration from the airline industry's\nhub-and-spoke model, where flights are assigned to optimize operational\nefficiency. In our approach, graph nodes (spokes) are dynamically reassigned to\na fixed number of virtual nodes (hubs) at each model layer. Recent work, Neural\nAtoms (Li et al., 2024), has demonstrated impressive and consistent\nimprovements over GNN baselines by utilizing such virtual nodes; their findings\nsuggest that the number of hubs strongly influences performance. However,\nincreasing the number of hubs typically raises complexity, requiring a\ntrade-off to maintain linear complexity. Our key insight is that each node only\nneeds to interact with a small subset of hubs to achieve linear complexity,\neven when the total number of hubs is large. To leverage all hubs without\nincurring additional computational costs, we propose a simple yet effective\nadaptive reassignment technique based on hub-hub similarity scores, eliminating\nthe need for expensive node-hub computations. Our experiments on LRGB indicate\na consistent improvement in results over the base method, Neural Atoms, while\nmaintaining a linear complexity. Remarkably, our sparse model achieves\nperformance on par with its non-sparse counterpart. Furthermore, ReHub\noutperforms competitive baselines and consistently ranks among top performers\nacross various benchmarks.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Of01ycuWssAaD7tt_8cEYIpTEvwzSAa_lfdvISij9CU","pdfSize":"595726"}