{"id":"2407.08680","title":"Generalizable Implicit Motion Modeling for Video Frame Interpolation","authors":"Zujin Guo, Wei Li, Chen Change Loy","authorsParsed":[["Guo","Zujin",""],["Li","Wei",""],["Loy","Chen Change",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:13:15 GMT"},{"version":"v2","created":"Sat, 20 Jul 2024 05:34:55 GMT"},{"version":"v3","created":"Mon, 29 Jul 2024 15:38:47 GMT"}],"updateDate":"2024-07-30","timestamp":1720717995000,"abstract":"  Motion modeling is critical in flow-based Video Frame Interpolation (VFI).\nExisting paradigms either consider linear combinations of bidirectional flows\nor directly predict bilateral flows for given timestamps without exploring\nfavorable motion priors, thus lacking the capability of effectively modeling\nspatiotemporal dynamics in real-world videos. To address this limitation, in\nthis study, we introduce Generalizable Implicit Motion Modeling (GIMM), a novel\nand effective approach to motion modeling for VFI. Specifically, to enable GIMM\nas an effective motion modeling paradigm, we design a motion encoding pipeline\nto model spatiotemporal motion latent from bidirectional flows extracted from\npre-trained flow estimators, effectively representing input-specific motion\npriors. Then, we implicitly predict arbitrary-timestep optical flows within two\nadjacent input frames via an adaptive coordinate-based neural network, with\nspatiotemporal coordinates and motion latent as inputs. Our GIMM can be\nsmoothly integrated with existing flow-based VFI works without further\nmodifications. We show that GIMM performs better than the current state of the\nart on the VFI benchmarks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"-SKzvA4f6bAHZ7gzqaAYQIrMtmxZDyIE7gYTrzfbpeM","pdfSize":"4315828"}