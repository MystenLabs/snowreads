{"id":"2407.12679","title":"Goldfish: Vision-Language Understanding of Arbitrarily Long Videos","authors":"Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman,\n  Mingchen Zhuge, Jian Ding, Deyao Zhu, J\\\"urgen Schmidhuber, Mohamed Elhoseiny","authorsParsed":[["Ataallah","Kirolos",""],["Shen","Xiaoqian",""],["Abdelrahman","Eslam",""],["Sleiman","Essam",""],["Zhuge","Mingchen",""],["Ding","Jian",""],["Zhu","Deyao",""],["Schmidhuber","JÃ¼rgen",""],["Elhoseiny","Mohamed",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 15:59:32 GMT"}],"updateDate":"2024-07-18","timestamp":1721231972000,"abstract":"  Most current LLM-based models for video understanding can process videos\nwithin minutes. However, they struggle with lengthy videos due to challenges\nsuch as \"noise and redundancy\", as well as \"memory and computation\"\nconstraints. In this paper, we present Goldfish, a methodology tailored for\ncomprehending videos of arbitrary lengths. We also introduce the TVQA-long\nbenchmark, specifically designed to evaluate models' capabilities in\nunderstanding long videos with questions in both vision and text content.\nGoldfish approaches these challenges with an efficient retrieval mechanism that\ninitially gathers the top-k video clips relevant to the instruction before\nproceeding to provide the desired response. This design of the retrieval\nmechanism enables the Goldfish to efficiently process arbitrarily long video\nsequences, facilitating its application in contexts such as movies or\ntelevision series. To facilitate the retrieval process, we developed\nMiniGPT4-Video that generates detailed descriptions for the video clips. In\naddressing the scarcity of benchmarks for long video evaluation, we adapted the\nTVQA short video benchmark for extended content analysis by aggregating\nquestions from entire episodes, thereby shifting the evaluation from partial to\nfull episode comprehension. We attained a 41.78% accuracy rate on the TVQA-long\nbenchmark, surpassing previous methods by 14.94%. Our MiniGPT4-Video also shows\nexceptional performance in short video comprehension, exceeding existing\nstate-of-the-art methods by 3.23%, 2.03%, 16.5% and 23.59% on the MSVD, MSRVTT,\nTGIF, and TVQA short video benchmarks, respectively. These results indicate\nthat our models have significant improvements in both long and short-video\nunderstanding. Our models and code have been made publicly available at\nhttps://vision-cair.github.io/Goldfish_website/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UEj2YecG6Z8DCFC-A8BG-HZYABMjmAss300-YMivs6A","pdfSize":"2838865"}