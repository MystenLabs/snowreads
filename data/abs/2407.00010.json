{"id":"2407.00010","title":"Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM\n  Inference Workloads","authors":"Grant Wilkins, Srinivasan Keshav, and Richard Mortier","authorsParsed":[["Wilkins","Grant",""],["Keshav","Srinivasan",""],["Mortier","Richard",""]],"versions":[{"version":"v1","created":"Thu, 25 Apr 2024 11:24:08 GMT"}],"updateDate":"2024-07-02","timestamp":1714044248000,"abstract":"  Both the training and use of Large Language Models (LLMs) require large\namounts of energy. Their increasing popularity, therefore, raises critical\nconcerns regarding the energy efficiency and sustainability of data centers\nthat host them. This paper addresses the challenge of reducing energy\nconsumption in data centers running LLMs. We propose a hybrid data center model\nthat uses a cost-based scheduling framework to dynamically allocate LLM tasks\nacross hardware accelerators that differ in their energy efficiencies and\ncomputational capabilities. Specifically, our workload-aware strategy\ndetermines whether tasks are processed on energy-efficient processors or\nhigh-performance GPUs based on the number of input and output tokens in a\nquery. Our analysis of a representative LLM dataset, finds that this hybrid\nstrategy can reduce CPU+GPU energy consumption by 7.5% compared to a\nworkload-unaware baseline.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OcLL6DBc-UdA0AGW0Jg0ZAO3slTSrag3fgQHeh4cEFc","pdfSize":"845606","objectId":"0xf042a61618ce6630c62e9c11409f4996520ebfe4c344d40ca01b02a07d102007","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"101"}
