{"id":"2407.05218","title":"Effect of Rotation Angle in Self-Supervised Pre-training is\n  Dataset-Dependent","authors":"Amy Saranchuk and Michael Guerzhoy","authorsParsed":[["Saranchuk","Amy",""],["Guerzhoy","Michael",""]],"versions":[{"version":"v1","created":"Fri, 21 Jun 2024 12:25:07 GMT"}],"updateDate":"2024-07-09","timestamp":1718972707000,"abstract":"  Self-supervised learning for pre-training (SSP) can help the network learn\nbetter low-level features, especially when the size of the training set is\nsmall. In contrastive pre-training, the network is pre-trained to distinguish\nbetween different versions of the input. For example, the network learns to\ndistinguish pairs (original, rotated) of images where the rotated image was\nrotated by angle $\\theta$ vs. other pairs of images. In this work, we show\nthat, when training using contrastive pre-training in this way, the angle\n$\\theta$ and the dataset interact in interesting ways. We hypothesize, and give\nsome evidence, that, for some datasets, the network can take \"shortcuts\" for\nparticular rotation angles $\\theta$ based on the distribution of the gradient\ndirections in the input, possibly avoiding learning features other than edges,\nbut our experiments do not seem to support that hypothesis. We demonstrate\nexperiments on three radiology datasets. We compute the saliency map indicating\nwhich pixels were important in the SSP process, and compare the saliency map to\nthe ground truth foreground/background segmentation. Our visualizations\nindicate that the effects of rotation angles in SSP are dataset-dependent. We\nbelieve the distribution of gradient orientations may play a role in this, but\nour experiments so far are inconclusive.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"k4cJA588p77WwHuYs_jny9wXgHcPKvbv0NFZtNVzIRc","pdfSize":"1433861"}