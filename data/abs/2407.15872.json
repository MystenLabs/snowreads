{"id":"2407.15872","title":"A reinforcement learning strategy to automate and accelerate\n  h/p-multigrid solvers","authors":"David Huergo, Laura Alonso, Saumitra Joshi, Adrian Juanicoteca,\n  Gonzalo Rubio and Esteban Ferrer","authorsParsed":[["Huergo","David",""],["Alonso","Laura",""],["Joshi","Saumitra",""],["Juanicoteca","Adrian",""],["Rubio","Gonzalo",""],["Ferrer","Esteban",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 21:26:28 GMT"}],"updateDate":"2024-07-24","timestamp":1721337988000,"abstract":"  We explore a reinforcement learning strategy to automate and accelerate\nh/p-multigrid methods in high-order solvers. Multigrid methods are very\nefficient but require fine-tuning of numerical parameters, such as the number\nof smoothing sweeps per level and the correction fraction (i.e., proportion of\nthe corrected solution that is transferred from a coarser grid to a finer\ngrid). The objective of this paper is to use a proximal policy optimization\nalgorithm to automatically tune the multigrid parameters and, by doing so,\nimprove stability and efficiency of the h/p-multigrid strategy.\n  Our findings reveal that the proposed reinforcement learning h/p-multigrid\napproach significantly accelerates and improves the robustness of steady-state\nsimulations for one dimensional advection-diffusion and nonlinear Burgers'\nequations, when discretized using high-order h/p methods, on uniform and\nnonuniform grids.\n","subjects":["Computing Research Repository/Machine Learning","Physics/Computational Physics"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"34XocEV7QJvOPjKMsWFHW0vb9ErgfExaIo9lf2y7AjA","pdfSize":"272321","objectId":"0xbf72232726a9777bc5471fc8c07851baeb7964a26f8ac5a6b408fed16bb88ea1","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
