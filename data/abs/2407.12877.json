{"id":"2407.12877","title":"Review-Feedback-Reason (ReFeR): A Novel Framework for NLG Evaluation and\n  Reasoning","authors":"Yaswanth Narsupalli, Abhranil Chandra, Sreevatsa Muppirala, Manish\n  Gupta, Pawan Goyal","authorsParsed":[["Narsupalli","Yaswanth",""],["Chandra","Abhranil",""],["Muppirala","Sreevatsa",""],["Gupta","Manish",""],["Goyal","Pawan",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 08:25:26 GMT"}],"updateDate":"2024-07-19","timestamp":1721118326000,"abstract":"  Assessing the quality of Natural Language Generation (NLG) outputs, such as\nthose produced by large language models (LLMs), poses significant challenges.\nTraditional approaches involve either resource-intensive human evaluations or\nautomatic metrics, which often exhibit a low correlation with human judgment.\nIn this study, we propose Review-Feedback-Reason (ReFeR), a novel evaluation\nframework for NLG using LLM agents. We rigorously test ReFeR using two\npre-existing benchmark datasets on diverse NLG tasks. The proposed framework\nnot only enhances the accuracy of NLG evaluation, surpassing previous\nbenchmarks by $\\sim$20\\%, but also generates constructive feedback and\nsignificantly improves collective reasoning. This feedback is then leveraged\nfor the creation of instruction-tuning datasets, which, when used to fine-tune\nsmaller models like Mistral-7B, makes them extremely good evaluators, yielding\na better correlation with human evaluations and performance nearly on par with\nGPT-3.5. We highlight the effectiveness of our methodology through its\napplication on three reasoning benchmarks, where it outperforms most of the\nstate-of-the-art methods, and also outperforms the reasoning capabilities of\nmodels like GPT-3.5 Turbo by $\\sim$11.67\\% and GPT-4 by $\\sim$1\\% on an\naverage.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Lf0YHDSFCpcuqpT1CoRS466_9SSR8wirUeNeKXKBE7Q","pdfSize":"2345360"}