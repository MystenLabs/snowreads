{"id":"2412.04353","title":"ActFusion: a Unified Diffusion Model for Action Segmentation and\n  Anticipation","authors":"Dayoung Gong, Suha Kwak, Minsu Cho","authorsParsed":[["Gong","Dayoung",""],["Kwak","Suha",""],["Cho","Minsu",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 17:12:35 GMT"}],"updateDate":"2024-12-06","timestamp":1733418755000,"abstract":"  Temporal action segmentation and long-term action anticipation are two\npopular vision tasks for the temporal analysis of actions in videos. Despite\napparent relevance and potential complementarity, these two problems have been\ninvestigated as separate and distinct tasks. In this work, we tackle these two\nproblems, action segmentation and action anticipation, jointly using a unified\ndiffusion model dubbed ActFusion. The key idea to unification is to train the\nmodel to effectively handle both visible and invisible parts of the sequence in\nan integrated manner; the visible part is for temporal segmentation, and the\ninvisible part is for future anticipation. To this end, we introduce a new\nanticipative masking strategy during training in which a late part of the video\nframes is masked as invisible, and learnable tokens replace these frames to\nlearn to predict the invisible future. Experimental results demonstrate the\nbi-directional benefits between action segmentation and anticipation. ActFusion\nachieves the state-of-the-art performance across the standard benchmarks of 50\nSalads, Breakfast, and GTEA, outperforming task-specific models in both of the\ntwo tasks with a single unified model through joint learning.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"V-sW-T1MDiWaEmZiwgttP2aS01JmchA_OTJ9xpy5wqA","pdfSize":"2763001"}