{"id":"2412.16670","title":"Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent\n  Diffusion Transformer","authors":"Boyuan Li, Xihua Wang, Ruihua Song and Wenbing Huang","authorsParsed":[["Li","Boyuan",""],["Wang","Xihua",""],["Song","Ruihua",""],["Huang","Wenbing",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 15:35:50 GMT"}],"updateDate":"2024-12-24","timestamp":1734795350000,"abstract":"  Multi-person interactive motion generation, a critical yet under-explored\ndomain in computer character animation, poses significant challenges such as\nintricate modeling of inter-human interactions beyond individual motions and\ngenerating two motions with huge differences from one text condition. Current\nresearch often employs separate module branches for individual motions, leading\nto a loss of interaction information and increased computational demands. To\naddress these challenges, we propose a novel, unified approach that models\nmulti-person motions and their interactions within a single latent space. Our\napproach streamlines the process by treating interactive motions as an\nintegrated data point, utilizing a Variational AutoEncoder (VAE) for\ncompression into a unified latent space, and performing a diffusion process\nwithin this space, guided by the natural language conditions. Experimental\nresults demonstrate our method's superiority over existing approaches in\ngeneration quality, performing text condition in particular when motions have\nsignificant asymmetry, and accelerating the generation efficiency while\npreserving high quality.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Graphics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2oQ9GL8Lwke_PkkyJ6ziV_iISWi2F7Fjb3dwEyrWb3M","pdfSize":"1745183"}