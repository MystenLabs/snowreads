{"id":"2412.20162","title":"Multi-Modality Driven LoRA for Adverse Condition Depth Estimation","authors":"Guanglei Yang, Rui Tian, Yongqiang Zhang, Zhun Zhong, Yongqiang Li,\n  Wangmeng Zuo","authorsParsed":[["Yang","Guanglei",""],["Tian","Rui",""],["Zhang","Yongqiang",""],["Zhong","Zhun",""],["Li","Yongqiang",""],["Zuo","Wangmeng",""]],"versions":[{"version":"v1","created":"Sat, 28 Dec 2024 14:23:58 GMT"}],"updateDate":"2024-12-31","timestamp":1735395838000,"abstract":"  The autonomous driving community is increasingly focused on addressing corner\ncase problems, particularly those related to ensuring driving safety under\nadverse conditions (e.g., nighttime, fog, rain). To this end, the task of\nAdverse Condition Depth Estimation (ACDE) has gained significant attention.\nPrevious approaches in ACDE have primarily relied on generative models, which\nnecessitate additional target images to convert the sunny condition into\nadverse weather, or learnable parameters for feature augmentation to adapt\ndomain gaps, resulting in increased model complexity and tuning efforts.\nFurthermore, unlike CLIP-based methods where textual and visual features have\nbeen pre-aligned, depth estimation models lack sufficient alignment between\nmultimodal features, hindering coherent understanding under adverse conditions.\nTo address these limitations, we propose Multi-Modality Driven LoRA (MMD-LoRA),\nwhich leverages low-rank adaptation matrices for efficient fine-tuning from\nsource-domain to target-domain. It consists of two core components: Prompt\nDriven Domain Alignment (PDDA) and Visual-Text Consistent Contrastive\nLearning(VTCCL). During PDDA, the image encoder with MMD-LoRA generates\ntarget-domain visual representations, supervised by alignment loss that the\nsource-target difference between language and image should be equal. Meanwhile,\nVTCCL bridges the gap between textual features from CLIP and visual features\nfrom diffusion model, pushing apart different weather representations (vision\nand text) and bringing together similar ones. Through extensive experiments,\nthe proposed method achieves state-of-the-art performance on the nuScenes and\nOxford RobotCar datasets, underscoring robustness and efficiency in adapting to\nvaried adverse environments.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wJQOOCkrLw5SDCh3memxUCmTOLnZz-oppCflFtN5HL0","pdfSize":"2739791"}