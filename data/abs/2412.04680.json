{"id":"2412.04680","title":"Superpixel Tokenization for Vision Transformers: Preserving Semantic\n  Integrity in Visual Tokens","authors":"Jaihyun Lew, Soohyuk Jang, Jaehoon Lee, Seungryong Yoo, Eunji Kim,\n  Saehyung Lee, Jisoo Mok, Siwon Kim, Sungroh Yoon","authorsParsed":[["Lew","Jaihyun",""],["Jang","Soohyuk",""],["Lee","Jaehoon",""],["Yoo","Seungryong",""],["Kim","Eunji",""],["Lee","Saehyung",""],["Mok","Jisoo",""],["Kim","Siwon",""],["Yoon","Sungroh",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 00:38:36 GMT"}],"updateDate":"2024-12-09","timestamp":1733445516000,"abstract":"  Transformers, a groundbreaking architecture proposed for Natural Language\nProcessing (NLP), have also achieved remarkable success in Computer Vision. A\ncornerstone of their success lies in the attention mechanism, which models\nrelationships among tokens. While the tokenization process in NLP inherently\nensures that a single token does not contain multiple semantics, the\ntokenization of Vision Transformer (ViT) utilizes tokens from uniformly\npartitioned square image patches, which may result in an arbitrary mixing of\nvisual concepts in a token. In this work, we propose to substitute the\ngrid-based tokenization in ViT with superpixel tokenization, which employs\nsuperpixels to generate a token that encapsulates a sole visual concept.\nUnfortunately, the diverse shapes, sizes, and locations of superpixels make\nintegrating superpixels into ViT tokenization rather challenging. Our\ntokenization pipeline, comprised of pre-aggregate extraction and\nsuperpixel-aware aggregation, overcomes the challenges that arise in superpixel\ntokenization. Extensive experiments demonstrate that our approach, which\nexhibits strong compatibility with existing frameworks, enhances the accuracy\nand robustness of ViT on various downstream tasks.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bGVTfaGJddP3DRX1Og7MK2Ez1XPhUsf3_Ilyg36oaf4","pdfSize":"4675450"}