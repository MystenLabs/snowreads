{"id":"2412.15285","title":"Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase\n  Pretraining","authors":"Steven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary,\n  Mohammad Shoeybi, Bryan Catanzaro","authorsParsed":[["Feng","Steven",""],["Prabhumoye","Shrimai",""],["Kong","Kezhi",""],["Su","Dan",""],["Patwary","Mostofa",""],["Shoeybi","Mohammad",""],["Catanzaro","Bryan",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 18:41:18 GMT"}],"updateDate":"2024-12-23","timestamp":1734547278000,"abstract":"  Pretraining large language models effectively requires strategic data\nselection, blending and ordering. However, key details about data mixtures\nespecially their scalability to longer token horizons and larger model sizes\nremain underexplored due to limited disclosure by model developers. To address\nthis, we formalize the concept of two-phase pretraining and conduct an\nextensive systematic study on how to select and mix data to maximize model\naccuracies for the two phases. Our findings illustrate that a two-phase\napproach for pretraining outperforms random data ordering and natural\ndistribution of tokens by 3.4% and 17% on average accuracies. We provide\nin-depth guidance on crafting optimal blends based on quality of the data\nsource and the number of epochs to be seen. We propose to design blends using\ndownsampled data at a smaller scale of 1T tokens and then demonstrate effective\nscaling of our approach to larger token horizon of 15T tokens and larger model\nsize of 25B model size. These insights provide a series of steps practitioners\ncan follow to design and scale their data blends.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"p-zByAvAsHboO2IpFoLgOwB4v5nvSnAV5_fAvqUN34c","pdfSize":"550765"}