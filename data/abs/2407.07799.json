{"id":"2407.07799","title":"Attribute or Abstain: Large Language Models as Long Document Assistants","authors":"Jan Buchmann, Xiao Liu, Iryna Gurevych","authorsParsed":[["Buchmann","Jan",""],["Liu","Xiao",""],["Gurevych","Iryna",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 16:16:02 GMT"}],"updateDate":"2024-07-11","timestamp":1720628162000,"abstract":"  LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiment with different approaches to attribution on 4 LLMs\nof different sizes, both prompted and fine-tuned. We find that citation, i.e.\nresponse generation and evidence extraction in one step, mostly performs best.\nWe investigate whether the ``Lost in the Middle'' phenomenon exists for\nattribution, but do not find this. We also find that evidence quality can\npredict response quality on datasets with simple responses, but not so for\ncomplex responses, as models struggle with providing evidence for complex\nclaims. We release code and data for further investigation.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rElpTyh6fqu7WTBFAuk8LjEjncnFxU_t_iDyvxoTuD0","pdfSize":"6041625","objectId":"0xf28b418605075f48a4efbf82220337534168fedf86b017d9e21100257f0d8834","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
