{"id":"2407.02977","title":"Large Language Models as Evaluators for Scientific Synthesis","authors":"Julia Evans, Jennifer D'Souza and S\\\"oren Auer","authorsParsed":[["Evans","Julia",""],["D'Souza","Jennifer",""],["Auer","SÃ¶ren",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 10:21:27 GMT"}],"updateDate":"2024-07-04","timestamp":1720002087000,"abstract":"  Our study explores how well the state-of-the-art Large Language Models\n(LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries\nor, more fittingly, scientific syntheses, comparing their evaluations to those\nof human annotators. We used a dataset of 100 research questions and their\nsyntheses made by GPT-4 from abstracts of five related papers, checked against\nhuman quality ratings. The study evaluates both the closed-source GPT-4 and the\nopen-source Mistral model's ability to rate these summaries and provide reasons\nfor their judgments. Preliminary results show that LLMs can offer logical\nexplanations that somewhat match the quality ratings, yet a deeper statistical\nanalysis shows a weak correlation between LLM and human ratings, suggesting the\npotential and current limitations of LLMs in scientific synthesis evaluation.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"EWJh-eff_x9aPvomO9ww6GCniMvlMNXj3jq5DhnN2iI","pdfSize":"251548"}