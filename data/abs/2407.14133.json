{"id":"2407.14133","title":"I Know About \"Up\"! Enhancing Spatial Reasoning in Visual Language Models\n  Through 3D Reconstruction","authors":"Zaiqiao Meng and Hao Zhou and Yifang Chen","authorsParsed":[["Meng","Zaiqiao",""],["Zhou","Hao",""],["Chen","Yifang",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 09:03:30 GMT"},{"version":"v2","created":"Thu, 12 Sep 2024 11:17:46 GMT"}],"updateDate":"2024-09-13","timestamp":1721379810000,"abstract":"  Visual Language Models (VLMs) are essential for various tasks, particularly\nvisual reasoning tasks, due to their robust multi-modal information\nintegration, visual reasoning capabilities, and contextual awareness. However,\nexisting \\VLMs{}' visual spatial reasoning capabilities are often inadequate,\nstruggling even with basic tasks such as distinguishing left from right. To\naddress this, we propose the \\ours{} model, designed to enhance the visual\nspatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D\nreconstruction model for obtaining different views of the input images and\nincorporates a prompting mechanism to further improve visual spatial reasoning.\nExperimental results on four visual spatial reasoning datasets show that our\n\\ours{} achieves up to 19.48% accuracy improvement, which indicates the\neffectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Y5_0mpdNJ6tYF8NAIwqyZOUrnHAFnA5mIRyb-C6rtzM","pdfSize":"2408186"}