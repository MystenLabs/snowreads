{
  "id": "2412.20115",
  "title": "Gradient Descent Methods for Regularized Optimization",
  "authors": "Filip Nikolovski, Irena Stojkovska, Katerina Hadzi-Velkova Saneva and\n  Zoran Hadzi-Velkov",
  "authorsParsed": [
    [
      "Nikolovski",
      "Filip",
      ""
    ],
    [
      "Stojkovska",
      "Irena",
      ""
    ],
    [
      "Saneva",
      "Katerina Hadzi-Velkova",
      ""
    ],
    [
      "Hadzi-Velkov",
      "Zoran",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 28 Dec 2024 10:54:15 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735383255000,
  "abstract": "  Regularization is a widely recognized technique in mathematical optimization.\nIt can be used to smooth out objective functions, refine the feasible solution\nset, or prevent overfitting in machine learning models. Due to its simplicity\nand robustness, the gradient descent (GD) method is one of the primary methods\nused for numerical optimization of differentiable objective functions. However,\nGD is not well-suited for solving $\\ell^1$ regularized optimization problems\nsince these problems are non-differentiable at zero, causing iteration updates\nto oscillate or fail to converge. Instead, a more effective version of GD,\ncalled the proximal gradient descent employs a technique known as\nsoft-thresholding to shrink the iteration updates toward zero, thus enabling\nsparsity in the solution. Motivated by the widespread applications of proximal\nGD in sparse and low-rank recovery across various engineering disciplines, we\nprovide an overview of the GD and proximal GD methods for solving regularized\noptimization problems. Furthermore, this paper proposes a novel algorithm for\nthe proximal GD method that incorporates a variable step size. Unlike\nconventional proximal GD, which uses a fixed step size based on the global\nLipschitz constant, our method estimates the Lipschitz constant locally at each\niteration and uses its reciprocal as the step size. This eliminates the need\nfor a global Lipschitz constant, which can be impractical to compute. Numerical\nexperiments we performed on synthetic and real-data sets show notable\nperformance improvement of the proposed method compared to the conventional\nproximal GD with constant step size, both in terms of number of iterations and\nin time requirements.\n",
  "subjects": [
    "Mathematics/Optimization and Control",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "RSrTrAROG6VzgoO8Wnm-l_xuup3dw4msyZLN2sCKGZM",
  "pdfSize": "335982"
}