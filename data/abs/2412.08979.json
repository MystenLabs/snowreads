{"id":"2412.08979","title":"A Wander Through the Multimodal Landscape: Efficient Transfer Learning\n  via Low-rank Sequence Multimodal Adapter","authors":"Zirun Guo, Xize Cheng, Yangyang Wu, Tao Jin","authorsParsed":[["Guo","Zirun",""],["Cheng","Xize",""],["Wu","Yangyang",""],["Jin","Tao",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 06:26:02 GMT"}],"updateDate":"2024-12-13","timestamp":1733984762000,"abstract":"  Efficient transfer learning methods such as adapter-based methods have shown\ngreat success in unimodal models and vision-language models. However, existing\nmethods have two main challenges in fine-tuning multimodal models. Firstly,\nthey are designed for vision-language tasks and fail to extend to situations\nwhere there are more than two modalities. Secondly, they exhibit limited\nexploitation of interactions between modalities and lack efficiency. To address\nthese issues, in this paper, we propose the loW-rank sequence multimodal\nadapter (Wander). We first use the outer product to fuse the information from\ndifferent modalities in an element-wise way effectively. For efficiency, we use\nCP decomposition to factorize tensors into rank-one components and achieve\nsubstantial parameter reduction. Furthermore, we implement a token-level\nlow-rank decomposition to extract more fine-grained features and sequence\nrelationships between modalities. With these designs, Wander enables\ntoken-level interactions between sequences of different modalities in a\nparameter-efficient way. We conduct extensive experiments on datasets with\ndifferent numbers of modalities, where Wander outperforms state-of-the-art\nefficient transfer learning methods consistently. The results fully demonstrate\nthe effectiveness, efficiency and universality of Wander.\n","subjects":["Computer Science/Machine Learning","Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JSZgBWXQirc8XUUUr8vmZbm-1ZUs4kFAyCwvqdAL5xE","pdfSize":"1150549"}