{"id":"2407.11888","title":"Ascend-CC: Confidential Computing on Heterogeneous NPU for Emerging\n  Generative AI Workloads","authors":"Aritra Dhar, Cl\\'ement Thorens, Lara Magdalena Lazier, Lukas Cavigelli","authorsParsed":[["Dhar","Aritra",""],["Thorens","Cl√©ment",""],["Lazier","Lara Magdalena",""],["Cavigelli","Lukas",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 16:17:28 GMT"}],"updateDate":"2024-07-17","timestamp":1721146648000,"abstract":"  Cloud workloads have dominated generative AI based on large language models\n(LLM). Specialized hardware accelerators, such as GPUs, NPUs, and TPUs, play a\nkey role in AI adoption due to their superior performance over general-purpose\nCPUs. The AI models and the data are often highly sensitive and come from\nmutually distrusting parties. Existing CPU-based TEEs such as Intel SGX or AMD\nSEV do not provide sufficient protection. Device-centric TEEs like Nvidia-CC\nonly address tightly coupled CPU-GPU systems with a proprietary solution\nrequiring TEE on the host CPU side. On the other hand, existing academic\nproposals are tailored toward specific CPU-TEE platforms.\n  To address this gap, we propose Ascend-CC, a confidential computing\narchitecture based on discrete NPU devices that requires no trust in the host\nsystem. Ascend-CC provides strong security by ensuring data and model\nencryption that protects not only the data but also the model parameters and\noperator binaries. Ascend-CC uses delegation-based memory semantics to ensure\nisolation from the host software stack, and task attestation provides strong\nmodel integrity guarantees. Our Ascend-CC implementation and evaluation with\nstate-of-the-art LLMs such as Llama2 and Llama3 shows that Ascend-CC introduces\nminimal overhead with no changes in the AI software stack.\n","subjects":["Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VLNp4nkVEMWZsIX-Sf7ovo4vOiy5bjlzEwv6PWPXXMA","pdfSize":"1155850"}