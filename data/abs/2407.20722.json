{"id":"2407.20722","title":"Persistent Sampling: Unleashing the Potential of Sequential Monte Carlo","authors":"Minas Karamanis and Uro\\v{s} Seljak","authorsParsed":[["Karamanis","Minas",""],["Seljak","Uro≈°",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 10:34:40 GMT"}],"updateDate":"2024-07-31","timestamp":1722335680000,"abstract":"  Sequential Monte Carlo (SMC) methods are powerful tools for Bayesian\ninference but suffer from requiring many particles for accurate estimates,\nleading to high computational costs. We introduce persistent sampling (PS), an\nextension of SMC that mitigates this issue by allowing particles from previous\niterations to persist. This generates a growing, weighted ensemble of particles\ndistributed across iterations. In each iteration, PS utilizes multiple\nimportance sampling and resampling from the mixture of all previous\ndistributions to produce the next generation of particles. This addresses\nparticle impoverishment and mode collapse, resulting in more accurate posterior\napproximations. Furthermore, this approach provides lower-variance marginal\nlikelihood estimates for model comparison. Additionally, the persistent\nparticles improve transition kernel adaptation for efficient exploration.\nExperiments on complex distributions show that PS consistently outperforms\nstandard methods, achieving lower squared bias in posterior moment estimation\nand significantly reduced marginal likelihood errors, all at a lower\ncomputational cost. PS offers a robust, efficient, and scalable framework for\nBayesian inference.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Statistics/Computation"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"47P893QYc1TA_w84HCWQGyb5aVlea1KuOpHTvQwYIA8","pdfSize":"1164577"}