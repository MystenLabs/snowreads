{"id":"2412.14737","title":"On Verbalized Confidence Scores for LLMs","authors":"Daniel Yang, Yao-Hung Hubert Tsai, Makoto Yamada","authorsParsed":[["Yang","Daniel",""],["Tsai","Yao-Hung Hubert",""],["Yamada","Makoto",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 11:10:36 GMT"}],"updateDate":"2024-12-20","timestamp":1734606636000,"abstract":"  The rise of large language models (LLMs) and their tight integration into our\ndaily life make it essential to dedicate efforts towards their trustworthiness.\nUncertainty quantification for LLMs can establish more human trust into their\nresponses, but also allows LLM agents to make more informed decisions based on\neach other's uncertainty. To estimate the uncertainty in a response, internal\ntoken logits, task-specific proxy models, or sampling of multiple responses are\ncommonly used. This work focuses on asking the LLM itself to verbalize its\nuncertainty with a confidence score as part of its output tokens, which is a\npromising way for prompt- and model-agnostic uncertainty quantification with\nlow overhead. Using an extensive benchmark, we assess the reliability of\nverbalized confidence scores with respect to different datasets, models, and\nprompt methods. Our results reveal that the reliability of these scores\nstrongly depends on how the model is asked, but also that it is possible to\nextract well-calibrated confidence scores with certain prompt methods. We argue\nthat verbalized confidence scores can become a simple but effective and\nversatile uncertainty quantification method in the future. Our code is\navailable at https://github.com/danielyxyang/llm-verbalized-uq .\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6XFLc6dVnIyZcZXLWp4rWi1q6aJMykhUMEf5b5nih_w","pdfSize":"1065257"}