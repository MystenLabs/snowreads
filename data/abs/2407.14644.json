{"id":"2407.14644","title":"Human-Interpretable Adversarial Prompt Attack on Large Language Models\n  with Situational Context","authors":"Nilanjana Das, Edward Raff, Manas Gaur","authorsParsed":[["Das","Nilanjana",""],["Raff","Edward",""],["Gaur","Manas",""]],"versions":[{"version":"v1","created":"Fri, 19 Jul 2024 19:47:26 GMT"},{"version":"v2","created":"Thu, 25 Jul 2024 22:44:54 GMT"}],"updateDate":"2024-07-29","timestamp":1721418446000,"abstract":"  Previous research on testing the vulnerabilities in Large Language Models\n(LLMs) using adversarial attacks has primarily focused on nonsensical prompt\ninjections, which are easily detected upon manual or automated review (e.g.,\nvia byte entropy). However, the exploration of innocuous human-understandable\nmalicious prompts augmented with adversarial injections remains limited. In\nthis research, we explore converting a nonsensical suffix attack into a\nsensible prompt via a situation-driven contextual re-writing. This allows us to\nshow suffix conversion without any gradients, using only LLMs to perform the\nattacks, and thus better understand the scope of possible risks. We combine an\nindependent, meaningful adversarial insertion and situations derived from\nmovies to check if this can trick an LLM. The situations are extracted from the\nIMDB dataset, and prompts are defined following a few-shot chain-of-thought\nprompting. Our approach demonstrates that a successful situation-driven attack\ncan be executed on both open-source and proprietary LLMs. We find that across\nmany LLMs, as few as 1 attempt produces an attack and that these attacks\ntransfer between LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZhV1GBHHMbawZ_imKDnIcntcQhE33BpoZNBol7aaGZ4","pdfSize":"244166"}