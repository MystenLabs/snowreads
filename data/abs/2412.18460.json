{"id":"2412.18460","title":"GeFL: Model-Agnostic Federated Learning with Generative Models","authors":"Honggu Kang, Seohyeon Cha, Joonhyuk Kang","authorsParsed":[["Kang","Honggu",""],["Cha","Seohyeon",""],["Kang","Joonhyuk",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 14:39:47 GMT"}],"updateDate":"2024-12-25","timestamp":1735051187000,"abstract":"  Federated learning (FL) is a promising paradigm in distributed learning while\npreserving the privacy of users. However, the increasing size of recent models\nmakes it unaffordable for a few users to encompass the model. It leads the\nusers to adopt heterogeneous models based on their diverse computing\ncapabilities and network bandwidth. Correspondingly, FL with heterogeneous\nmodels should be addressed, given that FL typically involves training a single\nglobal model. In this paper, we propose Generative Model-Aided Federated\nLearning (GeFL), incorporating a generative model that aggregates global\nknowledge across users of heterogeneous models. Our experiments on various\nclassification tasks demonstrate notable performance improvements of GeFL\ncompared to baselines, as well as limitations in terms of privacy and\nscalability. To tackle these concerns, we introduce a novel framework, GeFL-F.\nIt trains target networks aided by feature-generative models. We empirically\ndemonstrate the consistent performance gains of GeFL-F, while demonstrating\nbetter privacy preservation and robustness to a large number of clients. Codes\nare available at [1].\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"iYWnlItSK1VT0ruvuA8LSthDcQH4SQ3v1_QpIyny9Rs","pdfSize":"3264957"}