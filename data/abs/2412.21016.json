{
  "id": "2412.21016",
  "title": "Automated Robustness Testing for LLM-based NLP Software",
  "authors": "Mingxuan Xiao, Yan Xiao, Shunhui Ji, Hanbo Cai, Lei Xue, Pengcheng\n  Zhang",
  "authorsParsed": [
    [
      "Xiao",
      "Mingxuan",
      ""
    ],
    [
      "Xiao",
      "Yan",
      ""
    ],
    [
      "Ji",
      "Shunhui",
      ""
    ],
    [
      "Cai",
      "Hanbo",
      ""
    ],
    [
      "Xue",
      "Lei",
      ""
    ],
    [
      "Zhang",
      "Pengcheng",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 30 Dec 2024 15:33:34 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735572814000,
  "abstract": "  Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. To our knowledge, there are no known automated robustness testing\nmethods specifically designed for LLM-based NLP software. Given the complexity\nof LLMs and the unpredictability of real-world inputs (including prompts and\nexamples), it is essential to examine the robustness of overall inputs to\nensure the safety of such software.\n  To this end, this paper introduces the first AutOmated Robustness Testing\nfrAmework, AORTA, which reconceptualizes the testing process into a\ncombinatorial optimization problem. Existing testing methods designed for\nDNN-based software can be applied to LLM-based software by AORTA, but their\neffectiveness is limited. To address this, we propose a novel testing method\nfor LLM-based software within AORTA called Adaptive Beam Search. ABS is\ntailored for the expansive feature space of LLMs and improves testing\neffectiveness through an adaptive beam width and the capability for\nbacktracking.\n  We successfully embed 18 test methods in the designed framework AORTA and\ncompared the test validity of ABS with three datasets and five threat models.\nABS facilitates a more comprehensive and accurate robustness assessment before\nsoftware deployment, with an average test success rate of 86.138%. Compared to\nthe currently best-performing baseline PWWS, ABS significantly reduces the\ncomputational overhead by up to 3441.895 seconds per successful test case and\ndecreases the number of queries by 218.762 times on average. Furthermore, test\ncases generated by ABS exhibit greater naturalness and transferability.\n",
  "subjects": [
    "Computer Science/Software Engineering"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "0VCvDd3m0ynURrA5OK9G_yCMhxIwVjne69eyn7y73C4",
  "pdfSize": "3167230"
}