{
  "id": "2412.16819",
  "title": "Bi-Sparse Unsupervised Feature Selection",
  "authors": "Xianchao Xiu, Chenyi Huang, Pan Shang, Wanquan Liu",
  "authorsParsed": [
    [
      "Xiu",
      "Xianchao",
      ""
    ],
    [
      "Huang",
      "Chenyi",
      ""
    ],
    [
      "Shang",
      "Pan",
      ""
    ],
    [
      "Liu",
      "Wanquan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 22 Dec 2024 01:43:58 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734831838000,
  "abstract": "  To efficiently deal with high-dimensional datasets in many areas,\nunsupervised feature selection (UFS) has become a rising technique for\ndimension reduction. Even though there are many UFS methods, most of them only\nconsider the global structure of datasets by embedding a single sparse\nregularization or constraint. In this paper, we introduce a novel bi-sparse UFS\nmethod, called BSUFS, to simultaneously characterize both global and local\nstructures. The core idea of BSUFS is to incorporate $\\ell_{2,p}$-norm and\n$\\ell_q$-norm into the classical principal component analysis (PCA), which\nenables our proposed method to select relevant features and filter out\nirrelevant noise accurately. Here, the parameters $p$ and $q$ are within the\nrange of [0,1). Therefore, BSUFS not only constructs a unified framework for\nbi-sparse optimization, but also includes some existing works as special cases.\nTo solve the resulting non-convex model, we propose an efficient proximal\nalternating minimization (PAM) algorithm using Riemannian manifold optimization\nand sparse optimization techniques. Theoretically, PAM is proven to have global\nconvergence, i.e., for any random initial point, the generated sequence\nconverges to a critical point that satisfies the first-order optimality\ncondition. Extensive numerical experiments on synthetic and real-world datasets\ndemonstrate the effectiveness of our proposed BSUFS. Specifically, the average\naccuracy (ACC) is improved by at least 4.71% and the normalized mutual\ninformation (NMI) is improved by at least 3.14% on average compared to the\nexisting UFS competitors. The results validate the advantages of bi-sparse\noptimization in feature selection and show its potential for other fields in\nimage processing. Our code will be available at https://github.com/xianchaoxiu.\n",
  "subjects": [
    "Mathematics/Optimization and Control",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "cV2sNQZ0c3cnK_PPsQJyf6qdkKT950s4kNKpks3I0mk",
  "pdfSize": "6568375"
}