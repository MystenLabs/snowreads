{
  "id": "2412.15396",
  "title": "Learning Visual Composition through Improved Semantic Guidance",
  "authors": "Austin Stone, Hagen Soltau, Robert Geirhos, Xi Yi, Ye Xia, Bingyi Cao,\n  Kaifeng Chen, Abhijit Ogale, Jonathon Shlens",
  "authorsParsed": [
    [
      "Stone",
      "Austin",
      ""
    ],
    [
      "Soltau",
      "Hagen",
      ""
    ],
    [
      "Geirhos",
      "Robert",
      ""
    ],
    [
      "Yi",
      "Xi",
      ""
    ],
    [
      "Xia",
      "Ye",
      ""
    ],
    [
      "Cao",
      "Bingyi",
      ""
    ],
    [
      "Chen",
      "Kaifeng",
      ""
    ],
    [
      "Ogale",
      "Abhijit",
      ""
    ],
    [
      "Shlens",
      "Jonathon",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 19 Dec 2024 20:58:26 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734641906000,
  "abstract": "  Visual imagery does not consist of solitary objects, but instead reflects the\ncomposition of a multitude of fluid concepts. While there have been great\nadvances in visual representation learning, such advances have focused on\nbuilding better representations for a small number of discrete objects bereft\nof an understanding of how these objects are interacting. One can observe this\nlimitation in representations learned through captions or contrastive learning\n-- where the learned model treats an image essentially as a bag of words.\nSeveral works have attempted to address this limitation through the development\nof bespoke learned architectures to directly address the shortcomings in\ncompositional learning. In this work, we focus on simple, and scalable\napproaches. In particular, we demonstrate that by substantially improving\nweakly labeled data, i.e. captions, we can vastly improve the performance of\nstandard contrastive learning approaches. Previous CLIP models achieved near\nchance rate on challenging tasks probing compositional learning. However, our\nsimple approach boosts performance of CLIP substantially and surpasses all\nbespoke architectures. Furthermore, we showcase our results on a relatively new\ncaptioning benchmark derived from DOCCI. We demonstrate through a series of\nablations that a standard CLIP model trained with enhanced data may demonstrate\nimpressive performance on image retrieval tasks.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language",
    "Computer Science/Information Retrieval"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "3wRYtt83h-oTgxYPU2-fDSTaupJxz1JbgG-ecHwa0Qs",
  "pdfSize": "4363911"
}