{
  "id": "2412.01868",
  "title": "Composition of Experts: A Modular Compound AI System Leveraging Large\n  Language Models",
  "authors": "Swayambhoo Jain, Ravi Raju, Bo Li, Zoltan Csaki, Jonathan Li, Kaizhao\n  Liang, Guoyao Feng, Urmish Thakkar, Anand Sampat, Raghu Prabhakar, Sumati\n  Jairath",
  "authorsParsed": [
    [
      "Jain",
      "Swayambhoo",
      ""
    ],
    [
      "Raju",
      "Ravi",
      ""
    ],
    [
      "Li",
      "Bo",
      ""
    ],
    [
      "Csaki",
      "Zoltan",
      ""
    ],
    [
      "Li",
      "Jonathan",
      ""
    ],
    [
      "Liang",
      "Kaizhao",
      ""
    ],
    [
      "Feng",
      "Guoyao",
      ""
    ],
    [
      "Thakkar",
      "Urmish",
      ""
    ],
    [
      "Sampat",
      "Anand",
      ""
    ],
    [
      "Prabhakar",
      "Raghu",
      ""
    ],
    [
      "Jairath",
      "Sumati",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 07:43:21 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733125401000,
  "abstract": "  Large Language Models (LLMs) have achieved remarkable advancements, but their\nmonolithic nature presents challenges in terms of scalability, cost, and\ncustomization. This paper introduces the Composition of Experts (CoE), a\nmodular compound AI system leveraging multiple expert LLMs. CoE leverages a\nrouter to dynamically select the most appropriate expert for a given input,\nenabling efficient utilization of resources and improved performance. We\nformulate the general problem of training a CoE and discuss inherent\ncomplexities associated with it. We propose a two-step routing approach to\naddress these complexities that first uses a router to classify the input into\ndistinct categories followed by a category-to-expert mapping to obtain desired\nexperts. CoE offers a flexible and cost-effective solution to build compound AI\nsystems. Our empirical evaluation demonstrates the effectiveness of CoE in\nachieving superior performance with reduced computational overhead. Given that\nCoE comprises of many expert LLMs it has unique system requirements for\ncost-effective serving. We present an efficient implementation of CoE\nleveraging SambaNova SN40L RDUs unique three-tiered memory architecture. CoEs\nobtained using open weight LLMs Qwen/Qwen2-7B-Instruct, google/gemma-2-9b-it,\ngoogle/gemma-2-27b-it, meta-llama/Llama-3.1-70B-Instruct and\nQwen/Qwen2-72B-Instruct achieve a score of $59.4$ with merely $31$ billion\naverage active parameters on Arena-Hard and a score of $9.06$ with $54$ billion\naverage active parameters on MT-Bench.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "eneQL206cCGq5ePynHpG_uZ_IAccwkfvtv9PGyqEC7I",
  "pdfSize": "1573756"
}