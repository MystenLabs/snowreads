{"id":"2412.16530","title":"Improving Lip-synchrony in Direct Audio-Visual Speech-to-Speech\n  Translation","authors":"Lucas Goncalves, Prashant Mathur, Xing Niu, Brady Houston,\n  Chandrashekhar Lavania, Srikanth Vishnubhotla, Lijia Sun, Anthony Ferritto","authorsParsed":[["Goncalves","Lucas",""],["Mathur","Prashant",""],["Niu","Xing",""],["Houston","Brady",""],["Lavania","Chandrashekhar",""],["Vishnubhotla","Srikanth",""],["Sun","Lijia",""],["Ferritto","Anthony",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 08:15:52 GMT"}],"updateDate":"2024-12-24","timestamp":1734768952000,"abstract":"  Audio-Visual Speech-to-Speech Translation typically prioritizes improving\ntranslation quality and naturalness. However, an equally critical aspect in\naudio-visual content is lip-synchrony-ensuring that the movements of the lips\nmatch the spoken content-essential for maintaining realism in dubbed videos.\nDespite its importance, the inclusion of lip-synchrony constraints in AVS2S\nmodels has been largely overlooked. This study addresses this gap by\nintegrating a lip-synchrony loss into the training process of AVS2S models. Our\nproposed method significantly enhances lip-synchrony in direct audio-visual\nspeech-to-speech translation, achieving an average LSE-D score of 10.67,\nrepresenting a 9.2% reduction in LSE-D over a strong baseline across four\nlanguage pairs. Additionally, it maintains the naturalness and high quality of\nthe translated speech when overlaid onto the original video, without any\ndegradation in translation quality.\n","subjects":["Computer Science/Sound","Computer Science/Computation and Language","Computer Science/Computer Vision and Pattern Recognition","Computer Science/Multimedia","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"K36JHZTANytXLaKO8jonQZd_ZSKgaZxFsihowXUNcUU","pdfSize":"384059"}