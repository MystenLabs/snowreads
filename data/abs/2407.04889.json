{"id":"2407.04889","title":"Maximizing utility in multi-agent environments by anticipating the\n  behavior of other learners","authors":"Angelos Assos, Yuval Dagan and Constantinos Daskalakis","authorsParsed":[["Assos","Angelos",""],["Dagan","Yuval",""],["Daskalakis","Constantinos",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 23:16:18 GMT"}],"updateDate":"2024-07-09","timestamp":1720221378000,"abstract":"  Learning algorithms are often used to make decisions in sequential\ndecision-making environments. In multi-agent settings, the decisions of each\nagent can affect the utilities/losses of the other agents. Therefore, if an\nagent is good at anticipating the behavior of the other agents, in particular\nhow they will make decisions in each round as a function of their experience\nthat far, it could try to judiciously make its own decisions over the rounds of\nthe interaction so as to influence the other agents to behave in a way that\nultimately benefits its own utility. In this paper, we study repeated\ntwo-player games involving two types of agents: a learner, which employs an\nonline learning algorithm to choose its strategy in each round; and an\noptimizer, which knows the learner's utility function and the learner's online\nlearning algorithm. The optimizer wants to plan ahead to maximize its own\nutility, while taking into account the learner's behavior. We provide two\nresults: a positive result for repeated zero-sum games and a negative result\nfor repeated general-sum games. Our positive result is an algorithm for the\noptimizer, which exactly maximizes its utility against a learner that plays the\nReplicator Dynamics -- the continuous-time analogue of Multiplicative Weights\nUpdate (MWU). Additionally, we use this result to provide an algorithm for the\noptimizer against MWU, i.e.~for the discrete-time setting, which guarantees an\naverage utility for the optimizer that is higher than the value of the one-shot\ngame. Our negative result shows that, unless P=NP, there is no Fully Polynomial\nTime Approximation Scheme (FPTAS) for maximizing the utility of an optimizer\nagainst a learner that best-responds to the history in each round. Yet, this\nstill leaves open the question of whether there exists a polynomial-time\nalgorithm that optimizes the utility up to $o(T)$.\n","subjects":["Computing Research Repository/Computer Science and Game Theory","Computing Research Repository/Machine Learning","Computing Research Repository/Multiagent Systems"],"license":"http://creativecommons.org/publicdomain/zero/1.0/","blobId":"itn2JGejUy_31sMq60liT54PxU00YhPJp5GrNdnZkMo","pdfSize":"477182"}