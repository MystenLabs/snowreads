{
  "id": "2412.13527",
  "title": "Lyapunov Analysis For Monotonically Forward-Backward Accelerated\n  Algorithms",
  "authors": "Mingwei Fu, Bin Shi",
  "authorsParsed": [
    [
      "Fu",
      "Mingwei",
      ""
    ],
    [
      "Shi",
      "Bin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 06:09:00 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734502140000,
  "abstract": "  In the realm of gradient-based optimization, Nesterov's accelerated gradient\nmethod (NAG) is a landmark advancement, achieving an accelerated convergence\nrate that outperforms the vanilla gradient descent method for convex function.\nHowever, for strongly convex functions, whether NAG converges linearly remains\nan open question, as noted in the comprehensive review by Chambolle and Pock\n[2016]. This issue, aside from the critical step size, was addressed by Li et\nal. [2024a] using a high-resolution differential equation framework.\nFurthermore, Beck [2017, Section 10.7.4] introduced a monotonically convergent\nvariant of NAG, referred to as M-NAG. Despite these developments, the Lyapunov\nanalysis presented in [Li et al., 2024a] cannot be directly extended to M-NAG.\nIn this paper, we propose a modification to the iterative relation by\nintroducing a gradient term, leading to a new gradient-based iterative\nrelation. This adjustment allows for the construction of a novel Lyapunov\nfunction that excludes kinetic energy. The linear convergence derived from this\nLyapunov function is independent of both the parameters of the strongly convex\nfunctions and the step size, yielding a more general and robust result.\nNotably, we observe that the gradient iterative relation derived from M-NAG is\nequivalent to that from NAG when the position-velocity relation is applied.\nHowever, the Lyapunov analysis does not rely on the position-velocity relation,\nallowing us to extend the linear convergence to M-NAG. Finally, by utilizing\ntwo proximal inequalities, which serve as the proximal counterparts of strongly\nconvex inequalities, we extend the linear convergence to both the fast\niterative shrinkage-thresholding algorithm (FISTA) and its monotonic\ncounterpart (M-FISTA).\n",
  "subjects": [
    "Mathematics/Optimization and Control",
    "Computer Science/Numerical Analysis",
    "Mathematics/Numerical Analysis",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "twCTc6hpveUCAyxiQute-8PNrmMbIFG3HFLKMyBF1W0",
  "pdfSize": "341849"
}