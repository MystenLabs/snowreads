{
  "id": "2412.13602",
  "title": "Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games",
  "authors": "Wenye Lin, Jonathan Roberts, Yunhan Yang, Samuel Albanie, Zongqing Lu,\n  Kai Han",
  "authorsParsed": [
    [
      "Lin",
      "Wenye",
      ""
    ],
    [
      "Roberts",
      "Jonathan",
      ""
    ],
    [
      "Yang",
      "Yunhan",
      ""
    ],
    [
      "Albanie",
      "Samuel",
      ""
    ],
    [
      "Lu",
      "Zongqing",
      ""
    ],
    [
      "Han",
      "Kai",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 08:32:53 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734510773000,
  "abstract": "  Large Language Models (LLMs) are increasingly deployed in real-world\napplications that demand complex reasoning. To track progress, robust\nbenchmarks are required to evaluate their capabilities beyond superficial\npattern recognition. However, current LLM reasoning benchmarks often face\nchallenges such as insufficient interpretability, performance saturation or\ndata contamination. To address these challenges, we introduce GAMEBoT, a gaming\narena designed for rigorous and transparent assessment of LLM reasoning\ncapabilities. GAMEBoT decomposes complex reasoning in games into predefined\nmodular subproblems. This decomposition allows us to design a suite of\nChain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in\naddressing these subproblems before action selection. Furthermore, we develop a\nsuite of rule-based algorithms to generate ground truth for these subproblems,\nenabling rigorous validation of the LLMs' intermediate reasoning steps. This\napproach facilitates evaluation of both the quality of final actions and the\naccuracy of the underlying reasoning process. GAMEBoT also naturally alleviates\nthe risk of data contamination through dynamic games and head-to-head LLM\ncompetitions. We benchmark 17 prominent LLMs across eight games, encompassing\nvarious strategic abilities and game characteristics. Our results suggest that\nGAMEBoT presents a significant challenge, even when LLMs are provided with\ndetailed CoT prompts. Project page: \\url{https://visual-ai.github.io/gamebot}\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "A4x-YZX5BNLd4y4eVoa5mKS3Kf9YINL1cMKMW9Dpx-0",
  "pdfSize": "1551080"
}