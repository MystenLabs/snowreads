{"id":"2407.18480","title":"Scalable Graph Compressed Convolutions","authors":"Junshu Sun, Chenxue Yang, Shuhui Wang, Qingming Huang","authorsParsed":[["Sun","Junshu",""],["Yang","Chenxue",""],["Wang","Shuhui",""],["Huang","Qingming",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 03:14:13 GMT"}],"updateDate":"2024-07-29","timestamp":1721963653000,"abstract":"  Designing effective graph neural networks (GNNs) with message passing has two\nfundamental challenges, i.e., determining optimal message-passing pathways and\ndesigning local aggregators. Previous methods of designing optimal pathways are\nlimited with information loss on the input features. On the other hand,\nexisting local aggregators generally fail to extract multi-scale features and\napproximate diverse operators under limited parameter scales. In contrast to\nthese methods, Euclidean convolution has been proven as an expressive\naggregator, making it a perfect candidate for GNN construction. However, the\nchallenges of generalizing Euclidean convolution to graphs arise from the\nirregular structure of graphs. To bridge the gap between Euclidean space and\ngraph topology, we propose a differentiable method that applies permutations to\ncalibrate input graphs for Euclidean convolution. The permutations constrain\nall nodes in a row regardless of their input order and therefore enable the\nflexible generalization of Euclidean convolution to graphs. Based on the graph\ncalibration, we propose the Compressed Convolution Network (CoCN) for\nhierarchical graph representation learning. CoCN follows local feature-learning\nand global parameter-sharing mechanisms of convolution neural networks. The\nwhole model can be trained end-to-end, with compressed convolution applied to\nlearn individual node features and their corresponding structure features. CoCN\ncan further borrow successful practices from Euclidean convolution, including\nresidual connection and inception mechanism. We validate CoCN on both\nnode-level and graph-level benchmarks. CoCN achieves superior performance over\ncompetitive GNN baselines. Codes are available at\nhttps://github.com/sunjss/CoCN.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"UHMy-r2sdZVudrL8uHHxzYIW_7C6X_DKbo9JwU7QiMM","pdfSize":"6749330"}