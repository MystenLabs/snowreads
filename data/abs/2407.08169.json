{"id":"2407.08169","title":"Faster Machine Unlearning via Natural Gradient Descent","authors":"Omri Lev and Ashia Wilson","authorsParsed":[["Lev","Omri",""],["Wilson","Ashia",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 04:19:28 GMT"}],"updateDate":"2024-07-12","timestamp":1720671568000,"abstract":"  We address the challenge of efficiently and reliably deleting data from\nmachine learning models trained using Empirical Risk Minimization (ERM), a\nprocess known as machine unlearning. To avoid retraining models from scratch,\nwe propose a novel algorithm leveraging Natural Gradient Descent (NGD). Our\ntheoretical framework ensures strong privacy guarantees for convex models,\nwhile a practical Min/Max optimization algorithm is developed for non-convex\nmodels. Comprehensive evaluations show significant improvements in privacy,\ncomputational efficiency, and generalization compared to state-of-the-art\nmethods, advancing both the theoretical and practical aspects of machine\nunlearning.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"W5pqNb8_qepxqrhZlBzAA9v11SoXPB4SSA4fQvyWLZQ","pdfSize":"609704"}