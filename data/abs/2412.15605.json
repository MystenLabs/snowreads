{
  "id": "2412.15605",
  "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
  "authors": "Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, Hen-Hsen Huang",
  "authorsParsed": [
    [
      "Chan",
      "Brian J",
      ""
    ],
    [
      "Chen",
      "Chao-Ting",
      ""
    ],
    [
      "Cheng",
      "Jui-Hung",
      ""
    ],
    [
      "Huang",
      "Hen-Hsen",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 06:58:32 GMT"
    },
    {
      "version": "v2",
      "created": "Sun, 23 Feb 2025 19:48:12 GMT"
    }
  ],
  "updateDate": "2025-02-25",
  "timestamp": 1734677912000,
  "abstract": "  Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "GjhR1jya7bT_GteGYCyAP64u5bokSmshrmJlRZ-KtLM",
  "pdfSize": "121164"
}