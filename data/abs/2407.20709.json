{"id":"2407.20709","title":"A Case Study on Visual-Audio-Tactile Cross-Modal Retrieval","authors":"Jagoda Wojcik, Jiaqi Jiang, Jiacheng Wu and Shan Luo","authorsParsed":[["Wojcik","Jagoda",""],["Jiang","Jiaqi",""],["Wu","Jiacheng",""],["Luo","Shan",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 10:05:33 GMT"}],"updateDate":"2024-07-31","timestamp":1722333933000,"abstract":"  Cross-Modal Retrieval (CMR), which retrieves relevant items from one modality\n(e.g., audio) given a query in another modality (e.g., visual), has undergone\nsignificant advancements in recent years. This capability is crucial for robots\nto integrate and interpret information across diverse sensory inputs. However,\nthe retrieval space in existing robotic CMR approaches often consists of only\none modality, which limits the robot's performance. In this paper, we propose a\nnovel CMR model that incorporates three different modalities, i.e., visual,\naudio and tactile, for enhanced multi-modal object retrieval, named as VAT-CMR.\nIn this model, multi-modal representations are first fused to provide a\nholistic view of object features. To mitigate the semantic gaps between\nrepresentations of different modalities, a dominant modality is then selected\nduring the classification training phase to improve the distinctiveness of the\nrepresentations, so as to improve the retrieval performance. To evaluate our\nproposed approach, we conducted a case study and the results demonstrate that\nour VAT-CMR model surpasses competing approaches. Further, our proposed\ndominant modality selection significantly enhances cross-retrieval accuracy.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_G3umGili2uLbaD15l6xWdRDJ3KnVnxgbvLN2rOisZY","pdfSize":"4725538"}