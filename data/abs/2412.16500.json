{
  "id": "2412.16500",
  "title": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition",
  "authors": "Do June Min, Karel Mundnich, Andy Lapastora, Erfan Soltanmohammadi,\n  Srikanth Ronanki, and Kyu Han",
  "authorsParsed": [
    [
      "Min",
      "Do June",
      ""
    ],
    [
      "Mundnich",
      "Karel",
      ""
    ],
    [
      "Lapastora",
      "Andy",
      ""
    ],
    [
      "Soltanmohammadi",
      "Erfan",
      ""
    ],
    [
      "Ronanki",
      "Srikanth",
      ""
    ],
    [
      "Han",
      "Kyu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 06:16:04 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 2 Jan 2025 07:29:01 GMT"
    },
    {
      "version": "v3",
      "created": "Fri, 3 Jan 2025 07:18:30 GMT"
    }
  ],
  "updateDate": "2025-01-06",
  "timestamp": 1734761764000,
  "abstract": "  One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.\n",
  "subjects": [
    "Electrical Engineering and Systems Science/Audio and Speech Processing",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "EAIS6N3kPWUo9UQ6_6lRoBb-BsO7MFDzULNrx5jk63M",
  "pdfSize": "522257"
}