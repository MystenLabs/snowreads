{
  "id": "2412.02153",
  "title": "Revisiting the Initial Steps in Adaptive Gradient Descent Optimization",
  "authors": "Abulikemu Abuduweili and Changliu Liu",
  "authorsParsed": [
    [
      "Abuduweili",
      "Abulikemu",
      ""
    ],
    [
      "Liu",
      "Changliu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 04:28:14 GMT"
    },
    {
      "version": "v2",
      "created": "Tue, 11 Feb 2025 16:23:39 GMT"
    }
  ],
  "updateDate": "2025-02-12",
  "timestamp": 1733200094000,
  "abstract": "  Adaptive gradient optimization methods, such as Adam, are prevalent in\ntraining deep neural networks across diverse machine learning tasks due to\ntheir ability to achieve faster convergence. However, these methods often\nsuffer from suboptimal generalization compared to stochastic gradient descent\n(SGD) and exhibit instability, particularly when training Transformer models.\nIn this work, we show the standard initialization of the second-order moment\nestimation ($v_0 =0$) as a significant factor contributing to these\nlimitations. We introduce simple yet effective solutions: initializing the\nsecond-order moment estimation with non-zero values, using either data-driven\nor random initialization strategies. Empirical evaluations demonstrate that our\napproach not only stabilizes convergence but also enhances the final\nperformance of adaptive gradient optimizers. Furthermore, by adopting the\nproposed initialization strategies, Adam achieves performance comparable to\nmany recently proposed variants of adaptive gradient optimization methods. Our\ncode is available at https://github.com/Walleclipse/Adam_Initialization.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/publicdomain/zero/1.0/",
  "blobId": "9fNmEiErKLyC3WHrlgogExlW40TvxpSXw1oISwX8iRY",
  "pdfSize": "1033995"
}