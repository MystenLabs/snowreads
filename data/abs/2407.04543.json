{"id":"2407.04543","title":"Strengthening Structural Inductive Biases by Pre-training to Perform\n  Syntactic Transformations","authors":"Matthias Lindemann, Alexander Koller, Ivan Titov","authorsParsed":[["Lindemann","Matthias",""],["Koller","Alexander",""],["Titov","Ivan",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 14:29:44 GMT"}],"updateDate":"2024-07-08","timestamp":1720189784000,"abstract":"  Models need appropriate inductive biases to effectively learn from small\namounts of data and generalize systematically outside of the training\ndistribution. While Transformers are highly versatile and powerful, they can\nstill benefit from enhanced structural inductive biases for seq2seq tasks,\nespecially those involving syntactic transformations, such as converting active\nto passive voice or semantic parsing. In this paper, we propose to strengthen\nthe structural inductive bias of a Transformer by intermediate pre-training to\nperform synthetically generated syntactic transformations of dependency trees\ngiven a description of the transformation. Our experiments confirm that this\nhelps with few-shot learning of syntactic tasks such as chunking, and also\nimproves structural generalization for semantic parsing. Our analysis shows\nthat the intermediate pre-training leads to attention heads that keep track of\nwhich syntactic transformation needs to be applied to which token, and that the\nmodel can leverage these attention heads on downstream tasks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"fxPYZY5frNkVGJiONLFa0hzEMC1c8SA9-EVTW7voXnk","pdfSize":"597779"}