{
  "id": "2412.09751",
  "title": "AI Red-Teaming is a Sociotechnical System. Now What?",
  "authors": "Tarleton Gillespie, Ryland Shaw, Mary L. Gray, Jina Suh",
  "authorsParsed": [
    [
      "Gillespie",
      "Tarleton",
      ""
    ],
    [
      "Shaw",
      "Ryland",
      ""
    ],
    [
      "Gray",
      "Mary L.",
      ""
    ],
    [
      "Suh",
      "Jina",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 12 Dec 2024 22:48:19 GMT"
    }
  ],
  "updateDate": "2024-12-16",
  "timestamp": 1734043699000,
  "abstract": "  As generative AI technologies find more and more real-world applications, the\nimportance of testing their performance and safety seems paramount.\n``Red-teaming'' has quickly become the primary approach to test AI\nmodels--prioritized by AI companies, and enshrined in AI policy and regulation.\nMembers of red teams act as adversaries, probing AI systems to test their\nsafety mechanisms and uncover vulnerabilities. Yet we know too little about\nthis work and its implications. This essay calls for collaboration between\ncomputer scientists and social scientists to study the sociotechnical systems\nsurrounding AI technologies, including the work of red-teaming, to avoid\nrepeating the mistakes of the recent past. We highlight the importance of\nunderstanding the values and assumptions behind red-teaming, the labor\ninvolved, and the psychological impacts on red-teamers.\n",
  "subjects": [
    "Computer Science/Computers and Society",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Human-Computer Interaction"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "TO9S-TZtgUh9HtZQBeXqLUivQOMlKBVuzh3nSCDr8XE",
  "pdfSize": "155082"
}