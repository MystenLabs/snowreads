{
  "id": "2412.10416",
  "title": "SuperMerge: An Approach For Gradient-Based Model Merging",
  "authors": "Haoyu Yang, Zheng Zhang, Saket Sathe",
  "authorsParsed": [
    [
      "Yang",
      "Haoyu",
      ""
    ],
    [
      "Zhang",
      "Zheng",
      ""
    ],
    [
      "Sathe",
      "Saket",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 20:03:14 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 14 Feb 2025 17:40:13 GMT"
    }
  ],
  "updateDate": "2025-02-17",
  "timestamp": 1733774594000,
  "abstract": "  Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic,\nmonolithic, and possess the superpower to simultaneously support thousands of\ntasks. However, high-throughput applications often prefer smaller task-specific\nmodels because of their lower latency and cost. One challenge of using\ntask-specific models is the incremental need for solving newer tasks after the\nmodel is already deployed for existing tasks. A straightforward solution\nrequires fine-tuning the model again for both existing and new tasks, which is\ncomputationally expensive and time-consuming. To address this issue, we propose\na model merging based approach called SUPERMERGE. SUPERMERGE is a\ngradient-based method to systematically merge several fine-tuned models trained\non existing and new tasks. SUPERMERGE is designed to be lightweight and fast,\nand the merged model achieves similar performance to fully fine-tuned models on\nall tasks. Furthermore, we proposed a hierarchical model merging strategy to\nreduce the peak space requirement without sacrificing the performance of the\nmerged model. We experimentally demonstrate that SUPERMERGE outperforms\nexisting model merging methods on common natural language processing and\ncomputer vision tasks.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "U2AEDRpzjBdUp2q-kT4hjRek5n7YL-UhJvqFLTm3Qws",
  "pdfSize": "5409360"
}