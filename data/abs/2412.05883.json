{"id":"2412.05883","title":"Understanding the Impact of Graph Reduction on Adversarial Robustness in\n  Graph Neural Networks","authors":"Kerui Wu, Ka-Ho Chow, Wenqi Wei, Lei Yu","authorsParsed":[["Wu","Kerui",""],["Chow","Ka-Ho",""],["Wei","Wenqi",""],["Yu","Lei",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 10:22:23 GMT"}],"updateDate":"2024-12-10","timestamp":1733653343000,"abstract":"  As Graph Neural Networks (GNNs) become increasingly popular for learning from\nlarge-scale graph data across various domains, their susceptibility to\nadversarial attacks when using graph reduction techniques for scalability\nremains underexplored. In this paper, we present an extensive empirical study\nto investigate the impact of graph reduction techniques, specifically graph\ncoarsening and sparsification, on the robustness of GNNs against adversarial\nattacks. Through extensive experiments involving multiple datasets and GNN\narchitectures, we examine the effects of four sparsification and six coarsening\nmethods on the poisoning attacks. Our results indicate that, while graph\nsparsification can mitigate the effectiveness of certain poisoning attacks,\nsuch as Mettack, it has limited impact on others, like PGD. Conversely, graph\ncoarsening tends to amplify the adversarial impact, significantly reducing\nclassification accuracy as the reduction ratio decreases. Additionally, we\nprovide a novel analysis of the causes driving these effects and examine how\ndefensive GNN models perform under graph reduction, offering practical insights\nfor designing robust GNNs within graph acceleration systems.\n","subjects":["Computer Science/Machine Learning","Computer Science/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2aXLaBfs3XlmzR_N1EqDHyA63MrsQZNJzhYQaoq7NEw","pdfSize":"798425"}