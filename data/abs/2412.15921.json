{
  "id": "2412.15921",
  "title": "Less is More: Towards Green Code Large Language Models via Unified\n  Structural Pruning",
  "authors": "Guang Yang, Yu Zhou, Xiangyu Zhang, Wei Cheng, Ke Liu, Xiang Chen,\n  Terry Yue Zhuo, Taolue Chen",
  "authorsParsed": [
    [
      "Yang",
      "Guang",
      ""
    ],
    [
      "Zhou",
      "Yu",
      ""
    ],
    [
      "Zhang",
      "Xiangyu",
      ""
    ],
    [
      "Cheng",
      "Wei",
      ""
    ],
    [
      "Liu",
      "Ke",
      ""
    ],
    [
      "Chen",
      "Xiang",
      ""
    ],
    [
      "Zhuo",
      "Terry Yue",
      ""
    ],
    [
      "Chen",
      "Taolue",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 14:13:09 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734703989000,
  "abstract": "  The extensive application of Large Language Models (LLMs) in generative\ncoding tasks has raised concerns due to their high computational demands and\nenergy consumption. Unlike previous structural pruning methods designed for\nclassification models that deal with lowdimensional classification logits,\ngenerative Code LLMs produce high-dimensional token logit sequences, making\ntraditional pruning objectives inherently limited. Moreover, existing single\ncomponent pruning approaches further constrain the effectiveness when applied\nto generative Code LLMs. In response, we propose Flab-Pruner, an innovative\nunified structural pruning method that combines vocabulary, layer, and\nFeed-Forward Network (FFN) pruning. This approach effectively reduces model\nparameters while maintaining performance. Additionally, we introduce a\ncustomized code instruction data strategy for coding tasks to enhance the\nperformance recovery efficiency of the pruned model. Through extensive\nevaluations on three state-of-the-art Code LLMs across multiple generative\ncoding tasks, the results demonstrate that Flab-Pruner retains 97% of the\noriginal performance after pruning 22% of the parameters and achieves the same\nor even better performance after post-training. The pruned models exhibit\nsignificant improvements in storage, GPU usage, computational efficiency, and\nenvironmental impact, while maintaining well robustness. Our research provides\na sustainable solution for green software engineering and promotes the\nefficient deployment of LLMs in real-world generative coding intelligence\napplications.\n",
  "subjects": [
    "Computer Science/Software Engineering",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "gCTAK9oUrQ_PAnyOlwElCCYksIIuzG_z9PNqkg5ra8s",
  "pdfSize": "2790271"
}