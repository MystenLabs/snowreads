{"id":"2412.09800","title":"Infinite-dimensional next-generation reservoir computing","authors":"Lyudmila Grigoryeva, Hannah Lim Jing Ting, Juan-Pablo Ortega","authorsParsed":[["Grigoryeva","Lyudmila",""],["Ting","Hannah Lim Jing",""],["Ortega","Juan-Pablo",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 02:39:04 GMT"},{"version":"v2","created":"Mon, 16 Dec 2024 11:48:07 GMT"},{"version":"v3","created":"Fri, 21 Feb 2025 08:59:15 GMT"}],"updateDate":"2025-02-24","timestamp":1734057544000,"abstract":"  Next-generation reservoir computing (NG-RC) has attracted much attention due\nto its excellent performance in spatio-temporal forecasting of complex systems\nand its ease of implementation. This paper shows that NG-RC can be encoded as a\nkernel ridge regression that makes training efficient and feasible even when\nthe space of chosen polynomial features is very large. Additionally, an\nextension to an infinite number of covariates is possible, which makes the\nmethodology agnostic with respect to the lags into the past that are considered\nas explanatory factors, as well as with respect to the number of polynomial\ncovariates, an important hyperparameter in traditional NG-RC. We show that this\napproach has solid theoretical backing and good behavior based on kernel\nuniversality properties previously established in the literature. Various\nnumerical illustrations show that these generalizations of NG-RC outperform the\ntraditional approach in several forecasting applications.\n","subjects":["Computer Science/Machine Learning","Computer Science/Neural and Evolutionary Computing","Physics/Computational Physics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"Akbon-RG_yCdvoDHqx2podpPRt6KaysIdC9PQ9Qwc7A","pdfSize":"866995"}