{
  "id": "2412.15265",
  "title": "Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large\n  Language Models",
  "authors": "Yingshui Tan, Boren Zheng, Baihui Zheng, Kerui Cao, Huiyun Jing,\n  Jincheng Wei, Jiaheng Liu, Yancheng He, Wenbo Su, Xiangyong Zhu, Bo Zheng,\n  Kaifu Zhang",
  "authorsParsed": [
    [
      "Tan",
      "Yingshui",
      ""
    ],
    [
      "Zheng",
      "Boren",
      ""
    ],
    [
      "Zheng",
      "Baihui",
      ""
    ],
    [
      "Cao",
      "Kerui",
      ""
    ],
    [
      "Jing",
      "Huiyun",
      ""
    ],
    [
      "Wei",
      "Jincheng",
      ""
    ],
    [
      "Liu",
      "Jiaheng",
      ""
    ],
    [
      "He",
      "Yancheng",
      ""
    ],
    [
      "Su",
      "Wenbo",
      ""
    ],
    [
      "Zhu",
      "Xiangyong",
      ""
    ],
    [
      "Zheng",
      "Bo",
      ""
    ],
    [
      "Zhang",
      "Kaifu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 03:03:44 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 23 Dec 2024 11:06:56 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734404624000,
  "abstract": "  With the rapid advancement of Large Language Models (LLMs), significant\nsafety concerns have emerged. Fundamentally, the safety of large language\nmodels is closely linked to the accuracy, comprehensiveness, and clarity of\ntheir understanding of safety knowledge, particularly in domains such as law,\npolicy and ethics. This factuality ability is crucial in determining whether\nthese models can be deployed and applied safely and compliantly within specific\nregions. To address these challenges and better evaluate the factuality ability\nof LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark.\nChinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality,\nStatic, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA,\nwe perform a comprehensive evaluation on the factuality abilities of existing\nLLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG\nability and robustness against attacks.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "nf9UtpM05SOiK7MiLqnGFQbsq4572SVCPOLDJjFRrVg",
  "pdfSize": "4367563"
}