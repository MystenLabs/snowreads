{
  "id": "2412.05346",
  "title": "BadGPT-4o: stripping safety finetuning from GPT models",
  "authors": "Ekaterina Krupkina, Dmitrii Volkov",
  "authorsParsed": [
    [
      "Krupkina",
      "Ekaterina",
      ""
    ],
    [
      "Volkov",
      "Dmitrii",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 6 Dec 2024 13:56:36 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733493396000,
  "abstract": "  We show a version of Qi et al. 2023's simple fine-tuning poisoning technique\nstrips GPT-4o's safety guardrails without degrading the model. The BadGPT\nattack matches best white-box jailbreaks on HarmBench and StrongREJECT. It\nsuffers no token overhead or performance hits common to jailbreaks, as\nevaluated on tinyMMLU and open-ended generations. Despite having been known for\na year, this attack remains easy to execute.\n",
  "subjects": [
    "Computer Science/Cryptography and Security",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "xncxvRtP7ZVXF5roXKYi7FTJCRF0Xfwt8uxqO39KGyc",
  "pdfSize": "424084"
}