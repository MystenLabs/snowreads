{"id":"2412.11854","title":"Towards Understanding Systems Trade-offs in Retrieval-Augmented\n  Generation Model Inference","authors":"Michael Shen, Muhammad Umar, Kiwan Maeng, G. Edward Suh, Udit Gupta","authorsParsed":[["Shen","Michael",""],["Umar","Muhammad",""],["Maeng","Kiwan",""],["Suh","G. Edward",""],["Gupta","Udit",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 15:12:53 GMT"}],"updateDate":"2024-12-17","timestamp":1734361973000,"abstract":"  The rapid increase in the number of parameters in large language models\n(LLMs) has significantly increased the cost involved in fine-tuning and\nretraining LLMs, a necessity for keeping models up to date and improving\naccuracy. Retrieval-Augmented Generation (RAG) offers a promising approach to\nimproving the capabilities and accuracy of LLMs without the necessity of\nretraining. Although RAG eliminates the need for continuous retraining to\nupdate model data, it incurs a trade-off in the form of slower model inference\ntimes. Resultingly, the use of RAG in enhancing the accuracy and capabilities\nof LLMs often involves diverse performance implications and trade-offs based on\nits design. In an effort to begin tackling and mitigating the performance\npenalties associated with RAG from a systems perspective, this paper introduces\na detailed taxonomy and characterization of the different elements within the\nRAG ecosystem for LLMs that explore trade-offs within latency, throughput, and\nmemory. Our study reveals underlying inefficiencies in RAG for systems\ndeployment, that can result in TTFT latencies that are twice as long and\nunoptimized datastores that consume terabytes of storage.\n","subjects":["Computer Science/Hardware Architecture"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"x9U72oaYOvKkT0s7JJcDao2wjZ_5wHjbeFtV1qmHb44","pdfSize":"369475"}