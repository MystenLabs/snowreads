{"id":"2412.07386","title":"Algorithmic Phase Transitions in Language Models: A Mechanistic Case\n  Study of Arithmetic","authors":"Alan Sun, Ethan Sun, Warren Shepard","authorsParsed":[["Sun","Alan",""],["Sun","Ethan",""],["Shepard","Warren",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 10:32:01 GMT"}],"updateDate":"2024-12-11","timestamp":1733826721000,"abstract":"  Zero-shot capabilities of large language models make them powerful tools for\nsolving a range of tasks without explicit training. It remains unclear,\nhowever, how these models achieve such performance, or why they can zero-shot\nsome tasks but not others. In this paper, we shed some light on this phenomenon\nby defining and investigating algorithmic stability in language models --\nchanges in problem-solving strategy employed by the model as a result of\nchanges in task specification. We focus on a task where algorithmic stability\nis needed for generalization: two-operand arithmetic. Surprisingly, we find\nthat Gemma-2-2b employs substantially different computational models on closely\nrelated subtasks, i.e. four-digit versus eight-digit addition. Our findings\nsuggest that algorithmic instability may be a contributing factor to language\nmodels' poor zero-shot performance across certain logical reasoning tasks, as\nthey struggle to abstract different problem-solving strategies and smoothly\ntransition between them.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NkRY4qmIrKT0NCy7BCrhkUEHHnSqXQfbMo8Id2a4kNw","pdfSize":"1874421"}