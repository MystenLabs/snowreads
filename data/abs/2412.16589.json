{
  "id": "2412.16589",
  "title": "Improving FIM Code Completions via Context & Curriculum Based Learning",
  "authors": "Hitesh Sagtani, Rishabh Mehrotra, Beyang Liu",
  "authorsParsed": [
    [
      "Sagtani",
      "Hitesh",
      ""
    ],
    [
      "Mehrotra",
      "Rishabh",
      ""
    ],
    [
      "Liu",
      "Beyang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 21 Dec 2024 11:30:54 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734780654000,
  "abstract": "  Fill-in-the-Middle (FIM) models play a vital role in code completion tasks,\nleveraging both prefix and suffix context to provide more accurate and\ncontextually relevant suggestions. This paper presents approaches to improve\nFIM code completion while addressing the challenge of maintaining low latency\nfor real-time coding assistance. We enhance FIM code completion by\nincorporating context and curriculum examples in the training process. We\nidentify patterns where completion suggestions fail more frequently, revealing\ncomplexities that smaller language models struggle with. To address these\nchallenges, we develop a curriculum dataset by extracting hard-to-complete\npatterns from code repositories and generate context examples using semantic\nand static analysis tools (e.g. TSC compiler). We fine-tune various sized\nmodels, including StarCoder and DeepSeek, on this enhanced dataset. Our\nevaluation encompasses three key dimensions: the Santa Coder FIM task, the\nAmazon CCEval benchmark, and a new Multi-Line Infilling evaluation benchmark\nderived from SWE-bench. Comprehensive ablation studies across multiple model\nsizes reveal that while all fine-tuned models show improvements, the\nperformance gains are more pronounced for smaller parameter models and\nincorporating difficult-to-complete examples, as part of curriculum learning,\nimproves the code completion performance. This finding is particularly\nsignificant given the latency constraints of code completion tasks. While\nlarger models like GPT and Claude perform well in multi-line completions but\nare prohibitively challenging to use given high latency, and our fine-tuned\nmodels achieve a balance between performance and latency. Finally, we validate\nour approach through online A/B testing, demonstrating tangible improvements in\nCompletion Acceptance Rate (CAR) and Completion Persistence Rate (CPR), with\nzero latency impact.\n",
  "subjects": [
    "Computer Science/Information Retrieval"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "GYa55WpGkRA-tueau7F_bwucbVg51FoPu3BktnE6A5A",
  "pdfSize": "2148493"
}