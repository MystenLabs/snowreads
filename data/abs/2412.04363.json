{"id":"2412.04363","title":"Challenges in Trustworthy Human Evaluation of Chatbots","authors":"Wenting Zhao, Alexander M. Rush, Tanya Goyal","authorsParsed":[["Zhao","Wenting",""],["Rush","Alexander M.",""],["Goyal","Tanya",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 17:22:04 GMT"}],"updateDate":"2024-12-06","timestamp":1733419324000,"abstract":"  Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations.\n","subjects":["Computer Science/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ibbE4Wp5bSP3b4YBcZeOATEgNk4Hg3q2T3DhhGmEfrY","pdfSize":"426191"}