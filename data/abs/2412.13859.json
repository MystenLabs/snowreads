{
  "id": "2412.13859",
  "title": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models",
  "authors": "Anna Scius-Bertrand, Michael Jungo, Lars V\\\"ogtlin, Jean-Marc Spat and\n  Andreas Fischer",
  "authorsParsed": [
    [
      "Scius-Bertrand",
      "Anna",
      ""
    ],
    [
      "Jungo",
      "Michael",
      ""
    ],
    [
      "VÃ¶gtlin",
      "Lars",
      ""
    ],
    [
      "Spat",
      "Jean-Marc",
      ""
    ],
    [
      "Fischer",
      "Andreas",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 13:53:16 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734529996000,
  "abstract": "  Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "GBKkUGcEc8FewIhI6bp4Il1sQv5Cwmrqi5AtofGTwmA",
  "pdfSize": "1259662"
}