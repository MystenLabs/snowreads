{
  "id": "2412.08175",
  "title": "Analyzing and Mitigating Model Collapse in Rectified Flow Models",
  "authors": "Huminhao Zhu, Fangyikang Wang, Tianyu Ding, Qing Qu, Zhihui Zhu",
  "authorsParsed": [
    [
      "Zhu",
      "Huminhao",
      ""
    ],
    [
      "Wang",
      "Fangyikang",
      ""
    ],
    [
      "Ding",
      "Tianyu",
      ""
    ],
    [
      "Qu",
      "Qing",
      ""
    ],
    [
      "Zhu",
      "Zhihui",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 08:05:35 GMT"
    },
    {
      "version": "v2",
      "created": "Sun, 9 Feb 2025 10:02:55 GMT"
    }
  ],
  "updateDate": "2025-02-11",
  "timestamp": 1733904335000,
  "abstract": "  Training with synthetic data is becoming increasingly inevitable as synthetic\ncontent proliferates across the web, driven by the remarkable performance of\nrecent deep generative models. This reliance on synthetic data can also be\nintentional, as seen in Rectified Flow models, whose Reflow method iteratively\nuses self-generated data to straighten the flow and improve sampling\nefficiency. However, recent studies have shown that repeatedly training on\nself-generated samples can lead to model collapse (MC), where performance\ndegrades over time. Despite this, most recent work on MC either focuses on\nempirical observations or analyzes regression problems and maximum likelihood\nobjectives, leaving a rigorous theoretical analysis of reflow methods\nunexplored. In this paper, we aim to fill this gap by providing both\ntheoretical analysis and practical solutions for addressing MC in\ndiffusion/flow models. We begin by studying Denoising Autoencoders and prove\nperformance degradation when DAEs are iteratively trained on their own outputs.\nTo the best of our knowledge, we are the first to rigorously analyze model\ncollapse in DAEs and, by extension, in diffusion models and Rectified Flow. Our\nanalysis and experiments demonstrate that rectified flow also suffers from MC,\nleading to potential performance degradation in each reflow step. Additionally,\nwe prove that incorporating real data can prevent MC during recursive DAE\ntraining, supporting the recent trend of using real data as an effective\napproach for mitigating MC. Building on these insights, we propose a novel\nReal-data Augmented Reflow and a series of improved variants, which seamlessly\nintegrate real data into Reflow training by leveraging reverse flow. Empirical\nevaluations on standard image benchmarks confirm that RA Reflow effectively\nmitigates model collapse, preserving high-quality sample generation even with\nfewer sampling steps.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "NThHJgIvCfG01zSX2C8w-Wi8Wm3hnJXU-2vkQxKFd5s",
  "pdfSize": "6096093"
}