{"id":"2407.08044","title":"RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective\n  Weight-Activation Quantization","authors":"Xijie Huang, Zechun Liu, Shih-Yang Liu, Kwang-Ting Cheng","authorsParsed":[["Huang","Xijie",""],["Liu","Zechun",""],["Liu","Shih-Yang",""],["Cheng","Kwang-Ting",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 20:52:18 GMT"}],"updateDate":"2024-07-12","timestamp":1720644738000,"abstract":"  Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient\nFine-Tuning (PEFT)method, significantly enhances the training efficiency by\nupdating only a small portion of the weights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques have also been applied to LoRA\nmethods to reduce the memory footprint of fine-tuning. However, applying\nweight-activation quantization to the LoRA pipeline is under-explored, and we\nobserve substantial performance degradation primarily due to the presence of\nactivation outliers. In this work, we propose RoLoRA, the first LoRA-based\nscheme for effective weight-activation quantization. RoLoRA utilizes rotation\nfor outlier elimination and proposes rotation-aware fine-tuning to preserve the\noutlier-free characteristics in rotated LLMs. Experimental results show RoLoRA\nconsistently improves low-bit LoRA convergence and post-training quantization\nrobustness in weight-activation settings. We evaluate RoLoRA across\nLLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks\ncompared to LoRA baseline. We further demonstrate its effectiveness on Large\nMultimodal Models (LLaVA-1.5-7B). Codes are available at\nhttps://github.com/HuangOwen/RoLoRA\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"NkUwt77Ztzdc46xJF4U5uuzqFKMhV8XDumdBj-WXTJc","pdfSize":"1668964","objectId":"0x65156e7de56b77771f6e6c48578de61e373845f20f4504c0718198dfdc0958c1","registeredEpoch":"1","certifiedEpoch":"1","startEpoch":"1","endEpoch":"201"}
