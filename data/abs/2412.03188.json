{
  "id": "2412.03188",
  "title": "Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for\n  Traffic Prediction",
  "authors": "Ivan Kralj and Lodovico Giaretta and Gordan Je\\v{z}i\\'c and Ivana\n  Podnar \\v{Z}arko and \\v{S}ar\\=unas Girdzijauskas",
  "authorsParsed": [
    [
      "Kralj",
      "Ivan",
      ""
    ],
    [
      "Giaretta",
      "Lodovico",
      ""
    ],
    [
      "Ježić",
      "Gordan",
      ""
    ],
    [
      "Žarko",
      "Ivana Podnar",
      ""
    ],
    [
      "Girdzijauskas",
      "Šarūnas",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 10:20:21 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733307621000,
  "abstract": "  In smart mobility, large networks of geographically distributed sensors\nproduce vast amounts of high-frequency spatio-temporal data that must be\nprocessed in real time to avoid major disruptions. Traditional centralized\napproaches are increasingly unsuitable to this task, as they struggle to scale\nwith expanding sensor networks, and reliability issues in central components\ncan easily affect the whole deployment. To address these challenges, we explore\nand adapt semi-decentralized training techniques for Spatio-Temporal Graph\nNeural Networks (ST-GNNs) in smart mobility domain. We implement a simulation\nframework where sensors are grouped by proximity into multiple cloudlets, each\nhandling a subgraph of the traffic graph, fetching node features from other\ncloudlets to train its own local ST-GNN model, and exchanging model updates\nwith other cloudlets to ensure consistency, enhancing scalability and removing\nreliance on a centralized aggregator. We perform extensive comparative\nevaluation of four different ST-GNN training setups -- centralized, traditional\nFL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the\nMETR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed\npredictions. Experimental results show that semi-decentralized setups are\ncomparable to centralized approaches in performance metrics, while offering\nadvantages in terms of scalability and fault tolerance. In addition, we\nhighlight often overlooked issues in existing literature for distributed\nST-GNNs, such as the variation in model performance across different\ngeographical areas due to region-specific traffic patterns, and the significant\ncommunication overhead and computational costs that arise from the large\nreceptive field of GNNs, leading to substantial data transfers and increased\ncomputation of partial embeddings.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Distributed, Parallel, and Cluster Computing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "W5X0CSDdhv3Wt9FAcU0HPH3iDe6ISBgujSwE4vAP32c",
  "pdfSize": "1984209"
}