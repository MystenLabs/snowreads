{"id":"2412.11177","title":"A Progressive Transformer for Unifying Binary Code Embedding and\n  Knowledge Transfer","authors":"Hanxiao Lu, Hongyu Cai, Yiming Liang, Antonio Bianchi, and Z. Berkay\n  Celik","authorsParsed":[["Lu","Hanxiao",""],["Cai","Hongyu",""],["Liang","Yiming",""],["Bianchi","Antonio",""],["Celik","Z. Berkay",""]],"versions":[{"version":"v1","created":"Sun, 15 Dec 2024 13:04:29 GMT"},{"version":"v2","created":"Sun, 22 Dec 2024 07:53:33 GMT"}],"updateDate":"2024-12-24","timestamp":1734267869000,"abstract":"  Language model approaches have recently been integrated into binary analysis\ntasks, such as function similarity detection and function signature recovery.\nThese models typically employ a two-stage training process: pre-training via\nMasked Language Modeling (MLM) on machine code and fine-tuning for specific\ntasks. While MLM helps to understand binary code structures, it ignores\nessential code characteristics, including control and data flow, which\nnegatively affect model generalization. Recent work leverages domain-specific\nfeatures (e.g., control flow graphs and dynamic execution traces) in\ntransformer-based approaches to improve binary code semantic understanding.\nHowever, this approach involves complex feature engineering, a cumbersome and\ntime-consuming process that can introduce predictive uncertainty when dealing\nwith stripped or obfuscated code, leading to a performance drop. In this paper,\nwe introduce ProTST, a novel transformer-based methodology for binary code\nembedding. ProTST employs a hierarchical training process based on a unique\ntree-like structure, where knowledge progressively flows from fundamental tasks\nat the root to more specialized tasks at the leaves. This progressive\nteacher-student paradigm allows the model to build upon previously learned\nknowledge, resulting in high-quality embeddings that can be effectively\nleveraged for diverse downstream binary analysis tasks. The effectiveness of\nProTST is evaluated in seven binary analysis tasks, and the results show that\nProTST yields an average validation score (F1, MRR, and Recall@1) improvement\nof 14.8% compared to traditional two-stage training and an average validation\nscore of 10.7% compared to multimodal two-stage frameworks.\n","subjects":["Computer Science/Software Engineering","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ArIkL7Qn1-EMgDU29jFUFevCxs9T6GjwfLn6tHD-DUs","pdfSize":"4250773"}