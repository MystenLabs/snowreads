{"id":"2407.01437","title":"Needle in the Haystack for Memory Based Large Language Models","authors":"Elliot Nelson, Georgios Kollias, Payel Das, Subhajit Chaudhury, Soham\n  Dan","authorsParsed":[["Nelson","Elliot",""],["Kollias","Georgios",""],["Das","Payel",""],["Chaudhury","Subhajit",""],["Dan","Soham",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 16:32:16 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 17:20:34 GMT"}],"updateDate":"2024-07-15","timestamp":1719851536000,"abstract":"  Current large language models (LLMs) often perform poorly on simple fact\nretrieval tasks. Here we investigate if coupling a dynamically adaptable\nexternal memory to a LLM can alleviate this problem. For this purpose, we test\nLarimar, a recently proposed language model architecture which uses an external\nassociative memory, on long-context recall tasks including passkey and\nneedle-in-the-haystack tests. We demonstrate that the external memory of\nLarimar, which allows fast write and read of an episode of text samples, can be\nused at test time to handle contexts much longer than those seen during\ntraining. We further show that the latent readouts from the memory (to which\nlong contexts are written) control the decoder towards generating correct\noutputs, with the memory stored off of the GPU. Compared to existing\ntransformer-based LLM architectures for long-context recall tasks that use\nlarger parameter counts or modified attention mechanisms, a relatively smaller\nsize Larimar is able to maintain strong performance without any task-specific\ntraining or training on longer contexts.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"tWyJfihlvQ6foQwAa0qIDvKm9O4_1x0-B-autq71Ka4","pdfSize":"274780","objectId":"0x37216538a5d860ce2c96bee3887f415cbc756c14ea3d0144bc021e98fbbe4bd2","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
