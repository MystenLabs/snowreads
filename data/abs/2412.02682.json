{
  "id": "2412.02682",
  "title": "The Asymptotic Behavior of Attention in Transformers",
  "authors": "\\'Alvaro Rodr\\'iguez Abella, Jo\\~ao Pedro Silvestre, Paulo Tabuada",
  "authorsParsed": [
    [
      "Abella",
      "Álvaro Rodríguez",
      ""
    ],
    [
      "Silvestre",
      "João Pedro",
      ""
    ],
    [
      "Tabuada",
      "Paulo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 18:54:49 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733252089000,
  "abstract": "  A key component of transformers is the attention mechanism orchestrating how\neach token influences the propagation of every other token through a\ntransformer. In this paper we provide a rigorous, mathematical analysis of the\nasymptotic properties of attention in transformers. Although we present several\nresults based on different assumptions, all of them point to the same\nconclusion, all tokens asymptotically converge to each other, a phenomenon that\nhas been empirically reported in the literature. Our findings are carefully\ncompared with existing theoretical results and illustrated by simulations and\nexperimental studies using the GPT-2 model.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning",
    "Computer Science/Systems and Control",
    "Electrical Engineering and Systems Science/Systems and Control",
    "Mathematics/Dynamical Systems",
    "Mathematics/Optimization and Control"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "MmcUxFt9XDR6TlCq9nzAipSXO0Vsd-67DqCftTNki1M",
  "pdfSize": "1164604"
}