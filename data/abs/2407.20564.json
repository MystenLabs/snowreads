{"id":"2407.20564","title":"CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large\n  Language Models over Factual Knowledge","authors":"Tianshi Zheng, Jiaxin Bai, Yicheng Wang, Tianqing Fang, Yue Guo,\n  Yauwai Yim, Yangqiu Song","authorsParsed":[["Zheng","Tianshi",""],["Bai","Jiaxin",""],["Wang","Yicheng",""],["Fang","Tianqing",""],["Guo","Yue",""],["Yim","Yauwai",""],["Song","Yangqiu",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 05:40:32 GMT"}],"updateDate":"2024-07-31","timestamp":1722318032000,"abstract":"  While large language models (LLMs) have demonstrated impressive capabilities\nacross various natural language processing tasks by acquiring rich factual\nknowledge from their broad training data, their ability to synthesize and\nlogically reason with this knowledge in complex ways remains underexplored. In\nthis work, we present a systematic evaluation of state-of-the-art LLMs' complex\nlogical reasoning abilities through a novel benchmark of automatically\ngenerated complex reasoning questions over general domain and biomedical\nknowledge graphs. Our extensive experiments, employing diverse in-context\nlearning techniques, reveal that LLMs excel at reasoning over general world\nknowledge but face significant challenges with specialized domain-specific\nknowledge. We find that prompting with explicit Chain-of-Thought demonstrations\ncan substantially improve LLM performance on complex logical reasoning tasks\nwith diverse logical operations. Interestingly, our controlled evaluations\nuncover an asymmetry where LLMs display proficiency at set union operations,\nbut struggle considerably with set intersections - a key building block of\nlogical reasoning. To foster further work, we will publicly release our\nevaluation benchmark and code.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"lXpjstuA-vY0B3mGQjsw1cJZ3XVYftsTaLEb0DXIcZg","pdfSize":"990558"}