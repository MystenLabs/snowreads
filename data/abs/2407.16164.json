{"id":"2407.16164","title":"Representation Magnitude has a Liability to Privacy Vulnerability","authors":"Xingli Fang and Jung-Eun Kim","authorsParsed":[["Fang","Xingli",""],["Kim","Jung-Eun",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 04:13:52 GMT"}],"updateDate":"2024-07-24","timestamp":1721708032000,"abstract":"  The privacy-preserving approaches to machine learning (ML) models have made\nsubstantial progress in recent years. However, it is still opaque in which\ncircumstances and conditions the model becomes privacy-vulnerable, leading to a\nchallenge for ML models to maintain both performance and privacy. In this\npaper, we first explore the disparity between member and non-member data in the\nrepresentation of models under common training frameworks. We identify how the\nrepresentation magnitude disparity correlates with privacy vulnerability and\naddress how this correlation impacts privacy vulnerability. Based on the\nobservations, we propose Saturn Ring Classifier Module (SRCM), a plug-in\nmodel-level solution to mitigate membership privacy leakage. Through a confined\nyet effective representation space, our approach ameliorates models' privacy\nvulnerability while maintaining generalizability. The code of this work can be\nfound here: \\url{https://github.com/JEKimLab/AIES2024_SRCM}\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"FTVUxguDpBWilIZzw4C2KEANT38c5hpYcY0RsM0dcwE","pdfSize":"2130571"}