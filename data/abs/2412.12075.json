{
  "id": "2412.12075",
  "title": "CG-Bench: Clue-grounded Question Answering Benchmark for Long Video\n  Understanding",
  "authors": "Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu,\n  Yali Wang, Tong Lu, Limin Wang",
  "authorsParsed": [
    [
      "Chen",
      "Guo",
      ""
    ],
    [
      "Liu",
      "Yicheng",
      ""
    ],
    [
      "Huang",
      "Yifei",
      ""
    ],
    [
      "He",
      "Yuping",
      ""
    ],
    [
      "Pei",
      "Baoqi",
      ""
    ],
    [
      "Xu",
      "Jilan",
      ""
    ],
    [
      "Wang",
      "Yali",
      ""
    ],
    [
      "Lu",
      "Tong",
      ""
    ],
    [
      "Wang",
      "Limin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 18:46:45 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734374805000,
  "abstract": "  Most existing video understanding benchmarks for multimodal large language\nmodels (MLLMs) focus only on short videos. The limited number of benchmarks for\nlong video understanding often rely solely on multiple-choice questions (MCQs).\nHowever, because of the inherent limitation of MCQ-based evaluation and the\nincreasing reasoning ability of MLLMs, models can give the current answer\npurely by combining short video understanding with elimination, without\ngenuinely understanding the video content. To address this gap, we introduce\nCG-Bench, a novel benchmark designed for clue-grounded question answering in\nlong videos. CG-Bench emphasizes the model's ability to retrieve relevant clues\nfor questions, enhancing evaluation credibility. It features 1,219 manually\ncurated videos categorized by a granular system with 14 primary categories, 171\nsecondary categories, and 638 tertiary categories, making it the largest\nbenchmark for long video analysis. The benchmark includes 12,129 QA pairs in\nthree major question types: perception, reasoning, and hallucination.\nCompensating the drawbacks of pure MCQ-based evaluation, we design two novel\nclue-based evaluation methods: clue-grounded white box and black box\nevaluations, to assess whether the model generates answers based on the correct\nunderstanding of the video. We evaluate multiple closed-source and open-source\nMLLMs on CG-Bench. Results indicate that current models significantly\nunderperform in understanding long videos compared to short ones, and a\nsignificant gap exists between open-source and commercial models. We hope\nCG-Bench can advance the development of more trustworthy and capable MLLMs for\nlong video understanding. All annotations and video data are released at\nhttps://cg-bench.github.io/leaderboard/.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "cv_T5Hu9xBh5r342_3MxZpOqWhTGw7S41qSN19-RF4o",
  "pdfSize": "1456583"
}