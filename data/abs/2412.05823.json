{
  "id": "2412.05823",
  "title": "DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning\n  for Edge Devices",
  "authors": "Yongzhe Jia, Xuyun Zhang, Hongsheng Hu, Kim-Kwang Raymond Choo,\n  Lianyong Qi, Xiaolong Xu, Amin Beheshti, Wanchun Dou",
  "authorsParsed": [
    [
      "Jia",
      "Yongzhe",
      ""
    ],
    [
      "Zhang",
      "Xuyun",
      ""
    ],
    [
      "Hu",
      "Hongsheng",
      ""
    ],
    [
      "Choo",
      "Kim-Kwang Raymond",
      ""
    ],
    [
      "Qi",
      "Lianyong",
      ""
    ],
    [
      "Xu",
      "Xiaolong",
      ""
    ],
    [
      "Beheshti",
      "Amin",
      ""
    ],
    [
      "Dou",
      "Wanchun",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 8 Dec 2024 05:50:04 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733637004000,
  "abstract": "  Federated learning (FL) has emerged as a prominent machine learning paradigm\nin edge computing environments, enabling edge devices to collaboratively\noptimize a global model without sharing their private data. However, existing\nFL frameworks suffer from efficacy deterioration due to the system\nheterogeneity inherent in edge computing, especially in the presence of domain\nshifts across local data. In this paper, we propose a heterogeneous FL\nframework DapperFL, to enhance model performance across multiple domains. In\nDapperFL, we introduce a dedicated Model Fusion Pruning (MFP) module to produce\npersonalized compact local models for clients to address the system\nheterogeneity challenges. The MFP module prunes local models with fused\nknowledge obtained from both local and remaining domains, ensuring robustness\nto domain shifts. Additionally, we design a Domain Adaptive Regularization\n(DAR) module to further improve the overall performance of DapperFL. The DAR\nmodule employs regularization generated by the pruned model, aiming to learn\nrobust representations across domains. Furthermore, we introduce a specific\naggregation algorithm for aggregating heterogeneous local models with tailored\narchitectures and weights. We implement DapperFL on a realworld FL platform\nwith heterogeneous clients. Experimental results on benchmark datasets with\nmultiple domains demonstrate that DapperFL outperforms several state-of-the-art\nFL frameworks by up to 2.28%, while significantly achieving model volume\nreductions ranging from 20% to 80%. Our code is available at:\nhttps://github.com/jyzgh/DapperFL.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "z2KEmzJ_BFN5y39fM9UI-Plu7PfLGgDYR8yzUJs7a_A",
  "pdfSize": "1096242"
}