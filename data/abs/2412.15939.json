{"id":"2412.15939","title":"Reframing Image Difference Captioning with BLIP2IDC and Synthetic\n  Augmentation","authors":"Gautier Evennou, Antoine Chaffin, Vivien Chappelier and Ewa Kijak","authorsParsed":[["Evennou","Gautier",""],["Chaffin","Antoine",""],["Chappelier","Vivien",""],["Kijak","Ewa",""]],"versions":[{"version":"v1","created":"Fri, 20 Dec 2024 14:32:56 GMT"}],"updateDate":"2024-12-23","timestamp":1734705176000,"abstract":"  The rise of the generative models quality during the past years enabled the\ngeneration of edited variations of images at an important scale. To counter the\nharmful effects of such technology, the Image Difference Captioning (IDC) task\naims to describe the differences between two images. While this task is\nsuccessfully handled for simple 3D rendered images, it struggles on real-world\nimages. The reason is twofold: the training data-scarcity, and the difficulty\nto capture fine-grained differences between complex images. To address those\nissues, we propose in this paper a simple yet effective framework to both adapt\nexisting image captioning models to the IDC task and augment IDC datasets. We\nintroduce BLIP2IDC, an adaptation of BLIP2 to the IDC task at low computational\ncost, and show it outperforms two-streams approaches by a significant margin on\nreal-world IDC datasets. We also propose to use synthetic augmentation to\nimprove the performance of IDC models in an agnostic fashion. We show that our\nsynthetic augmentation strategy provides high quality data, leading to a\nchallenging new dataset well-suited for IDC named Syned1.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"xIlWJHfT5l_Vo35sdK8Ewu410OlwpmepbOuE1OhMV88","pdfSize":"6531545"}