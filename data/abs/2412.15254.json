{
  "id": "2412.15254",
  "title": "RIRO: Reshaping Inputs, Refining Outputs Unlocking the Potential of\n  Large Language Models in Data-Scarce Contexts",
  "authors": "Ali Hamdi, Hozaifa Kassab, Mohamed Bahaa, and Marwa Mohamed",
  "authorsParsed": [
    [
      "Hamdi",
      "Ali",
      ""
    ],
    [
      "Kassab",
      "Hozaifa",
      ""
    ],
    [
      "Bahaa",
      "Mohamed",
      ""
    ],
    [
      "Mohamed",
      "Marwa",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 15 Dec 2024 15:48:37 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734277717000,
  "abstract": "  Large language models (LLMs) have significantly advanced natural language\nprocessing, excelling in areas like text generation, summarization, and\nquestion-answering. Despite their capabilities, these models face challenges\nwhen fine-tuned on small, domain-specific datasets, often struggling to\ngeneralize and deliver accurate results with unfamiliar inputs. To tackle this\nissue, we introduce RIRO, a novel two-layer architecture designed to improve\nperformance in data-scarce environments. The first layer leverages advanced\nprompt engineering to reformulate inputs, ensuring better alignment with\ntraining data, while the second layer focuses on refining outputs to minimize\ninconsistencies. Through fine-tuning models like Phi-2, Falcon 7B, and Falcon\n1B, with Phi-2 outperforming the others. Additionally, we introduce a benchmark\nusing evaluation metrics such as cosine similarity, Levenshtein distance, BLEU\nscore, ROUGE-1, ROUGE-2, and ROUGE-L. While these advancements improve\nperformance, challenges like computational demands and overfitting persist,\nlimiting the potential of LLMs in data-scarce, high-stakes environments such as\nhealthcare, legal documentation, and software testing.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "rjuDVRYufBkH-iLD0-ujC5PPSxJVFEnyr_evrHyIF6k",
  "pdfSize": "629006"
}