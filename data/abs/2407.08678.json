{"id":"2407.08678","title":"How to beat a Bayesian adversary","authors":"Zihan Ding, Kexin Jin, Jonas Latz, Chenguang Liu","authorsParsed":[["Ding","Zihan",""],["Jin","Kexin",""],["Latz","Jonas",""],["Liu","Chenguang",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:12:42 GMT"}],"updateDate":"2024-07-12","timestamp":1720717962000,"abstract":"  Deep neural networks and other modern machine learning models are often\nsusceptible to adversarial attacks. Indeed, an adversary may often be able to\nchange a model's prediction through a small, directed perturbation of the\nmodel's input - an issue in safety-critical applications. Adversarially robust\nmachine learning is usually based on a minmax optimisation problem that\nminimises the machine learning loss under maximisation-based adversarial\nattacks.\n  In this work, we study adversaries that determine their attack using a\nBayesian statistical approach rather than maximisation. The resulting Bayesian\nadversarial robustness problem is a relaxation of the usual minmax problem. To\nsolve this problem, we propose Abram - a continuous-time particle system that\nshall approximate the gradient flow corresponding to the underlying learning\nproblem. We show that Abram approximates a McKean-Vlasov process and justify\nthe use of Abram by giving assumptions under which the McKean-Vlasov process\nfinds the minimiser of the Bayesian adversarial robustness problem. We discuss\ntwo ways to discretise Abram and show its suitability in benchmark adversarial\ndeep learning experiments.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control","Statistics/Computation","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"OXNn8jug18EqeCbNFq_VoSeM0FKJkMug0GpJe0pkv6c","pdfSize":"1125257"}