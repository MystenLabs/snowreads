{"id":"2412.05275","title":"MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models","authors":"Tuna Han Salih Meral, Hidir Yesiltepe, Connor Dunlop, Pinar Yanardag","authorsParsed":[["Meral","Tuna Han Salih",""],["Yesiltepe","Hidir",""],["Dunlop","Connor",""],["Yanardag","Pinar",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 18:59:12 GMT"}],"updateDate":"2024-12-09","timestamp":1733511552000,"abstract":"  Text-to-video models have demonstrated impressive capabilities in producing\ndiverse and captivating video content, showcasing a notable advancement in\ngenerative AI. However, these models generally lack fine-grained control over\nmotion patterns, limiting their practical applicability. We introduce\nMotionFlow, a novel framework designed for motion transfer in video diffusion\nmodels. Our method utilizes cross-attention maps to accurately capture and\nmanipulate spatial and temporal dynamics, enabling seamless motion transfers\nacross various contexts. Our approach does not require training and works on\ntest-time by leveraging the inherent capabilities of pre-trained video\ndiffusion models. In contrast to traditional approaches, which struggle with\ncomprehensive scene changes while maintaining consistent motion, MotionFlow\nsuccessfully handles such complex transformations through its attention-based\nmechanism. Our qualitative and quantitative experiments demonstrate that\nMotionFlow significantly outperforms existing models in both fidelity and\nversatility even during drastic scene alterations.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RyllXeThethd4geU__rNVkk03S5NjRaHf19Z13mwOyU","pdfSize":"11258097"}