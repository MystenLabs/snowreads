{
  "id": "2412.11965",
  "title": "Inferring Functionality of Attention Heads from their Parameters",
  "authors": "Amit Elhelo, Mor Geva",
  "authorsParsed": [
    [
      "Elhelo",
      "Amit",
      ""
    ],
    [
      "Geva",
      "Mor",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 16:45:33 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734367533000,
  "abstract": "  Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "E55hrVB80MpvO0ur13mL35um5y8sVUVAYGKuKTkWQ4Q",
  "pdfSize": "2000299"
}