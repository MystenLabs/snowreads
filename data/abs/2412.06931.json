{"id":"2412.06931","title":"Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based\n  Planning and Manoeuvrability-Driven Controls","authors":"Hoi-Yin Lee, Peng Zhou, Anqing Duan, Wanyu Ma, Chenguang Yang, David\n  Navarro-Alarcon","authorsParsed":[["Lee","Hoi-Yin",""],["Zhou","Peng",""],["Duan","Anqing",""],["Ma","Wanyu",""],["Yang","Chenguang",""],["Navarro-Alarcon","David",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 19:21:05 GMT"},{"version":"v2","created":"Thu, 13 Feb 2025 09:03:57 GMT"}],"updateDate":"2025-02-14","timestamp":1733772065000,"abstract":"  The ability to wield tools was once considered exclusive to human\nintelligence, but it's now known that many other animals, like crows, possess\nthis capability. Yet, robotic systems still fall short of matching biological\ndexterity. In this paper, we investigate the use of Large Language Models\n(LLMs), tool affordances, and object manoeuvrability for non-prehensile\ntool-based manipulation tasks. Our novel method leverages LLMs based on scene\ninformation and natural language instructions to enable symbolic task planning\nfor tool-object manipulation. This approach allows the system to convert the\nhuman language sentence into a sequence of feasible motion functions. We have\ndeveloped a novel manoeuvrability-driven controller using a new tool affordance\nmodel derived from visual feedback. This controller helps guide the robot's\ntool utilization and manipulation actions, even within confined areas, using a\nstepping incremental approach. The proposed methodology is evaluated with\nexperiments to prove its effectiveness under various manipulation scenarios.\n","subjects":["Computer Science/Robotics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"17CoBLiY7CmB6BP55cvQJKr0qVsJfOfJltDXiz1rrMc","pdfSize":"20745613"}