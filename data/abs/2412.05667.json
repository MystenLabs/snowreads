{
  "id": "2412.05667",
  "title": "Training neural networks without backpropagation using particles",
  "authors": "Deepak Kumar",
  "authorsParsed": [
    [
      "Kumar",
      "Deepak",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 7 Dec 2024 14:30:48 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 18 Dec 2024 12:57:22 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1733581848000,
  "abstract": "  Neural networks are a group of neurons stacked together in multiple layers to\nmimic the biological neurons in a human brain. Neural networks have been\ntrained using the backpropagation algorithm based on gradient descent strategy\nfor several decades. Several variants have been developed to improve the\nbackpropagation algorithm. The loss function for the neural network is\noptimized through backpropagation, but several local minima exist in the\nmanifold of the constructed neural network. We obtain several solutions\nmatching the minima. The gradient descent strategy cannot avoid the problem of\nlocal minima and gets stuck in the minima due to the initialization. Particle\nswarm optimization (PSO) was proposed to select the best local minima among the\nsearch space of the loss function. The search space is limited to the\ninstantiated particles in the PSO algorithm, and sometimes it cannot select the\nbest solution. In the proposed approach, we overcome the problem of gradient\ndescent and the limitation of the PSO algorithm by training individual neurons\nseparately, capable of collectively solving the problem as a group of neurons\nforming a network. Our code and data are available at\nhttps://github.com/dipkmr/train-nn-wobp/\n",
  "subjects": [
    "Computer Science/Neural and Evolutionary Computing",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "M0btn6BdoIpr7DcYAAZzqFmqhQL7BFOMBJGn2fP4ho0",
  "pdfSize": "1039310"
}