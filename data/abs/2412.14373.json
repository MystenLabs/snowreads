{"id":"2412.14373","title":"ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram\n  Language Modeling","authors":"William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding\n  Zhao","authorsParsed":[["Han","William",""],["Duan","Chaojing",""],["Rosenberg","Michael A.",""],["Liu","Emerson",""],["Zhao","Ding",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 22:13:21 GMT"}],"updateDate":"2024-12-20","timestamp":1734560001000,"abstract":"  Large Language Models (LLMs) have shown remarkable adaptability across\ndomains beyond text, specifically electrocardiograms (ECGs). More specifically,\nthere is a growing body of work exploring the task of generating text from a\nmulti-channeled ECG and corresponding textual prompt. Current approaches\ntypically involve pretraining an ECG-specific encoder with a self-supervised\nlearning (SSL) objective and using the features output by the pretrained\nencoder to finetune a LLM for natural language generation (NLG). However, these\nmethods are limited by 1) inefficiency from two-stage training and 2)\ninterpretability challenges with encoder-generated features. To address these\nlimitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE)\ntokenizer pipeline for autoregressive language modeling of ECGs. This approach\ncompresses and encodes ECG signals into tokens, enabling end-to-end LLM\ntraining by combining ECG and text tokens directly, while being much more\ninterpretable since the ECG tokens can be directly mapped back to the original\nsignal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only\nhalf the time and ~48% of the data required by two-stage approaches.\n","subjects":["Computer Science/Computation and Language","Electrical Engineering and Systems Science/Signal Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XG1l4DaWKLpSJpAamdV4PdXePCAIR4vZTLcei_V9Yvs","pdfSize":"8540640"}