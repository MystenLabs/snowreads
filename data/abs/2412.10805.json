{
  "id": "2412.10805",
  "title": "Are Language Models Agnostic to Linguistically Grounded Perturbations? A\n  Case Study of Indic Languages",
  "authors": "Poulami Ghosh, Raj Dabre, Pushpak Bhattacharyya",
  "authorsParsed": [
    [
      "Ghosh",
      "Poulami",
      ""
    ],
    [
      "Dabre",
      "Raj",
      ""
    ],
    [
      "Bhattacharyya",
      "Pushpak",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 14 Dec 2024 12:10:38 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734178238000,
  "abstract": "  Pre-trained language models (PLMs) are known to be susceptible to\nperturbations to the input text, but existing works do not explicitly focus on\nlinguistically grounded attacks, which are subtle and more prevalent in nature.\nIn this paper, we study whether PLMs are agnostic to linguistically grounded\nattacks or not. To this end, we offer the first study addressing this,\ninvestigating different Indic languages and various downstream tasks. Our\nfindings reveal that although PLMs are susceptible to linguistic perturbations,\nwhen compared to non-linguistic attacks, PLMs exhibit a slightly lower\nsusceptibility to linguistic attacks. This highlights that even constrained\nattacks are effective. Moreover, we investigate the implications of these\noutcomes across a range of languages, encompassing diverse language families\nand different scripts.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "BeZedqI7-TxcNx3mPyJ0Pu6lZtb4xzqgK3YOvKqI2uA",
  "pdfSize": "798386"
}