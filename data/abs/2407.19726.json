{"id":"2407.19726","title":"Do Text-to-Vis Benchmarks Test Real Use of Visualisations?","authors":"Hy Nguyen, Xuefei He, Andrew Reeson, Cecile Paris, Josiah Poon,\n  Jonathan K. Kummerfeld","authorsParsed":[["Nguyen","Hy",""],["He","Xuefei",""],["Reeson","Andrew",""],["Paris","Cecile",""],["Poon","Josiah",""],["Kummerfeld","Jonathan K.",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 06:13:28 GMT"},{"version":"v2","created":"Fri, 9 Aug 2024 00:54:02 GMT"},{"version":"v3","created":"Thu, 15 Aug 2024 07:26:26 GMT"}],"updateDate":"2024-08-16","timestamp":1722233608000,"abstract":"  Large language models are able to generate code for visualisations in\nresponse to user requests. This is a useful application, and an appealing one\nfor NLP research because plots of data provide grounding for language. However,\nthere are relatively few benchmarks, and it is unknown whether those that exist\nare representative of what people do in practice. This paper aims to answer\nthat question through an empirical study comparing benchmark datasets and code\nfrom public repositories. Our findings reveal a substantial gap in datasets,\nwith evaluations not testing the same distribution of chart types, attributes,\nand the number of actions. The only representative dataset requires\nmodification to become an end-to-end and practical benchmark. This shows that\nnew, more benchmarks are needed to support the development of systems that\ntruly address users' visualisation needs. These observations will guide future\ndata creation, highlighting which features hold genuine significance for users.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"bF9bGS6JL3fBOfjQZWmJkLa-jErS1M-8iMjOaNQ3iGM","pdfSize":"1361133"}