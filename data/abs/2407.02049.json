{"id":"2407.02049","title":"Accompanied Singing Voice Synthesis with Fully Text-controlled Melody","authors":"Ruiqi Li, Zhiqing Hong, Yongqi Wang, Lichao Zhang, Rongjie Huang, Siqi\n  Zheng, Zhou Zhao","authorsParsed":[["Li","Ruiqi",""],["Hong","Zhiqing",""],["Wang","Yongqi",""],["Zhang","Lichao",""],["Huang","Rongjie",""],["Zheng","Siqi",""],["Zhao","Zhou",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 08:23:38 GMT"}],"updateDate":"2024-07-03","timestamp":1719908618000,"abstract":"  Text-to-song (TTSong) is a music generation task that synthesizes accompanied\nsinging voices. Current TTSong methods, inherited from singing voice synthesis\n(SVS), require melody-related information that can sometimes be impractical,\nsuch as music scores or MIDI sequences. We present MelodyLM, the first TTSong\nmodel that generates high-quality song pieces with fully text-controlled\nmelodies, achieving minimal user requirements and maximum control flexibility.\nMelodyLM explicitly models MIDI as the intermediate melody-related feature and\nsequentially generates vocal tracks in a language model manner, conditioned on\ntextual and vocal prompts. The accompaniment music is subsequently synthesized\nby a latent diffusion model with hybrid conditioning for temporal alignment.\nWith minimal requirements, users only need to input lyrics and a reference\nvoice to synthesize a song sample. For full control, just input textual prompts\nor even directly input MIDI. Experimental results indicate that MelodyLM\nachieves superior performance in terms of both objective and subjective\nmetrics. Audio samples are available at https://melodylm666.github.io.\n","subjects":["Electrical Engineering and Systems Science/Audio and Speech Processing","Computing Research Repository/Computation and Language","Computing Research Repository/Sound"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ijmj-zEkE8NH5G7IRv7SAz30CKl1OLoJE-lQMTCHlsw","pdfSize":"706535"}