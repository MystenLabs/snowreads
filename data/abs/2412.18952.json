{
  "id": "2412.18952",
  "title": "Bridging Interpretability and Robustness Using LIME-Guided Model\n  Refinement",
  "authors": "Navid Nayyem, Abdullah Rakin, Longwei Wang",
  "authorsParsed": [
    [
      "Nayyem",
      "Navid",
      ""
    ],
    [
      "Rakin",
      "Abdullah",
      ""
    ],
    [
      "Wang",
      "Longwei",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 25 Dec 2024 17:32:45 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735147965000,
  "abstract": "  This paper explores the intricate relationship between interpretability and\nrobustness in deep learning models. Despite their remarkable performance across\nvarious tasks, deep learning models often exhibit critical vulnerabilities,\nincluding susceptibility to adversarial attacks, over-reliance on spurious\ncorrelations, and a lack of transparency in their decision-making processes. To\naddress these limitations, we propose a novel framework that leverages Local\nInterpretable Model-Agnostic Explanations (LIME) to systematically enhance\nmodel robustness. By identifying and mitigating the influence of irrelevant or\nmisleading features, our approach iteratively refines the model, penalizing\nreliance on these features during training. Empirical evaluations on multiple\nbenchmark datasets demonstrate that LIME-guided refinement not only improves\ninterpretability but also significantly enhances resistance to adversarial\nperturbations and generalization to out-of-distribution data.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "YCMtVLz6o99zFBRrNs_hiSGpE2u_VwXHJAU8OuQG-Is",
  "pdfSize": "679852"
}