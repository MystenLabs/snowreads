{
  "id": "2412.07210",
  "title": "EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large\n  Language Models",
  "authors": "Jialiang Cheng, Ning Gao, Yun Yue, Zhiling Ye, Jiadi Jiang, Jian Sha",
  "authorsParsed": [
    [
      "Cheng",
      "Jialiang",
      ""
    ],
    [
      "Gao",
      "Ning",
      ""
    ],
    [
      "Yue",
      "Yun",
      ""
    ],
    [
      "Ye",
      "Zhiling",
      ""
    ],
    [
      "Jiang",
      "Jiadi",
      ""
    ],
    [
      "Sha",
      "Jian",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 06:08:24 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 17 Feb 2025 02:57:12 GMT"
    }
  ],
  "updateDate": "2025-02-18",
  "timestamp": 1733810904000,
  "abstract": "  Distributed training methods are crucial for large language models (LLMs).\nHowever, existing distributed training methods often suffer from communication\nbottlenecks, stragglers, and limited elasticity, particularly in heterogeneous\nor large-scale environments. Local SGD methods have been proposed to address\nthese issues, but their effectiveness remains limited to small-scale training\ndue to additional memory overhead and lack of concerns on efficiency and\nstability. To tackle these issues, we propose EDiT, an innovative Efficient\nDistributed Training method that combines a tailored Local SGD approach with\nmodel sharding techniques to enhance large-scale training efficiency. EDiT\nperforms layer-wise parameter synchronization during forward pass, reducing\ncommunication and memory overhead and enabling overlap. Besides, EDiT employs a\npseudo gradient penalty strategy to suppress loss spikes, which ensures\ntraining stability and improves performance. Additionally, we introduce A-EDiT,\na fully asynchronous variant of EDiT that accommodates heterogeneous clusters.\nBuilding on EDiT/A-EDiT, we conduct a series of experiments to validate\nlarge-scale asynchronous training for LLMs, accompanied by comprehensive\nanalyses. Experimental results demonstrate the superior performance of\nEDiT/A-EDiT, establishing them as robust solutions for distributed LLM training\nin diverse computational ecosystems. The code is available at Atorch codebase:\nhttps://github.com/intelligent-machine-learning/atorch/tree/main/atorch/local_sgd.\n",
  "subjects": [
    "Computer Science/Distributed, Parallel, and Cluster Computing",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "goTZg4TmJ9tYbo5aEGhvFqDYxO26U-4jfy7GHDxshb4",
  "pdfSize": "2366803"
}