{"id":"2412.20496","title":"Random Matrix Theory for Stochastic Gradient Descent","authors":"Chanju Park, Matteo Favoni, Biagio Lucini, Gert Aarts","authorsParsed":[["Park","Chanju",""],["Favoni","Matteo",""],["Lucini","Biagio",""],["Aarts","Gert",""]],"versions":[{"version":"v1","created":"Sun, 29 Dec 2024 15:21:13 GMT"}],"updateDate":"2024-12-31","timestamp":1735485673000,"abstract":"  Investigating the dynamics of learning in machine learning algorithms is of\nparamount importance for understanding how and why an approach may be\nsuccessful. The tools of physics and statistics provide a robust setting for\nsuch investigations. Here we apply concepts from random matrix theory to\ndescribe stochastic weight matrix dynamics, using the framework of Dyson\nBrownian motion. We derive the linear scaling rule between the learning rate\n(step size) and the batch size, and identify universal and non-universal\naspects of weight matrix dynamics. We test our findings in the (near-)solvable\ncase of the Gaussian Restricted Boltzmann Machine and in a linear\none-hidden-layer neural network.\n","subjects":["Physics/High Energy Physics - Lattice","Condensed Matter/Disordered Systems and Neural Networks","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7_NEwFYQSMlhVUlQY_586M6LAbxrUIrR4sJtLtuMDgc","pdfSize":"995094"}