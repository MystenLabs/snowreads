{"id":"2407.10688","title":"Probability Passing for Graph Neural Networks: Graph Structure and\n  Representations Joint Learning","authors":"Ziyan Wang, Yaxuan He, Bin Liu","authorsParsed":[["Wang","Ziyan",""],["He","Yaxuan",""],["Liu","Bin",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 13:01:47 GMT"},{"version":"v2","created":"Wed, 18 Sep 2024 07:27:51 GMT"}],"updateDate":"2024-09-19","timestamp":1721048507000,"abstract":"  Graph Neural Networks (GNNs) have achieved notable success in the analysis of\nnon-Euclidean data across a wide range of domains. However, their applicability\nis constrained by the dependence on the observed graph structure. To solve this\nproblem, Latent Graph Inference (LGI) is proposed to infer a task-specific\nlatent structure by computing similarity or edge probability of node features\nand then apply a GNN to produce predictions. Even so, existing approaches\nneglect the noise from node features, which affects generated graph structure\nand performance. In this work, we introduce a novel method called Probability\nPassing to refine the generated graph structure by aggregating edge\nprobabilities of neighboring nodes based on observed graph. Furthermore, we\ncontinue to utilize the LGI framework, inputting the refined graph structure\nand node features into GNNs to obtain predictions. We name the proposed scheme\nas Probability Passing-based Graph Neural Network (PPGNN). Moreover, the\nanchor-based technique is employed to reduce complexity and improve efficiency.\nExperimental results demonstrate the effectiveness of the proposed method.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"e-0B76x6XQvFHr6C9kOjdVBm-8W7IxVp-XxRowhenRw","pdfSize":"736613","objectId":"0x17a4520cea52be08843cbdaa4d8b66f83a5defa77ceac8792595519619db2fb1","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
