{"id":"2412.07165","title":"A Method for Evaluating Hyperparameter Sensitivity in Reinforcement\n  Learning","authors":"Jacob Adkins, Michael Bowling, Adam White","authorsParsed":[["Adkins","Jacob",""],["Bowling","Michael",""],["White","Adam",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 03:55:18 GMT"},{"version":"v2","created":"Tue, 4 Feb 2025 05:17:26 GMT"}],"updateDate":"2025-02-05","timestamp":1733802918000,"abstract":"  The performance of modern reinforcement learning algorithms critically relies\non tuning ever-increasing numbers of hyperparameters. Often, small changes in a\nhyperparameter can lead to drastic changes in performance, and different\nenvironments require very different hyperparameter settings to achieve\nstate-of-the-art performance reported in the literature. We currently lack a\nscalable and widely accepted approach to characterizing these complex\ninteractions. This work proposes a new empirical methodology for studying,\ncomparing, and quantifying the sensitivity of an algorithm's performance to\nhyperparameter tuning for a given set of environments. We then demonstrate the\nutility of this methodology by assessing the hyperparameter sensitivity of\nseveral commonly used normalization variants of PPO. The results suggest that\nseveral algorithmic performance improvements may, in fact, be a result of an\nincreased reliance on hyperparameter tuning.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"B45hMCHijkaaAnhuthbI5CdTy-fB2XnGTU6TEirtXGM","pdfSize":"1052518"}