{"id":"2407.06795","title":"CycleSAM: One-Shot Surgical Scene Segmentation using Cycle-Consistent\n  Feature Matching to Prompt SAM","authors":"Aditya Murali, Pietro Mascagni, Didier Mutter, Nicolas Padoy","authorsParsed":[["Murali","Aditya",""],["Mascagni","Pietro",""],["Mutter","Didier",""],["Padoy","Nicolas",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 12:08:07 GMT"}],"updateDate":"2024-07-10","timestamp":1720526887000,"abstract":"  The recently introduced Segment-Anything Model (SAM) has the potential to\ngreatly accelerate the development of segmentation models. However, directly\napplying SAM to surgical images has key limitations including (1) the\nrequirement of image-specific prompts at test-time, thereby preventing fully\nautomated segmentation, and (2) ineffectiveness due to substantial domain gap\nbetween natural and surgical images. In this work, we propose CycleSAM, an\napproach for one-shot surgical scene segmentation that uses the training\nimage-mask pair at test-time to automatically identify points in the test\nimages that correspond to each object class, which can then be used to prompt\nSAM to produce object masks. To produce high-fidelity matches, we introduce a\nnovel spatial cycle-consistency constraint that enforces point proposals in the\ntest image to rematch to points within the object foreground region in the\ntraining image. Then, to address the domain gap, rather than directly using the\nvisual features from SAM, we employ a ResNet50 encoder pretrained on surgical\nimages in a self-supervised fashion, thereby maintaining high label-efficiency.\nWe evaluate CycleSAM for one-shot segmentation on two diverse surgical semantic\nsegmentation datasets, comprehensively outperforming baseline approaches and\nreaching up to 50% of fully-supervised performance.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LQhYxM4qNr26XhWolsCoUtXkxn7fhXR5JZtYtuYVbYY","pdfSize":"732809"}