{
  "id": "2412.18981",
  "title": "HAND: Hierarchical Attention Network for Multi-Scale Handwritten\n  Document Recognition and Layout Analysis",
  "authors": "Mohammed Hamdan, Abderrahmane Rahiche, Mohamed Cheriet",
  "authorsParsed": [
    [
      "Hamdan",
      "Mohammed",
      ""
    ],
    [
      "Rahiche",
      "Abderrahmane",
      ""
    ],
    [
      "Cheriet",
      "Mohamed",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 25 Dec 2024 20:36:29 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735158989000,
  "abstract": "  Handwritten document recognition (HDR) is one of the most challenging tasks\nin the field of computer vision, due to the various writing styles and complex\nlayouts inherent in handwritten texts. Traditionally, this problem has been\napproached as two separate tasks, handwritten text recognition and layout\nanalysis, and struggled to integrate the two processes effectively. This paper\nintroduces HAND (Hierarchical Attention Network for Multi-Scale Document), a\nnovel end-to-end and segmentation-free architecture for simultaneous text\nrecognition and layout analysis tasks. Our model's key components include an\nadvanced convolutional encoder integrating Gated Depth-wise Separable and\nOctave Convolutions for robust feature extraction, a Multi-Scale Adaptive\nProcessing (MSAP) framework that dynamically adjusts to document complexity and\na hierarchical attention decoder with memory-augmented and sparse attention\nmechanisms. These components enable our model to scale effectively from\nsingle-line to triple-column pages while maintaining computational efficiency.\nAdditionally, HAND adopts curriculum learning across five complexity levels. To\nimprove the recognition accuracy of complex ancient manuscripts, we fine-tune\nand integrate a Domain-Adaptive Pre-trained mT5 model for post-processing\nrefinement. Extensive evaluations on the READ 2016 dataset demonstrate the\nsuperior performance of HAND, achieving up to 59.8% reduction in CER for\nline-level recognition and 31.2% for page-level recognition compared to\nstate-of-the-art methods. The model also maintains a compact size of 5.60M\nparameters while establishing new benchmarks in both text recognition and\nlayout analysis. Source code and pre-trained models are available at :\nhttps://github.com/MHHamdan/HAND.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "PHdSqmv4DH1eKagjWzA7-QzKAki6SKCHfwOw_nfb3q8",
  "pdfSize": "2356685"
}