{
  "id": "2412.17561",
  "title": "S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit\n  Neural Field",
  "authors": "Zixi Liang, Guowei Xu, Haifeng Wu, Ye Huang, Wen Li, Lixin Duan",
  "authorsParsed": [
    [
      "Liang",
      "Zixi",
      ""
    ],
    [
      "Xu",
      "Guowei",
      ""
    ],
    [
      "Wu",
      "Haifeng",
      ""
    ],
    [
      "Huang",
      "Ye",
      ""
    ],
    [
      "Li",
      "Wen",
      ""
    ],
    [
      "Duan",
      "Lixin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 13:29:35 GMT"
    },
    {
      "version": "v2",
      "created": "Sat, 4 Jan 2025 08:42:33 GMT"
    }
  ],
  "updateDate": "2025-01-07",
  "timestamp": 1734960575000,
  "abstract": "  Learning-based methods have become increasingly popular in 3D indoor scene\nsynthesis (ISS), showing superior performance over traditional\noptimization-based approaches. These learning-based methods typically model\ndistributions on simple yet explicit scene representations using generative\nmodels. However, due to the oversimplified explicit representations that\noverlook detailed information and the lack of guidance from multimodal\nrelationships within the scene, most learning-based methods struggle to\ngenerate indoor scenes with realistic object arrangements and styles. In this\npaper, we introduce a new method, Scene Implicit Neural Field (S-INF), for\nindoor scene synthesis, aiming to learn meaningful representations of\nmultimodal relationships, to enhance the realism of indoor scene synthesis.\nS-INF assumes that the scene layout is often related to the object-detailed\ninformation. It disentangles the multimodal relationships into scene layout\nrelationships and detailed object relationships, fusing them later through\nimplicit neural fields (INFs). By learning specialized scene layout\nrelationships and projecting them into S-INF, we achieve a realistic generation\nof scene layout. Additionally, S-INF captures dense and detailed object\nrelationships through differentiable rendering, ensuring stylistic consistency\nacross objects. Through extensive experiments on the benchmark 3D-FRONT\ndataset, we demonstrate that our method consistently achieves state-of-the-art\nperformance under different types of ISS.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "u-DoWZTsUr9jiVdPTINmiSxQGNdYIkKsavxLIZbdE_c",
  "pdfSize": "2605078"
}