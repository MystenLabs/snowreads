{"id":"2407.12957","title":"R+X: Retrieval and Execution from Everyday Human Videos","authors":"Georgios Papagiannis, Norman Di Palo, Pietro Vitiello, Edward Johns","authorsParsed":[["Papagiannis","Georgios",""],["Di Palo","Norman",""],["Vitiello","Pietro",""],["Johns","Edward",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 18:59:56 GMT"}],"updateDate":"2024-07-19","timestamp":1721242796000,"abstract":"  We present R+X, a framework which enables robots to learn skills from long,\nunlabelled, first-person videos of humans performing everyday tasks. Given a\nlanguage command from a human, R+X first retrieves short video clips containing\nrelevant behaviour, and then executes the skill by conditioning an in-context\nimitation learning method on this behaviour. By leveraging a Vision Language\nModel (VLM) for retrieval, R+X does not require any manual annotation of the\nvideos, and by leveraging in-context learning for execution, robots can perform\ncommanded skills immediately, without requiring a period of training on the\nretrieved videos. Experiments studying a range of everyday household tasks show\nthat R+X succeeds at translating unlabelled human videos into robust robot\nskills, and that R+X outperforms several recent alternative methods. Videos are\navailable at https://www.robot-learning.uk/r-plus-x.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"hNbf49TH5FJBntbe3TuyEtBqfY17VqNCyBYSlrhkHNA","pdfSize":"20692956","objectId":"0xd3781d32011439110a310458018d5aea73e0e5ba697dd3f8ae0382739dc46514","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
