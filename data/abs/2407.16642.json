{"id":"2407.16642","title":"Sharp bounds on aggregate expert error","authors":"Aryeh Kontorovich","authorsParsed":[["Kontorovich","Aryeh",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 16:57:10 GMT"},{"version":"v2","created":"Sun, 15 Sep 2024 00:27:54 GMT"}],"updateDate":"2024-09-17","timestamp":1721753830000,"abstract":"  We revisit the classic problem of aggregating binary advice from\nconditionally independent experts, also known as the Naive Bayes setting. Our\nquantity of interest is the error probability of the optimal decision rule. In\nthe case of symmetric errors (sensitivity = specificity), reasonably tight\nbounds on the optimal error probability are known. In the general asymmetric\ncase, we are not aware of any nontrivial estimates on this quantity. Our\ncontribution consists of sharp upper and lower bounds on the optimal error\nprobability in the general case, which recover and sharpen the best known\nresults in the symmetric special case. Since this turns out to be equivalent to\nestimating the total variation distance between two product distributions, our\nresults also have bearing on this important and challenging problem.\n","subjects":["Mathematics/Probability","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"lAN2iXrlm21iwety7mNubS1ZCW4EguIQEN5G0fc4UxM","pdfSize":"164302"}