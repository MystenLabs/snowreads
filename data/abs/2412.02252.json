{"id":"2412.02252","title":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity","authors":"Da Ma and Lu Chen and Situo Zhang and Yuxun Miao and Su Zhu and Zhi\n  Chen and Hongshen Xu and Hanqi Li and Shuai Fan and Lei Pan and Kai Yu","authorsParsed":[["Ma","Da",""],["Chen","Lu",""],["Zhang","Situo",""],["Miao","Yuxun",""],["Zhu","Su",""],["Chen","Zhi",""],["Xu","Hongshen",""],["Li","Hanqi",""],["Fan","Shuai",""],["Pan","Lei",""],["Yu","Kai",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 08:29:27 GMT"}],"updateDate":"2024-12-04","timestamp":1733214567000,"abstract":"  The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wxUxQ4xBGZzke1fIBn1552Hh3Y7IBbXQz5RfuCs8EXo","pdfSize":"1515975"}