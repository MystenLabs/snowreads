{"id":"2407.13146","title":"PG-Rainbow: Using Distributional Reinforcement Learning in Policy\n  Gradient Methods","authors":"WooJae Jeon, KangJun Lee, Jeewoo Lee","authorsParsed":[["Jeon","WooJae",""],["Lee","KangJun",""],["Lee","Jeewoo",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 04:18:52 GMT"},{"version":"v2","created":"Fri, 19 Jul 2024 02:00:01 GMT"}],"updateDate":"2024-07-22","timestamp":1721276332000,"abstract":"  This paper introduces PG-Rainbow, a novel algorithm that incorporates a\ndistributional reinforcement learning framework with a policy gradient\nalgorithm. Existing policy gradient methods are sample inefficient and rely on\nthe mean of returns when calculating the state-action value function,\nneglecting the distributional nature of returns in reinforcement learning\ntasks. To address this issue, we use an Implicit Quantile Network that provides\nthe quantile information of the distribution of rewards to the critic network\nof the Proximal Policy Optimization algorithm. We show empirical results that\nthrough the integration of reward distribution information into the policy\nnetwork, the policy agent acquires enhanced capabilities to comprehensively\nevaluate the consequences of potential actions in a given state, facilitating\nmore sophisticated and informed decision-making processes. We evaluate the\nperformance of the proposed algorithm in the Atari-2600 game suite, simulated\nvia the Arcade Learning Environment (ALE).\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1MPSH_FB8_m6il43Jvsise1lxX3-CLzYPaLUBhI453Y","pdfSize":"2095118"}