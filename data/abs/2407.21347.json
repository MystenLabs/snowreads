{"id":"2407.21347","title":"Differentially Private Block-wise Gradient Shuffle for Deep Learning","authors":"David Zagardo","authorsParsed":[["Zagardo","David",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 05:32:37 GMT"}],"updateDate":"2024-08-01","timestamp":1722403957000,"abstract":"  Traditional Differentially Private Stochastic Gradient Descent (DP-SGD)\nintroduces statistical noise on top of gradients drawn from a Gaussian\ndistribution to ensure privacy. This paper introduces the novel Differentially\nPrivate Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning.\nBloGS builds off of existing private deep learning literature, but makes a\ndefinitive shift by taking a probabilistic approach to gradient noise\nintroduction through shuffling modeled after information theoretic privacy\nanalyses. The theoretical results presented in this paper show that the\ncombination of shuffling, parameter-specific block size selection, batch layer\nclipping, and gradient accumulation allows DP-BloGS to achieve training times\nclose to that of non-private training while maintaining similar privacy and\nutility guarantees to DP-SGD. DP-BloGS is found to be significantly more\nresistant to data extraction attempts than DP-SGD. The theoretical results are\nvalidated by the experimental findings.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"KxFP6bRBfskmka1mOqfmLGQRUrv0PijANTuSPEjffbQ","pdfSize":"6520051"}