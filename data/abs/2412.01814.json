{
  "id": "2412.01814",
  "title": "COSMOS: Cross-Modality Self-Distillation for Vision Language\n  Pre-training",
  "authors": "Sanghwan Kim, Rui Xiao, Mariana-Iuliana Georgescu, Stephan Alaniz,\n  Zeynep Akata",
  "authorsParsed": [
    [
      "Kim",
      "Sanghwan",
      ""
    ],
    [
      "Xiao",
      "Rui",
      ""
    ],
    [
      "Georgescu",
      "Mariana-Iuliana",
      ""
    ],
    [
      "Alaniz",
      "Stephan",
      ""
    ],
    [
      "Akata",
      "Zeynep",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 18:56:06 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733165766000,
  "abstract": "  Vision-Language Models (VLMs) trained with contrastive loss have achieved\nsignificant advancements in various vision and language tasks. However, the\nglobal nature of contrastive loss makes VLMs focus predominantly on foreground\nobjects, neglecting other crucial information in the image, which limits their\neffectiveness in downstream tasks. To address these challenges, we propose\nCOSMOS: CrOSs-MOdality Self-distillation for vision-language pre-training that\nintegrates a novel text-cropping strategy and cross-attention module into a\nself-supervised learning framework. We create global and local views of images\nand texts (i.e., multi-modal augmentations), which are essential for\nself-distillation in VLMs. We further introduce a cross-attention module,\nenabling COSMOS to learn comprehensive cross-modal representations optimized\nvia a cross-modality self-distillation loss. COSMOS consistently outperforms\nprevious strong baselines on various zero-shot downstream tasks, including\nretrieval, classification, and semantic segmentation. Additionally, it\nsurpasses CLIP-based models trained on larger datasets in visual perception and\ncontextual understanding tasks.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "8UKO17Z0VU6anzLJFK9tfbJ_O1hmvjityHGQQ4eWTQE",
  "pdfSize": "3774705"
}