{
  "id": "2412.11207",
  "title": "ProFe: Communication-Efficient Decentralized Federated Learning via\n  Distillation and Prototypes",
  "authors": "Pedro Miguel S\\'anchez S\\'anchez, Enrique Tom\\'as Mart\\'inez\n  Beltr\\'an, Miguel Fern\\'andez Llamas, G\\'er\\^ome Bovet, Gregorio Mart\\'inez\n  P\\'erez, Alberto Huertas Celdr\\'an",
  "authorsParsed": [
    [
      "Sánchez",
      "Pedro Miguel Sánchez",
      ""
    ],
    [
      "Beltrán",
      "Enrique Tomás Martínez",
      ""
    ],
    [
      "Llamas",
      "Miguel Fernández",
      ""
    ],
    [
      "Bovet",
      "Gérôme",
      ""
    ],
    [
      "Pérez",
      "Gregorio Martínez",
      ""
    ],
    [
      "Celdrán",
      "Alberto Huertas",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 15 Dec 2024 14:49:29 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734274169000,
  "abstract": "  Decentralized Federated Learning (DFL) trains models in a collaborative and\nprivacy-preserving manner while removing model centralization risks and\nimproving communication bottlenecks. However, DFL faces challenges in efficient\ncommunication management and model aggregation within decentralized\nenvironments, especially with heterogeneous data distributions. Thus, this\npaper introduces ProFe, a novel communication optimization algorithm for DFL\nthat combines knowledge distillation, prototype learning, and quantization\ntechniques. ProFe utilizes knowledge from large local models to train smaller\nones for aggregation, incorporates prototypes to better learn unseen classes,\nand applies quantization to reduce data transmitted during communication\nrounds. The performance of ProFe has been validated and compared to the\nliterature by using benchmark datasets like MNIST, CIFAR10, and CIFAR100.\nResults showed that the proposed algorithm reduces communication costs by up to\n~40-50% while maintaining or improving model performance. In addition, it adds\n~20% training time due to increased complexity, generating a trade-off.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Distributed, Parallel, and Cluster Computing",
    "Computer Science/Networking and Internet Architecture"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Es4pzYTFRVvwmAZXoA9YQnLEVQ-0iN8C0yD5CjHSEp8",
  "pdfSize": "1028411"
}