{"id":"2407.06756","title":"Frequency and Generalisation of Periodic Activation Functions in\n  Reinforcement Learning","authors":"Augustine N. Mavor-Parker, Matthew J. Sargent, Caswell Barry, Lewis\n  Griffin, Clare Lyle","authorsParsed":[["Mavor-Parker","Augustine N.",""],["Sargent","Matthew J.",""],["Barry","Caswell",""],["Griffin","Lewis",""],["Lyle","Clare",""]],"versions":[{"version":"v1","created":"Tue, 9 Jul 2024 11:07:41 GMT"}],"updateDate":"2024-07-10","timestamp":1720523261000,"abstract":"  Periodic activation functions, often referred to as learned Fourier features\nhave been widely demonstrated to improve sample efficiency and stability in a\nvariety of deep RL algorithms. Potentially incompatible hypotheses have been\nmade about the source of these improvements. One is that periodic activations\nlearn low frequency representations and as a result avoid overfitting to\nbootstrapped targets. Another is that periodic activations learn high frequency\nrepresentations that are more expressive, allowing networks to quickly fit\ncomplex value functions. We analyse these claims empirically, finding that\nperiodic representations consistently converge to high frequencies regardless\nof their initialisation frequency. We also find that while periodic activation\nfunctions improve sample efficiency, they exhibit worse generalization on\nstates with added observation noise -- especially when compared to otherwise\nequivalent networks with ReLU activation functions. Finally, we show that\nweight decay regularization is able to partially offset the overfitting of\nperiodic activation functions, delivering value functions that learn quickly\nwhile also generalizing.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"iBzrzWMtvZH2FQ2LWLLkcGWJLE8e6CK7-tTr1Hw6YdE","pdfSize":"3462106"}
