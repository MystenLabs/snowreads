{"id":"2407.04861","title":"Late Breaking Results: Fortifying Neural Networks: Safeguarding Against\n  Adversarial Attacks with Stochastic Computing","authors":"Faeze S. Banitaba, Sercan Aygun, M. Hassan Najafi","authorsParsed":[["Banitaba","Faeze S.",""],["Aygun","Sercan",""],["Najafi","M. Hassan",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 20:49:32 GMT"}],"updateDate":"2024-07-09","timestamp":1720212572000,"abstract":"  In neural network (NN) security, safeguarding model integrity and resilience\nagainst adversarial attacks has become paramount. This study investigates the\napplication of stochastic computing (SC) as a novel mechanism to fortify NN\nmodels. The primary objective is to assess the efficacy of SC to mitigate the\ndeleterious impact of attacks on NN results. Through a series of rigorous\nexperiments and evaluations, we explore the resilience of NNs employing SC when\nsubjected to adversarial attacks. Our findings reveal that SC introduces a\nrobust layer of defense, significantly reducing the susceptibility of networks\nto attack-induced alterations in their outcomes. This research contributes\nnovel insights into the development of more secure and reliable NN systems,\nessential for applications in sensitive domains where data integrity is of\nutmost concern.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Emerging Technologies"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"L3zweU3pt8EGPlGVQgTWRg7po1dDQt_Jd5pIKLGcMM8","pdfSize":"185511"}