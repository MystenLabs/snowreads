{"id":"2407.04362","title":"Towards Context-aware Support for Color Vision Deficiency: An Approach\n  Integrating LLM and AR","authors":"Shogo Morita, Yan Zhang, Takuto Yamauchi, Sinan Chen, Jialong Li,\n  Kenji Tei","authorsParsed":[["Morita","Shogo",""],["Zhang","Yan",""],["Yamauchi","Takuto",""],["Chen","Sinan",""],["Li","Jialong",""],["Tei","Kenji",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 09:03:52 GMT"}],"updateDate":"2024-07-08","timestamp":1720170232000,"abstract":"  People with color vision deficiency often face challenges in distinguishing\ncolors such as red and green, which can complicate daily tasks and require the\nuse of assistive tools or environmental adjustments. Current support tools\nmainly focus on presentation-based aids, like the color vision modes found in\niPhone accessibility settings. However, offering context-aware support, like\nindicating the doneness of meat, remains a challenge since task-specific\nsolutions are not cost-effective for all possible scenarios. To address this,\nour paper proposes an application that provides contextual and autonomous\nassistance. This application is mainly composed of: (i) an augmented reality\ninterface that efficiently captures context; and (ii) a multi-modal large\nlanguage model-based reasoner that serves to cognitize the context and then\nreason about the appropriate support contents. Preliminary user experiments\nwith two color vision deficient users across five different scenarios have\ndemonstrated the effectiveness and universality of our application.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Human-Computer Interaction"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HhzbpUH_nwh75OGpzVy7t1q10mJWFUU2oCQ1EN_PPvc","pdfSize":"3384129"}
