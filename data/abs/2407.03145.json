{"id":"2407.03145","title":"Enhancing Translation Accuracy of Large Language Models through\n  Continual Pre-Training on Parallel Data","authors":"Minato Kondo, Takehito Utsuro, Masaaki Nagata","authorsParsed":[["Kondo","Minato",""],["Utsuro","Takehito",""],["Nagata","Masaaki",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:23:36 GMT"}],"updateDate":"2024-07-04","timestamp":1720016616000,"abstract":"  In this paper, we propose a two-phase training approach where pre-trained\nlarge language models are continually pre-trained on parallel data and then\nsupervised fine-tuned with a small amount of high-quality parallel data. To\ninvestigate the effectiveness of our proposed approach, we conducted continual\npre-training with a 3.8B-parameter model and parallel data across eight\ndifferent formats. We evaluate these methods on thirteen test sets for\nJapanese-to-English and English-to-Japanese translation. The results\ndemonstrate that when utilizing parallel data in continual pre-training, it is\nessential to alternate between source and target sentences. Additionally, we\ndemonstrated that the translation accuracy improves only for translation\ndirections where the order of source and target sentences aligns between\ncontinual pre-training data and inference. In addition, we demonstrate that the\nLLM-based translation model is more robust in translating spoken language and\nachieves higher accuracy with less training data compared to supervised\nencoder-decoder models. We also show that the highest accuracy is achieved when\nthe data for continual pre-training consists of interleaved source and target\nsentences and when tags are added to the source sentences.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"ep3NmVVoz0TkinPbmr7vTLl5nu5sy5RdTGacXfZ-dTQ","pdfSize":"1095353"}
