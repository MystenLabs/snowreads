{
  "id": "2412.04144",
  "title": "If You Can't Use Them, Recycle Them: Optimizing Merging at Scale\n  Mitigates Performance Tradeoffs",
  "authors": "Muhammad Khalifa, Yi-Chern Tan, Arash Ahmadian, Tom Hosking, Honglak\n  Lee, Lu Wang, Ahmet \\\"Ust\\\"un, Tom Sherborne, Matthias Gall\\'e",
  "authorsParsed": [
    [
      "Khalifa",
      "Muhammad",
      ""
    ],
    [
      "Tan",
      "Yi-Chern",
      ""
    ],
    [
      "Ahmadian",
      "Arash",
      ""
    ],
    [
      "Hosking",
      "Tom",
      ""
    ],
    [
      "Lee",
      "Honglak",
      ""
    ],
    [
      "Wang",
      "Lu",
      ""
    ],
    [
      "Üstün",
      "Ahmet",
      ""
    ],
    [
      "Sherborne",
      "Tom",
      ""
    ],
    [
      "Gallé",
      "Matthias",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 13:12:51 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 12 Dec 2024 03:30:34 GMT"
    },
    {
      "version": "v3",
      "created": "Mon, 3 Feb 2025 20:31:39 GMT"
    }
  ],
  "updateDate": "2025-02-05",
  "timestamp": 1733404371000,
  "abstract": "  Model merging has shown great promise at combining expert models, but the\nbenefit of merging is unclear when merging \"generalist\" models trained on many\ntasks. We explore merging in the context of large (~100B) models, by recycling\ncheckpoints that exhibit tradeoffs among different tasks. Such checkpoints are\noften created in the process of developing a frontier model, and the suboptimal\nones are usually discarded. Given a pool of model checkpoints obtained from\ndifferent training runs (e.g., different stages, objectives, hyperparameters,\nand data mixtures), which naturally show tradeoffs across different language\ncapabilities (e.g., instruction following vs. code generation), we investigate\nwhether merging can recycle such suboptimal models into a Pareto-optimal one.\nOur optimization algorithm tunes the weight of each checkpoint in a linear\ncombination, resulting in such an optimal model that outperforms both\nindividual models and merge-based baselines. Further analysis shows that good\nmerges tend to include almost all checkpoints with non-zero weights, indicating\nthat even seemingly bad initial checkpoints can contribute to good final\nmerges.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "RZXtxejRIdBXVkDvNAT_Dmh-2QOXdX6VFwhUNLvPFKQ",
  "pdfSize": "747307"
}