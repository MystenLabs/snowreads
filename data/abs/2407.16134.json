{"id":"2407.16134","title":"Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory\n  for Gaussian Process Data","authors":"Hengyu Fu, Zehao Dou, Jiawei Guo, Mengdi Wang, Minshuo Chen","authorsParsed":[["Fu","Hengyu",""],["Dou","Zehao",""],["Guo","Jiawei",""],["Wang","Mengdi",""],["Chen","Minshuo",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 02:42:43 GMT"}],"updateDate":"2024-07-24","timestamp":1721702563000,"abstract":"  Diffusion Transformer, the backbone of Sora for video generation,\nsuccessfully scales the capacity of diffusion models, pioneering new avenues\nfor high-fidelity sequential data generation. Unlike static data such as\nimages, sequential data consists of consecutive data frames indexed by time,\nexhibiting rich spatial and temporal dependencies. These dependencies represent\nthe underlying dynamic model and are critical to validate the generated data.\nIn this paper, we make the first theoretical step towards bridging diffusion\ntransformers for capturing spatial-temporal dependencies. Specifically, we\nestablish score approximation and distribution estimation guarantees of\ndiffusion transformers for learning Gaussian process data with covariance\nfunctions of various decay patterns. We highlight how the spatial-temporal\ndependencies are captured and affect learning efficiency. Our study proposes a\nnovel transformer approximation theory, where the transformer acts to unroll an\nalgorithm. We support our theoretical results by numerical experiments,\nproviding strong evidence that spatial-temporal dependencies are captured\nwithin attention layers, aligning with our approximation theory.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Machine Learning","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"QhtklmpWHIEREZkoNVhg0z0MWEtGfVRdw7Qns0Pxico","pdfSize":"35947230","objectId":"0x87b749318b827b8db415983eb2d8de181bc695d342abdf2a7f6e17160ab2965e","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
