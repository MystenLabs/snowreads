{"id":"2407.12277","title":"Multimodal Reranking for Knowledge-Intensive Visual Question Answering","authors":"Haoyang Wen, Honglei Zhuang, Hamed Zamani, Alexander Hauptmann,\n  Michael Bendersky","authorsParsed":[["Wen","Haoyang",""],["Zhuang","Honglei",""],["Zamani","Hamed",""],["Hauptmann","Alexander",""],["Bendersky","Michael",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 02:58:52 GMT"}],"updateDate":"2024-07-18","timestamp":1721185132000,"abstract":"  Knowledge-intensive visual question answering requires models to effectively\nuse external knowledge to help answer visual questions. A typical pipeline\nincludes a knowledge retriever and an answer generator. However, a retriever\nthat utilizes local information, such as an image patch, may not provide\nreliable question-candidate relevance scores. Besides, the two-tower\narchitecture also limits the relevance score modeling of a retriever to select\ntop candidates for answer generator reasoning. In this paper, we introduce an\nadditional module, a multi-modal reranker, to improve the ranking quality of\nknowledge candidates for answer generation. Our reranking module takes\nmulti-modal information from both candidates and questions and performs\ncross-item interaction for better relevance score modeling. Experiments on\nOK-VQA and A-OKVQA show that multi-modal reranker from distant supervision\nprovides consistent improvements. We also find a training-testing discrepancy\nwith reranking in answer generation, where performance improves if training\nknowledge candidates are similar to or noisier than those used in testing.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wvBB4X984HqSHpHZVRCOmajN381WGZjbme1ewqUgfz0","pdfSize":"1352932","objectId":"0xb58b5b7d94decb2816de24775bc3a1970cf2ce893cfd5b000b23ae8f6ba75c93","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
