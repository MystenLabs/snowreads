{
  "id": "2412.06857",
  "title": "Comb Tensor Networks vs. Matrix Product States: Enhanced Efficiency in\n  High-Dimensional Spaces",
  "authors": "Danylo Kolesnyk, Yelyzaveta Vodovozova",
  "authorsParsed": [
    [
      "Kolesnyk",
      "Danylo",
      ""
    ],
    [
      "Vodovozova",
      "Yelyzaveta",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 8 Dec 2024 20:28:49 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733689729000,
  "abstract": "  Modern approaches to generative modeling of continuous data using tensor\nnetworks incorporate compression layers to capture the most meaningful features\nof high-dimensional inputs. These methods, however, rely on traditional Matrix\nProduct States (MPS) architectures. Here, we demonstrate that beyond a certain\nthreshold in data and bond dimensions, a comb-shaped tensor network\narchitecture can yield more efficient contractions than a standard MPS. This\nfinding suggests that for continuous and high-dimensional data distributions,\ntransitioning from MPS to a comb tensor network representation can\nsubstantially reduce computational overhead while maintaining accuracy.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Physics/Quantum Physics"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "VyRFkAP3yZAE9UHLjMzAzTBeu3yCFuIt7sYcxqbbniU",
  "pdfSize": "409625"
}