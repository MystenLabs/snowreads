{"id":"2407.15489","title":"Two Stacks Are Better Than One: A Comparison of Language Modeling and\n  Translation as Multilingual Pretraining Objectives","authors":"Zihao Li, Shaoxiong Ji, Timothee Mickus, Vincent Segonne, J\\\"org\n  Tiedemann","authorsParsed":[["Li","Zihao",""],["Ji","Shaoxiong",""],["Mickus","Timothee",""],["Segonne","Vincent",""],["Tiedemann","JÃ¶rg",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 09:16:30 GMT"}],"updateDate":"2024-07-23","timestamp":1721639790000,"abstract":"  Pretrained language models (PLMs) display impressive performances and have\ncaptured the attention of the NLP community. Establishing the best practices in\npretraining has therefore become a major point of focus for much of NLP\nresearch -- especially since the insights developed for monolingual English\nmodels need not carry to more complex multilingual. One significant caveat of\nthe current state of the art is that different works are rarely comparable:\nthey often discuss different parameter counts, training data, and evaluation\nmethodology.\n  This paper proposes a comparison of multilingual pretraining objectives in a\ncontrolled methodological environment. We ensure that training data and model\narchitectures are comparable, and discuss the downstream performances across 6\nlanguages that we observe in probing and fine-tuning scenarios. We make two key\nobservations: (1) the architecture dictates which pretraining objective is\noptimal; (2) multilingual translation is a very effective pre-training\nobjective under the right conditions. We make our code, data, and model weights\navailable at \\texttt{\\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pmPbKpu9ZwNQhE3uL11_VOr36MKhcoQZamspIYE5zJk","pdfSize":"283813","objectId":"0xed166917c7ebfed55bcde7c364ff3a8e6277ef97ecf8909497b633b40df16775","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
