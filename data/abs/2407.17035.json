{"id":"2407.17035","title":"Q-Ground: Image Quality Grounding with Large Multi-modality Models","authors":"Chaofeng Chen, Sensen Yang, Haoning Wu, Liang Liao, Zicheng Zhang,\n  Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin","authorsParsed":[["Chen","Chaofeng",""],["Yang","Sensen",""],["Wu","Haoning",""],["Liao","Liang",""],["Zhang","Zicheng",""],["Wang","Annan",""],["Sun","Wenxiu",""],["Yan","Qiong",""],["Lin","Weisi",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 06:42:46 GMT"}],"updateDate":"2024-07-25","timestamp":1721803366000,"abstract":"  Recent advances of large multi-modality models (LMM) have greatly improved\nthe ability of image quality assessment (IQA) method to evaluate and explain\nthe quality of visual content. However, these advancements are mostly focused\non overall quality assessment, and the detailed examination of local quality,\nwhich is crucial for comprehensive visual understanding, is still largely\nunexplored. In this work, we introduce Q-Ground, the first framework aimed at\ntackling fine-scale visual quality grounding by combining large multi-modality\nmodels with detailed visual quality analysis. Central to our contribution is\nthe introduction of the QGround-100K dataset, a novel resource containing 100k\ntriplets of (image, quality text, distortion segmentation) to facilitate deep\ninvestigations into visual quality. The dataset comprises two parts: one with\nhuman-labeled annotations for accurate quality assessment, and another labeled\nautomatically by LMMs such as GPT4V, which helps improve the robustness of\nmodel training while also reducing the costs of data collection. With the\nQGround-100K dataset, we propose a LMM-based method equipped with multi-scale\nfeature learning to learn models capable of performing both image quality\nanswering and distortion segmentation based on text prompts. This\ndual-capability approach not only refines the model's understanding of\nregion-aware image quality but also enables it to interactively respond to\ncomplex, text-based queries about image quality and specific distortions.\nQ-Ground takes a step towards sophisticated visual quality analysis in a finer\nscale, establishing a new benchmark for future research in the area. Codes and\ndataset are available at https://github.com/Q-Future/Q-Ground.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"7fLw0Dfj_WTrnA1Gvz11hVfoN5UbmAl7oJgao7pPt78","pdfSize":"4962400","objectId":"0x90f18502634b4ada15d6e92858a380c4404aa15f7af781aa34aab3d0ee0873c5","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
