{
  "id": "2412.00596",
  "title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded\n  Text-to-Video Generation",
  "authors": "Qiyao Xue, Xiangyu Yin, Boyuan Yang and Wei Gao",
  "authorsParsed": [
    [
      "Xue",
      "Qiyao",
      ""
    ],
    [
      "Yin",
      "Xiangyu",
      ""
    ],
    [
      "Yang",
      "Boyuan",
      ""
    ],
    [
      "Gao",
      "Wei",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 30 Nov 2024 22:02:12 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733004132000,
  "abstract": "  Text-to-video (T2V) generation has been recently enabled by transformer-based\ndiffusion models, but current T2V models lack capabilities in adhering to the\nreal-world common knowledge and physical rules, due to their limited\nunderstanding of physical realism and deficiency in temporal modeling. Existing\nsolutions are either data-driven or require extra model inputs, but cannot be\ngeneralizable to out-of-distribution domains. In this paper, we present PhyT2V,\na new data-independent T2V technique that expands the current T2V model's\ncapability of video generation to out-of-distribution domains, by enabling\nchain-of-thought and step-back reasoning in T2V prompting. Our experiments show\nthat PhyT2V improves existing T2V models' adherence to real-world physical\nrules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\nThe source codes are available at: https://github.com/pittisl/PhyT2V.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "Q-t7c7VOyNKOoyO-zjZNA5CUcQlN5ujXhogu7Rz7C90",
  "pdfSize": "11233517"
}