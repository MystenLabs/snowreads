{
  "id": "2412.01717",
  "title": "Driving Scene Synthesis on Free-form Trajectories with Generative Prior",
  "authors": "Zeyu Yang, Zijie Pan, Yuankun Yang, Xiatian Zhu, Li Zhang",
  "authorsParsed": [
    [
      "Yang",
      "Zeyu",
      ""
    ],
    [
      "Pan",
      "Zijie",
      ""
    ],
    [
      "Yang",
      "Yuankun",
      ""
    ],
    [
      "Zhu",
      "Xiatian",
      ""
    ],
    [
      "Zhang",
      "Li",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 17:07:53 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733159273000,
  "abstract": "  Driving scene synthesis along free-form trajectories is essential for driving\nsimulations to enable closed-loop evaluation of end-to-end driving policies.\nWhile existing methods excel at novel view synthesis on recorded trajectories,\nthey face challenges with novel trajectories due to limited views of driving\nvideos and the vastness of driving environments. To tackle this challenge, we\npropose a novel free-form driving view synthesis approach, dubbed DriveX, by\nleveraging video generative prior to optimize a 3D model across a variety of\ntrajectories. Concretely, we crafted an inverse problem that enables a video\ndiffusion model to be utilized as a prior for many-trajectory optimization of a\nparametric 3D model (e.g., Gaussian splatting). To seamlessly use the\ngenerative prior, we iteratively conduct this process during optimization. Our\nresulting model can produce high-fidelity virtual driving environments outside\nthe recorded trajectory, enabling free-form trajectory driving simulation.\nBeyond real driving scenes, DriveX can also be utilized to simulate virtual\ndriving worlds from AI-generated videos.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "YVkW29iNKQ9ihMBVLJL2zlx4hK3WeENYTw_wBfn5sHk",
  "pdfSize": "14366581"
}