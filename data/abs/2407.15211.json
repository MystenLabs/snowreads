{"id":"2407.15211","title":"When Do Universal Image Jailbreaks Transfer Between Vision-Language\n  Models?","authors":"Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Crist\\'obal\n  Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John\n  Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan\n  Perez","authorsParsed":[["Schaeffer","Rylan",""],["Valentine","Dan",""],["Bailey","Luke",""],["Chua","James",""],["Eyzaguirre","Crist√≥bal",""],["Durante","Zane",""],["Benton","Joe",""],["Miranda","Brando",""],["Sleight","Henry",""],["Hughes","John",""],["Agrawal","Rajashree",""],["Sharma","Mrinank",""],["Emmons","Scott",""],["Koyejo","Sanmi",""],["Perez","Ethan",""]],"versions":[{"version":"v1","created":"Sun, 21 Jul 2024 16:27:24 GMT"}],"updateDate":"2024-07-23","timestamp":1721579244000,"abstract":"  The integration of new modalities into frontier AI systems offers exciting\ncapabilities, but also increases the possibility such systems can be\nadversarially manipulated in undesirable ways. In this work, we focus on a\npopular class of vision-language models (VLMs) that generate text outputs\nconditioned on visual and textual inputs. We conducted a large-scale empirical\nstudy to assess the transferability of gradient-based universal image\n\"jailbreaks\" using a diverse set of over 40 open-parameter VLMs, including 18\nnew VLMs that we publicly release. Overall, we find that transferable\ngradient-based image jailbreaks are extremely difficult to obtain. When an\nimage jailbreak is optimized against a single VLM or against an ensemble of\nVLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits\nlittle-to-no transfer to any other VLMs; transfer is not affected by whether\nthe attacked and target VLMs possess matching vision backbones or language\nmodels, whether the language model underwent instruction-following and/or\nsafety-alignment training, or many other factors. Only two settings display\npartially successful transfer: between identically-pretrained and\nidentically-initialized VLMs with slightly different VLM training data, and\nbetween different training checkpoints of a single VLM. Leveraging these\nresults, we then demonstrate that transfer can be significantly improved\nagainst a specific target VLM by attacking larger ensembles of \"highly-similar\"\nVLMs. These results stand in stark contrast to existing evidence of universal\nand transferable text jailbreaks against language models and transferable\nadversarial attacks against image classifiers, suggesting that VLMs may be more\nrobust to gradient-based transfer attacks.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Cryptography and Security","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"1McTvWJJ1ioAvJxDhk_-XDJhsYFfmJbFRbXphwj7IqQ","pdfSize":"10577811"}