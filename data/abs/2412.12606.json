{
  "id": "2412.12606",
  "title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in\n  Large Multimodal Models",
  "authors": "YiFan Zhang, Shanglin Lei, Runqi Qiao, Zhuoma GongQue, Xiaoshuai Song,\n  Guanting Dong, Qiuna Tan, Zhe Wei, Peiqing Yang, Ye Tian, Yadong Xue, Xiaofei\n  Wang, Honggang Zhang",
  "authorsParsed": [
    [
      "Zhang",
      "YiFan",
      ""
    ],
    [
      "Lei",
      "Shanglin",
      ""
    ],
    [
      "Qiao",
      "Runqi",
      ""
    ],
    [
      "GongQue",
      "Zhuoma",
      ""
    ],
    [
      "Song",
      "Xiaoshuai",
      ""
    ],
    [
      "Dong",
      "Guanting",
      ""
    ],
    [
      "Tan",
      "Qiuna",
      ""
    ],
    [
      "Wei",
      "Zhe",
      ""
    ],
    [
      "Yang",
      "Peiqing",
      ""
    ],
    [
      "Tian",
      "Ye",
      ""
    ],
    [
      "Xue",
      "Yadong",
      ""
    ],
    [
      "Wang",
      "Xiaofei",
      ""
    ],
    [
      "Zhang",
      "Honggang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 07:06:10 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734419170000,
  "abstract": "  The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to\nevaluate the model's ability to analyze and reason beyond basic content. (2)\nRecognizing that people of different age groups have varying needs and\nperspectives when faced with the same scenario, our benchmark stratifies\nquestions into three age categories: young people, middle-aged people, and\nolder people. This design allows for a detailed assessment of LMMs'\ncapabilities in meeting the preferences and needs of different age groups. With\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\ntasks, indicating that existing LMMs still have considerable room for\nimprovement in addressing real-world applications. Looking ahead, we anticipate\nthat the MDI-Benchmark will open new pathways for aligning real-world\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\navailable at https://mdi-benchmark.github.io/\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "bEnltiCXOVKjImmaDIxW9Q0cuy-j8w1g3Z-vDH4OO84",
  "pdfSize": "7175608"
}