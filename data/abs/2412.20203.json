{
  "id": "2412.20203",
  "title": "No-regret learning in harmonic games: Extrapolation in the face of\n  conflicting interests",
  "authors": "Davide Legacci and Panayotis Mertikopoulos and Christos H.\n  Papadimitriou and Georgios Piliouras and Bary S. R. Pradelski",
  "authorsParsed": [
    [
      "Legacci",
      "Davide",
      ""
    ],
    [
      "Mertikopoulos",
      "Panayotis",
      ""
    ],
    [
      "Papadimitriou",
      "Christos H.",
      ""
    ],
    [
      "Piliouras",
      "Georgios",
      ""
    ],
    [
      "Pradelski",
      "Bary S. R.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 28 Dec 2024 16:28:13 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735403293000,
  "abstract": "  The long-run behavior of multi-agent learning - and, in particular, no-regret\nlearning - is relatively well-understood in potential games, where players have\naligned interests. By contrast, in harmonic games - the strategic counterpart\nof potential games, where players have conflicting interests - very little is\nknown outside the narrow subclass of 2-player zero-sum games with a fully-mixed\nequilibrium. Our paper seeks to partially fill this gap by focusing on the full\nclass of (generalized) harmonic games and examining the convergence properties\nof follow-the-regularized-leader (FTRL), the most widely studied class of\nno-regret learning schemes. As a first result, we show that the continuous-time\ndynamics of FTRL are Poincar\\'e recurrent, that is, they return arbitrarily\nclose to their starting point infinitely often, and hence fail to converge. In\ndiscrete time, the standard, \"vanilla\" implementation of FTRL may lead to even\nworse outcomes, eventually trapping the players in a perpetual cycle of\nbest-responses. However, if FTRL is augmented with a suitable extrapolation\nstep - which includes as special cases the optimistic and mirror-prox variants\nof FTRL - we show that learning converges to a Nash equilibrium from any\ninitial condition, and all players are guaranteed at most O(1) regret. These\nresults provide an in-depth understanding of no-regret learning in harmonic\ngames, nesting prior work on 2-player zero-sum games, and showing at a high\nlevel that harmonic games are the canonical complement of potential games, not\nonly from a strategic, but also from a dynamic viewpoint.\n",
  "subjects": [
    "Computer Science/Computer Science and Game Theory",
    "Computer Science/Machine Learning",
    "Computer Science/Multiagent Systems",
    "Mathematics/Optimization and Control"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "U_ERJN2qP-0uR6sUVsTEnXgOQBNHONilGbrBn5nKjNg",
  "pdfSize": "2094885"
}