{"id":"2407.21061","title":"Improving noisy student training for low-resource languages in\n  End-to-End ASR using CycleGAN and inter-domain losses","authors":"Chia-Yu Li and Ngoc Thang Vu","authorsParsed":[["Li","Chia-Yu",""],["Vu","Ngoc Thang",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 10:57:06 GMT"}],"updateDate":"2024-08-01","timestamp":1721991426000,"abstract":"  Training a semi-supervised end-to-end speech recognition system using noisy\nstudent training has significantly improved performance. However, this approach\nrequires a substantial amount of paired speech-text and unlabeled speech, which\nis costly for low-resource languages. Therefore, this paper considers a more\nextreme case of semi-supervised end-to-end automatic speech recognition where\nthere are limited paired speech-text, unlabeled speech (less than five hours),\nand abundant external text. Firstly, we observe improved performance by\ntraining the model using our previous work on semi-supervised learning\n\"CycleGAN and inter-domain losses\" solely with external text. Secondly, we\nenhance \"CycleGAN and inter-domain losses\" by incorporating automatic\nhyperparameter tuning, calling it \"enhanced CycleGAN inter-domain losses.\"\nThirdly, we integrate it into the noisy student training approach pipeline for\nlow-resource scenarios. Our experimental results, conducted on six non-English\nlanguages from Voxforge and Common Voice, show a 20% word error rate reduction\ncompared to the baseline teacher model and a 10% word error rate reduction\ncompared to the baseline best student model, highlighting the significant\nimprovements achieved through our proposed method.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"iYOmPd2VeUNa685KkOSFfCwi0q5sAGLCSBbqFUj9R6I","pdfSize":"1368572"}