{"id":"2412.01380","title":"Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking","authors":"Marco Federici, Davide Belli, Mart van Baalen, Amir Jalalirad, Andrii\n  Skliar, Bence Major, Markus Nagel, Paul Whatmough","authorsParsed":[["Federici","Marco",""],["Belli","Davide",""],["van Baalen","Mart",""],["Jalalirad","Amir",""],["Skliar","Andrii",""],["Major","Bence",""],["Nagel","Markus",""],["Whatmough","Paul",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 11:07:51 GMT"}],"updateDate":"2024-12-03","timestamp":1733137671000,"abstract":"  While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.\n","subjects":["Computer Science/Machine Learning","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KvbRXdk2weQFIWhYtpdgzuRr27IXq1Tt0zOWk95FBqU","pdfSize":"857228"}