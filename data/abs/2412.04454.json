{
  "id": "2412.04454",
  "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
  "authors": "Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita\n  Saha, Doyen Sahoo, Tao Yu, Caiming Xiong",
  "authorsParsed": [
    [
      "Xu",
      "Yiheng",
      ""
    ],
    [
      "Wang",
      "Zekun",
      ""
    ],
    [
      "Wang",
      "Junli",
      ""
    ],
    [
      "Lu",
      "Dunjie",
      ""
    ],
    [
      "Xie",
      "Tianbao",
      ""
    ],
    [
      "Saha",
      "Amrita",
      ""
    ],
    [
      "Sahoo",
      "Doyen",
      ""
    ],
    [
      "Yu",
      "Tao",
      ""
    ],
    [
      "Xiong",
      "Caiming",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 18:58:26 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733425106000,
  "abstract": "  Graphical User Interfaces (GUIs) are critical to human-computer interaction,\nyet automating GUI tasks remains challenging due to the complexity and\nvariability of visual environments. Existing approaches often rely on textual\nrepresentations of GUIs, which introduce limitations in generalization,\nefficiency, and scalability. In this paper, we introduce Aguvis, a unified pure\nvision-based framework for autonomous GUI agents that operates across various\nplatforms. Our approach leverages image-based observations, and grounding\ninstructions in natural language to visual elements, and employs a consistent\naction space to ensure cross-platform generalization. To address the\nlimitations of previous work, we integrate explicit planning and reasoning\nwithin the model, enhancing its ability to autonomously navigate and interact\nwith complex digital environments. We construct a large-scale dataset of GUI\nagent trajectories, incorporating multimodal reasoning and grounding, and\nemploy a two-stage training pipeline that first focuses on general GUI\ngrounding, followed by planning and reasoning. Through comprehensive\nexperiments, we demonstrate that Aguvis surpasses previous state-of-the-art\nmethods in both offline and real-world online scenarios, achieving, to our\nknowledge, the first fully autonomous pure vision GUI agent capable of\nperforming tasks independently without collaboration with external\nclosed-source models. We open-sourced all datasets, models, and training\nrecipes to facilitate future research at https://aguvis-project.github.io/.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "fk9gpBu5fWTcPvOE6g1IP7X1-RxTuwACpK0LDLsja4U",
  "pdfSize": "16974222"
}