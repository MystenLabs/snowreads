{"id":"2407.08464","title":"TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware\n  Representations","authors":"Junik Bae and Kwanyoung Park and Youngwoon Lee","authorsParsed":[["Bae","Junik",""],["Park","Kwanyoung",""],["Lee","Youngwoon",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 13:01:18 GMT"}],"updateDate":"2024-07-12","timestamp":1720702878000,"abstract":"  Unsupervised goal-conditioned reinforcement learning (GCRL) is a promising\nparadigm for developing diverse robotic skills without external supervision.\nHowever, existing unsupervised GCRL methods often struggle to cover a wide\nrange of states in complex environments due to their limited exploration and\nsparse or noisy rewards for GCRL. To overcome these challenges, we propose a\nnovel unsupervised GCRL method that leverages TemporaL Distance-aware\nRepresentations (TLDR). TLDR selects faraway goals to initiate exploration and\ncomputes intrinsic exploration rewards and goal-reaching rewards, based on\ntemporal distance. Specifically, our exploration policy seeks states with large\ntemporal distances (i.e. covering a large state space), while the\ngoal-conditioned policy learns to minimize the temporal distance to the goal\n(i.e. reaching the goal). Our experimental results in six simulated robotic\nlocomotion environments demonstrate that our method significantly outperforms\nprevious unsupervised GCRL methods in achieving a wide variety of states.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"WQFaI5Ahy88jlLIBblZ0eMICGfCtGlZkaYSJiGaBGUA","pdfSize":"5824409"}