{
  "id": "2412.18120",
  "title": "Do Language Models Understand the Cognitive Tasks Given to Them?\n  Investigations with the N-Back Paradigm",
  "authors": "Xiaoyang Hu and Richard L. Lewis",
  "authorsParsed": [
    [
      "Hu",
      "Xiaoyang",
      ""
    ],
    [
      "Lewis",
      "Richard L.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 24 Dec 2024 03:06:52 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 26 Dec 2024 16:31:53 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735009612000,
  "abstract": "  Cognitive tasks originally developed for humans are now increasingly used to\nstudy language models. While applying these tasks is often straightforward,\ninterpreting their results can be challenging. In particular, when a model\nunderperforms, it is often unclear whether this results from a limitation in\nthe cognitive ability being tested or a failure to understand the task itself.\nA recent study argues that GPT 3.5's declining performance on 2-back and 3-back\ntasks reflects a working memory capacity limit similar to humans (Gong et al.,\n2024). By analyzing a range of open-source language models of varying\nperformance levels on these tasks, we show that the poor performance instead\nreflects a limitation in task comprehension and task set maintenance. In\naddition, we challenge the best-performing model with progressively harder\nversions of the task (up to 10-back) and experiment with alternative prompting\nstrategies, before analyzing model attentions. Our larger aim is to contribute\nto the ongoing conversation around refining methodologies for the cognitive\nevaluation of language models.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "T2iV6OPP8XspTTeWzylAxmqxYwVeAIkrwIsX_e7HDMc",
  "pdfSize": "9506540"
}