{
  "id": "2412.11743",
  "title": "Generalized Bayesian deep reinforcement learning",
  "authors": "Shreya Sinha Roy, Richard G. Everitt, Christian P. Robert and\n  Ritabrata Dutta",
  "authorsParsed": [
    [
      "Roy",
      "Shreya Sinha",
      ""
    ],
    [
      "Everitt",
      "Richard G.",
      ""
    ],
    [
      "Robert",
      "Christian P.",
      ""
    ],
    [
      "Dutta",
      "Ritabrata",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 13:02:17 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734354137000,
  "abstract": "  Bayesian reinforcement learning (BRL) is a method that merges principles from\nBayesian statistics and reinforcement learning to make optimal decisions in\nuncertain environments. Similar to other model-based RL approaches, it involves\ntwo key components: (1) Inferring the posterior distribution of the data\ngenerating process (DGP) modeling the true environment and (2) policy learning\nusing the learned posterior. We propose to model the dynamics of the unknown\nenvironment through deep generative models assuming Markov dependence. In\nabsence of likelihood functions for these models we train them by learning a\ngeneralized predictive-sequential (or prequential) scoring rule (SR) posterior.\nWe use sequential Monte Carlo (SMC) samplers to draw samples from this\ngeneralized Bayesian posterior distribution. In conjunction, to achieve\nscalability in the high dimensional parameter space of the neural networks, we\nuse the gradient based Markov chain Monte Carlo (MCMC) kernels within SMC. To\njustify the use of the prequential scoring rule posterior we prove a\nBernstein-von Misses type theorem. For policy learning, we propose expected\nThompson sampling (ETS) to learn the optimal policy by maximizing the expected\nvalue function with respect to the posterior distribution. This improves upon\ntraditional Thompson sampling (TS) and its extensions which utilize only one\nsample drawn from the posterior distribution. This improvement is studied both\ntheoretically and using simulation studies assuming discrete action and\nstate-space. Finally we successfully extend our setup for a challenging problem\nwith continuous action space without theoretical guarantees.\n",
  "subjects": [
    "Statistics/Machine Learning",
    "Computer Science/Machine Learning",
    "Statistics/Methodology"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "pcgfGiLCADW34ax1KRbhmUQ6Yjl8KPm6lXnAIl3_XXY",
  "pdfSize": "4061009"
}