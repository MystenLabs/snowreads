{
  "id": "2412.15664",
  "title": "SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control",
  "authors": "Xiaohan Zhang and Sebastian Starke and Vladimir Guzov and Zhensong\n  Zhang and Eduardo P\\'erez Pellitero and Gerard Pons-Moll",
  "authorsParsed": [
    [
      "Zhang",
      "Xiaohan",
      ""
    ],
    [
      "Starke",
      "Sebastian",
      ""
    ],
    [
      "Guzov",
      "Vladimir",
      ""
    ],
    [
      "Zhang",
      "Zhensong",
      ""
    ],
    [
      "Pellitero",
      "Eduardo PÃ©rez",
      ""
    ],
    [
      "Pons-Moll",
      "Gerard",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 08:25:15 GMT"
    }
  ],
  "updateDate": "2024-12-23",
  "timestamp": 1734683115000,
  "abstract": "  Synthesizing natural human motion that adapts to complex environments while\nallowing creative control remains a fundamental challenge in motion synthesis.\nExisting models often fall short, either by assuming flat terrain or lacking\nthe ability to control motion semantics through text. To address these\nlimitations, we introduce SCENIC, a diffusion model designed to generate human\nmotion that adapts to dynamic terrains within virtual scenes while enabling\nsemantic control through natural language. The key technical challenge lies in\nsimultaneously reasoning about complex scene geometry while maintaining text\ncontrol. This requires understanding both high-level navigation goals and\nfine-grained environmental constraints. The model must ensure physical\nplausibility and precise navigation across varied terrain, while also\npreserving user-specified text control, such as ``carefully stepping over\nobstacles\" or ``walking upstairs like a zombie.\" Our solution introduces a\nhierarchical scene reasoning approach. At its core is a novel scene-dependent,\ngoal-centric canonicalization that handles high-level goal constraint, and is\ncomplemented by an ego-centric distance field that captures local geometric\ndetails. This dual representation enables our model to generate physically\nplausible motion across diverse 3D scenes. By implementing frame-wise text\nalignment, our system achieves seamless transitions between different motion\nstyles while maintaining scene constraints. Experiments demonstrate our novel\ndiffusion model generates arbitrarily long human motions that both adapt to\ncomplex scenes with varying terrain surfaces and respond to textual prompts.\nAdditionally, we show SCENIC can generalize to four real-scene datasets. Our\ncode, dataset, and models will be released at\n\\url{https://virtualhumans.mpi-inf.mpg.de/scenic/}.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "u5SFTjsxURAsjmmy7KTNbu5ZYMrLHQxtab9KpIMiL4E",
  "pdfSize": "22766058"
}