{"id":"2407.02596","title":"Towards More Realistic Extraction Attacks: An Adversarial Perspective","authors":"Yash More, Prakhar Ganesh, Golnoosh Farnadi","authorsParsed":[["More","Yash",""],["Ganesh","Prakhar",""],["Farnadi","Golnoosh",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 18:33:49 GMT"}],"updateDate":"2024-07-04","timestamp":1719945229000,"abstract":"  Language models are prone to memorizing large parts of their training data,\nmaking them vulnerable to extraction attacks. Existing research on these\nattacks remains limited in scope, often studying isolated trends rather than\nthe real-world interactions with these models. In this paper, we revisit\nextraction attacks from an adversarial perspective, exploiting the brittleness\nof language models. We find significant churn in extraction attack trends,\ni.e., even minor, unintuitive changes to the prompt, or targeting smaller\nmodels and older checkpoints, can exacerbate the risks of extraction by up to\n$2-4 \\times$. Moreover, relying solely on the widely accepted verbatim match\nunderestimates the extent of extracted information, and we provide various\nalternatives to more accurately capture the true risks of extraction. We\nconclude our discussion with data deduplication, a commonly suggested\nmitigation strategy, and find that while it addresses some memorization\nconcerns, it remains vulnerable to the same escalation of extraction risks\nagainst a real-world adversary. Our findings highlight the necessity of\nacknowledging an adversary's true capabilities to avoid underestimating\nextraction risks.\n","subjects":["Computing Research Repository/Cryptography and Security","Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"EI631dLVqgpCeRVklLyz9JnJ0BM2L_wY6c-KVQ3ksIM","pdfSize":"780345"}