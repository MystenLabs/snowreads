{"id":"2412.01420","title":"Task Adaptation of Reinforcement Learning-based NAS Agents through\n  Transfer Learning","authors":"Amber Cassimon, Siegfried Mercelis, Kevin Mets","authorsParsed":[["Cassimon","Amber",""],["Mercelis","Siegfried",""],["Mets","Kevin",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 12:00:27 GMT"},{"version":"v2","created":"Thu, 19 Dec 2024 15:51:33 GMT"}],"updateDate":"2024-12-20","timestamp":1733140827000,"abstract":"  Recently, a novel paradigm has been proposed for reinforcement learning-based\nNAS agents, that revolves around the incremental improvement of a given\narchitecture. We assess the abilities of such reinforcement learning agents to\ntransfer between different tasks. We perform our evaluation using the\nTrans-NASBench-101 benchmark, and consider the efficacy of the transferred\nagents, as well as how quickly they can be trained. We find that pretraining an\nagent on one task benefits the performance of the agent in another task in all\nbut 1 task when considering final performance. We also show that the training\nprocedure for an agent can be shortened significantly by pretraining it on\nanother task. Our results indicate that these effects occur regardless of the\nsource or target task, although they are more pronounced for some tasks than\nfor others. Our results show that transfer learning can be an effective tool in\nmitigating the computational cost of the initial training procedure for\nreinforcement learning-based NAS agents.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"oip58LeIlZVGbVWErqaQfceeetueiivh3QTw__oCAb8","pdfSize":"9213929"}