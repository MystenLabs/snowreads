{"id":"2407.15707","title":"Predicting the Best of N Visual Trackers","authors":"Basit Alawode, Sajid Javed, Arif Mahmood, and Jiri Matas","authorsParsed":[["Alawode","Basit",""],["Javed","Sajid",""],["Mahmood","Arif",""],["Matas","Jiri",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 15:17:09 GMT"}],"updateDate":"2024-07-23","timestamp":1721661429000,"abstract":"  We observe that the performance of SOTA visual trackers surprisingly strongly\nvaries across different video attributes and datasets. No single tracker\nremains the best performer across all tracking attributes and datasets. To\nbridge this gap, for a given video sequence, we predict the \"Best of the N\nTrackers\", called the BofN meta-tracker. At its core, a Tracking Performance\nPrediction Network (TP2N) selects a predicted best performing visual tracker\nfor the given video sequence using only a few initial frames. We also introduce\na frame-level BofN meta-tracker which keeps predicting best performer after\nregular temporal intervals. The TP2N is based on self-supervised learning\narchitectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with\nViT-S as a backbone performs the best. The video-level BofN meta-tracker\noutperforms, by a large margin, existing SOTA trackers on nine standard\nbenchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123,\nOTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN\nmeta-tracker effectively handling variations in the tracking scenarios within\nlong sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is\n88.7% and 91.1% with video and frame-level settings respectively. The best\nperforming tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average\noverlap is 67.88% and 70.98% with video and frame level settings, compared to\nthe best performing ARTrack, 64.12%. This work also presents an extensive\nevaluation of competitive tracking methods on all commonly used benchmarks,\nfollowing their protocols. The code, the trained models, and the results will\nsoon be made publicly available on\nhttps://github.com/BasitAlawode/Best_of_N_Trackers.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Electrical Engineering and Systems Science/Image and Video Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"E0fp-rnBnhjvUch1FiiMu_XV-a0DsFNIK6hqDbg_Anw","pdfSize":"4139591","objectId":"0x0d37af984c4b507717d51d0afb921bae163ab30b33d3809a756b9d0752340e11","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
