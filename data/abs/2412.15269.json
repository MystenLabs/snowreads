{"id":"2412.15269","title":"The Reliability Paradox: Exploring How Shortcut Learning Undermines\n  Language Model Calibration","authors":"Geetanjali Bihani and Julia Rayz","authorsParsed":[["Bihani","Geetanjali",""],["Rayz","Julia",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 08:04:28 GMT"}],"updateDate":"2024-12-23","timestamp":1734422668000,"abstract":"  The advent of pre-trained language models (PLMs) has enabled significant\nperformance gains in the field of natural language processing. However, recent\nstudies have found PLMs to suffer from miscalibration, indicating a lack of\naccuracy in the confidence estimates provided by these models. Current\nevaluation methods for PLM calibration often assume that lower calibration\nerror estimates indicate more reliable predictions. However, fine-tuned PLMs\noften resort to shortcuts, leading to overconfident predictions that create the\nillusion of enhanced performance but lack generalizability in their decision\nrules. The relationship between PLM reliability, as measured by calibration\nerror, and shortcut learning, has not been thoroughly explored thus far. This\npaper aims to investigate this relationship, studying whether lower calibration\nerror implies reliable decision rules for a language model. Our findings reveal\nthat models with seemingly superior calibration portray higher levels of\nnon-generalizable decision rules. This challenges the prevailing notion that\nwell-calibrated models are inherently reliable. Our study highlights the need\nto bridge the current gap between language model calibration and generalization\nobjectives, urging the development of comprehensive frameworks to achieve truly\nrobust and reliable language models.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"udbX5YAcYdLvSalJrgqpST96ovgyCY2kKGuHq9s4Mss","pdfSize":"572030"}