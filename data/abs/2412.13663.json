{"id":"2412.13663","title":"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast, Memory Efficient, and Long Context Finetuning and Inference","authors":"Benjamin Warner, Antoine Chaffin, Benjamin Clavi\\'e, Orion Weller,\n  Oskar Hallstr\\\"om, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal\n  Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli","authorsParsed":[["Warner","Benjamin",""],["Chaffin","Antoine",""],["Clavié","Benjamin",""],["Weller","Orion",""],["Hallström","Oskar",""],["Taghadouini","Said",""],["Gallagher","Alexis",""],["Biswas","Raja",""],["Ladhak","Faisal",""],["Aarsen","Tom",""],["Cooper","Nathan",""],["Adams","Griffin",""],["Howard","Jeremy",""],["Poli","Iacopo",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 09:39:44 GMT"},{"version":"v2","created":"Thu, 19 Dec 2024 06:32:26 GMT"}],"updateDate":"2024-12-20","timestamp":1734514784000,"abstract":"  Encoder-only transformer models such as BERT offer a great performance-size\ntradeoff for retrieval and classification tasks with respect to larger\ndecoder-only models. Despite being the workhorse of numerous production\npipelines, there have been limited Pareto improvements to BERT since its\nrelease. In this paper, we introduce ModernBERT, bringing modern model\noptimizations to encoder-only models and representing a major Pareto\nimprovement over older encoders. Trained on 2 trillion tokens with a native\n8192 sequence length, ModernBERT models exhibit state-of-the-art results on a\nlarge pool of evaluations encompassing diverse classification tasks and both\nsingle and multi-vector retrieval on different domains (including code). In\naddition to strong downstream performance, ModernBERT is also the most speed\nand memory efficient encoder and is designed for inference on common GPUs.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XAlaaKn9hiSndukPM6NOBXQ_uAKQDzMtCmS7E2rPKFg","pdfSize":"445479"}