{"id":"2412.07147","title":"MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation","authors":"Bo Li, Shaolin Zhu and Lijie Wen","authorsParsed":[["Li","Bo",""],["Zhu","Shaolin",""],["Wen","Lijie",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 03:12:35 GMT"},{"version":"v2","created":"Mon, 16 Dec 2024 09:28:53 GMT"}],"updateDate":"2024-12-17","timestamp":1733800355000,"abstract":"  Image Translation (IT) holds immense potential across diverse domains,\nenabling the translation of textual content within images into various\nlanguages. However, existing datasets often suffer from limitations in scale,\ndiversity, and quality, hindering the development and evaluation of IT models.\nTo address this issue, we introduce MIT-10M, a large-scale parallel corpus of\nmultilingual image translation with over 10M image-text pairs derived from\nreal-world data, which has undergone extensive data cleaning and multilingual\ntranslation validation. It contains 840K images in three sizes, 28 categories,\ntasks with three levels of difficulty and 14 languages image-text pairs, which\nis a considerable improvement on existing datasets. We conduct extensive\nexperiments to evaluate and train models on MIT-10M. The experimental results\nclearly indicate that our dataset has higher adaptability when it comes to\nevaluating the performance of the models in tackling challenging and complex\nimage translation tasks in the real world. Moreover, the performance of the\nmodel fine-tuned with MIT-10M has tripled compared to the baseline model,\nfurther confirming its superiority.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"NKLNLfmeuVdWCkoMkeslKcDXEjeCS-0hlYYJuuA0sJc","pdfSize":"3122360"}