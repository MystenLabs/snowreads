{
  "id": "2412.00811",
  "title": "Vid-Morp: Video Moment Retrieval Pretraining from Unlabeled Videos in\n  the Wild",
  "authors": "Peijun Bao, Chenqi Kong, Zihao Shao, Boon Poh Ng, Meng Hwa Er, Alex C.\n  Kot",
  "authorsParsed": [
    [
      "Bao",
      "Peijun",
      ""
    ],
    [
      "Kong",
      "Chenqi",
      ""
    ],
    [
      "Shao",
      "Zihao",
      ""
    ],
    [
      "Ng",
      "Boon Poh",
      ""
    ],
    [
      "Er",
      "Meng Hwa",
      ""
    ],
    [
      "Kot",
      "Alex C.",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 1 Dec 2024 13:49:21 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733060961000,
  "abstract": "  Given a natural language query, video moment retrieval aims to localize the\ndescribed temporal moment in an untrimmed video. A major challenge of this task\nis its heavy dependence on labor-intensive annotations for training. Unlike\nexisting works that directly train models on manually curated data, we propose\na novel paradigm to reduce annotation costs: pretraining the model on\nunlabeled, real-world videos. To support this, we introduce Video Moment\nRetrieval Pretraining (Vid-Morp), a large-scale dataset collected with minimal\nhuman intervention, consisting of over 50K videos captured in the wild and 200K\npseudo annotations. Direct pretraining on these imperfect pseudo annotations,\nhowever, presents significant challenges, including mismatched sentence-video\npairs and imprecise temporal boundaries. To address these issues, we propose\nthe ReCorrect algorithm, which comprises two main phases: semantics-guided\nrefinement and memory-consensus correction. The semantics-guided refinement\nenhances the pseudo labels by leveraging semantic similarity with video frames\nto clean out unpaired data and make initial adjustments to temporal boundaries.\nIn the following memory-consensus correction phase, a memory bank tracks the\nmodel predictions, progressively correcting the temporal boundaries based on\nconsensus within the memory. Comprehensive experiments demonstrate ReCorrect's\nstrong generalization abilities across multiple downstream settings. Zero-shot\nReCorrect achieves over 75% and 80% of the best fully-supervised performance on\ntwo benchmarks, while unsupervised ReCorrect reaches about 85% on both. The\ncode, dataset, and pretrained models are available at\nhttps://github.com/baopj/Vid-Morp.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "JSEt6F7t0nDRgSIc_oQVjKD5HHUvs8JGTOftti0JkZI",
  "pdfSize": "3109816"
}