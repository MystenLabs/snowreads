{"id":"2412.08225","title":"Improving Active Learning with a Bayesian Representation of Epistemic\n  Uncertainty","authors":"Jake Thomas and Jeremie Houssineau","authorsParsed":[["Thomas","Jake",""],["Houssineau","Jeremie",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 09:19:20 GMT"}],"updateDate":"2024-12-12","timestamp":1733908760000,"abstract":"  A popular strategy for active learning is to specifically target a reduction\nin epistemic uncertainty, since aleatoric uncertainty is often considered as\nbeing intrinsic to the system of interest and therefore not reducible. Yet,\ndistinguishing these two types of uncertainty remains challenging and there is\nno single strategy that consistently outperforms the others. We propose to use\na particular combination of probability and possibility theories, with the aim\nof using the latter to specifically represent epistemic uncertainty, and we\nshow how this combination leads to new active learning strategies that have\ndesirable properties. In order to demonstrate the efficiency of these\nstrategies in non-trivial settings, we introduce the notion of a possibilistic\nGaussian process (GP) and consider GP-based multiclass and binary\nclassification problems, for which the proposed methods display a strong\nperformance for both simulated and real datasets.\n","subjects":["Statistics/Methodology","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gKm6Uiox3BcWfjUMpO9iJk0qBEjWxZRzPCl7LNgugq0","pdfSize":"695638"}