{"id":"2412.14354","title":"State Space Models are Strong Text Rerankers","authors":"Zhichao Xu, Jinghua Yan, Ashim Gupta, Vivek Srikumar","authorsParsed":[["Xu","Zhichao",""],["Yan","Jinghua",""],["Gupta","Ashim",""],["Srikumar","Vivek",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 21:42:15 GMT"}],"updateDate":"2024-12-20","timestamp":1734558135000,"abstract":"  Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking -- a\ntask requiring fine-grained query-document interaction and long-context\nunderstanding -- remains underexplored.\n  This study benchmarks SSM-based architectures (specifically, Mamba-1 and\nMamba-2) against transformer-based models across various scales, architectures,\nand pre-training objectives, focusing on performance and efficiency in text\nreranking tasks. We find that (1) Mamba architectures achieve competitive text\nranking performance, comparable to transformer-based models of similar size;\n(2) they are less efficient in training and inference compared to transformers\nwith flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance\nand efficiency. These results underscore the potential of state space models as\na transformer alternative and highlight areas for improvement in future IR\napplications.\n","subjects":["Computer Science/Computation and Language","Computer Science/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Y0VmHamWhkFLSpkV0Uhx5gyHIlyfssC9xqt13zuILlY","pdfSize":"454574"}