{
  "id": "2412.11625",
  "title": "Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods",
  "authors": "Diana Bar-Or Nirman, Ariel Weizman, Amos Azaria",
  "authorsParsed": [
    [
      "Nirman",
      "Diana Bar-Or",
      ""
    ],
    [
      "Weizman",
      "Ariel",
      ""
    ],
    [
      "Azaria",
      "Amos",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 10:10:27 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734343827000,
  "abstract": "  While Large Language Models (LLMs) have become central tools in various\nfields, they often provide inaccurate or false information. This study examines\nuser preferences regarding falsehood responses from LLMs. Specifically, we\nevaluate preferences for LLM responses where false statements are explicitly\nmarked versus unmarked responses and preferences for confident falsehoods\ncompared to LLM disclaimers acknowledging a lack of knowledge. Additionally, we\ninvestigate how requiring users to assess the truthfulness of statements\ninfluences these preferences.\n  Surprisingly, 61\\% of users prefer unmarked falsehood responses over marked\nones, and 69\\% prefer confident falsehoods over LLMs admitting lack of\nknowledge. In all our experiments, a total of 300 users participated,\ncontributing valuable data to our analysis and conclusions. When users are\nrequired to evaluate the truthfulness of statements, preferences for unmarked\nand falsehood responses decrease slightly but remain high. These findings\nsuggest that user preferences, which influence LLM training via feedback\nmechanisms, may inadvertently encourage the generation of falsehoods. Future\nresearch should address the ethical and practical implications of aligning LLM\nbehavior with such preferences.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "9VZRnZY1dVHd7diajWVm8WMSmM9R1L-CcKKgrI0lFzI",
  "pdfSize": "788551"
}