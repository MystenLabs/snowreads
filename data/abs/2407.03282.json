{"id":"2407.03282","title":"LLM Internal States Reveal Hallucination Risk Faced With a Query","authors":"Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang,\n  Bryan Wilie, Pascale Fung","authorsParsed":[["Ji","Ziwei",""],["Chen","Delong",""],["Ishii","Etsuko",""],["Cahyawijaya","Samuel",""],["Bang","Yejin",""],["Wilie","Bryan",""],["Fung","Pascale",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 17:08:52 GMT"}],"updateDate":"2024-07-04","timestamp":1720026532000,"abstract":"  The hallucination problem of Large Language Models (LLMs) significantly\nlimits their reliability and trustworthiness. Humans have a self-awareness\nprocess that allows us to recognize what we don't know when faced with queries.\nInspired by this, our paper investigates whether LLMs can estimate their own\nhallucination risk before response generation. We analyze the internal\nmechanisms of LLMs broadly both in terms of training data sources and across 15\ndiverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.\nOur empirical analysis reveals two key insights: (1) LLM internal states\nindicate whether they have seen the query in training data or not; and (2) LLM\ninternal states show they are likely to hallucinate or not regarding the query.\nOur study explores particular neurons, activation layers, and tokens that play\na crucial role in the LLM perception of uncertainty and hallucination risk. By\na probing estimator, we leverage LLM self-assessment, achieving an average\nhallucination estimation accuracy of 84.32\\% at run time.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wdL1m8cLOfx0EC4Mls4uY49h5Bmux-eDyF1e7hqZE_g","pdfSize":"2051595"}