{
  "id": "2412.12979",
  "title": "Guiding Generative Protein Language Models with Reinforcement Learning",
  "authors": "Filippo Stocco, Maria Artigues-Lleixa, Andrea Hunklinger, Talal\n  Widatalla, Marc Guell, Noelia Ferruz",
  "authorsParsed": [
    [
      "Stocco",
      "Filippo",
      ""
    ],
    [
      "Artigues-Lleixa",
      "Maria",
      ""
    ],
    [
      "Hunklinger",
      "Andrea",
      ""
    ],
    [
      "Widatalla",
      "Talal",
      ""
    ],
    [
      "Guell",
      "Marc",
      ""
    ],
    [
      "Ferruz",
      "Noelia",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 14:58:37 GMT"
    },
    {
      "version": "v2",
      "created": "Tue, 28 Jan 2025 09:51:24 GMT"
    }
  ],
  "updateDate": "2025-01-29",
  "timestamp": 1734447517000,
  "abstract": "  Autoregressive protein language models (pLMs) have emerged as powerful tools\nto efficiently design functional proteins with extraordinary diversity, as\nevidenced by the successful generation of diverse enzyme families, including\nlysozymes or carbonic anhydrases. However, a fundamental limitation of pLMs is\ntheir propensity to sample from dense regions within the training distribution,\nwhich constrains their ability to sample from rare, high-value regions of the\nsequence space. This limitation becomes particularly critical in applications\ntargeting underrepresented distribution tails, such as engineering for\nenzymatic activity or binding affinity. To address this challenge, we implement\nDPO_pLM, a reinforcement learning (RL) framework for protein sequence\noptimization with pLMs. Drawing inspiration from the success of RL in aligning\nlanguage models to human preferences, we approach protein optimization as an\niterative process that fine-tunes pLM weights to maximize a reward provided by\nan external oracle. Our strategy demonstrates that RL can efficiently optimize\nfor a variety of custom properties without the need for additional data,\nachieving significant while preserving sequence diversity. We applied DPO_pLM\nto the design of EGFR binders, successfully identifying nanomolar binders\nwithin hours. Our code is publicly available at\nhttps://github.com/AI4PDLab/DPO_pLM.\n",
  "subjects": [
    "Quantitative Biology/Biomolecules"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "O9aMuLb__KSKyfpgh3e_cKfaU4ghCIXxtKiO9fmJLGc",
  "pdfSize": "3259048"
}