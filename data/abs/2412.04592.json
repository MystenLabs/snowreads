{"id":"2412.04592","title":"EgoPoints: Advancing Point Tracking for Egocentric Videos","authors":"Ahmad Darkhalil, Rhodri Guerrier, Adam W. Harley, Dima Damen","authorsParsed":[["Darkhalil","Ahmad",""],["Guerrier","Rhodri",""],["Harley","Adam W.",""],["Damen","Dima",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 20:12:29 GMT"}],"updateDate":"2024-12-09","timestamp":1733429549000,"abstract":"  We introduce EgoPoints, a benchmark for point tracking in egocentric videos.\nWe annotate 4.7K challenging tracks in egocentric sequences. Compared to the\npopular TAP-Vid-DAVIS evaluation benchmark, we include 9x more points that go\nout-of-view and 59x more points that require re-identification (ReID) after\nreturning to view. To measure the performance of models on these challenging\npoints, we introduce evaluation metrics that specifically monitor tracking\nperformance on points in-view, out-of-view, and points that require\nre-identification. We then propose a pipeline to create semi-real sequences,\nwith automatic ground truth. We generate 11K such sequences by combining\ndynamic Kubric objects with scene points from EPIC Fields. When fine-tuning\npoint tracking methods on these sequences and evaluating on our annotated\nEgoPoints sequences, we improve CoTracker across all metrics, including the\ntracking accuracy $\\delta^\\star_{\\text{avg}}$ by 2.7 percentage points and\naccuracy on ReID sequences (ReID$\\delta_{\\text{avg}}$) by 2.4 points. We also\nimprove $\\delta^\\star_{\\text{avg}}$ and ReID$\\delta_{\\text{avg}}$ of PIPs++ by\n0.3 and 2.8 respectively.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YrBUylOQsOCRpoSdesODRjUpHHYt9V8NZ0CZNs5L-jg","pdfSize":"15692316"}