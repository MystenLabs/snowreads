{"id":"2407.04493","title":"PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation","authors":"Yinghua Yao, Yuangang Pan, Jing Li, Ivor Tsang, Xin Yao","authorsParsed":[["Yao","Yinghua",""],["Pan","Yuangang",""],["Li","Jing",""],["Tsang","Ivor",""],["Yao","Xin",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 13:32:06 GMT"}],"updateDate":"2024-07-08","timestamp":1720186326000,"abstract":"  Recent advancements in the realm of deep generative models focus on\ngenerating samples that satisfy multiple desired properties. However, prevalent\napproaches optimize these property functions independently, thus omitting the\ntrade-offs among them. In addition, the property optimization is often\nimproperly integrated into the generative models, resulting in an unnecessary\ncompromise on generation quality (i.e., the quality of generated samples). To\naddress these issues, we formulate a constrained optimization problem. It seeks\nto optimize generation quality while ensuring that generated samples reside at\nthe Pareto front of multiple property objectives. Such a formulation enables\nthe generation of samples that cannot be further improved simultaneously on the\nconflicting property functions and preserves good quality of generated samples.\nBuilding upon this formulation, we introduce the PaRetO-gUided Diffusion model\n(PROUD), wherein the gradients in the denoising process are dynamically\nadjusted to enhance generation quality while the generated samples adhere to\nPareto optimality. Experimental evaluations on image generation and protein\ngeneration tasks demonstrate that our PROUD consistently maintains superior\ngeneration quality while approaching Pareto optimality across multiple property\nfunctions compared to various baselines.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"VjCbYLxYWCTVee2ri3bBEYzn5icQPDnkrbJX9TjBSC8","pdfSize":"16228986"}