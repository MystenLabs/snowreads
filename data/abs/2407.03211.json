{"id":"2407.03211","title":"How Does Quantization Affect Multilingual LLMs?","authors":"Kelly Marchisio, Saurabh Dash, Hongyu Chen, Dennis Aumiller, Ahmet\n  \\\"Ust\\\"un, Sara Hooker, Sebastian Ruder","authorsParsed":[["Marchisio","Kelly",""],["Dash","Saurabh",""],["Chen","Hongyu",""],["Aumiller","Dennis",""],["Üstün","Ahmet",""],["Hooker","Sara",""],["Ruder","Sebastian",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 15:39:40 GMT"}],"updateDate":"2024-07-04","timestamp":1720021180000,"abstract":"  Quantization techniques are widely used to improve inference speed and\ndeployment of large language models. While a wide body of work examines the\nimpact of quantized LLMs on English tasks, none have examined the effect of\nquantization across languages. We conduct a thorough analysis of quantized\nmultilingual LLMs, focusing on their performance across languages and at\nvarying scales. We use automatic benchmarks, LLM-as-a-Judge methods, and human\nevaluation, finding that (1) harmful effects of quantization are apparent in\nhuman evaluation, and automatic metrics severely underestimate the detriment: a\n1.7% average drop in Japanese across automatic tasks corresponds to a 16.0%\ndrop reported by human evaluators on realistic prompts; (2) languages are\ndisparately affected by quantization, with non-Latin script languages impacted\nworst; and (3) challenging tasks such as mathematical reasoning degrade\nfastest. As the ability to serve low-compute models is critical for wide global\nadoption of NLP technologies, our results urge consideration of multilingual\nperformance as a key evaluation criterion for efficient models.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"a8A6WVRckuVy-TfRqLRlKPn3E2f8oGFxGA0-Toahrlk","pdfSize":"551799"}