{"id":"2407.20584","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training","authors":"Weiyu Huang, Yuezhou Hu, Guohao Jian, Jun Zhu, Jianfei Chen","authorsParsed":[["Huang","Weiyu",""],["Hu","Yuezhou",""],["Jian","Guohao",""],["Zhu","Jun",""],["Chen","Jianfei",""]],"versions":[{"version":"v1","created":"Tue, 30 Jul 2024 06:33:44 GMT"},{"version":"v2","created":"Mon, 26 Aug 2024 13:19:48 GMT"}],"updateDate":"2024-08-27","timestamp":1722321224000,"abstract":"  The tremendous success of Large Language Models (LLMs) across various complex\ntasks relies heavily on their substantial scale, which raises challenges during\nmodel deployment due to their large memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often experience considerable performance degradation on\ncomplex language understanding tasks, calling into question the feasibility of\npruning in LLMs. To address this issue, we propose a pruning pipeline for\nsemi-structured sparse models via retraining, termed Adaptive Sparse Trainer\n(AST). Unlike previous one-shot pruning methods, AST incrementally transforms\ndense models into sparse ones by applying decay to masked weights while\nallowing the model to adaptively select masks throughout the training process.\nFurthermore, we observe that using distillation with a dense model as the\nteacher can prevent the sparse model from falling into local optima and\naccelerate convergence. In addition, we incorporate extra well-initialized\nparameters to further enhance model performance with minimal increase in memory\nfootprint. AST can significantly enhance model performance, approaching the\nlevel of dense models. When applied to the LLaMA2-7B model, AST reduces the\nzero-shot accuracy gap between dense and semi-structured sparse models to 1.12%\nacross multiple zero-shot tasks, utilizing less than 0.4% of the pretraining\ntokens. Our work demonstrates the feasibility of deploying semi-structured\nsparse large language models and introduces a novel method for achieving highly\ncompressed models when combined with existing quantization techniques.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6JsiCpc2wqyRbvuaZh-ZOY4plpnMmAd-KLdjNF0qmsQ","pdfSize":"848530"}