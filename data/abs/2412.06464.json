{"id":"2412.06464","title":"Gated Delta Networks: Improving Mamba2 with Delta Rule","authors":"Songlin Yang, Jan Kautz, Ali Hatamizadeh","authorsParsed":[["Yang","Songlin",""],["Kautz","Jan",""],["Hatamizadeh","Ali",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 13:09:04 GMT"}],"updateDate":"2024-12-10","timestamp":1733749744000,"abstract":"  Linear Transformers have gained attention as efficient alternatives to\nstandard Transformers, but their performance in retrieval and long-context\ntasks has been limited. To address these limitations, recent work has explored\ntwo distinct mechanisms: gating for adaptive memory control and the delta\nupdate rule for precise memory modifications. We observe that these mechanisms\nare complementary: gating enables rapid memory erasure while the delta rule\nfacilitates targeted updates. Building on this insight, we introduce the gated\ndelta rule and develop a parallel training algorithm optimized for modern\nhardware. Our proposed architecture, Gated DeltaNet, consistently surpasses\nexisting models like Mamba2 and DeltaNet across multiple benchmarks, including\nlanguage modeling, common-sense reasoning, in-context retrieval, length\nextrapolation, and long-context understanding. We further enhance performance\nby developing hybrid architectures that combine Gated DeltaNet layers with\nsliding window attention or Mamba2 layers, achieving both improved training\nefficiency and superior task performance.\n","subjects":["Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JO-zdLKc8RyD6ScYpEsYOtCM7HdrjwRB79u2U3eMXEQ","pdfSize":"405308"}