{"id":"2407.13764","title":"Shape of Motion: 4D Reconstruction from a Single Video","authors":"Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, Angjoo\n  Kanazawa","authorsParsed":[["Wang","Qianqian",""],["Ye","Vickie",""],["Gao","Hang",""],["Austin","Jake",""],["Li","Zhengqi",""],["Kanazawa","Angjoo",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 17:59:08 GMT"}],"updateDate":"2024-07-19","timestamp":1721325548000,"abstract":"  Monocular dynamic reconstruction is a challenging and long-standing vision\nproblem due to the highly ill-posed nature of the task. Existing approaches are\nlimited in that they either depend on templates, are effective only in\nquasi-static scenes, or fail to model 3D motion explicitly. In this work, we\nintroduce a method capable of reconstructing generic dynamic scenes, featuring\nexplicit, full-sequence-long 3D motion, from casually captured monocular\nvideos. We tackle the under-constrained nature of the problem with two key\ninsights: First, we exploit the low-dimensional structure of 3D motion by\nrepresenting scene motion with a compact set of SE3 motion bases. Each point's\nmotion is expressed as a linear combination of these bases, facilitating soft\ndecomposition of the scene into multiple rigidly-moving groups. Second, we\nutilize a comprehensive set of data-driven priors, including monocular depth\nmaps and long-range 2D tracks, and devise a method to effectively consolidate\nthese noisy supervisory signals, resulting in a globally consistent\nrepresentation of the dynamic scene. Experiments show that our method achieves\nstate-of-the-art performance for both long-range 3D/2D motion estimation and\nnovel view synthesis on dynamic scenes. Project Page:\nhttps://shape-of-motion.github.io/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"7TnzqyYZRzJkHcTtsKESFJ2WwctVAbU52o5dGibeWAo","pdfSize":"39560004"}