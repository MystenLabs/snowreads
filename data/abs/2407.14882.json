{"id":"2407.14882","title":"Reduced Effectiveness of Kolmogorov-Arnold Networks on Functions with\n  Noise","authors":"Haoran Shen and Chen Zeng and Jiahui Wang and Qiao Wang","authorsParsed":[["Shen","Haoran",""],["Zeng","Chen",""],["Wang","Jiahui",""],["Wang","Qiao",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 14:17:10 GMT"}],"updateDate":"2024-07-23","timestamp":1721485030000,"abstract":"  It has been observed that even a small amount of noise introduced into the\ndataset can significantly degrade the performance of KAN. In this brief note,\nwe aim to quantitatively evaluate the performance when noise is added to the\ndataset. We propose an oversampling technique combined with denoising to\nalleviate the impact of noise. Specifically, we employ kernel filtering based\non diffusion maps for pre-filtering the noisy data for training KAN network.\nOur experiments show that while adding i.i.d. noise with any fixed SNR, when we\nincrease the amount of training data by a factor of $r$, the test-loss (RMSE)\nof KANs will exhibit a performance trend like $\\text{test-loss} \\sim\n\\mathcal{O}(r^{-\\frac{1}{2}})$ as $r\\to +\\infty$. We conclude that applying\nboth oversampling and filtering strategies can reduce the detrimental effects\nof noise. Nevertheless, determining the optimal variance for the kernel\nfiltering process is challenging, and enhancing the volume of training data\nsubstantially increases the associated costs, because the training dataset\nneeds to be expanded multiple times in comparison to the initial clean data. As\na result, the noise present in the data ultimately diminishes the effectiveness\nof Kolmogorov-Arnold networks.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"RTr6ZOGoJtQDfWxtLUaGAy6X1l_3Ky4ooajRyNvrI-E","pdfSize":"949878"}