{"id":"2412.06699","title":"You See it, You Got it: Learning 3D Creation on Pose-Free Videos at\n  Scale","authors":"Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu\n  Tang, Xinlong Wang","authorsParsed":[["Ma","Baorui",""],["Gao","Huachen",""],["Deng","Haoge",""],["Luo","Zhengxiong",""],["Huang","Tiejun",""],["Tang","Lulu",""],["Wang","Xinlong",""]],"versions":[{"version":"v1","created":"Mon, 9 Dec 2024 17:44:56 GMT"},{"version":"v2","created":"Sat, 14 Dec 2024 15:42:05 GMT"}],"updateDate":"2024-12-17","timestamp":1733766296000,"abstract":"  Recent 3D generation models typically rely on limited-scale 3D `gold-labels'\nor 2D diffusion priors for 3D content creation. However, their performance is\nupper-bounded by constrained 3D priors due to the lack of scalable learning\nparadigms. In this work, we present See3D, a visual-conditional multi-view\ndiffusion model trained on large-scale Internet videos for open-world 3D\ncreation. The model aims to Get 3D knowledge by solely Seeing the visual\ncontents from the vast and rapidly growing video data -- You See it, You Got\nit. To achieve this, we first scale up the training data using a proposed data\ncuration pipeline that automatically filters out multi-view inconsistencies and\ninsufficient observations from source videos. This results in a high-quality,\nrichly diverse, large-scale dataset of multi-view images, termed WebVi3D,\ncontaining 320M frames from 16M video clips. Nevertheless, learning generic 3D\npriors from videos without explicit 3D geometry or camera pose annotations is\nnontrivial, and annotating poses for web-scale videos is prohibitively\nexpensive. To eliminate the need for pose conditions, we introduce an\ninnovative visual-condition - a purely 2D-inductive visual signal generated by\nadding time-dependent noise to the masked video data. Finally, we introduce a\nnovel visual-conditional 3D generation framework by integrating See3D into a\nwarping-based pipeline for high-fidelity 3D generation. Our numerical and\nvisual comparisons on single and sparse reconstruction benchmarks show that\nSee3D, trained on cost-effective and scalable video data, achieves notable\nzero-shot and open-world generation capabilities, markedly outperforming models\ntrained on costly and constrained 3D datasets. Please refer to our project page\nat: https://vision.baai.ac.cn/see3d\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-JSjwAbibplfbN3drHoBsOKbCLQicjF7N-RwT-yrpVg","pdfSize":"43528640"}