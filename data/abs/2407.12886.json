{"id":"2407.12886","title":"Whitening Not Recommended for Classification Tasks in LLMs","authors":"Ali Forooghi, Shaghayegh Sadeghi, Jianguo Lu","authorsParsed":[["Forooghi","Ali",""],["Sadeghi","Shaghayegh",""],["Lu","Jianguo",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 22:48:30 GMT"}],"updateDate":"2024-07-19","timestamp":1721170110000,"abstract":"  Sentence embedding is a cornerstone in NLP. Whitening has been claimed to be\nan effective operation to improve embedding quality obtained from Large\nLanguage Models (LLMs). However, we find that the efficacy of whitening is\nmodel-dependent and task-dependent. In particular, whitening degenerates\nembeddings for classification tasks. The conclusion is supported by extensive\nexperiments. We also explored a variety of whitening operations, including PCA,\nZCA, PCA-Cor, ZCA-Cor and Cholesky whitenings. A by-product of our research is\nembedding evaluation platform for LLMs called SentEval+.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"690iwHFZxSk38JYyfplWomxzpZfO-6KUDMymyJikVHQ","pdfSize":"3426157"}