{
  "id": "2412.02373",
  "title": "Active Negative Loss: A Robust Framework for Learning with Noisy Labels",
  "authors": "Xichen Ye, Yifan Wu, Yiwen Xu, Xiaoqiang Li, Weizhong Zhang, and Yifan\n  Chen",
  "authorsParsed": [
    [
      "Ye",
      "Xichen",
      ""
    ],
    [
      "Wu",
      "Yifan",
      ""
    ],
    [
      "Xu",
      "Yiwen",
      ""
    ],
    [
      "Li",
      "Xiaoqiang",
      ""
    ],
    [
      "Zhang",
      "Weizhong",
      ""
    ],
    [
      "Chen",
      "Yifan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 11:00:15 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733223615000,
  "abstract": "  Deep supervised learning has achieved remarkable success across a wide range\nof tasks, yet it remains susceptible to overfitting when confronted with noisy\nlabels. To address this issue, noise-robust loss functions offer an effective\nsolution for enhancing learning in the presence of label noise. In this work,\nwe systematically investigate the limitation of the recently proposed Active\nPassive Loss (APL), which employs Mean Absolute Error (MAE) as its passive loss\nfunction. Despite the robustness brought by MAE, one of its key drawbacks is\nthat it pays equal attention to clean and noisy samples; this feature slows\ndown convergence and potentially makes training difficult, particularly in\nlarge-scale datasets. To overcome these challenges, we introduce a novel loss\nfunction class, termed Normalized Negative Loss Functions (NNLFs), which serve\nas passive loss functions within the APL framework. NNLFs effectively address\nthe limitations of MAE by concentrating more on memorized clean samples. By\nreplacing MAE in APL with our proposed NNLFs, we enhance APL and present a new\nframework called Active Negative Loss (ANL). Moreover, in non-symmetric noise\nscenarios, we propose an entropy-based regularization technique to mitigate the\nvulnerability to the label imbalance. Extensive experiments demonstrate that\nthe new loss functions adopted by our ANL framework can achieve better or\ncomparable performance to state-of-the-art methods across various label noise\ntypes and in image segmentation tasks. The source code is available at:\nhttps://github.com/Virusdoll/Active-Negative-Loss.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "PcW1euPiS14wRWzzBMZqDR_bmioueT2eERZ2Aq0OaSA",
  "pdfSize": "1463280"
}