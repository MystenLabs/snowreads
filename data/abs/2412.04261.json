{
  "id": "2412.04261",
  "title": "Aya Expanse: Combining Research Breakthroughs for a New Multilingual\n  Frontier",
  "authors": "John Dang, Shivalika Singh, Daniel D'souza, Arash Ahmadian, Alejandro\n  Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy,\n  Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos,\n  Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis\n  Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh,\n  David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir\n  Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagn\\'e, Felipe\n  Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy,\n  Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza\n  Ermis, Ahmet \\\"Ust\\\"un, Sara Hooker",
  "authorsParsed": [
    [
      "Dang",
      "John",
      ""
    ],
    [
      "Singh",
      "Shivalika",
      ""
    ],
    [
      "D'souza",
      "Daniel",
      ""
    ],
    [
      "Ahmadian",
      "Arash",
      ""
    ],
    [
      "Salamanca",
      "Alejandro",
      ""
    ],
    [
      "Smith",
      "Madeline",
      ""
    ],
    [
      "Peppin",
      "Aidan",
      ""
    ],
    [
      "Hong",
      "Sungjin",
      ""
    ],
    [
      "Govindassamy",
      "Manoj",
      ""
    ],
    [
      "Zhao",
      "Terrence",
      ""
    ],
    [
      "Kublik",
      "Sandra",
      ""
    ],
    [
      "Amer",
      "Meor",
      ""
    ],
    [
      "Aryabumi",
      "Viraat",
      ""
    ],
    [
      "Campos",
      "Jon Ander",
      ""
    ],
    [
      "Tan",
      "Yi-Chern",
      ""
    ],
    [
      "Kocmi",
      "Tom",
      ""
    ],
    [
      "Strub",
      "Florian",
      ""
    ],
    [
      "Grinsztajn",
      "Nathan",
      ""
    ],
    [
      "Flet-Berliac",
      "Yannis",
      ""
    ],
    [
      "Locatelli",
      "Acyr",
      ""
    ],
    [
      "Lin",
      "Hangyu",
      ""
    ],
    [
      "Talupuru",
      "Dwarak",
      ""
    ],
    [
      "Venkitesh",
      "Bharat",
      ""
    ],
    [
      "Cairuz",
      "David",
      ""
    ],
    [
      "Yang",
      "Bowen",
      ""
    ],
    [
      "Chung",
      "Tim",
      ""
    ],
    [
      "Ko",
      "Wei-Yin",
      ""
    ],
    [
      "Shi",
      "Sylvie Shang",
      ""
    ],
    [
      "Shukayev",
      "Amir",
      ""
    ],
    [
      "Bae",
      "Sammie",
      ""
    ],
    [
      "Piktus",
      "Aleksandra",
      ""
    ],
    [
      "Castagné",
      "Roman",
      ""
    ],
    [
      "Cruz-Salinas",
      "Felipe",
      ""
    ],
    [
      "Kim",
      "Eddie",
      ""
    ],
    [
      "Crawhall-Stein",
      "Lucas",
      ""
    ],
    [
      "Morisot",
      "Adrien",
      ""
    ],
    [
      "Roy",
      "Sudip",
      ""
    ],
    [
      "Blunsom",
      "Phil",
      ""
    ],
    [
      "Zhang",
      "Ivan",
      ""
    ],
    [
      "Gomez",
      "Aidan",
      ""
    ],
    [
      "Frosst",
      "Nick",
      ""
    ],
    [
      "Fadaee",
      "Marzieh",
      ""
    ],
    [
      "Ermis",
      "Beyza",
      ""
    ],
    [
      "Üstün",
      "Ahmet",
      ""
    ],
    [
      "Hooker",
      "Sara",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 15:41:06 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733413266000,
  "abstract": "  We introduce the Aya Expanse model family, a new generation of 8B and 32B\nparameter multilingual language models, aiming to address the critical\nchallenge of developing highly performant multilingual models that match or\nsurpass the capabilities of monolingual models. By leveraging several years of\nresearch at Cohere For AI and Cohere, including advancements in data arbitrage,\nmultilingual preference training, and model merging, Aya Expanse sets a new\nstate-of-the-art in multilingual performance. Our evaluations on the\nArena-Hard-Auto dataset, translated into 23 languages, demonstrate that Aya\nExpanse 8B and 32B outperform leading open-weight models in their respective\nparameter classes, including Gemma 2, Qwen 2.5, and Llama 3.1, achieving up to\na 76.6% win-rate. Notably, Aya Expanse 32B outperforms Llama 3.1 70B, a model\nwith twice as many parameters, achieving a 54.0% win-rate. In this short\ntechnical report, we present extended evaluation results for the Aya Expanse\nmodel family and release their open-weights, together with a new multilingual\nevaluation dataset m-ArenaHard.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "1gVDpEGy8KdWnOQ1Gt1kOcxM1bZTaW6w_NPGUAMbSLE",
  "pdfSize": "1225567"
}