{
  "id": "2412.00547",
  "title": "Motion Dreamer: Realizing Physically Coherent Video Generation through\n  Scene-Aware Motion Reasoning",
  "authors": "Tianshuo Xu, Zhifei Chen, Leyi Wu, Hao Lu, Yuying Chen, Lihui Jiang,\n  Bingbing Liu, Yingcong Chen",
  "authorsParsed": [
    [
      "Xu",
      "Tianshuo",
      ""
    ],
    [
      "Chen",
      "Zhifei",
      ""
    ],
    [
      "Wu",
      "Leyi",
      ""
    ],
    [
      "Lu",
      "Hao",
      ""
    ],
    [
      "Chen",
      "Yuying",
      ""
    ],
    [
      "Jiang",
      "Lihui",
      ""
    ],
    [
      "Liu",
      "Bingbing",
      ""
    ],
    [
      "Chen",
      "Yingcong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 30 Nov 2024 17:40:49 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 8 Jan 2025 04:14:07 GMT"
    }
  ],
  "updateDate": "2025-01-09",
  "timestamp": 1732988449000,
  "abstract": "  Recent numerous video generation models, also known as world models, have\ndemonstrated the ability to generate plausible real-world videos. However, many\nstudies have shown that these models often produce motion results lacking\nlogical or physical coherence. In this paper, we revisit video generation\nmodels and find that single-stage approaches struggle to produce high-quality\nresults while maintaining coherent motion reasoning. To address this issue, we\npropose \\textbf{Motion Dreamer}, a two-stage video generation framework. In\nStage I, the model generates an intermediate motion representation-such as a\nsegmentation map or depth map-based on the input image and motion conditions,\nfocusing solely on the motion itself. In Stage II, the model uses this\nintermediate motion representation as a condition to generate a high-detail\nvideo. By decoupling motion reasoning from high-fidelity video synthesis, our\napproach allows for more accurate and physically plausible motion generation.\nWe validate the effectiveness of our approach on the Physion dataset and in\nautonomous driving scenarios. For example, given a single push, our model can\nsynthesize the sequential toppling of a set of dominoes. Similarly, by varying\nthe movements of ego-cars, our model can produce different effects on other\nvehicles. Our work opens new avenues in creating models that can reason about\nphysical interactions in a more coherent and realistic manner. Our webpage is\navailable: https://envision-research.github.io/MotionDreamer/.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "b0xGlGkaQgXg4OjkLcQYD_7fswJiOtI6dDR8MUmjH54",
  "pdfSize": "9511940"
}