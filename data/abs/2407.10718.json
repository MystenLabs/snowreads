{"id":"2407.10718","title":"Sibyl: Simple yet Effective Agent Framework for Complex Real-world\n  Reasoning","authors":"Yulong Wang, Tianhao Shen, Lifeng Liu, Jian Xie","authorsParsed":[["Wang","Yulong",""],["Shen","Tianhao",""],["Liu","Lifeng",""],["Xie","Jian",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 13:45:40 GMT"},{"version":"v2","created":"Tue, 16 Jul 2024 14:16:53 GMT"}],"updateDate":"2024-07-17","timestamp":1721051140000,"abstract":"  Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"9v_ZRXAIR1_QXZG-VLyC__FKQarlhTQJdLka4ckDwYs","pdfSize":"457526"}