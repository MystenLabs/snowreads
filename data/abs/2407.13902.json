{"id":"2407.13902","title":"EvaluateXAI: A Framework to Evaluate the Reliability and Consistency of\n  Rule-based XAI Techniques for Software Analytics Tasks","authors":"Md Abdul Awal, Chanchal K. Roy","authorsParsed":[["Awal","Md Abdul",""],["Roy","Chanchal K.",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 21:09:28 GMT"}],"updateDate":"2024-07-22","timestamp":1721336968000,"abstract":"  The advancement of machine learning (ML) models has led to the development of\nML-based approaches to improve numerous software engineering tasks in software\nmaintenance and evolution. Nevertheless, research indicates that despite their\npotential successes, ML models may not be employed in real-world scenarios\nbecause they often remain a black box to practitioners, lacking explainability\nin their reasoning. Recently, various rule-based model-agnostic Explainable AI\n(XAI) techniques, such as PyExplainer and LIME, have been employed to explain\nthe predictions of ML models in software analytics tasks. This paper assesses\nthe ability of these techniques (e.g., PyExplainer and LIME) to generate\nreliable and consistent explanations for ML models across various software\nanalytics tasks, including Just-in-Time (JIT) defect prediction, clone\ndetection, and the classification of useful code review comments. Our manual\ninvestigations find inconsistencies and anomalies in the explanations generated\nby these techniques. Therefore, we design a novel framework: Evaluation of\nExplainable AI (EvaluateXAI), along with granular-level evaluation metrics, to\nautomatically assess the effectiveness of rule-based XAI techniques in\ngenerating reliable and consistent explanations for ML models in software\nanalytics tasks. After conducting in-depth experiments involving seven\nstate-of-the-art ML models trained on five datasets and six evaluation metrics,\nwe find that none of the evaluation metrics reached 100\\%, indicating the\nunreliability of the explanations generated by XAI techniques. Additionally,\nPyExplainer and LIME failed to provide consistent explanations for 86.11% and\n77.78% of the experimental combinations, respectively. Therefore, our\nexperimental findings emphasize the necessity for further research in XAI to\nproduce reliable and consistent explanations for ML models in software\nanalytics tasks.\n","subjects":["Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"NVklFQ3x06s8xhJVdmTJ--AtAgdgMO5AFnZ-IkOFju4","pdfSize":"3608468","objectId":"0x0a58969b24c25efdd149d1c12376b0fd5cd179e50a3069d149c7cb706a1eacc3","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
