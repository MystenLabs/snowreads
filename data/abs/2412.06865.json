{
  "id": "2412.06865",
  "title": "FP=xINT:A Low-Bit Series Expansion Algorithm for Post-Training\n  Quantization",
  "authors": "Boyang Zhang, Daning Cheng, Yunquan Zhang, Fangmin Liu",
  "authorsParsed": [
    [
      "Zhang",
      "Boyang",
      ""
    ],
    [
      "Cheng",
      "Daning",
      ""
    ],
    [
      "Zhang",
      "Yunquan",
      ""
    ],
    [
      "Liu",
      "Fangmin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 08:50:28 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733734228000,
  "abstract": "  Post-Training Quantization (PTQ) converts pre-trained Full-Precision (FP)\nmodels into quantized versions without training. While existing methods reduce\nsize and computational costs, they also significantly degrade performance and\nquantization efficiency at extremely low settings due to quantization noise. We\nintroduce a deep model series expansion framework to address this issue,\nenabling rapid and accurate approximation of unquantized models without\ncalibration sets or fine-tuning. This is the first use of series expansion for\nneural network quantization. Specifically, our method expands the FP model into\nmultiple low-bit basis models. To ensure accurate quantization, we develop\nlow-bit basis model expansions at different granularities (tensor, layer,\nmodel), and theoretically confirm their convergence to the dense model, thus\nrestoring FP model accuracy. Additionally, we design AbelianAdd/Mul operations\nbetween isomorphic models in the low-bit expansion, forming an Abelian group to\nensure operation parallelism and commutativity. The experiments show that our\nalgorithm achieves state-of-the-art performance in low-bit settings; for\nexample, 4-bit quantization of ResNet-50 surpasses the original accuracy,\nreaching 77.03%. The code will be made public.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "f2uMHt8U0-7M36iFrLZMohS2eY5nh-IfazQ9UKySqsU",
  "pdfSize": "1794649"
}