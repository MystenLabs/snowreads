{"id":"2407.03036","title":"SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning","authors":"Bac Nguyen, Stefan Uhlich, Fabien Cardinaux, Lukas Mauch, Marzieh\n  Edraki, Aaron Courville","authorsParsed":[["Nguyen","Bac",""],["Uhlich","Stefan",""],["Cardinaux","Fabien",""],["Mauch","Lukas",""],["Edraki","Marzieh",""],["Courville","Aaron",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 11:56:55 GMT"}],"updateDate":"2024-07-04","timestamp":1720007815000,"abstract":"  Handling distribution shifts from training data, known as out-of-distribution\n(OOD) generalization, poses a significant challenge in the field of machine\nlearning. While a pre-trained vision-language model like CLIP has demonstrated\nremarkable zero-shot performance, further adaptation of the model to downstream\ntasks leads to undesirable degradation for OOD data. In this work, we introduce\nSparse Adaptation for Fine-Tuning (SAFT), a method that prevents fine-tuning\nfrom forgetting the general knowledge in the pre-trained model. SAFT only\nupdates a small subset of important parameters whose gradient magnitude is\nlarge, while keeping the other parameters frozen. SAFT is straightforward to\nimplement and conceptually simple. Extensive experiments show that with only\n0.1% of the model parameters, SAFT can significantly improve the performance of\nCLIP. It consistently outperforms baseline methods across several benchmarks.\nOn the few-shot learning benchmark of ImageNet and its variants, SAFT gives a\ngain of 5.15% on average over the conventional fine-tuning method in OOD\nsettings.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"PT25EAK_lCScbrDqiLKYVRfxbio8TUY8qm5c44tKdWQ","pdfSize":"3623193"}
