{"id":"2407.09781","title":"Dense Multimodal Alignment for Open-Vocabulary 3D Scene Understanding","authors":"Ruihuang Li, Zhengqiang Zhang, Chenhang He, Zhiyuan Ma, Vishal M.\n  Patel, Lei Zhang","authorsParsed":[["Li","Ruihuang",""],["Zhang","Zhengqiang",""],["He","Chenhang",""],["Ma","Zhiyuan",""],["Patel","Vishal M.",""],["Zhang","Lei",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 05:39:17 GMT"}],"updateDate":"2024-07-16","timestamp":1720849157000,"abstract":"  Recent vision-language pre-training models have exhibited remarkable\ngeneralization ability in zero-shot recognition tasks. Previous open-vocabulary\n3D scene understanding methods mostly focus on training 3D models using either\nimage or text supervision while neglecting the collective strength of all\nmodalities. In this work, we propose a Dense Multimodal Alignment (DMA)\nframework to densely co-embed different modalities into a common space for\nmaximizing their synergistic benefits. Instead of extracting coarse view- or\nregion-level text prompts, we leverage large vision-language models to extract\ncomplete category information and scalable scene descriptions to build the text\nmodality, and take image modality as the bridge to build dense point-pixel-text\nassociations. Besides, in order to enhance the generalization ability of the 2D\nmodel for downstream 3D tasks without compromising the open-vocabulary\ncapability, we employ a dual-path integration approach to combine frozen CLIP\nvisual features and learnable mask features. Extensive experiments show that\nour DMA method produces highly competitive open-vocabulary segmentation\nperformance on various indoor and outdoor tasks.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"FO5wBiY9Zkqv6WZAji0yQxvYathbcIxvBOh5NtEgDD4","pdfSize":"14157298"}