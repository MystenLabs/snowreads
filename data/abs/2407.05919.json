{"id":"2407.05919","title":"Fostering Trust and Quantifying Value of AI and ML","authors":"Dalmo Cirne and Veena Calambur","authorsParsed":[["Cirne","Dalmo",""],["Calambur","Veena",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 13:25:28 GMT"}],"updateDate":"2024-07-09","timestamp":1720445128000,"abstract":"  Artificial Intelligence (AI) and Machine Learning (ML) providers have a\nresponsibility to develop valid and reliable systems. Much has been discussed\nabout trusting AI and ML inferences (the process of running live data through a\ntrained AI model to make a prediction or solve a task), but little has been\ndone to define what that means. Those in the space of ML- based products are\nfamiliar with topics such as transparency, explainability, safety, bias, and so\nforth. Yet, there are no frameworks to quantify and measure those. Producing\never more trustworthy machine learning inferences is a path to increase the\nvalue of products (i.e., increased trust in the results) and to engage in\nconversations with users to gather feedback to improve products. In this paper,\nwe begin by examining the dynamic of trust between a provider (Trustor) and\nusers (Trustees). Trustors are required to be trusting and trustworthy, whereas\ntrustees need not be trusting nor trustworthy. The challenge for trustors is to\nprovide results that are good enough to make a trustee increase their level of\ntrust above a minimum threshold for: 1- doing business together; 2-\ncontinuation of service. We conclude by defining and proposing a framework, and\na set of viable metrics, to be used for computing a trust score and objectively\nunderstand how trustworthy a machine learning system can claim to be, plus\ntheir behavior over time.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pplblXpT6BICsX6K0AT6BeVNFg6KJy3-YJlN-zSvR80","pdfSize":"156389"}