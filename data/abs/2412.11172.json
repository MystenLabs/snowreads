{"id":"2412.11172","title":"Unpacking the Resilience of SNLI Contradiction Examples to Attacks","authors":"Chetan Verma and Archit Agarwal","authorsParsed":[["Verma","Chetan",""],["Agarwal","Archit",""]],"versions":[{"version":"v1","created":"Sun, 15 Dec 2024 12:47:28 GMT"}],"updateDate":"2024-12-17","timestamp":1734266848000,"abstract":"  Pre-trained models excel on NLI benchmarks like SNLI and MultiNLI, but their\ntrue language understanding remains uncertain. Models trained only on\nhypotheses and labels achieve high accuracy, indicating reliance on dataset\nbiases and spurious correlations. To explore this issue, we applied the\nUniversal Adversarial Attack to examine the model's vulnerabilities. Our\nanalysis revealed substantial drops in accuracy for the entailment and neutral\nclasses, whereas the contradiction class exhibited a smaller decline.\nFine-tuning the model on an augmented dataset with adversarial examples\nrestored its performance to near-baseline levels for both the standard and\nchallenge sets. Our findings highlight the value of adversarial triggers in\nidentifying spurious correlations and improving robustness while providing\ninsights into the resilience of the contradiction class to adversarial attacks.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"V-fPekKV0m-Wi8HjShtpWwxUA8zCjiJlWZfCb5EKjhE","pdfSize":"114899"}