{"id":"2407.16724","title":"Educating LLMs like Human Students: Structure-aware Injection of Domain\n  Knowledge","authors":"Kai Liu, Ze Chen, Zhihang Fu, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue\n  Wu, Jieping Ye","authorsParsed":[["Liu","Kai",""],["Chen","Ze",""],["Fu","Zhihang",""],["Jiang","Rongxin",""],["Zhou","Fan",""],["Chen","Yaowu",""],["Wu","Yue",""],["Ye","Jieping",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 12:38:48 GMT"}],"updateDate":"2024-07-25","timestamp":1721738328000,"abstract":"  This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AyqQF3PeqZFHp0nGtqC2r_drF5LFkM-pgVk9v2Fkl9M","pdfSize":"847518"}