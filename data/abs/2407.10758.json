{"id":"2407.10758","title":"Continual Deep Learning on the Edge via Stochastic Local Competition\n  among Subnetworks","authors":"Theodoros Christophides and Kyriakos Tolias and Sotirios Chatzis","authorsParsed":[["Christophides","Theodoros",""],["Tolias","Kyriakos",""],["Chatzis","Sotirios",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 14:36:05 GMT"}],"updateDate":"2024-07-16","timestamp":1721054165000,"abstract":"  Continual learning on edge devices poses unique challenges due to stringent\nresource constraints. This paper introduces a novel method that leverages\nstochastic competition principles to promote sparsity, significantly reducing\ndeep network memory footprint and computational demand. Specifically, we\npropose deep networks that comprise blocks of units that compete locally to win\nthe representation of each arising new task; competition takes place in a\nstochastic manner. This type of network organization results in sparse\ntask-specific representations from each network layer; the sparsity pattern is\nobtained during training and is different among tasks. Crucially, our method\nsparsifies both the weights and the weight gradients, thus facilitating\ntraining on edge devices. This is performed on the grounds of winning\nprobability for each unit in a block. During inference, the network retains\nonly the winning unit and zeroes-out all weights pertaining to non-winning\nunits for the task at hand. Thus, our approach is specifically tailored for\ndeployment on edge devices, providing an efficient and scalable solution for\ncontinual learning in resource-limited environments.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"UD2brGDw_aXS8sSRIEscQcIOuyV-Ns6s4qRqJXp0rho","pdfSize":"2245533"}