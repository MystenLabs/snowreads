{"id":"2412.11344","title":"Can AI Extract Antecedent Factors of Human Trust in AI? An Application\n  of Information Extraction for Scientific Literature in Behavioural and\n  Computer Sciences","authors":"Melanie McGrath, Harrison Bailey, Necva B\\\"ol\\\"uc\\\"u, Xiang Dai,\n  Sarvnaz Karimi, Cecile Paris","authorsParsed":[["McGrath","Melanie",""],["Bailey","Harrison",""],["Bölücü","Necva",""],["Dai","Xiang",""],["Karimi","Sarvnaz",""],["Paris","Cecile",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 00:02:38 GMT"}],"updateDate":"2024-12-17","timestamp":1734307358000,"abstract":"  Information extraction from the scientific literature is one of the main\ntechniques to transform unstructured knowledge hidden in the text into\nstructured data which can then be used for decision-making in down-stream\ntasks. One such area is Trust in AI, where factors contributing to human trust\nin artificial intelligence applications are studied. The relationships of these\nfactors with human trust in such applications are complex. We hence explore\nthis space from the lens of information extraction where, with the input of\ndomain experts, we carefully design annotation guidelines, create the first\nannotated English dataset in this domain, investigate an LLM-guided annotation,\nand benchmark it with state-of-the-art methods using large language models in\nnamed entity and relation extraction. Our results indicate that this problem\nrequires supervised learning which may not be currently feasible with\nprompt-based LLMs.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"iEt8MeTqNiGX9YwTRzcthGMT_Y4_troXIgDaI2v1m5M","pdfSize":"354439"}