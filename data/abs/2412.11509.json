{"id":"2412.11509","title":"Skip Tuning: Pre-trained Vision-Language Models are Effective and\n  Efficient Adapters Themselves","authors":"Shihan Wu, Ji Zhang, Pengpeng Zeng, Lianli Gao, Jingkuan Song, Heng\n  Tao Shen","authorsParsed":[["Wu","Shihan",""],["Zhang","Ji",""],["Zeng","Pengpeng",""],["Gao","Lianli",""],["Song","Jingkuan",""],["Shen","Heng Tao",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 07:33:23 GMT"}],"updateDate":"2024-12-17","timestamp":1734334403000,"abstract":"  Prompt tuning (PT) has long been recognized as an effective and efficient\nparadigm for transferring large pre-trained vision-language models (VLMs) to\ndownstream tasks by learning a tiny set of context vectors. Nevertheless, in\nthis work, we reveal that freezing the parameters of VLMs during learning the\ncontext vectors neither facilitates the transferability of pre-trained\nknowledge nor improves the memory and time efficiency significantly. Upon\nfurther investigation, we find that reducing both the length and width of the\nfeature-gradient propagation flows of the full fine-tuning (FT) baseline is key\nto achieving effective and efficient knowledge transfer. Motivated by this, we\npropose Skip Tuning, a novel paradigm for adapting VLMs to downstream tasks.\nUnlike existing PT or adapter-based methods, Skip Tuning applies Layer-wise\nSkipping (LSkip) and Class-wise Skipping (CSkip) upon the FT baseline without\nintroducing extra context vectors or adapter modules. Extensive experiments\nacross a wide spectrum of benchmarks demonstrate the superior effectiveness and\nefficiency of our Skip Tuning over both PT and adapter-based methods. Code:\nhttps://github.com/Koorye/SkipTuning.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"muFVJrCIDUWYYZ218zpGWuUjMneR51764RrF0UYMVlY","pdfSize":"5112542"}