{"id":"2407.02827","title":"Convergence of Implicit Gradient Descent for Training Two-Layer\n  Physics-Informed Neural Networks","authors":"Xianliang Xu, Ting Du, Wang Kong, Ye Li and Zhongyi Huang","authorsParsed":[["Xu","Xianliang",""],["Du","Ting",""],["Kong","Wang",""],["Li","Ye",""],["Huang","Zhongyi",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 06:10:41 GMT"},{"version":"v2","created":"Sat, 10 Aug 2024 13:30:32 GMT"}],"updateDate":"2024-08-13","timestamp":1719987041000,"abstract":"  Optimization algorithms are crucial in training physics-informed neural\nnetworks (PINNs), as unsuitable methods may lead to poor solutions. Compared to\nthe common gradient descent (GD) algorithm, implicit gradient descent (IGD)\noutperforms it in handling certain multi-scale problems. In this paper, we\nprovide convergence analysis for the IGD in training over-parameterized\ntwo-layer PINNs. We first demonstrate the positive definiteness of Gram\nmatrices for some general smooth activation functions, such as sigmoidal\nfunction, softplus function, tanh function, and others. Then,\nover-parameterization allows us to prove that the randomly initialized IGD\nconverges a globally optimal solution at a linear convergence rate. Moreover,\ndue to the distinct training dynamics of IGD compared to GD, the learning rate\ncan be selected independently of the sample size and the least eigenvalue of\nthe Gram matrix. Additionally, the novel approach used in our convergence\nanalysis imposes a milder requirement on the network width. Finally, empirical\nresults validate our theoretical findings.\n","subjects":["Computing Research Repository/Machine Learning","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"kMm98CyferVr6J3IEaIatiBDtV-xAWyiMMTCsh11E-Q","pdfSize":"513691"}