{
  "id": "2412.06868",
  "title": "Compression for Better: A General and Stable Lossless Compression\n  Framework",
  "authors": "Boyang Zhang, Daning Cheng, Yunquan Zhang, Fangmin Liu, Wenguang Chen",
  "authorsParsed": [
    [
      "Zhang",
      "Boyang",
      ""
    ],
    [
      "Cheng",
      "Daning",
      ""
    ],
    [
      "Zhang",
      "Yunquan",
      ""
    ],
    [
      "Liu",
      "Fangmin",
      ""
    ],
    [
      "Chen",
      "Wenguang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 09:55:54 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733738154000,
  "abstract": "  This work focus on how to stabilize and lossless model compression, aiming to\nreduce model complexity and enhance efficiency without sacrificing performance\ndue to compression errors. A key challenge is effectively leveraging\ncompression errors and defining the boundaries for lossless compression to\nminimize model loss. i.e., compression for better. Currently, there is no\nsystematic approach to determining this error boundary or understanding its\nspecific impact on model performance. We propose a general\n\\textbf{L}oss\\textbf{L}ess \\textbf{C}ompression theoretical framework\n(\\textbf{LLC}), which further delineates the compression neighborhood and\nhigher-order analysis boundaries through the total differential, thereby\nspecifying the error range within which a model can be compressed without loss.\nTo verify the effectiveness of LLC, we apply various compression techniques,\nincluding quantization and decomposition. Specifically, for quantization, we\nreformulate the classic quantization search problem as a grouped knapsack\nproblem within the lossless neighborhood, achieving lossless quantization while\nimproving computational efficiency. For decomposition, LLC addresses the\napproximation problem under low-rank constraints, automatically determining the\nrank for each layer and producing lossless low-rank models. We conduct\nextensive experiments on multiple neural network architectures on different\ndatasets. The results show that without fancy tricks, LLC can effectively\nachieve lossless model compression. Our code will be made publicly.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "fumJg9uhiaUNaAHskVnMKmqDFXPrEBaVlVPq0qgK2aw",
  "pdfSize": "1166150"
}