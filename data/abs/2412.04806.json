{"id":"2412.04806","title":"Rethinking Time Series Forecasting with LLMs via Nearest Neighbor\n  Contrastive Learning","authors":"Jayanie Bogahawatte, Sachith Seneviratne, Maneesha Perera, Saman\n  Halgamuge","authorsParsed":[["Bogahawatte","Jayanie",""],["Seneviratne","Sachith",""],["Perera","Maneesha",""],["Halgamuge","Saman",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 06:32:47 GMT"}],"updateDate":"2024-12-09","timestamp":1733466767000,"abstract":"  Adapting Large Language Models (LLMs) that are extensively trained on\nabundant text data, and customizing the input prompt to enable time series\nforecasting has received considerable attention. While recent work has shown\ngreat potential for adapting the learned prior of LLMs, the formulation of the\nprompt to finetune LLMs remains challenging as prompt should be aligned with\ntime series data. Additionally, current approaches do not effectively leverage\nword token embeddings which embody the rich representation space learned by\nLLMs. This emphasizes the need for a robust approach to formulate the prompt\nwhich utilizes the word token embeddings while effectively representing the\ncharacteristics of the time series. To address these challenges, we propose\nNNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting\nvia LLMs. First, we generate time series compatible text prototypes such that\neach text prototype represents both word token embeddings in its neighborhood\nand time series characteristics via end-to-end finetuning. Next, we draw\ninspiration from Nearest Neighbor Contrastive Learning to formulate the prompt\nwhile obtaining the top-$k$ nearest neighbor time series compatible text\nprototypes. We then fine-tune the layer normalization and positional embeddings\nof the LLM, keeping the other layers intact, reducing the trainable parameters\nand decreasing the computational cost. Our comprehensive experiments\ndemonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving\ncompetitive or superior performance over the state-of-the-art methods in\nlong-term and short-term forecasting tasks.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"t0onDqCApG4ir8KjGxmJQvJRUkrk7HMjKlBSl_97TkY","pdfSize":"3046720"}