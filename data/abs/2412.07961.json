{
  "id": "2412.07961",
  "title": "Forking Paths in Neural Text Generation",
  "authors": "Eric Bigelow, Ari Holtzman, Hidenori Tanaka, Tomer Ullman",
  "authorsParsed": [
    [
      "Bigelow",
      "Eric",
      ""
    ],
    [
      "Holtzman",
      "Ari",
      ""
    ],
    [
      "Tanaka",
      "Hidenori",
      ""
    ],
    [
      "Ullman",
      "Tomer",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 22:57:57 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733871477000,
  "abstract": "  Estimating uncertainty in Large Language Models (LLMs) is important for\nproperly evaluating LLMs, and ensuring safety for users. However, prior\napproaches to uncertainty estimation focus on the final answer in generated\ntext, ignoring intermediate steps that might dramatically impact the outcome.\nWe hypothesize that there exist key forking tokens, such that re-sampling the\nsystem at those specific tokens, but not others, leads to very different\noutcomes. To test this empirically, we develop a novel approach to representing\nuncertainty dynamics across individual tokens of text generation, and applying\nstatistical models to test our hypothesis. Our approach is highly flexible: it\ncan be applied to any dataset and any LLM, without fine tuning or accessing\nmodel weights. We use our method to analyze LLM responses on 7 different tasks\nacross 4 domains, spanning a wide range of typical use cases. We find many\nexamples of forking tokens, including surprising ones such as punctuation\nmarks, suggesting that LLMs are often just a single token away from saying\nsomething very different.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "TqfnjCJxslKxtCLmeuMn07AysQgX8UtJjbdnWl9oAIg",
  "pdfSize": "4200944"
}