{"id":"2412.13337","title":"Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small\n  LLMs","authors":"Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wang, Krishnateja\n  Killamsetty, Shivchander Sudalairaj, Wenlong Zhao, Seungwook Han, Abhishek\n  Bhandwaldar, Guangxuan Xu, Kai Xu, Ligong Han, Luke Inglis, Akash Srivastava","authorsParsed":[["Pareja","Aldo",""],["Nayak","Nikhil Shivakumar",""],["Wang","Hao",""],["Killamsetty","Krishnateja",""],["Sudalairaj","Shivchander",""],["Zhao","Wenlong",""],["Han","Seungwook",""],["Bhandwaldar","Abhishek",""],["Xu","Guangxuan",""],["Xu","Kai",""],["Han","Ligong",""],["Inglis","Luke",""],["Srivastava","Akash",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 21:16:59 GMT"}],"updateDate":"2024-12-19","timestamp":1734470219000,"abstract":"  The rise of large language models (LLMs) has created a significant disparity:\nindustrial research labs with their computational resources, expert teams, and\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\ndevelopers and small organizations face barriers due to limited resources. In\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\nparameters) for their cost-efficiency and accessibility. We explore various\ntraining configurations and strategies across four open-source pre-trained\nmodels. We provide detailed documentation of these configurations, revealing\nfindings that challenge several common training practices, including\nhyperparameter recommendations from TULU and phased training recommended by\nOrca. Key insights from our work include: (i) larger batch sizes paired with\nlower learning rates lead to improved model performance on benchmarks such as\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\nsuch as lower gradient norms and higher loss values, are strong indicators of\nbetter final model performance, enabling early termination of sub-optimal runs\nand significant computational savings; (iii) through a thorough exploration of\nhyperparameters like warmup steps and learning rate schedules, we provide\nguidance for practitioners and find that certain simplifications do not\ncompromise performance; and (iv) we observed no significant difference in\nperformance between phased and stacked training strategies, but stacked\ntraining is simpler and more sample efficient. With these findings holding\nrobustly across datasets and models, we hope this study serves as a guide for\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\nfor LLM research.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"DyxaYoZyd_9tm5pBAKCo4ySYTKruzAm0CZzjs-Z3WJY","pdfSize":"2577398"}