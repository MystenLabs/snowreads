{"id":"2407.17723","title":"Your Graph Recommender is Provably a Single-view Graph Contrastive\n  Learning","authors":"Wenjie Yang, Shengzhong Zhang, Jiaxing Guo, Zengfeng Huang","authorsParsed":[["Yang","Wenjie",""],["Zhang","Shengzhong",""],["Guo","Jiaxing",""],["Huang","Zengfeng",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 02:53:11 GMT"}],"updateDate":"2024-07-26","timestamp":1721875991000,"abstract":"  Graph recommender (GR) is a type of graph neural network (GNNs) encoder that\nis customized for extracting information from the user-item interaction graph.\nDue to its strong performance on the recommendation task, GR has gained\nsignificant attention recently. Graph contrastive learning (GCL) is also a\npopular research direction that aims to learn, often unsupervised, GNNs with\ncertain contrastive objectives. As a general graph representation learning\nmethod, GCLs have been widely adopted with the supervised recommendation loss\nfor joint training of GRs. Despite the intersection of GR and GCL research,\ntheoretical understanding of the relationship between the two fields is\nsurprisingly sparse. This vacancy inevitably leads to inefficient scientific\nresearch.\n  In this paper, we aim to bridge the gap between the field of GR and GCL from\nthe perspective of encoders and loss functions. With mild assumptions, we\ntheoretically show an astonishing fact that graph recommender is equivalent to\na commonly-used single-view graph contrastive model. Specifically, we find that\n(1) the classic encoder in GR is essentially a linear graph convolutional\nnetwork with one-hot inputs, and (2) the loss function in GR is well bounded by\na single-view GCL loss with certain hyperparameters. The first observation\nenables us to explain crucial designs of GR models, e.g., the removal of\nself-loop and nonlinearity. And the second finding can easily prompt many\ncross-field research directions. We empirically show a remarkable result that\nthe recommendation loss and the GCL loss can be used interchangeably. The fact\nthat we can train GR models solely with the GCL loss is particularly\ninsightful, since before this work, GCLs were typically viewed as unsupervised\nmethods that need fine-tuning. We also discuss some potential future works\ninspired by our theory.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uyR5l6Kt9J9vpNNqMxFHB4Yx3Swrn5HSrasdUcKoM7o","pdfSize":"807018"}