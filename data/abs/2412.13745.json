{
  "id": "2412.13745",
  "title": "Learning Complex Word Embeddings in Classical and Quantum Spaces",
  "authors": "Carys Harvey, Stephen Clark, Douglas Brown and Konstantinos\n  Meichanetzidis",
  "authorsParsed": [
    [
      "Harvey",
      "Carys",
      ""
    ],
    [
      "Clark",
      "Stephen",
      ""
    ],
    [
      "Brown",
      "Douglas",
      ""
    ],
    [
      "Meichanetzidis",
      "Konstantinos",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 11:26:51 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734521211000,
  "abstract": "  We present a variety of methods for training complex-valued word embeddings,\nbased on the classical Skip-gram model, with a straightforward adaptation\nsimply replacing the real-valued vectors with arbitrary vectors of complex\nnumbers. In a more \"physically-inspired\" approach, the vectors are produced by\nparameterised quantum circuits (PQCs), which are unitary transformations\nresulting in normalised vectors which have a probabilistic interpretation. We\ndevelop a complex-valued version of the highly optimised C code version of\nSkip-gram, which allows us to easily produce complex embeddings trained on a\n3.8B-word corpus for a vocabulary size of over 400k, for which we are then able\nto train a separate PQC for each word. We evaluate the complex embeddings on a\nset of standard similarity and relatedness datasets, for some models obtaining\nresults competitive with the classical baseline. We find that, while training\nthe PQCs directly tends to harm performance, the quantum word embeddings from\nthe two-stage process perform as well as the classical Skip-gram embeddings\nwith comparable numbers of parameters. This enables a highly scalable route to\nlearning embeddings in complex spaces which scales with the size of the\nvocabulary rather than the size of the training corpus. In summary, we\ndemonstrate how to produce a large set of high-quality word embeddings for use\nin complex-valued and quantum-inspired NLP models, and for exploring potential\nadvantage in quantum NLP models.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "5lLtZ3auMhw2FC2AeKbuGAF21c00odkeYXbc3nt28k4",
  "pdfSize": "1213182"
}