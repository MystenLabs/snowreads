{"id":"2412.14587","title":"Spike2Former: Efficient Spiking Transformer for High-performance Image\n  Segmentation","authors":"Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li","authorsParsed":[["Lei","Zhenxin",""],["Yao","Man",""],["Hu","Jiakui",""],["Luo","Xinhao",""],["Lu","Yanye",""],["Xu","Bo",""],["Li","Guoqi",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 07:13:15 GMT"}],"updateDate":"2024-12-20","timestamp":1734592395000,"abstract":"  Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly\nin image segmentation tasks. The reason is that directly converting neural\nnetworks with complex architectural designs for segmentation tasks into spiking\nversions leads to performance degradation and non-convergence. To address this\nchallenge, we first identify the modules in the architecture design that lead\nto the severe reduction in spike firing, make targeted improvements, and\npropose Spike2Former architecture. Second, we propose normalized integer\nspiking neurons to solve the training stability problem of SNNs with complex\narchitectures. We set a new state-of-the-art for SNNs in various semantic\nsegmentation datasets, with a significant improvement of +12.7% mIoU and 5.0\nefficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU\nand 6.6 efficiency on CityScapes.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence","Computer Science/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uw1JXm3hzCxuz6qwQIDEPaP7vpsejLFYRbX5d6PG3zo","pdfSize":"5876316"}