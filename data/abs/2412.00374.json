{
  "id": "2412.00374",
  "title": "LQ-Adapter: ViT-Adapter with Learnable Queries for Gallbladder Cancer\n  Detection from Ultrasound Image",
  "authors": "Chetan Madan, Mayuna Gupta, Soumen Basu, Pankaj Gupta, Chetan Arora",
  "authorsParsed": [
    [
      "Madan",
      "Chetan",
      ""
    ],
    [
      "Gupta",
      "Mayuna",
      ""
    ],
    [
      "Basu",
      "Soumen",
      ""
    ],
    [
      "Gupta",
      "Pankaj",
      ""
    ],
    [
      "Arora",
      "Chetan",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 30 Nov 2024 06:51:13 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1732949473000,
  "abstract": "  We focus on the problem of Gallbladder Cancer (GBC) detection from Ultrasound\n(US) images. The problem presents unique challenges to modern Deep Neural\nNetwork (DNN) techniques due to low image quality arising from noise, textures,\nand viewpoint variations. Tackling such challenges would necessitate precise\nlocalization performance by the DNN to identify the discerning features for the\ndownstream malignancy prediction. While several techniques have been proposed\nin the recent years for the problem, all of these methods employ complex custom\narchitectures. Inspired by the success of foundational models for natural image\ntasks, along with the use of adapters to fine-tune such models for the custom\ntasks, we investigate the merit of one such design, ViT-Adapter, for the GBC\ndetection problem. We observe that ViT-Adapter relies predominantly on a\nprimitive CNN-based spatial prior module to inject the localization information\nvia cross-attention, which is inefficient for our problem due to the small\npathology sizes, and variability in their appearances due to non-regular\nstructure of the malignancy. In response, we propose, LQ-Adapter, a modified\nAdapter design for ViT, which improves localization information by leveraging\nlearnable content queries over the basic spatial prior module. Our method\nsurpasses existing approaches, enhancing the mean IoU (mIoU) scores by 5.4%,\n5.8%, and 2.7% over ViT-Adapters, DINO, and FocalNet-DINO, respectively on the\nUS image-based GBC detection dataset, and establishing a new state-of-the-art\n(SOTA). Additionally, we validate the applicability and effectiveness of\nLQ-Adapter on the Kvasir-Seg dataset for polyp detection from colonoscopy\nimages. Superior performance of our design on this problem as well showcases\nits capability to handle diverse medical imaging tasks across different\ndatasets. Code is released at https://github.com/ChetanMadan/LQ-Adapter\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "ifdC2Tlh1ZDgaD6aqXm7f3RqVnhVyMYSM0cjHFRJ4H4",
  "pdfSize": "1845807"
}