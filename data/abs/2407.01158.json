{"id":"2407.01158","title":"Learning to Explore and Select for Coverage-Conditioned\n  Retrieval-Augmented Generation","authors":"Takyoung Kim, Kyungjae Lee, Young Rok Jang, Ji Yong Cho, Gangwoo Kim,\n  Minseok Cho, Moontae Lee","authorsParsed":[["Kim","Takyoung",""],["Lee","Kyungjae",""],["Jang","Young Rok",""],["Cho","Ji Yong",""],["Kim","Gangwoo",""],["Cho","Minseok",""],["Lee","Moontae",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 10:26:19 GMT"}],"updateDate":"2024-07-02","timestamp":1719829579000,"abstract":"  Interactions with billion-scale large language models typically yield\nlong-form responses due to their extensive parametric capacities, along with\nretrieval-augmented features. While detailed responses provide insightful\nviewpoint of a specific subject, they frequently generate redundant and less\nengaging content that does not meet user interests. In this work, we focus on\nthe role of query outlining (i.e., selected sequence of queries) in scenarios\nthat users request a specific range of information, namely coverage-conditioned\n($C^2$) scenarios. For simulating $C^2$ scenarios, we construct QTree, 10K sets\nof information-seeking queries decomposed with various perspectives on certain\ntopics. By utilizing QTree, we train QPlanner, a 7B language model generating\ncustomized query outlines that follow coverage-conditioned queries. We analyze\nthe effectiveness of generated outlines through automatic and human evaluation,\ntargeting on retrieval-augmented generation (RAG). Moreover, the experimental\nresults demonstrate that QPlanner with alignment training can further provide\noutlines satisfying diverse user interests. Our resources are available at\nhttps://github.com/youngerous/qtree.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"TgyfcydbpH6bLnATmD-IdKxRYVy4FuVU3iDlEBVL5k8","pdfSize":"2022811"}