{"id":"2412.09586","title":"Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders","authors":"Fiona Ryan, Ajay Bati, Sangmin Lee, Daniel Bolya, Judy Hoffman, James\n  M. Rehg","authorsParsed":[["Ryan","Fiona",""],["Bati","Ajay",""],["Lee","Sangmin",""],["Bolya","Daniel",""],["Hoffman","Judy",""],["Rehg","James M.",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 18:55:30 GMT"}],"updateDate":"2024-12-13","timestamp":1734029730000,"abstract":"  We address the problem of gaze target estimation, which aims to predict where\na person is looking in a scene. Predicting a person's gaze target requires\nreasoning both about the person's appearance and the contents of the scene.\nPrior works have developed increasingly complex, hand-crafted pipelines for\ngaze target estimation that carefully fuse features from separate scene\nencoders, head encoders, and auxiliary models for signals like depth and pose.\nMotivated by the success of general-purpose feature extractors on a variety of\nvisual tasks, we propose Gaze-LLE, a novel transformer framework that\nstreamlines gaze target estimation by leveraging features from a frozen DINOv2\nencoder. We extract a single feature representation for the scene, and apply a\nperson-specific positional prompt to decode gaze with a lightweight module. We\ndemonstrate state-of-the-art performance across several gaze benchmarks and\nprovide extensive analysis to validate our design choices. Our code is\navailable at: http://github.com/fkryan/gazelle .\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HJhR7BULI3OCE220CGyn5steBTcBqNV78ykdJSI0OsM","pdfSize":"9389597"}