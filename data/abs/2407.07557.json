{"id":"2407.07557","title":"Federated Foundation Model for Cardiac CT Imaging","authors":"Malte T\\\"olle, Philipp Garthe, Clemens Scherer, Jan Moritz Seliger,\n  Andreas Leha, Nina Kr\\\"uger, Stefan Simm, Simon Martin, Sebastian Eble,\n  Halvar Kelm, Moritz Bednorz, Florian Andr\\'e, Peter Bannas, Gerhard Diller,\n  Norbert Frey, Stefan Gro{\\ss}, Anja Hennemuth, Lars Kaderali, Alexander\n  Meyer, Eike Nagel, Stefan Orwat, Moritz Seiffert, Tim Friede, Tim Seidler,\n  Sandy Engelhardt","authorsParsed":[["Tölle","Malte",""],["Garthe","Philipp",""],["Scherer","Clemens",""],["Seliger","Jan Moritz",""],["Leha","Andreas",""],["Krüger","Nina",""],["Simm","Stefan",""],["Martin","Simon",""],["Eble","Sebastian",""],["Kelm","Halvar",""],["Bednorz","Moritz",""],["André","Florian",""],["Bannas","Peter",""],["Diller","Gerhard",""],["Frey","Norbert",""],["Groß","Stefan",""],["Hennemuth","Anja",""],["Kaderali","Lars",""],["Meyer","Alexander",""],["Nagel","Eike",""],["Orwat","Stefan",""],["Seiffert","Moritz",""],["Friede","Tim",""],["Seidler","Tim",""],["Engelhardt","Sandy",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 11:30:50 GMT"}],"updateDate":"2024-07-11","timestamp":1720611050000,"abstract":"  Federated learning (FL) is a renowned technique for utilizing decentralized\ndata while preserving privacy. However, real-world applications often involve\ninherent challenges such as partially labeled datasets, where not all clients\npossess expert annotations of all labels of interest, leaving large portions of\nunlabeled data unused. In this study, we conduct the largest federated cardiac\nCT imaging analysis to date, focusing on partially labeled datasets ($n=8,124$)\nof Transcatheter Aortic Valve Implantation (TAVI) patients over eight hospital\nclients. Transformer architectures, which are the major building blocks of\ncurrent foundation models, have shown superior performance when trained on\nlarger cohorts than traditional CNNs. However, when trained on small\ntask-specific labeled sample sizes, it is currently not feasible to exploit\ntheir underlying attention mechanism for improved performance. Therefore, we\ndeveloped a two-stage semi-supervised learning strategy that distills knowledge\nfrom several task-specific CNNs (landmark detection and segmentation of\ncalcification) into a single transformer model by utilizing large amounts of\nunlabeled data typically residing unused in hospitals to mitigate these issues.\nThis method not only improves the predictive accuracy and generalizability of\ntransformer-based architectures but also facilitates the simultaneous learning\nof all partial labels within a single transformer model across the federation.\nAdditionally, we show that our transformer-based model extracts more meaningful\nfeatures for further downstream tasks than the UNet-based one by only training\nthe last layer to also solve segmentation of coronary arteries. We make the\ncode and weights of the final model openly available, which can serve as a\nfoundation model for further research in cardiac CT imaging.\n","subjects":["Electrical Engineering and Systems Science/Image and Video Processing","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"GaKRz2xdHGBvAc0l4iv1KG16wHFwVl9vHXXo2kJQH9I","pdfSize":"5496914"}