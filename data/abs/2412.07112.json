{"id":"2412.07112","title":"Maya: An Instruction Finetuned Multilingual Multimodal Model","authors":"Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung,\n  Bala Krishna S Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, S\n  M Iftekhar Uddin, Shayekh Bin Islam, Roshan Santhosh, Snegha A, Drishti\n  Sharma, Chen Liu, Isha Chaturvedi, Genta Indra Winata, Ashvanth.S, Snehanshu\n  Mukherjee, Alham Fikri Aji","authorsParsed":[["Alam","Nahid",""],["Kanjula","Karthik Reddy",""],["Guthikonda","Surya",""],["Chung","Timothy",""],["Vegesna","Bala Krishna S",""],["Das","Abhipsha",""],["Susevski","Anthony",""],["Chan","Ryan Sze-Yin",""],["Uddin","S M Iftekhar",""],["Islam","Shayekh Bin",""],["Santhosh","Roshan",""],["A","Snegha",""],["Sharma","Drishti",""],["Liu","Chen",""],["Chaturvedi","Isha",""],["Winata","Genta Indra",""],["S","Ashvanth.",""],["Mukherjee","Snehanshu",""],["Aji","Alham Fikri",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 01:57:17 GMT"}],"updateDate":"2024-12-11","timestamp":1733795837000,"abstract":"  The rapid development of large Vision-Language Models (VLMs) has led to\nimpressive results on academic benchmarks, primarily in widely spoken\nlanguages. However, significant gaps remain in the ability of current VLMs to\nhandle low-resource languages and varied cultural contexts, largely due to a\nlack of high-quality, diverse, and safety-vetted data. Consequently, these\nmodels often struggle to understand low-resource languages and cultural nuances\nin a manner free from toxicity. To address these limitations, we introduce\nMaya, an open-source Multimodal Multilingual model. Our contributions are\nthreefold: 1) a multilingual image-text pretraining dataset in eight languages,\nbased on the LLaVA pretraining dataset; 2) a thorough analysis of toxicity\nwithin the LLaVA dataset, followed by the creation of a novel toxicity-free\nversion across eight languages; and 3) a multilingual image-text model\nsupporting these languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"G40q5Yo5OBqah8bot2ZNQRxU0wkOnTlqykuKDX0SMgo","pdfSize":"2825087"}