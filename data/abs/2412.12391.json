{"id":"2412.12391","title":"Efficient Scaling of Diffusion Transformers for Text-to-Image Generation","authors":"Hao Li, Shamit Lal, Zhiheng Li, Yusheng Xie, Ying Wang, Yang Zou,\n  Orchid Majumder, R. Manmatha, Zhuowen Tu, Stefano Ermon, Stefano Soatto,\n  Ashwin Swaminathan","authorsParsed":[["Li","Hao",""],["Lal","Shamit",""],["Li","Zhiheng",""],["Xie","Yusheng",""],["Wang","Ying",""],["Zou","Yang",""],["Majumder","Orchid",""],["Manmatha","R.",""],["Tu","Zhuowen",""],["Ermon","Stefano",""],["Soatto","Stefano",""],["Swaminathan","Ashwin",""]],"versions":[{"version":"v1","created":"Mon, 16 Dec 2024 22:59:26 GMT"}],"updateDate":"2024-12-18","timestamp":1734389966000,"abstract":"  We empirically study the scaling properties of various Diffusion Transformers\n(DiTs) for text-to-image generation by performing extensive and rigorous\nablations, including training scaled DiTs ranging from 0.3B upto 8B parameters\non datasets up to 600M images. We find that U-ViT, a pure self-attention based\nDiT model provides a simpler design and scales more effectively in comparison\nwith cross-attention based DiT variants, which allows straightforward expansion\nfor extra conditions and other modalities. We identify a 2.3B U-ViT model can\nget better performance than SDXL UNet and other DiT variants in controlled\nsetting. On the data scaling side, we investigate how increasing dataset size\nand enhanced long caption improve the text-image alignment performance and the\nlearning efficiency.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dVyFmhzHd_CZ3uhSjxsasuN4TmZajrAtXREqMNAtIWU","pdfSize":"6955424"}