{"id":"2407.20279","title":"Robust and Efficient Transfer Learning via Supernet Transfer in\n  Warm-started Neural Architecture Search","authors":"Prabhant Singh, Joaquin Vanschoren","authorsParsed":[["Singh","Prabhant",""],["Vanschoren","Joaquin",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 00:17:57 GMT"}],"updateDate":"2024-07-31","timestamp":1721953077000,"abstract":"  Hand-designing Neural Networks is a tedious process that requires significant\nexpertise. Neural Architecture Search (NAS) frameworks offer a very useful and\npopular solution that helps to democratize AI. However, these NAS frameworks\nare often computationally expensive to run, which limits their applicability\nand accessibility. In this paper, we propose a novel transfer learning\napproach, capable of effectively transferring pretrained supernets based on\nOptimal Transport or multi-dataset pretaining. This method can be generally\napplied to NAS methods based on Differentiable Architecture Search (DARTS).\nThrough extensive experiments across dozens of image classification tasks, we\ndemonstrate that transferring pretrained supernets in this way can not only\ndrastically speed up the supernet training which then finds optimal models (3\nto 5 times faster on average), but even yield that outperform those found when\nrunning DARTS methods from scratch. We also observe positive transfer to almost\nall target datasets, making it very robust. Besides drastically improving the\napplicability of NAS methods, this also opens up new applications for continual\nlearning and related fields.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0tXbuWwKQfDqIeeiVGSyRjo8D55iAQBeBT9PAeI77kw","pdfSize":"1068090","objectId":"0x413901768df4697d7b59a86fc4f6f34ee1427eecca6e2d08d2685fbf6783c7bb","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
