{
  "id": "2412.12706",
  "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
  "authors": "Jiebin Zhang, Dawei Zhu, Yifan Song, Wenhao Wu, Chuqiao Kuang,\n  Xiaoguang Li, Lifeng Shang, Qun Liu, Sujian Li",
  "authorsParsed": [
    [
      "Zhang",
      "Jiebin",
      ""
    ],
    [
      "Zhu",
      "Dawei",
      ""
    ],
    [
      "Song",
      "Yifan",
      ""
    ],
    [
      "Wu",
      "Wenhao",
      ""
    ],
    [
      "Kuang",
      "Chuqiao",
      ""
    ],
    [
      "Li",
      "Xiaoguang",
      ""
    ],
    [
      "Shang",
      "Lifeng",
      ""
    ],
    [
      "Liu",
      "Qun",
      ""
    ],
    [
      "Li",
      "Sujian",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 09:20:31 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 20 Feb 2025 12:14:49 GMT"
    }
  ],
  "updateDate": "2025-02-21",
  "timestamp": 1734427231000,
  "abstract": "  As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "zvvqzXAuDV3fC_Mcuf1h19ftUxhHVkKTEQOL9-8x8vI",
  "pdfSize": "584984"
}