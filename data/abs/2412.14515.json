{"id":"2412.14515","title":"Relational Programming with Foundation Models","authors":"Ziyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William\n  Dodds, Neelay Velingker, Rajeev Alur, Mayur Naik","authorsParsed":[["Li","Ziyang",""],["Huang","Jiani",""],["Liu","Jason",""],["Zhu","Felix",""],["Zhao","Eric",""],["Dodds","William",""],["Velingker","Neelay",""],["Alur","Rajeev",""],["Naik","Mayur",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 04:26:45 GMT"}],"updateDate":"2024-12-20","timestamp":1734582405000,"abstract":"  Foundation models have vast potential to enable diverse AI applications. The\npowerful yet incomplete nature of these models has spurred a wide range of\nmechanisms to augment them with capabilities such as in-context learning,\ninformation retrieval, and code interpreting. We propose Vieira, a declarative\nframework that unifies these mechanisms in a general solution for programming\nwith foundation models. Vieira follows a probabilistic relational paradigm and\ntreats foundation models as stateless functions with relational inputs and\noutputs. It supports neuro-symbolic applications by enabling the seamless\ncombination of such models with logic programs, as well as complex, multi-modal\napplications by streamlining the composition of diverse sub-models. We\nimplement Vieira by extending the Scallop compiler with a foreign interface\nthat supports foundation models as plugins. We implement plugins for 12\nfoundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9\nchallenging tasks that span language, vision, and structured and vector\ndatabases. Our evaluation shows that programs in Vieira are concise, can\nincorporate modern foundation models, and have comparable or better accuracy\nthan competitive baselines.\n","subjects":["Computer Science/Artificial Intelligence","Computer Science/Programming Languages"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"0NzALexlgl2ydvz2e5L32vXT_YiXrzVB6Ry3J3VYVEI","pdfSize":"22237334"}