{"id":"2407.15504","title":"Fundamental Limits of Prompt Compression: A Rate-Distortion Framework\n  for Black-Box Language Models","authors":"Adway Girish, Alliot Nagle, Marco Bondaschi, Michael Gastpar, Ashok\n  Vardhan Makkuva, Hyeji Kim","authorsParsed":[["Girish","Adway",""],["Nagle","Alliot",""],["Bondaschi","Marco",""],["Gastpar","Michael",""],["Makkuva","Ashok Vardhan",""],["Kim","Hyeji",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 09:40:13 GMT"}],"updateDate":"2024-07-23","timestamp":1721641213000,"abstract":"  We formalize the problem of prompt compression for large language models\n(LLMs) and present a framework to unify token-level prompt compression methods\nwhich create hard prompts for black-box models. We derive the distortion-rate\nfunction for this setup as a linear program, and provide an efficient algorithm\nto compute this fundamental limit via the dual of the linear program. Using the\ndistortion-rate function as the baseline, we study the performance of existing\ncompression schemes on a synthetic dataset consisting of prompts generated from\na Markov chain, natural language queries, and their respective answers. Our\nempirical analysis demonstrates the criticality of query-aware prompt\ncompression, where the compressor has knowledge of the downstream task/query\nfor the black-box LLM. We show that there is a large gap between the\nperformance of current prompt compression methods and the optimal strategy, and\npropose a query-aware, variable-rate adaptation of a prior work to close the\ngap. We extend our experiments to a small natural language dataset to further\nconfirm our findings on our synthetic dataset.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language","Computing Research Repository/Information Theory","Mathematics/Information Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"cJtvnpR87PLLq0-ahBaFyTm06pZHoHjFn8o4kNg2hvc","pdfSize":"1521316"}