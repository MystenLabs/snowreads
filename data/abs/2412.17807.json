{"id":"2412.17807","title":"Cross-View Referring Multi-Object Tracking","authors":"Sijia Chen, En Yu, Wenbing Tao","authorsParsed":[["Chen","Sijia",""],["Yu","En",""],["Tao","Wenbing",""]],"versions":[{"version":"v1","created":"Mon, 23 Dec 2024 18:58:39 GMT"}],"updateDate":"2024-12-24","timestamp":1734980319000,"abstract":"  Referring Multi-Object Tracking (RMOT) is an important topic in the current\ntracking field. Its task form is to guide the tracker to track objects that\nmatch the language description. Current research mainly focuses on referring\nmulti-object tracking under single-view, which refers to a view sequence or\nmultiple unrelated view sequences. However, in the single-view, some\nappearances of objects are easily invisible, resulting in incorrect matching of\nobjects with the language description. In this work, we propose a new task,\ncalled Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the\ncross-view to obtain the appearances of objects from multiple views, avoiding\nthe problem of the invisible appearances of objects in RMOT task. CRMOT is a\nmore challenging task of accurately tracking the objects that match the\nlanguage description and maintaining the identity consistency of objects in\neach cross-view. To advance CRMOT task, we construct a cross-view referring\nmulti-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named\nCRTrack. Specifically, it provides 13 different scenes and 221 language\ndescriptions. Furthermore, we propose an end-to-end cross-view referring\nmulti-object tracking method, named CRTracker. Extensive experiments on the\nCRTrack benchmark verify the effectiveness of our method. The dataset and code\nare available at https://github.com/chen-si-jia/CRMOT.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"MH_zkQzbTZQjxbTvMx7Mv8sgbpgQTT-09It_r4h0q1w","pdfSize":"45719055"}