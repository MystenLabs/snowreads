{"id":"2412.05184","title":"QueEn: A Large Language Model for Quechua-English Translation","authors":"Junhao Chen, Peng Shu, Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan,\n  Yifan Zhou, Zhengliang Liu, Lewis C Howe, Tianming Liu","authorsParsed":[["Chen","Junhao",""],["Shu","Peng",""],["Li","Yiwei",""],["Zhao","Huaqin",""],["Jiang","Hanqi",""],["Pan","Yi",""],["Zhou","Yifan",""],["Liu","Zhengliang",""],["Howe","Lewis C",""],["Liu","Tianming",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 17:04:21 GMT"}],"updateDate":"2024-12-09","timestamp":1733504661000,"abstract":"  Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. In this paper, we propose QueEn, a novel approach for Quechua-English\ntranslation that combines Retrieval-Augmented Generation (RAG) with\nparameter-efficient fine-tuning techniques. Our method leverages external\nlinguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for\nefficient model adaptation. Experimental results show that our approach\nsubstantially exceeds baseline models, with a BLEU score of 17.6 compared to\n1.5 for standard GPT models. The integration of RAG with fine-tuning allows our\nsystem to address the challenges of low-resource language translation while\nmaintaining computational efficiency. This work contributes to the broader goal\nof preserving endangered languages through advanced language technologies.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"iAZfKl4iZwRYt6SGG9MQjqNxz31O7_ouD1i8eOouXA4","pdfSize":"533094"}