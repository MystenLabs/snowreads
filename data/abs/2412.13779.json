{"id":"2412.13779","title":"Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization","authors":"Yichen Li, Yuying Wang, Tianzhe Xiao, Haozhao Wang, Yining Qi, Ruixuan\n  Li","authorsParsed":[["Li","Yichen",""],["Wang","Yuying",""],["Xiao","Tianzhe",""],["Wang","Haozhao",""],["Qi","Yining",""],["Li","Ruixuan",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 12:16:41 GMT"}],"updateDate":"2024-12-19","timestamp":1734524201000,"abstract":"  Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.\n","subjects":["Computer Science/Machine Learning","Computer Science/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZMLgWoNMqYGtO5A96OxZUDKP_LQQcQXEvb8I_7j3K_8","pdfSize":"519318"}