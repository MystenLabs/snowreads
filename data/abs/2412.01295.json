{
  "id": "2412.01295",
  "title": "FedAH: Aggregated Head for Personalized Federated Learning",
  "authors": "Pengzhan Zhou, Yuepeng He, Yijun Zhai, Kaixin Gao, Chao Chen, Zhida\n  Qin, Chong Zhang, Songtao Guo",
  "authorsParsed": [
    [
      "Zhou",
      "Pengzhan",
      ""
    ],
    [
      "He",
      "Yuepeng",
      ""
    ],
    [
      "Zhai",
      "Yijun",
      ""
    ],
    [
      "Gao",
      "Kaixin",
      ""
    ],
    [
      "Chen",
      "Chao",
      ""
    ],
    [
      "Qin",
      "Zhida",
      ""
    ],
    [
      "Zhang",
      "Chong",
      ""
    ],
    [
      "Guo",
      "Songtao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 09:08:51 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733130531000,
  "abstract": "  Recently, Federated Learning (FL) has gained popularity for its\nprivacy-preserving and collaborative learning capabilities. Personalized\nFederated Learning (PFL), building upon FL, aims to address the issue of\nstatistical heterogeneity and achieve personalization. Personalized-head-based\nPFL is a common and effective PFL method that splits the model into a feature\nextractor and a head, where the feature extractor is collaboratively trained\nand shared, while the head is locally trained and not shared. However,\nretaining the head locally, although achieving personalization, prevents the\nmodel from learning global knowledge in the head, thus affecting the\nperformance of the personalized model. To solve this problem, we propose a\nnovel PFL method called Federated Learning with Aggregated Head (FedAH), which\ninitializes the head with an Aggregated Head at each iteration. The key feature\nof FedAH is to perform element-level aggregation between the local model head\nand the global model head to introduce global information from the global model\nhead. To evaluate the effectiveness of FedAH, we conduct extensive experiments\non five benchmark datasets in the fields of computer vision and natural\nlanguage processing. FedAH outperforms ten state-of-the-art FL methods in terms\nof test accuracy by 2.87%. Additionally, FedAH maintains its advantage even in\nscenarios where some clients drop out unexpectedly. Our code is open-accessed\nat https://github.com/heyuepeng/FedAH.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Distributed, Parallel, and Cluster Computing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "4J8VW-PmsQaFs4nWhXewr_5z-gji2uv0Tofxb10LYrs",
  "pdfSize": "437340"
}