{
  "id": "2412.12432",
  "title": "Three Things to Know about Deep Metric Learning",
  "authors": "Yash Patel, Giorgos Tolias, Jiri Matas",
  "authorsParsed": [
    [
      "Patel",
      "Yash",
      ""
    ],
    [
      "Tolias",
      "Giorgos",
      ""
    ],
    [
      "Matas",
      "Jiri",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 00:49:12 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734396552000,
  "abstract": "  This paper addresses supervised deep metric learning for open-set image\nretrieval, focusing on three key aspects: the loss function, mixup\nregularization, and model initialization. In deep metric learning, optimizing\nthe retrieval evaluation metric, recall@k, via gradient descent is desirable\nbut challenging due to its non-differentiable nature. To overcome this, we\npropose a differentiable surrogate loss that is computed on large batches,\nnearly equivalent to the entire training set. This computationally intensive\nprocess is made feasible through an implementation that bypasses the GPU memory\nlimitations. Additionally, we introduce an efficient mixup regularization\ntechnique that operates on pairwise scalar similarities, effectively increasing\nthe batch size even further. The training process is further enhanced by\ninitializing the vision encoder using foundational models, which are\npre-trained on large-scale datasets. Through a systematic study of these\ncomponents, we demonstrate that their synergy enables large models to nearly\nsolve popular benchmarks.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "R19D11GOHsfEXUGty-jH_Q_WRRCmVCsyDKyKGYwPGuk",
  "pdfSize": "935507"
}