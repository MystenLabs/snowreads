{
  "id": "2412.10942",
  "title": "Meta-evaluating stability measures: MAX-Senstivity & AVG-Sensitivity",
  "authors": "Miquel Mir\\'o-Nicolau, Antoni Jaume-i-Cap\\'o, Gabriel Moy\\`a-Alcover",
  "authorsParsed": [
    [
      "Miró-Nicolau",
      "Miquel",
      ""
    ],
    [
      "Jaume-i-Capó",
      "Antoni",
      ""
    ],
    [
      "Moyà-Alcover",
      "Gabriel",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 14 Dec 2024 19:34:32 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734204872000,
  "abstract": "  The use of eXplainable Artificial Intelligence (XAI) systems has introduced a\nset of challenges that need resolution. The XAI robustness, or stability, has\nbeen one of the goals of the community from its beginning. Multiple authors\nhave proposed evaluating this feature using objective evaluation measures.\nNonetheless, many questions remain. With this work, we propose a novel approach\nto meta-evaluate these metrics, i.e. analyze the correctness of the evaluators.\nWe propose two new tests that allowed us to evaluate two different stability\nmeasures: AVG-Sensitiviy and MAX-Senstivity. We tested their reliability in the\npresence of perfect and robust explanations, generated with a Decision Tree; as\nwell as completely random explanations and prediction. The metrics results\nshowed their incapacity of identify as erroneous the random explanations,\nhighlighting their overall unreliability.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "BakfgO-WLvNDm_xyvw8wC3YiU57X0r5txwprS_tECyg",
  "pdfSize": "332417"
}