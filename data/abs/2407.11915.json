{"id":"2407.11915","title":"Imitation of human motion achieves natural head movements for humanoid\n  robots in an active-speaker detection task","authors":"Bosong Ding, Murat Kirtay, Giacomo Spigler","authorsParsed":[["Ding","Bosong",""],["Kirtay","Murat",""],["Spigler","Giacomo",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 17:08:40 GMT"}],"updateDate":"2024-07-23","timestamp":1721149720000,"abstract":"  Head movements are crucial for social human-human interaction. They can\ntransmit important cues (e.g., joint attention, speaker detection) that cannot\nbe achieved with verbal interaction alone. This advantage also holds for\nhuman-robot interaction. Even though modeling human motions through generative\nAI models has become an active research area within robotics in recent years,\nthe use of these methods for producing head movements in human-robot\ninteraction remains underexplored. In this work, we employed a generative AI\npipeline to produce human-like head movements for a Nao humanoid robot. In\naddition, we tested the system on a real-time active-speaker tracking task in a\ngroup conversation setting. Overall, the results show that the Nao robot\nsuccessfully imitates human head movements in a natural manner while actively\ntracking the speakers during the conversation. Code and data from this study\nare available at https://github.com/dingdingding60/Humanoids2024HRI\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qDhoRV-JFnwQ1bsw5-XhdJBbA7xdiedZgGHJ6SO8SRc","pdfSize":"3617524"}