{
  "id": "2412.01756",
  "title": "Adversarial Sample-Based Approach for Tighter Privacy Auditing in Final\n  Model-Only Scenarios",
  "authors": "Sangyeon Yoon, Wonje Jeung, Albert No",
  "authorsParsed": [
    [
      "Yoon",
      "Sangyeon",
      ""
    ],
    [
      "Jeung",
      "Wonje",
      ""
    ],
    [
      "No",
      "Albert",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 17:52:16 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 24 Feb 2025 07:27:11 GMT"
    }
  ],
  "updateDate": "2025-02-25",
  "timestamp": 1733161936000,
  "abstract": "  Auditing Differentially Private Stochastic Gradient Descent (DP-SGD) in the\nfinal model setting is challenging and often results in empirical lower bounds\nthat are significantly looser than theoretical privacy guarantees. We introduce\na novel auditing method that achieves tighter empirical lower bounds without\nadditional assumptions by crafting worst-case adversarial samples through\nloss-based input-space auditing. Our approach surpasses traditional\ncanary-based heuristics and is effective in final model-only scenarios.\nSpecifically, with a theoretical privacy budget of $\\varepsilon = 10.0$, our\nmethod achieves empirical lower bounds of $4.914$, compared to the baseline of\n$4.385$ for MNIST. Our work offers a practical framework for reliable and\naccurate privacy auditing in differentially private machine learning.\n",
  "subjects": [
    "Computer Science/Cryptography and Security",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "J50lWVz7BtryWKsyarfshAWIJ9xvo_XGJk8qop_tGFY",
  "pdfSize": "756892"
}