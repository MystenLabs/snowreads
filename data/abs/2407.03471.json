{"id":"2407.03471","title":"Learning Action and Reasoning-Centric Image Editing from Videos and\n  Simulations","authors":"Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva\n  Portelance, Christopher Pal, Siva Reddy","authorsParsed":[["Krojer","Benno",""],["Vattikonda","Dheeraj",""],["Lara","Luis",""],["Jampani","Varun",""],["Portelance","Eva",""],["Pal","Christopher",""],["Reddy","Siva",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 19:36:33 GMT"},{"version":"v2","created":"Fri, 9 Aug 2024 22:03:59 GMT"}],"updateDate":"2024-08-13","timestamp":1720035393000,"abstract":"  An image editing model should be able to perform diverse edits, ranging from\nobject replacement, changing attributes or style, to performing actions or\nmovement, which require many forms of reasoning. Current general\ninstruction-guided editing models have significant shortcomings with action and\nreasoning-centric edits. Object, attribute or stylistic changes can be learned\nfrom visually static datasets. On the other hand, high-quality data for action\nand reasoning-centric edits is scarce and has to come from entirely different\nsources that cover e.g. physical dynamics, temporality and spatial reasoning.\nTo this end, we meticulously curate the AURORA Dataset\n(Action-Reasoning-Object-Attribute), a collection of high-quality training\ndata, human-annotated and curated from videos and simulation engines. We focus\non a key aspect of quality training data: triplets (source image, prompt,\ntarget image) contain a single meaningful visual change described by the\nprompt, i.e., truly minimal changes between source and target images. To\ndemonstrate the value of our dataset, we evaluate an AURORA-finetuned model on\na new expert-curated benchmark (AURORA-Bench) covering 8 diverse editing tasks.\nOur model significantly outperforms previous editing models as judged by human\nraters. For automatic evaluations, we find important flaws in previous metrics\nand caution their use for semantically hard editing tasks. Instead, we propose\na new automatic metric that focuses on discriminative understanding. We hope\nthat our efforts : (1) curating a quality training dataset and an evaluation\nbenchmark, (2) developing critical evaluations, and (3) releasing a\nstate-of-the-art model, will fuel further progress on general image editing.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-FF3XJNZNyVZeVnC5jw4xOVghvxeCDpnBsSOjaxPq7M","pdfSize":"20613705"}