{"id":"2412.13435","title":"Lightweight Safety Classification Using Pruned Language Models","authors":"Mason Sawtell, Tula Masterman, Sandi Besen, Jim Brown","authorsParsed":[["Sawtell","Mason",""],["Masterman","Tula",""],["Besen","Sandi",""],["Brown","Jim",""]],"versions":[{"version":"v1","created":"Wed, 18 Dec 2024 02:13:13 GMT"}],"updateDate":"2024-12-19","timestamp":1734487993000,"abstract":"  In this paper, we introduce a novel technique for content safety and prompt\ninjection classification for Large Language Models. Our technique, Layer\nEnhanced Classification (LEC), trains a Penalized Logistic Regression (PLR)\nclassifier on the hidden state of an LLM's optimal intermediate transformer\nlayer. By combining the computational efficiency of a streamlined PLR\nclassifier with the sophisticated language understanding of an LLM, our\napproach delivers superior performance surpassing GPT-4o and special-purpose\nmodels fine-tuned for each task. We find that small general-purpose models\n(Qwen 2.5 sizes 0.5B, 1.5B, and 3B) and other transformer-based architectures\nlike DeBERTa v3 are robust feature extractors allowing simple classifiers to be\neffectively trained on fewer than 100 high-quality examples. Importantly, the\nintermediate transformer layers of these models typically outperform the final\nlayer across both classification tasks. Our results indicate that a single\ngeneral-purpose LLM can be used to classify content safety, detect prompt\ninjections, and simultaneously generate output tokens. Alternatively, these\nrelatively small LLMs can be pruned to the optimal intermediate layer and used\nexclusively as robust feature extractors. Since our results are consistent on\ndifferent transformer architectures, we infer that robust feature extraction is\nan inherent capability of most, if not all, LLMs.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"ZJyZCg1lijl70CY7ZAIZHE4xek18A2wigEhvRApMKmE","pdfSize":"2395513"}