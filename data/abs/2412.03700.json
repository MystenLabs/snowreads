{
  "id": "2412.03700",
  "title": "Good practices for evaluation of machine learning systems",
  "authors": "Luciana Ferrer, Odette Scharenborg, Tom B\\\"ackstr\\\"om",
  "authorsParsed": [
    [
      "Ferrer",
      "Luciana",
      ""
    ],
    [
      "Scharenborg",
      "Odette",
      ""
    ],
    [
      "Bäckström",
      "Tom",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 20:30:16 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733344216000,
  "abstract": "  Many development decisions affect the results obtained from ML experiments:\ntraining data, features, model architecture, hyperparameters, test data, etc.\nAmong these aspects, arguably the most important design decisions are those\nthat involve the evaluation procedure. This procedure is what determines\nwhether the conclusions drawn from the experiments will or will not generalize\nto unseen data and whether they will be relevant to the application of\ninterest. If the data is incorrectly selected, the wrong metric is chosen for\nevaluation or the significance of the comparisons between models is\noverestimated, conclusions may be misleading or result in suboptimal\ndevelopment decisions. To avoid such problems, the evaluation protocol should\nbe very carefully designed before experimentation starts.\n  In this work we discuss the main aspects involved in the design of the\nevaluation protocol: data selection, metric selection, and statistical\nsignificance. This document is not meant to be an exhaustive tutorial on each\nof these aspects. Instead, the goal is to explain the main guidelines that\nshould be followed in each case. We include examples taken from the speech\nprocessing field, and provide a list of common mistakes related to each aspect.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Yzd_UF0Y56RyE0qOrACfowdLxo5AWbeaqiLHoC2CINE",
  "pdfSize": "145262"
}