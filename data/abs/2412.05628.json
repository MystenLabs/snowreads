{"id":"2412.05628","title":"Remix-DiT: Mixing Diffusion Transformers for Multi-Expert Denoising","authors":"Gongfan Fang, Xinyin Ma, Xinchao Wang","authorsParsed":[["Fang","Gongfan",""],["Ma","Xinyin",""],["Wang","Xinchao",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 11:52:41 GMT"}],"updateDate":"2024-12-10","timestamp":1733572361000,"abstract":"  Transformer-based diffusion models have achieved significant advancements\nacross a variety of generative tasks. However, producing high-quality outputs\ntypically necessitates large transformer models, which result in substantial\ntraining and inference overhead. In this work, we investigate an alternative\napproach involving multiple experts for denoising, and introduce Remix-DiT, a\nnovel method designed to enhance output quality at a low cost. The goal of\nRemix-DiT is to craft N diffusion experts for different denoising timesteps,\nyet without the need for expensive training of N independent models. To achieve\nthis, Remix-DiT employs K basis models (where K < N) and utilizes learnable\nmixing coefficients to adaptively craft expert models. This design offers two\nsignificant advantages: first, although the total model size is increased, the\nmodel produced by the mixing operation shares the same architecture as a plain\nmodel, making the overall model as efficient as a standard diffusion\ntransformer. Second, the learnable mixing adaptively allocates model capacity\nacross timesteps, thereby effectively improving generation quality. Experiments\nconducted on the ImageNet dataset demonstrate that Remix-DiT achieves promising\nresults compared to standard diffusion transformers and other multiple-expert\nmethods. The code is available at https://github.com/VainF/Remix-DiT.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fk_iO2z9vOy_dcBFgtzjuCaDr5NHVB6eVtjtAbKkHsY","pdfSize":"1792760"}