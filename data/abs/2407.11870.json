{"id":"2407.11870","title":"Fusion LiDAR-Inertial-Encoder data for High-Accuracy SLAM","authors":"Manh Do Duc, Thanh Nguyen Canh, Minh DoNgoc, Xiem HoangVan","authorsParsed":[["Duc","Manh Do",""],["Canh","Thanh Nguyen",""],["DoNgoc","Minh",""],["HoangVan","Xiem",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 15:54:28 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 07:59:43 GMT"}],"updateDate":"2024-07-18","timestamp":1721145268000,"abstract":"  In the realm of robotics, achieving simultaneous localization and mapping\n(SLAM) is paramount for autonomous navigation, especially in challenging\nenvironments like texture-less structures. This paper proposed a\nfactor-graph-based model that tightly integrates IMU and encoder sensors to\nenhance positioning in such environments. The system operates by meticulously\nevaluating the data from each sensor. Based on these evaluations, weights are\ndynamically adjusted to prioritize the more reliable source of information at\nany given moment. The robot's state is initialized using IMU data, while the\nencoder aids motion estimation in long corridors. Discrepancies between the two\nstates are used to correct IMU drift. The effectiveness of this method is\ndemonstrably validated through experimentation. Compared to Karto SLAM, a\nwidely used SLAM algorithm, this approach achieves an improvement of 26.98% in\nrotation angle error and 67.68% reduction in position error. These results\nconvincingly demonstrate the method's superior accuracy and robustness in\ntexture-less environments.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"aJDUpxRblQI6Ri08dFs5xbzxZ54PEqc8irVFdvFYkVU","pdfSize":"1839992"}