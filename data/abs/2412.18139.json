{
  "id": "2412.18139",
  "title": "Ensuring Consistency for In-Image Translation",
  "authors": "Chengpeng Fu, Xiaocheng Feng, Yichong Huang, Wenshuai Huo, Baohang Li,\n  Zhirui Zhang, Yunfei Lu, Dandan Tu, Duyu Tang, Hui Wang, Bing Qin, Ting Liu",
  "authorsParsed": [
    [
      "Fu",
      "Chengpeng",
      ""
    ],
    [
      "Feng",
      "Xiaocheng",
      ""
    ],
    [
      "Huang",
      "Yichong",
      ""
    ],
    [
      "Huo",
      "Wenshuai",
      ""
    ],
    [
      "Li",
      "Baohang",
      ""
    ],
    [
      "Zhang",
      "Zhirui",
      ""
    ],
    [
      "Lu",
      "Yunfei",
      ""
    ],
    [
      "Tu",
      "Dandan",
      ""
    ],
    [
      "Tang",
      "Duyu",
      ""
    ],
    [
      "Wang",
      "Hui",
      ""
    ],
    [
      "Qin",
      "Bing",
      ""
    ],
    [
      "Liu",
      "Ting",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 24 Dec 2024 03:50:03 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1735012203000,
  "abstract": "  The in-image machine translation task involves translating text embedded\nwithin images, with the translated results presented in image format. While\nthis task has numerous applications in various scenarios such as film poster\ntranslation and everyday scene image translation, existing methods frequently\nneglect the aspect of consistency throughout this process. We propose the need\nto uphold two types of consistency in this task: translation consistency and\nimage generation consistency. The former entails incorporating image\ninformation during translation, while the latter involves maintaining\nconsistency between the style of the text-image and the original image,\nensuring background integrity. To address these consistency requirements, we\nintroduce a novel two-stage framework named HCIIT (High-Consistency In-Image\nTranslation) which involves text-image translation using a multimodal\nmultilingual large language model in the first stage and image backfilling with\na diffusion model in the second stage. Chain of thought learning is utilized in\nthe first stage to enhance the model's ability to leverage image information\nduring translation. Subsequently, a diffusion model trained for\nstyle-consistent text-image generation ensures uniformity in text style within\nimages and preserves background details. A dataset comprising 400,000\nstyle-consistent pseudo text-image pairs is curated for model training. Results\nobtained on both curated test sets and authentic image test sets validate the\neffectiveness of our framework in ensuring consistency and producing\nhigh-quality translated images.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "pYj7w1fGtH2GaaPzTbb25huTltryfJA9Y-sM5RfOixY",
  "pdfSize": "4999743"
}