{"id":"2407.06304","title":"VIMI: Grounding Video Generation through Multi-modal Instruction","authors":"Yuwei Fang, Willi Menapace, Aliaksandr Siarohin, Tsai-Shien Chen,\n  Kuan-Chien Wang, Ivan Skorokhodov, Graham Neubig, Sergey Tulyakov","authorsParsed":[["Fang","Yuwei",""],["Menapace","Willi",""],["Siarohin","Aliaksandr",""],["Chen","Tsai-Shien",""],["Wang","Kuan-Chien",""],["Skorokhodov","Ivan",""],["Neubig","Graham",""],["Tulyakov","Sergey",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 18:12:49 GMT"}],"updateDate":"2024-07-10","timestamp":1720462369000,"abstract":"  Existing text-to-video diffusion models rely solely on text-only encoders for\ntheir pretraining. This limitation stems from the absence of large-scale\nmultimodal prompt video datasets, resulting in a lack of visual grounding and\nrestricting their versatility and application in multimodal integration. To\naddress this, we construct a large-scale multimodal prompt dataset by employing\nretrieval methods to pair in-context examples with the given text prompts and\nthen utilize a two-stage training strategy to enable diverse video generation\ntasks within the same model. In the first stage, we propose a multimodal\nconditional video generation framework for pretraining on these augmented\ndatasets, establishing a foundational model for grounded video generation.\nSecondly, we finetune the model from the first stage on three video generation\ntasks, incorporating multi-modal instructions. This process further refines the\nmodel's ability to handle diverse inputs and tasks, ensuring seamless\nintegration of multi-modal information. After this two-stage train-ing process,\nVIMI demonstrates multimodal understanding capabilities, producing contextually\nrich and personalized videos grounded in the provided inputs, as shown in\nFigure 1. Compared to previous visual grounded video generation methods, VIMI\ncan synthesize consistent and temporally coherent videos with large motion\nwhile retaining the semantic control. Lastly, VIMI also achieves\nstate-of-the-art text-to-video generation results on UCF101 benchmark.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"0KFV6LDcWfEJ1PNBms1auYVUZTkHXqpPt3LrUxUK4Q0","pdfSize":"4827502"}