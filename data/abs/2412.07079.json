{
  "id": "2412.07079",
  "title": "Light Field Image Quality Assessment With Auxiliary Learning Based on\n  Depthwise and Anglewise Separable Convolutions",
  "authors": "Qiang Qu, Xiaoming Chen, Vera Chung, Zhibo Chen",
  "authorsParsed": [
    [
      "Qu",
      "Qiang",
      ""
    ],
    [
      "Chen",
      "Xiaoming",
      ""
    ],
    [
      "Chung",
      "Vera",
      ""
    ],
    [
      "Chen",
      "Zhibo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 00:42:25 GMT"
    }
  ],
  "updateDate": "2024-12-11",
  "timestamp": 1733791345000,
  "abstract": "  In multimedia broadcasting, no-reference image quality assessment (NR-IQA) is\nused to indicate the user-perceived quality of experience (QoE) and to support\nintelligent data transmission while optimizing user experience. This paper\nproposes an improved no-reference light field image quality assessment\n(NR-LFIQA) metric for future immersive media broadcasting services. First, we\nextend the concept of depthwise separable convolution (DSC) to the spatial\ndomain of light field image (LFI) and introduce \"light field depthwise\nseparable convolution (LF-DSC)\", which can extract the LFI's spatial features\nefficiently. Second, we further theoretically extend the LF-DSC to the angular\nspace of LFI and introduce the novel concept of \"light field anglewise\nseparable convolution (LF-ASC)\", which is capable of extracting both the\nspatial and angular features for comprehensive quality assessment with low\ncomplexity. Third, we define the spatial and angular feature estimations as\nauxiliary tasks in aiding the primary NR-LFIQA task by providing spatial and\nangular quality features as hints. To the best of our knowledge, this work is\nthe first exploration of deep auxiliary learning with spatial-angular hints on\nNR-LFIQA. Experiments were conducted in mainstream LFI datasets such as\nWin5-LID and SMART with comparisons to the mainstream full reference IQA\nmetrics as well as the state-of-the-art NR-LFIQA methods. The experimental\nresults show that the proposed metric yields overall 42.86% and 45.95% smaller\nprediction errors than the second-best benchmarking metric in Win5-LID and\nSMART, respectively. In some challenging cases with particular distortion\ntypes, the proposed metric can reduce the errors significantly by more than\n60%.\n",
  "subjects": [
    "Electrical Engineering and Systems Science/Image and Video Processing",
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "MZ0JlXIbybWl7jf-96GlvNZXWKCcPfUHCodu_BLje6M",
  "pdfSize": "17464261"
}