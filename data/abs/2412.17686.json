{
  "id": "2412.17686",
  "title": "Large Language Model Safety: A Holistic Survey",
  "authors": "Dan Shi, Tianhao Shen, Yufei Huang, Zhigen Li, Yongqi Leng, Renren\n  Jin, Chuang Liu, Xinwei Wu, Zishan Guo, Linhao Yu, Ling Shi, Bojian Jiang,\n  Deyi Xiong",
  "authorsParsed": [
    [
      "Shi",
      "Dan",
      ""
    ],
    [
      "Shen",
      "Tianhao",
      ""
    ],
    [
      "Huang",
      "Yufei",
      ""
    ],
    [
      "Li",
      "Zhigen",
      ""
    ],
    [
      "Leng",
      "Yongqi",
      ""
    ],
    [
      "Jin",
      "Renren",
      ""
    ],
    [
      "Liu",
      "Chuang",
      ""
    ],
    [
      "Wu",
      "Xinwei",
      ""
    ],
    [
      "Guo",
      "Zishan",
      ""
    ],
    [
      "Yu",
      "Linhao",
      ""
    ],
    [
      "Shi",
      "Ling",
      ""
    ],
    [
      "Jiang",
      "Bojian",
      ""
    ],
    [
      "Xiong",
      "Deyi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 16:11:27 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1734970287000,
  "abstract": "  The rapid development and deployment of large language models (LLMs) have\nintroduced a new frontier in artificial intelligence, marked by unprecedented\ncapabilities in natural language understanding and generation. However, the\nincreasing integration of these models into critical applications raises\nsubstantial safety concerns, necessitating a thorough examination of their\npotential risks and associated mitigation strategies.\n  This survey provides a comprehensive overview of the current landscape of LLM\nsafety, covering four major categories: value misalignment, robustness to\nadversarial attacks, misuse, and autonomous AI risks. In addition to the\ncomprehensive review of the mitigation methodologies and evaluation resources\non these four aspects, we further explore four topics related to LLM safety:\nthe safety implications of LLM agents, the role of interpretability in\nenhancing LLM safety, the technology roadmaps proposed and abided by a list of\nAI companies and institutes for LLM safety, and AI governance aimed at LLM\nsafety with discussions on international cooperation, policy proposals, and\nprospective regulatory directions.\n  Our findings underscore the necessity for a proactive, multifaceted approach\nto LLM safety, emphasizing the integration of technical solutions, ethical\nconsiderations, and robust governance frameworks. This survey is intended to\nserve as a foundational resource for academy researchers, industry\npractitioners, and policymakers, offering insights into the challenges and\nopportunities associated with the safe integration of LLMs into society.\nUltimately, it seeks to contribute to the safe and beneficial development of\nLLMs, aligning with the overarching goal of harnessing AI for societal\nadvancement and well-being. A curated list of related papers has been publicly\navailable at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "onYtvzRegs2GLBfd3bJY1IJrFQnk6x1bYM16Lu51hL0",
  "pdfSize": "2172527"
}