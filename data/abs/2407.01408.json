{"id":"2407.01408","title":"Semantic Compositions Enhance Vision-Language Contrastive Learning","authors":"Maxwell Aladago, Lorenzo Torresani, Soroush Vosoughi","authorsParsed":[["Aladago","Maxwell",""],["Torresani","Lorenzo",""],["Vosoughi","Soroush",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 15:58:20 GMT"}],"updateDate":"2024-07-02","timestamp":1719849500000,"abstract":"  In the field of vision-language contrastive learning, models such as CLIP\ncapitalize on matched image-caption pairs as positive examples and leverage\nwithin-batch non-matching pairs as negatives. This approach has led to\nremarkable outcomes in zero-shot image classification, cross-modal retrieval,\nand linear evaluation tasks. We show that the zero-shot classification and\nretrieval capabilities of CLIP-like models can be improved significantly\nthrough the introduction of semantically composite examples during pretraining.\nInspired by CutMix in vision categorization, we create semantically composite\nimage-caption pairs by merging elements from two distinct instances in the\ndataset via a novel procedure. Our method fuses the captions and blends 50% of\neach image to form a new composite sample. This simple technique (termed CLIP-C\nfor CLIP Compositions), devoid of any additional computational overhead or\nincrease in model parameters, significantly improves zero-shot image\nclassification and cross-modal retrieval. The benefits of CLIP-C are\nparticularly pronounced in settings with relatively limited pretraining data.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"-eXQ9pTasbk0ozKDLAUaEGmy43NfDblFvT2HdbZ9nAQ","pdfSize":"2781263","objectId":"0x83b66461222a402fde2ff7478c5bc340da00f5c7d9d68b924191a19a0bfcd6d9","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
