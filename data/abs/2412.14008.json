{
  "id": "2412.14008",
  "title": "FarExStance: Explainable Stance Detection for Farsi",
  "authors": "Majid Zarharan, Maryam Hashemi, Malika Behroozrazegh, Sauleh Eetemadi,\n  Mohammad Taher Pilehvar, Jennifer Foster",
  "authorsParsed": [
    [
      "Zarharan",
      "Majid",
      ""
    ],
    [
      "Hashemi",
      "Maryam",
      ""
    ],
    [
      "Behroozrazegh",
      "Malika",
      ""
    ],
    [
      "Eetemadi",
      "Sauleh",
      ""
    ],
    [
      "Pilehvar",
      "Mohammad Taher",
      ""
    ],
    [
      "Foster",
      "Jennifer",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 16:24:20 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734539060000,
  "abstract": "  We introduce FarExStance, a new dataset for explainable stance detection in\nFarsi. Each instance in this dataset contains a claim, the stance of an article\nor social media post towards that claim, and an extractive explanation which\nprovides evidence for the stance label. We compare the performance of a\nfine-tuned multilingual RoBERTa model to several large language models in\nzero-shot, few-shot, and parameter-efficient fine-tuned settings on our new\ndataset. On stance detection, the most accurate models are the fine-tuned\nRoBERTa model, the LLM Aya-23-8B which has been fine-tuned using\nparameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the\nquality of the explanations, our automatic evaluation metrics indicate that\nfew-shot GPT-4o generates the most coherent explanations, while our human\nevaluation reveals that the best Overall Explanation Score (OES) belongs to\nfew-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced\nexplanations most closely aligned with the reference explanations.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "4tTRxr3IwpCKM1-KXG-0oLLPWjqZ8QO3Ke66XBgHvR4",
  "pdfSize": "2555273"
}