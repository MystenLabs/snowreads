{"id":"2412.00927","title":"VISTA: Enhancing Long-Duration and High-Resolution Video Understanding\n  by Video Spatiotemporal Augmentation","authors":"Weiming Ren, Huan Yang, Jie Min, Cong Wei, Wenhu Chen","authorsParsed":[["Ren","Weiming",""],["Yang","Huan",""],["Min","Jie",""],["Wei","Cong",""],["Chen","Wenhu",""]],"versions":[{"version":"v1","created":"Sun, 1 Dec 2024 18:27:28 GMT"}],"updateDate":"2024-12-03","timestamp":1733077648000,"abstract":"  Current large multimodal models (LMMs) face significant challenges in\nprocessing and comprehending long-duration or high-resolution videos, which is\nmainly due to the lack of high-quality datasets. To address this issue from a\ndata-centric perspective, we propose VISTA, a simple yet effective Video\nSpatiotemporal Augmentation framework that synthesizes long-duration and\nhigh-resolution video instruction-following pairs from existing video-caption\ndatasets. VISTA spatially and temporally combines videos to create new\nsynthetic videos with extended durations and enhanced resolutions, and\nsubsequently produces question-answer pairs pertaining to these newly\nsynthesized videos. Based on this paradigm, we develop seven video augmentation\nmethods and curate VISTA-400K, a video instruction-following dataset aimed at\nenhancing long-duration and high-resolution video understanding. Finetuning\nvarious video LMMs on our data resulted in an average improvement of 3.3%\nacross four challenging benchmarks for long-video understanding. Furthermore,\nwe introduce the first comprehensive high-resolution video understanding\nbenchmark HRVideoBench, on which our finetuned models achieve a 6.5%\nperformance gain. These results highlight the effectiveness of our framework.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Wvh5BfLtG2sbXh2gvYtr6h2QCweeOtoov7f-AErxxkw","pdfSize":"13093596"}