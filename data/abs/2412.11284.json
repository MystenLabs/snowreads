{
  "id": "2412.11284",
  "title": "Learning Normal Flow Directly From Event Neighborhoods",
  "authors": "Dehao Yuan, Levi Burner, Jiayi Wu, Minghui Liu, Jingxi Chen, Yiannis\n  Aloimonos, Cornelia Ferm\\\"uller",
  "authorsParsed": [
    [
      "Yuan",
      "Dehao",
      ""
    ],
    [
      "Burner",
      "Levi",
      ""
    ],
    [
      "Wu",
      "Jiayi",
      ""
    ],
    [
      "Liu",
      "Minghui",
      ""
    ],
    [
      "Chen",
      "Jingxi",
      ""
    ],
    [
      "Aloimonos",
      "Yiannis",
      ""
    ],
    [
      "Ferm√ºller",
      "Cornelia",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 15 Dec 2024 19:09:45 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734289785000,
  "abstract": "  Event-based motion field estimation is an important task. However, current\noptical flow methods face challenges: learning-based approaches, often\nframe-based and relying on CNNs, lack cross-domain transferability, while\nmodel-based methods, though more robust, are less accurate. To address the\nlimitations of optical flow estimation, recent works have focused on normal\nflow, which can be more reliably measured in regions with limited texture or\nstrong edges. However, existing normal flow estimators are predominantly\nmodel-based and suffer from high errors.\n  In this paper, we propose a novel supervised point-based method for normal\nflow estimation that overcomes the limitations of existing event learning-based\napproaches. Using a local point cloud encoder, our method directly estimates\nper-event normal flow from raw events, offering multiple unique advantages: 1)\nIt produces temporally and spatially sharp predictions. 2) It supports more\ndiverse data augmentation, such as random rotation, to improve robustness\nacross various domains. 3) It naturally supports uncertainty quantification via\nensemble inference, which benefits downstream tasks. 4) It enables training and\ninference on undistorted data in normalized camera coordinates, improving\ntransferability across cameras. Extensive experiments demonstrate our method\nachieves better and more consistent performance than state-of-the-art methods\nwhen transferred across different datasets. Leveraging this transferability, we\ntrain our model on the union of datasets and release it for public use.\nFinally, we introduce an egomotion solver based on a maximum-margin problem\nthat uses normal flow and IMU to achieve strong performance in challenging\nscenarios.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "pqLc34UDm6CVCiuEAQvOim_s0fOCBiXcWpPC7IezrxQ",
  "pdfSize": "14092440"
}