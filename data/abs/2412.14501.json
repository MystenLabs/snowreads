{"id":"2412.14501","title":"Do Large Language Models Defend Inferentialist Semantics?: On the\n  Logical Expressivism and Anti-Representationalism of LLMs","authors":"Yuzuki Arai and Sho Tsugawa","authorsParsed":[["Arai","Yuzuki",""],["Tsugawa","Sho",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 03:48:40 GMT"}],"updateDate":"2024-12-20","timestamp":1734580120000,"abstract":"  The philosophy of language, which has historically been developed through an\nanthropocentric lens, is now being forced to move towards post-anthropocentrism\ndue to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude\n(Anthropic), which are considered to possess linguistic abilities comparable to\nthose of humans. Traditionally, LLMs have been explained through distributional\nsemantics as their foundational semantics. However, recent research is\nexploring alternative foundational semantics beyond distributional semantics.\nThis paper proposes Robert Brandom's inferentialist semantics as an suitable\nfoundational semantics for LLMs, specifically focusing on the issue of\nlinguistic representationalism within this post-anthropocentric trend. Here, we\nshow that the anti-representationalism and logical expressivism of inferential\nsemantics, as well as quasi-compositionality, are useful in interpreting the\ncharacteristics and behaviors of LLMs. Further, we propose a \\emph{consensus\ntheory of truths} for LLMs. This paper argues that the characteristics of LLMs\nchallenge mainstream assumptions in philosophy of language, such as semantic\nexternalism and compositionality. We believe the argument in this paper leads\nto a re-evaluation of anti\\hyphen{}representationalist views of language,\npotentially leading to new developments in the philosophy of language.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_QVhVuhdmknQw3Vo14vYOjiWCWtNDr_LaQFantQL07k","pdfSize":"598755"}