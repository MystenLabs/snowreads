{
  "id": "2412.05288",
  "title": "StackEval: Benchmarking LLMs in Coding Assistance",
  "authors": "Nidhish Shah, Zulkuf Genc, Dogu Araci",
  "authorsParsed": [
    [
      "Shah",
      "Nidhish",
      ""
    ],
    [
      "Genc",
      "Zulkuf",
      ""
    ],
    [
      "Araci",
      "Dogu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 21 Nov 2024 11:20:48 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1732188048000,
  "abstract": "  We present two comprehensive benchmarks to evaluate the performance of\nlanguage models in coding assistance tasks, covering code writing, debugging,\ncode review, and conceptual understanding. Our main contribution includes two\ncurated datasets: StackEval, a large-scale benchmark derived from Stack\nOverflow questions, and StackUnseen, a dynamic benchmark featuring the most\nrecent Stack Overflow content. These benchmarks offer novel insights into the\ncapabilities and limitations of LLMs, particularly in handling new and emerging\ncontent. Additionally, we assess LLMs' proficiency as judges for coding tasks\nusing a curated, human-annotated dataset, exploring their evaluation\ncapabilities and potential biases, including whether they favor their own\ngenerated solutions. Our findings underscore the potential of these benchmarks\nto advance LLM development and application in coding assistance. To ensure\nreproducibility, we publicly share our datasets and evaluation code at\nhttps://github.com/ProsusAI/stack-eval .\n",
  "subjects": [
    "Computer Science/Software Engineering",
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "KR-bkCe0s3iOtRIabRre5H82uTpqoE6mn7dC0qj_m1o",
  "pdfSize": "609223"
}