{"id":"2407.13335","title":"OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction","authors":"Yini Fang, Jingling Yu, Haozheng Zhang, Ralf van der Lans, Bertram Shi","authorsParsed":[["Fang","Yini",""],["Yu","Jingling",""],["Zhang","Haozheng",""],["van der Lans","Ralf",""],["Shi","Bertram",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 09:33:17 GMT"}],"updateDate":"2024-07-19","timestamp":1721295197000,"abstract":"  Visual search is important in our daily life. The efficient allocation of\nvisual attention is critical to effectively complete visual search tasks. Prior\nresearch has predominantly modelled the spatial allocation of visual attention\nin images at the pixel level, e.g. using a saliency map. However, emerging\nevidence shows that visual attention is guided by objects rather than pixel\nintensities. This paper introduces the Object-level Attention Transformer\n(OAT), which predicts human scanpaths as they search for a target object within\na cluttered scene of distractors. OAT uses an encoder-decoder architecture. The\nencoder captures information about the position and appearance of the objects\nwithin an image and about the target. The decoder predicts the gaze scanpath as\na sequence of object fixations, by integrating output features from both the\nencoder and decoder. We also propose a new positional encoding that better\nreflects spatial relationships between objects. We evaluated OAT on the Amazon\nbook cover dataset and a new dataset for visual search that we collected. OAT's\npredicted gaze scanpaths align more closely with human gaze patterns, compared\nto predictions by algorithms based on spatial attention on both established\nmetrics and a novel behavioural-based metric. Our results demonstrate the\ngeneralization ability of OAT, as it accurately predicts human scanpaths for\nunseen layouts and target objects.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"539nf1FcbJA-PHWc34Ys6Pk15ZztO6DIFdhRMqvlhQA","pdfSize":"7433986"}