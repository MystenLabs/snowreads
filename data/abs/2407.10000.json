{"id":"2407.10000","title":"On Characterizing and Mitigating Imbalances in Multi-Instance Partial\n  Label Learning","authors":"Kaifu Wang, Efthymia Tsamoura, Dan Roth","authorsParsed":[["Wang","Kaifu",""],["Tsamoura","Efthymia",""],["Roth","Dan",""]],"versions":[{"version":"v1","created":"Sat, 13 Jul 2024 20:56:34 GMT"}],"updateDate":"2024-07-16","timestamp":1720904194000,"abstract":"  Multi-Instance Partial Label Learning (MI-PLL) is a weakly-supervised\nlearning setting encompassing partial label learning, latent structural\nlearning, and neurosymbolic learning. Differently from supervised learning, in\nMI-PLL, the inputs to the classifiers at training-time are tuples of instances\n$\\textbf{x}$, while the supervision signal is generated by a function $\\sigma$\nover the gold labels of $\\textbf{x}$. The gold labels are hidden during\ntraining. In this paper, we focus on characterizing and mitigating learning\nimbalances, i.e., differences in the errors occurring when classifying\ninstances of different classes (aka class-specific risks), under MI-PLL. The\nphenomenon of learning imbalances has been extensively studied in the context\nof long-tail learning; however, the nature of MI-PLL introduces new challenges.\nOur contributions are as follows. From a theoretical perspective, we\ncharacterize the learning imbalances by deriving class-specific risk bounds\nthat depend upon the function $\\sigma$. Our theory reveals that learning\nimbalances exist in MI-PLL even when the hidden labels are uniformly\ndistributed. On the practical side, we introduce a technique for estimating the\nmarginal of the hidden labels using only MI-PLL data. Then, we introduce\nalgorithms that mitigate imbalances at training- and testing-time, by treating\nthe marginal of the hidden labels as a constraint. The first algorithm relies\non a novel linear programming formulation of MI-PLL for pseudo-labeling. The\nsecond one adjusts a model's scores based on robust optimal transport. We\ndemonstrate the effectiveness of our techniques using strong neurosymbolic and\nlong-tail learning baselines, discussing also open challenges.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"E_BoclieyYIBG19Rz658jhdGZaTDh4splGBKYkxQh4M","pdfSize":"1137410"}