{"id":"2407.03967","title":"Investigating the Role of Instruction Variety and Task Difficulty in\n  Robotic Manipulation Tasks","authors":"Amit Parekh, Nikolas Vitsakis, Alessandro Suglia, Ioannis Konstas","authorsParsed":[["Parekh","Amit",""],["Vitsakis","Nikolas",""],["Suglia","Alessandro",""],["Konstas","Ioannis",""]],"versions":[{"version":"v1","created":"Thu, 4 Jul 2024 14:36:49 GMT"}],"updateDate":"2024-07-08","timestamp":1720103809000,"abstract":"  Evaluating the generalisation capabilities of multimodal models based solely\non their performance on out-of-distribution data fails to capture their true\nrobustness. This work introduces a comprehensive evaluation framework that\nsystematically examines the role of instructions and inputs in the\ngeneralisation abilities of such models, considering architectural design,\ninput perturbations across language and vision modalities, and increased task\ncomplexity. The proposed framework uncovers the resilience of multimodal models\nto extreme instruction perturbations and their vulnerability to observational\nchanges, raising concerns about overfitting to spurious correlations. By\nemploying this evaluation framework on current Transformer-based multimodal\nmodels for robotic manipulation tasks, we uncover limitations and suggest\nfuture advancements should focus on architectural and training innovations that\nbetter integrate multimodal inputs, enhancing a model's generalisation prowess\nby prioritising sensitivity to input content over incidental correlations.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2-hglErswuWff-fHTvOz10GOw5jqEs9st5DngZ_w3uE","pdfSize":"5846033"}
