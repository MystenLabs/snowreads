{"id":"2407.11144","title":"YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language\n  Parallel Corpus","authors":"Garrett Tanzer, Biao Zhang","authorsParsed":[["Tanzer","Garrett",""],["Zhang","Biao",""]],"versions":[{"version":"v1","created":"Mon, 15 Jul 2024 18:08:34 GMT"}],"updateDate":"2024-07-17","timestamp":1721066914000,"abstract":"  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4bdEWVgmDstwNUheVFL66anqypDyyDwelEM8e7NTufY","pdfSize":"439293"}