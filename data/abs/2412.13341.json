{"id":"2412.13341","title":"Concept-ROT: Poisoning Concepts in Large Language Models with Model\n  Editing","authors":"Keltin Grimes, Marco Christiani, David Shriver, Marissa Connor","authorsParsed":[["Grimes","Keltin",""],["Christiani","Marco",""],["Shriver","David",""],["Connor","Marissa",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 21:29:30 GMT"}],"updateDate":"2024-12-19","timestamp":1734470970000,"abstract":"  Model editing methods modify specific behaviors of Large Language Models by\naltering a small, targeted set of network weights and require very little data\nand compute. These methods can be used for malicious applications such as\ninserting misinformation or simple trojans that result in adversary-specified\nbehaviors when a trigger word is present. While previous editing methods have\nfocused on relatively constrained scenarios that link individual words to fixed\noutputs, we show that editing techniques can integrate more complex behaviors\nwith similar effectiveness. We develop Concept-ROT, a model editing-based\nmethod that efficiently inserts trojans which not only exhibit complex output\nbehaviors, but also trigger on high-level concepts -- presenting an entirely\nnew class of trojan attacks. Specifically, we insert trojans into frontier\nsafety-tuned LLMs which trigger only in the presence of concepts such as\n'computer science' or 'ancient civilizations.' When triggered, the trojans\njailbreak the model, causing it to answer harmful questions that it would\notherwise refuse. Our results further motivate concerns over the practicality\nand potential ramifications of trojan attacks on Machine Learning models.\n","subjects":["Computer Science/Machine Learning","Computer Science/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"-UmSoi1kd8KQOw0XwlvvZ-SUApZLw9FaPusdD-ub1n0","pdfSize":"1277286"}