{
  "id": "2412.09945",
  "title": "Going Beyond Feature Similarity: Effective Dataset distillation based on\n  Class-aware Conditional Mutual Information",
  "authors": "Xinhao Zhong, Bin Chen, Hao Fang, Xulin Gu, Shu-Tao Xia, En-Hui Yang",
  "authorsParsed": [
    [
      "Zhong",
      "Xinhao",
      ""
    ],
    [
      "Chen",
      "Bin",
      ""
    ],
    [
      "Fang",
      "Hao",
      ""
    ],
    [
      "Gu",
      "Xulin",
      ""
    ],
    [
      "Xia",
      "Shu-Tao",
      ""
    ],
    [
      "Yang",
      "En-Hui",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 08:10:47 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 21 Feb 2025 13:50:09 GMT"
    }
  ],
  "updateDate": "2025-02-24",
  "timestamp": 1734077447000,
  "abstract": "  Dataset distillation (DD) aims to minimize the time and memory consumption\nneeded for training deep neural networks on large datasets, by creating a\nsmaller synthetic dataset that has similar performance to that of the full real\ndataset. However, current dataset distillation methods often result in\nsynthetic datasets that are excessively difficult for networks to learn from,\ndue to the compression of a substantial amount of information from the original\ndata through metrics measuring feature similarity, e,g., distribution matching\n(DM). In this work, we introduce conditional mutual information (CMI) to assess\nthe class-aware complexity of a dataset and propose a novel method by\nminimizing CMI. Specifically, we minimize the distillation loss while\nconstraining the class-aware complexity of the synthetic dataset by minimizing\nits empirical CMI from the feature space of pre-trained networks,\nsimultaneously. Conducting on a thorough set of experiments, we show that our\nmethod can serve as a general regularization method to existing DD methods and\nimprove the performance and training efficiency.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "MMT0qasKbM1M7McP0nPydajmUwVtm-W3nAEcqVygUVo",
  "pdfSize": "2586388"
}