{"id":"2407.04803","title":"The Impact of Quantization and Pruning on Deep Reinforcement Learning\n  Models","authors":"Heng Lu, Mehdi Alemi, and Reza Rawassizadeh","authorsParsed":[["Lu","Heng",""],["Alemi","Mehdi",""],["Rawassizadeh","Reza",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 18:21:17 GMT"}],"updateDate":"2024-07-09","timestamp":1720203677000,"abstract":"  Deep reinforcement learning (DRL) has achieved remarkable success across\nvarious domains, such as video games, robotics, and, recently, large language\nmodels. However, the computational costs and memory requirements of DRL models\noften limit their deployment in resource-constrained environments. The\nchallenge underscores the urgent need to explore neural network compression\nmethods to make RDL models more practical and broadly applicable. Our study\ninvestigates the impact of two prominent compression methods, quantization and\npruning on DRL models. We examine how these techniques influence four\nperformance factors: average return, memory, inference time, and battery\nutilization across various DRL algorithms and environments. Despite the\ndecrease in model size, we identify that these compression techniques generally\ndo not improve the energy efficiency of DRL models, but the model size\ndecreases. We provide insights into the trade-offs between model compression\nand DRL performance, offering guidelines for deploying efficient DRL models in\nresource-constrained settings.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LJOcP2BK09VbGqoY0w-pic7z4xcv1SVKsUf9bPiKOLw","pdfSize":"6456274"}