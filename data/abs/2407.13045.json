{"id":"2407.13045","title":"Dynamic Programming Principle and Hamilton-Jacobi-Bellman Equation for\n  Optimal Control Problems with Uncertainty","authors":"M. Soledad Aronna, Michele Palladino, Oscar Sierra","authorsParsed":[["Aronna","M. Soledad",""],["Palladino","Michele",""],["Sierra","Oscar",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 22:49:49 GMT"}],"updateDate":"2024-07-19","timestamp":1721256589000,"abstract":"  We study the properties of the value function associated with an optimal\ncontrol problem with uncertainties, known as average or Riemann-Stieltjes\nproblem. Uncertainties are assumed to belong to a compact metric probability\nspace, and appear in the dynamics, in the terminal cost and in the initial\ncondition, which yield an infinite-dimensional formulation. By stating the\nproblem as an evolution equation in a Hilbert space, we show that the value\nfunction is the unique lower semi-continuous proximal solution of the\nHamilton-Jacobi-Bellman (HJB) equation. Our approach relies on invariance\nproperties and the dynamic programming principle.\n","subjects":["Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"aHu385q3hnlKiQab3SEH3j5YXdjwa-pBHwOWNKXwYVU","pdfSize":"303046"}