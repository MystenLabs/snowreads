{"id":"2412.18507","title":"An Empirical Analysis of Federated Learning Models Subject to\n  Label-Flipping Adversarial Attack","authors":"Kunal Bhatnagar and Sagana Chattanathan and Angela Dang and Bhargav\n  Eranki and Ronnit Rana and Charan Sridhar and Siddharth Vedam and Angie Yao\n  and Mark Stamp","authorsParsed":[["Bhatnagar","Kunal",""],["Chattanathan","Sagana",""],["Dang","Angela",""],["Eranki","Bhargav",""],["Rana","Ronnit",""],["Sridhar","Charan",""],["Vedam","Siddharth",""],["Yao","Angie",""],["Stamp","Mark",""]],"versions":[{"version":"v1","created":"Tue, 24 Dec 2024 15:47:25 GMT"}],"updateDate":"2024-12-25","timestamp":1735055245000,"abstract":"  In this paper, we empirically analyze adversarial attacks on selected\nfederated learning models. The specific learning models considered are\nMultinominal Logistic Regression (MLR), Support Vector Classifier (SVC),\nMultilayer Perceptron (MLP), Convolution Neural Network (CNN), %Recurrent\nNeural Network (RNN), Random Forest, XGBoost, and Long Short-Term Memory\n(LSTM). For each model, we simulate label-flipping attacks, experimenting\nextensively with 10 federated clients and 100 federated clients. We vary the\npercentage of adversarial clients from 10% to 100% and, simultaneously, the\npercentage of labels flipped by each adversarial client is also varied from 10%\nto 100%. Among other results, we find that models differ in their inherent\nrobustness to the two vectors in our label-flipping attack, i.e., the\npercentage of adversarial clients, and the percentage of labels flipped by each\nadversarial client. We discuss the potential practical implications of our\nresults.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0BdVJcSw98HMbH-Fg9xQmQO9f0ot9Fn64eLhi-4Ya_4","pdfSize":"544617"}