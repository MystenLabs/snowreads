{"id":"2412.01014","title":"Detecting Memorization in Large Language Models","authors":"Eduardo Slonski","authorsParsed":[["Slonski","Eduardo",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 00:17:43 GMT"}],"updateDate":"2024-12-03","timestamp":1733098663000,"abstract":"  Large language models (LLMs) have achieved impressive results in natural\nlanguage processing but are prone to memorizing portions of their training\ndata, which can compromise evaluation metrics, raise privacy concerns, and\nlimit generalization. Traditional methods for detecting memorization rely on\noutput probabilities or loss functions, often lacking precision due to\nconfounding factors like common language patterns. In this paper, we introduce\nan analytical method that precisely detects memorization by examining neuron\nactivations within the LLM. By identifying specific activation patterns that\ndifferentiate between memorized and not memorized tokens, we train\nclassification probes that achieve near-perfect accuracy. The approach can also\nbe applied to other mechanisms, such as repetition, as demonstrated in this\nstudy, highlighting its versatility. Intervening on these activations allows us\nto suppress memorization without degrading overall performance, enhancing\nevaluation integrity by ensuring metrics reflect genuine generalization.\nAdditionally, our method supports large-scale labeling of tokens and sequences,\ncrucial for next-generation AI models, improving training efficiency and\nresults. Our findings contribute to model interpretability and offer practical\ntools for analyzing and controlling internal mechanisms in LLMs.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NwfD4SH3B5yR_j025HPWhqBoXVjKCHvXs9MyUagUqQ0","pdfSize":"8357787"}