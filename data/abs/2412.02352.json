{
  "id": "2412.02352",
  "title": "LoRA Diffusion: Zero-Shot LoRA Synthesis for Diffusion Model\n  Personalization",
  "authors": "Ethan Smith, Rami Seid, Alberto Hojel, Paramita Mishra, Jianbo Wu",
  "authorsParsed": [
    [
      "Smith",
      "Ethan",
      ""
    ],
    [
      "Seid",
      "Rami",
      ""
    ],
    [
      "Hojel",
      "Alberto",
      ""
    ],
    [
      "Mishra",
      "Paramita",
      ""
    ],
    [
      "Wu",
      "Jianbo",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 10:17:15 GMT"
    }
  ],
  "updateDate": "2024-12-04",
  "timestamp": 1733221035000,
  "abstract": "  Low-Rank Adaptation (LoRA) and other parameter-efficient fine-tuning (PEFT)\nmethods provide low-memory, storage-efficient solutions for personalizing\ntext-to-image models. However, these methods offer little to no improvement in\nwall-clock training time or the number of steps needed for convergence compared\nto full model fine-tuning. While PEFT methods assume that shifts in generated\ndistributions (from base to fine-tuned models) can be effectively modeled\nthrough weight changes in a low-rank subspace, they fail to leverage knowledge\nof common use cases, which typically focus on capturing specific styles or\nidentities. Observing that desired outputs often comprise only a small subset\nof the possible domain covered by LoRA training, we propose reducing the search\nspace by incorporating a prior over regions of interest. We demonstrate that\ntraining a hypernetwork model to generate LoRA weights can achieve competitive\nquality for specific domains while enabling near-instantaneous conditioning on\nuser input, in contrast to traditional training methods that require thousands\nof steps.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "5b-eWg4XZA_ieSdHVZOe5VNqdu2U3aZ-1m9WL3VI7ss",
  "pdfSize": "2361254"
}