{"id":"2412.00471","title":"LLaMA-Gene: A General-purpose Gene Task Large Language Model Based on\n  Instruction Fine-tuning","authors":"Wang Liang","authorsParsed":[["Liang","Wang",""]],"versions":[{"version":"v1","created":"Sat, 30 Nov 2024 13:10:39 GMT"}],"updateDate":"2024-12-03","timestamp":1732972239000,"abstract":"  Building a general-purpose task model similar to ChatGPT has been an\nimportant research direction for gene large language models. Instruction\nfine-tuning is a key component in building ChatGPT, but existing instructions\nare primarily based on natural language. Natural language and gene sequences\nhave significant differences in tokenization and encoding. Therefore,\nconstructing a multilingual model that can handle both natural language and\ngene sequences is crucial for solving this problem.In this paper, we expand the\ncapabilities of the LLaMA large language model to include gene language. This\ninvolves expanding the vocabulary using the Byte Pair Encoding (BPE) method,\nspecifically tailored for DNA and protein sequences, and conducting further\npre-training on these sequences. We then convert various downstream gene task\ndata into a unified format for instruction fine-tuning and further fine-tune\nthe model on this data.Our study demonstrates that a mixed model of gene and\nnatural language, fine-tuned with instructions, achieves results comparable to\nthe current state-of-the-art (SOTA) in tasks such as gene classification and\ngene sequence interaction. This provides a promising direction for building a\nunified large language model for gene tasks.\n","subjects":["Quantitative Biology/Genomics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"55SWeabg0cavusnqk-_7UE7DzhGwLYb0s3hB7rOfabw","pdfSize":"394776"}