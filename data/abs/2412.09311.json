{"id":"2412.09311","title":"Advancing Attribution-Based Neural Network Explainability through\n  Relative Absolute Magnitude Layer-Wise Relevance Propagation and\n  Multi-Component Evaluation","authors":"Davor Vukadin, Petar Afri\\'c, Marin \\v{S}ili\\'c, Goran Dela\\v{c}","authorsParsed":[["Vukadin","Davor",""],["Afrić","Petar",""],["Šilić","Marin",""],["Delač","Goran",""]],"versions":[{"version":"v1","created":"Thu, 12 Dec 2024 14:25:56 GMT"}],"updateDate":"2024-12-13","timestamp":1734013556000,"abstract":"  Recent advancement in deep-neural network performance led to the development\nof new state-of-the-art approaches in numerous areas. However, the black-box\nnature of neural networks often prohibits their use in areas where model\nexplainability and model transparency are crucial. Over the years, researchers\nproposed many algorithms to aid neural network understanding and provide\nadditional information to the human expert. One of the most popular methods\nbeing Layer-Wise Relevance Propagation (LRP). This method assigns local\nrelevance based on the pixel-wise decomposition of nonlinear classifiers. With\nthe rise of attribution method research, there has emerged a pressing need to\nassess and evaluate their performance. Numerous metrics have been proposed,\neach assessing an individual property of attribution methods such as\nfaithfulness, robustness or localization. Unfortunately, no single metric is\ndeemed optimal for every case, and researchers often use several metrics to\ntest the quality of the attribution maps. In this work, we address the\nshortcomings of the current LRP formulations and introduce a novel method for\ndetermining the relevance of input neurons through layer-wise relevance\npropagation. Furthermore, we apply this approach to the recently developed\nVision Transformer architecture and evaluate its performance against existing\nmethods on two image classification datasets, namely ImageNet and PascalVOC.\nOur results clearly demonstrate the advantage of our proposed method.\nFurthermore, we discuss the insufficiencies of current evaluation metrics for\nattribution-based explainability and propose a new evaluation metric that\ncombines the notions of faithfulness, robustness and contrastiveness. We\nutilize this new metric to evaluate the performance of various\nattribution-based methods. Our code is available at:\nhttps://github.com/davor10105/relative-absolute-magnitude-propagation\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"lhULTjPHWawlnSsxx27IiItnMOoGPa3fR9fXbE3BSyg","pdfSize":"28469233"}