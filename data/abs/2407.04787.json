{"id":"2407.04787","title":"Re-Tuning: Overcoming the Compositionality Limits of Large Language\n  Models with Recursive Tuning","authors":"Eric Pasewark, Kyle Montgomery, Kefei Duan, Dawn Song, Chenguang Wang","authorsParsed":[["Pasewark","Eric",""],["Montgomery","Kyle",""],["Duan","Kefei",""],["Song","Dawn",""],["Wang","Chenguang",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 18:02:28 GMT"}],"updateDate":"2024-07-09","timestamp":1720202548000,"abstract":"  We present a new method for large language models to solve compositional\ntasks. Although they have shown strong performance on traditional language\nunderstanding tasks, large language models struggle to solve compositional\ntasks, where the solution depends on solving smaller instances of the same\nproblem. We propose a natural approach to solve compositional tasks\nrecursively. Our method, Re-Tuning, tunes models to break down a problem into\nsubproblems, solve those subproblems, and combine the results. We show that our\nmethod significantly improves model performance on three representative\ncompositional tasks: integer addition, dynamic programming, and parity.\nCompared to state-of-the-art methods that keep intermediate steps towards\nsolving the problems, Re-Tuning achieves significantly higher accuracy and is\nmore GPU memory efficient.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LbukdQaKFl636y6ZIdTv1Pi4lZsxERW20D3yL8DBHeo","pdfSize":"3412808"}
