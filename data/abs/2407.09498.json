{"id":"2407.09498","title":"OT-VP: Optimal Transport-guided Visual Prompting for Test-Time\n  Adaptation","authors":"Yunbei Zhang, Akshay Mehra, Jihun Hamm","authorsParsed":[["Zhang","Yunbei",""],["Mehra","Akshay",""],["Hamm","Jihun",""]],"versions":[{"version":"v1","created":"Wed, 12 Jun 2024 18:30:03 GMT"},{"version":"v2","created":"Tue, 10 Sep 2024 07:03:32 GMT"}],"updateDate":"2024-09-11","timestamp":1718217003000,"abstract":"  Vision Transformers (ViTs) have demonstrated remarkable capabilities in\nlearning representations, but their performance is compromised when applied to\nunseen domains. Previous methods either engage in prompt learning during the\ntraining phase or modify model parameters at test time through entropy\nminimization. The former often overlooks unlabeled target data, while the\nlatter doesn't fully address domain shifts. In this work, our approach, Optimal\nTransport-guided Test-Time Visual Prompting (OT-VP), handles these problems by\nleveraging prompt learning at test time to align the target and source domains\nwithout accessing the training process or altering pre-trained model\nparameters. This method involves learning a universal visual prompt for the\ntarget domain by optimizing the Optimal Transport distance.OT-VP, with only\nfour learned prompt tokens, exceeds state-of-the-art performance across three\nstylistic datasets-PACS, VLCS, OfficeHome, and one corrupted dataset\nImageNet-C. Additionally, OT-VP operates efficiently, both in terms of memory\nand computation, and is adaptable for extension to online settings.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_ucUQriKCez-fyw_eHRnbPjAoZrjrZKM5XJqNf6JFfk","pdfSize":"758293"}