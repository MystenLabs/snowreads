{"id":"2412.16849","title":"OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks\n  with Reinforcement Fine-Tuning","authors":"Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Yuhang Wang, Jinlin Xiao,\n  Jitao Sang","authorsParsed":[["Zhang","Yuxiang",""],["Yang","Yuqi",""],["Shu","Jiangming",""],["Wang","Yuhang",""],["Xiao","Jinlin",""],["Sang","Jitao",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 04:21:30 GMT"}],"updateDate":"2024-12-24","timestamp":1734841290000,"abstract":"  OpenAI's recent introduction of Reinforcement Fine-Tuning (RFT) showcases the\npotential of reasoning foundation model and offers a new paradigm for\nfine-tuning beyond simple pattern imitation. This technical report presents\n\\emph{OpenRFT}, our attempt to fine-tune generalist reasoning models for\ndomain-specific tasks under the same settings as RFT. OpenRFT addresses two key\nchallenges of lacking reasoning step data and the limited quantity of training\nsamples, by leveraging the domain-specific samples in three ways: question\naugmentation, synthesizing reasoning-process data, and few-shot ICL. The\nevaluation is conducted on SciKnowEval, where OpenRFT achieves notable\nperformance gains with only $100$ domain-specific samples for each task. More\nexperimental results will be updated continuously in later versions. Source\ncodes, datasets, and models are disclosed at:\nhttps://github.com/ADaM-BJTU/OpenRFT\n","subjects":["Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"uSzyIQNVKaS3qiNmA5IWv-xeyCH97niyeFx0OlPcSkA","pdfSize":"1622889"}