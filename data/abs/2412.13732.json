{
  "id": "2412.13732",
  "title": "Modeling Multi-modal Cross-interaction for Multi-label Few-shot Image\n  Classification Based on Local Feature Selection",
  "authors": "Kun Yan, Zied Bouraoui, Fangyun Wei, Chang Xu, Ping Wang, Shoaib\n  Jameel, Steven Schockaert",
  "authorsParsed": [
    [
      "Yan",
      "Kun",
      ""
    ],
    [
      "Bouraoui",
      "Zied",
      ""
    ],
    [
      "Wei",
      "Fangyun",
      ""
    ],
    [
      "Xu",
      "Chang",
      ""
    ],
    [
      "Wang",
      "Ping",
      ""
    ],
    [
      "Jameel",
      "Shoaib",
      ""
    ],
    [
      "Schockaert",
      "Steven",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 11:10:18 GMT"
    },
    {
      "version": "v2",
      "created": "Mon, 24 Feb 2025 14:34:07 GMT"
    }
  ],
  "updateDate": "2025-02-25",
  "timestamp": 1734520218000,
  "abstract": "  The aim of multi-label few-shot image classification (ML-FSIC) is to assign\nsemantic labels to images, in settings where only a small number of training\nexamples are available for each label. A key feature of the multi-label setting\nis that an image often has several labels, which typically refer to objects\nappearing in different regions of the image. When estimating label prototypes,\nin a metric-based setting, it is thus important to determine which regions are\nrelevant for which labels, but the limited amount of training data and the\nnoisy nature of local features make this highly challenging. As a solution, we\npropose a strategy in which label prototypes are gradually refined. First, we\ninitialize the prototypes using word embeddings, which allows us to leverage\nprior knowledge about the meaning of the labels. Second, taking advantage of\nthese initial prototypes, we then use a Loss Change Measurement (LCM) strategy\nto select the local features from the training images (i.e. the support set)\nthat are most likely to be representative of a given label. Third, we construct\nthe final prototype of the label by aggregating these representative local\nfeatures using a multi-modal cross-interaction mechanism, which again relies on\nthe initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC,\nNUS-WIDE, and iMaterialist show that our model substantially improves the\ncurrent state-of-the-art.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "9Ml6monHQVWS2QLko0YAXqyXj-ssXXfGc7YDI5ND9E8",
  "pdfSize": "3326227"
}