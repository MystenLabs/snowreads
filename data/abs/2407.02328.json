{"id":"2407.02328","title":"Efficient Sparse Attention needs Adaptive Token Release","authors":"Chaoran Zhang, Lixin Zou, Dan Luo, Min Tang, Xiangyang Luo, Zihao Li\n  and Chenliang Li","authorsParsed":[["Zhang","Chaoran",""],["Zou","Lixin",""],["Luo","Dan",""],["Tang","Min",""],["Luo","Xiangyang",""],["Li","Zihao",""],["Li","Chenliang",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 14:58:44 GMT"}],"updateDate":"2024-07-03","timestamp":1719932324000,"abstract":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide array of text-centric tasks. However, their `large'\nscale introduces significant computational and storage challenges, particularly\nin managing the key-value states of the transformer, which limits their wider\napplicability. Therefore, we propose to adaptively release resources from\ncaches and rebuild the necessary key-value states. Particularly, we accomplish\nthis by a lightweight controller module to approximate an ideal top-$K$ sparse\nattention. This module retains the tokens with the highest top-$K$ attention\nweights and simultaneously rebuilds the discarded but necessary tokens, which\nmay become essential for future decoding. Comprehensive experiments in natural\nlanguage generation and modeling reveal that our method is not only competitive\nwith full attention in terms of performance but also achieves a significant\nthroughput improvement of up to 221.8%. The code for replication is available\non the https://github.com/WHUIR/ADORE.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"CX_2_M1Rdl6kKmtl-uLTf_OJwmt5aJY5q3TVmO2fmhU","pdfSize":"802975"}