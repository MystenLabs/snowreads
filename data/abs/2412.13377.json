{"id":"2412.13377","title":"DateLogicQA: Benchmarking Temporal Biases in Large Language Models","authors":"Gagan Bhatia, MingZe Tang, Cristina Mahanta, Madiha Kazi","authorsParsed":[["Bhatia","Gagan",""],["Tang","MingZe",""],["Mahanta","Cristina",""],["Kazi","Madiha",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 23:25:47 GMT"}],"updateDate":"2024-12-19","timestamp":1734477947000,"abstract":"  This paper introduces DateLogicQA, a benchmark with 190 questions covering\ndiverse date formats, temporal contexts, and reasoning types. We propose the\nSemantic Integrity Metric to assess tokenization quality and analyse two\nbiases: Representation-Level Bias, affecting embeddings, and Logical-Level\nBias, influencing reasoning outputs. Our findings provide a comprehensive\nevaluation of LLMs' capabilities and limitations in temporal reasoning,\nhighlighting key challenges in handling temporal data accurately. The GitHub\nrepository for our work is available at\nhttps://github.com/gagan3012/EAIS-Temporal-Bias\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"Oas0OPFPCSdEupyv_WvT9e-VXTxnfi8K5efY5OjEU48","pdfSize":"3639111"}