{"id":"2407.18041","title":"How to Train the Teacher Model for Effective Knowledge Distillation","authors":"Shayan Mohajer Hamidi, Xizhen Deng, Renhao Tan, Linfeng Ye, Ahmed\n  Hussein Salamah","authorsParsed":[["Hamidi","Shayan Mohajer",""],["Deng","Xizhen",""],["Tan","Renhao",""],["Ye","Linfeng",""],["Salamah","Ahmed Hussein",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 13:39:11 GMT"}],"updateDate":"2024-07-26","timestamp":1721914751000,"abstract":"  Recently, it was shown that the role of the teacher in knowledge distillation\n(KD) is to provide the student with an estimate of the true Bayes conditional\nprobability density (BCPD). Notably, the new findings propose that the\nstudent's error rate can be upper-bounded by the mean squared error (MSE)\nbetween the teacher's output and BCPD. Consequently, to enhance KD efficacy,\nthe teacher should be trained such that its output is close to BCPD in MSE\nsense. This paper elucidates that training the teacher model with MSE loss\nequates to minimizing the MSE between its output and BCPD, aligning with its\ncore responsibility of providing the student with a BCPD estimate closely\nresembling it in MSE terms. In this respect, through a comprehensive set of\nexperiments, we demonstrate that substituting the conventional teacher trained\nwith cross-entropy loss with one trained using MSE loss in state-of-the-art KD\nmethods consistently boosts the student's accuracy, resulting in improvements\nof up to 2.6\\%.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"p2nX5TDcxEAo_5hOG3MTQBQf42b7rhNPSS98WikZPT4","pdfSize":"877035","objectId":"0xb68b4a04b0be9f998a2e3b4305169c2da7d124e45aaae820e2a9368b8d851655","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
