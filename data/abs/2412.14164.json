{
  "id": "2412.14164",
  "title": "MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning",
  "authors": "Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen,\n  Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, Zhuang Liu",
  "authorsParsed": [
    [
      "Tong",
      "Shengbang",
      ""
    ],
    [
      "Fan",
      "David",
      ""
    ],
    [
      "Zhu",
      "Jiachen",
      ""
    ],
    [
      "Xiong",
      "Yunyang",
      ""
    ],
    [
      "Chen",
      "Xinlei",
      ""
    ],
    [
      "Sinha",
      "Koustuv",
      ""
    ],
    [
      "Rabbat",
      "Michael",
      ""
    ],
    [
      "LeCun",
      "Yann",
      ""
    ],
    [
      "Xie",
      "Saining",
      ""
    ],
    [
      "Liu",
      "Zhuang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 18:58:50 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734548330000,
  "abstract": "  In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "YVIAVsgPR68A_5bRdxXWyGStiP4b_PVdoXRxt2TaxC8",
  "pdfSize": "1657890"
}