{
  "id": "2412.05508",
  "title": "Optimizing Returns from Experimentation Programs",
  "authors": "Timothy Sudijono, Simon Ejdemyr, Apoorva Lal, Martin Tingley",
  "authorsParsed": [
    [
      "Sudijono",
      "Timothy",
      ""
    ],
    [
      "Ejdemyr",
      "Simon",
      ""
    ],
    [
      "Lal",
      "Apoorva",
      ""
    ],
    [
      "Tingley",
      "Martin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 7 Dec 2024 02:41:03 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733539263000,
  "abstract": "  Experimentation in online digital platforms is used to inform decision\nmaking. Specifically, the goal of many experiments is to optimize a metric of\ninterest. Null hypothesis statistical testing can be ill-suited to this task,\nas it is indifferent to the magnitude of effect sizes and opportunity costs.\nGiven access to a pool of related past experiments, we discuss how\nexperimentation practice should change when the goal is optimization. We survey\nthe literature on empirical Bayes analyses of A/B test portfolios, and single\nout the A/B Testing Problem (Azevedo et al., 2020) as a starting point, which\ntreats experimentation as a constrained optimization problem. We show that the\nframework can be solved with dynamic programming and implemented by\nappropriately tuning $p$-value thresholds. Furthermore, we develop several\nextensions of the A/B Testing Problem and discuss the implications of these\nresults on experimentation programs in industry. For example, under no-cost\nassumptions, firms should be testing many more ideas, reducing test allocation\nsizes, and relaxing $p$-value thresholds away from $p = 0.05$.\n",
  "subjects": [
    "Statistics/Methodology",
    "Economics/Econometrics",
    "Statistics/Applications"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "D7xBAAp0Jz-ZFK5EKcHKc1oQuU8TW--Ff8vR7Bjd1Tc",
  "pdfSize": "1187748"
}