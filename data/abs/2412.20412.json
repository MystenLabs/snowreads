{"id":"2412.20412","title":"Multi-Objective Large Language Model Unlearning","authors":"Zibin Pan, Shuwen Zhang, Yuesheng Zheng, Chi Li, Yuheng Cheng, Junhua\n  Zhao","authorsParsed":[["Pan","Zibin",""],["Zhang","Shuwen",""],["Zheng","Yuesheng",""],["Li","Chi",""],["Cheng","Yuheng",""],["Zhao","Junhua",""]],"versions":[{"version":"v1","created":"Sun, 29 Dec 2024 09:35:56 GMT"},{"version":"v2","created":"Sat, 4 Jan 2025 13:27:04 GMT"}],"updateDate":"2025-01-07","timestamp":1735464956000,"abstract":"  Machine unlearning in the domain of large language models (LLMs) has\nattracted great attention recently, which aims to effectively eliminate\nundesirable behaviors from LLMs without full retraining from scratch. In this\npaper, we explore the Gradient Ascent (GA) approach in LLM unlearning, which is\na proactive way to decrease the prediction probability of the model on the\ntarget data in order to remove their influence. We analyze two challenges that\nrender the process impractical: gradient explosion and catastrophic forgetting.\nTo address these issues, we propose Multi-Objective Large Language Model\nUnlearning (MOLLM) algorithm. We first formulate LLM unlearning as a\nmulti-objective optimization problem, in which the cross-entropy loss is\nmodified to the unlearning version to overcome the gradient explosion issue. A\ncommon descent update direction is then calculated, which enables the model to\nforget the target data while preserving the utility of the LLM. Our empirical\nresults verify that MoLLM outperforms the SOTA GA-based LLM unlearning methods\nin terms of unlearning effect and model utility preservation. The source code\nis available at https://github.com/zibinpan/MOLLM.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"2TSvM1bXEQwyLAcI7bpYMIiYFosUPPCMR0zuq90MWMM","pdfSize":"408257"}