{"id":"2412.14446","title":"VLM-AD: End-to-End Autonomous Driving through Vision-Language Model\n  Supervision","authors":"Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory P. Meyer, Siva Karthik\n  Mustikovela, Siddhartha Srinivasa, Eric M. Wolff, Xin Huang","authorsParsed":[["Xu","Yi",""],["Hu","Yuxin",""],["Zhang","Zaiwei",""],["Meyer","Gregory P.",""],["Mustikovela","Siva Karthik",""],["Srinivasa","Siddhartha",""],["Wolff","Eric M.",""],["Huang","Xin",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 01:53:36 GMT"}],"updateDate":"2024-12-20","timestamp":1734573216000,"abstract":"  Human drivers rely on commonsense reasoning to navigate diverse and dynamic\nreal-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models\nare typically optimized to mimic driving patterns observed in data, without\ncapturing the underlying reasoning processes. This limitation constrains their\nability to handle challenging driving scenarios. To close this gap, we propose\nVLM-AD, a method that leverages vision-language models (VLMs) as teachers to\nenhance training by providing additional supervision that incorporates\nunstructured reasoning information and structured action labels. Such\nsupervision enhances the model's ability to learn richer feature\nrepresentations that capture the rationale behind driving patterns.\nImportantly, our method does not require a VLM during inference, making it\npractical for real-time deployment. When integrated with state-of-the-art\nmethods, VLM-AD achieves significant improvements in planning accuracy and\nreduced collision rates on the nuScenes dataset.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"eLo3VUS5_rFxlkOdKntF-4aGFDMB0Bh8hE7MlJgZm-o","pdfSize":"5133496"}