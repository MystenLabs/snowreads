{"id":"2407.12007","title":"People will agree what I think: Investigating LLM's False Consensus\n  Effect","authors":"Junhyuk Choi, Yeseon Hong, and Bugeun Kim","authorsParsed":[["Choi","Junhyuk",""],["Hong","Yeseon",""],["Kim","Bugeun",""]],"versions":[{"version":"v1","created":"Sun, 16 Jun 2024 03:29:28 GMT"}],"updateDate":"2024-07-18","timestamp":1718508568000,"abstract":"  Large Language Models (LLMs) have recently been widely adopted on interactive\nsystems requiring communications. As the false belief in a model can harm the\nusability of such systems, LLMs should not have cognitive biases that humans\nhave. Especially psychologists focused on the False Consensus Effect (FCE),\nwhich can distract smooth communication by posing false beliefs. However,\nprevious studies have less examined FCE in LLMs thoroughly, which needs more\nconsideration of confounding biases, general situations, and prompt changes.\nTherefore, in this paper, we conduct two studies to deeply examine the FCE\nphenomenon in LLMs. In Study 1, we investigate whether LLMs have FCE. In Study\n2, we explore how various prompting styles affect the demonstration of FCE. As\na result of these studies, we identified that popular LLMs have FCE. Also, the\nresult specifies the conditions when the strength of FCE becomes larger or\nsmaller compared to normal usage.\n","subjects":["Computing Research Repository/Human-Computer Interaction","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"2lH6Civhhtr8cEJ0_-f0SFXC-52efF2m93aXPWxkuMs","pdfSize":"525278"}