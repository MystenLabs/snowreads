{"id":"2407.01411","title":"HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into\n  Multi-Task Transformers for Sequence Labelling","authors":"Jesus-German Ortiz-Barajas, Helena Gomez-Adorno, Thamar Solorio","authorsParsed":[["Ortiz-Barajas","Jesus-German",""],["Gomez-Adorno","Helena",""],["Solorio","Thamar",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 16:00:53 GMT"},{"version":"v2","created":"Tue, 2 Jul 2024 06:21:41 GMT"},{"version":"v3","created":"Sun, 25 Aug 2024 09:39:23 GMT"}],"updateDate":"2024-08-27","timestamp":1719849653000,"abstract":"  We present HyperLoader, a simple approach that combines different\nparameter-efficient fine-tuning methods in a multi-task setting. To achieve\nthis goal, our model uses a hypernetwork to generate the weights of these\nmodules based on the task, the transformer layer, and its position within this\nlayer. Our method combines the benefits of multi-task learning by capturing the\nstructure of all tasks while reducing the task interference problem by\nencapsulating the task-specific knowledge in the generated weights and the\nbenefits of combining different parameter-efficient methods to outperform\nfull-fine tuning. We provide empirical evidence that HyperLoader outperforms\nprevious approaches in most datasets and obtains the best average performance\nacross tasks in high-resource and low-resource scenarios.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"IMzOgRA9g72TZ6Kus7qUqTg9NnN8LUgbzxrU5oTx6w4","pdfSize":"630783","objectId":"0x75d7e2d3f7243dddbc44c22bf211a41add5070b58fd35bb1cf4ebb2822c3c083","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
