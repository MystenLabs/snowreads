{"id":"2407.11902","title":"Encapsulating Knowledge in One Prompt","authors":"Qi Li, Runpeng Yu, Xinchao Wang","authorsParsed":[["Li","Qi",""],["Yu","Runpeng",""],["Wang","Xinchao",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 16:35:23 GMT"}],"updateDate":"2024-07-17","timestamp":1721147723000,"abstract":"  This paradigm encapsulates knowledge from various models into a solitary\nprompt without altering the original models or requiring access to the training\ndata, which enables us to achieve efficient and convenient knowledge transfer\nin more realistic scenarios. From a practicality standpoint, this paradigm not\nonly for the first time proves the effectiveness of Visual Prompt in data\ninaccessible contexts, but also solves the problems of low model reusability\nand high storage resource consumption faced by traditional Data-Free Knowledge\nTransfer, which means that we can realize the parallel knowledge transfer of\nmultiple models without modifying any source model. Extensive experiments\nacross various datasets and models demonstrate the efficacy of the proposed\nKiOP knowledge transfer paradigm. Without access to real training data and with\nrigorous storage capacity constraints, it is also capable of yielding\nconsiderable outcomes when dealing with cross-model backbone setups and\nhandling parallel knowledge transfer processing requests with multiple (more\nthan 2) models.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8kR6y7dRRVmpUNstETDjx5EIcJpl4lTfUIDgKeMUGU4","pdfSize":"3201262"}