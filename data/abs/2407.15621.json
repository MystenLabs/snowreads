{"id":"2407.15621","title":"RadioRAG: Factual Large Language Models for Enhanced Diagnostics in\n  Radiology Using Dynamic Retrieval Augmented Generation","authors":"Soroosh Tayebi Arasteh, Mahshad Lotfinia, Keno Bressem, Robert\n  Siepmann, Dyke Ferber, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung,\n  Daniel Truhn","authorsParsed":[["Arasteh","Soroosh Tayebi",""],["Lotfinia","Mahshad",""],["Bressem","Keno",""],["Siepmann","Robert",""],["Ferber","Dyke",""],["Kuhl","Christiane",""],["Kather","Jakob Nikolas",""],["Nebelung","Sven",""],["Truhn","Daniel",""]],"versions":[{"version":"v1","created":"Mon, 22 Jul 2024 13:29:56 GMT"}],"updateDate":"2024-07-23","timestamp":1721654996000,"abstract":"  Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) in medicine. However LLMs often generate outdated or\ninaccurate information based on static training datasets. Retrieval augmented\ngeneration (RAG) mitigates this by integrating outside data sources. While\nprevious RAG systems used pre-assembled, fixed databases with limited\nflexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end\nframework that retrieves data from authoritative radiologic online sources in\nreal-time. RadioRAG is evaluated using a dedicated radiologic\nquestion-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of\nvarious LLMs when answering radiology-specific questions with and without\naccess to additional online information via RAG. Using 80 questions from RSNA\nCase Collection across radiologic subspecialties and 24 additional\nexpert-curated questions, for which the correct gold-standard answers were\navailable, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B\nand 70B]) were prompted with and without RadioRAG. RadioRAG retrieved\ncontext-specific information from www.radiopaedia.org in real-time and\nincorporated them into its reply. RadioRAG consistently improved diagnostic\naccuracy across all LLMs, with relative improvements ranging from 2% to 54%. It\nmatched or exceeded question answering without RAG across radiologic\nsubspecialties, particularly in breast imaging and emergency radiology.\nHowever, degree of improvement varied among models; GPT-3.5-turbo and\nMixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2\nshowed no improvement, highlighting variability in its effectiveness. LLMs\nbenefit when provided access to domain-specific data beyond their training\ndata. For radiology, RadioRAG establishes a robust framework that substantially\nimproves diagnostic accuracy and factuality in radiological question answering.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"QMR1sSs_djLjWBayaWWduiyC4BzpL_77lqdE5WvUAAU","pdfSize":"1069779"}