{"id":"2407.08693","title":"Robotic Control via Embodied Chain-of-Thought Reasoning","authors":"Micha{\\l} Zawalski and William Chen and Karl Pertsch and Oier Mees and\n  Chelsea Finn and Sergey Levine","authorsParsed":[["Zawalski","Micha≈Ç",""],["Chen","William",""],["Pertsch","Karl",""],["Mees","Oier",""],["Finn","Chelsea",""],["Levine","Sergey",""]],"versions":[{"version":"v1","created":"Thu, 11 Jul 2024 17:31:01 GMT"},{"version":"v2","created":"Fri, 12 Jul 2024 19:19:34 GMT"}],"updateDate":"2024-07-16","timestamp":1720719061000,"abstract":"  A key limitation of learned robot control policies is their inability to\ngeneralize outside their training data. Recent works on vision-language-action\nmodels (VLAs) have shown that the use of large, internet pre-trained\nvision-language models as the backbone of learned robot policies can\nsubstantially improve their robustness and generalization ability. Yet, one of\nthe most exciting capabilities of large vision-language models in other domains\nis their ability to reason iteratively through complex problems. Can that same\ncapability be brought into robotics to allow policies to improve performance by\nreasoning about a given task before acting? Naive use of \"chain-of-thought\"\n(CoT) style prompting is significantly less effective with standard VLAs\nbecause of the relatively simple training examples that are available to them.\nAdditionally, purely semantic reasoning about sub-tasks, as is common in\nregular CoT, is insufficient for robot policies that need to ground their\nreasoning in sensory observations and the robot state. To this end, we\nintroduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we\ntrain VLAs to perform multiple steps of reasoning about plans, sub-tasks,\nmotions, and visually grounded features like object bounding boxes and end\neffector positions, before predicting the robot action. We design a scalable\npipeline for generating synthetic training data for ECoT on large robot\ndatasets. We demonstrate, that ECoT increases the absolute success rate of\nOpenVLA, the current strongest open-source VLA policy, by 28% across\nchallenging generalization tasks, without any additional robot training data.\nAdditionally, ECoT makes it easier for humans to interpret a policy's failures\nand correct its behavior using natural language.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"4YPTF99VhjROI4UKK5epgW8_emgHlCRdpoGBE4tsMd0","pdfSize":"4819291"}