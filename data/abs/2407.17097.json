{"id":"2407.17097","title":"Towards Robust Knowledge Tracing Models via k-Sparse Attention","authors":"Shuyan Huang, Zitao Liu, Xiangyu Zhao, Weiqi Luo, Jian Weng","authorsParsed":[["Huang","Shuyan",""],["Liu","Zitao",""],["Zhao","Xiangyu",""],["Luo","Weiqi",""],["Weng","Jian",""]],"versions":[{"version":"v1","created":"Wed, 24 Jul 2024 08:49:18 GMT"}],"updateDate":"2024-07-25","timestamp":1721810958000,"abstract":"  Knowledge tracing (KT) is the problem of predicting students' future\nperformance based on their historical interaction sequences. With the advanced\ncapability of capturing contextual long-term dependency, attention mechanism\nbecomes one of the essential components in many deep learning based KT (DLKT)\nmodels. In spite of the impressive performance achieved by these attentional\nDLKT models, many of them are often vulnerable to run the risk of overfitting,\nespecially on small-scale educational datasets. Therefore, in this paper, we\npropose \\textsc{sparseKT}, a simple yet effective framework to improve the\nrobustness and generalization of the attention based DLKT approaches.\nSpecifically, we incorporate a k-selection module to only pick items with the\nhighest attention scores. We propose two sparsification heuristics : (1)\nsoft-thresholding sparse attention and (2) top-$K$ sparse attention. We show\nthat our \\textsc{sparseKT} is able to help attentional KT models get rid of\nirrelevant student interactions and have comparable predictive performance when\ncompared to 11 state-of-the-art KT models on three publicly available\nreal-world educational datasets. To encourage reproducible research, we make\nour data and code publicly available at\n\\url{https://github.com/pykt-team/pykt-toolkit}\\footnote{We merged our model to\nthe \\textsc{pyKT} benchmark at \\url{https://pykt.org/}.}.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"dD3TjjAJ4fdFQQpm9pt5kKVKmmbVNwK-0rneAo7FKc8","pdfSize":"2131802"}