{
  "id": "2412.12687",
  "title": "Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models",
  "authors": "Seungeun Oh, Jinhyuk Kim, Jihong Park, Seung-Woo Ko, Tony Q. S. Quek,\n  Seong-Lyun Kim",
  "authorsParsed": [
    [
      "Oh",
      "Seungeun",
      ""
    ],
    [
      "Kim",
      "Jinhyuk",
      ""
    ],
    [
      "Park",
      "Jihong",
      ""
    ],
    [
      "Ko",
      "Seung-Woo",
      ""
    ],
    [
      "Quek",
      "Tony Q. S.",
      ""
    ],
    [
      "Kim",
      "Seong-Lyun",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 09:08:18 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 18 Dec 2024 08:14:35 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734426498000,
  "abstract": "  This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),\nwherein the SLM locally measures its output uncertainty and skips both uplink\ntransmissions and LLM operations for tokens that are likely to be accepted.\nThis opportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputations by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Distributed, Parallel, and Cluster Computing",
    "Computer Science/Information Theory",
    "Computer Science/Networking and Internet Architecture",
    "Electrical Engineering and Systems Science/Signal Processing",
    "Mathematics/Information Theory"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "BnfJ-CVxSp_YOw9WCRwxh1r1x3oMjsbKdzQp-CLpGhE",
  "pdfSize": "4594484"
}