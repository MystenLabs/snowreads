{"id":"2407.05013","title":"Progress or Regress? Self-Improvement Reversal in Post-training","authors":"Ting Wu, Xuefeng Li, Pengfei Liu","authorsParsed":[["Wu","Ting",""],["Li","Xuefeng",""],["Liu","Pengfei",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 09:07:11 GMT"}],"updateDate":"2024-07-09","timestamp":1720256831000,"abstract":"  Self-improvement through post-training methods such as iterative preference\nlearning has been acclaimed for enhancing the problem-solving capabilities\n(e.g., mathematical reasoning) of Large Language Models (LLMs) without human\nintervention. However, as exploration deepens, it becomes crucial to assess\nwhether these improvements genuinely signify progress in solving more\nchallenging problems or if they could lead to unintended regressions. To\naddress this, we propose a comprehensive evaluative framework that goes beyond\nthe superficial pass@1 metric to scrutinize the underlying enhancements of\npost-training paradigms for self-improvement. Through rigorous experimentation\nand analysis across diverse problem-solving tasks, the empirical results point\nout the phenomenon of \\emph{self-improvement reversal}, where models showing\nimproved performance across benchmarks will paradoxically exhibit declines in\nbroader, essential capabilities, like output diversity and out-of-distribution\n(OOD) generalization. These findings indicate that current self-improvement\npractices through post-training are inadequate for equipping models to tackle\nmore complex problems. Furthermore, they underscore the necessity of our\ncritical evaluation metrics in discerning the \\emph{progress or regress}\ndichotomy for self-improving LLMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LI4eGlUm5tCTRZDzJ3Gi330xn66q7a9zSvgpeeTQH7s","pdfSize":"2403910"}
