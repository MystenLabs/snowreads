{"id":"2412.07191","title":"A Step towards Automated and Generalizable Tactile Map Generation using\n  Generative Adversarial Networks","authors":"David G Hobson, Majid Komeili","authorsParsed":[["Hobson","David G",""],["Komeili","Majid",""]],"versions":[{"version":"v1","created":"Tue, 10 Dec 2024 04:59:03 GMT"}],"updateDate":"2024-12-11","timestamp":1733806743000,"abstract":"  Blindness and visual impairments affect many people worldwide. For help with\nnavigation, people with visual impairments often rely on tactile maps that\nutilize raised surfaces and edges to convey information through touch. Although\nthese maps are helpful, they are often not widely available and current tools\nto automate their production have similar limitations including only working at\ncertain scales, for particular world regions, or adhering to specific tactile\nmap standards. To address these shortcomings, we train a proof-of-concept model\nas a first step towards applying computer vision techniques to help automate\nthe generation of tactile maps. We create a first-of-its-kind tactile maps\ndataset of street-views from Google Maps spanning 6500 locations and including\ndifferent tactile line- and area-like features. Generative adversarial network\n(GAN) models trained on a single zoom successfully identify key map elements,\nremove extraneous ones, and perform inpainting with median F1 and\nintersection-over-union (IoU) scores of better than 0.97 across all features.\nModels trained on two zooms experience only minor drops in performance, and\ngeneralize well both to unseen map scales and world regions. Finally, we\ndiscuss future directions towards a full implementation of a tactile map\nsolution that builds on our results.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fc-XtxMsFAhH1jofO0m5rWO2dv9YimKtggIMaSZKieQ","pdfSize":"3476902"}