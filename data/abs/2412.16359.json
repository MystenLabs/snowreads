{
  "id": "2412.16359",
  "title": "Human-Readable Adversarial Prompts: An Investigation into LLM\n  Vulnerabilities Using Situational Context",
  "authors": "Nilanjana Das, Edward Raff, Manas Gaur",
  "authorsParsed": [
    [
      "Das",
      "Nilanjana",
      ""
    ],
    [
      "Raff",
      "Edward",
      ""
    ],
    [
      "Gaur",
      "Manas",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 21:43:52 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734731032000,
  "abstract": "  Previous research on LLM vulnerabilities often relied on nonsensical\nadversarial prompts, which were easily detectable by automated methods. We\naddress this gap by focusing on human-readable adversarial prompts, a more\nrealistic and potent threat. Our key contributions are situation-driven attacks\nleveraging movie scripts to create contextually relevant, human-readable\nprompts that successfully deceive LLMs, adversarial suffix conversion to\ntransform nonsensical adversarial suffixes into meaningful text, and\nAdvPrompter with p-nucleus sampling, a method to generate diverse,\nhuman-readable adversarial suffixes, improving attack efficacy in models like\nGPT-3.5 and Gemma 7B. Our findings demonstrate that LLMs can be tricked by\nsophisticated adversaries into producing harmful responses with human-readable\nadversarial prompts and that there exists a scope for improvement when it comes\nto robust LLMs.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "ESU44s-T-qE3SUgCirDSdn7GaPh2DvNXGGScWI_2Wrs",
  "pdfSize": "1828093"
}