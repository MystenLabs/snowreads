{"id":"2407.01909","title":"Pinyin Regularization in Error Correction for Chinese Speech Recognition\n  with Large Language Models","authors":"Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang","authorsParsed":[["Tang","Zhiyuan",""],["Wang","Dong",""],["Huang","Shen",""],["Shang","Shidong",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 03:16:47 GMT"}],"updateDate":"2024-07-03","timestamp":1719890207000,"abstract":"  Recent studies have demonstrated the efficacy of large language models (LLMs)\nin error correction for automatic speech recognition (ASR). However, much of\nthe research focuses on the English language. This paper redirects the\nattention to Chinese. Firstly, we construct a specialized benchmark dataset\naimed at error correction for Chinese ASR with 724K hypotheses-transcription\npairs, named the Chinese Hypotheses Paradise dataset (ChineseHP), which\ncontains a wide range of scenarios and presents significant challenges.\nSubsequently, we conduct a preliminary evaluation using the dataset for both\ndirect-prompting and fine-tuning pre-trained LLMs. Furthermore, we propose a\nstraightforward method of Pinyin regularization for prompts, which involves the\ntranscription of Pinyin directly from text hypotheses. The experimental results\nreveal that Pinyin regularization consistently enhances the error-correcting\nability of LLMs when compared with those without regularization. The dataset is\navailable on the website.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"h24BrkSzW41LAPd-V0OzBIltle-Vr01bSzV6KdBukUY","pdfSize":"404844"}