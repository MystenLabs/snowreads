{"id":"2407.11591","title":"AdaptEval: Evaluating Large Language Models on Domain Adaptation for\n  Text Summarization","authors":"Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell","authorsParsed":[["Afzal","Anum",""],["Chalumattu","Ribin",""],["Matthes","Florian",""],["Mascarell","Laura",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 10:50:39 GMT"},{"version":"v2","created":"Mon, 22 Jul 2024 13:47:08 GMT"}],"updateDate":"2024-07-23","timestamp":1721127039000,"abstract":"  Despite the advances in the abstractive summarization task using Large\nLanguage Models (LLM), there is a lack of research that asses their abilities\nto easily adapt to different domains. We evaluate the domain adaptation\nabilities of a wide range of LLMs on the summarization task across various\ndomains in both fine-tuning and in-context learning settings. We also present\nAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a\ndomain benchmark and a set of metrics to facilitate the analysis of domain\nadaptation. Our results demonstrate that LLMs exhibit comparable performance in\nthe in-context learning setting, regardless of their parameter scale.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TbIJ33z2B_TnUWi6fX__9x-Q5N4k42pdsGDCCF2oybs","pdfSize":"274678"}