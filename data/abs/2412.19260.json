{
  "id": "2412.19260",
  "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes",
  "authors": "Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen,\n  Fei Xia, Thomas Lin",
  "authorsParsed": [
    [
      "Abacha",
      "Asma Ben",
      ""
    ],
    [
      "Yim",
      "Wen-wai",
      ""
    ],
    [
      "Fu",
      "Yujuan",
      ""
    ],
    [
      "Sun",
      "Zhaoyi",
      ""
    ],
    [
      "Yetisgen",
      "Meliha",
      ""
    ],
    [
      "Xia",
      "Fei",
      ""
    ],
    [
      "Lin",
      "Thomas",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 26 Dec 2024 15:54:10 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 2 Jan 2025 18:46:05 GMT"
    }
  ],
  "updateDate": "2025-01-03",
  "timestamp": 1735228450000,
  "abstract": "  Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Mr7QidiE7bTy0WeS75efYW04DMx4VN2PrJz2k8MUR4E",
  "pdfSize": "954006"
}