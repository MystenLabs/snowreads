{"id":"2412.13243","title":"In-Context Learning Distillation for Efficient Few-Shot Fine-Tuning","authors":"Yifei Duan, Liu Li, Zirui Zhai, Jinxia Yao","authorsParsed":[["Duan","Yifei",""],["Li","Liu",""],["Zhai","Zirui",""],["Yao","Jinxia",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 18:49:21 GMT"}],"updateDate":"2024-12-19","timestamp":1734461361000,"abstract":"  We applied few-shot in-context learning on the OPT-1.3B model for the natural\nlanguage inference task and employed knowledge distillation to internalize the\ncontext information, reducing model parameter from 1.3B to 125M and achieving a\nsize reduction from 2.5GB to 0.25GB. Compared to using in-context learning\nalone on similarly sized models, this context distillation approach achieved a\nnearly 50% improvement in out-of-domain accuracy, demonstrating superior\nknowledge transfer capabilities over prompt-based methods. Furthermore, this\napproach reduced memory consumption by up to 60% while delivering a 20%\nimprovement in out-of-domain accuracy compared to conventional pattern-based\nfine-tuning.\n","subjects":["Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"TgKJ4xhNAN85PCL-9epQ9bAIsumP8bZlTmAvmO8UJDI","pdfSize":"1001373"}