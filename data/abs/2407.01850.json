{"id":"2407.01850","title":"Purple-teaming LLMs with Adversarial Defender Training","authors":"Jingyan Zhou, Kun Li, Junan Li, Jiawen Kang, Minda Hu, Xixin Wu, Helen\n  Meng","authorsParsed":[["Zhou","Jingyan",""],["Li","Kun",""],["Li","Junan",""],["Kang","Jiawen",""],["Hu","Minda",""],["Wu","Xixin",""],["Meng","Helen",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 23:25:30 GMT"}],"updateDate":"2024-07-03","timestamp":1719876330000,"abstract":"  Existing efforts in safeguarding LLMs are limited in actively exposing the\nvulnerabilities of the target LLM and readily adapting to newly emerging safety\nrisks. To address this, we present Purple-teaming LLMs with Adversarial\nDefender training (PAD), a pipeline designed to safeguard LLMs by novelly\nincorporating the red-teaming (attack) and blue-teaming (safety training)\ntechniques. In PAD, we automatically collect conversational data that cover the\nvulnerabilities of an LLM around specific safety risks in a self-play manner,\nwhere the attacker aims to elicit unsafe responses and the defender generates\nsafe responses to these attacks. We then update both modules in a generative\nadversarial network style by training the attacker to elicit more unsafe\nresponses and updating the defender to identify them and explain the unsafe\nreason. Experimental results demonstrate that PAD significantly outperforms\nexisting baselines in both finding effective attacks and establishing a robust\nsafe guardrail. Furthermore, our findings indicate that PAD excels in striking\na balance between safety and overall model quality. We also reveal key\nchallenges in safeguarding LLMs, including defending multi-turn attacks and the\nneed for more delicate strategies to identify specific risks.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"h15tu99y1H8y3j9WK9uzSltr-9JHH5CQ5jDo8kk7HdU","pdfSize":"3054172"}