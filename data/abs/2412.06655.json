{
  "id": "2412.06655",
  "title": "Off-Policy Maximum Entropy RL with Future State and Action Visitation\n  Measures",
  "authors": "Adrien Bolland, Gaspard Lambrechts, Damien Ernst",
  "authorsParsed": [
    [
      "Bolland",
      "Adrien",
      ""
    ],
    [
      "Lambrechts",
      "Gaspard",
      ""
    ],
    [
      "Ernst",
      "Damien",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 16:56:06 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733763366000,
  "abstract": "  We introduce a new maximum entropy reinforcement learning framework based on\nthe distribution of states and actions visited by a policy. More precisely, an\nintrinsic reward function is added to the reward function of the Markov\ndecision process that shall be controlled. For each state and action, this\nintrinsic reward is the relative entropy of the discounted distribution of\nstates and actions (or features from these states and actions) visited during\nthe next time steps. We first prove that an optimal exploration policy, which\nmaximizes the expected discounted sum of intrinsic rewards, is also a policy\nthat maximizes a lower bound on the state-action value function of the decision\nprocess under some assumptions. We also prove that the visitation distribution\nused in the intrinsic reward definition is the fixed point of a contraction\noperator. Following, we describe how to adapt existing algorithms to learn this\nfixed point and compute the intrinsic rewards to enhance exploration. A new\npractical off-policy maximum entropy reinforcement learning algorithm is\nfinally introduced. Empirically, exploration policies have good state-action\nspace coverage, and high-performing control policies are computed efficiently.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Statistics/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "2Pb2MQVVimWU5Q7Se6aZ-2gSebRsXCn8gWVRU0FMQrI",
  "pdfSize": "2518866"
}