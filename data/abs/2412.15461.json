{"id":"2412.15461","title":"Asymptotic Extinction in Large Coordination Games","authors":"Desmond Chan, Bart De Keijzer, Tobias Galla, Stefanos Leonardos,\n  Carmine Ventre","authorsParsed":[["Chan","Desmond",""],["De Keijzer","Bart",""],["Galla","Tobias",""],["Leonardos","Stefanos",""],["Ventre","Carmine",""]],"versions":[{"version":"v1","created":"Thu, 19 Dec 2024 23:38:22 GMT"}],"updateDate":"2024-12-23","timestamp":1734651502000,"abstract":"  We study the exploration-exploitation trade-off for large multiplayer\ncoordination games where players strategise via Q-Learning, a common learning\nframework in multi-agent reinforcement learning. Q-Learning is known to have\ntwo shortcomings, namely non-convergence and potential equilibrium selection\nproblems, when there are multiple fixed points, called Quantal Response\nEquilibria (QRE). Furthermore, whilst QRE have full support for finite games,\nit is not clear how Q-Learning behaves as the game becomes large. In this\npaper, we characterise the critical exploration rate that guarantees\nconvergence to a unique fixed point, addressing the two shortcomings above.\nUsing a generating-functional method, we show that this rate increases with the\nnumber of players and the alignment of their payoffs. For many-player\ncoordination games with perfectly aligned payoffs, this exploration rate is\nroughly twice that of $p$-player zero-sum games. As for large games, we provide\na structural result for QRE, which suggests that as the game size increases,\nQ-Learning converges to a QRE near the boundary of the simplex of the action\nspace, a phenomenon we term asymptotic extinction, where a constant fraction of\nthe actions are played with zero probability at a rate $o(1/N)$ for an\n$N$-action game.\n","subjects":["Computer Science/Computer Science and Game Theory","Condensed Matter/Disordered Systems and Neural Networks"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"P2LoT4X5XNS9vMar-RUYtNOE1otefOa57pjRc8R4nns","pdfSize":"14404251"}