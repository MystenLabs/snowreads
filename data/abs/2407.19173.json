{"id":"2407.19173","title":"FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity\n  Measurement of Persian Social Networks Informal Texts","authors":"Seyed Mojtaba Sadjadi, Zeinab Rajabi, Leila Rabiei, Mohammad-Shahram\n  Moin","authorsParsed":[["Sadjadi","Seyed Mojtaba",""],["Rajabi","Zeinab",""],["Rabiei","Leila",""],["Moin","Mohammad-Shahram",""]],"versions":[{"version":"v1","created":"Sat, 27 Jul 2024 05:04:49 GMT"}],"updateDate":"2024-07-30","timestamp":1722056689000,"abstract":"  One fundamental task for NLP is to determine the similarity between two texts\nand evaluate the extent of their likeness. The previous methods for the Persian\nlanguage have low accuracy and are unable to comprehend the structure and\nmeaning of texts effectively. Additionally, these methods primarily focus on\nformal texts, but in real-world applications of text processing, there is a\nneed for robust methods that can handle colloquial texts. This requires\nalgorithms that consider the structure and significance of words based on\ncontext, rather than just the frequency of words. The lack of a proper dataset\nfor this task in the Persian language makes it important to develop such\nalgorithms and construct a dataset for Persian text. This paper introduces a\nnew transformer-based model to measure semantic similarity between Persian\ninformal short texts from social networks. In addition, a Persian dataset named\nFarSSiM has been constructed for this purpose, using real data from social\nnetworks and manually annotated and verified by a linguistic expert team. The\nproposed model involves training a large language model using the BERT\narchitecture from scratch. This model, called FarSSiBERT, is pre-trained on\napproximately 104 million Persian informal short texts from social networks,\nmaking it one of a kind in the Persian language. Moreover, a novel specialized\ninformal language tokenizer is provided that not only performs tokenization on\nformal texts well but also accurately identifies tokens that other Persian\ntokenizers are unable to recognize. It has been demonstrated that our proposed\nmodel outperforms ParsBERT, laBSE, and multilingual BERT in the Pearson and\nSpearman's coefficient criteria. Additionally, the pre-trained large language\nmodel has great potential for use in other NLP tasks on colloquial text and as\na tokenizer for less-known informal words.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"_8bBk-o5eUv9v8z7hwscW3pc1yndmC6KcijEh1SBJM4","pdfSize":"550550","objectId":"0xe8c713787149ac00bdf11740cba83c8f06b302e9c75f6be9ae27088655bd1de3","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
