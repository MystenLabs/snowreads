{"id":"2412.19720","title":"Sharpening Neural Implicit Functions with Frequency Consolidation Priors","authors":"Chao Chen, Yu-Shen Liu, Zhizhong Han","authorsParsed":[["Chen","Chao",""],["Liu","Yu-Shen",""],["Han","Zhizhong",""]],"versions":[{"version":"v1","created":"Fri, 27 Dec 2024 16:18:46 GMT"}],"updateDate":"2024-12-30","timestamp":1735316326000,"abstract":"  Signed Distance Functions (SDFs) are vital implicit representations to\nrepresent high fidelity 3D surfaces. Current methods mainly leverage a neural\nnetwork to learn an SDF from various supervisions including signed distances,\n3D point clouds, or multi-view images. However, due to various reasons\nincluding the bias of neural network on low frequency content, 3D unaware\nsampling, sparsity in point clouds, or low resolutions of images, neural\nimplicit representations still struggle to represent geometries with high\nfrequency components like sharp structures, especially for the ones learned\nfrom images or point clouds. To overcome this challenge, we introduce a method\nto sharpen a low frequency SDF observation by recovering its high frequency\ncomponents, pursuing a sharper and more complete surface. Our key idea is to\nlearn a mapping from a low frequency observation to a full frequency coverage\nin a data-driven manner, leading to a prior knowledge of shape consolidation in\nthe frequency domain, dubbed frequency consolidation priors. To better\ngeneralize a learned prior to unseen shapes, we introduce to represent\nfrequency components as embeddings and disentangle the embedding of the low\nfrequency component from the embedding of the full frequency component. This\ndisentanglement allows the prior to generalize on an unseen low frequency\nobservation by simply recovering its full frequency embedding through a\ntest-time self-reconstruction. Our evaluations under widely used benchmarks or\nreal scenes show that our method can recover high frequency component and\nproduce more accurate surfaces than the latest methods. The code, data, and\npre-trained models are available at \\url{https://github.com/chenchao15/FCP}.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pac2TBB5CIoYX56-I9zvXjgJuwUHB4ouLIC9LX6yrlc","pdfSize":"11223194"}