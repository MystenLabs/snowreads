{"id":"2407.02878","title":"Efficient Fusion and Task Guided Embedding for End-to-end Autonomous\n  Driving","authors":"Yipin Guo, Yilin Lang, Qinyuan Ren","authorsParsed":[["Guo","Yipin",""],["Lang","Yilin",""],["Ren","Qinyuan",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 07:45:58 GMT"},{"version":"v2","created":"Wed, 17 Jul 2024 00:50:39 GMT"}],"updateDate":"2024-07-18","timestamp":1719992758000,"abstract":"  To address the challenges of sensor fusion and safety risk prediction,\ncontemporary closed-loop autonomous driving neural networks leveraging\nimitation learning typically require a substantial volume of parameters and\ncomputational resources to run neural networks. Given the constrained\ncomputational capacities of onboard vehicular computers, we introduce a compact\nyet potent solution named EfficientFuser. This approach employs EfficientViT\nfor visual information extraction and integrates feature maps via cross\nattention. Subsequently, it utilizes a decoder-only transformer for the\namalgamation of multiple features. For prediction purposes, learnable vectors\nare embedded as tokens to probe the association between the task and sensor\nfeatures through attention. Evaluated on the CARLA simulation platform,\nEfficientFuser demonstrates remarkable efficiency, utilizing merely 37.6% of\nthe parameters and 8.7% of the computations compared to the state-of-the-art\nlightweight method with only 0.4% lower driving score, and the safety score\nneared that of the leading safety-enhanced method, showcasing its efficacy and\npotential for practical deployment in autonomous driving systems.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_zwxALfRfW32O-qrIlzM5KP-YNLHqv12tP77BBdov2c","pdfSize":"1736967"}