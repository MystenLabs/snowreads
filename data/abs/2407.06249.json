{"id":"2407.06249","title":"CodeUpdateArena: Benchmarking Knowledge Editing on API Updates","authors":"Zeyu Leo Liu, Shrey Pandit, Xi Ye, Eunsol Choi, Greg Durrett","authorsParsed":[["Liu","Zeyu Leo",""],["Pandit","Shrey",""],["Ye","Xi",""],["Choi","Eunsol",""],["Durrett","Greg",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 17:55:04 GMT"}],"updateDate":"2024-07-10","timestamp":1720461304000,"abstract":"  Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Software Engineering"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"VHPv2FKX0S3PUfNNamOgh9cLIgZ1gZcF0GYbtLdMBJo","pdfSize":"5471058"}