{"id":"2407.01300","title":"Collaborative Performance Prediction for Large Language Models","authors":"Qiyuan Zhang, Fuyuan Lyu, Xue Liu, Chen Ma","authorsParsed":[["Zhang","Qiyuan",""],["Lyu","Fuyuan",""],["Liu","Xue",""],["Ma","Chen",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 13:56:42 GMT"}],"updateDate":"2024-07-02","timestamp":1719842202000,"abstract":"  Comprehensively understanding and accurately predicting the performance of\nlarge language models across diverse downstream tasks has emerged as a pivotal\nchallenge in NLP research. The pioneering scaling law on downstream works\ndemonstrated intrinsic similarities within model families and utilized such\nsimilarities for performance prediction. However, they tend to overlook the\nsimilarities between model families and only consider design factors listed in\nthe original scaling law. To overcome these limitations, we introduce a novel\nframework, Collaborative Performance Prediction (CPP), which significantly\nenhances prediction accuracy by leveraging the historical performance of\nvarious models on downstream tasks and other design factors for both model and\ntask. We also collect a collaborative data sourced from online platforms\ncontaining both historical performance and additional design factors. With the\nsupport of the collaborative data, CPP not only surpasses traditional scaling\nlaws in predicting the performance of scaled LLMs but also facilitates a\ndetailed analysis of factor importance, an area previously overlooked.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XqH_xQuOjNlK6BBjkCSFzDOPbRGoGgHk6wCm60WP_Q8","pdfSize":"2908097"}