{
  "id": "2412.04291",
  "title": "Evolutionary Pre-Prompt Optimization for Mathematical Reasoning",
  "authors": "Mathurin Videau, Alessandro Leite, Marc Schoenauer, Olivier Teytaud",
  "authorsParsed": [
    [
      "Videau",
      "Mathurin",
      ""
    ],
    [
      "Leite",
      "Alessandro",
      ""
    ],
    [
      "Schoenauer",
      "Marc",
      ""
    ],
    [
      "Teytaud",
      "Olivier",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 16:12:06 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733415126000,
  "abstract": "  Recent advancements have highlighted that large language models (LLMs), when\ngiven a small set of task-specific examples, demonstrate remarkable\nproficiency, a capability that extends to complex reasoning tasks. In\nparticular, the combination of few-shot learning with the chain-of-thought\n(CoT) approach has been pivotal in steering models towards more logically\nconsistent conclusions. This paper explores the optimization of example\nselection for designing effective CoT pre-prompts and shows that the choice of\nthe optimization algorithm, typically in favor of comparison-based methods such\nas evolutionary computation, significantly enhances efficacy and feasibility.\nSpecifically, thanks to a limited exploitative and overfitted optimization,\nEvolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the\nnaive few-shot approach exceeding 10 absolute points in exact match scores on\nbenchmark datasets such as GSM8k and MathQA. These gains are consistent across\nvarious contexts and are further amplified when integrated with\nself-consistency (SC)\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "A-P3RquQjnCAWeYkOKgN3JyMtfoRfI1_WojMtx8l62U",
  "pdfSize": "1729714"
}