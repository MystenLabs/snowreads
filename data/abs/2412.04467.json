{"id":"2412.04467","title":"VisionZip: Longer is Better but Not Necessary in Vision Language Models","authors":"Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li,\n  Bei Yu, Jiaya Jia","authorsParsed":[["Yang","Senqiao",""],["Chen","Yukang",""],["Tian","Zhuotao",""],["Wang","Chengyao",""],["Li","Jingyao",""],["Yu","Bei",""],["Jia","Jiaya",""]],"versions":[{"version":"v1","created":"Thu, 5 Dec 2024 18:59:53 GMT"}],"updateDate":"2024-12-06","timestamp":1733425193000,"abstract":"  Recent advancements in vision-language models have enhanced performance by\nincreasing the length of visual tokens, making them much longer than text\ntokens and significantly raising computational costs. However, we observe that\nthe visual tokens generated by popular vision encoders, such as CLIP and\nSigLIP, contain significant redundancy. To address this, we introduce\nVisionZip, a simple yet effective method that selects a set of informative\ntokens for input to the language model, reducing visual token redundancy and\nimproving efficiency while maintaining model performance. The proposed\nVisionZip can be widely applied to image and video understanding tasks and is\nwell-suited for multi-turn dialogues in real-world scenarios, where previous\nmethods tend to underperform. Experimental results show that VisionZip\noutperforms the previous state-of-the-art method by at least 5% performance\ngains across nearly all settings. Moreover, our method significantly enhances\nmodel inference speed, improving the prefilling time by 8x and enabling the\nLLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while\nachieving better results. Furthermore, we analyze the causes of this redundancy\nand encourage the community to focus on extracting better visual features\nrather than merely increasing token length. Our code is available at\nhttps://github.com/dvlab-research/VisionZip .\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence","Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"ugMz8JUBjqogPGAyv7MIHr_t64owPyYRb2834e5t5v4","pdfSize":"6771502"}