{
  "id": "2412.20679",
  "title": "Differentiable Convex Optimization Layers in Neural Architectures:\n  Foundations and Perspectives",
  "authors": "Calder Katyal",
  "authorsParsed": [
    [
      "Katyal",
      "Calder",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 30 Dec 2024 03:18:24 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735528704000,
  "abstract": "  The integration of optimization problems within neural network architectures\nrepresents a fundamental shift from traditional approaches to handling\nconstraints in deep learning. While it is long known that neural networks can\nincorporate soft constraints with techniques such as regularization, strict\nadherence to hard constraints is generally more difficult. A recent advance in\nthis field, however, has addressed this problem by enabling the direct\nembedding of optimization layers as differentiable components within deep\nnetworks. This paper surveys the evolution and current state of this approach,\nfrom early implementations limited to quadratic programming, to more recent\nframeworks supporting general convex optimization problems. We provide a\ncomprehensive review of the background, theoretical foundations, and emerging\napplications of this technology. Our analysis includes detailed mathematical\nproofs and an examination of various use cases that demonstrate the potential\nof this hybrid approach. This work synthesizes developments at the intersection\nof optimization theory and deep learning, offering insights into both current\ncapabilities and future research directions in this rapidly evolving field.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Mathematics/Optimization and Control"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "O88Gejc-0g7bP1sx6vTLZnJTyy9bVxoUaI-vhjkDlzo",
  "pdfSize": "236684"
}