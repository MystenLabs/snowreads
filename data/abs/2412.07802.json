{"id":"2412.07802","title":"Language Model as Visual Explainer","authors":"Xingyi Yang, Xinchao Wang","authorsParsed":[["Yang","Xingyi",""],["Wang","Xinchao",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 20:46:23 GMT"}],"updateDate":"2024-12-12","timestamp":1733690783000,"abstract":"  In this paper, we present Language Model as Visual Explainer LVX, a\nsystematic approach for interpreting the internal workings of vision models\nusing a tree-structured linguistic explanation, without the need for model\ntraining. Central to our strategy is the collaboration between vision models\nand LLM to craft explanations. On one hand, the LLM is harnessed to delineate\nhierarchical visual attributes, while concurrently, a text-to-image API\nretrieves images that are most aligned with these textual concepts. By mapping\nthe collected texts and images to the vision model's embedding space, we\nconstruct a hierarchy-structured visual embedding tree. This tree is\ndynamically pruned and grown by querying the LLM using language templates,\ntailoring the explanation to the model. Such a scheme allows us to seamlessly\nincorporate new attributes while eliminating undesired concepts based on the\nmodel's representations. When applied to testing samples, our method provides\nhuman-understandable explanations in the form of attribute-laden trees. Beyond\nexplanation, we retrained the vision model by calibrating it on the generated\nconcept hierarchy, allowing the model to incorporate the refined knowledge of\nvisual attributes. To access the effectiveness of our approach, we introduce\nnew benchmarks and conduct rigorous evaluations, demonstrating its\nplausibility, faithfulness, and stability.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"HT14sEwNxUovsFimHmJrbUZ7LAjsMYD7M_njbVhsrUo","pdfSize":"19090554"}