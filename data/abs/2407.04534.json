{"id":"2407.04534","title":"Introducing 'Inside' Out of Distribution","authors":"Teddy Lazebnik","authorsParsed":[["Lazebnik","Teddy",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 14:22:13 GMT"}],"updateDate":"2024-07-08","timestamp":1720189333000,"abstract":"  Detecting and understanding out-of-distribution (OOD) samples is crucial in\nmachine learning (ML) to ensure reliable model performance. Current OOD\nstudies, in general, and in the context of ML, in particular, primarily focus\non extrapolatory OOD (outside), neglecting potential cases of interpolatory OOD\n(inside). This study introduces a novel perspective on OOD by suggesting OOD\ncan be divided into inside and outside cases. In addition, following this\nframework, we examine the inside-outside OOD profiles of datasets and their\nimpact on ML model performance. Our analysis shows that different\ninside-outside OOD profiles lead to nuanced declines in ML model performance,\nhighlighting the importance of distinguishing between these two cases for\ndeveloping effective counter-OOD methods.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"tZuZzFyyueKmsbcB60XM9_dkF8t9XVfWFI7WxiL7ZOU","pdfSize":"1307876"}