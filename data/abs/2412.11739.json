{
  "id": "2412.11739",
  "title": "Asymmetric Learning for Spectral Graph Neural Networks",
  "authors": "Fangbing Liu, Qing Wang",
  "authorsParsed": [
    [
      "Liu",
      "Fangbing",
      ""
    ],
    [
      "Wang",
      "Qing",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 16 Dec 2024 13:00:49 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734354049000,
  "abstract": "  Optimizing spectral graph neural networks (GNNs) remains a critical challenge\nin the field, yet the underlying processes are not well understood. In this\npaper, we investigate the inherent differences between graph convolution\nparameters and feature transformation parameters in spectral GNNs and their\nimpact on the optimization landscape. Our analysis reveals that these\ndifferences contribute to a poorly conditioned problem, resulting in suboptimal\nperformance. To address this issue, we introduce the concept of the block\ncondition number of the Hessian matrix, which characterizes the difficulty of\npoorly conditioned problems in spectral GNN optimization. We then propose an\nasymmetric learning approach, dynamically preconditioning gradients during\ntraining to alleviate poorly conditioned problems. Theoretically, we\ndemonstrate that asymmetric learning can reduce block condition numbers,\nfacilitating easier optimization. Extensive experiments on eighteen benchmark\ndatasets show that asymmetric learning consistently improves the performance of\nspectral GNNs for both heterophilic and homophilic graphs. This improvement is\nespecially notable for heterophilic graphs, where the optimization process is\ngenerally more complex than for homophilic graphs. Code is available at\nhttps://github.com/Mia-321/asym-opt.git.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "zf1CD0Jlb0AJJja05pVu6RoYBVyLHWOrgZo3GV1vZ3E",
  "pdfSize": "800182"
}