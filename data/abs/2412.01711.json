{
  "id": "2412.01711",
  "title": "Towards Resource Efficient and Interpretable Bias Mitigation in Large\n  Language Models",
  "authors": "Schrasing Tong, Eliott Zemour, Rawisara Lohanimit, Lalana Kagal",
  "authorsParsed": [
    [
      "Tong",
      "Schrasing",
      ""
    ],
    [
      "Zemour",
      "Eliott",
      ""
    ],
    [
      "Lohanimit",
      "Rawisara",
      ""
    ],
    [
      "Kagal",
      "Lalana",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 16:56:08 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733158568000,
  "abstract": "  Although large language models (LLMs) have demonstrated their effectiveness\nin a wide range of applications, they have also been observed to perpetuate\nunwanted biases present in the training data, potentially leading to harm for\nmarginalized communities. In this paper, we mitigate bias by leveraging small\nbiased and anti-biased expert models to obtain a debiasing signal that will be\nadded to the LLM output at decoding-time. This approach combines resource\nefficiency with interpretability and can be optimized for mitigating specific\ntypes of bias, depending on the target use case. Experiments on mitigating\ngender, race, and religion biases show a reduction in bias on several local and\nglobal bias metrics while preserving language model performance.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "n8BN8AsS3UPkLezX5silJwbwchzUrSlMAxk4ZVXPhVI",
  "pdfSize": "893741"
}