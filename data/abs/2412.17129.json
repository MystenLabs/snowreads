{
  "id": "2412.17129",
  "title": "Uncovering the Visual Contribution in Audio-Visual Speech Recognition",
  "authors": "Zhaofeng Lin, Naomi Harte",
  "authorsParsed": [
    [
      "Lin",
      "Zhaofeng",
      ""
    ],
    [
      "Harte",
      "Naomi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 22 Dec 2024 18:34:58 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 17 Jan 2025 11:26:55 GMT"
    }
  ],
  "updateDate": "2025-01-20",
  "timestamp": 1734892498000,
  "abstract": "  Audio-Visual Speech Recognition (AVSR) combines auditory and visual speech\ncues to enhance the accuracy and robustness of speech recognition systems.\nRecent advancements in AVSR have improved performance in noisy environments\ncompared to audio-only counterparts. However, the true extent of the visual\ncontribution, and whether AVSR systems fully exploit the available cues in the\nvisual domain, remains unclear. This paper assesses AVSR systems from a\ndifferent perspective, by considering human speech perception. We use three\nsystems: Auto-AVSR, AVEC and AV-RelScore. We first quantify the visual\ncontribution using effective SNR gains at 0 dB and then investigate the use of\nvisual information in terms of its temporal distribution and word-level\ninformativeness. We show that low WER does not guarantee high SNR gains. Our\nresults suggest that current methods do not fully exploit visual information,\nand we recommend future research to report effective SNR gains alongside WERs.\n",
  "subjects": [
    "Electrical Engineering and Systems Science/Audio and Speech Processing",
    "Computer Science/Sound"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "i8aXVTq-PcpXUihGOLNTTXqRKZz4wlWVhAg0rbxrIRQ",
  "pdfSize": "217093"
}