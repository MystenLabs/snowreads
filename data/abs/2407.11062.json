{"id":"2407.11062","title":"EfficientQAT: Efficient Quantization-Aware Training for Large Language\n  Models","authors":"Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng\n  Zhang, Yu Qiao, Ping Luo","authorsParsed":[["Chen","Mengzhao",""],["Shao","Wenqi",""],["Xu","Peng",""],["Wang","Jiahao",""],["Gao","Peng",""],["Zhang","Kaipeng",""],["Qiao","Yu",""],["Luo","Ping",""]],"versions":[{"version":"v1","created":"Wed, 10 Jul 2024 17:53:30 GMT"}],"updateDate":"2024-07-17","timestamp":1720634010000,"abstract":"  Large language models (LLMs) are integral to modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it demands substantial training\nresources to optimize model weights and quantization parameters. To address\nthis, we propose Efficient Quantization-Aware Training (EfficientQAT), a novel\nquantization technique for compressing LLMs. EfficientQAT involves two\nconsecutive phases: Block-wise training of all parameters (Block-AP) and\nend-to-end training of quantization parameters (E2E-QP). Block-AP sequentially\nconducts quantization-aware training for all parameters in each transformer\nblock with block-wise reconstruction, maintaining efficiency by avoiding\ntraining the entire LLM. Initialized with quantized model, E2E-QP then trains\nonly quantization parameters (step sizes) end-to-end, enhancing efficiency with\na fixed quantized backbone and reduced trainable parameter count. Extensive\nexperiments demonstrate that EfficientQAT outperforms previous quantization\nmethods across a range of models, including base LLMs, instruction-tuned LLMs,\nand multimodal LLMs, with scales from 7B to 70B parameters at various\nquantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model\non a single A100-80GB GPU in 41 hours, with less than 3\\% accuracy degradation\ncompared to the full precision (69.48 vs. 72.41). Notably, this INT2 quantized\n70B model obtains a 1.67 accuracy gain over the Llama-2-13B model (69.48 vs.\n67.81) while requiring less memory (19.2GB vs. 24.2GB). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"64OX7pKGwKe_PKIkMBI4n1O5N8JwdvFwOWlyPcrWUNs","pdfSize":"593885","objectId":"0x68377d1cd2c67b8eee9bb846502dafb2a8832fda6e786794e01b2a7189dc154e","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
