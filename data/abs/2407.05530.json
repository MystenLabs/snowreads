{"id":"2407.05530","title":"This&That: Language-Gesture Controlled Video Generation for Robot\n  Planning","authors":"Boyang Wang, Nikhil Sridhar, Chao Feng, Mark Van der Merwe, Adam\n  Fishman, Nima Fazeli, Jeong Joon Park","authorsParsed":[["Wang","Boyang",""],["Sridhar","Nikhil",""],["Feng","Chao",""],["Van der Merwe","Mark",""],["Fishman","Adam",""],["Fazeli","Nima",""],["Park","Jeong Joon",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 00:28:41 GMT"}],"updateDate":"2024-07-09","timestamp":1720398521000,"abstract":"  We propose a robot learning method for communicating, planning, and executing\na wide range of tasks, dubbed This&That. We achieve robot planning for general\ntasks by leveraging the power of video generative models trained on\ninternet-scale data containing rich physical and semantic context. In this\nwork, we tackle three fundamental challenges in video-based planning: 1)\nunambiguous task communication with simple human instructions, 2) controllable\nvideo generation that respects user intents, and 3) translating visual planning\ninto robot actions. We propose language-gesture conditioning to generate\nvideos, which is both simpler and clearer than existing language-only methods,\nespecially in complex and uncertain environments. We then suggest a behavioral\ncloning design that seamlessly incorporates the video plans. This&That\ndemonstrates state-of-the-art effectiveness in addressing the above three\nchallenges, and justifies the use of video generation as an intermediate\nrepresentation for generalizable task planning and execution. Project website:\nhttps://cfeng16.github.io/this-and-that/.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"Kmv1hvYZequqBIX0dNc5lh6RwEFgut4hVsCBpnz70zc","pdfSize":"20480411"}
