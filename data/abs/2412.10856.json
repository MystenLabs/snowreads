{
  "id": "2412.10856",
  "title": "RWKV-Lite: Deeply Compressed RWKV for Resource-Constrained Devices",
  "authors": "Wonkyo Choe, Yangfeng Ji, and Felix Xiaozhu Lin",
  "authorsParsed": [
    [
      "Choe",
      "Wonkyo",
      ""
    ],
    [
      "Ji",
      "Yangfeng",
      ""
    ],
    [
      "Lin",
      "Felix Xiaozhu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 14 Dec 2024 15:11:07 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 19 Dec 2024 20:34:57 GMT"
    },
    {
      "version": "v3",
      "created": "Fri, 31 Jan 2025 06:34:12 GMT"
    }
  ],
  "updateDate": "2025-02-03",
  "timestamp": 1734189067000,
  "abstract": "  To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Performance"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "B3__pg7hnieTOcV9tGTFHKMPL8VHLK05eZpr1kv63Bo",
  "pdfSize": "1760004"
}