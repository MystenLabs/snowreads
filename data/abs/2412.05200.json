{
  "id": "2412.05200",
  "title": "Are Frontier Large Language Models Suitable for Q&A in Science Centres?",
  "authors": "Jacob Watson, Fabr\\'icio G\\'oes, Marco Volpe, Talles Medeiros",
  "authorsParsed": [
    [
      "Watson",
      "Jacob",
      ""
    ],
    [
      "Góes",
      "Fabrício",
      ""
    ],
    [
      "Volpe",
      "Marco",
      ""
    ],
    [
      "Medeiros",
      "Talles",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 6 Dec 2024 17:28:43 GMT"
    }
  ],
  "updateDate": "2024-12-09",
  "timestamp": 1733506123000,
  "abstract": "  This paper investigates the suitability of frontier Large Language Models\n(LLMs) for Q&A interactions in science centres, with the aim of boosting\nvisitor engagement while maintaining factual accuracy. Using a dataset of\nquestions collected from the National Space Centre in Leicester (UK), we\nevaluated responses generated by three leading models: OpenAI's GPT-4, Claude\n3.5 Sonnet, and Google Gemini 1.5. Each model was prompted for both standard\nand creative responses tailored to an 8-year-old audience, and these responses\nwere assessed by space science experts based on accuracy, engagement, clarity,\nnovelty, and deviation from expected answers. The results revealed a trade-off\nbetween creativity and accuracy, with Claude outperforming GPT and Gemini in\nboth maintaining clarity and engaging young audiences, even when asked to\ngenerate more creative responses. Nonetheless, experts observed that higher\nnovelty was generally associated with reduced factual reliability across all\nmodels. This study highlights the potential of LLMs in educational settings,\nemphasizing the need for careful prompt engineering to balance engagement with\nscientific rigor.\n",
  "subjects": [
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "FYWFxhpcTKAN7UnH2n_cxiDbTz3JFQr6uLoSHFfKv20",
  "pdfSize": "610663"
}