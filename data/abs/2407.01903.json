{"id":"2407.01903","title":"Text-Aware Diffusion for Policy Learning","authors":"Calvin Luo, Mandy He, Zilai Zeng, Chen Sun","authorsParsed":[["Luo","Calvin",""],["He","Mandy",""],["Zeng","Zilai",""],["Sun","Chen",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 03:08:20 GMT"}],"updateDate":"2024-07-03","timestamp":1719889700000,"abstract":"  Training an agent to achieve particular goals or perform desired behaviors is\noften accomplished through reinforcement learning, especially in the absence of\nexpert demonstrations. However, supporting novel goals or behaviors through\nreinforcement learning requires the ad-hoc design of appropriate reward\nfunctions, which quickly becomes intractable. To address this challenge, we\npropose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a\npretrained, frozen text-conditioned diffusion model to compute dense zero-shot\nreward signals for text-aligned policy learning. We hypothesize that\nlarge-scale pretrained generative models encode rich priors that can supervise\na policy to behave not only in a text-aligned manner, but also in alignment\nwith a notion of naturalness summarized from internet-scale training data. In\nour experiments, we demonstrate that TADPoLe is able to learn policies for\nnovel goal-achievement and continuous locomotion behaviors specified by natural\nlanguage, in both Humanoid and Dog environments. The behaviors are learned\nzero-shot without ground-truth rewards or expert demonstrations, and are\nqualitatively more natural according to human evaluation. We further show that\nTADPoLe performs competitively when applied to robotic manipulation tasks in\nthe Meta-World environment.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_1ZhWWo3EbxYdMsbsa0aLz19Nz4viap1gvCaiI8XcNo","pdfSize":"49105391"}