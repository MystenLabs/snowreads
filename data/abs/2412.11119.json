{
  "id": "2412.11119",
  "title": "Impact of Adversarial Attacks on Deep Learning Model Explainability",
  "authors": "Gazi Nazia Nur, Mohammad Ahnaf Sadat",
  "authorsParsed": [
    [
      "Nur",
      "Gazi Nazia",
      ""
    ],
    [
      "Sadat",
      "Mohammad Ahnaf",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 15 Dec 2024 08:41:37 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734252097000,
  "abstract": "  In this paper, we investigate the impact of adversarial attacks on the\nexplainability of deep learning models, which are commonly criticized for their\nblack-box nature despite their capacity for autonomous feature extraction. This\nblack-box nature can affect the perceived trustworthiness of these models. To\naddress this, explainability techniques such as GradCAM, SmoothGrad, and LIME\nhave been developed to clarify model decision-making processes. Our research\nfocuses on the robustness of these explanations when models are subjected to\nadversarial attacks, specifically those involving subtle image perturbations\nthat are imperceptible to humans but can significantly mislead models. For\nthis, we utilize attack methods like the Fast Gradient Sign Method (FGSM) and\nthe Basic Iterative Method (BIM) and observe their effects on model accuracy\nand explanations. The results reveal a substantial decline in model accuracy,\nwith accuracies dropping from 89.94% to 58.73% and 45.50% under FGSM and BIM\nattacks, respectively. Despite these declines in accuracy, the explanation of\nthe models measured by metrics such as Intersection over Union (IoU) and Root\nMean Square Error (RMSE) shows negligible changes, suggesting that these\nmetrics may not be sensitive enough to detect the presence of adversarial\nperturbations.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "WfKrn5MMviqp_syiPTjNEcwfsxizfz0VQrRmMaoZMLg",
  "pdfSize": "888493"
}