{"id":"2407.14733","title":"Hard Prompts Made Interpretable: Sparse Entropy Regularization for\n  Prompt Tuning with RL","authors":"Yunseon Choi, Sangmin Bae, Seonghyun Ban, Minchan Jeong, Chuheng\n  Zhang, Lei Song, Li Zhao, Jiang Bian, Kee-Eung Kim","authorsParsed":[["Choi","Yunseon",""],["Bae","Sangmin",""],["Ban","Seonghyun",""],["Jeong","Minchan",""],["Zhang","Chuheng",""],["Song","Lei",""],["Zhao","Li",""],["Bian","Jiang",""],["Kim","Kee-Eung",""]],"versions":[{"version":"v1","created":"Sat, 20 Jul 2024 03:10:19 GMT"}],"updateDate":"2024-07-23","timestamp":1721445019000,"abstract":"  With the advent of foundation models, prompt tuning has positioned itself as\nan important technique for directing model behaviors and eliciting desired\nresponses. Prompt tuning regards selecting appropriate keywords included into\nthe input, thereby adapting to the downstream task without adjusting or\nfine-tuning the model parameters. There is a wide range of work in prompt\ntuning, from approaches that directly harness the backpropagated gradient\nsignals from the model, to those employing black-box optimization such as\nreinforcement learning (RL) methods. Our primary focus is on RLPrompt, which\naims to find optimal prompt tokens leveraging soft Q-learning. While the\nresults show promise, we have observed that the prompts frequently appear\nunnatural, which impedes their interpretability. We address this limitation by\nusing sparse Tsallis entropy regularization, a principled approach to filtering\nout unlikely tokens from consideration. We extensively evaluate our approach\nacross various tasks, including few-shot text classification, unsupervised text\nstyle transfer, and textual inversion from images. The results indicate a\nnotable improvement over baselines, highlighting the efficacy of our approach\nin addressing the challenges of prompt tuning. Moreover, we show that the\nprompts discovered using our method are more natural and interpretable compared\nto those from other baselines.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"NWVo5HE7-Lc5M_AAgakmX3YkeeyRYfe_sEAC8l9KNTo","pdfSize":"19973883"}