{
  "id": "2412.12441",
  "title": "Numerical Pruning for Efficient Autoregressive Models",
  "authors": "Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Jing Liu, Ruiyi Zhang, Ryan\n  A. Rossi, Hao Tan, Tong Yu, Xiang Chen, Yufan Zhou, Tong Sun, Pu Zhao, Yanzhi\n  Wang, Jiuxiang Gu",
  "authorsParsed": [
    [
      "Shen",
      "Xuan",
      ""
    ],
    [
      "Song",
      "Zhao",
      ""
    ],
    [
      "Zhou",
      "Yufa",
      ""
    ],
    [
      "Chen",
      "Bo",
      ""
    ],
    [
      "Liu",
      "Jing",
      ""
    ],
    [
      "Zhang",
      "Ruiyi",
      ""
    ],
    [
      "Rossi",
      "Ryan A.",
      ""
    ],
    [
      "Tan",
      "Hao",
      ""
    ],
    [
      "Yu",
      "Tong",
      ""
    ],
    [
      "Chen",
      "Xiang",
      ""
    ],
    [
      "Zhou",
      "Yufan",
      ""
    ],
    [
      "Sun",
      "Tong",
      ""
    ],
    [
      "Zhao",
      "Pu",
      ""
    ],
    [
      "Wang",
      "Yanzhi",
      ""
    ],
    [
      "Gu",
      "Jiuxiang",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 01:09:23 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734397763000,
  "abstract": "  Transformers have emerged as the leading architecture in deep learning,\nproving to be versatile and highly effective across diverse domains beyond\nlanguage and image processing. However, their impressive performance often\nincurs high computational costs due to their substantial model size. This paper\nfocuses on compressing decoder-only transformer-based autoregressive models\nthrough structural weight pruning to improve the model efficiency while\npreserving performance for both language and image generation tasks.\nSpecifically, we propose a training-free pruning method that calculates a\nnumerical score with Newton's method for the Attention and MLP modules,\nrespectively. Besides, we further propose another compensation algorithm to\nrecover the pruned model for better performance. To verify the effectiveness of\nour method, we provide both theoretical support and extensive experiments. Our\nexperiments show that our method achieves state-of-the-art performance with\nreduced memory usage and faster generation speeds on GPUs.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "WlB5mtdphdph3R4XlCOltnS4qzOjgaPV5PCgRv3NNIM",
  "pdfSize": "2584940"
}