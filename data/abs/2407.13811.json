{"id":"2407.13811","title":"Which objects help me to act effectively? Reasoning about\n  physically-grounded affordances","authors":"Anne Kemmeren, Gertjan Burghouts, Michael van Bekkum, Wouter Meijer,\n  Jelle van Mil","authorsParsed":[["Kemmeren","Anne",""],["Burghouts","Gertjan",""],["van Bekkum","Michael",""],["Meijer","Wouter",""],["van Mil","Jelle",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 11:08:57 GMT"}],"updateDate":"2024-07-22","timestamp":1721300937000,"abstract":"  For effective interactions with the open world, robots should understand how\ninteractions with known and novel objects help them towards their goal. A key\naspect of this understanding lies in detecting an object's affordances, which\nrepresent the potential effects that can be achieved by manipulating the object\nin various ways. Our approach leverages a dialogue of large language models\n(LLMs) and vision-language models (VLMs) to achieve open-world affordance\ndetection. Given open-vocabulary descriptions of intended actions and effects,\nthe useful objects in the environment are found. By grounding our system in the\nphysical world, we account for the robot's embodiment and the intrinsic\nproperties of the objects it encounters. In our experiments, we have shown that\nour method produces tailored outputs based on different embodiments or intended\neffects. The method was able to select a useful object from a set of\ndistractors. Finetuning the VLM for physical properties improved overall\nperformance. These results underline the importance of grounding the affordance\nsearch in the physical world, by taking into account robot embodiment and the\nphysical properties of objects.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"v8aSalFmfIRAqyBRe1WnxhAH-LDDxAWqyJNgczS6j1Q","pdfSize":"11848186"}