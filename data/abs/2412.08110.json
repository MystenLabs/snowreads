{"id":"2412.08110","title":"Barking Up The Syntactic Tree: Enhancing VLM Training with Syntactic\n  Losses","authors":"Jiayun Luo, Mir Rayat Imtiaz Hossain, Boyang Li, Leonid Sigal","authorsParsed":[["Luo","Jiayun",""],["Hossain","Mir Rayat Imtiaz",""],["Li","Boyang",""],["Sigal","Leonid",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 05:36:18 GMT"}],"updateDate":"2024-12-12","timestamp":1733895378000,"abstract":"  Vision-Language Models (VLMs) achieved strong performance on a variety of\ntasks (e.g., image-text retrieval, visual question answering). However, most\nVLMs rely on coarse-grained image-caption pairs for alignment, relying on data\nvolume to resolve ambiguities and ground linguistic concepts in images. The\nricher semantic and syntactic structure within text is largely overlooked. To\naddress this, we propose HIerarchically STructured Learning (HIST) that\nenhances VLM training without any additional supervision, by hierarchically\ndecomposing captions into the constituent Subject, Noun Phrases, and Composite\nPhrases. Entailment between these constituent components allows us to formulate\nadditional regularization constraints on the VLM attention maps. Specifically,\nwe introduce two novel loss functions: (1) Subject Loss, which aligns image\ncontent with the subject of corresponding phrase, acting as an entailment of\nstandard contrastive/matching losses at the Phrase level; (2) Addition Loss, to\nbalance attention across multiple objects. HIST is general, and can be applied\nto any VLM for which attention between vision and language can be computed; we\nillustrate its efficacy on BLIP and ALBEF. HIST outperforms baseline VLMs,\nachieving up to +9.8% improvement in visual grounding, +6.3% in multi-object\nreferring segmentation, +1.1% in image-text retrieval, and +0.2% in visual\nquestion answering, underscoring the value of structuring learning in VLMs.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Computation and Language","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"wu5mO94tprbA1EAC0T3tF0fH977g7Lsf7hwa1ksG5JM","pdfSize":"21983326"}