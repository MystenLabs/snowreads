{"id":"2407.13343","title":"Learning-From-Mistakes Prompting for Indigenous Language Translation","authors":"You-Cheng Liao, Chen-Jui Yu, Chi-Yi Lin, He-Feng Yun, Yen-Hsiang Wang,\n  Hsiao-Min Li, Yao-Chung Fan","authorsParsed":[["Liao","You-Cheng",""],["Yu","Chen-Jui",""],["Lin","Chi-Yi",""],["Yun","He-Feng",""],["Wang","Yen-Hsiang",""],["Li","Hsiao-Min",""],["Fan","Yao-Chung",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 09:41:20 GMT"}],"updateDate":"2024-07-19","timestamp":1721295680000,"abstract":"  Using large language models, this paper presents techniques to improve\nextremely low-resourced indigenous language translations. Our approaches are\ngrounded in the use of (1) the presence of a datastore consisting of a limited\nnumber of parallel translation examples, (2) the inherent capabilities of LLMs\nlike GPT-3.5, and (3) a word-level translation dictionary. We harness the\npotential of LLMs and in-context learning techniques in such a setting for\nusing LLMs as universal translators for extremely low-resourced languages. Our\nmethodology hinges on utilizing LLMs as language compilers for selected\nlanguage pairs, hypothesizing that they could internalize syntactic structures\nto facilitate accurate translation. We introduce three techniques: KNNPrompting\nwith Retrieved Prompting Context, Chain-of-Thought Prompting and\nLearningfrom-Mistakes Prompting, with the last method addressing past errors.\nThe evaluation results suggest that, even with limited corpora, LLMs can\neffectively translate extremely low-resource languages when paired with proper\nprompting.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JwWibZYOnWPRVneOz2jucVIrf6u6E8fJBsukp2mqRlI","pdfSize":"955149","objectId":"0x8b81f5d4ef3071f7c62d66d0a4692ad1c87dff60136eebf476375953f755f9eb","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
