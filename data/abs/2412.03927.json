{
  "id": "2412.03927",
  "title": "MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language\n  Models",
  "authors": "Ming-Chang Chiu, Shicheng Wen, Pin-Yu Chen, Xuezhe Ma",
  "authorsParsed": [
    [
      "Chiu",
      "Ming-Chang",
      ""
    ],
    [
      "Wen",
      "Shicheng",
      ""
    ],
    [
      "Chen",
      "Pin-Yu",
      ""
    ],
    [
      "Ma",
      "Xuezhe",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 5 Dec 2024 07:06:17 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733382377000,
  "abstract": "  In vision-language models (VLMs), the ability to perceive and interpret color\nand physical environment is crucial for achieving contextually accurate\nunderstanding and interaction. However, despite advances in multimodal\nmodeling, there remains a significant lack of specialized datasets that\nrigorously evaluate a model's capacity to discern subtle color variations and\nspatial context -- critical elements for situational comprehension and reliable\ndeployment across real-world applications. Toward that goal, we curate\nMegaCOIN, a high-quality, human-labeled dataset based on \\emph{real} images\nwith various contextual attributes. MegaCOIN consists of two parts:\nMegaCOIN-Instruct, which serves as a supervised fine-tuning (SFT) dataset for\nVLMs; and MegaCOIN-Bench, an annotated test set that can be used as a\nstand-alone QA dataset. MegaCOIN~provides three annotated features for 220,000\nreal images: foreground color, background color, and description of an object's\nphysical environment, constituting 660k human annotations. In addition,\nMegaCOIN can be applied to benchmark domain generalization (DG) algorithms. We\nexplore benchmarking DG methods in the linear probing setup for VLM and show\nsome new insights. Last but not least, we show that VLMs, including GPT-4o,\nhave subpar color recognition capabilities, and fine-tuning with MegaCOIN can\nresult in improved performance on visual evaluation tasks. In certain cases,\nMegaCOIN fine-tuned small-scale opensource models such as LLaVA and Bunny can\noutperform closed-source GPT-4o. We hope the utilities of MegaCOIN can shed\nlight on the directions VLMs can improve and provide a more complex platform\nfor domain generalization algorithms.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "etfxNY_Xid0bLrpmo1homnPVftenEKL-63nlJCsR8eU",
  "pdfSize": "1675852"
}