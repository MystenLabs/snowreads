{"id":"2407.04603","title":"AWT: Transferring Vision-Language Models via Augmentation, Weighting,\n  and Transportation","authors":"Yuhan Zhu, Yuyang Ji, Zhiyu Zhao, Gangshan Wu, Limin Wang","authorsParsed":[["Zhu","Yuhan",""],["Ji","Yuyang",""],["Zhao","Zhiyu",""],["Wu","Gangshan",""],["Wang","Limin",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 15:52:23 GMT"}],"updateDate":"2024-07-08","timestamp":1720194743000,"abstract":"  Pre-trained vision-language models (VLMs) have shown impressive results in\nvarious visual classification tasks. However, we often fail to fully unleash\ntheir potential when adapting them for new concept understanding due to limited\ninformation on new classes. To address this limitation, we introduce a novel\nadaptation framework, AWT (Augment, Weight, then Transport). AWT comprises\nthree key components: augmenting inputs with diverse visual perspectives and\nenriched class descriptions through image transformations and language models;\ndynamically weighting inputs based on the prediction entropy; and employing\noptimal transport to mine semantic correlations in the vision-language space.\nAWT can be seamlessly integrated into various VLMs, enhancing their zero-shot\ncapabilities without additional training and facilitating few-shot learning\nthrough an integrated multimodal adapter module. We verify AWT in multiple\nchallenging scenarios, including zero-shot and few-shot image classification,\nzero-shot video action recognition, and out-of-distribution generalization. AWT\nconsistently outperforms the state-of-the-art methods in each setting. In\naddition, our extensive studies further demonstrate AWT's effectiveness and\nadaptability across different VLMs, architectures, and scales.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"8FCkYErYoN9fVCfpaCg0HxJjOIxpO-K2Sa1gatqLrfg","pdfSize":"15103621"}