{
  "id": "2412.17592",
  "title": "Investigating Length Issues in Document-level Machine Translation",
  "authors": "Ziqian Peng, Rachel Bawden, Fran\\c{c}ois Yvon",
  "authorsParsed": [
    [
      "Peng",
      "Ziqian",
      ""
    ],
    [
      "Bawden",
      "Rachel",
      ""
    ],
    [
      "Yvon",
      "Fran√ßois",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 14:08:45 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734962925000,
  "abstract": "  Transformer architectures are increasingly effective at processing and\ngenerating very long chunks of texts, opening new perspectives for\ndocument-level machine translation (MT). In this work, we challenge the ability\nof MT systems to handle texts comprising up to several thousands of tokens. We\ndesign and implement a new approach designed to precisely measure the effect of\nlength increments on MT outputs. Our experiments with two representative\narchitectures unambiguously show that (a)~translation performance decreases\nwith the length of the input text; (b)~the position of sentences within the\ndocument matters and translation quality is higher for sentences occurring\nearlier in a document. We further show that manipulating the distribution of\ndocument lengths and of positional embeddings only marginally mitigates such\nproblems. Our results suggest that even though document-level MT is\ncomputationally feasible, it does not yet match the performance of\nsentence-based MT.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "F4kUfyTWHQ8SufeKkiu0dH7-soce1k3DZWkdmqtp69g",
  "pdfSize": "585318"
}