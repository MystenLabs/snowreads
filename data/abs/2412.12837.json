{"id":"2412.12837","title":"Scrutinizing the Vulnerability of Decentralized Learning to Membership\n  Inference Attacks","authors":"Ousmane Touat, Jezekael Brunon, Yacine Belal, Julien Nicolas, Mohamed\n  Maouche, C\\'esar Sabater, Sonia Ben Mokhtar","authorsParsed":[["Touat","Ousmane",""],["Brunon","Jezekael",""],["Belal","Yacine",""],["Nicolas","Julien",""],["Maouche","Mohamed",""],["Sabater","CÃ©sar",""],["Mokhtar","Sonia Ben",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 12:02:47 GMT"},{"version":"v2","created":"Thu, 6 Feb 2025 11:47:26 GMT"}],"updateDate":"2025-02-07","timestamp":1734436967000,"abstract":"  The primary promise of decentralized learning is to allow users to engage in\nthe training of machine learning models in a collaborative manner while keeping\ntheir data on their premises and without relying on any central entity.\nHowever, this paradigm necessitates the exchange of model parameters or\ngradients between peers. Such exchanges can be exploited to infer sensitive\ninformation about training data, which is achieved through privacy attacks (e.g\nMembership Inference Attacks -- MIA). In order to devise effective defense\nmechanisms, it is important to understand the factors that increase/reduce the\nvulnerability of a given decentralized learning architecture to MIA. In this\nstudy, we extensively explore the vulnerability to MIA of various decentralized\nlearning architectures by varying the graph structure (e.g number of\nneighbors), the graph dynamics, and the aggregation strategy, across diverse\ndatasets and data distributions. Our key finding, which to the best of our\nknowledge we are the first to report, is that the vulnerability to MIA is\nheavily correlated to (i) the local model mixing strategy performed by each\nnode upon reception of models from neighboring nodes and (ii) the global mixing\nproperties of the communication graph. We illustrate these results\nexperimentally using four datasets and by theoretically analyzing the mixing\nproperties of various decentralized architectures. Our paper draws a set of\nlessons learned for devising decentralized learning systems that reduce by\ndesign the vulnerability to MIA.\n","subjects":["Computer Science/Machine Learning","Computer Science/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"X0PRs25f6bN0JijVO2dDODXXong6aKyeubTLcQTaVAc","pdfSize":"1087393"}