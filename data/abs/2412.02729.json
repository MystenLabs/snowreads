{
  "id": "2412.02729",
  "title": "Resource-Adaptive Successive Doubling for Hyperparameter Optimization\n  with Large Datasets on High-Performance Computing Systems",
  "authors": "Marcel Aach, Rakesh Sarma, Helmut Neukirchen, Morris Riedel, Andreas\n  Lintermann",
  "authorsParsed": [
    [
      "Aach",
      "Marcel",
      ""
    ],
    [
      "Sarma",
      "Rakesh",
      ""
    ],
    [
      "Neukirchen",
      "Helmut",
      ""
    ],
    [
      "Riedel",
      "Morris",
      ""
    ],
    [
      "Lintermann",
      "Andreas",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 3 Dec 2024 11:25:48 GMT"
    }
  ],
  "updateDate": "2024-12-05",
  "timestamp": 1733225148000,
  "abstract": "  On High-Performance Computing (HPC) systems, several hyperparameter\nconfigurations can be evaluated in parallel to speed up the Hyperparameter\nOptimization (HPO) process. State-of-the-art HPO methods follow a bandit-based\napproach and build on top of successive halving, where the final performance of\na combination is estimated based on a lower than fully trained fidelity\nperformance metric and more promising combinations are assigned more resources\nover time. Frequently, the number of epochs is treated as a resource, letting\nmore promising combinations train longer. Another option is to use the number\nof workers as a resource and directly allocate more workers to more promising\nconfigurations via data-parallel training. This article proposes a novel\nResource-Adaptive Successive Doubling Algorithm (RASDA), which combines a\nresource-adaptive successive doubling scheme with the plain Asynchronous\nSuccessive Halving Algorithm (ASHA). Scalability of this approach is shown on\nup to 1,024 Graphics Processing Units (GPUs) on modern HPC systems. It is\napplied to different types of Neural Networks (NNs) and trained on large\ndatasets from the Computer Vision (CV), Computational Fluid Dynamics (CFD), and\nAdditive Manufacturing (AM) domains, where performing more than one full\ntraining run is usually infeasible. Empirical results show that RASDA\noutperforms ASHA by a factor of up to 1.9 with respect to the runtime. At the\nsame time, the solution quality of final ASHA models is maintained or even\nsurpassed by the implicit batch size scheduling of RASDA. With RASDA,\nsystematic HPO is applied to a terabyte-scale scientific dataset for the first\ntime in the literature, enabling efficient optimization of complex models on\nmassive scientific data. The implementation of RASDA is available on\nhttps://github.com/olympiquemarcel/rasda\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Distributed, Parallel, and Cluster Computing"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "NiQYltj3_BkKwd51q-BS80bt5qAnYS9LoGrhbKiAM0M",
  "pdfSize": "966999"
}