{"id":"2412.12464","title":"LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph\n  Reasoning for Cold-start Sequential Recommendation","authors":"Keigo Sakurai, Ren Togo, Takahiro Ogawa, Miki Haseyama","authorsParsed":[["Sakurai","Keigo",""],["Togo","Ren",""],["Ogawa","Takahiro",""],["Haseyama","Miki",""]],"versions":[{"version":"v1","created":"Tue, 17 Dec 2024 01:52:15 GMT"}],"updateDate":"2024-12-18","timestamp":1734400335000,"abstract":"  Knowledge Graphs (KGs) represent relationships between entities in a graph\nstructure and have been widely studied as promising tools for realizing\nrecommendations that consider the accurate content information of items.\nHowever, traditional KG-based recommendation methods face fundamental\nchallenges: insufficient consideration of temporal information and poor\nperformance in cold-start scenarios. On the other hand, Large Language Models\n(LLMs) can be considered databases with a wealth of knowledge learned from the\nweb data, and they have recently gained attention due to their potential\napplication as recommendation systems. Although approaches that treat LLMs as\nrecommendation systems can leverage LLMs' high recommendation literacy, their\ninput token limitations make it impractical to consider the entire\nrecommendation domain dataset and result in scalability issues. To address\nthese challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning\nmodel (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive\nexploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we\ntrained a recommendation agent through reinforcement learning using a reward\nfunction that integrates different recommendation strategies, including LLM's\nintuition and KG embeddings. By incorporating temporal awareness through prompt\nengineering and generating textual representations of user preferences from\nlimited interactions, LIKR can improve recommendation performance in cold-start\nscenarios. Furthermore, LIKR can avoid scalability issues by using KGs to\nrepresent recommendation domain datasets and limiting the LLM's output to KG\nexploration strategies. Experiments on real-world datasets demonstrate that our\nmodel outperforms state-of-the-art recommendation methods in cold-start\nsequential recommendation scenarios.\n","subjects":["Computer Science/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"tF7sX11MzByGBf_pwKFfNGpAHWjEvj8FlqM2xgH3GHA","pdfSize":"927095"}