{
  "id": "2412.00884",
  "title": "Leveraging Intermediate Neural Collapse with Simplex ETFs for Efficient\n  Deep Neural Networks",
  "authors": "Emily Liu",
  "authorsParsed": [
    [
      "Liu",
      "Emily",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 1 Dec 2024 16:44:55 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733071495000,
  "abstract": "  Neural collapse is a phenomenon observed during the terminal phase of neural\nnetwork training, characterized by the convergence of network activations,\nclass means, and linear classifier weights to a simplex equiangular tight frame\n(ETF), a configuration of vectors that maximizes mutual distance within a\nsubspace. This phenomenon has been linked to improved interpretability,\nrobustness, and generalization in neural networks. However, its potential to\nguide neural network training and regularization remains underexplored.\nPrevious research has demonstrated that constraining the final layer of a\nneural network to a simplex ETF can reduce the number of trainable parameters\nwithout sacrificing model accuracy. Furthermore, deep fully connected networks\nexhibit neural collapse not only in the final layer but across all layers\nbeyond a specific effective depth. Using these insights, we propose two novel\ntraining approaches: Adaptive-ETF, a generalized framework that enforces\nsimplex ETF constraints on all layers beyond the effective depth, and\nETF-Transformer, which applies simplex ETF constraints to the feedforward\nlayers within transformer blocks. We show that these approaches achieve\ntraining and testing performance comparable to those of their baseline\ncounterparts while significantly reducing the number of learnable parameters.\n",
  "subjects": [
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "mRWq23H5PJU1Gh4zqyHqBMymVbpmvdNDXWKr5dj8qkY",
  "pdfSize": "849352"
}