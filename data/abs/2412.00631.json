{
  "id": "2412.00631",
  "title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific\n  Instruction Tuning",
  "authors": "Yang Wu, Huayi Zhang, Yizheng Jiao, Lin Ma, Xiaozhong Liu, Jinhong Yu,\n  Dongyu Zhang, Dezhi Yu, Wei Xu",
  "authorsParsed": [
    [
      "Wu",
      "Yang",
      ""
    ],
    [
      "Zhang",
      "Huayi",
      ""
    ],
    [
      "Jiao",
      "Yizheng",
      ""
    ],
    [
      "Ma",
      "Lin",
      ""
    ],
    [
      "Liu",
      "Xiaozhong",
      ""
    ],
    [
      "Yu",
      "Jinhong",
      ""
    ],
    [
      "Zhang",
      "Dongyu",
      ""
    ],
    [
      "Yu",
      "Dezhi",
      ""
    ],
    [
      "Xu",
      "Wei",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 1 Dec 2024 01:01:09 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733014869000,
  "abstract": "  Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human-controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "i2ejWLCsnlCTKT7RaZlsYQdOkJ4PBXcCeOkxr1NdVbs",
  "pdfSize": "1105186"
}