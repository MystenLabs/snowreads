{"id":"2407.08976","title":"Computational-Statistical Trade-off in Kernel Two-Sample Testing with\n  Random Fourier Features","authors":"Ikjun Choi and Ilmun Kim","authorsParsed":[["Choi","Ikjun",""],["Kim","Ilmun",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 04:08:01 GMT"}],"updateDate":"2024-07-15","timestamp":1720757281000,"abstract":"  Recent years have seen a surge in methods for two-sample testing, among which\nthe Maximum Mean Discrepancy (MMD) test has emerged as an effective tool for\nhandling complex and high-dimensional data. Despite its success and widespread\nadoption, the primary limitation of the MMD test has been its quadratic-time\ncomplexity, which poses challenges for large-scale analysis. While various\napproaches have been proposed to expedite the procedure, it has been unclear\nwhether it is possible to attain the same power guarantee as the MMD test at\nsub-quadratic time cost. To fill this gap, we revisit the approximated MMD test\nusing random Fourier features, and investigate its computational-statistical\ntrade-off. We start by revealing that the approximated MMD test is pointwise\nconsistent in power only when the number of random features approaches\ninfinity. We then consider the uniform power of the test and study the\ntime-power trade-off under the minimax testing framework. Our result shows\nthat, by carefully choosing the number of random features, it is possible to\nattain the same minimax separation rates as the MMD test within sub-quadratic\ntime. We demonstrate this point under different distributional assumptions such\nas densities in a Sobolev ball. Our theoretical findings are corroborated by\nsimulation studies.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning","Mathematics/Statistics Theory","Statistics/Statistics Theory"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"5Kt7RlDFdOyMN-zC18ODpU0Ak11Pn0_BQ2OD9HtNHvY","pdfSize":"1106869"}