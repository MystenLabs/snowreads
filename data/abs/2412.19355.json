{"id":"2412.19355","title":"Quantum-Inspired Weight-Constrained Neural Network: Reducing Variable\n  Numbers by 100x Compared to Standard Neural Networks","authors":"Shaozhi Li, M Sabbir Salek, Binayyak Roy, Yao Wang, Mashrur Chowdhury","authorsParsed":[["Li","Shaozhi",""],["Salek","M Sabbir",""],["Roy","Binayyak",""],["Wang","Yao",""],["Chowdhury","Mashrur",""]],"versions":[{"version":"v1","created":"Thu, 26 Dec 2024 21:35:12 GMT"}],"updateDate":"2024-12-30","timestamp":1735248912000,"abstract":"  Although quantum machine learning has shown great promise, the practical\napplication of quantum computers remains constrained in the noisy\nintermediate-scale quantum era. To take advantage of quantum machine learning,\nwe investigate the underlying mathematical principles of these quantum models\nand adapt them to classical machine learning frameworks. Specifically, we\ndevelop a classical weight-constrained neural network that generates weights\nbased on quantum-inspired insights. We find that this approach can reduce the\nnumber of variables in a classical neural network by a factor of 135 while\npreserving its learnability. In addition, we develop a dropout method to\nenhance the robustness of quantum machine learning models, which are highly\nsusceptible to adversarial attacks. This technique can also be applied to\nimprove the adversarial resilience of the classical weight-constrained neural\nnetwork, which is essential for industry applications, such as self-driving\nvehicles. Our work offers a novel approach to reduce the complexity of large\nclassical neural networks, addressing a critical challenge in machine learning.\n","subjects":["Physics/Quantum Physics"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"stVon3StEFoJO0Z8pNCEPKRfoJ359lRqQC29Xgl4Odk","pdfSize":"36814504"}