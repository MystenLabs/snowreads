{
  "id": "2412.20116",
  "title": "Multi-agent reinforcement learning in the all-or-nothing public goods\n  game on networks",
  "authors": "Benedikt Valentin Meylahn",
  "authorsParsed": [
    [
      "Meylahn",
      "Benedikt Valentin",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sat, 28 Dec 2024 11:01:28 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735383688000,
  "abstract": "  We study interpersonal trust by means of the all-or-nothing public goods game\nbetween agents on a network. The agents are endowed with the simple yet\nadaptive learning rule, exponential moving average, by which they estimate the\nbehavior of their neighbors in the network. Theoretically we show that in the\nlong-time limit this multi-agent reinforcement learning process always\neventually results in indefinite contribution to the public good or indefinite\ndefection (no agent contributing to the public good). However, by simulation of\nthe pre-limit behavior, we see that on complex network structures there may be\nmixed states in which the process seems to stabilize before actual convergence\nto states in which agent beliefs and actions are all the same. In these\nmetastable states the local network characteristics can determine whether\nagents have high or low trust in their neighbors. More generally it is found\nthat more dense networks result in lower rates of contribution to the public\ngood. This has implications for how one can spread global contribution toward a\npublic good by enabling smaller local interactions.\n",
  "subjects": [
    "Computer Science/Computer Science and Game Theory",
    "Physics/Physics and Society"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "3b8Xj33MDvssHGr2TXX5WN6aXwsey_JCBiA7X1YTb_g",
  "pdfSize": "2853807"
}