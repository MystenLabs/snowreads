{"id":"2407.16664","title":"Towards scalable efficient on-device ASR with transfer learning","authors":"Laxmi Pandey, Ke Li, Jinxi Guo, Debjyoti Paul, Arthur Guo, Jay\n  Mahadeokar, Xuedong Zhang","authorsParsed":[["Pandey","Laxmi",""],["Li","Ke",""],["Guo","Jinxi",""],["Paul","Debjyoti",""],["Guo","Arthur",""],["Mahadeokar","Jay",""],["Zhang","Xuedong",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 17:29:02 GMT"}],"updateDate":"2024-07-24","timestamp":1721755742000,"abstract":"  Multilingual pretraining for transfer learning significantly boosts the\nrobustness of low-resource monolingual ASR models. This study systematically\ninvestigates three main aspects: (a) the impact of transfer learning on model\nperformance during initial training or fine-tuning, (b) the influence of\ntransfer learning across dataset domains and languages, and (c) the effect on\nrare-word recognition compared to non-rare words. Our finding suggests that\nRNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word\nError Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across\nlanguages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%\ncompared to monolingual baselines for MLS and in-house datasets. Out-of-domain\npretraining leads to 28% higher WERR than in-domain pretraining. Both rare and\nnon-rare words benefit, with rare words showing greater improvements with\nout-of-domain pretraining, and non-rare words with in-domain pretraining.\n","subjects":["Computing Research Repository/Computation and Language","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"yIjnFKkGbO6dMQchp7ytZGaAuAS4lnSpUU5vEppLRaI","pdfSize":"231794"}