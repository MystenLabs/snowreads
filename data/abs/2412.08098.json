{"id":"2412.08098","title":"What You See Is Not Always What You Get: An Empirical Study of Code\n  Comprehension by Large Language Models","authors":"Bangshuo Zhu, Jiawen Wen, Huaming Chen","authorsParsed":[["Zhu","Bangshuo",""],["Wen","Jiawen",""],["Chen","Huaming",""]],"versions":[{"version":"v1","created":"Wed, 11 Dec 2024 04:52:41 GMT"},{"version":"v2","created":"Fri, 14 Feb 2025 05:21:37 GMT"}],"updateDate":"2025-02-17","timestamp":1733892761000,"abstract":"  Recent studies have demonstrated outstanding capabilities of large language\nmodels (LLMs) in software engineering tasks, including code generation and\ncomprehension. While LLMs have shown significant potential in assisting with\ncoding, it is perceived that LLMs are vulnerable to adversarial attacks. In\nthis paper, we investigate the vulnerability of LLMs to imperceptible attacks,\nwhere hidden character manipulation in source code misleads LLMs' behaviour\nwhile remaining undetectable to human reviewers. We devise these attacks into\nfour distinct categories and analyse their impacts on code analysis and\ncomprehension tasks. These four types of imperceptible coding character attacks\ninclude coding reordering, invisible coding characters, code deletions, and\ncode homoglyphs. To comprehensively benchmark the robustness of current LLMs\nsolutions against the attacks, we present a systematic experimental evaluation\non multiple state-of-the-art LLMs. Our experimental design introduces two key\nperformance metrics, namely model confidence using log probabilities of\nresponse, and the response correctness. A set of controlled experiments are\nconducted using a large-scale perturbed and unperturbed code snippets as the\nprimary prompt input. Our findings confirm the susceptibility of LLMs to\nimperceptible coding character attacks, while different LLMs present different\nnegative correlations between perturbation magnitude and performance. These\nresults highlight the urgent need for robust LLMs capable of manoeuvring\nbehaviours under imperceptible adversarial conditions. We anticipate this work\nprovides valuable insights for enhancing the security and trustworthiness of\nLLMs in software engineering applications.\n","subjects":["Computer Science/Software Engineering","Computer Science/Artificial Intelligence","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gX_iP-SWE8-0MgHkAjbKwbjWIeXLt6c54fSSe8OQX34","pdfSize":"675535"}