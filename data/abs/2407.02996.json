{"id":"2407.02996","title":"Are Large Language Models Consistent over Value-laden Questions?","authors":"Jared Moore, Tanvi Deshpande, Diyi Yang","authorsParsed":[["Moore","Jared",""],["Deshpande","Tanvi",""],["Yang","Diyi",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 10:53:54 GMT"}],"updateDate":"2024-07-04","timestamp":1720004034000,"abstract":"  Large language models (LLMs) appear to bias their survey answers toward\ncertain values. Nonetheless, some argue that LLMs are too inconsistent to\nsimulate particular values. Are they? To answer, we first define value\nconsistency as the similarity of answers across (1) paraphrases of one\nquestion, (2) related questions under one topic, (3) multiple-choice and\nopen-ended use-cases of one question, and (4) multilingual translations of a\nquestion to English, Chinese, German, and Japanese. We apply these measures to\na few large ($>=34b$), open LLMs including llama-3, as well as gpt-4o, using\neight thousand questions spanning more than 300 topics. Unlike prior work, we\nfind that models are relatively consistent across paraphrases, use-cases,\ntranslations, and within a topic. Still, some inconsistencies remain. Models\nare more consistent on uncontroversial topics (e.g., in the U.S.,\n\"Thanksgiving\") than on controversial ones (\"euthanasia\"). Base models are both\nmore consistent compared to fine-tuned models and are uniform in their\nconsistency across topics, while fine-tuned models are more inconsistent about\nsome topics (\"euthanasia\") than others (\"women's rights\") like our human\nsubjects (n=165).\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"tu7hPR6sPMe_CbQpnhbOiclFo7PACbaaRPb7oeiZt80","pdfSize":"2230821"}