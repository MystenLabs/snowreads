{
  "id": "2412.00731",
  "title": "Refine3DNet: Scaling Precision in 3D Object Reconstruction from\n  Multi-View RGB Images using Attention",
  "authors": "Ajith Balakrishnan, Sreeja S, Linu Shine",
  "authorsParsed": [
    [
      "Balakrishnan",
      "Ajith",
      ""
    ],
    [
      "S",
      "Sreeja",
      ""
    ],
    [
      "Shine",
      "Linu",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 1 Dec 2024 08:53:39 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733043219000,
  "abstract": "  Generating 3D models from multi-view 2D RGB images has gained significant\nattention, extending the capabilities of technologies like Virtual Reality,\nRobotic Vision, and human-machine interaction. In this paper, we introduce a\nhybrid strategy combining CNNs and transformers, featuring a visual\nauto-encoder with self-attention mechanisms and a 3D refiner network, trained\nusing a novel Joint Train Separate Optimization (JTSO) algorithm. Encoded\nfeatures from unordered inputs are transformed into an enhanced feature map by\nthe self-attention layer, decoded into an initial 3D volume, and further\nrefined. Our network generates 3D voxels from single or multiple 2D images from\narbitrary viewpoints. Performance evaluations using the ShapeNet datasets show\nthat our approach, combined with JTSO, outperforms state-of-the-art techniques\nin single and multi-view 3D reconstruction, achieving the highest mean\nintersection over union (IOU) scores, surpassing other models by 4.2% in\nsingle-view reconstruction.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "tWetrgBh6zB-hPP5MzV3hyBZ2EcqDSy0Yiam0ASUIFk",
  "pdfSize": "2302986"
}