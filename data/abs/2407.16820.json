{"id":"2407.16820","title":"Domain Adaptation of Visual Policies with a Single Demonstration","authors":"Weiyao Wang and Gregory D. Hager","authorsParsed":[["Wang","Weiyao",""],["Hager","Gregory D.",""]],"versions":[{"version":"v1","created":"Tue, 23 Jul 2024 20:17:28 GMT"}],"updateDate":"2024-07-25","timestamp":1721765848000,"abstract":"  Deploying machine learning algorithms for robot tasks in real-world\napplications presents a core challenge: overcoming the domain gap between the\ntraining and the deployment environment. This is particularly difficult for\nvisuomotor policies that utilize high-dimensional images as input, particularly\nwhen those images are generated via simulation. A common method to tackle this\nissue is through domain randomization, which aims to broaden the span of the\ntraining distribution to cover the test-time distribution. However, this\napproach is only effective when the domain randomization encompasses the actual\nshifts in the test-time distribution. We take a different approach, where we\nmake use of a single demonstration (a prompt) to learn policy that adapts to\nthe testing target environment. Our proposed framework, PromptAdapt, leverages\nthe Transformer architecture's capacity to model sequential data to learn\ndemonstration-conditioned visual policies, allowing for in-context adaptation\nto a target domain that is distinct from training. Our experiments in both\nsimulation and real-world settings show that PromptAdapt is a strong\ndomain-adapting policy that outperforms baseline methods by a large margin\nunder a range of domain shifts, including variations in lighting, color,\ntexture, and camera pose. Videos and more information can be viewed at project\nwebpage: https://sites.google.com/view/promptadapt.\n","subjects":["Computing Research Repository/Robotics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"fjoUGgSmWYOTWq8wtsYyCuSAq_vFUAlSsdxS2F9M75A","pdfSize":"1319278"}