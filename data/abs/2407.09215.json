{"id":"2407.09215","title":"HUP-3D: A 3D multi-view synthetic dataset for assisted-egocentric\n  hand-ultrasound pose estimation","authors":"Manuel Birlo, Razvan Caramalau, Philip J. \"Eddie\" Edwards, Brian\n  Dromey, Matthew J. Clarkson, and Danail Stoyanov","authorsParsed":[["Birlo","Manuel",""],["Caramalau","Razvan",""],["Edwards","Philip J. \"Eddie\"",""],["Dromey","Brian",""],["Clarkson","Matthew J.",""],["Stoyanov","Danail",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 12:25:42 GMT"}],"updateDate":"2024-07-15","timestamp":1720787142000,"abstract":"  We present HUP-3D, a 3D multi-view multi-modal synthetic dataset for\nhand-ultrasound (US) probe pose estimation in the context of obstetric\nultrasound. Egocentric markerless 3D joint pose estimation has potential\napplications in mixed reality based medical education. The ability to\nunderstand hand and probe movements programmatically opens the door to tailored\nguidance and mentoring applications. Our dataset consists of over 31k sets of\nRGB, depth and segmentation mask frames, including pose related ground truth\ndata, with a strong emphasis on image diversity and complexity. Adopting a\ncamera viewpoint-based sphere concept allows us to capture a variety of views\nand generate multiple hand grasp poses using a pre-trained network.\nAdditionally, our approach includes a software-based image rendering concept,\nenhancing diversity with various hand and arm textures, lighting conditions,\nand background images. Furthermore, we validated our proposed dataset with\nstate-of-the-art learning models and we obtained the lowest hand-object\nkeypoint errors. The dataset and other details are provided with the\nsupplementary material. The source code of our grasp generation and rendering\npipeline will be made publicly available.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"453kj0ZOgcHMruI7j4mwH65meNHyImevhzoP-InVmnk","pdfSize":"4865654"}