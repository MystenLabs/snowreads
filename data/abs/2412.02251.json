{"id":"2412.02251","title":"Selective Reviews of Bandit Problems in AI via a Statistical View","authors":"Pengjie Zhou, Haoyu Wei, Huiming Zhang","authorsParsed":[["Zhou","Pengjie",""],["Wei","Haoyu",""],["Zhang","Huiming",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 08:28:47 GMT"},{"version":"v2","created":"Tue, 18 Feb 2025 17:42:49 GMT"},{"version":"v3","created":"Wed, 19 Feb 2025 18:48:18 GMT"}],"updateDate":"2025-02-20","timestamp":1733214527000,"abstract":"  Reinforcement Learning (RL) is a widely researched area in artificial\nintelligence that focuses on teaching agents decision-making through\ninteractions with their environment. A key subset includes stochastic\nmulti-armed bandit (MAB) and continuum-armed bandit (SCAB) problems, which\nmodel sequential decision-making under uncertainty. This review outlines the\nfoundational models and assumptions of bandit problems, explores non-asymptotic\ntheoretical tools like concentration inequalities and minimax regret bounds,\nand compares frequentist and Bayesian algorithms for managing\nexploration-exploitation trade-offs. Additionally, we explore K-armed\ncontextual bandits and SCAB, focusing on their methodologies and regret\nanalyses. We also examine the connections between SCAB problems and functional\ndata analysis. Finally, we highlight recent advances and ongoing challenges in\nthe field.\n","subjects":["Statistics/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Machine Learning","Economics/Econometrics","Mathematics/Probability"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gTi8RPFVi9aEzvvRCuSJWxydwgomr-Pyqm_UatRWzMk","pdfSize":"1395854"}