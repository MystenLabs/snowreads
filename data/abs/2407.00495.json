{"id":"2407.00495","title":"A Bayesian Solution To The Imitation Gap","authors":"Risto Vuorio, Mattie Fellows, Cong Lu, Cl\\'emence Grislain, Shimon\n  Whiteson","authorsParsed":[["Vuorio","Risto",""],["Fellows","Mattie",""],["Lu","Cong",""],["Grislain","Cl√©mence",""],["Whiteson","Shimon",""]],"versions":[{"version":"v1","created":"Sat, 29 Jun 2024 17:13:37 GMT"}],"updateDate":"2024-07-02","timestamp":1719681217000,"abstract":"  In many real-world settings, an agent must learn to act in environments where\nno reward signal can be specified, but a set of expert demonstrations is\navailable. Imitation learning (IL) is a popular framework for learning policies\nfrom such demonstrations. However, in some cases, differences in observability\nbetween the expert and the agent can give rise to an imitation gap such that\nthe expert's policy is not optimal for the agent and a naive application of IL\ncan fail catastrophically. In particular, if the expert observes the Markov\nstate and the agent does not, then the expert will not demonstrate the\ninformation-gathering behavior needed by the agent but not the expert. In this\npaper, we propose a Bayesian solution to the Imitation Gap (BIG), first using\nthe expert demonstrations, together with a prior specifying the cost of\nexploratory behavior that is not demonstrated, to infer a posterior over\nrewards with Bayesian inverse reinforcement learning (IRL). BIG then uses the\nreward posterior to learn a Bayes-optimal policy. Our experiments show that\nBIG, unlike IL, allows the agent to explore at test time when presented with an\nimitation gap, whilst still learning to behave optimally using expert\ndemonstrations when no such gap exists.\n","subjects":["Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"pIA1hevM8qJv32kXKojPkiXBtC6PbHiWJ3wXBtNPdig","pdfSize":"1114850"}