{"id":"2412.02193","title":"LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language\n  Models","authors":"Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico\n  Tombari, Manling Li, Nick Haber, Jiajun Wu","authorsParsed":[["Sun","Fan-Yun",""],["Liu","Weiyu",""],["Gu","Siyi",""],["Lim","Dylan",""],["Bhat","Goutam",""],["Tombari","Federico",""],["Li","Manling",""],["Haber","Nick",""],["Wu","Jiajun",""]],"versions":[{"version":"v1","created":"Tue, 3 Dec 2024 06:15:04 GMT"}],"updateDate":"2024-12-04","timestamp":1733206504000,"abstract":"  Open-universe 3D layout generation arranges unlabeled 3D assets conditioned\non language instruction. Large language models (LLMs) struggle with generating\nphysically plausible 3D scenes and adherence to input instructions,\nparticularly in cluttered scenes. We introduce LayoutVLM, a framework and scene\nlayout representation that exploits the semantic knowledge of Vision-Language\nModels (VLMs) and supports differentiable optimization to ensure physical\nplausibility. LayoutVLM employs VLMs to generate two mutually reinforcing\nrepresentations from visually marked images, and a self-consistent decoding\nprocess to improve VLMs spatial planning. Our experiments show that LayoutVLM\naddresses the limitations of existing LLM and constraint-based approaches,\nproducing physically plausible 3D layouts better aligned with the semantic\nintent of input language instructions. We also demonstrate that fine-tuning\nVLMs with the proposed scene layout representation extracted from existing\nscene datasets can improve performance.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"Fpajq37EiA9ojQDdZNUf9_IX288R1Nr2Xfb3KPjlo9A","pdfSize":"5063475"}