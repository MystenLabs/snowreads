{"id":"2412.01819","title":"Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis","authors":"Anton Voronov, Denis Kuznedelev, Mikhail Khoroshikh, Valentin\n  Khrulkov, Dmitry Baranchuk","authorsParsed":[["Voronov","Anton",""],["Kuznedelev","Denis",""],["Khoroshikh","Mikhail",""],["Khrulkov","Valentin",""],["Baranchuk","Dmitry",""]],"versions":[{"version":"v1","created":"Mon, 2 Dec 2024 18:57:41 GMT"},{"version":"v2","created":"Tue, 3 Dec 2024 18:44:06 GMT"},{"version":"v3","created":"Thu, 5 Dec 2024 18:58:43 GMT"}],"updateDate":"2024-12-06","timestamp":1733165861000,"abstract":"  This work presents Switti, a scale-wise transformer for text-to-image\ngeneration. Starting from existing next-scale prediction AR models, we first\nexplore them for T2I generation and propose architectural modifications to\nimprove their convergence and overall performance. We then argue that\nscale-wise transformers do not require causality and propose a non-causal\ncounterpart facilitating ~11% faster sampling and lower memory usage while also\nachieving slightly better generation quality. Furthermore, we reveal that\nclassifier-free guidance at high-resolution scales is often unnecessary and can\neven degrade performance. By disabling guidance at these scales, we achieve an\nadditional sampling acceleration of ~20% and improve the generation of\nfine-grained details. Extensive human preference studies and automated\nevaluations show that Switti outperforms existing T2I AR models and competes\nwith state-of-the-art T2I diffusion models while being up to 7 times faster.\n","subjects":["Computer Science/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"W-E4A5wVio70IS7a-TZkTDvqlfc4wgBmmyjbhbDq_Zo","pdfSize":"52663753"}