{"id":"2407.19605","title":"Look Hear: Gaze Prediction for Speech-directed Human Attention","authors":"Sounak Mondal, Seoyoung Ahn, Zhibo Yang, Niranjan Balasubramanian,\n  Dimitris Samaras, Gregory Zelinsky, Minh Hoai","authorsParsed":[["Mondal","Sounak",""],["Ahn","Seoyoung",""],["Yang","Zhibo",""],["Balasubramanian","Niranjan",""],["Samaras","Dimitris",""],["Zelinsky","Gregory",""],["Hoai","Minh",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 22:35:08 GMT"},{"version":"v2","created":"Tue, 10 Sep 2024 01:34:13 GMT"}],"updateDate":"2024-09-11","timestamp":1722206108000,"abstract":"  For computer systems to effectively interact with humans using spoken\nlanguage, they need to understand how the words being generated affect the\nusers' moment-by-moment attention. Our study focuses on the incremental\nprediction of attention as a person is seeing an image and hearing a referring\nexpression defining the object in the scene that should be fixated by gaze. To\npredict the gaze scanpaths in this incremental object referral task, we\ndeveloped the Attention in Referral Transformer model or ART, which predicts\nthe human fixations spurred by each word in a referring expression. ART uses a\nmultimodal transformer encoder to jointly learn gaze behavior and its\nunderlying grounding tasks, and an autoregressive transformer decoder to\npredict, for each word, a variable number of fixations based on fixation\nhistory. To train ART, we created RefCOCO-Gaze, a large-scale dataset of 19,738\nhuman gaze scanpaths, corresponding to 2,094 unique image-expression pairs,\nfrom 220 participants performing our referral task. In our quantitative and\nqualitative analyses, ART not only outperforms existing methods in scanpath\nprediction, but also appears to capture several human attention patterns, such\nas waiting, scanning, and verification.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"0SKPxC-YiKVw5aiW97KqLg_KcC4un1tEetYvFUDrjyI","pdfSize":"12650242"}