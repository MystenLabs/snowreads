{
  "id": "2412.18524",
  "title": "HTR-JAND: Handwritten Text Recognition with Joint Attention Network and\n  Knowledge Distillation",
  "authors": "Mohammed Hamdan, Abderrahmane Rahiche, Mohamed Cheriet",
  "authorsParsed": [
    [
      "Hamdan",
      "Mohammed",
      ""
    ],
    [
      "Rahiche",
      "Abderrahmane",
      ""
    ],
    [
      "Cheriet",
      "Mohamed",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 24 Dec 2024 16:08:24 GMT"
    }
  ],
  "updateDate": "2024-12-25",
  "timestamp": 1735056504000,
  "abstract": "  Despite significant advances in deep learning, current Handwritten Text\nRecognition (HTR) systems struggle with the inherent complexity of historical\ndocuments, including diverse writing styles, degraded text quality, and\ncomputational efficiency requirements across multiple languages and time\nperiods. This paper introduces HTR-JAND (HTR-JAND: Handwritten Text Recognition\nwith Joint Attention Network and Knowledge Distillation), an efficient HTR\nframework that combines advanced feature extraction with knowledge\ndistillation. Our architecture incorporates three key components: (1) a CNN\narchitecture integrating FullGatedConv2d layers with Squeeze-and-Excitation\nblocks for adaptive feature extraction, (2) a Combined Attention mechanism\nfusing Multi-Head Self-Attention with Proxima Attention for robust sequence\nmodeling, and (3) a Knowledge Distillation framework enabling efficient model\ncompression while preserving accuracy through curriculum-based training. The\nHTR-JAND framework implements a multi-stage training approach combining\ncurriculum learning, synthetic data generation, and multi-task learning for\ncross-dataset knowledge transfer. We enhance recognition accuracy through\ncontext-aware T5 post-processing, particularly effective for historical\ndocuments. Comprehensive evaluations demonstrate HTR-JAND's effectiveness,\nachieving state-of-the-art Character Error Rates (CER) of 1.23\\%, 1.02\\%, and\n2.02\\% on IAM, RIMES, and Bentham datasets respectively. Our Student model\nachieves a 48\\% parameter reduction (0.75M versus 1.5M parameters) while\nmaintaining competitive performance through efficient knowledge transfer.\nSource code and pre-trained models are available at\n\\href{https://github.com/DocumentRecognitionModels/HTR-JAND}{Github}.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "9Ba9iuQWqspz6-Q8W3Mw3s_FrBctVw0c7KNUkcvsDeQ",
  "pdfSize": "2017005"
}