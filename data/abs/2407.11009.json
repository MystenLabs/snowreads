{"id":"2407.11009","title":"CharED: Character-wise Ensemble Decoding for Large Language Models","authors":"Kevin Gu, Eva Tuecke, Dmitriy Katz, Raya Horesh, David Alvarez-Melis,\n  Mikhail Yurochkin","authorsParsed":[["Gu","Kevin",""],["Tuecke","Eva",""],["Katz","Dmitriy",""],["Horesh","Raya",""],["Alvarez-Melis","David",""],["Yurochkin","Mikhail",""]],"versions":[{"version":"v1","created":"Tue, 25 Jun 2024 22:35:07 GMT"}],"updateDate":"2024-07-17","timestamp":1719354907000,"abstract":"  Large language models (LLMs) have shown remarkable potential for problem\nsolving, with open source models achieving increasingly impressive performance\non benchmarks measuring areas from logical reasoning to mathematical ability.\nEnsembling models can further improve capabilities across a variety of domains.\nHowever, conventional methods of combining models at inference time such as\nshallow fusion necessitate a shared vocabulary and tokenization, and\nalternatives like fine-tuning for domain-specific performance are both time\nconsuming and computationally expensive. We therefore present an inference-time\nensembling algorithm aimed at \"averaging\" outputs from multiple LLMs and\nillustrate its improved performance across multiple domains compared to its\nconstituent models alone. Character-wise ensemble decoding, CharED, finds the\nmarginal distribution of each character for an individual model and performs a\nweighted average to generate an output, character by character. In coding,\nmath, and toxicity benchmarks, we find our proposed model able to combine\ncomplimentary strengths of multiple LLMs, regardless of vocabulary,\ntokenization, or model size.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nQy_ewLYpc7G1wiBGutbrN_4jzq4EMVtLbEz3ObxNrw","pdfSize":"606165","objectId":"0x8c1f07708773914ca90d6f19197c41e68d9cfbc5b0fe4fa27a6363ad152d1205","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
