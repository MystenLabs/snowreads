{"id":"2407.04251","title":"Unified Interpretation of Smoothing Methods for Negative Sampling Loss\n  Functions in Knowledge Graph Embedding","authors":"Xincan Feng, Hidetaka Kamigaito, Katsuhiko Hayashi and Taro Watanabe","authorsParsed":[["Feng","Xincan",""],["Kamigaito","Hidetaka",""],["Hayashi","Katsuhiko",""],["Watanabe","Taro",""]],"versions":[{"version":"v1","created":"Fri, 5 Jul 2024 04:38:17 GMT"}],"updateDate":"2024-07-08","timestamp":1720154297000,"abstract":"  Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks\nin NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has\nan important role in automatically completing KGs by scoring their links with\nKG Embedding (KGE). To handle many entities in training, KGE relies on Negative\nSampling (NS) loss that can reduce the computational cost by sampling. Since\nthe appearance frequencies for each link are at most one in KGs, sparsity is an\nessential and inevitable problem. The NS loss is no exception. As a solution,\nthe NS loss in KGE relies on smoothing methods like Self-Adversarial Negative\nSampling (SANS) and subsampling. However, it is uncertain what kind of\nsmoothing method is suitable for this purpose due to the lack of theoretical\nunderstanding. This paper provides theoretical interpretations of the smoothing\nmethods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive\nNegative Sampling (TANS), that can cover the characteristics of the\nconventional smoothing methods. Experimental results of TransE, DistMult,\nComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets\nand their sparser subsets show the soundness of our interpretation and\nperformance improvement by our TANS.\n","subjects":["Computing Research Repository/Computation and Language","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"YJJHxr-cbl9WaqDfFw7xjDgCTiInyuqxTF8bI8RBYoY","pdfSize":"1296692"}