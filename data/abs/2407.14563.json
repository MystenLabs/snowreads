{"id":"2407.14563","title":"Learning Visual Grounding from Generative Vision and Language Model","authors":"Shijie Wang, Dahun Kim, Ali Taalimi, Chen Sun, Weicheng Kuo","authorsParsed":[["Wang","Shijie",""],["Kim","Dahun",""],["Taalimi","Ali",""],["Sun","Chen",""],["Kuo","Weicheng",""]],"versions":[{"version":"v1","created":"Thu, 18 Jul 2024 20:29:49 GMT"}],"updateDate":"2024-07-23","timestamp":1721334589000,"abstract":"  Visual grounding tasks aim to localize image regions based on natural\nlanguage references. In this work, we explore whether generative VLMs\npredominantly trained on image-text data could be leveraged to scale up the\ntext annotation of visual grounding data. We find that grounding knowledge\nalready exists in generative VLM and can be elicited by proper prompting. We\nthus prompt a VLM to generate object-level descriptions by feeding it object\nregions from existing object detection datasets. We further propose attribute\nmodeling to explicitly capture the important object attributes, and spatial\nrelation modeling to capture inter-object relationship, both of which are\ncommon linguistic pattern in referring expression. Our constructed dataset\n(500K images, 1M objects, 16M referring expressions) is one of the largest\ngrounding datasets to date, and the first grounding dataset with purely\nmodel-generated queries and human-annotated objects. To verify the quality of\nthis data, we conduct zero-shot transfer experiments to the popular RefCOCO\nbenchmarks for both referring expression comprehension (REC) and segmentation\n(RES) tasks. On both tasks, our model significantly outperform the\nstate-of-the-art approaches without using human annotated visual grounding\ndata. Our results demonstrate the promise of generative VLM to scale up visual\ngrounding in the real world. Code and models will be released.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Jxjw_WEBr_6kEutvscruHAhmTCq7h0O71J6WIHRPWuk","pdfSize":"4406438"}