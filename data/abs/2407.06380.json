{"id":"2407.06380","title":"Data, Data Everywhere: A Guide for Pretraining Dataset Construction","authors":"Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Bo Liu, Aastha\n  Jhunjhunwala, Zhilin Wang, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro","authorsParsed":[["Parmar","Jupinder",""],["Prabhumoye","Shrimai",""],["Jennings","Joseph",""],["Liu","Bo",""],["Jhunjhunwala","Aastha",""],["Wang","Zhilin",""],["Patwary","Mostofa",""],["Shoeybi","Mohammad",""],["Catanzaro","Bryan",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 20:47:58 GMT"}],"updateDate":"2024-07-10","timestamp":1720471678000,"abstract":"  The impressive capabilities of recent language models can be largely\nattributed to the multi-trillion token pretraining datasets that they are\ntrained on. However, model developers fail to disclose their construction\nmethodology which has lead to a lack of open information on how to develop\neffective pretraining sets. To address this issue, we perform the first\nsystematic study across the entire pipeline of pretraining set construction.\nFirst, we run ablations on existing techniques for pretraining set development\nto identify which methods translate to the largest gains in model accuracy on\ndownstream evaluations. Then, we categorize the most widely used data source,\nweb crawl snapshots, across the attributes of toxicity, quality, type of\nspeech, and domain. Finally, we show how such attribute information can be used\nto further refine and improve the quality of a pretraining set. These findings\nconstitute an actionable set of steps that practitioners can use to develop\nhigh quality pretraining sets.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"javDNaFqwez0iYFJixMOAxp_Qavl9tI4BXJtGIov9QQ","pdfSize":"1592561"}