{"id":"2407.19126","title":"Greedy Output Approximation: Towards Efficient Structured Pruning for\n  LLMs Without Retraining","authors":"Jianwei Li and Yijun Dong and Qi Lei","authorsParsed":[["Li","Jianwei",""],["Dong","Yijun",""],["Lei","Qi",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 23:53:59 GMT"}],"updateDate":"2024-07-30","timestamp":1722038039000,"abstract":"  To remove redundant components of large language models (LLMs) without\nincurring significant computational costs, this work focuses on single-shot\npruning without a retraining phase. We simplify the pruning process for\nTransformer-based LLMs by identifying a depth-2 pruning structure that\nfunctions independently. Additionally, we propose two inference-aware pruning\ncriteria derived from the optimization perspective of output approximation,\nwhich outperforms traditional training-aware metrics such as gradient and\nHessian. We also introduce a two-step reconstruction technique to mitigate\npruning errors without model retraining. Experimental results demonstrate that\nour approach significantly reduces computational costs and hardware\nrequirements while maintaining superior performance across various datasets and\nmodels.\n","subjects":["Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"RejVKAJYClH5PCl5FM1na68wmEP9aomugYwpKAyfuAM","pdfSize":"1498823","objectId":"0x9c81d4bb5ff31f827347bd2357d6e9af48f1d423809106d0f1b298ea25ccb269","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
