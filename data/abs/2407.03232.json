{"id":"2407.03232","title":"Single Character Perturbations Break LLM Alignment","authors":"Leon Lin, Hannah Brown, Kenji Kawaguchi, Michael Shieh","authorsParsed":[["Lin","Leon",""],["Brown","Hannah",""],["Kawaguchi","Kenji",""],["Shieh","Michael",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 16:03:10 GMT"}],"updateDate":"2024-07-04","timestamp":1720022590000,"abstract":"  When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\navailable at https://github.com/hannah-aught/space_attack.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"55vI8CdNSXGhKX2VhQQmPTI1eH2uPPuBODwSDyFxyII","pdfSize":"796664"}