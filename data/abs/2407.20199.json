{"id":"2407.20199","title":"Emergence in non-neural models: grokking modular arithmetic via average\n  gradient outer product","authors":"Neil Mallinar, Daniel Beaglehole, Libin Zhu, Adityanarayanan\n  Radhakrishnan, Parthe Pandit, Mikhail Belkin","authorsParsed":[["Mallinar","Neil",""],["Beaglehole","Daniel",""],["Zhu","Libin",""],["Radhakrishnan","Adityanarayanan",""],["Pandit","Parthe",""],["Belkin","Mikhail",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 17:28:58 GMT"}],"updateDate":"2024-07-30","timestamp":1722274138000,"abstract":"  Neural networks trained to solve modular arithmetic tasks exhibit grokking, a\nphenomenon where the test accuracy starts improving long after the model\nachieves 100% training accuracy in the training process. It is often taken as\nan example of \"emergence\", where model ability manifests sharply through a\nphase transition. In this work, we show that the phenomenon of grokking is not\nspecific to neural networks nor to gradient descent-based optimization.\nSpecifically, we show that this phenomenon occurs when learning modular\narithmetic with Recursive Feature Machines (RFM), an iterative algorithm that\nuses the Average Gradient Outer Product (AGOP) to enable task-specific feature\nlearning with general machine learning models. When used in conjunction with\nkernel machines, iterating RFM results in a fast transition from random, near\nzero, test accuracy to perfect test accuracy. This transition cannot be\npredicted from the training loss, which is identically zero, nor from the test\nloss, which remains constant in initial iterations. Instead, as we show, the\ntransition is completely determined by feature learning: RFM gradually learns\nblock-circulant features to solve modular arithmetic. Paralleling the results\nfor RFM, we show that neural networks that solve modular arithmetic also learn\nblock-circulant features. Furthermore, we present theoretical evidence that RFM\nuses such block-circulant features to implement the Fourier Multiplication\nAlgorithm, which prior work posited as the generalizing solution neural\nnetworks learn on these tasks. Our results demonstrate that emergence can\nresult purely from learning task-relevant features and is not specific to\nneural architectures nor gradient descent-based optimization methods.\nFurthermore, our work provides more evidence for AGOP as a key mechanism for\nfeature learning in neural networks.\n","subjects":["Statistics/Machine Learning","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"Yr9Ijq8K6Fy6rmP2Q4zRxsR_jDierECJjenB3bBwmnQ","pdfSize":"4045050"}