{
  "id": "2412.07316",
  "title": "Preserving Speaker Information in Direct Speech-to-Speech Translation\n  with Non-Autoregressive Generation and Pretraining",
  "authors": "Rui Zhou, Akinori Ito and Takashi Nose",
  "authorsParsed": [
    [
      "Zhou",
      "Rui",
      ""
    ],
    [
      "Ito",
      "Akinori",
      ""
    ],
    [
      "Nose",
      "Takashi",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 10 Dec 2024 08:58:51 GMT"
    },
    {
      "version": "v2",
      "created": "Wed, 11 Dec 2024 03:05:26 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733821131000,
  "abstract": "  Speech-to-Speech Translation (S2ST) refers to the conversion of speech in one\nlanguage into semantically equivalent speech in another language, facilitating\ncommunication between speakers of different languages. Speech-to-Discrete Unit\nTranslation (S2UT), a mainstream approach for end-to-end S2ST, addresses\nchallenges such as error propagation across modules and slow inference speed\noften encountered in traditional cascade systems. However, as discrete units\nprimarily capture content information, conventional S2UT methods fail to retain\nspeaker-specific characteristics from the source. Our previous work, SC-S2UT,\nintroduced a speaker adapter and a unit-to-mel structure, enabling the\npreservation of speaker information and non-autoregressive speech generation.\nBuilding on this foundation, this study proposes a self-supervised pretraining\nmethod to enrich the information extracted by both the speaker adapter and the\nunit-to-mel structure. Additionally, we investigate different feature fusion\nstrategies to further improve the integration of speaker and content features.\nExperiments conducted on the CVSS-T dataset for ES-EN and FR-EN tasks\ndemonstrate that our proposed method achieves a BLEU score improvement of 1.14\ncompared to SC-S2UT, along with significant enhancements in MOS and speaker\nsimilarity. Furthermore, our approach achieves translation quality comparable\nto traditional S2UT, with only a minimal increase of 0.04s per utterance in\ninference time, while maintaining high speaker similarity. These results\nvalidate the effectiveness of the proposed method.\n",
  "subjects": [
    "Computer Science/Sound",
    "Computer Science/Multimedia",
    "Electrical Engineering and Systems Science/Audio and Speech Processing"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "jhdNp3hf0AV9vFTvgp8pnWI5EdPPbsyeag3Jr_iUlps",
  "pdfSize": "1244622"
}