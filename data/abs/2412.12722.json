{
  "id": "2412.12722",
  "title": "Defending LVLMs Against Vision Attacks through Partial-Perception\n  Supervision",
  "authors": "Qi Zhou, Tianlin Li, Qing Guo, Dongxia Wang, Yun Lin, Yang Liu, Jin\n  Song Dong",
  "authorsParsed": [
    [
      "Zhou",
      "Qi",
      ""
    ],
    [
      "Li",
      "Tianlin",
      ""
    ],
    [
      "Guo",
      "Qing",
      ""
    ],
    [
      "Wang",
      "Dongxia",
      ""
    ],
    [
      "Lin",
      "Yun",
      ""
    ],
    [
      "Liu",
      "Yang",
      ""
    ],
    [
      "Dong",
      "Jin Song",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 09:38:58 GMT"
    }
  ],
  "updateDate": "2024-12-18",
  "timestamp": 1734428338000,
  "abstract": "  Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Cryptography and Security"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "hKsb44JqvpDxFLQC0ugj5Msf4vrFZRDiLHjx50az8EQ",
  "pdfSize": "1877531"
}