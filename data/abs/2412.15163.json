{
  "id": "2412.15163",
  "title": "Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents",
  "authors": "Jessica Woodgate, Paul Marshall, Nirav Ajmeri",
  "authorsParsed": [
    [
      "Woodgate",
      "Jessica",
      ""
    ],
    [
      "Marshall",
      "Paul",
      ""
    ],
    [
      "Ajmeri",
      "Nirav",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 19 Dec 2024 18:38:13 GMT"
    }
  ],
  "updateDate": "2024-12-20",
  "timestamp": 1734633493000,
  "abstract": "  Social norms are standards of behaviour common in a society. However, when\nagents make decisions without considering how others are impacted, norms can\nemerge that lead to the subjugation of certain agents. We present RAWL-E, a\nmethod to create ethical norm-learning agents. RAWL-E agents operationalise\nmaximin, a fairness principle from Rawlsian ethics, in their decision-making\nprocesses to promote ethical norms by balancing societal well-being with\nindividual goals. We evaluate RAWL-E agents in simulated harvesting scenarios.\nWe find that norms emerging in RAWL-E agent societies enhance social welfare,\nfairness, and robustness, and yield higher minimum experience compared to those\nthat emerge in agent societies that do not implement Rawlsian ethics.\n",
  "subjects": [
    "Computer Science/Multiagent Systems",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "lvdI8YeKqRzeFhDRfRO-dfMay_Xdx2d2y2Ws68M96aY",
  "pdfSize": "339811"
}