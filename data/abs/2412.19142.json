{
  "id": "2412.19142",
  "title": "CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian\n  Splatting",
  "authors": "Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao,\n  Humphrey Shi, Yunchao Wei",
  "authorsParsed": [
    [
      "Jiao",
      "Siyu",
      ""
    ],
    [
      "Dong",
      "Haoye",
      ""
    ],
    [
      "Yin",
      "Yuyang",
      ""
    ],
    [
      "Jie",
      "Zequn",
      ""
    ],
    [
      "Qian",
      "Yinlong",
      ""
    ],
    [
      "Zhao",
      "Yao",
      ""
    ],
    [
      "Shi",
      "Humphrey",
      ""
    ],
    [
      "Wei",
      "Yunchao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 26 Dec 2024 09:54:25 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1735206865000,
  "abstract": "  Recent works in 3D multimodal learning have made remarkable progress.\nHowever, typically 3D multimodal models are only capable of handling point\nclouds. Compared to the emerging 3D representation technique, 3D Gaussian\nSplatting (3DGS), the spatially sparse point cloud cannot depict the texture\ninformation of 3D objects, resulting in inferior reconstruction capabilities.\nThis limitation constrains the potential of point cloud-based 3D multimodal\nrepresentation learning. In this paper, we present CLIP-GS, a novel multimodal\nrepresentation learning framework grounded in 3DGS. We introduce the GS\nTokenizer to generate serialized gaussian tokens, which are then processed\nthrough transformer layers pre-initialized with weights from point cloud\nmodels, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss\nbetween 3DGS and the visual-text embeddings of CLIP, and we introduce an image\nvoting loss to guide the directionality and convergence of gradient\noptimization. Furthermore, we develop an efficient way to generate triplets of\n3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal\nrepresentations. Leveraging the well-aligned multimodal representations,\nCLIP-GS demonstrates versatility and outperforms point cloud-based models on\nvarious 3D tasks, including multimodal retrieval, zero-shot, and few-shot\nclassification.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "Qvd5XqKpdfnJjsoXtsYxYJXih5Q2rWsiPnIwIug5bn8",
  "pdfSize": "3000443"
}