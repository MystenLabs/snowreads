{"id":"2407.21072","title":"Beyond Metrics: A Critical Analysis of the Variability in Large Language\n  Model Evaluation Frameworks","authors":"Marco AF Pimentel, Cl\\'ement Christophe, Tathagata Raha, Prateek\n  Munjal, Praveen K Kanithi, Shadab Khan","authorsParsed":[["Pimentel","Marco AF",""],["Christophe","Cl√©ment",""],["Raha","Tathagata",""],["Munjal","Prateek",""],["Kanithi","Praveen K",""],["Khan","Shadab",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 03:37:14 GMT"}],"updateDate":"2024-08-01","timestamp":1722224234000,"abstract":"  As large language models (LLMs) continue to evolve, the need for robust and\nstandardized evaluation benchmarks becomes paramount. Evaluating the\nperformance of these models is a complex challenge that requires careful\nconsideration of various linguistic tasks, model architectures, and\nbenchmarking methodologies. In recent years, various frameworks have emerged as\nnoteworthy contributions to the field, offering comprehensive evaluation tests\nand benchmarks for assessing the capabilities of LLMs across diverse domains.\nThis paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and\nimpact on advancing the state-of-the-art in natural language processing.\n","subjects":["Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"KD4ipwOe8YcdkkGiSU5_VAuDosFYwMOxnCYQHoYitlE","pdfSize":"553539"}