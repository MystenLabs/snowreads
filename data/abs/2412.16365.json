{
  "id": "2412.16365",
  "title": "Overview of the First Workshop on Language Models for Low-Resource\n  Languages (LoResLM 2025)",
  "authors": "Hansi Hettiarachchi, Tharindu Ranasinghe, Paul Rayson, Ruslan Mitkov,\n  Mohamed Gaber, Damith Premasiri, Fiona Anting Tan, Lasitha Uyangodage",
  "authorsParsed": [
    [
      "Hettiarachchi",
      "Hansi",
      ""
    ],
    [
      "Ranasinghe",
      "Tharindu",
      ""
    ],
    [
      "Rayson",
      "Paul",
      ""
    ],
    [
      "Mitkov",
      "Ruslan",
      ""
    ],
    [
      "Gaber",
      "Mohamed",
      ""
    ],
    [
      "Premasiri",
      "Damith",
      ""
    ],
    [
      "Tan",
      "Fiona Anting",
      ""
    ],
    [
      "Uyangodage",
      "Lasitha",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 20 Dec 2024 21:55:32 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734731732000,
  "abstract": "  The first Workshop on Language Models for Low-Resource Languages (LoResLM\n2025) was held in conjunction with the 31st International Conference on\nComputational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates.\nThis workshop mainly aimed to provide a forum for researchers to share and\ndiscuss their ongoing work on language models (LMs) focusing on low-resource\nlanguages, following the recent advancements in neural language models and\ntheir linguistic biases towards high-resource languages. LoResLM 2025 attracted\nnotable interest from the natural language processing (NLP) community,\nresulting in 35 accepted papers from 52 submissions. These contributions cover\na broad range of low-resource languages from eight language families and 13\ndiverse research areas, paving the way for future possibilities and promoting\nlinguistic inclusivity in NLP.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "FKIem5yUv3xsYN5aVLgBZj91dpIAcqswefP2eKBmBa8",
  "pdfSize": "241332"
}