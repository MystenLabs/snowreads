{
  "id": "2412.13666",
  "title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized\n  Disinformation Generation",
  "authors": "Aneta Zugecova, Dominik Macko, Ivan Srba, Robert Moro, Jakub Kopal,\n  Katarina Marcincinova, Matus Mesarcik",
  "authorsParsed": [
    [
      "Zugecova",
      "Aneta",
      ""
    ],
    [
      "Macko",
      "Dominik",
      ""
    ],
    [
      "Srba",
      "Ivan",
      ""
    ],
    [
      "Moro",
      "Robert",
      ""
    ],
    [
      "Kopal",
      "Jakub",
      ""
    ],
    [
      "Marcincinova",
      "Katarina",
      ""
    ],
    [
      "Mesarcik",
      "Matus",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 18 Dec 2024 09:48:53 GMT"
    }
  ],
  "updateDate": "2024-12-19",
  "timestamp": 1734515333000,
  "abstract": "  The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts rises\nmany concerns regarding their misuse. Previous research has shown that LLMs can\nbe effectively misused for generating disinformation news articles following\npredefined narratives. Their capabilities to generate personalized (in various\naspects) content have also been evaluated and mostly found usable. However, a\ncombination of personalization and disinformation abilities of LLMs has not\nbeen comprehensively studied yet. Such a dangerous combination should trigger\nintegrated safety filters of the LLMs, if there are some. This study fills this\ngap by evaluation of vulnerabilities of recent open and closed LLMs, and their\nwillingness to generate personalized disinformation news articles in English.\nWe further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Computers and Society"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "bMdLBzW956_3E1Fr7Pjl7KaaBlnh7KAIqlAzDg2NHu0",
  "pdfSize": "536038"
}