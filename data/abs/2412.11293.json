{
  "id": "2412.11293",
  "title": "A Comparative Study on Dynamic Graph Embedding based on Mamba and\n  Transformers",
  "authors": "Ashish Parmanand Pandey, Alan John Varghese, Sarang Patil, Mengjia Xu",
  "authorsParsed": [
    [
      "Pandey",
      "Ashish Parmanand",
      ""
    ],
    [
      "Varghese",
      "Alan John",
      ""
    ],
    [
      "Patil",
      "Sarang",
      ""
    ],
    [
      "Xu",
      "Mengjia",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Sun, 15 Dec 2024 19:56:56 GMT"
    }
  ],
  "updateDate": "2024-12-17",
  "timestamp": 1734292616000,
  "abstract": "  Dynamic graph embedding has emerged as an important technique for modeling\ncomplex time-evolving networks across diverse domains. While transformer-based\nmodels have shown promise in capturing long-range dependencies in temporal\ngraph data, they face scalability challenges due to quadratic computational\ncomplexity. This study presents a comparative analysis of dynamic graph\nembedding approaches using transformers and the recently proposed Mamba\narchitecture, a state-space model with linear complexity. We introduce three\nnovel models: TransformerG2G augment with graph convolutional networks,\nDG-Mamba, and GDG-Mamba with graph isomorphism network edge convolutions. Our\nexperiments on multiple benchmark datasets demonstrate that Mamba-based models\nachieve comparable or superior performance to transformer-based approaches in\nlink prediction tasks while offering significant computational efficiency gains\non longer sequences. Notably, DG-Mamba variants consistently outperform\ntransformer-based models on datasets with high temporal variability, such as\nUCI, Bitcoin, and Reality Mining, while maintaining competitive performance on\nmore stable graphs like SBM. We provide insights into the learned temporal\ndependencies through analysis of attention weights and state matrices,\nrevealing the models' ability to capture complex temporal patterns. By\neffectively combining state-space models with graph neural networks, our work\naddresses key limitations of previous approaches and contributes to the growing\nbody of research on efficient temporal graph representation learning. These\nfindings offer promising directions for scaling dynamic graph embedding to\nlarger, more complex real-world networks, potentially enabling new applications\nin areas such as social network analysis, financial modeling, and biological\nsystem dynamics.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "lrvOthdjPnoL1axmPG1Q4NvURdAvhfBELQSyIyIFbLs",
  "pdfSize": "7100679"
}