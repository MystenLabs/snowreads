{
  "id": "2412.03715",
  "title": "PathletRL++: Optimizing Trajectory Pathlet Extraction and Dictionary\n  Formation via Reinforcement Learning",
  "authors": "Gian Alix, Arian Haghparast, Manos Papagelis",
  "authorsParsed": [
    [
      "Alix",
      "Gian",
      ""
    ],
    [
      "Haghparast",
      "Arian",
      ""
    ],
    [
      "Papagelis",
      "Manos",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 21:09:43 GMT"
    }
  ],
  "updateDate": "2024-12-06",
  "timestamp": 1733346583000,
  "abstract": "  Advances in tracking technologies have spurred the rapid growth of\nlarge-scale trajectory data. Building a compact collection of pathlets,\nreferred to as a trajectory pathlet dictionary, is essential for supporting\nmobility-related applications. Existing methods typically adopt a top-down\napproach, generating numerous candidate pathlets and selecting a subset,\nleading to high memory usage and redundant storage from overlapping pathlets.\nTo overcome these limitations, we propose a bottom-up strategy that\nincrementally merges basic pathlets to build the dictionary, reducing memory\nrequirements by up to 24,000 times compared to baseline methods. The approach\nbegins with unit-length pathlets and iteratively merges them while optimizing\nutility, which is defined using newly introduced metrics of trajectory loss and\nrepresentability. We develop a deep reinforcement learning framework,\nPathletRL, which utilizes Deep Q-Networks (DQN) to approximate the utility\nfunction, resulting in a compact and efficient pathlet dictionary. Experiments\non both synthetic and real-world datasets demonstrate that our method\noutperforms state-of-the-art techniques, reducing the size of the constructed\ndictionary by up to 65.8%. Additionally, our results show that only half of the\ndictionary pathlets are needed to reconstruct 85% of the original trajectory\ndata. Building on PathletRL, we introduce PathletRL++, which extends the\noriginal model by incorporating a richer state representation and an improved\nreward function to optimize decision-making during pathlet merging. These\nenhancements enable the agent to gain a more nuanced understanding of the\nenvironment, leading to higher-quality pathlet dictionaries. PathletRL++\nachieves even greater dictionary size reduction, surpassing the performance of\nPathletRL, while maintaining high trajectory representability.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "bU1-WCC0vSVq9tzHan03WGH-ODCZdqGo6gpw0322204",
  "pdfSize": "5832001"
}