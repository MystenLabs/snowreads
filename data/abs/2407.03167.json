{"id":"2407.03167","title":"Tail calibration of probabilistic forecasts","authors":"Sam Allen, Jonathan Koh, Johan Segers, Johanna Ziegel","authorsParsed":[["Allen","Sam",""],["Koh","Jonathan",""],["Segers","Johan",""],["Ziegel","Johanna",""]],"versions":[{"version":"v1","created":"Wed, 3 Jul 2024 14:41:31 GMT"}],"updateDate":"2024-07-04","timestamp":1720017691000,"abstract":"  Probabilistic forecasts comprehensively describe the uncertainty in the\nunknown future outcome, making them essential for decision making and risk\nmanagement. While several methods have been introduced to evaluate\nprobabilistic forecasts, existing evaluation techniques are ill-suited to the\nevaluation of tail properties of such forecasts. However, these tail properties\nare often of particular interest to forecast users due to the severe impacts\ncaused by extreme outcomes. In this work, we introduce a general notion of tail\ncalibration for probabilistic forecasts, which allows forecasters to assess the\nreliability of their predictions for extreme outcomes. We study the\nrelationships between tail calibration and standard notions of forecast\ncalibration, and discuss connections to peaks-over-threshold models in extreme\nvalue theory. Diagnostic tools are introduced and applied in a case study on\nEuropean precipitation forecasts\n","subjects":["Statistics/Methodology"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"5OiGOvrgqlIyY8ck-Jczq8w1eeX0O_r0u7ygoN9ocDs","pdfSize":"2696946"}