{"id":"2407.01601","title":"Unveiling and Controlling Anomalous Attention Distribution in\n  Transformers","authors":"Ruiqing Yan, Xingbo Du, Haoyu Deng, Linghan Zheng, Qiuzhuang Sun,\n  Jifang Hu, Yuhang Shao, Penghao Jiang, Jinrong Jiang, Lian Zhao","authorsParsed":[["Yan","Ruiqing",""],["Du","Xingbo",""],["Deng","Haoyu",""],["Zheng","Linghan",""],["Sun","Qiuzhuang",""],["Hu","Jifang",""],["Shao","Yuhang",""],["Jiang","Penghao",""],["Jiang","Jinrong",""],["Zhao","Lian",""]],"versions":[{"version":"v1","created":"Wed, 26 Jun 2024 11:53:35 GMT"},{"version":"v2","created":"Wed, 3 Jul 2024 16:19:59 GMT"}],"updateDate":"2024-07-04","timestamp":1719402815000,"abstract":"  With the advent of large models based on the Transformer architecture,\nresearchers have observed an anomalous phenomenon in the Attention\nmechanism--there is a very high attention on the first element, which is\nprevalent across Transformer-based models. It is crucial to understand it for\nthe development of techniques focusing on attention distribution, such as\nKey-Value (KV) Cache compression and infinite extrapolation; however, the\nlatent cause leaves to be unknown. In this paper, we analyze such a phenomenon\nfrom the perspective of waiver phenomenon, which involves reducing the internal\nvalues of certain elements in the sequence, allowing them to absorb excess\nattention without affecting their contribution to information. In specific\nmodels, due to differences in positional encoding and attention patterns, we\nhave found that the selection of waiver elements by the model can be\ncategorized into two methods: positional-encoding-based and\nfeature-distribution-within-elements-based.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"3wd6irGY5_I1QSkJdGv7uEYiHS3t37btILFybjxODHM","pdfSize":"2076544"}