{"id":"2407.20179","title":"Theia: Distilling Diverse Vision Foundation Models for Robot Learning","authors":"Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria\n  Minniti, Tarik Kelestemur, David Watkins, and Laura Herlant","authorsParsed":[["Shang","Jinghuan",""],["Schmeckpeper","Karl",""],["May","Brandon B.",""],["Minniti","Maria Vittoria",""],["Kelestemur","Tarik",""],["Watkins","David",""],["Herlant","Laura",""]],"versions":[{"version":"v1","created":"Mon, 29 Jul 2024 17:08:21 GMT"}],"updateDate":"2024-07-30","timestamp":1722272901000,"abstract":"  Vision-based robot policy learning, which maps visual inputs to actions,\nnecessitates a holistic understanding of diverse visual tasks beyond\nsingle-task needs like classification or segmentation. Inspired by this, we\nintroduce Theia, a vision foundation model for robot learning that distills\nmultiple off-the-shelf vision foundation models trained on varied vision tasks.\nTheia's rich visual representations encode diverse visual knowledge, enhancing\ndownstream robot learning. Extensive experiments demonstrate that Theia\noutperforms its teacher models and prior robot learning models using less\ntraining data and smaller model sizes. Additionally, we quantify the quality of\npre-trained visual representations and hypothesize that higher entropy in\nfeature norm distributions leads to improved robot learning performance. Code\nand models are available at https://github.com/bdaiinstitute/theia.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"qEWp1I011r84bBRH2irXfQBeyxiFis96Th3aXraaF9Y","pdfSize":"12749144","objectId":"0x880a355c6954bfc62fee53f4d98534e14bad103e9daa651c055d3a41de21508f","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
