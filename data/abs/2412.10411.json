{"id":"2412.10411","title":"Pre-trained protein language model for codon optimization","authors":"Shashank Pathak and Guohui Lin","authorsParsed":[["Pathak","Shashank",""],["Lin","Guohui",""]],"versions":[{"version":"v1","created":"Sun, 8 Dec 2024 03:01:38 GMT"}],"updateDate":"2024-12-17","timestamp":1733626898000,"abstract":"  Motivation: Codon optimization of Open Reading Frame (ORF) sequences is\nessential for enhancing mRNA stability and expression in applications like mRNA\nvaccines, where codon choice can significantly impact protein yield which\ndirectly impacts immune strength. In this work, we investigate the use of a\npre-trained protein language model (PPLM) for getting a rich representation of\namino acids which could be utilized for codon optimization. This leaves us with\na simpler fine-tuning task over PPLM in optimizing ORF sequences.\n  Results: The ORFs generated by our proposed models outperformed their natural\ncounterparts encoding the same proteins on computational metrics for stability\nand expression. They also demonstrated enhanced performance against the\nbenchmark ORFs used in mRNA vaccines for the SARS-CoV-2 viral spike protein and\nthe varicella-zoster virus (VZV). These results highlight the potential of\nadapting PPLM for designing ORFs tailored to encode target antigens in mRNA\nvaccines.\n","subjects":["Quantitative Biology/Quantitative Methods","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"6uozY5OXm5WOejGBBsRhsGS5P7L37v-Ln-pRTahkcOA","pdfSize":"5100291"}