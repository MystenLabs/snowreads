{
  "id": "2412.05434",
  "title": "Diversity Over Quantity: A Lesson From Few Shot Relation Classification",
  "authors": "Amir DN Cohen, Shauli Ravfogel, Shaltiel Shmidman, Yoav Goldberg",
  "authorsParsed": [
    [
      "Cohen",
      "Amir DN",
      ""
    ],
    [
      "Ravfogel",
      "Shauli",
      ""
    ],
    [
      "Shmidman",
      "Shaltiel",
      ""
    ],
    [
      "Goldberg",
      "Yoav",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 6 Dec 2024 21:41:01 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733521261000,
  "abstract": "  In few-shot relation classification (FSRC), models must generalize to novel\nrelations with only a few labeled examples. While much of the recent progress\nin NLP has focused on scaling data size, we argue that diversity in relation\ntypes is more crucial for FSRC performance. In this work, we demonstrate that\ntraining on a diverse set of relations significantly enhances a model's ability\nto generalize to unseen relations, even when the overall dataset size remains\nfixed.\n  We introduce REBEL-FS, a new FSRC benchmark that incorporates an order of\nmagnitude more relation types than existing datasets. Through systematic\nexperiments, we show that increasing the diversity of relation types in the\ntraining data leads to consistent gains in performance across various few-shot\nlearning scenarios, including high-negative settings. Our findings challenge\nthe common assumption that more data alone leads to better performance and\nsuggest that targeted data curation focused on diversity can substantially\nreduce the need for large-scale datasets in FSRC.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "1FnwrkwacO6zlBa0Tg43wcxykUHPxq-yRA3XtKF7z6o",
  "pdfSize": "528577"
}