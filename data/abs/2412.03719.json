{"id":"2412.03719","title":"From Language Models over Tokens to Language Models over Characters","authors":"Tim Vieira, Ben LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Brian\n  DuSell, John Terilla, Timothy J. O'Donnell, Ryan Cotterell","authorsParsed":[["Vieira","Tim",""],["LeBrun","Ben",""],["Giulianelli","Mario",""],["Gastaldi","Juan Luis",""],["DuSell","Brian",""],["Terilla","John",""],["O'Donnell","Timothy J.",""],["Cotterell","Ryan",""]],"versions":[{"version":"v1","created":"Wed, 4 Dec 2024 21:19:20 GMT"}],"updateDate":"2024-12-06","timestamp":1733347160000,"abstract":"  Modern language models are internally -- and mathematically -- distributions\nover token strings rather than \\emph{character} strings, posing numerous\nchallenges for programmers building user applications on top of them. For\nexample, if a prompt is specified as a character string, it must be tokenized\nbefore passing it to the token-level language model. Thus, the tokenizer and\nconsequent analyses are very sensitive to the specification of the prompt\n(e.g., if the prompt ends with a space or not). This paper presents algorithms\nfor converting token-level language models to character-level ones. We present\nboth exact and approximate algorithms. In the empirical portion of the paper,\nwe benchmark the practical runtime and approximation quality. We find that --\neven with a small computation budget -- our method is able to accurately\napproximate the character-level distribution (less than 0.00021 excess bits /\ncharacter) at reasonably fast speeds (46.3 characters / second) on the Llama\n3.1 8B language model.\n","subjects":["Computer Science/Computation and Language","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zPZ6Onz-ufe3ZxWmDvuOo6ACCVd4Dr7AD9WTNCAAgnQ","pdfSize":"777396"}