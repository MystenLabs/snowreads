{"id":"2407.19441","title":"Competition-based Adaptive ReLU for Deep Neural Networks","authors":"Junjia Chen, Zhibin Pan","authorsParsed":[["Chen","Junjia",""],["Pan","Zhibin",""]],"versions":[{"version":"v1","created":"Sun, 28 Jul 2024 09:38:13 GMT"}],"updateDate":"2024-07-30","timestamp":1722159493000,"abstract":"  Activation functions introduce nonlinearity into deep neural networks. Most\npopular activation functions allow positive values to pass through while\nblocking or suppressing negative values. From the idea that positive values and\nnegative values are equally important, and they must compete for activation, we\nproposed a new Competition-based Adaptive ReLU (CAReLU). CAReLU scales the\ninput values based on the competition results between positive values and\nnegative values. It defines two parameters to adjust the scaling strategy and\ncan be trained uniformly with other network parameters. We verify the\neffectiveness of CAReLU on image classification, super-resolution, and natural\nlanguage processing tasks. In the experiment, our method performs better than\nother widely used activation functions. In the case of replacing ReLU in\nResNet-18 with our proposed activation function, it improves the classification\naccuracy on the CIFAR-100 dataset. The effectiveness and the new perspective on\nthe utilization of competition results between positive values and negative\nvalues make CAReLU a promising activation function.\n","subjects":["Computing Research Repository/Neural and Evolutionary Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"21axJG0us1fm0cJceqk3qfAR8NkUlJ-8EPLsrt7y4Sw","pdfSize":"514667","objectId":"0xacf2fb3992319ec073812d65dcf0474a9d5b561972b23f1f0662a344ba1d1fab","registeredEpoch":"2","certifiedEpoch":"2","startEpoch":"2","endEpoch":"202"}
