{"id":"2407.04942","title":"FOSP: Fine-tuning Offline Safe Policy through World Models","authors":"Chenyang Cao, Yucheng Xin, Silang Wu, Longxiang He, Zichen Yan, Junbo\n  Tan, Xueqian Wang","authorsParsed":[["Cao","Chenyang",""],["Xin","Yucheng",""],["Wu","Silang",""],["He","Longxiang",""],["Yan","Zichen",""],["Tan","Junbo",""],["Wang","Xueqian",""]],"versions":[{"version":"v1","created":"Sat, 6 Jul 2024 03:22:57 GMT"}],"updateDate":"2024-07-09","timestamp":1720236177000,"abstract":"  Model-based Reinforcement Learning (RL) has shown its high training\nefficiency and capability of handling high-dimensional tasks. Regarding safety\nissues, safe model-based RL can achieve nearly zero-cost performance and\neffectively manage the trade-off between performance and safety. Nevertheless,\nprior works still pose safety challenges due to the online exploration in\nreal-world deployment. To address this, some offline RL methods have emerged as\nsolutions, which learn from a static dataset in a safe way by avoiding\ninteractions with the environment. In this paper, we aim to further enhance\nsafety during the deployment stage for vision-based robotic tasks by\nfine-tuning an offline-trained policy. We incorporate in-sample optimization,\nmodel-based policy expansion, and reachability guidance to construct a safe\noffline-to-online framework. Moreover, our method proves to improve the\ngeneralization of offline policy in unseen safety-constrained scenarios.\nFinally, the efficiency of our method is validated on simulation benchmarks\nwith five vision-only tasks and a real robot by solving some deployment\nproblems using limited data.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"gUQV5FGxZIj7VO77AtwFep6CjcRmUmO1h_geZSuqWgQ","pdfSize":"2351521"}