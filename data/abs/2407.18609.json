{"id":"2407.18609","title":"Denoising L\\'evy Probabilistic Models","authors":"Dario Shariatian, Umut Simsekli, Alain Durmus","authorsParsed":[["Shariatian","Dario",""],["Simsekli","Umut",""],["Durmus","Alain",""]],"versions":[{"version":"v1","created":"Fri, 26 Jul 2024 09:00:18 GMT"}],"updateDate":"2024-07-29","timestamp":1721984418000,"abstract":"  Investigating noise distribution beyond Gaussian in diffusion generative\nmodels is an open problem. The Gaussian case has seen success experimentally\nand theoretically, fitting a unified SDE framework for score-based and\ndenoising formulations. Recent studies suggest heavy-tailed noise distributions\ncan address mode collapse and manage datasets with class imbalance, heavy\ntails, or outliers. Yoon et al. (NeurIPS 2023) introduced the L\\'evy-Ito model\n(LIM), extending the SDE framework to heavy-tailed SDEs with $\\alpha$-stable\nnoise. Despite its theoretical elegance and performance gains, LIM's complex\nmathematics may limit its accessibility and broader adoption. This study takes\na simpler approach by extending the denoising diffusion probabilistic model\n(DDPM) with $\\alpha$-stable noise, creating the denoising L\\'evy probabilistic\nmodel (DLPM). Using elementary proof techniques, we show DLPM reduces to\nrunning vanilla DDPM with minimal changes, allowing the use of existing\nimplementations with minimal changes. DLPM and LIM have different training\nalgorithms and, unlike the Gaussian case, they admit different backward\nprocesses and sampling algorithms. Our experiments demonstrate that DLPM\nachieves better coverage of data distribution tail, improved generation of\nunbalanced datasets, and faster computation times with fewer backward steps.\n","subjects":["Computing Research Repository/Machine Learning","Statistics/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"S10z8XDL5w1nX9j5Ag66lYsVRttWRnPy8_a5x1Px4Qs","pdfSize":"1006422"}