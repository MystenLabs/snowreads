{
  "id": "2412.20846",
  "title": "Are LLMs Really Not Knowledgable? Mining the Submerged Knowledge in\n  LLMs' Memory",
  "authors": "Xingjian Tao, Yiwei Wang, Yujun Cai, Zhicheng Yang, Jing Tang",
  "authorsParsed": [
    [
      "Tao",
      "Xingjian",
      ""
    ],
    [
      "Wang",
      "Yiwei",
      ""
    ],
    [
      "Cai",
      "Yujun",
      ""
    ],
    [
      "Yang",
      "Zhicheng",
      ""
    ],
    [
      "Tang",
      "Jing",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 30 Dec 2024 10:29:18 GMT"
    }
  ],
  "updateDate": "2024-12-31",
  "timestamp": 1735554558000,
  "abstract": "  Large language models (LLMs) have shown promise as potential knowledge bases,\nyet they often struggle with question-answering tasks and are prone to\nhallucinations. While previous research attributes these issues to knowledge\ngaps in the model's parameters, our investigation reveals a different\nphenomenon: LLMs often retain correct knowledge even when generating incorrect\nanswers. Through analysis of model's internal representations, we find that\ncorrect answers frequently appear among high-probability tokens despite not\nbeing selected as final outputs. Based on this observation, we introduce\nHits@k, a new metric to assess knowledge retention independent of expression\naccuracy. Our extensive experiments demonstrate that LLMs store significantly\nmore knowledge than their QA performance suggests. Building on these findings,\nwe develop SkipUnsure, a method to improve answer accuracy by leveraging\ndetected but unexpressed knowledge. Experiments on both open-domain and\nspecific-domain datasets show consistent improvements, with accuracy gains of\nup to 11.8% on DBPedia and 6.3% on IMDB, without requiring model retraining.\n",
  "subjects": [
    "Computer Science/Computation and Language"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "ZUoWl7830J8CVU9_0F7A-uffkufK8uhMKgpGyceLn8o",
  "pdfSize": "814097"
}