{"id":"2407.05732","title":"FairPFN: Transformers Can do Counterfactual Fairness","authors":"Jake Robertson, Noah Hollmann, Noor Awad, Frank Hutter","authorsParsed":[["Robertson","Jake",""],["Hollmann","Noah",""],["Awad","Noor",""],["Hutter","Frank",""]],"versions":[{"version":"v1","created":"Mon, 8 Jul 2024 08:36:44 GMT"}],"updateDate":"2024-07-09","timestamp":1720427804000,"abstract":"  Machine Learning systems are increasingly prevalent across healthcare, law\nenforcement, and finance but often operate on historical data, which may carry\nbiases against certain demographic groups. Causal and counterfactual fairness\nprovides an intuitive way to define fairness that closely aligns with legal\nstandards. Despite its theoretical benefits, counterfactual fairness comes with\nseveral practical limitations, largely related to the reliance on domain\nknowledge and approximate causal discovery techniques in constructing a causal\nmodel. In this study, we take a fresh perspective on counterfactually fair\nprediction, building upon recent work in in context learning (ICL) and prior\nfitted networks (PFNs) to learn a transformer called FairPFN. This model is\npretrained using synthetic fairness data to eliminate the causal effects of\nprotected attributes directly from observational data, removing the requirement\nof access to the correct causal model in practice. In our experiments, we\nthoroughly assess the effectiveness of FairPFN in eliminating the causal impact\nof protected attributes on a series of synthetic case studies and real world\ndatasets. Our findings pave the way for a new and promising research area:\ntransformers for causal and counterfactual fairness.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence","Computing Research Repository/Computers and Society"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AndT2ziKExNt6EjwCGH4HH6nlOGmuZVYopaVjY8yGeg","pdfSize":"8267798"}