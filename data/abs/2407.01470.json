{"id":"2407.01470","title":"DogeRM: Equipping Reward Models with Domain Knowledge through Model\n  Merging","authors":"Tzu-Han Lin and Chen-An Li and Hung-yi Lee and Yun-Nung Chen","authorsParsed":[["Lin","Tzu-Han",""],["Li","Chen-An",""],["Lee","Hung-yi",""],["Chen","Yun-Nung",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 17:01:54 GMT"}],"updateDate":"2024-07-02","timestamp":1719853314000,"abstract":"  Reinforcement learning from human feedback (RLHF) is a popular strategy for\naligning large language models (LLMs) with desired behaviors. Reward modeling\nis a crucial step in RLHF. However, collecting paired preference data for\ntraining reward models is often costly and time-consuming, especially for\ndomain-specific preferences requiring expert annotation. To address this\nchallenge, we propose the \\textbf{Do}main knowled\\textbf{ge} merged\n\\textbf{R}eward \\textbf{M}odel (DogeRM), a novel framework that integrates\ndomain-specific knowledge into a general reward model by model merging. The\nexperiments demonstrate that DogeRM enhances performance across different\nbenchmarks and provide a detailed analysis showcasing the effects of model\nmerging, showing the great potential of facilitating model alignment.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"rnezWOnrN3PfCdqBC3XkoUBi97ETjFEnqKcKIDBIAbQ","pdfSize":"5102267"}