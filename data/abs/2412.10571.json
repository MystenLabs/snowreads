{"id":"2412.10571","title":"Evidence Contextualization and Counterfactual Attribution for\n  Conversational QA over Heterogeneous Data with RAG Systems","authors":"Rishiraj Saha Roy, Joel Schlotthauer, Chris Hinze, Andreas Foltyn,\n  Luzian Hahn, Fabian Kuech","authorsParsed":[["Roy","Rishiraj Saha",""],["Schlotthauer","Joel",""],["Hinze","Chris",""],["Foltyn","Andreas",""],["Hahn","Luzian",""],["Kuech","Fabian",""]],"versions":[{"version":"v1","created":"Fri, 13 Dec 2024 21:28:17 GMT"},{"version":"v2","created":"Wed, 18 Dec 2024 22:01:52 GMT"},{"version":"v3","created":"Mon, 23 Dec 2024 16:12:59 GMT"}],"updateDate":"2024-12-24","timestamp":1734125297000,"abstract":"  Retrieval Augmented Generation (RAG) works as a backbone for interacting with\nan enterprise's own data via Conversational Question Answering (ConvQA). In a\nRAG system, a retriever fetches passages from a collection in response to a\nquestion, which are then included in the prompt of a large language model (LLM)\nfor generating a natural language (NL) answer. However, several RAG systems\ntoday suffer from two shortcomings: (i) retrieved passages usually contain\ntheir raw text and lack appropriate document context, negatively impacting both\nretrieval and answering quality; and (ii) attribution strategies that explain\nanswer generation typically rely only on similarity between the answer and the\nretrieved passages, thereby only generating plausible but not causal\nexplanations. In this work, we demonstrate RAGONITE, a RAG system that remedies\nthe above concerns by: (i) contextualizing evidence with source metadata and\nsurrounding text; and (ii) computing counterfactual attribution, a causal\nexplanation approach where the contribution of an evidence to an answer is\ndetermined by the similarity of the original response to the answer obtained by\nremoving that evidence. To evaluate our proposals, we release a new benchmark\nConfQuestions: it has 300 hand-created conversational questions, each in\nEnglish and German, coupled with ground truth URLs, completed questions, and\nanswers from 215 public Confluence pages. These documents are typical of\nenterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE\non ConfQuestions show the viability of our ideas: contextualization improves\nRAG performance, and counterfactual explanations outperform standard\nattribution.\n","subjects":["Computer Science/Computation and Language","Computer Science/Information Retrieval"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"lMxZ1XRVPAjaHpuL05fcxd6MpjjC8orwlrtDLsvdwiY","pdfSize":"2910837"}