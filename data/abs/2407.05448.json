{"id":"2407.05448","title":"Self-supervised Learning via Cluster Distance Prediction for Operating\n  Room Context Awareness","authors":"Idris Hamoud, Alexandros Karargyris, Aidean Sharghi, Omid Mohareri,\n  Nicolas Padoy","authorsParsed":[["Hamoud","Idris",""],["Karargyris","Alexandros",""],["Sharghi","Aidean",""],["Mohareri","Omid",""],["Padoy","Nicolas",""]],"versions":[{"version":"v1","created":"Sun, 7 Jul 2024 17:17:52 GMT"}],"updateDate":"2024-07-09","timestamp":1720372672000,"abstract":"  Semantic segmentation and activity classification are key components to\ncreating intelligent surgical systems able to understand and assist clinical\nworkflow. In the Operating Room, semantic segmentation is at the core of\ncreating robots aware of clinical surroundings, whereas activity classification\naims at understanding OR workflow at a higher level. State-of-the-art semantic\nsegmentation and activity recognition approaches are fully supervised, which is\nnot scalable. Self-supervision can decrease the amount of annotated data\nneeded. We propose a new 3D self-supervised task for OR scene understanding\nutilizing OR scene images captured with ToF cameras. Contrary to other\nself-supervised approaches, where handcrafted pretext tasks are focused on 2D\nimage features, our proposed task consists of predicting the relative 3D\ndistance of image patches by exploiting the depth maps. Learning 3D spatial\ncontext generates discriminative features for our downstream tasks. Our\napproach is evaluated on two tasks and datasets containing multi-view data\ncaptured from clinical scenarios. We demonstrate a noteworthy improvement of\nperformance on both tasks, specifically on low-regime data where utility of\nself-supervised learning is the highest.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"cW1PO1u11B4UyZUpyz9D5dBiF_hl9IYFYmEf6xhGMNs","pdfSize":"2105029"}
