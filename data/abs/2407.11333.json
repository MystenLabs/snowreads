{"id":"2407.11333","title":"Disentangled Acoustic Fields For Multimodal Physical Scene Understanding","authors":"Jie Yin, Andrew Luo, Yilun Du, Anoop Cherian, Tim K. Marks, Jonathan\n  Le Roux, and Chuang Gan","authorsParsed":[["Yin","Jie",""],["Luo","Andrew",""],["Du","Yilun",""],["Cherian","Anoop",""],["Marks","Tim K.",""],["Roux","Jonathan Le",""],["Gan","Chuang",""]],"versions":[{"version":"v1","created":"Tue, 16 Jul 2024 02:54:54 GMT"}],"updateDate":"2024-07-17","timestamp":1721098494000,"abstract":"  We study the problem of multimodal physical scene understanding, where an\nembodied agent needs to find fallen objects by inferring object properties,\ndirection, and distance of an impact sound source. Previous works adopt\nfeed-forward neural networks to directly regress the variables from sound,\nleading to poor generalization and domain adaptation issues. In this paper, we\nillustrate that learning a disentangled model of acoustic formation, referred\nto as disentangled acoustic field (DAF), to capture the sound generation and\npropagation process, enables the embodied agent to construct a spatial\nuncertainty map over where the objects may have fallen. We demonstrate that our\nanalysis-by-synthesis framework can jointly infer sound properties by\nexplicitly decomposing and factorizing the latent space of the disentangled\nmodel. We further show that the spatial uncertainty map can significantly\nimprove the success rate for the localization of fallen objects by proposing\nmultiple plausible exploration locations.\n","subjects":["Computing Research Repository/Robotics","Computing Research Repository/Sound","Electrical Engineering and Systems Science/Audio and Speech Processing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"_w0i5uKXQ0KQ_3aTubEPuX82RwNr2fAU3UlyzVTz7Uk","pdfSize":"898298"}