{
  "id": "2412.17497",
  "title": "Advantages of density in tensor network geometries for gradient based\n  training",
  "authors": "Sergi Masot-Llima and Artur Garcia-Saez",
  "authorsParsed": [
    [
      "Masot-Llima",
      "Sergi",
      ""
    ],
    [
      "Garcia-Saez",
      "Artur",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 23 Dec 2024 11:54:44 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734954884000,
  "abstract": "  Tensor networks are a very powerful data structure tool originating from\nquantum system simulations. In recent years, they have seen increased use in\nmachine learning, mostly in trainings with gradient-based techniques, due to\ntheir flexibility and performance exploiting hardware acceleration. As\nans\\\"atze, tensor networks can be used with flexible geometries, and it is\nknown that for highly regular ones their dimensionality has a large impact in\nperformance and representation power. For heterogeneous structures, however,\nthese effects are not completely characterized. In this article, we train\ntensor networks with different geometries to encode a random quantum state, and\nsee that densely connected structures achieve better infidelities than more\nsparse structures, with higher success rates and less time. Additionally, we\ngive some general insight on how to improve memory requirements on these sparse\nstructures and its impact on the trainings. Finally, as we use HPC resources\nfor the calculations, we discuss the requirements for this approach and\nshowcase performance improvements with GPU acceleration on a last-generation\nsupercomputer.\n",
  "subjects": [
    "Physics/Quantum Physics"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "2OLzqGvVkEUmxSckAvcYnrryhmiQqFaCmYpELVRCJwI",
  "pdfSize": "4926806"
}