{"id":"2412.21022","title":"Text Classification: Neural Networks VS Machine Learning Models VS\n  Pre-trained Models","authors":"Christos Petridis","authorsParsed":[["Petridis","Christos",""]],"versions":[{"version":"v1","created":"Mon, 30 Dec 2024 15:44:05 GMT"}],"updateDate":"2024-12-31","timestamp":1735573445000,"abstract":"  Text classification is a very common task nowadays and there are many\nefficient methods and algorithms that we can employ to accomplish it.\nTransformers have revolutionized the field of deep learning, particularly in\nNatural Language Processing (NLP) and have rapidly expanded to other domains\nsuch as computer vision, time-series analysis and more. The transformer model\nwas firstly introduced in the context of machine translation and its\narchitecture relies on self-attention mechanisms to capture complex\nrelationships within data sequences. It is able to handle long-range\ndependencies more effectively than traditional neural networks (such as\nRecurrent Neural Networks and Multilayer Perceptrons). In this work, we present\na comparison between different techniques to perform text classification. We\ntake into consideration seven pre-trained models, three standard neural\nnetworks and three machine learning models. For standard neural networks and\nmachine learning models we also compare two embedding techniques: TF-IDF and\nGloVe, with the latter consistently outperforming the former. Finally, we\ndemonstrate the results from our experiments where pre-trained models such as\nBERT and DistilBERT always perform better than standard models/algorithms.\n","subjects":["Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"eO7geJ-9cij6a4RIU63IzKtLmMZfO9qLpQKe2t5d6So","pdfSize":"1033607"}