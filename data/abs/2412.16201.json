{
  "id": "2412.16201",
  "title": "CLIP-RLDrive: Human-Aligned Autonomous Driving via CLIP-Based Reward\n  Shaping in Reinforcement Learning",
  "authors": "Erfan Doroudian, Hamid Taghavifar",
  "authorsParsed": [
    [
      "Doroudian",
      "Erfan",
      ""
    ],
    [
      "Taghavifar",
      "Hamid",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Tue, 17 Dec 2024 00:12:45 GMT"
    }
  ],
  "updateDate": "2024-12-24",
  "timestamp": 1734394365000,
  "abstract": "  This paper presents CLIP-RLDrive, a new reinforcement learning (RL)-based\nframework for improving the decision-making of autonomous vehicles (AVs) in\ncomplex urban driving scenarios, particularly in unsignalized intersections. To\nachieve this goal, the decisions for AVs are aligned with human-like\npreferences through Contrastive Language-Image Pretraining (CLIP)-based reward\nshaping. One of the primary difficulties in RL scheme is designing a suitable\nreward model, which can often be challenging to achieve manually due to the\ncomplexity of the interactions and the driving scenarios. To deal with this\nissue, this paper leverages Vision-Language Models (VLMs), particularly CLIP,\nto build an additional reward model based on visual and textual cues.\n",
  "subjects": [
    "Computer Science/Robotics",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning",
    "Computer Science/Systems and Control",
    "Electrical Engineering and Systems Science/Systems and Control"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "x29d-SFi6ZRdxWCY8ZYmi3lNbmR5eUEmydTV4LUbhj8",
  "pdfSize": "959915"
}