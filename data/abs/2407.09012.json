{"id":"2407.09012","title":"TCAN: Animating Human Images with Temporally Consistent Pose Guidance\n  using Diffusion Models","authors":"Jeongho Kim, Min-Jung Kim, Junsoo Lee, and Jaegul Choo","authorsParsed":[["Kim","Jeongho",""],["Kim","Min-Jung",""],["Lee","Junsoo",""],["Choo","Jaegul",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 06:02:13 GMT"}],"updateDate":"2024-07-15","timestamp":1720764133000,"abstract":"  Pose-driven human-image animation diffusion models have shown remarkable\ncapabilities in realistic human video synthesis. Despite the promising results\nachieved by previous approaches, challenges persist in achieving temporally\nconsistent animation and ensuring robustness with off-the-shelf pose detectors.\nIn this paper, we present TCAN, a pose-driven human image animation method that\nis robust to erroneous poses and consistent over time. In contrast to previous\nmethods, we utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption pairs. To\nkeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling the\nnetwork to align the latent space between the pose and appearance features.\nAdditionally, by introducing an additional temporal layer to the ControlNet, we\nenhance robustness against outliers of the pose detector. Through the analysis\nof attention maps over the temporal axis, we also designed a novel temperature\nmap leveraging pose information, allowing for a more static background.\nExtensive experiments demonstrate that the proposed method can achieve\npromising results in video synthesis tasks encompassing various poses, like\nchibi. Project Page: https://eccv2024tcan.github.io/\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"7r0Ny5MuwyQ3xmLF07ih4G93rRRrESVzzwcHS6_O2OU","pdfSize":"9163502"}