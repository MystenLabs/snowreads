{"id":"2412.16794","title":"Gradient-Based Non-Linear Inverse Learning","authors":"Abhishake, Nicole M\\\"ucke, and Tapio Helin","authorsParsed":[["Abhishake","",""],["MÃ¼cke","Nicole",""],["Helin","Tapio",""]],"versions":[{"version":"v1","created":"Sat, 21 Dec 2024 22:38:17 GMT"}],"updateDate":"2024-12-24","timestamp":1734820697000,"abstract":"  We study statistical inverse learning in the context of nonlinear inverse\nproblems under random design. Specifically, we address a class of nonlinear\nproblems by employing gradient descent (GD) and stochastic gradient descent\n(SGD) with mini-batching, both using constant step sizes. Our analysis derives\nconvergence rates for both algorithms under classical a priori assumptions on\nthe smoothness of the target function. These assumptions are expressed in terms\nof the integral operator associated with the tangent kernel, as well as through\na bound on the effective dimension. Additionally, we establish stopping times\nthat yield minimax-optimal convergence rates within the classical reproducing\nkernel Hilbert space (RKHS) framework. These results demonstrate the efficacy\nof GD and SGD in achieving optimal rates for nonlinear inverse problems in\nrandom design.\n","subjects":["Statistics/Machine Learning","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eW20dxvVqUkKb6zyrVtAQqbXEzXMewaLLHxBbizE5n0","pdfSize":"455071"}