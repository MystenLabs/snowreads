{
  "id": "2412.19048",
  "title": "Jasper and Stella: distillation of SOTA embedding models",
  "authors": "Dun Zhang, Jiacheng Li, Ziyang Zeng, and Fulong Wang",
  "authorsParsed": [
    [
      "Zhang",
      "Dun",
      ""
    ],
    [
      "Li",
      "Jiacheng",
      ""
    ],
    [
      "Zeng",
      "Ziyang",
      ""
    ],
    [
      "Wang",
      "Fulong",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Thu, 26 Dec 2024 04:05:28 GMT"
    },
    {
      "version": "v2",
      "created": "Thu, 23 Jan 2025 16:01:22 GMT"
    }
  ],
  "updateDate": "2025-01-24",
  "timestamp": 1735185928000,
  "abstract": "  A crucial component in many deep learning applications, such as Frequently\nAsked Questions (FAQ) and Retrieval-Augmented Generation (RAG), is dense\nretrieval. In this process, embedding models transform raw text into numerical\nvectors. However, the embedding models that currently excel on text embedding\nbenchmarks, like the Massive Text Embedding Benchmark (MTEB), often have\nnumerous parameters and high vector dimensionality. This poses challenges for\ntheir application in real-world scenarios. To address this issue, we propose a\nnovel multi-stage distillation framework that enables a smaller student\nembedding model to distill multiple larger teacher embedding models through\nthree carefully designed losses. Meanwhile, we utilize Matryoshka\nRepresentation Learning (MRL) to reduce the vector dimensionality of the\nstudent embedding model effectively. Our student model named Jasper with 2\nbillion parameters, built upon the Stella embedding model, obtained the No.3\nposition on the MTEB leaderboard (as of December 24, 2024), achieving an\naverage 71.54 score across 56 datasets. We have released the model and data on\nthe Hugging Face Hub\n(https://huggingface.co/infgrad/jasper_en_vision_language_v1)\n(https://huggingface.co/datasets/infgrad/jasper_text_distill_dataset), and the\ntraining codes are available in this project repository\n(https://github.com/NLPJCL/RAG-Retrieval).\n",
  "subjects": [
    "Computer Science/Information Retrieval"
  ],
  "license": "http://creativecommons.org/licenses/by-sa/4.0/",
  "blobId": "_QCgXAN97L-ouFWu0v2QgKr_vFrKTCxXTUMJ5VCd8-M",
  "pdfSize": "381446"
}