{
  "id": "2412.08385",
  "title": "NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment\n  Prediction Dataset and Specialized Language Model for Enhanced Decision\n  Analysis",
  "authors": "Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra,\n  Noel Shallum, Kripabandhu Ghosh and Arnab Bhattacharya",
  "authorsParsed": [
    [
      "Nigam",
      "Shubham Kumar",
      ""
    ],
    [
      "Patnaik",
      "Balaramamahanthi Deepak",
      ""
    ],
    [
      "Mishra",
      "Shivam",
      ""
    ],
    [
      "Shallum",
      "Noel",
      ""
    ],
    [
      "Ghosh",
      "Kripabandhu",
      ""
    ],
    [
      "Bhattacharya",
      "Arnab",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 11 Dec 2024 13:50:17 GMT"
    }
  ],
  "updateDate": "2024-12-12",
  "timestamp": 1733925017000,
  "abstract": "  The integration of artificial intelligence (AI) in legal judgment prediction\n(LJP) has the potential to transform the legal landscape, particularly in\njurisdictions like India, where a significant backlog of cases burdens the\nlegal system. This paper introduces NyayaAnumana, the largest and most diverse\ncorpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945\npreprocessed cases. NyayaAnumana, which combines the words \"Nyay\" (judgment)\nand \"Anuman\" (prediction or inference) respectively for most major Indian\nlanguages, includes a wide range of cases from the Supreme Court, High Courts,\nTribunal Courts, District Courts, and Daily Orders and, thus, provides\nunparalleled diversity and coverage. Our dataset surpasses existing datasets\nlike PredEx and ILDC, offering a comprehensive foundation for advanced AI\nresearch in the legal domain.\n  In addition to the dataset, we present INLegalLlama, a domain-specific\ngenerative large language model (LLM) tailored to the intricacies of the Indian\nlegal system. It is developed through a two-phase training approach over a base\nLLaMa model. First, Indian legal documents are injected using continual\npretraining. Second, task-specific supervised finetuning is done. This method\nallows the model to achieve a deeper understanding of legal contexts.\n  Our experiments demonstrate that incorporating diverse court data\nsignificantly boosts model accuracy, achieving approximately 90% F1-score in\nprediction tasks. INLegalLlama not only improves prediction accuracy but also\noffers comprehensible explanations, addressing the need for explainability in\nAI-assisted legal decisions.\n",
  "subjects": [
    "Computer Science/Computation and Language",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Information Retrieval",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "a5uEFg5R3Z2kmbaWN916zJzTwHcQoWzp5MTBpZB-2Ao",
  "pdfSize": "928963"
}