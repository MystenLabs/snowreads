{"id":"2407.09087","title":"On the Role of Discrete Tokenization in Visual Representation Learning","authors":"Tianqi Du, Yifei Wang, Yisen Wang","authorsParsed":[["Du","Tianqi",""],["Wang","Yifei",""],["Wang","Yisen",""]],"versions":[{"version":"v1","created":"Fri, 12 Jul 2024 08:25:31 GMT"}],"updateDate":"2024-07-15","timestamp":1720772731000,"abstract":"  In the realm of self-supervised learning (SSL), masked image modeling (MIM)\nhas gained popularity alongside contrastive learning methods. MIM involves\nreconstructing masked regions of input images using their unmasked portions. A\nnotable subset of MIM methodologies employs discrete tokens as the\nreconstruction target, but the theoretical underpinnings of this choice remain\nunderexplored. In this paper, we explore the role of these discrete tokens,\naiming to unravel their benefits and limitations. Building upon the connection\nbetween MIM and contrastive learning, we provide a comprehensive theoretical\nunderstanding on how discrete tokenization affects the model's generalization\ncapabilities. Furthermore, we propose a novel metric named TCAS, which is\nspecifically designed to assess the effectiveness of discrete tokens within the\nMIM framework. Inspired by this metric, we contribute an innovative tokenizer\ndesign and propose a corresponding MIM method named ClusterMIM. It demonstrates\nsuperior performance on a variety of benchmark datasets and ViT backbones. Code\nis available at https://github.com/PKU-ML/ClusterMIM.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Computer Vision and Pattern Recognition"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"AZHfW3_kp5kUQnRz8q0bt7axsWQL_pV1rqvjgKr5TWw","pdfSize":"540620"}