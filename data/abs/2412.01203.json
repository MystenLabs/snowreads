{
  "id": "2412.01203",
  "title": "Domain Adaptive Diabetic Retinopathy Grading with Model Absence and\n  Flowing Data",
  "authors": "Wenxin Su, Song Tang, Xiaofeng Liu, Xiaojing Yi, Mao Ye, Chunxiao Zu,\n  Jiahao Li, and Xiatian Zhu",
  "authorsParsed": [
    [
      "Su",
      "Wenxin",
      ""
    ],
    [
      "Tang",
      "Song",
      ""
    ],
    [
      "Liu",
      "Xiaofeng",
      ""
    ],
    [
      "Yi",
      "Xiaojing",
      ""
    ],
    [
      "Ye",
      "Mao",
      ""
    ],
    [
      "Zu",
      "Chunxiao",
      ""
    ],
    [
      "Li",
      "Jiahao",
      ""
    ],
    [
      "Zhu",
      "Xiatian",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 2 Dec 2024 07:14:25 GMT"
    }
  ],
  "updateDate": "2024-12-03",
  "timestamp": 1733123665000,
  "abstract": "  Domain shift (the difference between source and target domains) poses a\nsignificant challenge in clinical applications, e.g., Diabetic Retinopathy (DR)\ngrading. Despite considering certain clinical requirements, like source data\nprivacy, conventional transfer methods are predominantly model-centered and\noften struggle to prevent model-targeted attacks. In this paper, we address a\nchallenging Online Model-aGnostic Domain Adaptation (OMG-DA) setting, driven by\nthe demands of clinical environments. This setting is characterized by the\nabsence of the model and the flow of target data. To tackle the new challenge,\nwe propose a novel approach, Generative Unadversarial ExampleS (GUES), which\nenables adaptation from a data-centric perspective. Specifically, we first\ntheoretically reformulate conventional perturbation optimization in a\ngenerative way--learning a perturbation generation function with a latent input\nvariable. During model instantiation, we leverage a Variational AutoEncoder to\nexpress this function. The encoder with the reparameterization trick predicts\nthe latent input, whilst the decoder is responsible for the generation.\nFurthermore, the saliency map is selected as pseudo-perturbation labels.\nBecause it not only captures potential lesions but also theoretically provides\nan upper bound on the function input, enabling the identification of the latent\nvariable. Extensive comparative experiments on DR benchmarks with both frozen\npre-trained models and trainable models demonstrate the superiority of GUES,\nshowing robustness even with small batch size.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "sxlPktdhp6cxdYjZ5kDDo90nA35ft98eKFeD5h9xpnA",
  "pdfSize": "4114273"
}