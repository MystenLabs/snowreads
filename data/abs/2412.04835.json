{"id":"2412.04835","title":"Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards\n  for Visuomotor Robot Policy Alignment","authors":"Ran Tian, Yilin Wu, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik,\n  and Andrea Bajcsy","authorsParsed":[["Tian","Ran",""],["Wu","Yilin",""],["Xu","Chenfeng",""],["Tomizuka","Masayoshi",""],["Malik","Jitendra",""],["Bajcsy","Andrea",""]],"versions":[{"version":"v1","created":"Fri, 6 Dec 2024 08:04:02 GMT"}],"updateDate":"2024-12-09","timestamp":1733472242000,"abstract":"  Visuomotor robot policies, increasingly pre-trained on large-scale datasets,\npromise significant advancements across robotics domains. However, aligning\nthese policies with end-user preferences remains a challenge, particularly when\nthe preferences are hard to specify. While reinforcement learning from human\nfeedback (RLHF) has become the predominant mechanism for alignment in\nnon-embodied domains like large language models, it has not seen the same\nsuccess in aligning visuomotor policies due to the prohibitive amount of human\nfeedback required to learn visual reward functions. To address this limitation,\nwe propose Representation-Aligned Preference-based Learning (RAPL), an\nobservation-only method for learning visual rewards from significantly less\nhuman preference feedback. Unlike traditional RLHF, RAPL focuses human feedback\non fine-tuning pre-trained vision encoders to align with the end-user's visual\nrepresentation and then constructs a dense visual reward via feature matching\nin this aligned representation space. We first validate RAPL through simulation\nexperiments in the X-Magical benchmark and Franka Panda robotic manipulation,\ndemonstrating that it can learn rewards aligned with human preferences, more\nefficiently uses preference data, and generalizes across robot embodiments.\nFinally, our hardware experiments align pre-trained Diffusion Policies for\nthree object manipulation tasks. We find that RAPL can fine-tune these policies\nwith 5x less real human preference data, taking the first step towards\nminimizing human feedback while maximizing visuomotor robot policy alignment.\n","subjects":["Computer Science/Robotics","Computer Science/Artificial Intelligence","Computer Science/Computer Vision and Pattern Recognition","Computer Science/Machine Learning"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"zJpmIqnMQ0NqeSEJ4-tIGo_x2Lce6hgcVhVi7CnbCAo","pdfSize":"5082742"}