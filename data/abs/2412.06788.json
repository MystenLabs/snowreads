{"id":"2412.06788","title":"Poison Attacks and Adversarial Prompts Against an Informed University\n  Virtual Assistant","authors":"Ivan A. Fernandez, Subash Neupane, Sudip Mittal, Shahram Rahimi","authorsParsed":[["Fernandez","Ivan A.",""],["Neupane","Subash",""],["Mittal","Sudip",""],["Rahimi","Shahram",""]],"versions":[{"version":"v1","created":"Sun, 3 Nov 2024 05:34:38 GMT"}],"updateDate":"2024-12-11","timestamp":1730612078000,"abstract":"  Recent research has shown that large language models (LLMs) are particularly\nvulnerable to adversarial attacks. Since the release of ChatGPT, various\nindustries are adopting LLM-based chatbots and virtual assistants in their data\nworkflows. The rapid development pace of AI-based systems is being driven by\nthe potential of Generative AI (GenAI) to assist humans in decision making. The\nimmense optimism behind GenAI often overshadows the adversarial risks\nassociated with these technologies. A threat actor can use security gaps, poor\nsafeguards, and limited data governance to carry out attacks that grant\nunauthorized access to the system and its data. As a proof-of-concept, we\nassess the performance of BarkPlug, the Mississippi State University chatbot,\nagainst data poison attacks from a red team perspective.\n","subjects":["Computer Science/Cryptography and Security"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"ZFFr57ziOQBmsH_GC1AmPbvTWBqh_FhixqcQV33L0_I","pdfSize":"605246"}