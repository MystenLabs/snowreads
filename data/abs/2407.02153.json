{"id":"2407.02153","title":"Equidistribution-based training of Free Knot Splines and ReLU Neural\n  Networks","authors":"Simone Appella, Simon Arridge, Chris Budd, Teo Deveney, Lisa Maria\n  Kreusser","authorsParsed":[["Appella","Simone",""],["Arridge","Simon",""],["Budd","Chris",""],["Deveney","Teo",""],["Kreusser","Lisa Maria",""]],"versions":[{"version":"v1","created":"Tue, 2 Jul 2024 10:51:36 GMT"}],"updateDate":"2024-07-03","timestamp":1719917496000,"abstract":"  We consider the problem of one-dimensional function approximation using\nshallow neural networks (NN) with a rectified linear unit (ReLU) activation\nfunction and compare their training with traditional methods such as univariate\nFree Knot Splines (FKS). ReLU NNs and FKS span the same function space, and\nthus have the same theoretical expressivity. In the case of ReLU NNs, we show\nthat their ill-conditioning degrades rapidly as the width of the network\nincreases. This often leads to significantly poorer approximation in contrast\nto the FKS representation, which remains well-conditioned as the number of\nknots increases. We leverage the theory of optimal piecewise linear\ninterpolants to improve the training procedure for a ReLU NN. Using the\nequidistribution principle, we propose a two-level procedure for training the\nFKS by first solving the nonlinear problem of finding the optimal knot\nlocations of the interpolating FKS. Determining the optimal knots then acts as\na good starting point for training the weights of the FKS. The training of the\nFKS gives insights into how we can train a ReLU NN effectively to give an\nequally accurate approximation. More precisely, we combine the training of the\nReLU NN with an equidistribution based loss to find the breakpoints of the ReLU\nfunctions, combined with preconditioning the ReLU NN approximation (to take an\nFKS form) to find the scalings of the ReLU functions, leads to a\nwell-conditioned and reliable method of finding an accurate ReLU NN\napproximation to a target function. We test this method on a series or regular,\nsingular, and rapidly varying target functions and obtain good results\nrealising the expressivity of the network in this case.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Numerical Analysis","Mathematics/Numerical Analysis"],"license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","blobId":"cl0HjzXzugdSD9og__bRz3zKALHTKhyVILOZJJq6LaU","pdfSize":"3559306"}