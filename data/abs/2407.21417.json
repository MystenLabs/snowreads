{"id":"2407.21417","title":"Dancing in Chains: Reconciling Instruction Following and Faithfulness in\n  Language Models","authors":"Zhengxuan Wu and Yuhao Zhang and Peng Qi and Yumo Xu and Rujun Han and\n  Yian Zhang and Jifan Chen and Bonan Min and Zhiheng Huang","authorsParsed":[["Wu","Zhengxuan",""],["Zhang","Yuhao",""],["Qi","Peng",""],["Xu","Yumo",""],["Han","Rujun",""],["Zhang","Yian",""],["Chen","Jifan",""],["Min","Bonan",""],["Huang","Zhiheng",""]],"versions":[{"version":"v1","created":"Wed, 31 Jul 2024 08:05:04 GMT"}],"updateDate":"2024-08-01","timestamp":1722413104000,"abstract":"  Modern language models (LMs) need to follow human instructions while being\nfaithful; yet, they often fail to achieve both. Here, we provide concrete\nevidence of a trade-off between instruction following (i.e., follow open-ended\ninstructions) and faithfulness (i.e., ground responses in given context) when\ntraining LMs with these objectives. For instance, fine-tuning LLaMA-7B on\ninstruction following datasets renders it less faithful. Conversely,\ninstruction-tuned Vicuna-7B shows degraded performance at following\ninstructions when further optimized on tasks that require contextual grounding.\nOne common remedy is multi-task learning (MTL) with data mixing, yet it remains\nfar from achieving a synergic outcome. We propose a simple yet effective method\nthat relies on Rejection Sampling for Continued Self-instruction Tuning\n(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find\nthat less is more, as training ReSet with high-quality, yet substantially\nsmaller data (three-fold less) yields superior results. Our findings offer a\nbetter understanding of objective discrepancies in alignment training of LMs.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"eJFHET1m4fkMYLOaZcWXE2ZKV4mc5fbsmG78Ibdz8ak","pdfSize":"2359903"}