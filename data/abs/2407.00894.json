{"id":"2407.00894","title":"How to Leverage Digit Embeddings to Represent Numbers?","authors":"Jasivan Alex Sivakumar and Nafise Sadat Moosavi","authorsParsed":[["Sivakumar","Jasivan Alex",""],["Moosavi","Nafise Sadat",""]],"versions":[{"version":"v1","created":"Mon, 1 Jul 2024 01:31:41 GMT"}],"updateDate":"2024-07-02","timestamp":1719797501000,"abstract":"  Apart from performing arithmetic operations, understanding numbers themselves\nis still a challenge for existing language models. Simple generalisations, such\nas solving 100+200 instead of 1+2, can substantially affect model performance\n(Sivakumar and Moosavi, 2023). Among various techniques, character-level\nembeddings of numbers have emerged as a promising approach to improve number\nrepresentation. However, this method has limitations as it leaves the task of\naggregating digit representations to the model, which lacks direct supervision\nfor this process. In this paper, we explore the use of mathematical priors to\ncompute aggregated digit embeddings and explicitly incorporate these aggregates\ninto transformer models. This can be achieved either by adding a special token\nto the input embeddings or by introducing an additional loss function to\nenhance correct predictions. We evaluate the effectiveness of incorporating\nthis explicit aggregation, analysing its strengths and shortcomings, and\ndiscuss future directions to better benefit from this approach. Our methods,\nwhile simple, are compatible with any pretrained model and require only a few\nlines of code, which we have made publicly available.\n","subjects":["Computing Research Repository/Computation and Language"],"license":"http://creativecommons.org/licenses/by-sa/4.0/","blobId":"faXR0S8NRQ1OI2SBTw3cYYn_W8mPyeobZ_v6gWlK7tc","pdfSize":"527756","objectId":"0x25b85a5f347b48d722c1f9b95cd0f6f0a683235b78fad5fe3b224055f923a32b","registeredEpoch":"3","certifiedEpoch":"3","startEpoch":"3","endEpoch":"203"}
