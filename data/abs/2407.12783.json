{"id":"2407.12783","title":"SMooDi: Stylized Motion Diffusion Model","authors":"Lei Zhong, Yiming Xie, Varun Jampani, Deqing Sun, Huaizu Jiang","authorsParsed":[["Zhong","Lei",""],["Xie","Yiming",""],["Jampani","Varun",""],["Sun","Deqing",""],["Jiang","Huaizu",""]],"versions":[{"version":"v1","created":"Wed, 17 Jul 2024 17:59:42 GMT"}],"updateDate":"2024-07-18","timestamp":1721239182000,"abstract":"  We introduce a novel Stylized Motion Diffusion model, dubbed SMooDi, to\ngenerate stylized motion driven by content texts and style motion sequences.\nUnlike existing methods that either generate motion of various content or\ntransfer style from one sequence to another, SMooDi can rapidly generate motion\nacross a broad range of content and diverse styles. To this end, we tailor a\npre-trained text-to-motion model for stylization. Specifically, we propose\nstyle guidance to ensure that the generated motion closely matches the\nreference style, alongside a lightweight style adaptor that directs the motion\ntowards the desired style while ensuring realism. Experiments across various\napplications demonstrate that our proposed framework outperforms existing\nmethods in stylized motion generation.\n","subjects":["Computing Research Repository/Computer Vision and Pattern Recognition","Computing Research Repository/Graphics"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"nY8qROZtccKHdW5g6t5hjk-oSKfUndV6r53L74mxl38","pdfSize":"14049197"}