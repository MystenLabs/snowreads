{
  "id": "2412.09972",
  "title": "Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial\n  Data Management Perspective",
  "authors": "Yuchen Fang, Yuxuan Liang, Bo Hui, Zezhi Shao, Liwei Deng, Xu Liu,\n  Xinke Jiang, Kai Zheng",
  "authorsParsed": [
    [
      "Fang",
      "Yuchen",
      ""
    ],
    [
      "Liang",
      "Yuxuan",
      ""
    ],
    [
      "Hui",
      "Bo",
      ""
    ],
    [
      "Shao",
      "Zezhi",
      ""
    ],
    [
      "Deng",
      "Liwei",
      ""
    ],
    [
      "Liu",
      "Xu",
      ""
    ],
    [
      "Jiang",
      "Xinke",
      ""
    ],
    [
      "Zheng",
      "Kai",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Fri, 13 Dec 2024 08:59:18 GMT"
    },
    {
      "version": "v2",
      "created": "Tue, 31 Dec 2024 03:52:09 GMT"
    }
  ],
  "updateDate": "2025-01-03",
  "timestamp": 1734080358000,
  "abstract": "  Road traffic forecasting is crucial in real-world intelligent transportation\nscenarios like traffic dispatching and path planning in city management and\npersonal traveling. Spatio-temporal graph neural networks (STGNNs) stand out as\nthe mainstream solution in this task. Nevertheless, the quadratic complexity of\nremarkable dynamic spatial modeling-based STGNNs has become the bottleneck over\nlarge-scale traffic data. From the spatial data management perspective, we\npresent a novel Transformer framework called PatchSTG to efficiently and\ndynamically model spatial dependencies for large-scale traffic forecasting with\ninterpretability and fidelity. Specifically, we design a novel irregular\nspatial patching to reduce the number of points involved in the dynamic\ncalculation of Transformer. The irregular spatial patching first utilizes the\nleaf K-dimensional tree (KDTree) to recursively partition irregularly\ndistributed traffic points into leaf nodes with a small capacity, and then\nmerges leaf nodes belonging to the same subtree into occupancy-equaled and\nnon-overlapped patches through padding and backtracking. Based on the patched\ndata, depth and breadth attention are used interchangeably in the encoder to\ndynamically learn local and global spatial knowledge from points in a patch and\npoints with the same index of patches. Experimental results on four real world\nlarge-scale traffic datasets show that our PatchSTG achieves train speed and\nmemory utilization improvements up to $10\\times$ and $4\\times$ with the\nstate-of-the-art performance.\n",
  "subjects": [
    "Computer Science/Machine Learning",
    "Computer Science/Artificial Intelligence"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  "blobId": "nzbY4Ek9-b3RF0EnJGLInM2HOwY8k9i765OQ9rgsUBw",
  "pdfSize": "4973327"
}