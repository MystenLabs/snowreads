{"id":"2412.00661","title":"Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning","authors":"Emile Anand, Ishani Karmarkar, Guannan Qu","authorsParsed":[["Anand","Emile",""],["Karmarkar","Ishani",""],["Qu","Guannan",""]],"versions":[{"version":"v1","created":"Sun, 1 Dec 2024 03:45:17 GMT"},{"version":"v2","created":"Wed, 29 Jan 2025 22:54:55 GMT"}],"updateDate":"2025-01-31","timestamp":1733024717000,"abstract":"  Designing efficient algorithms for multi-agent reinforcement learning (MARL)\nis fundamentally challenging because the size of the joint state and action\nspaces grows exponentially in the number of agents. These difficulties are\nexacerbated when balancing sequential global decision-making with local agent\ninteractions. In this work, we propose a new algorithm $\\texttt{SUBSAMPLE-MFQ}$\n($\\textbf{Subsample}$-$\\textbf{M}$ean-$\\textbf{F}$ield-$\\textbf{Q}$-learning)\nand a decentralized randomized policy for a system with $n$ agents. For $k\\leq\nn$, our algorithm learns a policy for the system in time polynomial in $k$. We\nshow that this learned policy converges to the optimal policy on the order of\n$\\tilde{O}(1/\\sqrt{k})$ as the number of subsampled agents $k$ increases. We\nempirically validate our method in Gaussian squeeze and global exploration\nsettings.\n","subjects":["Computer Science/Machine Learning","Computer Science/Artificial Intelligence","Computer Science/Multiagent Systems","Computer Science/Systems and Control","Electrical Engineering and Systems Science/Systems and Control","Mathematics/Optimization and Control"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"JPLOHCSwEsjP7PQeEBo5JX7-CQhTkFYdUXEjgkpG0lA","pdfSize":"990128"}