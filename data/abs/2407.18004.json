{"id":"2407.18004","title":"Optimal Broadcast Schedules in Logarithmic Time with Applications to\n  Broadcast, All-Broadcast, Reduction and All-Reduction","authors":"Jesper Larsson Tr\\\"aff","authorsParsed":[["Tr√§ff","Jesper Larsson",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 12:59:59 GMT"},{"version":"v2","created":"Fri, 26 Jul 2024 09:28:51 GMT"}],"updateDate":"2024-07-29","timestamp":1721912399000,"abstract":"  We give optimally fast $O(\\log p)$ time (per processor) algorithms for\ncomputing round-optimal broadcast schedules for message-passing parallel\ncomputing systems. This affirmatively answers difficult questions posed in a\nSPAA 2022 BA and a CLUSTER 2022 paper. We observe that the computed schedules\nand circulant communication graph can likewise be used for reduction,\nall-broadcast and all-reduction as well, leading to new, round-optimal\nalgorithms for these problems. These observations affirmatively answer open\nquestions posed in a CLUSTER 2023 paper.\n  The problem is to broadcast $n$ indivisible blocks of data from a given root\nprocessor to all other processors in a (subgraph of a) fully connected network\nof $p$ processors with fully bidirectional, one-ported communication\ncapabilities. In this model, $n-1+\\lceil\\log_2 p\\rceil$ communication rounds\nare required. Our new algorithms compute for each processor in the network\nreceive and send schedules each of size $\\lceil\\log_2 p\\rceil$ that determine\nuniquely in $O(1)$ time for each communication round the new block that the\nprocessor will receive, and the already received block it has to send. Schedule\ncomputations are done independently per processor without communication. The\nbroadcast communication subgraph is an easily computable, directed,\n$\\lceil\\log_2 p\\rceil$-regular circulant graph also used elsewhere. We show how\nthe schedule computations can be done in optimal time and space of $O(\\log p)$,\nimproving significantly over previous results of $O(p\\log^2 p)$ and $O(\\log^3\np)$, respectively. The schedule computation and broadcast algorithms are simple\nto implement, but correctness and complexity are not obvious. The schedules are\nused for new implementations of the MPI (Message-Passing Interface) collectives\nMPI_Bcast, MPI_Allgatherv, MPI_Reduce and MPI_Reduce_scatter. Preliminary\nexperimental results are given.\n","subjects":["Computing Research Repository/Distributed, Parallel, and Cluster Computing"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"97unMRGXXGpuiqqvvaPR3h2NLE1ZiTQMru-0Kc7sFz4","pdfSize":"482846"}