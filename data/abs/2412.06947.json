{
  "id": "2412.06947",
  "title": "PyraNet: A Large Scale Hierarchical Verilog Dataset",
  "authors": "Bardia Nadimi and Ghali Omar Boutaib and Hao Zheng",
  "authorsParsed": [
    [
      "Nadimi",
      "Bardia",
      ""
    ],
    [
      "Boutaib",
      "Ghali Omar",
      ""
    ],
    [
      "Zheng",
      "Hao",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Mon, 9 Dec 2024 19:45:54 GMT"
    },
    {
      "version": "v2",
      "created": "Fri, 27 Dec 2024 01:07:02 GMT"
    }
  ],
  "updateDate": "2024-12-30",
  "timestamp": 1733773554000,
  "abstract": "  Recently, there has been a growing interest in leveraging Large Language\nModels for Verilog code generation. However, the current quality of the\ngenerated Verilog code remains suboptimal. This is largely due to the absence\nof well-defined, well-organized datasets with high-quality samples, as well as\na lack of innovative fine-tuning methods and models specifically trained on\nVerilog. In this paper, we introduce a novel open-source dataset and a\ncorresponding fine-tuning technique, which utilizes a multi-layered structure\nthat we refer to as PyraNet. Our experiments demonstrate that employing the\nproposed dataset and fine-tuning approach leads to a more accurate fine-tuned\nmodel, producing syntactically and functionally correct Verilog code. The\nevaluation results show improvements by up-to $32.6\\%$ in comparison to the\nCodeLlama-7B baseline model and up-to $16.7\\%$ in comparison to the\nstate-of-the-art models using VerilogEval evaluation platform.\n",
  "subjects": [
    "Computer Science/Hardware Architecture",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning",
    "Computer Science/Programming Languages"
  ],
  "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  "blobId": "-x14ZKpL1R6ZwKB_7bsK9JluaJ4VRUDgJcpewLryS0M",
  "pdfSize": "2533184"
}