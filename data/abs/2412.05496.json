{"id":"2412.05496","title":"Flex Attention: A Programming Model for Generating Optimized Attention\n  Kernels","authors":"Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, Horace He","authorsParsed":[["Dong","Juechu",""],["Feng","Boyuan",""],["Guessous","Driss",""],["Liang","Yanbo",""],["He","Horace",""]],"versions":[{"version":"v1","created":"Sat, 7 Dec 2024 01:46:38 GMT"}],"updateDate":"2024-12-10","timestamp":1733535998000,"abstract":"  Over the past 7 years, attention has become one of the most important\nprimitives in deep learning. The primary approach to optimize attention is\nFlashAttention, which fuses the operation together, drastically improving both\nthe runtime and the memory consumption. However, the importance of\nFlashAttention combined with its monolithic nature poses a problem for\nresearchers aiming to try new attention variants -- a \"software lottery\". This\nproblem is exacerbated by the difficulty of writing efficient fused attention\nkernels, resisting traditional compiler-based approaches. We introduce\nFlexAttention, a novel compiler-driven programming model that allows\nimplementing the majority of attention variants in a few lines of idiomatic\nPyTorch code. We demonstrate that many existing attention variants (e.g. Alibi,\nDocument Masking, PagedAttention, etc.) can be implemented via FlexAttention,\nand that we achieve competitive performance compared to these handwritten\nkernels. Finally, we demonstrate how FlexAttention allows for easy composition\nof attention variants, solving the combinatorial explosion of attention\nvariants.\n","subjects":["Computer Science/Machine Learning","Computer Science/Performance","Computer Science/Programming Languages"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"XBSjX4vrR5B0fmjWdzkUczaVshvv97ei-V8oVKKMf-A","pdfSize":"3724167"}