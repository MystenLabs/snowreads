{"id":"2412.10381","title":"Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream\n  Allocation in Feed","authors":"Jingxin Liu, Xiang Gao, Yisha Li, Xin Li, Haiyang Lu, Ben Wang","authorsParsed":[["Liu","Jingxin",""],["Gao","Xiang",""],["Li","Yisha",""],["Li","Xin",""],["Lu","Haiyang",""],["Wang","Ben",""]],"versions":[{"version":"v1","created":"Thu, 28 Nov 2024 04:06:02 GMT"},{"version":"v2","created":"Thu, 23 Jan 2025 15:03:43 GMT"},{"version":"v3","created":"Fri, 24 Jan 2025 12:25:30 GMT"},{"version":"v4","created":"Fri, 31 Jan 2025 09:43:40 GMT"}],"updateDate":"2025-02-03","timestamp":1732766762000,"abstract":"  In the context of a short video & live stream mixed recommendation scenario,\nthe live stream recommendation system (RS) decides whether to allocate at most\none live stream into the video feed for each user request. To maximize\nlong-term user engagement, it is crucial to determine an optimal live stream\npolicy for accurate live stream allocation. The inappropriate live stream\nallocation policy can significantly affect the duration of the usage app and\nuser retention, which ignores the long-term negative impact of live stream\nallocation. Recently, reinforcement learning (RL) has been widely applied in\nrecommendation systems to capture long-term user engagement. However,\ntraditional RL algorithms often face divergence and instability problems, which\nrestricts the application and deployment in the large-scale industrial\nrecommendation systems, especially in the aforementioned challenging scenario.\nTo address these challenges, we propose a novel Supervised Learning-enhanced\nMulti-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a\nsupervised learning-enhanced actor-critic framework that incorporates variance\nreduction techniques, where multi-task reward learning helps restrict\nbootstrapping error accumulation during critic learning. Additionally, we\ndesign a multi-group state decomposition module for both actor and critic\nnetworks to reduce prediction variance and improve model stability. We also\npropose a novel reward function to prevent overly greedy live stream\nallocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy\nevaluation (OPE) and online A/B testing. Experimental results demonstrate that\nthe proposed method not only outperforms baseline methods under the\nplatform-level constraints but also exhibits enhanced stability in online\nrecommendation scenarios.\n","subjects":["Computer Science/Information Retrieval","Computer Science/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"RbXIZHhmb3yPx8L3Lt0ex5LC2gp5T1J4pHyYWvWvhmg","pdfSize":"1070615"}