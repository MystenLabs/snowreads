{"id":"2407.18143","title":"Maximum Entropy On-Policy Actor-Critic via Entropy Advantage Estimation","authors":"Jean Seong Bjorn Choe and Jong-Kook Kim","authorsParsed":[["Choe","Jean Seong Bjorn",""],["Kim","Jong-Kook",""]],"versions":[{"version":"v1","created":"Thu, 25 Jul 2024 15:48:24 GMT"}],"updateDate":"2024-07-26","timestamp":1721922504000,"abstract":"  Entropy Regularisation is a widely adopted technique that enhances policy\noptimisation performance and stability. A notable form of entropy\nregularisation is augmenting the objective with an entropy term, thereby\nsimultaneously optimising the expected return and the entropy. This framework,\nknown as maximum entropy reinforcement learning (MaxEnt RL), has shown\ntheoretical and empirical successes. However, its practical application in\nstraightforward on-policy actor-critic settings remains surprisingly\nunderexplored. We hypothesise that this is due to the difficulty of managing\nthe entropy reward in practice. This paper proposes a simple method of\nseparating the entropy objective from the MaxEnt RL objective, which\nfacilitates the implementation of MaxEnt RL in on-policy settings. Our\nempirical evaluations demonstrate that extending Proximal Policy Optimisation\n(PPO) and Trust Region Policy Optimisation (TRPO) within the MaxEnt framework\nimproves policy optimisation performance in both MuJoCo and Procgen tasks.\nAdditionally, our results highlight MaxEnt RL's capacity to enhance\ngeneralisation.\n","subjects":["Computing Research Repository/Machine Learning","Computing Research Repository/Artificial Intelligence"],"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","blobId":"gmi5mCvwsVrrtRg4Aux_zz_XEtm7es-0mt4gB3qEy0U","pdfSize":"2385208"}