{"id":"2412.16974","title":"Cannot or Should Not? Automatic Analysis of Refusal Composition in\n  IFT/RLHF Datasets and Refusal Behavior of Black-Box LLMs","authors":"Alexander von Recum, Christoph Schnabl, Gabor Hollbeck, Silas Alberti,\n  Philip Blinde, Marvin von Hagen","authorsParsed":[["von Recum","Alexander",""],["Schnabl","Christoph",""],["Hollbeck","Gabor",""],["Alberti","Silas",""],["Blinde","Philip",""],["von Hagen","Marvin",""]],"versions":[{"version":"v1","created":"Sun, 22 Dec 2024 11:16:53 GMT"}],"updateDate":"2024-12-24","timestamp":1734866213000,"abstract":"  Refusals - instances where large language models (LLMs) decline or fail to\nfully execute user instructions - are crucial for both AI safety and AI\ncapabilities and the reduction of hallucinations in particular. These behaviors\nare learned during post-training, especially in instruction fine-tuning (IFT)\nand reinforcement learning from human feedback (RLHF). However, existing\ntaxonomies and evaluation datasets for refusals are inadequate, often focusing\nsolely on should-not-related (instead of cannot-related) categories, and\nlacking tools for auditing refusal content in black-box LLM outputs.\n  We present a comprehensive framework for classifying LLM refusals: (a) a\ntaxonomy of 16 refusal categories, (b) a human-annotated dataset of over 8,600\ninstances from publicly available IFT and RLHF datasets, (c) a synthetic\ndataset with 8,000 examples for each refusal category, and (d) classifiers\ntrained for refusal classification.\n  Our work enables precise auditing of refusal behaviors in black-box LLMs and\nautomatic analyses of refusal patterns in large IFT and RLHF datasets. This\nfacilitates the strategic adjustment of LLM refusals, contributing to the\ndevelopment of more safe and reliable LLMs.\n","subjects":["Computer Science/Artificial Intelligence","Computer Science/Computation and Language"],"license":"http://creativecommons.org/licenses/by/4.0/","blobId":"LNPb3eAhDBkPQv8nEYx3jaJ4gPaXFBy_LMEUBICds_U","pdfSize":"8677465"}