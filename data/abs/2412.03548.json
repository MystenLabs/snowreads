{
  "id": "2412.03548",
  "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
  "authors": "Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen,\n  Linda G. Shapiro, Ranjay Krishna",
  "authorsParsed": [
    [
      "Bigverdi",
      "Mahtab",
      ""
    ],
    [
      "Luo",
      "Zelun",
      ""
    ],
    [
      "Hsieh",
      "Cheng-Yu",
      ""
    ],
    [
      "Shen",
      "Ethan",
      ""
    ],
    [
      "Chen",
      "Dongping",
      ""
    ],
    [
      "Shapiro",
      "Linda G.",
      ""
    ],
    [
      "Krishna",
      "Ranjay",
      ""
    ]
  ],
  "versions": [
    {
      "version": "v1",
      "created": "Wed, 4 Dec 2024 18:45:35 GMT"
    },
    {
      "version": "v2",
      "created": "Sun, 8 Dec 2024 05:18:30 GMT"
    }
  ],
  "updateDate": "2024-12-10",
  "timestamp": 1733337935000,
  "abstract": "  Multimodal language models (MLMs) still face challenges in fundamental visual\nperception tasks where specialized models excel. Tasks requiring reasoning\nabout 3D structures benefit from depth estimation, and reasoning about 2D\nobject instances benefits from object detection. Yet, MLMs can not produce\nintermediate depth or boxes to reason over. Finetuning MLMs on relevant data\ndoesn't generalize well and outsourcing computation to specialized vision tools\nis too compute-intensive and memory-inefficient. To address this, we introduce\nPerception Tokens, intrinsic image representations designed to assist reasoning\ntasks where language is insufficient. Perception tokens act as auxiliary\nreasoning tokens, akin to chain-of-thought prompts in language models. For\nexample, in a depth-related task, an MLM augmented with perception tokens can\nreason by generating a depth map as tokens, enabling it to solve the problem\neffectively. We propose AURORA, a training method that augments MLMs with\nperception tokens for improved reasoning over visual inputs. AURORA leverages a\nVQVAE to transform intermediate image representations, such as depth maps into\na tokenized format and bounding box tokens, which is then used in a multi-task\ntraining framework. AURORA achieves notable improvements across counting\nbenchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench,\noutperforming finetuning approaches in generalization across datasets. It also\nimproves on relative depth: over +6% on BLINK. With perception tokens, AURORA\nexpands the scope of MLMs beyond language-based reasoning, paving the way for\nmore effective visual reasoning capabilities.\n",
  "subjects": [
    "Computer Science/Computer Vision and Pattern Recognition",
    "Computer Science/Artificial Intelligence",
    "Computer Science/Machine Learning"
  ],
  "license": "http://creativecommons.org/licenses/by/4.0/",
  "blobId": "euJq26--Cwk3_juHt_G0beGVKaniVfTfTIQEKlPr6-8",
  "pdfSize": "7393579"
}